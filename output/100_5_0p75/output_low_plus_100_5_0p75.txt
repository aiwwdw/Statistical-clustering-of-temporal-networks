nohup: ignoring input
  0%|          | 0/100 [00:00<?, ?it/s]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/adj_generator.py:108: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)
  Y = torch.tensor(Y)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  1%|          | 1/100 [48:28<79:59:03, 2908.52s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  2%|▏         | 2/100 [1:42:57<84:57:11, 3120.73s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  3%|▎         | 3/100 [2:27:03<78:14:45, 2903.98s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  4%|▍         | 4/100 [3:18:00<79:02:38, 2964.15s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  5%|▌         | 5/100 [4:00:50<74:28:37, 2822.29s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  6%|▌         | 6/100 [4:54:35<77:15:56, 2959.11s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  7%|▋         | 7/100 [5:39:42<74:18:36, 2876.52s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  8%|▊         | 8/100 [6:17:55<68:45:47, 2690.73s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  9%|▉         | 9/100 [7:05:01<69:05:15, 2733.14s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
-----------------------------------------------------------------------------------------
This iteration is 0
True Objective function: Loss = -10815.607276960178
Iteration 0: Loss = -19140.85058157874
Iteration 10: Loss = -10929.745309294234
Iteration 20: Loss = -10929.25384834273
Iteration 30: Loss = -10928.725032104847
Iteration 40: Loss = -10928.029664738242
Iteration 50: Loss = -10927.232759961207
Iteration 60: Loss = -10926.588266606648
Iteration 70: Loss = -10926.277573046767
Iteration 80: Loss = -10926.23625993907
Iteration 90: Loss = -10926.25730485385
1
Iteration 100: Loss = -10926.272824470378
2
Iteration 110: Loss = -10926.280422794109
3
Stopping early at iteration 109 due to no improvement.
pi: tensor([[0.9690, 0.0310],
        [0.9806, 0.0194]], dtype=torch.float64)
alpha: tensor([0.9693, 0.0307])
beta: tensor([[[0.1571, 0.1679],
         [0.0031, 0.1617]],

        [[0.6577, 0.1566],
         [0.9595, 0.4434]],

        [[0.3312, 0.1429],
         [0.2447, 0.6067]],

        [[0.7351, 0.2844],
         [0.3676, 0.3287]],

        [[0.3779, 0.2483],
         [0.3083, 0.8617]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.009009494263563287
Global Adjusted Rand Index: 0.000692393593012233
Average Adjusted Rand Index: -0.001954123955583528
tensor([[-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19289.08588153118
Iteration 100: Loss = -10935.213296442334
Iteration 200: Loss = -10933.038241448598
Iteration 300: Loss = -10932.18273951885
Iteration 400: Loss = -10930.355358157698
Iteration 500: Loss = -10929.07199066199
Iteration 600: Loss = -10928.410263141435
Iteration 700: Loss = -10928.123242641579
Iteration 800: Loss = -10927.953021801088
Iteration 900: Loss = -10927.839767337839
Iteration 1000: Loss = -10927.76218813135
Iteration 1100: Loss = -10927.706324298733
Iteration 1200: Loss = -10927.66405502772
Iteration 1300: Loss = -10927.630910240772
Iteration 1400: Loss = -10927.604461299437
Iteration 1500: Loss = -10927.583015940081
Iteration 1600: Loss = -10927.565638595168
Iteration 1700: Loss = -10927.551458080627
Iteration 1800: Loss = -10927.539954197073
Iteration 1900: Loss = -10927.53062272911
Iteration 2000: Loss = -10927.52302775378
Iteration 2100: Loss = -10927.51688342878
Iteration 2200: Loss = -10927.511767297008
Iteration 2300: Loss = -10927.50752660204
Iteration 2400: Loss = -10927.503925274877
Iteration 2500: Loss = -10927.500789867336
Iteration 2600: Loss = -10927.498008757217
Iteration 2700: Loss = -10927.4955239306
Iteration 2800: Loss = -10927.493345836529
Iteration 2900: Loss = -10927.491344632272
Iteration 3000: Loss = -10927.48948404434
Iteration 3100: Loss = -10927.487787822403
Iteration 3200: Loss = -10927.486242831326
Iteration 3300: Loss = -10927.484847558933
Iteration 3400: Loss = -10927.483497596493
Iteration 3500: Loss = -10927.482252918842
Iteration 3600: Loss = -10927.481183789941
Iteration 3700: Loss = -10927.480708851632
Iteration 3800: Loss = -10927.48469371664
1
Iteration 3900: Loss = -10927.484317556908
2
Iteration 4000: Loss = -10927.482132005627
3
Iteration 4100: Loss = -10927.478712062288
Iteration 4200: Loss = -10927.491073031155
1
Iteration 4300: Loss = -10927.475029225152
Iteration 4400: Loss = -10927.474508833515
Iteration 4500: Loss = -10927.558217972479
1
Iteration 4600: Loss = -10927.473126372977
Iteration 4700: Loss = -10927.472662318409
Iteration 4800: Loss = -10927.472094051664
Iteration 4900: Loss = -10927.471553070147
Iteration 5000: Loss = -10927.528592557142
1
Iteration 5100: Loss = -10927.470634346486
Iteration 5200: Loss = -10927.470265865293
Iteration 5300: Loss = -10927.469821548912
Iteration 5400: Loss = -10927.471246209238
1
Iteration 5500: Loss = -10927.469151357493
Iteration 5600: Loss = -10927.46881212616
Iteration 5700: Loss = -10927.606838545995
1
Iteration 5800: Loss = -10927.468204625893
Iteration 5900: Loss = -10927.467924749739
Iteration 6000: Loss = -10927.467835896912
Iteration 6100: Loss = -10927.467497002319
Iteration 6200: Loss = -10927.467209667642
Iteration 6300: Loss = -10927.466984991792
Iteration 6400: Loss = -10927.468357773527
1
Iteration 6500: Loss = -10927.466596421751
Iteration 6600: Loss = -10927.466416971698
Iteration 6700: Loss = -10927.466221739074
Iteration 6800: Loss = -10927.466064694037
Iteration 6900: Loss = -10927.465914382296
Iteration 7000: Loss = -10927.465736880707
Iteration 7100: Loss = -10927.53833336942
1
Iteration 7200: Loss = -10927.46545150297
Iteration 7300: Loss = -10927.465333901993
Iteration 7400: Loss = -10927.465203658903
Iteration 7500: Loss = -10927.482339262588
1
Iteration 7600: Loss = -10927.465000608603
Iteration 7700: Loss = -10927.464919153868
Iteration 7800: Loss = -10927.464794646177
Iteration 7900: Loss = -10927.465974028131
1
Iteration 8000: Loss = -10927.464651229548
Iteration 8100: Loss = -10927.464571039727
Iteration 8200: Loss = -10927.464744543915
1
Iteration 8300: Loss = -10927.464436737393
Iteration 8400: Loss = -10927.464294014793
Iteration 8500: Loss = -10927.464216299297
Iteration 8600: Loss = -10927.766441604186
1
Iteration 8700: Loss = -10927.464075079946
Iteration 8800: Loss = -10927.46399598424
Iteration 8900: Loss = -10927.463954262354
Iteration 9000: Loss = -10927.466630328216
1
Iteration 9100: Loss = -10927.4638260833
Iteration 9200: Loss = -10927.46379189652
Iteration 9300: Loss = -10927.46399050368
1
Iteration 9400: Loss = -10927.463708036817
Iteration 9500: Loss = -10927.463617719499
Iteration 9600: Loss = -10927.463574084573
Iteration 9700: Loss = -10927.463810213176
1
Iteration 9800: Loss = -10927.46353092344
Iteration 9900: Loss = -10927.463480441109
Iteration 10000: Loss = -10927.49631505339
1
Iteration 10100: Loss = -10927.463441826494
Iteration 10200: Loss = -10927.463417694507
Iteration 10300: Loss = -10927.463340563781
Iteration 10400: Loss = -10927.470958437547
1
Iteration 10500: Loss = -10927.463334294176
Iteration 10600: Loss = -10927.463297790997
Iteration 10700: Loss = -10927.486185092162
1
Iteration 10800: Loss = -10927.463217011746
Iteration 10900: Loss = -10927.463214312718
Iteration 11000: Loss = -10927.463160084664
Iteration 11100: Loss = -10927.463674741884
1
Iteration 11200: Loss = -10927.46303856255
Iteration 11300: Loss = -10927.462997280669
Iteration 11400: Loss = -10927.476972176693
1
Iteration 11500: Loss = -10927.462859198853
Iteration 11600: Loss = -10927.462783139445
Iteration 11700: Loss = -10927.609989184773
1
Iteration 11800: Loss = -10927.462615473058
Iteration 11900: Loss = -10927.462489755078
Iteration 12000: Loss = -10927.462391092049
Iteration 12100: Loss = -10927.468200464085
1
Iteration 12200: Loss = -10927.462073722609
Iteration 12300: Loss = -10927.461928858154
Iteration 12400: Loss = -10927.518198398766
1
Iteration 12500: Loss = -10927.46163716239
Iteration 12600: Loss = -10927.46148652702
Iteration 12700: Loss = -10927.462866329603
1
Iteration 12800: Loss = -10927.461236369702
Iteration 12900: Loss = -10927.461816017567
1
Iteration 13000: Loss = -10927.460957566462
Iteration 13100: Loss = -10927.463301260663
1
Iteration 13200: Loss = -10927.460490720581
Iteration 13300: Loss = -10927.47723819194
1
Iteration 13400: Loss = -10927.558171742174
2
Iteration 13500: Loss = -10926.015799161547
Iteration 13600: Loss = -10925.800945538409
Iteration 13700: Loss = -10925.786638722804
Iteration 13800: Loss = -10925.781254705737
Iteration 13900: Loss = -10925.778123556762
Iteration 14000: Loss = -10925.776606618341
Iteration 14100: Loss = -10925.787909039815
1
Iteration 14200: Loss = -10925.78061425934
2
Iteration 14300: Loss = -10925.83565741766
3
Iteration 14400: Loss = -10925.775737189624
Iteration 14500: Loss = -10925.77254998981
Iteration 14600: Loss = -10925.77293723728
1
Iteration 14700: Loss = -10925.8369299495
2
Iteration 14800: Loss = -10925.772255038682
Iteration 14900: Loss = -10925.77237148683
1
Iteration 15000: Loss = -10925.773551598046
2
Iteration 15100: Loss = -10925.771107041686
Iteration 15200: Loss = -10925.770948199686
Iteration 15300: Loss = -10925.77136927408
1
Iteration 15400: Loss = -10925.770797372565
Iteration 15500: Loss = -10925.781855611214
1
Iteration 15600: Loss = -10925.770519605901
Iteration 15700: Loss = -10925.788837948809
1
Iteration 15800: Loss = -10925.770368739939
Iteration 15900: Loss = -10925.770215935674
Iteration 16000: Loss = -10925.770226816461
1
Iteration 16100: Loss = -10925.7701912514
Iteration 16200: Loss = -10925.775353031853
1
Iteration 16300: Loss = -10925.770010596732
Iteration 16400: Loss = -10925.769990576819
Iteration 16500: Loss = -10925.772651756777
1
Iteration 16600: Loss = -10925.769971679656
Iteration 16700: Loss = -10925.77011580849
1
Iteration 16800: Loss = -10925.787896198923
2
Iteration 16900: Loss = -10925.777049534228
3
Iteration 17000: Loss = -10925.774093738763
4
Iteration 17100: Loss = -10925.769922779073
Iteration 17200: Loss = -10925.771488159213
1
Iteration 17300: Loss = -10925.769440384252
Iteration 17400: Loss = -10925.769995386901
1
Iteration 17500: Loss = -10925.76942220242
Iteration 17600: Loss = -10925.841214598038
1
Iteration 17700: Loss = -10925.769500047496
2
Iteration 17800: Loss = -10925.769550204353
3
Iteration 17900: Loss = -10925.8186938137
4
Iteration 18000: Loss = -10925.769374134916
Iteration 18100: Loss = -10925.80496207376
1
Iteration 18200: Loss = -10925.769320798676
Iteration 18300: Loss = -10925.813523780937
1
Iteration 18400: Loss = -10925.769632587577
2
Iteration 18500: Loss = -10925.769647206018
3
Iteration 18600: Loss = -10925.791815258723
4
Iteration 18700: Loss = -10925.821122681908
5
Iteration 18800: Loss = -10925.77326704752
6
Iteration 18900: Loss = -10925.769657877128
7
Iteration 19000: Loss = -10925.77124716075
8
Iteration 19100: Loss = -10925.769478416796
9
Iteration 19200: Loss = -10925.769368897978
10
Stopping early at iteration 19200 due to no improvement.
tensor([[-7.2135,  2.5983],
        [-7.2247,  2.6095],
        [-7.2134,  2.5981],
        [-7.2162,  2.6010],
        [-7.2228,  2.6076],
        [-7.2106,  2.5954],
        [-7.2247,  2.6095],
        [-7.2246,  2.6094],
        [-7.2247,  2.6094],
        [-7.2048,  2.5896],
        [-7.2216,  2.6064],
        [-7.2248,  2.6096],
        [-7.2102,  2.5950],
        [-7.2244,  2.6092],
        [-7.2162,  2.6010],
        [-7.2247,  2.6095],
        [-7.2248,  2.6096],
        [-7.2239,  2.6087],
        [-7.2244,  2.6091],
        [-7.2185,  2.6033],
        [-7.2215,  2.6063],
        [-7.2216,  2.6064],
        [-7.2247,  2.6094],
        [-7.2247,  2.6094],
        [-7.2128,  2.5976],
        [-7.2189,  2.6037],
        [-7.2239,  2.6087],
        [-7.2248,  2.6095],
        [-7.2191,  2.6039],
        [-7.2244,  2.6092],
        [-7.2162,  2.6010],
        [-7.2245,  2.6092],
        [-7.2050,  2.5898],
        [-7.2249,  2.6097],
        [-7.2235,  2.6082],
        [-7.2049,  2.5897],
        [-7.2249,  2.6097],
        [-7.2162,  2.6010],
        [-7.2246,  2.6094],
        [-7.2245,  2.6093],
        [-7.2242,  2.6090],
        [-7.2235,  2.6083],
        [-7.2161,  2.6009],
        [-7.2235,  2.6083],
        [-7.2078,  2.5926],
        [-7.2216,  2.6064],
        [-7.2051,  2.5899],
        [-7.2104,  2.5951],
        [-7.2235,  2.6082],
        [-7.2247,  2.6095],
        [-7.2249,  2.6096],
        [-7.2235,  2.6082],
        [-7.2216,  2.6063],
        [-7.2245,  2.6093],
        [-7.2022,  2.5870],
        [-7.2134,  2.5982],
        [-7.2244,  2.6092],
        [-7.2207,  2.6055],
        [-7.2135,  2.5983],
        [-7.2248,  2.6096],
        [-7.2246,  2.6093],
        [-7.2075,  2.5922],
        [-7.2248,  2.6096],
        [-7.2189,  2.6037],
        [-7.2136,  2.5983],
        [-7.2247,  2.6095],
        [-7.2189,  2.6037],
        [-7.2248,  2.6096],
        [-7.2161,  2.6009],
        [-7.2215,  2.6063],
        [-7.2246,  2.6094],
        [-7.2226,  2.6074],
        [-7.2235,  2.6083],
        [-7.2241,  2.6089],
        [-7.2249,  2.6097],
        [-7.2235,  2.6083],
        [-7.2189,  2.6037],
        [-7.2247,  2.6095],
        [-7.2235,  2.6083],
        [-7.2248,  2.6096],
        [-7.2235,  2.6083],
        [-7.2243,  2.6091],
        [-7.2245,  2.6093],
        [-7.2229,  2.6076],
        [-7.2162,  2.6010],
        [-7.2243,  2.6091],
        [-7.2249,  2.6097],
        [-7.2078,  2.5925],
        [-7.2250,  2.6098],
        [-7.2216,  2.6064],
        [-7.2215,  2.6063],
        [-7.2247,  2.6095],
        [-7.2216,  2.6064],
        [-7.2106,  2.5954],
        [-7.2248,  2.6096],
        [-7.2235,  2.6082],
        [-7.2246,  2.6094],
        [-7.2244,  2.6092],
        [-7.2235,  2.6082],
        [-7.2246,  2.6094]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.7791e-01, 2.2085e-02],
        [1.0000e+00, 4.5980e-08]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([5.4447e-05, 9.9995e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1592, 0.1606],
         [0.0031, 0.1610]],

        [[0.6577, 0.2234],
         [0.9595, 0.4434]],

        [[0.3312, 0.0667],
         [0.2447, 0.6067]],

        [[0.7351, 0.2900],
         [0.3676, 0.3287]],

        [[0.3779, 0.2538],
         [0.3083, 0.8617]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
Global Adjusted Rand Index: -0.0016111090894400308
Average Adjusted Rand Index: -0.0004385435808572055
Iteration 0: Loss = -22622.820246047362
Iteration 10: Loss = -10926.443951528061
Iteration 20: Loss = -10926.28410889376
Iteration 30: Loss = -10926.283304796365
Iteration 40: Loss = -10926.285108065453
1
Iteration 50: Loss = -10926.286280644683
2
Iteration 60: Loss = -10926.286915444289
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.9693, 0.0307],
        [0.9804, 0.0196]], dtype=torch.float64)
alpha: tensor([0.9696, 0.0304])
beta: tensor([[[0.1571, 0.1675],
         [0.3351, 0.1595]],

        [[0.7478, 0.1564],
         [0.3999, 0.1332]],

        [[0.8052, 0.1418],
         [0.0754, 0.3855]],

        [[0.3532, 0.2847],
         [0.7468, 0.5662]],

        [[0.3178, 0.2489],
         [0.0861, 0.6833]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.009009494263563287
Global Adjusted Rand Index: 0.000692393593012233
Average Adjusted Rand Index: -0.001954123955583528
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22622.78136952015
Iteration 100: Loss = -10957.811417056038
Iteration 200: Loss = -10931.029924097857
Iteration 300: Loss = -10928.458658638981
Iteration 400: Loss = -10927.737250385166
Iteration 500: Loss = -10927.32701395039
Iteration 600: Loss = -10927.062550397397
Iteration 700: Loss = -10926.871710202546
Iteration 800: Loss = -10926.730340041353
Iteration 900: Loss = -10926.617139393318
Iteration 1000: Loss = -10926.521079506818
Iteration 1100: Loss = -10926.437518006096
Iteration 1200: Loss = -10926.357458943117
Iteration 1300: Loss = -10926.293249465896
Iteration 1400: Loss = -10926.245012536843
Iteration 1500: Loss = -10926.202687870265
Iteration 1600: Loss = -10926.163414555269
Iteration 1700: Loss = -10926.129840481342
Iteration 1800: Loss = -10926.103227516709
Iteration 1900: Loss = -10926.079961867916
Iteration 2000: Loss = -10926.05855875583
Iteration 2100: Loss = -10926.03762607627
Iteration 2200: Loss = -10926.023913921024
Iteration 2300: Loss = -10926.015539068689
Iteration 2400: Loss = -10926.008931916991
Iteration 2500: Loss = -10926.003786540292
Iteration 2600: Loss = -10925.999846454384
Iteration 2700: Loss = -10925.996567664604
Iteration 2800: Loss = -10925.993661864648
Iteration 2900: Loss = -10925.990996098635
Iteration 3000: Loss = -10925.98829894393
Iteration 3100: Loss = -10925.985267252034
Iteration 3200: Loss = -10925.982332461961
Iteration 3300: Loss = -10925.980555214766
Iteration 3400: Loss = -10925.97919389943
Iteration 3500: Loss = -10925.977989154344
Iteration 3600: Loss = -10925.97690930741
Iteration 3700: Loss = -10925.975945491386
Iteration 3800: Loss = -10925.975041831856
Iteration 3900: Loss = -10925.97420803653
Iteration 4000: Loss = -10925.973425221368
Iteration 4100: Loss = -10925.972716002532
Iteration 4200: Loss = -10925.97200605747
Iteration 4300: Loss = -10925.971347055882
Iteration 4400: Loss = -10925.970757835086
Iteration 4500: Loss = -10925.970172067298
Iteration 4600: Loss = -10925.969621347896
Iteration 4700: Loss = -10925.969095442977
Iteration 4800: Loss = -10925.968630616864
Iteration 4900: Loss = -10925.968120383783
Iteration 5000: Loss = -10925.967636987441
Iteration 5100: Loss = -10925.967213848471
Iteration 5200: Loss = -10925.966791012488
Iteration 5300: Loss = -10925.966371544426
Iteration 5400: Loss = -10925.966007196519
Iteration 5500: Loss = -10925.965508936204
Iteration 5600: Loss = -10925.96475450195
Iteration 5700: Loss = -10925.954601875901
Iteration 5800: Loss = -10925.950864737933
Iteration 5900: Loss = -10925.947265261097
Iteration 6000: Loss = -10925.951871650172
1
Iteration 6100: Loss = -10925.938873886476
Iteration 6200: Loss = -10925.933307339932
Iteration 6300: Loss = -10925.94414589446
1
Iteration 6400: Loss = -10925.950553734205
2
Iteration 6500: Loss = -10925.92784527458
Iteration 6600: Loss = -10925.987382079997
1
Iteration 6700: Loss = -10925.936716924289
2
Iteration 6800: Loss = -10925.92321167581
Iteration 6900: Loss = -10925.925961502253
1
Iteration 7000: Loss = -10925.928145974525
2
Iteration 7100: Loss = -10925.924801486359
3
Iteration 7200: Loss = -10925.921037509996
Iteration 7300: Loss = -10925.925699837208
1
Iteration 7400: Loss = -10925.921365948043
2
Iteration 7500: Loss = -10925.91992024433
Iteration 7600: Loss = -10925.919735964608
Iteration 7700: Loss = -10925.919302788669
Iteration 7800: Loss = -10925.922877878786
1
Iteration 7900: Loss = -10925.918811285435
Iteration 8000: Loss = -10925.920460471858
1
Iteration 8100: Loss = -10925.918390706991
Iteration 8200: Loss = -10925.920597389197
1
Iteration 8300: Loss = -10925.917988074722
Iteration 8400: Loss = -10925.921654354348
1
Iteration 8500: Loss = -10925.917762859006
Iteration 8600: Loss = -10925.957812321294
1
Iteration 8700: Loss = -10925.917484752428
Iteration 8800: Loss = -10925.917341131393
Iteration 8900: Loss = -10925.919219234469
1
Iteration 9000: Loss = -10925.917159244427
Iteration 9100: Loss = -10925.93093834169
1
Iteration 9200: Loss = -10925.91701205476
Iteration 9300: Loss = -10925.916884778879
Iteration 9400: Loss = -10925.919819235321
1
Iteration 9500: Loss = -10925.916782573728
Iteration 9600: Loss = -10926.153426052135
1
Iteration 9700: Loss = -10925.916643416274
Iteration 9800: Loss = -10925.916572174061
Iteration 9900: Loss = -10925.929397949641
1
Iteration 10000: Loss = -10925.916501189924
Iteration 10100: Loss = -10925.922091713592
1
Iteration 10200: Loss = -10925.916370876566
Iteration 10300: Loss = -10925.91631539443
Iteration 10400: Loss = -10925.917023525053
1
Iteration 10500: Loss = -10925.916232036067
Iteration 10600: Loss = -10925.916272982093
1
Iteration 10700: Loss = -10925.916211921638
Iteration 10800: Loss = -10925.921342364889
1
Iteration 10900: Loss = -10925.916161099472
Iteration 11000: Loss = -10925.916057212999
Iteration 11100: Loss = -10925.933844019444
1
Iteration 11200: Loss = -10925.916004523608
Iteration 11300: Loss = -10925.915996834028
Iteration 11400: Loss = -10925.916183650812
1
Iteration 11500: Loss = -10925.91591040142
Iteration 11600: Loss = -10925.925943923286
1
Iteration 11700: Loss = -10925.915930723619
2
Iteration 11800: Loss = -10925.916201964668
3
Iteration 11900: Loss = -10926.167329713116
4
Iteration 12000: Loss = -10925.915874656872
Iteration 12100: Loss = -10925.95150435703
1
Iteration 12200: Loss = -10925.915787418724
Iteration 12300: Loss = -10925.963445409494
1
Iteration 12400: Loss = -10925.915785718553
Iteration 12500: Loss = -10925.939249013187
1
Iteration 12600: Loss = -10925.915750482509
Iteration 12700: Loss = -10925.915709775974
Iteration 12800: Loss = -10925.916333527168
1
Iteration 12900: Loss = -10925.915696509455
Iteration 13000: Loss = -10925.915667789523
Iteration 13100: Loss = -10925.915838527446
1
Iteration 13200: Loss = -10925.915675965982
2
Iteration 13300: Loss = -10925.943565855552
3
Iteration 13400: Loss = -10925.99636525067
4
Iteration 13500: Loss = -10925.915635956879
Iteration 13600: Loss = -10925.915840050664
1
Iteration 13700: Loss = -10925.919211093002
2
Iteration 13800: Loss = -10925.91575786339
3
Iteration 13900: Loss = -10925.915589964152
Iteration 14000: Loss = -10925.916515584964
1
Iteration 14100: Loss = -10925.915636604776
2
Iteration 14200: Loss = -10925.915574153876
Iteration 14300: Loss = -10925.916203270845
1
Iteration 14400: Loss = -10925.91562146366
2
Iteration 14500: Loss = -10925.915548839748
Iteration 14600: Loss = -10925.915773808425
1
Iteration 14700: Loss = -10925.915572489035
2
Iteration 14800: Loss = -10925.917385689261
3
Iteration 14900: Loss = -10925.915599218208
4
Iteration 15000: Loss = -10925.915525496808
Iteration 15100: Loss = -10925.915679037855
1
Iteration 15200: Loss = -10925.915496250691
Iteration 15300: Loss = -10925.917182609532
1
Iteration 15400: Loss = -10925.920603115239
2
Iteration 15500: Loss = -10926.083192228183
3
Iteration 15600: Loss = -10925.915554914569
4
Iteration 15700: Loss = -10925.91587014916
5
Iteration 15800: Loss = -10925.91550379685
6
Iteration 15900: Loss = -10925.916203648285
7
Iteration 16000: Loss = -10925.915516796444
8
Iteration 16100: Loss = -10925.91599701507
9
Iteration 16200: Loss = -10925.921732211276
10
Stopping early at iteration 16200 due to no improvement.
tensor([[-9.6733e-01, -3.0501e+00],
        [-1.2669e-02, -1.6940e+00],
        [-1.2627e+00, -3.3526e+00],
        [ 3.1626e-01, -1.7026e+00],
        [-4.6024e-01, -2.2748e+00],
        [-3.6081e-01, -2.5113e+00],
        [-2.3117e-01, -1.9094e+00],
        [ 9.5331e-03, -1.4073e+00],
        [ 4.6474e-02, -1.4370e+00],
        [ 3.7854e-01, -1.9040e+00],
        [ 2.4806e-01, -1.6354e+00],
        [ 1.1209e-01, -1.5044e+00],
        [-9.2861e-02, -2.2426e+00],
        [ 6.0491e-02, -1.6834e+00],
        [ 1.9108e-01, -1.8280e+00],
        [ 1.0873e-01, -1.5688e+00],
        [ 3.2262e-02, -1.4456e+00],
        [ 1.1117e-01, -1.6348e+00],
        [-3.7209e-01, -2.1167e+00],
        [-5.5631e-01, -2.5122e+00],
        [-2.0291e-01, -2.0857e+00],
        [-6.9604e-01, -2.5803e+00],
        [-1.2596e-01, -1.8057e+00],
        [-2.6975e-01, -1.8150e+00],
        [ 2.2331e-01, -1.8594e+00],
        [ 2.4507e-01, -1.7084e+00],
        [ 8.2730e-02, -1.6632e+00],
        [-4.6511e-01, -2.1459e+00],
        [-4.0305e-01, -2.3500e+00],
        [-5.5059e-01, -1.8329e+00],
        [ 1.6788e-01, -1.8529e+00],
        [ 1.1663e-01, -1.6345e+00],
        [ 2.6985e-01, -2.0090e+00],
        [ 5.3270e-02, -1.4954e+00],
        [ 2.0607e-01, -1.6083e+00],
        [-5.7108e-01, -2.8540e+00],
        [ 1.5371e-04, -1.6158e+00],
        [-1.2991e+00, -3.3161e+00],
        [ 1.3029e-01, -1.5475e+00],
        [-2.0257e-01, -1.9500e+00],
        [ 1.4211e-01, -1.5381e+00],
        [ 3.4567e-03, -1.8130e+00],
        [-1.2247e-01, -2.1441e+00],
        [ 1.9921e-01, -1.6149e+00],
        [ 2.7778e-01, -1.9373e+00],
        [-4.4803e-01, -2.3316e+00],
        [ 4.3368e-01, -1.8457e+00],
        [ 4.4418e-02, -2.1037e+00],
        [ 1.9584e-01, -1.6154e+00],
        [-2.4172e-02, -1.7042e+00],
        [-1.3260e+00, -2.9396e+00],
        [ 9.5449e-02, -1.7257e+00],
        [ 2.3013e-01, -1.6518e+00],
        [-5.9461e-01, -2.2108e+00],
        [ 3.5479e-01, -1.9921e+00],
        [-3.9609e-01, -2.4784e+00],
        [-3.1316e-02, -1.7124e+00],
        [-4.1704e-01, -2.3040e+00],
        [ 3.0665e-01, -1.7778e+00],
        [ 1.0847e-02, -1.5349e+00],
        [ 1.0129e-01, -1.5120e+00],
        [ 3.7463e-01, -1.8410e+00],
        [-4.0386e-01, -2.0866e+00],
        [-1.2497e+00, -3.2025e+00],
        [ 3.4163e-01, -1.7392e+00],
        [ 9.2799e-02, -1.5207e+00],
        [ 2.3071e-01, -1.7176e+00],
        [ 4.4954e-02, -1.6405e+00],
        [ 3.0370e-01, -1.7166e+00],
        [ 2.8197e-02, -1.8542e+00],
        [-2.1640e-01, -1.5009e+00],
        [ 1.7361e-02, -1.7926e+00],
        [ 1.8423e-01, -1.6333e+00],
        [-1.4699e+00, -3.1453e+00],
        [-2.1883e-01, -1.7063e+00],
        [-1.3635e-01, -1.9540e+00],
        [-1.7410e-01, -2.1246e+00],
        [ 2.6209e-02, -1.6543e+00],
        [-2.5362e-01, -2.0686e+00],
        [-3.5757e-01, -1.7111e+00],
        [-7.2480e-01, -2.5393e+00],
        [-1.6013e-03, -1.6814e+00],
        [-3.7063e-01, -2.1194e+00],
        [ 1.6270e-01, -1.6585e+00],
        [ 1.1474e-01, -1.9024e+00],
        [-1.7646e-01, -1.8554e+00],
        [ 1.1467e-01, -1.5012e+00],
        [ 2.2698e-01, -1.9895e+00],
        [ 1.2038e-02, -1.5381e+00],
        [ 1.7164e-01, -1.7156e+00],
        [-4.4723e-01, -2.3311e+00],
        [-2.3778e-01, -1.8555e+00],
        [ 2.0724e-01, -1.6778e+00],
        [-6.0649e-01, -2.7576e+00],
        [-1.5331e-01, -1.5087e+00],
        [ 1.0055e-01, -1.7193e+00],
        [-2.1302e-01, -1.7626e+00],
        [-4.9579e-01, -2.2459e+00],
        [ 1.5377e-01, -1.6600e+00],
        [ 8.3795e-02, -1.5960e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.6678e-01, 3.3219e-02],
        [9.9998e-01, 2.1590e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8587, 0.1413], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1586, 0.1663],
         [0.3351, 0.1782]],

        [[0.7478, 0.1594],
         [0.3999, 0.1332]],

        [[0.8052, 0.1508],
         [0.0754, 0.3855]],

        [[0.3532, 0.2820],
         [0.7468, 0.5662]],

        [[0.3178, 0.2455],
         [0.0861, 0.6833]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.009009494263563287
Global Adjusted Rand Index: 0.000692393593012233
Average Adjusted Rand Index: -0.001954123955583528
Iteration 0: Loss = -21424.5989886144
Iteration 10: Loss = -10931.62743239709
Iteration 20: Loss = -10930.84740418845
Iteration 30: Loss = -10927.418131769165
Iteration 40: Loss = -10927.36332674197
Iteration 50: Loss = -10927.36057143251
Iteration 60: Loss = -10927.359307866174
Iteration 70: Loss = -10927.358687486974
Iteration 80: Loss = -10927.3583715081
Iteration 90: Loss = -10927.358213198739
Iteration 100: Loss = -10927.358133640313
Iteration 110: Loss = -10927.358114192291
Iteration 120: Loss = -10927.35809997503
Iteration 130: Loss = -10927.358042585556
Iteration 140: Loss = -10927.358072846344
1
Iteration 150: Loss = -10927.358073975267
2
Iteration 160: Loss = -10927.358073494772
3
Stopping early at iteration 159 due to no improvement.
pi: tensor([[0.9811, 0.0189],
        [0.9798, 0.0202]], dtype=torch.float64)
alpha: tensor([0.9813, 0.0187])
beta: tensor([[[0.1594, 0.1441],
         [0.8744, 0.0771]],

        [[0.3128, 0.1476],
         [0.0997, 0.0808]],

        [[0.4767, 0.0682],
         [0.8618, 0.6783]],

        [[0.3934, 0.2949],
         [0.6429, 0.4217]],

        [[0.8645, 0.1167],
         [0.0224, 0.0975]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00032696474682979756
Average Adjusted Rand Index: 0.0005435279048114025
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21424.783871017495
Iteration 100: Loss = -10943.451225836063
Iteration 200: Loss = -10934.36963042102
Iteration 300: Loss = -10933.078298857567
Iteration 400: Loss = -10932.501595244876
Iteration 500: Loss = -10932.19466751704
Iteration 600: Loss = -10932.0111025968
Iteration 700: Loss = -10931.889173485733
Iteration 800: Loss = -10931.800253395355
Iteration 900: Loss = -10931.728597229845
Iteration 1000: Loss = -10931.646075871593
Iteration 1100: Loss = -10929.676652034428
Iteration 1200: Loss = -10929.192505019573
Iteration 1300: Loss = -10928.996319040683
Iteration 1400: Loss = -10928.87649511827
Iteration 1500: Loss = -10928.7698834985
Iteration 1600: Loss = -10928.642070936969
Iteration 1700: Loss = -10928.371210329233
Iteration 1800: Loss = -10927.734629013801
Iteration 1900: Loss = -10927.489025900402
Iteration 2000: Loss = -10927.350862674495
Iteration 2100: Loss = -10927.242868430538
Iteration 2200: Loss = -10927.14422053587
Iteration 2300: Loss = -10927.043091007994
Iteration 2400: Loss = -10926.92916060709
Iteration 2500: Loss = -10926.818493155071
Iteration 2600: Loss = -10926.711687040617
Iteration 2700: Loss = -10926.525317551614
Iteration 2800: Loss = -10926.416573048333
Iteration 2900: Loss = -10926.447715954202
1
Iteration 3000: Loss = -10926.245827636822
Iteration 3100: Loss = -10926.461222023121
1
Iteration 3200: Loss = -10926.121127531125
Iteration 3300: Loss = -10926.071873750689
Iteration 3400: Loss = -10926.112257872906
1
Iteration 3500: Loss = -10925.992585968363
Iteration 3600: Loss = -10925.961064218782
Iteration 3700: Loss = -10925.934270622425
Iteration 3800: Loss = -10925.910382428361
Iteration 3900: Loss = -10925.890052864683
Iteration 4000: Loss = -10925.8734944067
Iteration 4100: Loss = -10926.094530510229
1
Iteration 4200: Loss = -10925.849962296992
Iteration 4300: Loss = -10925.841564918468
Iteration 4400: Loss = -10925.834394733356
Iteration 4500: Loss = -10925.828174303282
Iteration 4600: Loss = -10925.822758004502
Iteration 4700: Loss = -10925.818399917078
Iteration 4800: Loss = -10925.818180309134
Iteration 4900: Loss = -10925.81174855599
Iteration 5000: Loss = -10925.809208866285
Iteration 5100: Loss = -10925.808619490983
Iteration 5200: Loss = -10925.805085438424
Iteration 5300: Loss = -10925.803487184836
Iteration 5400: Loss = -10925.802175652176
Iteration 5500: Loss = -10925.801264258216
Iteration 5600: Loss = -10925.799988442586
Iteration 5700: Loss = -10925.799067426982
Iteration 5800: Loss = -10925.898280053112
1
Iteration 5900: Loss = -10925.797527389104
Iteration 6000: Loss = -10925.796890494657
Iteration 6100: Loss = -10925.796288189526
Iteration 6200: Loss = -10925.795722540066
Iteration 6300: Loss = -10925.795198703809
Iteration 6400: Loss = -10925.794730144144
Iteration 6500: Loss = -10925.795712342895
1
Iteration 6600: Loss = -10925.793808022763
Iteration 6700: Loss = -10925.79344045261
Iteration 6800: Loss = -10925.834994654346
1
Iteration 6900: Loss = -10925.792736059586
Iteration 7000: Loss = -10925.792335313297
Iteration 7100: Loss = -10925.79201446542
Iteration 7200: Loss = -10925.791765739685
Iteration 7300: Loss = -10925.79138242913
Iteration 7400: Loss = -10925.791110535725
Iteration 7500: Loss = -10925.955003569312
1
Iteration 7600: Loss = -10925.790586624304
Iteration 7700: Loss = -10925.790300626797
Iteration 7800: Loss = -10925.790045999453
Iteration 7900: Loss = -10925.789893817857
Iteration 8000: Loss = -10925.789494065304
Iteration 8100: Loss = -10925.789195538564
Iteration 8200: Loss = -10925.789450525908
1
Iteration 8300: Loss = -10925.788664782855
Iteration 8400: Loss = -10925.78835251799
Iteration 8500: Loss = -10925.788055574412
Iteration 8600: Loss = -10925.787934860027
Iteration 8700: Loss = -10925.787286148416
Iteration 8800: Loss = -10925.786851489398
Iteration 8900: Loss = -10925.792908383955
1
Iteration 9000: Loss = -10925.785717969522
Iteration 9100: Loss = -10925.7849688364
Iteration 9200: Loss = -10925.814239292737
1
Iteration 9300: Loss = -10925.78299762767
Iteration 9400: Loss = -10925.781675934917
Iteration 9500: Loss = -10925.780790021843
Iteration 9600: Loss = -10925.777788506699
Iteration 9700: Loss = -10925.774967502377
Iteration 9800: Loss = -10925.771524132884
Iteration 9900: Loss = -10925.768684675779
Iteration 10000: Loss = -10925.766523526718
Iteration 10100: Loss = -10925.765028875347
Iteration 10200: Loss = -10925.764621442257
Iteration 10300: Loss = -10925.793129218418
1
Iteration 10400: Loss = -10925.76745870295
2
Iteration 10500: Loss = -10925.76353735005
Iteration 10600: Loss = -10925.867168576118
1
Iteration 10700: Loss = -10925.778654060565
2
Iteration 10800: Loss = -10925.767112911748
3
Iteration 10900: Loss = -10925.76194944561
Iteration 11000: Loss = -10925.76147535334
Iteration 11100: Loss = -10925.797979521189
1
Iteration 11200: Loss = -10925.765107602887
2
Iteration 11300: Loss = -10925.761893453988
3
Iteration 11400: Loss = -10925.766502697927
4
Iteration 11500: Loss = -10925.761109173176
Iteration 11600: Loss = -10925.772673443098
1
Iteration 11700: Loss = -10925.761261588545
2
Iteration 11800: Loss = -10925.76258076516
3
Iteration 11900: Loss = -10925.76177252572
4
Iteration 12000: Loss = -10925.760981321208
Iteration 12100: Loss = -10925.76321681222
1
Iteration 12200: Loss = -10925.760951055827
Iteration 12300: Loss = -10925.793432493696
1
Iteration 12400: Loss = -10925.760857895044
Iteration 12500: Loss = -10925.76865695264
1
Iteration 12600: Loss = -10925.765658463528
2
Iteration 12700: Loss = -10925.816695432486
3
Iteration 12800: Loss = -10925.760808337487
Iteration 12900: Loss = -10925.76500800034
1
Iteration 13000: Loss = -10925.761964243935
2
Iteration 13100: Loss = -10925.76087488729
3
Iteration 13200: Loss = -10925.761352657963
4
Iteration 13300: Loss = -10925.76080003741
Iteration 13400: Loss = -10925.761146446046
1
Iteration 13500: Loss = -10925.8004826877
2
Iteration 13600: Loss = -10925.760659427084
Iteration 13700: Loss = -10925.772787265516
1
Iteration 13800: Loss = -10925.76066846189
2
Iteration 13900: Loss = -10926.255111950295
3
Iteration 14000: Loss = -10925.760677677308
4
Iteration 14100: Loss = -10925.760655486007
Iteration 14200: Loss = -10925.775102576265
1
Iteration 14300: Loss = -10925.760635921371
Iteration 14400: Loss = -10925.76064395752
1
Iteration 14500: Loss = -10925.7609784689
2
Iteration 14600: Loss = -10925.760703186272
3
Iteration 14700: Loss = -10925.760596129941
Iteration 14800: Loss = -10925.810020529949
1
Iteration 14900: Loss = -10925.768069290585
2
Iteration 15000: Loss = -10925.760993146441
3
Iteration 15100: Loss = -10925.768971045041
4
Iteration 15200: Loss = -10925.760639610362
5
Iteration 15300: Loss = -10925.809275971242
6
Iteration 15400: Loss = -10925.760552045313
Iteration 15500: Loss = -10925.775030336918
1
Iteration 15600: Loss = -10925.760553099464
2
Iteration 15700: Loss = -10925.773041401215
3
Iteration 15800: Loss = -10925.760538211627
Iteration 15900: Loss = -10925.760555602541
1
Iteration 16000: Loss = -10925.760610892532
2
Iteration 16100: Loss = -10925.76052598678
Iteration 16200: Loss = -10925.779916873851
1
Iteration 16300: Loss = -10925.760526749724
2
Iteration 16400: Loss = -10925.791477638842
3
Iteration 16500: Loss = -10925.760585322902
4
Iteration 16600: Loss = -10925.766846327622
5
Iteration 16700: Loss = -10925.76051093941
Iteration 16800: Loss = -10925.761800299464
1
Iteration 16900: Loss = -10925.763431220148
2
Iteration 17000: Loss = -10925.90929861786
3
Iteration 17100: Loss = -10925.760506902268
Iteration 17200: Loss = -10925.760697046448
1
Iteration 17300: Loss = -10925.811178987744
2
Iteration 17400: Loss = -10925.760500014905
Iteration 17500: Loss = -10925.760719063443
1
Iteration 17600: Loss = -10925.761610001238
2
Iteration 17700: Loss = -10925.77640549869
3
Iteration 17800: Loss = -10925.76056178527
4
Iteration 17900: Loss = -10925.761418795017
5
Iteration 18000: Loss = -10925.761199540091
6
Iteration 18100: Loss = -10925.76055343928
7
Iteration 18200: Loss = -10925.76314038567
8
Iteration 18300: Loss = -10925.760572452617
9
Iteration 18400: Loss = -10925.771257746403
10
Stopping early at iteration 18400 due to no improvement.
tensor([[-0.5039, -1.6300],
        [-0.3997, -1.3726],
        [-0.1816, -1.3065],
        [-0.2604, -1.3632],
        [-0.3816, -1.4095],
        [-0.1694, -1.3184],
        [-0.2277, -1.2017],
        [-1.4058, -2.2825],
        [-0.2505, -1.1511],
        [-0.1949, -1.3905],
        [-0.5800, -1.6358],
        [-0.4892, -1.4377],
        [-0.2809, -1.4307],
        [-0.3609, -1.3608],
        [-0.1499, -1.2526],
        [-0.6652, -1.6392],
        [-0.2850, -1.1863],
        [-0.1971, -1.1970],
        [-0.2235, -1.2234],
        [-0.1915, -1.2709],
        [-0.2240, -1.2797],
        [-0.1681, -1.2237],
        [-0.3813, -1.3549],
        [-0.5628, -1.4870],
        [-0.1693, -1.2961],
        [-0.1950, -1.2745],
        [-0.2004, -1.2004],
        [-0.3218, -1.2954],
        [-0.1763, -1.2561],
        [-0.4861, -1.3156],
        [-0.6115, -1.7135],
        [-0.2045, -1.2038],
        [-0.5546, -1.7508],
        [-0.3863, -1.3114],
        [-0.3330, -1.3618],
        [-0.1197, -1.3152],
        [-0.2659, -1.2151],
        [-0.1444, -1.2478],
        [-1.2102, -2.1837],
        [-0.9900, -1.9895],
        [-0.4685, -1.4421],
        [-0.2215, -1.2499],
        [-0.4512, -1.5541],
        [-0.2138, -1.2416],
        [-0.1072, -1.2798],
        [-0.1708, -1.2262],
        [-0.1750, -1.3710],
        [-1.0769, -2.2264],
        [-0.1816, -1.2102],
        [-0.2441, -1.2178],
        [-0.7005, -1.6499],
        [-0.3759, -1.4046],
        [-0.1750, -1.2217],
        [-0.2233, -1.1719],
        [-0.2049, -1.4247],
        [-0.2627, -1.3895],
        [-0.2465, -1.2202],
        [-1.4255, -2.4811],
        [-0.1326, -1.2581],
        [-0.4035, -1.3285],
        [-0.3384, -1.2875],
        [-0.1090, -1.2818],
        [-0.4313, -1.4050],
        [-0.2297, -1.3094],
        [-0.2715, -1.3972],
        [-0.2518, -1.2002],
        [-0.1557, -1.2362],
        [-0.2584, -1.2316],
        [-0.1534, -1.2565],
        [-0.2453, -1.3013],
        [-0.3036, -1.1329],
        [-0.2104, -1.2389],
        [-0.4930, -1.5214],
        [-0.3331, -1.3072],
        [-0.3351, -1.2353],
        [-0.2764, -1.3048],
        [-0.4454, -1.5059],
        [-0.7865, -1.7602],
        [-0.3825, -1.4100],
        [-1.2000, -2.0529],
        [-0.2546, -1.2826],
        [-0.2452, -1.2187],
        [-0.2550, -1.2545],
        [-0.8501, -1.8777],
        [-0.1420, -1.2447],
        [-0.3314, -1.3053],
        [-0.2325, -1.1815],
        [-0.9385, -2.1105],
        [-0.4713, -1.3964],
        [-0.4956, -1.5501],
        [-0.1723, -1.2280],
        [-0.2444, -1.1928],
        [-0.3470, -1.4025],
        [-0.1241, -1.2730],
        [-0.2849, -1.1375],
        [-0.8654, -1.8928],
        [-1.4599, -2.3843],
        [-0.2909, -1.2905],
        [-0.3134, -1.3417],
        [-0.2168, -1.1902]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.7774e-01, 2.2255e-02],
        [9.9999e-01, 5.8764e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7347, 0.2653], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1596, 0.1619],
         [0.8744, 0.1651]],

        [[0.3128, 0.1578],
         [0.0997, 0.0808]],

        [[0.4767, 0.0667],
         [0.8618, 0.6783]],

        [[0.3934, 0.2903],
         [0.6429, 0.4217]],

        [[0.8645, 0.2537],
         [0.0224, 0.0975]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
Global Adjusted Rand Index: 0.000692393593012233
Average Adjusted Rand Index: -0.0004385435808572055
Iteration 0: Loss = -23559.511199762754
Iteration 10: Loss = -10926.339586550808
Iteration 20: Loss = -10926.267789453557
Iteration 30: Loss = -10926.276595741794
1
Iteration 40: Loss = -10926.282042596777
2
Iteration 50: Loss = -10926.284811142006
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.9692, 0.0308],
        [0.9805, 0.0195]], dtype=torch.float64)
alpha: tensor([0.9695, 0.0305])
beta: tensor([[[0.1571, 0.1676],
         [0.2053, 0.1602]],

        [[0.3549, 0.1564],
         [0.8909, 0.6149]],

        [[0.2782, 0.1422],
         [0.4834, 0.0933]],

        [[0.5490, 0.2846],
         [0.0680, 0.3328]],

        [[0.8983, 0.2487],
         [0.9464, 0.2477]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.009009494263563287
Global Adjusted Rand Index: 0.000692393593012233
Average Adjusted Rand Index: -0.001954123955583528
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23559.152190420526
Iteration 100: Loss = -10970.39939741737
Iteration 200: Loss = -10956.207151682125
Iteration 300: Loss = -10949.816503234062
Iteration 400: Loss = -10942.875691837477
Iteration 500: Loss = -10934.295455994694
Iteration 600: Loss = -10930.178811281343
Iteration 700: Loss = -10929.54305506443
Iteration 800: Loss = -10929.179635865237
Iteration 900: Loss = -10928.941685296968
Iteration 1000: Loss = -10928.776875323661
Iteration 1100: Loss = -10928.653279248849
Iteration 1200: Loss = -10928.557243515605
Iteration 1300: Loss = -10928.480103987205
Iteration 1400: Loss = -10928.414670870492
Iteration 1500: Loss = -10928.351220270535
Iteration 1600: Loss = -10928.260726925319
Iteration 1700: Loss = -10928.072118654582
Iteration 1800: Loss = -10927.866484305541
Iteration 1900: Loss = -10927.750392113614
Iteration 2000: Loss = -10927.688437482779
Iteration 2100: Loss = -10927.649953890068
Iteration 2200: Loss = -10927.622530448269
Iteration 2300: Loss = -10927.601311212107
Iteration 2400: Loss = -10927.583949644735
Iteration 2500: Loss = -10927.569333717951
Iteration 2600: Loss = -10927.556724286225
Iteration 2700: Loss = -10927.54568509846
Iteration 2800: Loss = -10927.535858629708
Iteration 2900: Loss = -10927.527086853834
Iteration 3000: Loss = -10927.519134511627
Iteration 3100: Loss = -10927.511893612831
Iteration 3200: Loss = -10927.505274898022
Iteration 3300: Loss = -10927.499173738655
Iteration 3400: Loss = -10927.493508269954
Iteration 3500: Loss = -10927.48820278137
Iteration 3600: Loss = -10927.483208061003
Iteration 3700: Loss = -10927.537869125314
1
Iteration 3800: Loss = -10927.473927135057
Iteration 3900: Loss = -10927.469490563688
Iteration 4000: Loss = -10927.465178767927
Iteration 4100: Loss = -10927.464944926362
Iteration 4200: Loss = -10927.456393053997
Iteration 4300: Loss = -10927.451488426892
Iteration 4400: Loss = -10927.464755856956
1
Iteration 4500: Loss = -10927.43558159153
Iteration 4600: Loss = -10927.360942782168
Iteration 4700: Loss = -10927.311923783287
Iteration 4800: Loss = -10927.271766754977
Iteration 4900: Loss = -10927.194466959441
Iteration 5000: Loss = -10927.173867917054
Iteration 5100: Loss = -10927.147648605742
Iteration 5200: Loss = -10927.07591835011
Iteration 5300: Loss = -10927.024612715784
Iteration 5400: Loss = -10926.867486561898
Iteration 5500: Loss = -10926.6406953093
Iteration 5600: Loss = -10926.163887663159
Iteration 5700: Loss = -10925.936164622473
Iteration 5800: Loss = -10925.868532727225
Iteration 5900: Loss = -10925.83639074682
Iteration 6000: Loss = -10925.81961634991
Iteration 6100: Loss = -10926.001859961698
1
Iteration 6200: Loss = -10925.801746327481
Iteration 6300: Loss = -10925.79648170667
Iteration 6400: Loss = -10925.792721719747
Iteration 6500: Loss = -10925.789368321513
Iteration 6600: Loss = -10925.786859690283
Iteration 6700: Loss = -10925.784813414537
Iteration 6800: Loss = -10925.783098543532
Iteration 6900: Loss = -10925.781646558014
Iteration 7000: Loss = -10925.780389017325
Iteration 7100: Loss = -10925.779586862876
Iteration 7200: Loss = -10925.778368808347
Iteration 7300: Loss = -10925.777588674222
Iteration 7400: Loss = -10925.799158387077
1
Iteration 7500: Loss = -10925.776216491418
Iteration 7600: Loss = -10925.775635203845
Iteration 7700: Loss = -10925.91726416102
1
Iteration 7800: Loss = -10925.774649470564
Iteration 7900: Loss = -10925.774204673551
Iteration 8000: Loss = -10925.773805891966
Iteration 8100: Loss = -10925.773481909284
Iteration 8200: Loss = -10925.773188043266
Iteration 8300: Loss = -10925.772899636631
Iteration 8400: Loss = -10925.781375311692
1
Iteration 8500: Loss = -10925.772406240372
Iteration 8600: Loss = -10925.772174077214
Iteration 8700: Loss = -10926.043740677018
1
Iteration 8800: Loss = -10925.77179002849
Iteration 8900: Loss = -10925.771578944357
Iteration 9000: Loss = -10925.776026784994
1
Iteration 9100: Loss = -10925.771259443754
Iteration 9200: Loss = -10925.77234712445
1
Iteration 9300: Loss = -10925.774743061354
2
Iteration 9400: Loss = -10925.774924258303
3
Iteration 9500: Loss = -10925.771268849323
4
Iteration 9600: Loss = -10925.791954464461
5
Iteration 9700: Loss = -10925.772283852964
6
Iteration 9800: Loss = -10925.77073599404
Iteration 9900: Loss = -10925.770371847919
Iteration 10000: Loss = -10925.77499854644
1
Iteration 10100: Loss = -10925.775576787706
2
Iteration 10200: Loss = -10925.771011562745
3
Iteration 10300: Loss = -10925.773094438586
4
Iteration 10400: Loss = -10925.770473213915
5
Iteration 10500: Loss = -10925.771291886138
6
Iteration 10600: Loss = -10925.77092952515
7
Iteration 10700: Loss = -10925.76987206042
Iteration 10800: Loss = -10925.776300288937
1
Iteration 10900: Loss = -10925.845766029564
2
Iteration 11000: Loss = -10925.769667265213
Iteration 11100: Loss = -10925.793243521028
1
Iteration 11200: Loss = -10925.769500284929
Iteration 11300: Loss = -10925.77123484419
1
Iteration 11400: Loss = -10925.774639156203
2
Iteration 11500: Loss = -10925.849904597431
3
Iteration 11600: Loss = -10925.769434928035
Iteration 11700: Loss = -10925.770062927411
1
Iteration 11800: Loss = -10925.771397442111
2
Iteration 11900: Loss = -10925.769559308415
3
Iteration 12000: Loss = -10925.770955694707
4
Iteration 12100: Loss = -10925.78590838207
5
Iteration 12200: Loss = -10925.77941432324
6
Iteration 12300: Loss = -10925.769200333394
Iteration 12400: Loss = -10925.780172101879
1
Iteration 12500: Loss = -10925.772681715816
2
Iteration 12600: Loss = -10925.798136481008
3
Iteration 12700: Loss = -10925.770243533156
4
Iteration 12800: Loss = -10925.806627275786
5
Iteration 12900: Loss = -10925.771413071949
6
Iteration 13000: Loss = -10925.792619614826
7
Iteration 13100: Loss = -10925.808265348267
8
Iteration 13200: Loss = -10925.774046698665
9
Iteration 13300: Loss = -10925.773928535267
10
Stopping early at iteration 13300 due to no improvement.
tensor([[-5.3159,  3.9273],
        [-5.3201,  3.9270],
        [-5.4732,  3.7593],
        [-5.3303,  3.9277],
        [-5.7615,  3.4898],
        [-5.5047,  3.7456],
        [-6.9508,  2.3356],
        [-6.8865,  2.3963],
        [-6.4150,  2.8773],
        [-6.4920,  2.7697],
        [-5.3072,  3.9166],
        [-5.4559,  3.8166],
        [-5.4564,  3.7838],
        [-5.4012,  3.8786],
        [-5.6350,  3.6285],
        [-5.5859,  3.6986],
        [-5.9094,  3.3825],
        [-5.3350,  3.9343],
        [-5.5929,  3.6439],
        [-5.9193,  3.3514],
        [-5.5266,  3.7429],
        [-5.4938,  3.7553],
        [-5.3300,  3.9405],
        [-5.5478,  3.7170],
        [-5.4523,  3.7694],
        [-6.6217,  2.6546],
        [-5.7213,  3.5439],
        [-5.3860,  3.8916],
        [-5.3599,  3.9101],
        [-5.3504,  3.9391],
        [-5.3404,  3.9057],
        [-5.3904,  3.8783],
        [-5.4614,  3.7959],
        [-5.4147,  3.8694],
        [-6.1507,  3.1299],
        [-5.3915,  3.8586],
        [-5.3912,  3.8693],
        [-5.3404,  3.9005],
        [-6.2225,  3.0598],
        [-5.5769,  3.7017],
        [-5.3337,  3.9446],
        [-5.7716,  3.4703],
        [-5.3544,  3.8985],
        [-5.9804,  3.2583],
        [-5.9365,  3.3279],
        [-6.4886,  2.7573],
        [-5.4510,  3.8091],
        [-5.4313,  3.8177],
        [-5.3304,  3.9091],
        [-5.4275,  3.8501],
        [-6.2009,  3.0876],
        [-5.4342,  3.8111],
        [-5.3354,  3.9219],
        [-5.6071,  3.6087],
        [-5.3488,  3.9019],
        [-5.3304,  3.9316],
        [-5.3464,  3.9180],
        [-5.3326,  3.9409],
        [-5.3344,  3.9211],
        [-5.3668,  3.8925],
        [-5.3345,  3.9446],
        [-6.7215,  2.5076],
        [-5.3247,  3.9360],
        [-5.3478,  3.8966],
        [-6.2998,  2.9700],
        [-5.3755,  3.8937],
        [-5.3471,  3.8957],
        [-5.4176,  3.8315],
        [-6.1423,  3.1311],
        [-5.4434,  3.8275],
        [-5.7137,  3.5847],
        [-6.3658,  2.9107],
        [-5.3528,  3.9014],
        [-5.3647,  3.9158],
        [-5.4768,  3.7676],
        [-6.2443,  3.0033],
        [-5.7008,  3.5697],
        [-5.4614,  3.8020],
        [-5.4188,  3.8534],
        [-5.5856,  3.7016],
        [-5.3342,  3.9153],
        [-5.7602,  3.5198],
        [-5.5558,  3.6864],
        [-5.4074,  3.8469],
        [-5.5554,  3.6946],
        [-5.6319,  3.6535],
        [-5.8715,  3.4134],
        [-5.3373,  3.9153],
        [-6.0906,  3.1887],
        [-5.3254,  3.9175],
        [-6.9470,  2.3318],
        [-5.3268,  3.9405],
        [-5.5784,  3.6895],
        [-5.3787,  3.8626],
        [-6.9294,  2.3142],
        [-5.3399,  3.9304],
        [-5.6025,  3.6824],
        [-5.3344,  3.9062],
        [-5.4677,  3.7678],
        [-5.6315,  3.6456]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.7796e-01, 2.2041e-02],
        [9.9989e-01, 1.0817e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.5142e-05, 9.9990e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1593, 0.1607],
         [0.2053, 0.1608]],

        [[0.3549, 0.1584],
         [0.8909, 0.6149]],

        [[0.2782, 0.0667],
         [0.4834, 0.0933]],

        [[0.5490, 0.2902],
         [0.0680, 0.3328]],

        [[0.8983, 0.2540],
         [0.9464, 0.2477]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
Global Adjusted Rand Index: -0.0016111090894400308
Average Adjusted Rand Index: -0.0004385435808572055
Iteration 0: Loss = -24503.198728051557
Iteration 10: Loss = -10930.303354235324
Iteration 20: Loss = -10930.151719145904
Iteration 30: Loss = -10929.996692593182
Iteration 40: Loss = -10929.807279413373
Iteration 50: Loss = -10929.554411534218
Iteration 60: Loss = -10929.199021542832
Iteration 70: Loss = -10928.687059946256
Iteration 80: Loss = -10927.9817561379
Iteration 90: Loss = -10927.184971706454
Iteration 100: Loss = -10926.557766089469
Iteration 110: Loss = -10926.269343670114
Iteration 120: Loss = -10926.237202701634
Iteration 130: Loss = -10926.258570751537
1
Iteration 140: Loss = -10926.273456188364
2
Iteration 150: Loss = -10926.280753486995
3
Stopping early at iteration 149 due to no improvement.
pi: tensor([[0.0194, 0.9806],
        [0.0310, 0.9690]], dtype=torch.float64)
alpha: tensor([0.0307, 0.9693])
beta: tensor([[[0.1616, 0.1679],
         [0.0805, 0.1571]],

        [[0.3115, 0.1566],
         [0.7933, 0.1449]],

        [[0.1327, 0.1429],
         [0.3305, 0.1530]],

        [[0.1080, 0.2844],
         [0.6900, 0.6686]],

        [[0.3255, 0.2484],
         [0.0560, 0.8569]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.009009494263563287
Global Adjusted Rand Index: 0.000692393593012233
Average Adjusted Rand Index: -0.001954123955583528
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24417.383752034988
Iteration 100: Loss = -11128.377509840742
Iteration 200: Loss = -10991.074760148418
Iteration 300: Loss = -10949.40370738155
Iteration 400: Loss = -10935.659107262596
Iteration 500: Loss = -10933.023476474658
Iteration 600: Loss = -10931.930118173283
Iteration 700: Loss = -10931.3264343646
Iteration 800: Loss = -10930.943322235813
Iteration 900: Loss = -10930.692866174059
Iteration 1000: Loss = -10930.521735930446
Iteration 1100: Loss = -10930.400428238032
Iteration 1200: Loss = -10930.323625671923
Iteration 1300: Loss = -10930.235668293819
Iteration 1400: Loss = -10930.175856970978
Iteration 1500: Loss = -10930.407652247372
1
Iteration 1600: Loss = -10930.066488596018
Iteration 1700: Loss = -10929.705454544834
Iteration 1800: Loss = -10928.791548995738
Iteration 1900: Loss = -10928.4570448654
Iteration 2000: Loss = -10928.378896955857
Iteration 2100: Loss = -10928.334607914367
Iteration 2200: Loss = -10928.307246912496
Iteration 2300: Loss = -10928.28015455366
Iteration 2400: Loss = -10928.261210992776
Iteration 2500: Loss = -10928.247766624765
Iteration 2600: Loss = -10928.232079896874
Iteration 2700: Loss = -10928.22059375283
Iteration 2800: Loss = -10928.274159002352
1
Iteration 2900: Loss = -10928.201692606797
Iteration 3000: Loss = -10928.193901373224
Iteration 3100: Loss = -10928.18690822279
Iteration 3200: Loss = -10928.180692183334
Iteration 3300: Loss = -10928.174869375758
Iteration 3400: Loss = -10928.169678483644
Iteration 3500: Loss = -10928.17033193815
1
Iteration 3600: Loss = -10928.160829757002
Iteration 3700: Loss = -10928.156969018139
Iteration 3800: Loss = -10928.153784792874
Iteration 3900: Loss = -10928.150158834593
Iteration 4000: Loss = -10928.147102615138
Iteration 4100: Loss = -10928.144362852867
Iteration 4200: Loss = -10928.14198155286
Iteration 4300: Loss = -10928.139376128966
Iteration 4400: Loss = -10928.137185093963
Iteration 4500: Loss = -10928.136324950045
Iteration 4600: Loss = -10928.133207394783
Iteration 4700: Loss = -10928.131416830667
Iteration 4800: Loss = -10928.131981573379
1
Iteration 4900: Loss = -10928.128165999407
Iteration 5000: Loss = -10928.126724977752
Iteration 5100: Loss = -10928.141920503602
1
Iteration 5200: Loss = -10928.124061448352
Iteration 5300: Loss = -10928.122870254652
Iteration 5400: Loss = -10928.163409186072
1
Iteration 5500: Loss = -10928.1206667808
Iteration 5600: Loss = -10928.119674865362
Iteration 5700: Loss = -10928.118804007834
Iteration 5800: Loss = -10928.117835676867
Iteration 5900: Loss = -10928.116955983705
Iteration 6000: Loss = -10928.116134733133
Iteration 6100: Loss = -10928.11537359363
Iteration 6200: Loss = -10928.114635107051
Iteration 6300: Loss = -10928.113896906401
Iteration 6400: Loss = -10928.113195906286
Iteration 6500: Loss = -10928.112262240289
Iteration 6600: Loss = -10928.111593121259
Iteration 6700: Loss = -10928.114219633455
1
Iteration 6800: Loss = -10928.110554879107
Iteration 6900: Loss = -10928.110042727065
Iteration 7000: Loss = -10928.124135070346
1
Iteration 7100: Loss = -10928.108981519465
Iteration 7200: Loss = -10928.241762248255
1
Iteration 7300: Loss = -10928.108174089075
Iteration 7400: Loss = -10928.107806035736
Iteration 7500: Loss = -10928.111122088483
1
Iteration 7600: Loss = -10928.107047186437
Iteration 7700: Loss = -10928.106706962926
Iteration 7800: Loss = -10928.107128748296
1
Iteration 7900: Loss = -10928.105805598216
Iteration 8000: Loss = -10928.105949620682
1
Iteration 8100: Loss = -10928.112388426658
2
Iteration 8200: Loss = -10928.1065726376
3
Iteration 8300: Loss = -10928.104905883312
Iteration 8400: Loss = -10928.107807058563
1
Iteration 8500: Loss = -10928.118974628962
2
Iteration 8600: Loss = -10928.105702780238
3
Iteration 8700: Loss = -10928.106766682844
4
Iteration 8800: Loss = -10928.103979935418
Iteration 8900: Loss = -10928.104000376228
1
Iteration 9000: Loss = -10928.104512885171
2
Iteration 9100: Loss = -10928.110923012988
3
Iteration 9200: Loss = -10928.106987030247
4
Iteration 9300: Loss = -10928.120575727671
5
Iteration 9400: Loss = -10928.187031482492
6
Iteration 9500: Loss = -10928.103039958205
Iteration 9600: Loss = -10928.12407285297
1
Iteration 9700: Loss = -10928.102834945737
Iteration 9800: Loss = -10928.102696980242
Iteration 9900: Loss = -10928.102600194256
Iteration 10000: Loss = -10928.100696787287
Iteration 10100: Loss = -10928.040481470964
Iteration 10200: Loss = -10928.07604879522
1
Iteration 10300: Loss = -10928.036586782848
Iteration 10400: Loss = -10928.033933411794
Iteration 10500: Loss = -10928.034111920711
1
Iteration 10600: Loss = -10928.037314069326
2
Iteration 10700: Loss = -10928.062355822845
3
Iteration 10800: Loss = -10928.033965273353
4
Iteration 10900: Loss = -10928.033804606534
Iteration 11000: Loss = -10928.034604605791
1
Iteration 11100: Loss = -10928.041711507254
2
Iteration 11200: Loss = -10928.041120068783
3
Iteration 11300: Loss = -10928.03331953301
Iteration 11400: Loss = -10928.081730688833
1
Iteration 11500: Loss = -10928.0331890494
Iteration 11600: Loss = -10928.171381196293
1
Iteration 11700: Loss = -10928.033171022407
Iteration 11800: Loss = -10928.033120575632
Iteration 11900: Loss = -10928.033456468313
1
Iteration 12000: Loss = -10928.033029269827
Iteration 12100: Loss = -10928.05390612653
1
Iteration 12200: Loss = -10928.05715510773
2
Iteration 12300: Loss = -10928.033146939526
3
Iteration 12400: Loss = -10928.034304734885
4
Iteration 12500: Loss = -10927.446503799623
Iteration 12600: Loss = -10927.442005750301
Iteration 12700: Loss = -10927.440809602193
Iteration 12800: Loss = -10927.440757428523
Iteration 12900: Loss = -10927.440767835704
1
Iteration 13000: Loss = -10927.445667981277
2
Iteration 13100: Loss = -10927.451544704183
3
Iteration 13200: Loss = -10927.466966532907
4
Iteration 13300: Loss = -10927.484892864737
5
Iteration 13400: Loss = -10927.510748816518
6
Iteration 13500: Loss = -10927.436281555914
Iteration 13600: Loss = -10927.437465772275
1
Iteration 13700: Loss = -10927.436242828973
Iteration 13800: Loss = -10927.462709054573
1
Iteration 13900: Loss = -10927.436218285997
Iteration 14000: Loss = -10927.436225077363
1
Iteration 14100: Loss = -10927.436214127692
Iteration 14200: Loss = -10927.43658304989
1
Iteration 14300: Loss = -10927.447102285712
2
Iteration 14400: Loss = -10927.436157408865
Iteration 14500: Loss = -10927.43645931653
1
Iteration 14600: Loss = -10927.436657619433
2
Iteration 14700: Loss = -10927.43618361395
3
Iteration 14800: Loss = -10927.541689495903
4
Iteration 14900: Loss = -10927.436123439305
Iteration 15000: Loss = -10927.444769085836
1
Iteration 15100: Loss = -10927.4361321692
2
Iteration 15200: Loss = -10927.437063526015
3
Iteration 15300: Loss = -10927.541306007302
4
Iteration 15400: Loss = -10927.436121874323
Iteration 15500: Loss = -10927.466582468407
1
Iteration 15600: Loss = -10927.436115042821
Iteration 15700: Loss = -10927.487865352889
1
Iteration 15800: Loss = -10927.436120719867
2
Iteration 15900: Loss = -10927.436097294481
Iteration 16000: Loss = -10927.4477807197
1
Iteration 16100: Loss = -10927.400606737365
Iteration 16200: Loss = -10927.407295973479
1
Iteration 16300: Loss = -10927.400966996165
2
Iteration 16400: Loss = -10927.400278922203
Iteration 16500: Loss = -10927.400358879064
1
Iteration 16600: Loss = -10927.417773895166
2
Iteration 16700: Loss = -10927.400205373367
Iteration 16800: Loss = -10927.40438814712
1
Iteration 16900: Loss = -10927.40020578046
2
Iteration 17000: Loss = -10927.410190796989
3
Iteration 17100: Loss = -10927.402684821582
4
Iteration 17200: Loss = -10927.406358969918
5
Iteration 17300: Loss = -10927.400642908902
6
Iteration 17400: Loss = -10927.403203628972
7
Iteration 17500: Loss = -10927.400710552782
8
Iteration 17600: Loss = -10927.406563959801
9
Iteration 17700: Loss = -10927.400336268473
10
Stopping early at iteration 17700 due to no improvement.
tensor([[-10.9710,   7.8490],
        [ -9.0954,   7.5969],
        [-11.4453,   7.4211],
        [-10.8300,   8.0475],
        [-10.8842,   7.7147],
        [ -9.2931,   7.9055],
        [ -9.6350,   8.1225],
        [ -9.5140,   8.0491],
        [-10.0040,   8.5000],
        [ -9.4802,   7.5711],
        [ -9.1272,   7.1424],
        [ -9.3407,   7.5682],
        [ -9.8211,   7.6375],
        [-10.8471,   6.2319],
        [ -9.5958,   7.8840],
        [ -9.6570,   7.7752],
        [ -8.9584,   7.5203],
        [-11.6939,   7.0947],
        [ -9.3353,   7.7048],
        [-12.1823,   7.5670],
        [-10.5211,   8.0661],
        [ -9.7691,   8.3822],
        [-10.0346,   8.1419],
        [ -9.6052,   7.9013],
        [ -9.3594,   7.8819],
        [ -9.8829,   8.4300],
        [-10.2822,   7.6495],
        [ -9.8264,   8.4380],
        [ -9.5325,   7.5484],
        [ -8.9571,   7.3554],
        [-11.1712,   7.6362],
        [ -9.5258,   6.8524],
        [ -9.7640,   7.5520],
        [-10.5811,   8.0236],
        [ -9.2280,   7.8417],
        [ -9.8252,   7.5579],
        [-10.0284,   8.1179],
        [ -8.8923,   7.4449],
        [-10.9476,   7.3641],
        [ -8.7676,   7.2148],
        [-10.1170,   7.3422],
        [ -9.8810,   7.7081],
        [ -9.3699,   7.9734],
        [ -9.1040,   7.7176],
        [ -9.2218,   7.8325],
        [ -9.7425,   8.3187],
        [ -9.6940,   8.1741],
        [-10.1538,   8.0274],
        [ -9.2766,   7.8120],
        [ -9.3491,   7.5220],
        [ -8.8470,   7.4544],
        [-10.6861,   8.2320],
        [ -9.6754,   8.2043],
        [-10.4506,   7.7424],
        [-10.0446,   7.8901],
        [-10.9183,   8.2636],
        [-10.0809,   8.1087],
        [ -9.9028,   8.5019],
        [ -9.6778,   8.2169],
        [ -9.6592,   7.8706],
        [ -9.8607,   7.7414],
        [ -9.1389,   7.7227],
        [ -9.2220,   7.4685],
        [ -9.4261,   7.7672],
        [-12.0186,   7.4849],
        [ -9.1129,   7.6985],
        [ -9.5475,   8.1073],
        [-10.0327,   8.6199],
        [-10.9336,   6.6099],
        [-10.8624,   7.6864],
        [ -9.7868,   7.1597],
        [-10.9013,   7.8926],
        [ -9.8493,   7.8445],
        [-11.2979,   7.9623],
        [ -9.5397,   8.0783],
        [ -9.0375,   7.6134],
        [ -9.2247,   7.7037],
        [ -9.4725,   7.9906],
        [ -9.2838,   7.8694],
        [ -9.5214,   8.1350],
        [ -9.3253,   6.5655],
        [ -9.9544,   8.2551],
        [ -9.2132,   7.7528],
        [ -9.0656,   7.6025],
        [-11.0293,   8.1960],
        [ -9.8981,   8.2278],
        [ -9.3113,   7.6722],
        [ -9.2041,   7.7801],
        [-10.2111,   7.8099],
        [ -9.8630,   7.8087],
        [-10.4991,   8.1610],
        [-10.3664,   8.1216],
        [ -9.6466,   7.8572],
        [-10.1882,   7.7109],
        [ -9.9069,   8.5205],
        [ -9.1991,   7.7847],
        [ -9.4546,   7.9959],
        [ -9.3961,   7.1400],
        [-10.0332,   6.0649],
        [-10.3263,   8.2450]], dtype=torch.float64, requires_grad=True)
pi: tensor([[3.0035e-02, 9.6997e-01],
        [1.0000e+00, 1.1331e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([3.2608e-08, 1.0000e+00], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1589, 0.1647],
         [0.0805, 0.1601]],

        [[0.3115, 0.1863],
         [0.7933, 0.1449]],

        [[0.1327, 0.1440],
         [0.3305, 0.1530]],

        [[0.1080, 0.2871],
         [0.6900, 0.6686]],

        [[0.3255, 0.1782],
         [0.0560, 0.8569]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0007748402262652058
Global Adjusted Rand Index: -0.0019899786030521897
Average Adjusted Rand Index: -5.4658976481974e-05
10815.607276960178
new:  [0.000692393593012233, 0.000692393593012233, -0.0016111090894400308, -0.0019899786030521897] [-0.001954123955583528, -0.0004385435808572055, -0.0004385435808572055, -5.4658976481974e-05] [10925.921732211276, 10925.771257746403, 10925.773928535267, 10927.400336268473]
prior:  [0.000692393593012233, 0.00032696474682979756, 0.000692393593012233, 0.000692393593012233] [-0.001954123955583528, 0.0005435279048114025, -0.001954123955583528, -0.001954123955583528] [10926.286915444289, 10927.358073494772, 10926.284811142006, 10926.280753486995]
-----------------------------------------------------------------------------------------
This iteration is 1
True Objective function: Loss = -10967.10133548985
Iteration 0: Loss = -50465.532679375814
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.2520,    nan]],

        [[0.3537,    nan],
         [0.3050, 0.9933]],

        [[0.4447,    nan],
         [0.5096, 0.4615]],

        [[0.6946,    nan],
         [0.2500, 0.3467]],

        [[0.1496,    nan],
         [0.1808, 0.3842]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -49494.24195120637
Iteration 100: Loss = -11010.32253664561
Iteration 200: Loss = -11006.138719273678
Iteration 300: Loss = -11004.380555342057
Iteration 400: Loss = -11003.633784745498
Iteration 500: Loss = -11003.252153257357
Iteration 600: Loss = -11003.012364870394
Iteration 700: Loss = -11002.84332803598
Iteration 800: Loss = -11002.716437923911
Iteration 900: Loss = -11002.61685583978
Iteration 1000: Loss = -11002.535865348265
Iteration 1100: Loss = -11002.467740776727
Iteration 1200: Loss = -11002.408518918584
Iteration 1300: Loss = -11002.357344826232
Iteration 1400: Loss = -11002.31452660482
Iteration 1500: Loss = -11002.27708955576
Iteration 1600: Loss = -11002.24554221802
Iteration 1700: Loss = -11002.216483122807
Iteration 1800: Loss = -11002.19273814128
Iteration 1900: Loss = -11002.172643876173
Iteration 2000: Loss = -11002.155946456058
Iteration 2100: Loss = -11002.140112511612
Iteration 2200: Loss = -11002.125893724424
Iteration 2300: Loss = -11002.11199227963
Iteration 2400: Loss = -11002.098194027614
Iteration 2500: Loss = -11002.083548255086
Iteration 2600: Loss = -11002.068179682796
Iteration 2700: Loss = -11002.059357810536
Iteration 2800: Loss = -11002.031427776907
Iteration 2900: Loss = -11002.008512453267
Iteration 3000: Loss = -11002.017889690072
1
Iteration 3100: Loss = -11001.954899611346
Iteration 3200: Loss = -11001.925819373075
Iteration 3300: Loss = -11001.89293324869
Iteration 3400: Loss = -11001.85647503715
Iteration 3500: Loss = -11001.770823916711
Iteration 3600: Loss = -11001.14992534198
Iteration 3700: Loss = -10998.924154647382
Iteration 3800: Loss = -10998.641249710134
Iteration 3900: Loss = -10998.535947240562
Iteration 4000: Loss = -10998.473848232969
Iteration 4100: Loss = -10998.433493417095
Iteration 4200: Loss = -10998.401243111053
Iteration 4300: Loss = -10998.37608117993
Iteration 4400: Loss = -10998.728165636046
1
Iteration 4500: Loss = -10998.325879644615
Iteration 4600: Loss = -10998.29626778795
Iteration 4700: Loss = -10998.27487228405
Iteration 4800: Loss = -10998.258967661728
Iteration 4900: Loss = -10998.24423600821
Iteration 5000: Loss = -10998.230684875343
Iteration 5100: Loss = -10998.248642941653
1
Iteration 5200: Loss = -10998.204820511102
Iteration 5300: Loss = -10998.19221196205
Iteration 5400: Loss = -10998.180911987243
Iteration 5500: Loss = -10998.166441983516
Iteration 5600: Loss = -10998.151671036023
Iteration 5700: Loss = -10998.132073511308
Iteration 5800: Loss = -10998.108391758742
Iteration 5900: Loss = -10998.049632157712
Iteration 6000: Loss = -10997.522596669482
Iteration 6100: Loss = -10997.221450967305
Iteration 6200: Loss = -10997.27110536334
1
Iteration 6300: Loss = -10997.042237135774
Iteration 6400: Loss = -10997.02414259779
Iteration 6500: Loss = -10997.015758297088
Iteration 6600: Loss = -10997.041899635411
1
Iteration 6700: Loss = -10997.004095317328
Iteration 6800: Loss = -10996.991773253225
Iteration 6900: Loss = -10995.424969397738
Iteration 7000: Loss = -10995.404709012302
Iteration 7100: Loss = -10995.385393796572
Iteration 7200: Loss = -10995.383056603396
Iteration 7300: Loss = -10995.381115137687
Iteration 7400: Loss = -10995.379481367816
Iteration 7500: Loss = -10995.377820571066
Iteration 7600: Loss = -10995.376552639475
Iteration 7700: Loss = -10995.375072141955
Iteration 7800: Loss = -10995.39042570215
1
Iteration 7900: Loss = -10995.371821914741
Iteration 8000: Loss = -10995.356273256111
Iteration 8100: Loss = -10995.254999061211
Iteration 8200: Loss = -10995.252202934083
Iteration 8300: Loss = -10995.245582722766
Iteration 8400: Loss = -10995.239600437395
Iteration 8500: Loss = -10995.26248867872
1
Iteration 8600: Loss = -10995.237351990609
Iteration 8700: Loss = -10995.236702643235
Iteration 8800: Loss = -10995.236071370038
Iteration 8900: Loss = -10995.235744905622
Iteration 9000: Loss = -10995.235100036278
Iteration 9100: Loss = -10995.234632385564
Iteration 9200: Loss = -10995.347155841933
1
Iteration 9300: Loss = -10995.233814313307
Iteration 9400: Loss = -10995.233419975662
Iteration 9500: Loss = -10995.236182899755
1
Iteration 9600: Loss = -10995.233420943916
2
Iteration 9700: Loss = -10995.232174292902
Iteration 9800: Loss = -10995.231307692877
Iteration 9900: Loss = -10995.269762421985
1
Iteration 10000: Loss = -10995.102385920454
Iteration 10100: Loss = -10995.105382043545
1
Iteration 10200: Loss = -10995.10170584111
Iteration 10300: Loss = -10995.102429745399
1
Iteration 10400: Loss = -10995.10127660446
Iteration 10500: Loss = -10995.101022104871
Iteration 10600: Loss = -10995.10996528393
1
Iteration 10700: Loss = -10995.100657364395
Iteration 10800: Loss = -10995.105081640038
1
Iteration 10900: Loss = -10995.100353271364
Iteration 11000: Loss = -10995.100151765513
Iteration 11100: Loss = -10995.133783019643
1
Iteration 11200: Loss = -10995.099851622568
Iteration 11300: Loss = -10995.109392956529
1
Iteration 11400: Loss = -10995.099788839017
Iteration 11500: Loss = -10995.099974302206
1
Iteration 11600: Loss = -10995.102678205894
2
Iteration 11700: Loss = -10995.092010051922
Iteration 11800: Loss = -10995.093933945273
1
Iteration 11900: Loss = -10995.091589213962
Iteration 12000: Loss = -10995.097025912779
1
Iteration 12100: Loss = -10995.091067140056
Iteration 12200: Loss = -10995.405811434535
1
Iteration 12300: Loss = -10995.06872799972
Iteration 12400: Loss = -10995.06852179551
Iteration 12500: Loss = -10995.24068253884
1
Iteration 12600: Loss = -10995.068387997046
Iteration 12700: Loss = -10995.068253442796
Iteration 12800: Loss = -10995.068222249265
Iteration 12900: Loss = -10995.06891781052
1
Iteration 13000: Loss = -10995.068104050537
Iteration 13100: Loss = -10995.068076079671
Iteration 13200: Loss = -10995.118511039791
1
Iteration 13300: Loss = -10995.067942433021
Iteration 13400: Loss = -10995.067934119203
Iteration 13500: Loss = -10995.140732063379
1
Iteration 13600: Loss = -10995.0678833131
Iteration 13700: Loss = -10995.067838997926
Iteration 13800: Loss = -10995.067797851847
Iteration 13900: Loss = -10995.067982023427
1
Iteration 14000: Loss = -10995.067728984559
Iteration 14100: Loss = -10995.084947708398
1
Iteration 14200: Loss = -10995.067678321297
Iteration 14300: Loss = -10995.213093936509
1
Iteration 14400: Loss = -10995.06762996256
Iteration 14500: Loss = -10995.067595555269
Iteration 14600: Loss = -10995.067888505408
1
Iteration 14700: Loss = -10995.067518523454
Iteration 14800: Loss = -10995.067454140088
Iteration 14900: Loss = -10995.069261917686
1
Iteration 15000: Loss = -10995.01856939831
Iteration 15100: Loss = -10995.01846470157
Iteration 15200: Loss = -10994.853583294644
Iteration 15300: Loss = -10994.741900866557
Iteration 15400: Loss = -10994.740874096904
Iteration 15500: Loss = -10994.741463872544
1
Iteration 15600: Loss = -10994.740829366707
Iteration 15700: Loss = -10994.746672433479
1
Iteration 15800: Loss = -10994.740353111087
Iteration 15900: Loss = -10994.995150661767
1
Iteration 16000: Loss = -10994.737062657612
Iteration 16100: Loss = -10994.737032774745
Iteration 16200: Loss = -10994.739256827503
1
Iteration 16300: Loss = -10994.737016164048
Iteration 16400: Loss = -10994.737576097255
1
Iteration 16500: Loss = -10994.742206830746
2
Iteration 16600: Loss = -10994.738433052999
3
Iteration 16700: Loss = -10994.733102904775
Iteration 16800: Loss = -10994.732831287154
Iteration 16900: Loss = -10994.717378981733
Iteration 17000: Loss = -10994.739342034954
1
Iteration 17100: Loss = -10994.70251502998
Iteration 17200: Loss = -10995.012197265632
1
Iteration 17300: Loss = -10994.702483017472
Iteration 17400: Loss = -10994.70243774541
Iteration 17500: Loss = -10994.702640794903
1
Iteration 17600: Loss = -10994.467959583326
Iteration 17700: Loss = -10994.478367828096
1
Iteration 17800: Loss = -10994.47712721241
2
Iteration 17900: Loss = -10994.461297275628
Iteration 18000: Loss = -10994.463518730689
1
Iteration 18100: Loss = -10994.497399771353
2
Iteration 18200: Loss = -10994.461202425788
Iteration 18300: Loss = -10994.47550420339
1
Iteration 18400: Loss = -10994.461140231724
Iteration 18500: Loss = -10994.44657336164
Iteration 18600: Loss = -10994.4074498781
Iteration 18700: Loss = -10994.35444637356
Iteration 18800: Loss = -10994.353038816847
Iteration 18900: Loss = -10994.343851617741
Iteration 19000: Loss = -10994.344051965234
1
Iteration 19100: Loss = -10994.343856064177
2
Iteration 19200: Loss = -10994.343841086074
Iteration 19300: Loss = -10994.344833112098
1
Iteration 19400: Loss = -10994.343978150351
2
Iteration 19500: Loss = -10994.358597639592
3
Iteration 19600: Loss = -10994.343857218832
4
Iteration 19700: Loss = -10994.347559686608
5
Iteration 19800: Loss = -10994.382563622283
6
Iteration 19900: Loss = -10994.336623389117
tensor([[ 5.0830e+00, -9.6982e+00],
        [ 4.9604e+00, -9.5756e+00],
        [ 5.4556e+00, -1.0071e+01],
        [ 7.6615e-01, -5.3814e+00],
        [ 1.4853e+00, -6.1005e+00],
        [ 5.4188e+00, -1.0034e+01],
        [-8.6477e-01, -3.7505e+00],
        [ 5.2797e+00, -9.8950e+00],
        [ 5.4810e+00, -1.0096e+01],
        [ 4.6522e+00, -9.2674e+00],
        [ 2.1547e+00, -6.7699e+00],
        [ 5.1775e+00, -9.7927e+00],
        [ 2.5615e+00, -7.1767e+00],
        [-1.5094e-01, -4.4643e+00],
        [ 5.2036e+00, -9.8188e+00],
        [ 5.5545e+00, -1.0170e+01],
        [ 4.5871e+00, -9.2023e+00],
        [ 1.3626e+00, -5.9778e+00],
        [ 5.4534e+00, -1.0069e+01],
        [ 5.2486e+00, -9.8638e+00],
        [ 5.3874e+00, -1.0003e+01],
        [ 5.4704e+00, -1.0086e+01],
        [ 5.1331e+00, -9.7483e+00],
        [ 5.4597e+00, -1.0075e+01],
        [ 4.9946e+00, -9.6099e+00],
        [-1.5980e+00, -3.0172e+00],
        [-6.3973e-01, -3.9755e+00],
        [ 5.0811e+00, -9.6964e+00],
        [-1.1268e+00, -3.4884e+00],
        [-1.8671e+00, -2.7482e+00],
        [ 5.0759e+00, -9.6911e+00],
        [ 5.5904e+00, -1.0206e+01],
        [-3.8252e+00, -7.9005e-01],
        [ 5.2390e+00, -9.8543e+00],
        [ 5.1006e+00, -9.7158e+00],
        [ 1.5800e+00, -6.1953e+00],
        [-1.8487e+00, -2.7665e+00],
        [-1.8673e-01, -4.4285e+00],
        [-1.7006e-01, -4.4452e+00],
        [ 5.4400e+00, -1.0055e+01],
        [-1.5007e+00, -3.1145e+00],
        [-6.7701e-01, -3.9382e+00],
        [ 5.8164e+00, -1.0432e+01],
        [ 5.1955e+00, -9.8107e+00],
        [ 5.3112e+00, -9.9264e+00],
        [ 5.3969e+00, -1.0012e+01],
        [ 5.3842e+00, -9.9994e+00],
        [ 9.6905e-01, -5.5843e+00],
        [ 4.8360e-01, -5.0988e+00],
        [-5.9373e-01, -4.0215e+00],
        [-9.8486e-01, -3.6304e+00],
        [ 5.3829e+00, -9.9981e+00],
        [-7.0744e-01, -3.9078e+00],
        [-5.1762e+00,  5.6094e-01],
        [ 2.1096e+00, -6.7248e+00],
        [ 1.9881e+00, -6.6033e+00],
        [ 5.0915e+00, -9.7067e+00],
        [ 5.5451e+00, -1.0160e+01],
        [ 5.4943e+00, -1.0110e+01],
        [-3.9541e+00, -6.6111e-01],
        [ 2.9461e-01, -4.9098e+00],
        [-7.4086e-01, -3.8744e+00],
        [ 5.2445e+00, -9.8597e+00],
        [ 5.4111e+00, -1.0026e+01],
        [-3.2884e-01, -4.2864e+00],
        [-2.6809e+00, -1.9343e+00],
        [ 5.2636e+00, -9.8788e+00],
        [ 5.4246e+00, -1.0040e+01],
        [-3.2891e+00, -1.3261e+00],
        [ 4.6845e+00, -9.2997e+00],
        [ 5.2351e+00, -9.8504e+00],
        [ 5.1880e+00, -9.8032e+00],
        [ 1.1642e-01, -4.7316e+00],
        [ 1.9459e+00, -6.5611e+00],
        [ 5.3552e+00, -9.9704e+00],
        [ 5.1645e+00, -9.7797e+00],
        [ 4.3665e+00, -8.9817e+00],
        [ 5.3651e+00, -9.9803e+00],
        [ 5.1929e+00, -9.8082e+00],
        [ 5.1001e+00, -9.7154e+00],
        [-2.1901e-03, -4.6130e+00],
        [ 5.5207e+00, -1.0136e+01],
        [ 5.3184e+00, -9.9336e+00],
        [ 5.2616e+00, -9.8768e+00],
        [ 2.9344e+00, -7.5496e+00],
        [ 1.2494e-02, -4.6277e+00],
        [ 5.0412e+00, -9.6564e+00],
        [ 1.5012e-01, -4.7653e+00],
        [ 3.6852e+00, -8.3004e+00],
        [ 4.8452e+00, -9.4604e+00],
        [ 4.8214e+00, -9.4366e+00],
        [ 5.4431e+00, -1.0058e+01],
        [ 2.3061e+00, -6.9214e+00],
        [ 5.4551e+00, -1.0070e+01],
        [-3.9879e-01, -4.2164e+00],
        [ 5.4113e+00, -1.0027e+01],
        [-1.0229e+00, -3.5923e+00],
        [-7.5297e-01, -3.8622e+00],
        [ 5.1412e+00, -9.7564e+00],
        [ 5.0670e+00, -9.6822e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 2.3132e-08],
        [1.9295e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9395, 0.0605], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1592, 0.2034],
         [0.2520, 0.2329]],

        [[0.3537, 0.1779],
         [0.3050, 0.9933]],

        [[0.4447, 0.2432],
         [0.5096, 0.4615]],

        [[0.6946, 0.1995],
         [0.2500, 0.3467]],

        [[0.1496, 0.1218],
         [0.1808, 0.3842]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.004166919759460609
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.01644068827141728
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.022328462449853457
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.004851708247904022
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0015221131464426677
Global Adjusted Rand Index: -0.0008552510891647506
Average Adjusted Rand Index: -0.001618935162664451
Iteration 0: Loss = -22997.021428093423
Iteration 10: Loss = -11001.194567801205
Iteration 20: Loss = -11000.630234778097
Iteration 30: Loss = -11000.43466791493
Iteration 40: Loss = -11000.406277930124
Iteration 50: Loss = -11000.402802263507
Iteration 60: Loss = -11000.402817987679
1
Iteration 70: Loss = -11000.403444591631
2
Iteration 80: Loss = -11000.404166545924
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.6531, 0.3469],
        [0.6310, 0.3690]], dtype=torch.float64)
alpha: tensor([0.6445, 0.3555])
beta: tensor([[[0.1803, 0.1522],
         [0.8391, 0.1297]],

        [[0.8008, 0.1547],
         [0.1586, 0.7631]],

        [[0.0964, 0.1557],
         [0.5543, 0.8066]],

        [[0.5517, 0.1537],
         [0.0046, 0.7199]],

        [[0.3298, 0.1482],
         [0.3869, 0.8607]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.00043790211203557114
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.006467401572035518
time is 2
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 32
Adjusted Rand Index: 0.11322004561344752
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.006872454341195593
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.05102040816326531
Global Adjusted Rand Index: 0.025192560138998486
Average Adjusted Rand Index: 0.032841520886767464
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22996.988646960843
Iteration 100: Loss = -11081.446404821494
Iteration 200: Loss = -11017.164236664223
Iteration 300: Loss = -11005.254594089689
Iteration 400: Loss = -11003.858911158226
Iteration 500: Loss = -11003.273075585283
Iteration 600: Loss = -11002.942748971622
Iteration 700: Loss = -11002.733629699405
Iteration 800: Loss = -11002.5912894972
Iteration 900: Loss = -11002.488845067679
Iteration 1000: Loss = -11002.41187668303
Iteration 1100: Loss = -11002.351957721095
Iteration 1200: Loss = -11002.30373624706
Iteration 1300: Loss = -11002.26412167727
Iteration 1400: Loss = -11002.230806008778
Iteration 1500: Loss = -11002.201879677677
Iteration 1600: Loss = -11002.175827551313
Iteration 1700: Loss = -11002.151251245437
Iteration 1800: Loss = -11002.124404180277
Iteration 1900: Loss = -11002.086466196195
Iteration 2000: Loss = -11002.06482518284
Iteration 2100: Loss = -11002.047035355345
Iteration 2200: Loss = -11002.030029855589
Iteration 2300: Loss = -11002.006110621393
Iteration 2400: Loss = -11001.4863600921
Iteration 2500: Loss = -11001.152205851504
Iteration 2600: Loss = -11001.013345178508
Iteration 2700: Loss = -11000.914926857453
Iteration 2800: Loss = -11000.83872391912
Iteration 2900: Loss = -11000.769852549989
Iteration 3000: Loss = -11000.659969643188
Iteration 3100: Loss = -10999.623300848772
Iteration 3200: Loss = -10999.528120724264
Iteration 3300: Loss = -10999.48349404374
Iteration 3400: Loss = -10999.454091610973
Iteration 3500: Loss = -10999.408982790934
Iteration 3600: Loss = -10998.79650796287
Iteration 3700: Loss = -10998.522391988492
Iteration 3800: Loss = -10998.473038926968
Iteration 3900: Loss = -10998.450020593384
Iteration 4000: Loss = -10998.435441618392
Iteration 4100: Loss = -10998.592801017836
1
Iteration 4200: Loss = -10998.416760618391
Iteration 4300: Loss = -10998.410151011467
Iteration 4400: Loss = -10998.40461558477
Iteration 4500: Loss = -10998.586821065886
1
Iteration 4600: Loss = -10998.395929574775
Iteration 4700: Loss = -10998.392372929347
Iteration 4800: Loss = -10998.389218876477
Iteration 4900: Loss = -10998.386432799252
Iteration 5000: Loss = -10998.383796118753
Iteration 5100: Loss = -10998.381479865051
Iteration 5200: Loss = -10998.379253726473
Iteration 5300: Loss = -10998.377511656407
Iteration 5400: Loss = -10998.375448241472
Iteration 5500: Loss = -10998.37376349421
Iteration 5600: Loss = -10998.377092011906
1
Iteration 5700: Loss = -10998.370740688826
Iteration 5800: Loss = -10998.369366996643
Iteration 5900: Loss = -10998.368277513468
Iteration 6000: Loss = -10998.366963322203
Iteration 6100: Loss = -10998.3658501386
Iteration 6200: Loss = -10998.364772300643
Iteration 6300: Loss = -10998.36412979202
Iteration 6400: Loss = -10998.362957280908
Iteration 6500: Loss = -10998.362086679317
Iteration 6600: Loss = -10998.361261250631
Iteration 6700: Loss = -10998.360668011434
Iteration 6800: Loss = -10998.35983570479
Iteration 6900: Loss = -10998.35917215076
Iteration 7000: Loss = -10998.35842998559
Iteration 7100: Loss = -10998.353953690079
Iteration 7200: Loss = -10998.353239731548
Iteration 7300: Loss = -10998.35270056158
Iteration 7400: Loss = -10998.352203764118
Iteration 7500: Loss = -10998.35171924443
Iteration 7600: Loss = -10998.35127772521
Iteration 7700: Loss = -10998.351491078536
1
Iteration 7800: Loss = -10998.350504970986
Iteration 7900: Loss = -10998.350026665039
Iteration 8000: Loss = -10998.34967676656
Iteration 8100: Loss = -10998.372015092198
1
Iteration 8200: Loss = -10998.348972318792
Iteration 8300: Loss = -10998.34866740501
Iteration 8400: Loss = -10998.348326912552
Iteration 8500: Loss = -10998.349569669363
1
Iteration 8600: Loss = -10998.347767706726
Iteration 8700: Loss = -10998.347497137273
Iteration 8800: Loss = -10998.347219804707
Iteration 8900: Loss = -10998.347681739013
1
Iteration 9000: Loss = -10998.346769874655
Iteration 9100: Loss = -10998.346458943912
Iteration 9200: Loss = -10998.355109055597
1
Iteration 9300: Loss = -10998.346085221823
Iteration 9400: Loss = -10998.34585184934
Iteration 9500: Loss = -10998.345673443504
Iteration 9600: Loss = -10998.383424781097
1
Iteration 9700: Loss = -10998.34530223665
Iteration 9800: Loss = -10998.345112675608
Iteration 9900: Loss = -10998.34560904598
1
Iteration 10000: Loss = -10998.34477114746
Iteration 10100: Loss = -10998.344618416118
Iteration 10200: Loss = -10998.402671315022
1
Iteration 10300: Loss = -10998.344327627643
Iteration 10400: Loss = -10998.344273561996
Iteration 10500: Loss = -10998.344225214454
Iteration 10600: Loss = -10998.34390336591
Iteration 10700: Loss = -10998.343915876798
1
Iteration 10800: Loss = -10998.343648648883
Iteration 10900: Loss = -10998.343547058224
Iteration 11000: Loss = -10998.34337051635
Iteration 11100: Loss = -10998.345211884416
1
Iteration 11200: Loss = -10998.34314839947
Iteration 11300: Loss = -10998.343070110572
Iteration 11400: Loss = -10998.376374577998
1
Iteration 11500: Loss = -10998.34297569401
Iteration 11600: Loss = -10998.342775842113
Iteration 11700: Loss = -10998.343988785158
1
Iteration 11800: Loss = -10998.342591728575
Iteration 11900: Loss = -10998.343265480333
1
Iteration 12000: Loss = -10998.342604521065
2
Iteration 12100: Loss = -10998.34235732346
Iteration 12200: Loss = -10998.342289086924
Iteration 12300: Loss = -10998.342794803351
1
Iteration 12400: Loss = -10998.34221206876
Iteration 12500: Loss = -10998.342149973836
Iteration 12600: Loss = -10998.342731713856
1
Iteration 12700: Loss = -10998.341981044863
Iteration 12800: Loss = -10998.34484735783
1
Iteration 12900: Loss = -10998.341884241938
Iteration 13000: Loss = -10998.342224901662
1
Iteration 13100: Loss = -10998.341807522891
Iteration 13200: Loss = -10998.342768098622
1
Iteration 13300: Loss = -10998.34179408396
Iteration 13400: Loss = -10998.34165327619
Iteration 13500: Loss = -10998.63475622705
1
Iteration 13600: Loss = -10998.341573959166
Iteration 13700: Loss = -10998.341554695684
Iteration 13800: Loss = -10998.34162265069
1
Iteration 13900: Loss = -10998.341999212944
2
Iteration 14000: Loss = -10998.341847764914
3
Iteration 14100: Loss = -10998.341465019756
Iteration 14200: Loss = -10998.341433695756
Iteration 14300: Loss = -10998.344449853303
1
Iteration 14400: Loss = -10998.341352658386
Iteration 14500: Loss = -10998.341408726918
1
Iteration 14600: Loss = -10998.420188856346
2
Iteration 14700: Loss = -10998.341297476803
Iteration 14800: Loss = -10998.34129795678
1
Iteration 14900: Loss = -10998.344558177178
2
Iteration 15000: Loss = -10998.341542503842
3
Iteration 15100: Loss = -10998.34282014026
4
Iteration 15200: Loss = -10998.341196346262
Iteration 15300: Loss = -10998.371000514866
1
Iteration 15400: Loss = -10998.341155741482
Iteration 15500: Loss = -10998.341215644012
1
Iteration 15600: Loss = -10998.34147129701
2
Iteration 15700: Loss = -10998.34124465037
3
Iteration 15800: Loss = -10998.341838143011
4
Iteration 15900: Loss = -10998.341661359105
5
Iteration 16000: Loss = -10998.341808393829
6
Iteration 16100: Loss = -10998.341614014527
7
Iteration 16200: Loss = -10998.341077607221
Iteration 16300: Loss = -10998.477093277696
1
Iteration 16400: Loss = -10998.340937029609
Iteration 16500: Loss = -10998.342079706961
1
Iteration 16600: Loss = -10998.340913964621
Iteration 16700: Loss = -10998.340878497049
Iteration 16800: Loss = -10998.347692772075
1
Iteration 16900: Loss = -10998.340869404901
Iteration 17000: Loss = -10998.341383679734
1
Iteration 17100: Loss = -10998.341069984293
2
Iteration 17200: Loss = -10998.34127715048
3
Iteration 17300: Loss = -10998.341400755917
4
Iteration 17400: Loss = -10998.34083501998
Iteration 17500: Loss = -10998.34176954325
1
Iteration 17600: Loss = -10998.340792601855
Iteration 17700: Loss = -10998.347732265749
1
Iteration 17800: Loss = -10998.340687840626
Iteration 17900: Loss = -10998.34821243801
1
Iteration 18000: Loss = -10998.297261755157
Iteration 18100: Loss = -10998.295617117037
Iteration 18200: Loss = -10998.296339332672
1
Iteration 18300: Loss = -10998.295593772707
Iteration 18400: Loss = -10998.308909552883
1
Iteration 18500: Loss = -10998.295585439379
Iteration 18600: Loss = -10998.29558627727
1
Iteration 18700: Loss = -10998.302709243346
2
Iteration 18800: Loss = -10998.295586929265
3
Iteration 18900: Loss = -10998.295584144094
Iteration 19000: Loss = -10998.692545416445
1
Iteration 19100: Loss = -10998.29556106526
Iteration 19200: Loss = -10998.295551858908
Iteration 19300: Loss = -10998.295553647442
1
Iteration 19400: Loss = -10998.295581777997
2
Iteration 19500: Loss = -10998.295547892325
Iteration 19600: Loss = -10998.295541415331
Iteration 19700: Loss = -10998.332443588792
1
Iteration 19800: Loss = -10998.295564449832
2
Iteration 19900: Loss = -10998.295564662842
3
tensor([[ -9.9059,   7.5888],
        [-10.3282,   7.5914],
        [ -9.8626,   8.3367],
        [ -9.0180,   7.6270],
        [ -6.5744,   5.1818],
        [ -9.4017,   7.6063],
        [ -9.0152,   7.5500],
        [-10.6647,   8.2777],
        [-12.0322,   7.5389],
        [ -9.2027,   7.4851],
        [ -9.5604,   7.7371],
        [ -9.3293,   7.8302],
        [-10.4056,   7.3953],
        [ -9.4847,   7.9182],
        [-10.0789,   6.8993],
        [ -9.8578,   7.9336],
        [ -9.3100,   7.8441],
        [-10.4532,   7.8801],
        [ -9.9891,   8.3173],
        [ -9.9970,   7.9512],
        [ -9.7920,   7.9225],
        [-10.7661,   6.7809],
        [ -9.6161,   8.2289],
        [-10.1561,   8.4879],
        [-10.2099,   7.8368],
        [ -9.3231,   7.5224],
        [-10.7688,   7.6499],
        [-10.1620,   7.9492],
        [ -9.8469,   7.6920],
        [ -9.7351,   7.9440],
        [ -9.7419,   8.3356],
        [-11.1266,   7.2748],
        [ -9.4991,   7.4105],
        [ -9.7816,   8.1615],
        [ -9.4200,   7.9575],
        [ -9.2357,   7.8487],
        [-10.9578,   7.9156],
        [ -9.2191,   7.8319],
        [ -9.8976,   8.2863],
        [ -9.6190,   7.9536],
        [ -3.5746,   2.1599],
        [ -9.7247,   8.3182],
        [-10.2097,   8.2291],
        [-10.1420,   8.1325],
        [ -9.7839,   7.9372],
        [ -9.5735,   8.1869],
        [ -9.7724,   8.3408],
        [ -9.9670,   8.4078],
        [ -9.6240,   7.8708],
        [-10.9764,   7.4175],
        [ -9.4613,   8.0565],
        [ -9.8412,   8.1714],
        [ -9.8383,   7.9271],
        [  1.5142,  -3.4131],
        [-10.7917,   8.5436],
        [ -9.6434,   8.2468],
        [ -9.9905,   7.7564],
        [ -9.5821,   8.1938],
        [ -9.5217,   8.1162],
        [ -9.3065,   7.0384],
        [-10.4527,   7.7178],
        [ -5.0920,   3.6655],
        [-10.2625,   7.4871],
        [ -9.8242,   8.1323],
        [ -9.2010,   7.8092],
        [ -2.0759,   0.6337],
        [ -9.1747,   7.6933],
        [-10.6280,   8.7031],
        [ -9.4126,   7.9237],
        [ -9.6905,   7.2317],
        [-10.7322,   6.5403],
        [ -9.6933,   8.0312],
        [ -9.1791,   7.4391],
        [ -9.3553,   7.9225],
        [ -9.8461,   7.5992],
        [ -9.7220,   8.1371],
        [ -9.3407,   7.7854],
        [ -9.5712,   8.1590],
        [ -9.6179,   8.1033],
        [-10.1800,   7.0888],
        [-11.0014,   7.6831],
        [ -9.5686,   8.0513],
        [ -9.7778,   8.2865],
        [ -5.5477,   4.1564],
        [ -9.3203,   7.8992],
        [-10.3450,   7.7196],
        [-11.2268,   6.6116],
        [ -9.6403,   7.4848],
        [-10.0180,   8.4308],
        [ -9.5048,   7.9836],
        [ -9.0657,   7.6793],
        [-11.9363,   7.3211],
        [-10.0502,   8.1877],
        [-10.2691,   8.4038],
        [ -4.8996,   1.9713],
        [-10.1972,   8.4002],
        [-10.1220,   7.8051],
        [ -9.3525,   7.9426],
        [ -9.7601,   8.1415],
        [ -9.6932,   8.2795]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 7.7393e-07],
        [1.0016e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0106, 0.9894], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3835, 0.2094],
         [0.8391, 0.1622]],

        [[0.8008, 0.2141],
         [0.1586, 0.7631]],

        [[0.0964, 0.2603],
         [0.5543, 0.8066]],

        [[0.5517, 0.2295],
         [0.0046, 0.7199]],

        [[0.3298, 0.0842],
         [0.3869, 0.8607]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: -0.006096049957872825
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0007489257970643992
Average Adjusted Rand Index: -0.001175382953740917
Iteration 0: Loss = -33301.69334492916
Iteration 10: Loss = -11002.165318256084
Iteration 20: Loss = -11001.392565934144
Iteration 30: Loss = -11000.713452252503
Iteration 40: Loss = -11000.509516333095
Iteration 50: Loss = -11000.40478688339
Iteration 60: Loss = -11000.323750542346
Iteration 70: Loss = -11000.226161156726
Iteration 80: Loss = -11000.08716167534
Iteration 90: Loss = -10999.950301678511
Iteration 100: Loss = -10999.874147085196
Iteration 110: Loss = -10999.837028283793
Iteration 120: Loss = -10999.81680422457
Iteration 130: Loss = -10999.80469990564
Iteration 140: Loss = -10999.797166993778
Iteration 150: Loss = -10999.792171760775
Iteration 160: Loss = -10999.788863285592
Iteration 170: Loss = -10999.786612218311
Iteration 180: Loss = -10999.785075680376
Iteration 190: Loss = -10999.784024939916
Iteration 200: Loss = -10999.783261417853
Iteration 210: Loss = -10999.782809124243
Iteration 220: Loss = -10999.782451456214
Iteration 230: Loss = -10999.782176659024
Iteration 240: Loss = -10999.782056448752
Iteration 250: Loss = -10999.78191664994
Iteration 260: Loss = -10999.781885410875
Iteration 270: Loss = -10999.781830570675
Iteration 280: Loss = -10999.781796084544
Iteration 290: Loss = -10999.781812059282
1
Iteration 300: Loss = -10999.781785957071
Iteration 310: Loss = -10999.78183260522
1
Iteration 320: Loss = -10999.781813690384
2
Iteration 330: Loss = -10999.781839189527
3
Stopping early at iteration 329 due to no improvement.
pi: tensor([[0.8568, 0.1432],
        [0.8179, 0.1821]], dtype=torch.float64)
alpha: tensor([0.8512, 0.1488])
beta: tensor([[[0.1522, 0.1755],
         [0.2361, 0.2141]],

        [[0.3853, 0.1773],
         [0.5173, 0.4912]],

        [[0.9271, 0.2062],
         [0.0872, 0.4630]],

        [[0.7430, 0.1790],
         [0.8932, 0.1875]],

        [[0.9889, 0.1752],
         [0.6291, 0.2949]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.022723709951211306
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001337111052208804
Average Adjusted Rand Index: -0.004997688552185113
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33301.0881103036
Iteration 100: Loss = -11072.640672228288
Iteration 200: Loss = -11025.075141917423
Iteration 300: Loss = -11005.905903444595
Iteration 400: Loss = -11002.234880854605
Iteration 500: Loss = -11001.376036613854
Iteration 600: Loss = -11000.867649669524
Iteration 700: Loss = -11000.470342042505
Iteration 800: Loss = -11000.27469958491
Iteration 900: Loss = -10999.819624057931
Iteration 1000: Loss = -10999.3199967126
Iteration 1100: Loss = -10998.39199764897
Iteration 1200: Loss = -10998.14408781873
Iteration 1300: Loss = -10997.875840095758
Iteration 1400: Loss = -10997.192121827527
Iteration 1500: Loss = -10996.991146300252
Iteration 1600: Loss = -10996.445922024606
Iteration 1700: Loss = -10996.34894517933
Iteration 1800: Loss = -10996.039286625726
Iteration 1900: Loss = -10995.496562519977
Iteration 2000: Loss = -10994.703744153638
Iteration 2100: Loss = -10994.116577958832
Iteration 2200: Loss = -10993.642751561312
Iteration 2300: Loss = -10993.5179423515
Iteration 2400: Loss = -10993.394099581457
Iteration 2500: Loss = -10993.099472662025
Iteration 2600: Loss = -10992.566253545361
Iteration 2700: Loss = -10991.811659933617
Iteration 2800: Loss = -10990.913916321935
Iteration 2900: Loss = -10990.229303611493
Iteration 3000: Loss = -10989.341364796763
Iteration 3100: Loss = -10929.49715130488
Iteration 3200: Loss = -10928.021727203506
Iteration 3300: Loss = -10927.918544030174
Iteration 3400: Loss = -10927.889076208765
Iteration 3500: Loss = -10927.871562444438
Iteration 3600: Loss = -10927.859866757286
Iteration 3700: Loss = -10927.853337636616
Iteration 3800: Loss = -10927.849346166702
Iteration 3900: Loss = -10927.845695441198
Iteration 4000: Loss = -10927.838552490388
Iteration 4100: Loss = -10927.80361883569
Iteration 4200: Loss = -10927.761095549495
Iteration 4300: Loss = -10927.756508200098
Iteration 4400: Loss = -10927.751758697117
Iteration 4500: Loss = -10927.752459119147
1
Iteration 4600: Loss = -10927.747713797216
Iteration 4700: Loss = -10927.635356605679
Iteration 4800: Loss = -10927.631794659806
Iteration 4900: Loss = -10927.304182432894
Iteration 5000: Loss = -10927.252538585519
Iteration 5100: Loss = -10927.257967550078
1
Iteration 5200: Loss = -10927.246495020137
Iteration 5300: Loss = -10927.243765886522
Iteration 5400: Loss = -10927.242824689289
Iteration 5500: Loss = -10927.241651072622
Iteration 5600: Loss = -10926.851073320939
Iteration 5700: Loss = -10923.617817522778
Iteration 5800: Loss = -10923.618611789832
1
Iteration 5900: Loss = -10923.616933815176
Iteration 6000: Loss = -10923.61648700324
Iteration 6100: Loss = -10923.618898783901
1
Iteration 6200: Loss = -10923.615309437148
Iteration 6300: Loss = -10922.820780156042
Iteration 6400: Loss = -10922.837654560968
1
Iteration 6500: Loss = -10922.800102926069
Iteration 6600: Loss = -10922.79665645923
Iteration 6700: Loss = -10922.771831086438
Iteration 6800: Loss = -10922.7985508664
1
Iteration 6900: Loss = -10922.75350157752
Iteration 7000: Loss = -10922.75324437854
Iteration 7100: Loss = -10922.752776490288
Iteration 7200: Loss = -10922.752367005047
Iteration 7300: Loss = -10922.751465349402
Iteration 7400: Loss = -10922.724677751117
Iteration 7500: Loss = -10922.723800646603
Iteration 7600: Loss = -10922.736707722052
1
Iteration 7700: Loss = -10922.726840207457
2
Iteration 7800: Loss = -10922.719304830855
Iteration 7900: Loss = -10922.718112485622
Iteration 8000: Loss = -10922.718581595414
1
Iteration 8100: Loss = -10922.717975984084
Iteration 8200: Loss = -10922.717952589024
Iteration 8300: Loss = -10922.71832105185
1
Iteration 8400: Loss = -10922.710992510927
Iteration 8500: Loss = -10916.961233415726
Iteration 8600: Loss = -10916.953082371389
Iteration 8700: Loss = -10916.938201505358
Iteration 8800: Loss = -10916.938315492687
1
Iteration 8900: Loss = -10916.937324313125
Iteration 9000: Loss = -10916.936287652961
Iteration 9100: Loss = -10916.95625386468
1
Iteration 9200: Loss = -10916.951104542639
2
Iteration 9300: Loss = -10916.26131216309
Iteration 9400: Loss = -10916.287181296473
1
Iteration 9500: Loss = -10916.247423572528
Iteration 9600: Loss = -10916.245977451432
Iteration 9700: Loss = -10916.246389029298
1
Iteration 9800: Loss = -10916.254330686868
2
Iteration 9900: Loss = -10916.24524965822
Iteration 10000: Loss = -10916.245681810493
1
Iteration 10100: Loss = -10916.246645976866
2
Iteration 10200: Loss = -10916.301531397603
3
Iteration 10300: Loss = -10916.25407312718
4
Iteration 10400: Loss = -10916.246205405274
5
Iteration 10500: Loss = -10916.274252121593
6
Iteration 10600: Loss = -10916.244854133382
Iteration 10700: Loss = -10916.240429916441
Iteration 10800: Loss = -10916.238766679215
Iteration 10900: Loss = -10916.238010621142
Iteration 11000: Loss = -10916.252552879536
1
Iteration 11100: Loss = -10916.258362322493
2
Iteration 11200: Loss = -10916.22533698038
Iteration 11300: Loss = -10916.224838879958
Iteration 11400: Loss = -10916.237993274872
1
Iteration 11500: Loss = -10916.224653716157
Iteration 11600: Loss = -10916.224347097492
Iteration 11700: Loss = -10916.223806400503
Iteration 11800: Loss = -10916.223672012515
Iteration 11900: Loss = -10916.223879231173
1
Iteration 12000: Loss = -10916.230446142801
2
Iteration 12100: Loss = -10916.222779656302
Iteration 12200: Loss = -10915.892802431244
Iteration 12300: Loss = -10916.014181323453
1
Iteration 12400: Loss = -10915.889545675946
Iteration 12500: Loss = -10915.889724271688
1
Iteration 12600: Loss = -10915.905035078067
2
Iteration 12700: Loss = -10915.889496547861
Iteration 12800: Loss = -10915.889927522256
1
Iteration 12900: Loss = -10915.904431521736
2
Iteration 13000: Loss = -10916.045213066825
3
Iteration 13100: Loss = -10915.887624595385
Iteration 13200: Loss = -10915.887638012584
1
Iteration 13300: Loss = -10915.892907770312
2
Iteration 13400: Loss = -10915.897086316128
3
Iteration 13500: Loss = -10915.887645786865
4
Iteration 13600: Loss = -10915.888552902392
5
Iteration 13700: Loss = -10913.963696391744
Iteration 13800: Loss = -10913.800387045905
Iteration 13900: Loss = -10913.800541020886
1
Iteration 14000: Loss = -10913.828642729432
2
Iteration 14100: Loss = -10913.801611402001
3
Iteration 14200: Loss = -10913.800205685298
Iteration 14300: Loss = -10913.797774119845
Iteration 14400: Loss = -10913.79686280128
Iteration 14500: Loss = -10913.797741967692
1
Iteration 14600: Loss = -10913.871519198108
2
Iteration 14700: Loss = -10913.806374340636
3
Iteration 14800: Loss = -10913.8072342817
4
Iteration 14900: Loss = -10913.800496830387
5
Iteration 15000: Loss = -10913.797183194614
6
Iteration 15100: Loss = -10913.795354594957
Iteration 15200: Loss = -10913.857302864417
1
Iteration 15300: Loss = -10913.807982848874
2
Iteration 15400: Loss = -10913.7297778537
Iteration 15500: Loss = -10913.729289018509
Iteration 15600: Loss = -10913.727411182983
Iteration 15700: Loss = -10913.73235321414
1
Iteration 15800: Loss = -10913.725903472168
Iteration 15900: Loss = -10913.72653885183
1
Iteration 16000: Loss = -10913.725735887567
Iteration 16100: Loss = -10913.727646749994
1
Iteration 16200: Loss = -10913.707537332044
Iteration 16300: Loss = -10913.707565650539
1
Iteration 16400: Loss = -10913.707979189867
2
Iteration 16500: Loss = -10913.707269811843
Iteration 16600: Loss = -10913.708905712954
1
Iteration 16700: Loss = -10913.717641540721
2
Iteration 16800: Loss = -10913.491247127638
Iteration 16900: Loss = -10913.491368703424
1
Iteration 17000: Loss = -10913.488841293998
Iteration 17100: Loss = -10913.496125691
1
Iteration 17200: Loss = -10913.506037468236
2
Iteration 17300: Loss = -10913.47837818817
Iteration 17400: Loss = -10913.478342412014
Iteration 17500: Loss = -10913.715216286602
1
Iteration 17600: Loss = -10913.4779811055
Iteration 17700: Loss = -10913.480765179003
1
Iteration 17800: Loss = -10913.477997156431
2
Iteration 17900: Loss = -10913.479248640842
3
Iteration 18000: Loss = -10913.478051494512
4
Iteration 18100: Loss = -10913.478218466626
5
Iteration 18200: Loss = -10913.478210986783
6
Iteration 18300: Loss = -10913.477047974724
Iteration 18400: Loss = -10913.52897285805
1
Iteration 18500: Loss = -10913.476210526262
Iteration 18600: Loss = -10913.481135624483
1
Iteration 18700: Loss = -10913.481592041382
2
Iteration 18800: Loss = -10913.476236521661
3
Iteration 18900: Loss = -10913.494470488095
4
Iteration 19000: Loss = -10913.47980342201
5
Iteration 19100: Loss = -10913.546090378772
6
Iteration 19200: Loss = -10913.529968983672
7
Iteration 19300: Loss = -10913.49757054531
8
Iteration 19400: Loss = -10913.476141750252
Iteration 19500: Loss = -10913.477307377265
1
Iteration 19600: Loss = -10913.477616364951
2
Iteration 19700: Loss = -10913.47614658672
3
Iteration 19800: Loss = -10913.67523287044
4
Iteration 19900: Loss = -10913.477643422313
5
tensor([[-1.7333, -0.2343],
        [ 0.0152, -1.8854],
        [ 0.7503, -2.1376],
        [-5.3553,  1.6902],
        [-0.6189, -0.7823],
        [ 1.9337, -3.3492],
        [-4.6989,  3.1620],
        [ 2.5349, -3.9333],
        [-2.0263,  0.5732],
        [ 1.3241, -2.7767],
        [ 0.5631, -1.9702],
        [-1.6102,  0.1585],
        [ 1.9852, -3.8418],
        [ 1.1328, -3.2692],
        [ 1.6043, -3.0000],
        [ 0.9897, -2.5488],
        [ 0.5489, -2.2445],
        [-4.1386,  2.5253],
        [ 1.5491, -2.9413],
        [-3.0224,  1.1598],
        [ 0.5999, -2.1029],
        [ 0.8883, -3.5849],
        [-2.7293,  0.1705],
        [-0.6005, -1.1212],
        [-2.5335, -0.7182],
        [ 3.3759, -5.1735],
        [-4.1133,  2.4724],
        [ 0.9713, -2.3590],
        [ 3.5373, -5.4784],
        [-2.9197,  1.2694],
        [-4.7928,  2.4118],
        [ 1.2649, -2.6641],
        [-5.0410,  2.7797],
        [-3.6407,  1.5994],
        [-2.5057,  0.1005],
        [-2.2418, -0.0381],
        [-2.8581,  1.2617],
        [-2.0787,  0.5923],
        [-3.5213,  1.9903],
        [ 0.4280, -3.8095],
        [ 3.8287, -5.3634],
        [-1.1479, -1.4255],
        [ 0.8107, -2.3583],
        [ 2.4280, -3.8630],
        [-0.2829, -2.8010],
        [ 0.9925, -2.5938],
        [ 0.5421, -1.9798],
        [ 1.9933, -4.7415],
        [-5.4028,  3.6196],
        [-3.6062,  1.7567],
        [-2.6241,  1.1583],
        [ 2.4865, -3.8863],
        [-5.8404,  4.2313],
        [-4.7017,  3.0952],
        [ 3.4865, -4.8848],
        [ 2.7940, -5.4468],
        [ 1.9646, -3.4090],
        [-2.8177,  1.2606],
        [ 1.4699, -3.6402],
        [-2.0284,  0.5306],
        [-3.1129, -0.4131],
        [ 0.5177, -2.4489],
        [ 3.8711, -5.5484],
        [-2.5645,  0.3144],
        [-3.2657, -1.0906],
        [ 2.7769, -4.9795],
        [ 1.4187, -2.8051],
        [ 2.0731, -3.6616],
        [-3.3106,  1.9143],
        [-4.6432,  2.3425],
        [-0.9525, -0.4423],
        [ 0.8490, -2.2503],
        [-3.3175,  1.6317],
        [ 2.7854, -4.2307],
        [ 0.8549, -3.0990],
        [-0.9192, -2.1372],
        [-5.6915,  2.5728],
        [ 3.6333, -5.0348],
        [ 1.9566, -4.0890],
        [ 2.0230, -3.9882],
        [-5.5733,  2.8397],
        [-2.4039,  1.0173],
        [-3.8763,  2.4188],
        [ 1.4132, -3.0176],
        [ 2.5616, -3.9666],
        [-1.6136, -0.2593],
        [-2.3990,  0.2135],
        [-6.8932,  5.0848],
        [-1.0511, -2.3041],
        [-2.1296,  0.6627],
        [ 0.1562, -2.7223],
        [-0.2959, -1.6576],
        [ 0.6086, -3.7057],
        [-1.1118, -0.2844],
        [-4.7959,  0.1807],
        [-0.1467, -1.2398],
        [-6.8708,  2.2556],
        [-4.5219,  3.1340],
        [ 0.4590, -1.8550],
        [-4.9388,  2.9163]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7706, 0.2294],
        [0.1856, 0.8144]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5308, 0.4692], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1937, 0.1136],
         [0.2361, 0.2406]],

        [[0.3853, 0.1087],
         [0.5173, 0.4912]],

        [[0.9271, 0.1110],
         [0.0872, 0.4630]],

        [[0.7430, 0.1087],
         [0.8932, 0.1875]],

        [[0.9889, 0.0939],
         [0.6291, 0.2949]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080740404436667
time is 1
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 90
Adjusted Rand Index: 0.6363374553319541
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 94
Adjusted Rand Index: 0.7720712031338862
time is 3
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
time is 4
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448351863643042
Global Adjusted Rand Index: 0.788120058370458
Average Adjusted Rand Index: 0.7887478727370115
Iteration 0: Loss = -31450.855354054213
Iteration 10: Loss = -11001.624636249497
Iteration 20: Loss = -11000.964910622006
Iteration 30: Loss = -11000.59816896896
Iteration 40: Loss = -11000.404782364181
Iteration 50: Loss = -11000.319660318892
Iteration 60: Loss = -11000.290552455834
Iteration 70: Loss = -11000.281428451945
Iteration 80: Loss = -11000.267522992193
Iteration 90: Loss = -11000.199603246205
Iteration 100: Loss = -11000.029563016084
Iteration 110: Loss = -10999.886303664302
Iteration 120: Loss = -10999.831676531094
Iteration 130: Loss = -10999.81138204227
Iteration 140: Loss = -10999.800999254383
Iteration 150: Loss = -10999.794661822676
Iteration 160: Loss = -10999.790505028715
Iteration 170: Loss = -10999.7877562419
Iteration 180: Loss = -10999.78589264827
Iteration 190: Loss = -10999.784542247418
Iteration 200: Loss = -10999.783681398669
Iteration 210: Loss = -10999.78304836277
Iteration 220: Loss = -10999.782659768114
Iteration 230: Loss = -10999.782316742172
Iteration 240: Loss = -10999.782127691216
Iteration 250: Loss = -10999.78196119437
Iteration 260: Loss = -10999.78193412692
Iteration 270: Loss = -10999.781822045052
Iteration 280: Loss = -10999.781810440058
Iteration 290: Loss = -10999.781797895132
Iteration 300: Loss = -10999.781830316831
1
Iteration 310: Loss = -10999.781796804547
Iteration 320: Loss = -10999.78180457536
1
Iteration 330: Loss = -10999.78181836582
2
Iteration 340: Loss = -10999.781845193118
3
Stopping early at iteration 339 due to no improvement.
pi: tensor([[0.8567, 0.1433],
        [0.8178, 0.1822]], dtype=torch.float64)
alpha: tensor([0.8511, 0.1489])
beta: tensor([[[0.1522, 0.1755],
         [0.5034, 0.2141]],

        [[0.4293, 0.1773],
         [0.7080, 0.0882]],

        [[0.5517, 0.2062],
         [0.8258, 0.3816]],

        [[0.2333, 0.1790],
         [0.5202, 0.5713]],

        [[0.1591, 0.1752],
         [0.9702, 0.4083]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.022723709951211306
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001337111052208804
Average Adjusted Rand Index: -0.004997688552185113
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31451.058067258276
Iteration 100: Loss = -11078.76756363119
Iteration 200: Loss = -11032.461976837556
Iteration 300: Loss = -11012.795959376235
Iteration 400: Loss = -11006.19046580004
Iteration 500: Loss = -11004.842333691446
Iteration 600: Loss = -11004.148982776156
Iteration 700: Loss = -11003.7072632774
Iteration 800: Loss = -11003.400435082056
Iteration 900: Loss = -11003.17424623945
Iteration 1000: Loss = -11002.999486074854
Iteration 1100: Loss = -11002.861269146695
Iteration 1200: Loss = -11002.749269006848
Iteration 1300: Loss = -11002.656193449713
Iteration 1400: Loss = -11002.577644043678
Iteration 1500: Loss = -11002.510656349754
Iteration 1600: Loss = -11002.453550589446
Iteration 1700: Loss = -11002.40546402497
Iteration 1800: Loss = -11002.36545751982
Iteration 1900: Loss = -11002.332036656695
Iteration 2000: Loss = -11002.30387326723
Iteration 2100: Loss = -11002.27979306941
Iteration 2200: Loss = -11002.259028450058
Iteration 2300: Loss = -11002.240935840218
Iteration 2400: Loss = -11002.225010101809
Iteration 2500: Loss = -11002.210930024297
Iteration 2600: Loss = -11002.1983249068
Iteration 2700: Loss = -11002.187048657017
Iteration 2800: Loss = -11002.176902333944
Iteration 2900: Loss = -11002.167688013973
Iteration 3000: Loss = -11002.159339546537
Iteration 3100: Loss = -11002.151655191952
Iteration 3200: Loss = -11002.14469312106
Iteration 3300: Loss = -11002.138253312174
Iteration 3400: Loss = -11002.13232642086
Iteration 3500: Loss = -11002.126846563411
Iteration 3600: Loss = -11002.121786512907
Iteration 3700: Loss = -11002.116958311988
Iteration 3800: Loss = -11002.11252580185
Iteration 3900: Loss = -11002.108239083005
Iteration 4000: Loss = -11002.104167735815
Iteration 4100: Loss = -11002.100223157368
Iteration 4200: Loss = -11002.096243715341
Iteration 4300: Loss = -11002.092123322118
Iteration 4400: Loss = -11002.087320343717
Iteration 4500: Loss = -11002.08003557967
Iteration 4600: Loss = -11002.068380214598
Iteration 4700: Loss = -11002.051401403394
Iteration 4800: Loss = -11002.029128208844
Iteration 4900: Loss = -11002.014228148731
Iteration 5000: Loss = -11001.998978369656
Iteration 5100: Loss = -11001.96943424407
Iteration 5200: Loss = -11001.91560815643
Iteration 5300: Loss = -11001.821081783879
Iteration 5400: Loss = -11001.699672905415
Iteration 5500: Loss = -11001.647048295974
Iteration 5600: Loss = -11001.626318935176
Iteration 5700: Loss = -11001.59339576981
Iteration 5800: Loss = -11001.558337069851
Iteration 5900: Loss = -11001.516656232347
Iteration 6000: Loss = -11001.467287962258
Iteration 6100: Loss = -11001.436554007876
Iteration 6200: Loss = -11001.393236354967
Iteration 6300: Loss = -11001.357190823017
Iteration 6400: Loss = -11001.328490670881
Iteration 6500: Loss = -11001.46502615765
1
Iteration 6600: Loss = -11001.22003332018
Iteration 6700: Loss = -11000.567588449683
Iteration 6800: Loss = -10999.807405051943
Iteration 6900: Loss = -10999.604637648365
Iteration 7000: Loss = -10999.515814065298
Iteration 7100: Loss = -10999.478825876758
Iteration 7200: Loss = -10999.55892654526
1
Iteration 7300: Loss = -10999.427680481078
Iteration 7400: Loss = -10999.382360388132
Iteration 7500: Loss = -10998.816382965062
Iteration 7600: Loss = -10998.39722886714
Iteration 7700: Loss = -10998.331408114524
Iteration 7800: Loss = -10998.299468332194
Iteration 7900: Loss = -10998.27968261579
Iteration 8000: Loss = -10998.264204567642
Iteration 8100: Loss = -10997.29195906213
Iteration 8200: Loss = -10997.276833048218
Iteration 8300: Loss = -10995.324588817559
Iteration 8400: Loss = -10995.3184103771
Iteration 8500: Loss = -10995.26169772943
Iteration 8600: Loss = -10995.052312299485
Iteration 8700: Loss = -10995.050433873454
Iteration 8800: Loss = -10995.050247627825
Iteration 8900: Loss = -10995.047725365795
Iteration 9000: Loss = -10995.046652803052
Iteration 9100: Loss = -10995.04582126061
Iteration 9200: Loss = -10995.044826365493
Iteration 9300: Loss = -10995.043989848935
Iteration 9400: Loss = -10995.0432824816
Iteration 9500: Loss = -10995.038925923871
Iteration 9600: Loss = -10994.994116891361
Iteration 9700: Loss = -10995.063696190286
1
Iteration 9800: Loss = -10994.927773883344
Iteration 9900: Loss = -10994.927302730075
Iteration 10000: Loss = -10994.926923505802
Iteration 10100: Loss = -10994.92708835795
1
Iteration 10200: Loss = -10994.926238225979
Iteration 10300: Loss = -10994.925960639917
Iteration 10400: Loss = -10994.943683641402
1
Iteration 10500: Loss = -10994.925381858446
Iteration 10600: Loss = -10994.925174218239
Iteration 10700: Loss = -10994.927901250003
1
Iteration 10800: Loss = -10994.924738177056
Iteration 10900: Loss = -10994.924541214612
Iteration 11000: Loss = -10994.924304019045
Iteration 11100: Loss = -10994.928868169683
1
Iteration 11200: Loss = -10994.923974789652
Iteration 11300: Loss = -10994.923804334341
Iteration 11400: Loss = -10994.924986609933
1
Iteration 11500: Loss = -10994.923527751083
Iteration 11600: Loss = -10994.923363555079
Iteration 11700: Loss = -10995.251172921451
1
Iteration 11800: Loss = -10994.919243272248
Iteration 11900: Loss = -10994.897423324739
Iteration 12000: Loss = -10995.09011626203
1
Iteration 12100: Loss = -10994.897159990087
Iteration 12200: Loss = -10994.897092458079
Iteration 12300: Loss = -10995.008775621487
1
Iteration 12400: Loss = -10994.896878045025
Iteration 12500: Loss = -10995.018939742531
1
Iteration 12600: Loss = -10994.896745548062
Iteration 12700: Loss = -10994.896613462082
Iteration 12800: Loss = -10994.897127728911
1
Iteration 12900: Loss = -10994.896520994558
Iteration 13000: Loss = -10994.896604057885
1
Iteration 13100: Loss = -10994.896421430485
Iteration 13200: Loss = -10994.896356788531
Iteration 13300: Loss = -10994.902516732489
1
Iteration 13400: Loss = -10994.896236169727
Iteration 13500: Loss = -10994.897566171354
1
Iteration 13600: Loss = -10994.897591157158
2
Iteration 13700: Loss = -10994.896164924008
Iteration 13800: Loss = -10994.897149001854
1
Iteration 13900: Loss = -10994.896203672417
2
Iteration 14000: Loss = -10994.921392029159
3
Iteration 14100: Loss = -10994.900810819194
4
Iteration 14200: Loss = -10994.895984649282
Iteration 14300: Loss = -10994.895977737335
Iteration 14400: Loss = -10994.895878149662
Iteration 14500: Loss = -10994.900910077276
1
Iteration 14600: Loss = -10994.895840045625
Iteration 14700: Loss = -10994.897930394045
1
Iteration 14800: Loss = -10994.89578869226
Iteration 14900: Loss = -10994.906671330094
1
Iteration 15000: Loss = -10994.896491465592
2
Iteration 15100: Loss = -10994.895768699651
Iteration 15200: Loss = -10994.895761347496
Iteration 15300: Loss = -10994.911408555206
1
Iteration 15400: Loss = -10994.897271894113
2
Iteration 15500: Loss = -10994.89579491113
3
Iteration 15600: Loss = -10994.897206707445
4
Iteration 15700: Loss = -10994.903571921604
5
Iteration 15800: Loss = -10994.895669396103
Iteration 15900: Loss = -10994.903008683177
1
Iteration 16000: Loss = -10994.89613194139
2
Iteration 16100: Loss = -10994.897880664541
3
Iteration 16200: Loss = -10994.896838282984
4
Iteration 16300: Loss = -10994.895684612464
5
Iteration 16400: Loss = -10994.89995621979
6
Iteration 16500: Loss = -10994.896723119615
7
Iteration 16600: Loss = -10994.895816985701
8
Iteration 16700: Loss = -10994.96032614985
9
Iteration 16800: Loss = -10994.89556328115
Iteration 16900: Loss = -10994.909904494561
1
Iteration 17000: Loss = -10994.895516606211
Iteration 17100: Loss = -10994.897991800133
1
Iteration 17200: Loss = -10994.895527535384
2
Iteration 17300: Loss = -10994.898669123904
3
Iteration 17400: Loss = -10994.89551999861
4
Iteration 17500: Loss = -10994.896594722366
5
Iteration 17600: Loss = -10994.89550735357
Iteration 17700: Loss = -10994.933579552137
1
Iteration 17800: Loss = -10994.895819576965
2
Iteration 17900: Loss = -10994.899064856247
3
Iteration 18000: Loss = -10994.90582986252
4
Iteration 18100: Loss = -10994.895508682988
5
Iteration 18200: Loss = -10994.902197087007
6
Iteration 18300: Loss = -10994.895455098836
Iteration 18400: Loss = -10994.899575228957
1
Iteration 18500: Loss = -10994.895463046956
2
Iteration 18600: Loss = -10994.895859027009
3
Iteration 18700: Loss = -10994.896458829895
4
Iteration 18800: Loss = -10994.89557767369
5
Iteration 18900: Loss = -10994.89680822293
6
Iteration 19000: Loss = -10994.896359094053
7
Iteration 19100: Loss = -10994.895658429614
8
Iteration 19200: Loss = -10994.895577437692
9
Iteration 19300: Loss = -10994.895924030227
10
Stopping early at iteration 19300 due to no improvement.
tensor([[ 7.1008e+00, -8.8027e+00],
        [ 7.2038e+00, -8.6061e+00],
        [ 7.1834e+00, -9.2105e+00],
        [ 6.3178e+00, -8.1926e+00],
        [ 3.4067e+00, -4.9637e+00],
        [ 7.4409e+00, -8.8286e+00],
        [-2.3950e-01, -3.1931e+00],
        [ 7.2471e+00, -9.0743e+00],
        [ 7.4432e+00, -8.9717e+00],
        [ 6.4470e+00, -8.8987e+00],
        [ 7.3816e+00, -8.7686e+00],
        [ 7.1227e+00, -8.8654e+00],
        [ 7.4889e+00, -9.0501e+00],
        [ 6.7961e+00, -8.5154e+00],
        [ 5.9331e+00, -1.0548e+01],
        [ 7.7614e+00, -9.1478e+00],
        [ 6.5134e+00, -7.9120e+00],
        [ 7.3674e+00, -8.7640e+00],
        [ 7.0772e+00, -9.7545e+00],
        [ 7.1192e+00, -9.3995e+00],
        [ 7.5003e+00, -8.8898e+00],
        [ 7.4330e+00, -8.8565e+00],
        [ 6.6995e+00, -9.5821e+00],
        [ 7.0813e+00, -9.5502e+00],
        [ 6.7145e+00, -8.9197e+00],
        [ 1.8456e-02, -1.5774e+00],
        [ 6.8310e+00, -8.2220e+00],
        [ 7.2166e+00, -8.9914e+00],
        [ 6.5555e+00, -8.1315e+00],
        [ 7.5010e+00, -8.8944e+00],
        [ 7.2229e+00, -8.6370e+00],
        [ 7.3076e+00, -9.1693e+00],
        [-2.1453e+00,  7.1277e-01],
        [ 7.2624e+00, -8.6487e+00],
        [ 7.1000e+00, -8.9744e+00],
        [ 6.9165e+00, -9.2971e+00],
        [-3.3594e-01, -1.6363e+00],
        [ 6.2903e+00, -8.9728e+00],
        [ 7.0832e+00, -8.5087e+00],
        [ 7.3451e+00, -1.0117e+01],
        [-5.2695e-03, -1.7132e+00],
        [-8.8644e-02, -3.6759e+00],
        [ 7.3957e+00, -9.0436e+00],
        [ 7.1809e+00, -8.5984e+00],
        [ 7.2397e+00, -9.2496e+00],
        [ 7.3888e+00, -9.6441e+00],
        [ 7.3388e+00, -9.5047e+00],
        [ 6.9866e+00, -1.1032e+01],
        [ 6.6772e+00, -8.5675e+00],
        [ 1.0639e+00, -2.7221e+00],
        [ 7.5404e+00, -8.9310e+00],
        [ 7.8641e+00, -9.3007e+00],
        [ 7.3737e+00, -8.8231e+00],
        [-3.4137e+00,  2.0151e+00],
        [ 6.7581e+00, -1.1373e+01],
        [ 7.5889e+00, -9.0113e+00],
        [ 6.6336e+00, -9.4872e+00],
        [ 7.5794e+00, -9.1641e+00],
        [ 7.2174e+00, -9.9980e+00],
        [-2.1954e+00,  7.6146e-01],
        [ 6.7080e+00, -9.0343e+00],
        [ 6.6383e+00, -8.3312e+00],
        [ 7.4035e+00, -9.1516e+00],
        [ 7.6062e+00, -9.6729e+00],
        [ 6.7930e+00, -8.4300e+00],
        [-1.0726e+00, -3.7645e-01],
        [ 7.3063e+00, -8.7089e+00],
        [ 7.4112e+00, -9.2919e+00],
        [-1.7932e+00,  1.7204e-01],
        [ 6.6557e+00, -8.5199e+00],
        [ 7.3971e+00, -8.8341e+00],
        [ 7.0422e+00, -9.5071e+00],
        [ 5.6194e+00, -1.0235e+01],
        [ 7.1795e+00, -8.5712e+00],
        [ 1.0966e+00, -2.5135e+00],
        [ 7.1455e+00, -8.9246e+00],
        [ 6.5582e+00, -8.5005e+00],
        [ 6.9102e+00, -9.7027e+00],
        [ 7.5575e+00, -9.3124e+00],
        [ 7.2961e+00, -8.7113e+00],
        [ 7.0686e+00, -8.6290e+00],
        [ 7.5736e+00, -9.5898e+00],
        [ 6.7930e+00, -1.0502e+01],
        [ 6.9437e+00, -9.9361e+00],
        [ 7.2288e+00, -8.7993e+00],
        [ 5.9162e+00, -1.0389e+01],
        [ 6.9973e+00, -8.8590e+00],
        [ 5.8870e+00, -9.8074e+00],
        [ 7.1263e+00, -1.1341e+01],
        [ 6.9354e+00, -8.3220e+00],
        [ 6.8156e+00, -8.5619e+00],
        [ 7.2307e+00, -8.6180e+00],
        [ 7.0439e+00, -9.1448e+00],
        [ 7.4663e+00, -9.3078e+00],
        [ 6.6083e+00, -8.5040e+00],
        [ 7.2324e+00, -8.6553e+00],
        [ 2.2613e-01, -2.4433e+00],
        [ 6.6180e+00, -9.3426e+00],
        [ 7.2186e+00, -8.7143e+00],
        [ 6.9772e+00, -9.2602e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 1.5595e-07],
        [9.9653e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9484, 0.0516], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1595, 0.2070],
         [0.5034, 0.2265]],

        [[0.4293, 0.1813],
         [0.7080, 0.0882]],

        [[0.5517, 0.2497],
         [0.8258, 0.3816]],

        [[0.2333, 0.2058],
         [0.5202, 0.5713]],

        [[0.1591, 0.1220],
         [0.9702, 0.4083]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.004166919759460609
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.01644068827141728
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.022328462449853457
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.004851708247904022
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0015221131464426677
Global Adjusted Rand Index: -0.0008552510891647506
Average Adjusted Rand Index: -0.001618935162664451
Iteration 0: Loss = -16981.13549508794
Iteration 10: Loss = -11002.291598751573
Iteration 20: Loss = -11002.039398816021
Iteration 30: Loss = -11001.352774231193
Iteration 40: Loss = -11000.769979684586
Iteration 50: Loss = -11000.335923983592
Iteration 60: Loss = -11000.077386099463
Iteration 70: Loss = -10999.94629072623
Iteration 80: Loss = -10999.879470030024
Iteration 90: Loss = -10999.842415022316
Iteration 100: Loss = -10999.820581049418
Iteration 110: Loss = -10999.807180665088
Iteration 120: Loss = -10999.798715127374
Iteration 130: Loss = -10999.793237626245
Iteration 140: Loss = -10999.789591124283
Iteration 150: Loss = -10999.787122297617
Iteration 160: Loss = -10999.78541011347
Iteration 170: Loss = -10999.784270468519
Iteration 180: Loss = -10999.78345328364
Iteration 190: Loss = -10999.782907936706
Iteration 200: Loss = -10999.782533876427
Iteration 210: Loss = -10999.782275319882
Iteration 220: Loss = -10999.782043018695
Iteration 230: Loss = -10999.781945070037
Iteration 240: Loss = -10999.781856127853
Iteration 250: Loss = -10999.781825554293
Iteration 260: Loss = -10999.781770496305
Iteration 270: Loss = -10999.781812339144
1
Iteration 280: Loss = -10999.7817746857
2
Iteration 290: Loss = -10999.78182198647
3
Stopping early at iteration 289 due to no improvement.
pi: tensor([[0.8574, 0.1426],
        [0.8185, 0.1815]], dtype=torch.float64)
alpha: tensor([0.8518, 0.1482])
beta: tensor([[[0.1522, 0.1756],
         [0.1926, 0.2141]],

        [[0.7210, 0.1773],
         [0.3165, 0.2820]],

        [[0.4974, 0.2064],
         [0.4423, 0.7532]],

        [[0.6796, 0.1791],
         [0.0546, 0.9783]],

        [[0.9255, 0.1752],
         [0.0157, 0.1288]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.022723709951211306
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0012836029363061082
Average Adjusted Rand Index: -0.004544741990242261
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16980.8732858839
Iteration 100: Loss = -11005.723820740372
Iteration 200: Loss = -11001.234321398248
Iteration 300: Loss = -11000.854217204913
Iteration 400: Loss = -11000.598664998355
Iteration 500: Loss = -11000.410880759271
Iteration 600: Loss = -11000.290754957257
Iteration 700: Loss = -11000.215896944092
Iteration 800: Loss = -11000.161664604386
Iteration 900: Loss = -11000.116707478213
Iteration 1000: Loss = -11000.069259429722
Iteration 1100: Loss = -11000.010209543221
Iteration 1200: Loss = -10999.930249628302
Iteration 1300: Loss = -10999.836096314219
Iteration 1400: Loss = -10999.756628947276
Iteration 1500: Loss = -10999.700135416027
Iteration 1600: Loss = -10999.654009035847
Iteration 1700: Loss = -10999.606017247897
Iteration 1800: Loss = -10999.54657957919
Iteration 1900: Loss = -10999.471297458045
Iteration 2000: Loss = -10999.385704573855
Iteration 2100: Loss = -10999.207983622175
Iteration 2200: Loss = -10998.961438025097
Iteration 2300: Loss = -10998.659060374708
Iteration 2400: Loss = -10998.370028809106
Iteration 2500: Loss = -10997.897368747263
Iteration 2600: Loss = -10996.157323813914
Iteration 2700: Loss = -10995.073151717308
Iteration 2800: Loss = -10994.753058407734
Iteration 2900: Loss = -10994.635935654447
Iteration 3000: Loss = -10994.510339586235
Iteration 3100: Loss = -10994.452735703282
Iteration 3200: Loss = -10994.423940241648
Iteration 3300: Loss = -10994.383095504307
Iteration 3400: Loss = -10994.36058207192
Iteration 3500: Loss = -10994.342867344465
Iteration 3600: Loss = -10994.328753843365
Iteration 3700: Loss = -10994.316828142502
Iteration 3800: Loss = -10994.30704260169
Iteration 3900: Loss = -10994.313556703051
1
Iteration 4000: Loss = -10994.291564920713
Iteration 4100: Loss = -10994.285423425463
Iteration 4200: Loss = -10994.280673130112
Iteration 4300: Loss = -10994.275271161512
Iteration 4400: Loss = -10994.271156098157
Iteration 4500: Loss = -10994.36872691162
1
Iteration 4600: Loss = -10994.264062516258
Iteration 4700: Loss = -10994.261031548036
Iteration 4800: Loss = -10994.258373286444
Iteration 4900: Loss = -10994.255952144365
Iteration 5000: Loss = -10994.253644864906
Iteration 5100: Loss = -10994.25158557181
Iteration 5200: Loss = -10994.24977075712
Iteration 5300: Loss = -10994.248021221752
Iteration 5400: Loss = -10994.246419195077
Iteration 5500: Loss = -10994.26772272574
1
Iteration 5600: Loss = -10994.243581616125
Iteration 5700: Loss = -10994.242374078729
Iteration 5800: Loss = -10994.250852766905
1
Iteration 5900: Loss = -10994.24014803755
Iteration 6000: Loss = -10994.239131326944
Iteration 6100: Loss = -10994.23820591471
Iteration 6200: Loss = -10994.237530746776
Iteration 6300: Loss = -10994.23650680957
Iteration 6400: Loss = -10994.23568214528
Iteration 6500: Loss = -10994.238648042114
1
Iteration 6600: Loss = -10994.234306403077
Iteration 6700: Loss = -10994.233615595449
Iteration 6800: Loss = -10994.247214898667
1
Iteration 6900: Loss = -10994.232388475084
Iteration 7000: Loss = -10994.231896645584
Iteration 7100: Loss = -10994.231388679073
Iteration 7200: Loss = -10994.242036567535
1
Iteration 7300: Loss = -10994.230463238688
Iteration 7400: Loss = -10994.230012419564
Iteration 7500: Loss = -10994.23428936734
1
Iteration 7600: Loss = -10994.229288590517
Iteration 7700: Loss = -10994.228928129518
Iteration 7800: Loss = -10994.232592532671
1
Iteration 7900: Loss = -10994.228326433526
Iteration 8000: Loss = -10994.228024114695
Iteration 8100: Loss = -10994.227931933341
Iteration 8200: Loss = -10994.227521064897
Iteration 8300: Loss = -10994.227251037988
Iteration 8400: Loss = -10994.302688843634
1
Iteration 8500: Loss = -10994.226787143982
Iteration 8600: Loss = -10994.22658991202
Iteration 8700: Loss = -10994.649021791494
1
Iteration 8800: Loss = -10994.226187583938
Iteration 8900: Loss = -10994.22599146211
Iteration 9000: Loss = -10994.255321934552
1
Iteration 9100: Loss = -10994.225668446985
Iteration 9200: Loss = -10994.225515880173
Iteration 9300: Loss = -10994.338816439102
1
Iteration 9400: Loss = -10994.225249377436
Iteration 9500: Loss = -10994.225125018393
Iteration 9600: Loss = -10994.225427775847
1
Iteration 9700: Loss = -10994.224938457528
Iteration 9800: Loss = -10994.226377830524
1
Iteration 9900: Loss = -10994.224677286535
Iteration 10000: Loss = -10994.224599726922
Iteration 10100: Loss = -10994.224492505658
Iteration 10200: Loss = -10994.22800293823
1
Iteration 10300: Loss = -10994.224265778066
Iteration 10400: Loss = -10994.224259091387
Iteration 10500: Loss = -10994.226594053545
1
Iteration 10600: Loss = -10994.291049780384
2
Iteration 10700: Loss = -10994.224002288362
Iteration 10800: Loss = -10994.226032712308
1
Iteration 10900: Loss = -10994.283728346541
2
Iteration 11000: Loss = -10994.228959297778
3
Iteration 11100: Loss = -10994.223992957059
Iteration 11200: Loss = -10994.261076756293
1
Iteration 11300: Loss = -10994.223533207889
Iteration 11400: Loss = -10994.227301262426
1
Iteration 11500: Loss = -10994.223154595189
Iteration 11600: Loss = -10994.242434718944
1
Iteration 11700: Loss = -10994.223005130712
Iteration 11800: Loss = -10994.22303472048
1
Iteration 11900: Loss = -10994.222896690731
Iteration 12000: Loss = -10994.222922748704
1
Iteration 12100: Loss = -10994.223845825585
2
Iteration 12200: Loss = -10994.223302548009
3
Iteration 12300: Loss = -10994.242572951394
4
Iteration 12400: Loss = -10994.222744505794
Iteration 12500: Loss = -10994.230054938935
1
Iteration 12600: Loss = -10994.22280593053
2
Iteration 12700: Loss = -10994.222656241602
Iteration 12800: Loss = -10994.246499794373
1
Iteration 12900: Loss = -10994.222604487513
Iteration 13000: Loss = -10994.225543585271
1
Iteration 13100: Loss = -10994.222535540912
Iteration 13200: Loss = -10994.238127185223
1
Iteration 13300: Loss = -10994.353407608605
2
Iteration 13400: Loss = -10994.222467472455
Iteration 13500: Loss = -10994.222804193874
1
Iteration 13600: Loss = -10994.222740370855
2
Iteration 13700: Loss = -10994.222456275744
Iteration 13800: Loss = -10994.239419836713
1
Iteration 13900: Loss = -10994.242802014549
2
Iteration 14000: Loss = -10994.222392656873
Iteration 14100: Loss = -10994.222906324538
1
Iteration 14200: Loss = -10994.30767149838
2
Iteration 14300: Loss = -10994.22230885688
Iteration 14400: Loss = -10994.223864082356
1
Iteration 14500: Loss = -10994.22267741527
2
Iteration 14600: Loss = -10994.222312183916
3
Iteration 14700: Loss = -10994.230000793328
4
Iteration 14800: Loss = -10994.222297165827
Iteration 14900: Loss = -10994.233357745208
1
Iteration 15000: Loss = -10994.222317048427
2
Iteration 15100: Loss = -10994.222533806504
3
Iteration 15200: Loss = -10994.222297845283
4
Iteration 15300: Loss = -10994.222401693632
5
Iteration 15400: Loss = -10994.222526188581
6
Iteration 15500: Loss = -10994.222331179602
7
Iteration 15600: Loss = -10994.22315675911
8
Iteration 15700: Loss = -10994.222215697213
Iteration 15800: Loss = -10994.2227649452
1
Iteration 15900: Loss = -10994.224427735704
2
Iteration 16000: Loss = -10994.222233986147
3
Iteration 16100: Loss = -10994.30034391657
4
Iteration 16200: Loss = -10994.288058235317
5
Iteration 16300: Loss = -10994.22221786628
6
Iteration 16400: Loss = -10994.222725329537
7
Iteration 16500: Loss = -10994.255228376012
8
Iteration 16600: Loss = -10994.225497203694
9
Iteration 16700: Loss = -10994.22226827054
10
Stopping early at iteration 16700 due to no improvement.
tensor([[ 2.2122, -3.6174],
        [ 3.9755, -5.7516],
        [ 4.4063, -6.0952],
        [ 1.9283, -3.8212],
        [ 2.8750, -4.4981],
        [ 2.2731, -5.3777],
        [ 0.2719, -2.3682],
        [ 3.2550, -5.4560],
        [ 2.5156, -4.0290],
        [ 1.6885, -4.9604],
        [ 3.6195, -5.0410],
        [ 2.6425, -4.7043],
        [ 3.8910, -5.5417],
        [ 1.2895, -2.8651],
        [ 6.6992, -8.2952],
        [ 1.3650, -3.1528],
        [ 2.2537, -3.6435],
        [ 2.4947, -4.7128],
        [ 4.0822, -5.7045],
        [ 3.3679, -4.7563],
        [ 2.7904, -4.5095],
        [ 3.5447, -5.1106],
        [ 2.1981, -3.5846],
        [ 3.3616, -4.7787],
        [ 2.0120, -5.2506],
        [-0.7485, -2.0151],
        [ 0.6722, -2.4851],
        [ 6.1516, -8.4800],
        [ 0.0419, -2.1288],
        [-0.7113, -1.6123],
        [ 2.7994, -4.4696],
        [ 5.2990, -7.0143],
        [-2.2228,  0.7918],
        [ 2.5817, -4.6173],
        [ 6.2538, -8.5147],
        [ 2.1800, -5.2840],
        [-0.3240, -1.0789],
        [ 1.2984, -2.6892],
        [ 0.9525, -3.1690],
        [ 7.0240, -8.4445],
        [ 0.0245, -1.4111],
        [-0.3150, -3.3981],
        [ 4.8892, -6.4652],
        [ 3.2463, -4.6660],
        [ 2.6754, -4.0623],
        [ 6.5238, -9.0569],
        [ 3.1503, -4.7510],
        [ 2.1671, -4.1269],
        [ 2.0002, -3.3961],
        [ 0.9310, -2.3421],
        [ 0.5464, -1.9521],
        [ 3.0533, -5.0896],
        [ 0.8513, -2.2758],
        [-3.6496,  1.9573],
        [ 2.4223, -6.1002],
        [ 3.3721, -4.8638],
        [ 4.1375, -5.8024],
        [ 2.3779, -4.7777],
        [ 2.6378, -7.2531],
        [-2.6091,  0.6730],
        [ 1.7678, -3.1830],
        [ 0.7352, -2.2039],
        [ 2.4547, -3.8451],
        [ 3.6320, -5.0915],
        [ 0.8775, -2.9544],
        [-1.3093, -0.4651],
        [ 1.5965, -4.4057],
        [ 6.9937, -8.4009],
        [-1.7724,  0.1742],
        [ 3.3207, -5.0707],
        [ 6.5433, -8.0398],
        [ 1.8611, -3.8534],
        [ 1.5919, -3.1296],
        [ 3.1574, -5.0734],
        [ 0.7625, -2.3244],
        [ 1.5747, -4.3265],
        [ 1.6141, -3.3208],
        [ 2.3153, -3.7265],
        [ 2.7282, -5.0289],
        [ 6.4659, -8.4548],
        [ 1.4574, -2.9332],
        [ 4.0432, -5.4892],
        [ 2.7204, -5.2385],
        [ 0.8171, -5.3231],
        [ 6.6422, -8.2831],
        [ 1.4425, -3.0293],
        [ 3.3144, -5.2654],
        [ 0.8981, -3.8277],
        [ 5.0137, -6.5123],
        [ 2.7119, -4.1710],
        [ 2.2230, -3.9833],
        [ 6.9454, -8.3318],
        [ 3.7401, -5.1267],
        [ 3.1834, -4.9875],
        [ 0.9697, -2.7087],
        [ 1.4396, -3.6273],
        [ 0.3098, -2.1845],
        [ 0.5756, -2.3542],
        [ 2.2880, -3.9087],
        [ 1.2461, -2.6489]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 3.2406e-07],
        [2.5122e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9363, 0.0637], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1592, 0.2018],
         [0.1926, 0.2270]],

        [[0.7210, 0.1770],
         [0.3165, 0.2820]],

        [[0.4974, 0.2415],
         [0.4423, 0.7532]],

        [[0.6796, 0.1987],
         [0.0546, 0.9783]],

        [[0.9255, 0.1233],
         [0.0157, 0.1288]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.004166919759460609
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.01644068827141728
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.022328462449853457
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.004851708247904022
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0015221131464426677
Global Adjusted Rand Index: -0.0008552510891647506
Average Adjusted Rand Index: -0.001618935162664451
10967.10133548985
new:  [-0.0007489257970643992, 0.788120058370458, -0.0008552510891647506, -0.0008552510891647506] [-0.001175382953740917, 0.7887478727370115, -0.001618935162664451, -0.001618935162664451] [10998.29557458841, 10913.47737762734, 10994.895924030227, 10994.22226827054]
prior:  [0.025192560138998486, -0.001337111052208804, -0.001337111052208804, -0.0012836029363061082] [0.032841520886767464, -0.004997688552185113, -0.004997688552185113, -0.004544741990242261] [11000.404166545924, 10999.781839189527, 10999.781845193118, 10999.78182198647]
-----------------------------------------------------------------------------------------
This iteration is 2
True Objective function: Loss = -10817.83581310901
Iteration 0: Loss = -15493.5726956668
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.9121,    nan]],

        [[0.3086,    nan],
         [0.5559, 0.3838]],

        [[0.1887,    nan],
         [0.6501, 0.3881]],

        [[0.3971,    nan],
         [0.3545, 0.5976]],

        [[0.1708,    nan],
         [0.4953, 0.6216]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16187.393778870171
Iteration 100: Loss = -10843.373101291238
Iteration 200: Loss = -10841.74720313895
Iteration 300: Loss = -10841.144565198601
Iteration 400: Loss = -10840.851341155889
Iteration 500: Loss = -10840.685662770273
Iteration 600: Loss = -10840.582835309446
Iteration 700: Loss = -10840.514695739932
Iteration 800: Loss = -10840.467398826167
Iteration 900: Loss = -10840.433370244691
Iteration 1000: Loss = -10840.408051012033
Iteration 1100: Loss = -10840.388731484867
Iteration 1200: Loss = -10840.373704108488
Iteration 1300: Loss = -10840.361689201885
Iteration 1400: Loss = -10840.35196022376
Iteration 1500: Loss = -10840.343976920163
Iteration 1600: Loss = -10840.337413964258
Iteration 1700: Loss = -10840.331772634501
Iteration 1800: Loss = -10840.327072358392
Iteration 1900: Loss = -10840.32300901995
Iteration 2000: Loss = -10840.319491672575
Iteration 2100: Loss = -10840.316426216536
Iteration 2200: Loss = -10840.313756410927
Iteration 2300: Loss = -10840.311365772333
Iteration 2400: Loss = -10840.309236446119
Iteration 2500: Loss = -10840.307340021685
Iteration 2600: Loss = -10840.305602811975
Iteration 2700: Loss = -10840.304018316227
Iteration 2800: Loss = -10840.30256050861
Iteration 2900: Loss = -10840.30123226949
Iteration 3000: Loss = -10840.300015860823
Iteration 3100: Loss = -10840.298872032849
Iteration 3200: Loss = -10840.297816810098
Iteration 3300: Loss = -10840.296858787284
Iteration 3400: Loss = -10840.295945107539
Iteration 3500: Loss = -10840.295032380782
Iteration 3600: Loss = -10840.294288891782
Iteration 3700: Loss = -10840.293489794636
Iteration 3800: Loss = -10840.292759983913
Iteration 3900: Loss = -10840.29208955993
Iteration 4000: Loss = -10840.291453315314
Iteration 4100: Loss = -10840.29084661247
Iteration 4200: Loss = -10840.290248100368
Iteration 4300: Loss = -10840.289699589413
Iteration 4400: Loss = -10840.289146871672
Iteration 4500: Loss = -10840.288694846278
Iteration 4600: Loss = -10840.288236011405
Iteration 4700: Loss = -10840.287780904222
Iteration 4800: Loss = -10840.28731047313
Iteration 4900: Loss = -10840.286908767403
Iteration 5000: Loss = -10840.28657175529
Iteration 5100: Loss = -10840.286180619447
Iteration 5200: Loss = -10840.285812048036
Iteration 5300: Loss = -10840.285590871312
Iteration 5400: Loss = -10840.285188713155
Iteration 5500: Loss = -10840.28485950828
Iteration 5600: Loss = -10840.284571170907
Iteration 5700: Loss = -10840.284287354794
Iteration 5800: Loss = -10840.284019532388
Iteration 5900: Loss = -10840.283756749694
Iteration 6000: Loss = -10840.283633154666
Iteration 6100: Loss = -10840.283265021824
Iteration 6200: Loss = -10840.283044419459
Iteration 6300: Loss = -10840.471131484719
1
Iteration 6400: Loss = -10840.282636560272
Iteration 6500: Loss = -10840.28244101902
Iteration 6600: Loss = -10840.282248622245
Iteration 6700: Loss = -10840.569152417022
1
Iteration 6800: Loss = -10840.281838231911
Iteration 6900: Loss = -10840.281699727668
Iteration 7000: Loss = -10840.281496769845
Iteration 7100: Loss = -10840.296596010463
1
Iteration 7200: Loss = -10840.281160178885
Iteration 7300: Loss = -10840.28102705135
Iteration 7400: Loss = -10840.280835401902
Iteration 7500: Loss = -10840.2813892914
1
Iteration 7600: Loss = -10840.280503066117
Iteration 7700: Loss = -10840.28034292854
Iteration 7800: Loss = -10840.280147330548
Iteration 7900: Loss = -10840.280071389307
Iteration 8000: Loss = -10840.279752197002
Iteration 8100: Loss = -10840.279533527391
Iteration 8200: Loss = -10840.279296555573
Iteration 8300: Loss = -10840.279066914509
Iteration 8400: Loss = -10840.278637327547
Iteration 8500: Loss = -10840.27814381164
Iteration 8600: Loss = -10840.335220159519
1
Iteration 8700: Loss = -10840.276879260424
Iteration 8800: Loss = -10840.275721487362
Iteration 8900: Loss = -10840.273892650997
Iteration 9000: Loss = -10840.271340287412
Iteration 9100: Loss = -10840.265898954302
Iteration 9200: Loss = -10840.254860494853
Iteration 9300: Loss = -10840.25737529609
1
Iteration 9400: Loss = -10840.23489340067
Iteration 9500: Loss = -10840.235859573952
1
Iteration 9600: Loss = -10840.208667091805
Iteration 9700: Loss = -10840.205256069472
Iteration 9800: Loss = -10840.199833221535
Iteration 9900: Loss = -10840.191332562847
Iteration 10000: Loss = -10840.208654007363
1
Iteration 10100: Loss = -10840.186229782108
Iteration 10200: Loss = -10840.183370191906
Iteration 10300: Loss = -10840.176560372991
Iteration 10400: Loss = -10840.174471314756
Iteration 10500: Loss = -10840.17327339386
Iteration 10600: Loss = -10840.17247310215
Iteration 10700: Loss = -10840.175009574288
1
Iteration 10800: Loss = -10840.199769227907
2
Iteration 10900: Loss = -10840.198404841238
3
Iteration 11000: Loss = -10840.171870173528
Iteration 11100: Loss = -10840.177450360685
1
Iteration 11200: Loss = -10840.196715569042
2
Iteration 11300: Loss = -10840.213256344425
3
Iteration 11400: Loss = -10840.168656067826
Iteration 11500: Loss = -10840.168593127133
Iteration 11600: Loss = -10840.170822737184
1
Iteration 11700: Loss = -10840.172155789332
2
Iteration 11800: Loss = -10840.168368379109
Iteration 11900: Loss = -10840.170501922785
1
Iteration 12000: Loss = -10840.165569718973
Iteration 12100: Loss = -10840.148852017686
Iteration 12200: Loss = -10840.150976215447
1
Iteration 12300: Loss = -10840.137806456558
Iteration 12400: Loss = -10840.130822386567
Iteration 12500: Loss = -10840.13241439913
1
Iteration 12600: Loss = -10840.122051669048
Iteration 12700: Loss = -10840.163963792433
1
Iteration 12800: Loss = -10840.11655055067
Iteration 12900: Loss = -10840.11702530351
1
Iteration 13000: Loss = -10840.114919167063
Iteration 13100: Loss = -10840.144120837536
1
Iteration 13200: Loss = -10840.154182424094
2
Iteration 13300: Loss = -10840.111257820654
Iteration 13400: Loss = -10840.16937038661
1
Iteration 13500: Loss = -10840.030909799001
Iteration 13600: Loss = -10840.0111894656
Iteration 13700: Loss = -10840.017594119461
1
Iteration 13800: Loss = -10840.011175473954
Iteration 13900: Loss = -10840.014837432256
1
Iteration 14000: Loss = -10840.01227282516
2
Iteration 14100: Loss = -10840.011539847063
3
Iteration 14200: Loss = -10840.304389508085
4
Iteration 14300: Loss = -10840.017067072764
5
Iteration 14400: Loss = -10840.113159677949
6
Iteration 14500: Loss = -10840.011116723446
Iteration 14600: Loss = -10840.024415311573
1
Iteration 14700: Loss = -10840.0250817615
2
Iteration 14800: Loss = -10840.011047294935
Iteration 14900: Loss = -10840.014954172553
1
Iteration 15000: Loss = -10840.011083511827
2
Iteration 15100: Loss = -10840.01374266689
3
Iteration 15200: Loss = -10840.011106242466
4
Iteration 15300: Loss = -10840.013377565496
5
Iteration 15400: Loss = -10840.011093490395
6
Iteration 15500: Loss = -10840.011073176473
7
Iteration 15600: Loss = -10840.012042542674
8
Iteration 15700: Loss = -10840.018920638917
9
Iteration 15800: Loss = -10840.012369638469
10
Stopping early at iteration 15800 due to no improvement.
tensor([[-7.2857e-02, -4.5424e+00],
        [-3.2801e-02, -4.5824e+00],
        [ 1.4321e-02, -4.6295e+00],
        [-3.9213e-02, -4.5760e+00],
        [-5.1391e-02, -4.5638e+00],
        [ 2.3198e-02, -4.6384e+00],
        [ 4.9662e-02, -4.6649e+00],
        [ 5.9370e-03, -4.6212e+00],
        [-4.6563e-02, -4.5687e+00],
        [-2.1890e-02, -4.5933e+00],
        [-4.8562e-02, -4.5667e+00],
        [-2.3398e-02, -4.5918e+00],
        [-6.1510e-02, -4.5537e+00],
        [-1.6908e-02, -4.5983e+00],
        [-2.5478e-02, -4.5897e+00],
        [-5.5210e-02, -4.5600e+00],
        [ 8.8271e-03, -4.6240e+00],
        [-8.1640e-04, -4.6144e+00],
        [ 3.1983e-02, -4.6472e+00],
        [-1.1298e-02, -4.6039e+00],
        [-2.4321e-02, -4.5909e+00],
        [-1.5552e-02, -4.5997e+00],
        [-3.7619e-02, -4.5776e+00],
        [-1.5591e-02, -4.5996e+00],
        [-8.6433e-03, -4.6066e+00],
        [ 1.3330e-02, -4.6286e+00],
        [-3.2730e-02, -4.5825e+00],
        [-4.5199e-02, -4.5700e+00],
        [-1.4058e-02, -4.6012e+00],
        [-2.3941e-02, -4.5913e+00],
        [-3.0630e-02, -4.5846e+00],
        [-2.8855e-02, -4.5864e+00],
        [-5.7677e-02, -4.5575e+00],
        [ 2.8069e-02, -4.6433e+00],
        [-4.8424e-02, -4.5668e+00],
        [ 3.3737e-02, -4.6490e+00],
        [ 7.2894e-02, -4.6881e+00],
        [ 4.7817e-03, -4.6200e+00],
        [-5.6043e-02, -4.5592e+00],
        [-6.2927e-03, -4.6089e+00],
        [-3.0259e-02, -4.5850e+00],
        [-2.7831e-02, -4.5874e+00],
        [-2.2389e-02, -4.5928e+00],
        [-4.9453e-03, -4.6103e+00],
        [-4.3968e-02, -4.5713e+00],
        [ 2.2188e-03, -4.6174e+00],
        [-1.3258e-02, -4.6020e+00],
        [ 1.8406e-02, -4.6336e+00],
        [ 2.1469e-02, -4.6367e+00],
        [-1.2747e-02, -4.6025e+00],
        [ 3.0507e-02, -4.6457e+00],
        [ 1.4410e-02, -4.6296e+00],
        [-3.1573e-02, -4.5836e+00],
        [ 4.2218e-03, -4.6194e+00],
        [-7.9451e-03, -4.6073e+00],
        [-1.3866e-02, -4.6014e+00],
        [-2.7379e-02, -4.5878e+00],
        [ 2.3383e-02, -4.6386e+00],
        [-2.7576e-02, -4.5876e+00],
        [-5.8874e-02, -4.5563e+00],
        [ 1.4416e-02, -4.6296e+00],
        [-3.3589e-03, -4.6119e+00],
        [ 1.6053e-02, -4.6313e+00],
        [-5.8279e-03, -4.6094e+00],
        [-4.5018e-02, -4.5702e+00],
        [ 5.7760e-03, -4.6210e+00],
        [-1.4214e-02, -4.6010e+00],
        [-1.3890e-02, -4.6013e+00],
        [ 1.3728e-02, -4.6289e+00],
        [-5.1232e-02, -4.5640e+00],
        [-5.5066e-03, -4.6097e+00],
        [-2.9865e-02, -4.5854e+00],
        [-2.6734e-02, -4.5885e+00],
        [-4.9077e-02, -4.5661e+00],
        [-3.7652e-02, -4.5776e+00],
        [-6.9522e-02, -4.5457e+00],
        [-2.3216e-02, -4.5920e+00],
        [ 1.0218e-02, -4.6254e+00],
        [-2.6456e-02, -4.5888e+00],
        [-2.1551e-02, -4.5937e+00],
        [ 5.4670e-03, -4.6207e+00],
        [ 3.0647e-02, -4.6459e+00],
        [-4.4880e-02, -4.5703e+00],
        [-4.1785e-02, -4.5734e+00],
        [ 3.9506e-02, -4.6547e+00],
        [-7.4380e-02, -4.5408e+00],
        [-5.8726e-03, -4.6093e+00],
        [-7.7634e-02, -4.5376e+00],
        [-2.9378e-02, -4.5858e+00],
        [-1.1735e-03, -4.6140e+00],
        [-3.9276e-02, -4.5759e+00],
        [-1.5058e-02, -4.6002e+00],
        [-1.2630e-02, -4.6026e+00],
        [-1.1541e-02, -4.6037e+00],
        [ 1.7917e-01, -4.7944e+00],
        [-2.9185e-02, -4.5860e+00],
        [-3.7042e-02, -4.5782e+00],
        [ 2.0169e-02, -4.6354e+00],
        [ 1.4722e-02, -4.6299e+00],
        [ 9.7239e-02, -4.7125e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[2.8364e-01, 7.1636e-01],
        [1.8583e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9899, 0.0101], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1605, 0.1584],
         [0.9121, 0.1581]],

        [[0.3086, 0.1626],
         [0.5559, 0.3838]],

        [[0.1887, 0.1626],
         [0.6501, 0.3881]],

        [[0.3971, 0.1702],
         [0.3545, 0.5976]],

        [[0.1708, 0.0759],
         [0.4953, 0.6216]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0032200952747109884
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -19989.48003553654
Iteration 10: Loss = -10840.605181910938
Iteration 20: Loss = -10840.254704958561
Iteration 30: Loss = -10840.176376203544
Iteration 40: Loss = -10840.15207350494
Iteration 50: Loss = -10840.143991975143
Iteration 60: Loss = -10840.141216419
Iteration 70: Loss = -10840.140191346078
Iteration 80: Loss = -10840.13985468033
Iteration 90: Loss = -10840.139674647318
Iteration 100: Loss = -10840.139600724167
Iteration 110: Loss = -10840.139576835296
Iteration 120: Loss = -10840.139588537822
1
Iteration 130: Loss = -10840.1395833389
2
Iteration 140: Loss = -10840.139597415327
3
Stopping early at iteration 139 due to no improvement.
pi: tensor([[0.1943, 0.8057],
        [0.2143, 0.7857]], dtype=torch.float64)
alpha: tensor([0.2101, 0.7899])
beta: tensor([[[0.1543, 0.1582],
         [0.8089, 0.1582]],

        [[0.7010, 0.1602],
         [0.3077, 0.3833]],

        [[0.0392, 0.1599],
         [0.4387, 0.1011]],

        [[0.6907, 0.1489],
         [0.0233, 0.4433]],

        [[0.0824, 0.1542],
         [0.5085, 0.7901]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19989.172343961454
Iteration 100: Loss = -10965.537233102074
Iteration 200: Loss = -10913.29786046829
Iteration 300: Loss = -10890.56461055186
Iteration 400: Loss = -10878.87573907088
Iteration 500: Loss = -10868.724753181325
Iteration 600: Loss = -10862.730591478416
Iteration 700: Loss = -10859.072846594792
Iteration 800: Loss = -10855.834410187375
Iteration 900: Loss = -10849.5323698929
Iteration 1000: Loss = -10848.52076578308
Iteration 1100: Loss = -10848.312652129829
Iteration 1200: Loss = -10848.190881504845
Iteration 1300: Loss = -10847.177334331942
Iteration 1400: Loss = -10845.90934937321
Iteration 1500: Loss = -10844.74747009284
Iteration 1600: Loss = -10843.059041963126
Iteration 1700: Loss = -10842.499240061186
Iteration 1800: Loss = -10842.269245367219
Iteration 1900: Loss = -10842.245503026183
Iteration 2000: Loss = -10842.227801071522
Iteration 2100: Loss = -10842.212908039315
Iteration 2200: Loss = -10842.198736948094
Iteration 2300: Loss = -10842.184569219675
Iteration 2400: Loss = -10842.170553655635
Iteration 2500: Loss = -10842.160620149598
Iteration 2600: Loss = -10842.151113451737
Iteration 2700: Loss = -10842.141879758143
Iteration 2800: Loss = -10842.133605305498
Iteration 2900: Loss = -10842.126643012907
Iteration 3000: Loss = -10842.121108109843
Iteration 3100: Loss = -10842.116035111576
Iteration 3200: Loss = -10842.108124051696
Iteration 3300: Loss = -10840.063373915644
Iteration 3400: Loss = -10840.043133817784
Iteration 3500: Loss = -10839.72482191099
Iteration 3600: Loss = -10839.697262902175
Iteration 3700: Loss = -10839.694837998437
Iteration 3800: Loss = -10839.69611615586
1
Iteration 3900: Loss = -10839.690650415721
Iteration 4000: Loss = -10839.688758966357
Iteration 4100: Loss = -10839.687008248276
Iteration 4200: Loss = -10839.685399690796
Iteration 4300: Loss = -10839.683900782906
Iteration 4400: Loss = -10839.68254661
Iteration 4500: Loss = -10839.681177888362
Iteration 4600: Loss = -10839.687908290494
1
Iteration 4700: Loss = -10839.67887226296
Iteration 4800: Loss = -10839.677861535949
Iteration 4900: Loss = -10839.677451184873
Iteration 5000: Loss = -10839.67615264673
Iteration 5100: Loss = -10839.675215243695
Iteration 5200: Loss = -10839.67444165005
Iteration 5300: Loss = -10839.676118148383
1
Iteration 5400: Loss = -10839.673285200419
Iteration 5500: Loss = -10839.672780399056
Iteration 5600: Loss = -10839.671860063767
Iteration 5700: Loss = -10839.671364340287
Iteration 5800: Loss = -10839.670700622086
Iteration 5900: Loss = -10839.671000364779
1
Iteration 6000: Loss = -10839.777352517804
2
Iteration 6100: Loss = -10839.669254523553
Iteration 6200: Loss = -10839.668864831845
Iteration 6300: Loss = -10839.674302301451
1
Iteration 6400: Loss = -10839.668062729555
Iteration 6500: Loss = -10839.672919085266
1
Iteration 6600: Loss = -10839.667326241279
Iteration 6700: Loss = -10839.745261415792
1
Iteration 6800: Loss = -10839.666652879698
Iteration 6900: Loss = -10839.794927612926
1
Iteration 7000: Loss = -10839.66612196546
Iteration 7100: Loss = -10839.666269118687
1
Iteration 7200: Loss = -10839.665725289044
Iteration 7300: Loss = -10839.666223672699
1
Iteration 7400: Loss = -10839.665432166054
Iteration 7500: Loss = -10839.664996257912
Iteration 7600: Loss = -10839.667159811406
1
Iteration 7700: Loss = -10839.66465409692
Iteration 7800: Loss = -10839.677975845108
1
Iteration 7900: Loss = -10839.664305373308
Iteration 8000: Loss = -10839.66421062398
Iteration 8100: Loss = -10839.669163113464
1
Iteration 8200: Loss = -10839.663882854817
Iteration 8300: Loss = -10839.664366794612
1
Iteration 8400: Loss = -10839.686537245607
2
Iteration 8500: Loss = -10839.663867800875
Iteration 8600: Loss = -10839.693734605193
1
Iteration 8700: Loss = -10839.663307835806
Iteration 8800: Loss = -10839.66575501934
1
Iteration 8900: Loss = -10839.668861230535
2
Iteration 9000: Loss = -10839.671197357116
3
Iteration 9100: Loss = -10839.692547263117
4
Iteration 9200: Loss = -10839.662872942874
Iteration 9300: Loss = -10839.668514427589
1
Iteration 9400: Loss = -10839.663442493029
2
Iteration 9500: Loss = -10839.662712319314
Iteration 9600: Loss = -10839.671104405723
1
Iteration 9700: Loss = -10839.662467345452
Iteration 9800: Loss = -10839.662743013658
1
Iteration 9900: Loss = -10839.664817808889
2
Iteration 10000: Loss = -10839.663117943855
3
Iteration 10100: Loss = -10839.730402485791
4
Iteration 10200: Loss = -10839.668004715857
5
Iteration 10300: Loss = -10839.663796506899
6
Iteration 10400: Loss = -10839.729579087727
7
Iteration 10500: Loss = -10839.692422773374
8
Iteration 10600: Loss = -10839.663366846742
9
Iteration 10700: Loss = -10839.664880594877
10
Stopping early at iteration 10700 due to no improvement.
tensor([[ -8.4375,   6.4072],
        [ -8.4960,   7.0057],
        [ -8.5012,   7.0576],
        [ -8.6230,   5.4139],
        [ -8.0953,   5.4315],
        [ -8.4434,   6.9565],
        [ -8.5635,   6.4136],
        [-11.2041,   6.5889],
        [ -7.3721,   5.9840],
        [ -8.1217,   6.5992],
        [ -8.1657,   6.3393],
        [ -8.4813,   6.5201],
        [ -8.8281,   7.2078],
        [ -7.7949,   6.0948],
        [ -8.0516,   6.4750],
        [ -8.3940,   6.9505],
        [ -9.2219,   7.0473],
        [ -8.8252,   7.2705],
        [ -8.4789,   6.8383],
        [ -7.0225,   5.4035],
        [ -8.9758,   7.2955],
        [ -8.1043,   5.6627],
        [ -8.7069,   7.2324],
        [ -8.7340,   7.3153],
        [ -7.6891,   6.2216],
        [ -9.5374,   6.1360],
        [ -8.2004,   5.7789],
        [ -8.0076,   6.4405],
        [ -8.8030,   7.1026],
        [ -8.6203,   6.2086],
        [ -8.4766,   7.0890],
        [ -8.6314,   7.0522],
        [ -8.2311,   6.7623],
        [ -8.9009,   7.2591],
        [ -8.2221,   6.8113],
        [ -8.8151,   7.3705],
        [ -8.9307,   7.4681],
        [ -7.5715,   5.6100],
        [ -7.5845,   6.1976],
        [ -9.7460,   7.0012],
        [ -7.0161,   5.6287],
        [ -8.2274,   6.8364],
        [ -8.6871,   6.9592],
        [ -8.8082,   7.0244],
        [ -8.3599,   6.4241],
        [ -7.3839,   5.9006],
        [ -7.7974,   5.9467],
        [ -8.2727,   6.8844],
        [ -9.3025,   7.1069],
        [ -9.0353,   6.3753],
        [ -9.5343,   7.1657],
        [ -7.3837,   5.6932],
        [ -7.6156,   5.8003],
        [ -8.9490,   7.3392],
        [ -8.5046,   7.0796],
        [ -9.0309,   6.7158],
        [ -8.5746,   6.6375],
        [ -9.0468,   7.3909],
        [ -9.1814,   6.2507],
        [ -8.6287,   7.2057],
        [ -8.7968,   6.2104],
        [ -9.0236,   7.5733],
        [ -8.9187,   5.5763],
        [ -8.6500,   7.2432],
        [ -7.8796,   6.1308],
        [ -7.6409,   6.1705],
        [ -8.6196,   6.9894],
        [ -8.9326,   7.4578],
        [ -8.9477,   6.9871],
        [ -8.2177,   6.8104],
        [ -8.9011,   7.0863],
        [ -8.5807,   7.1034],
        [ -9.2809,   7.3876],
        [ -8.0121,   6.6086],
        [ -8.4609,   7.0069],
        [ -8.3780,   5.8933],
        [ -8.5308,   7.0695],
        [ -8.8963,   6.7978],
        [ -8.6423,   6.8371],
        [ -8.7608,   7.2452],
        [ -8.9375,   6.7188],
        [ -9.9340,   5.8300],
        [ -9.0617,   7.4203],
        [ -9.2028,   7.0858],
        [ -8.6178,   7.2081],
        [ -8.1156,   6.7133],
        [ -8.7117,   7.1136],
        [ -8.5551,   6.9601],
        [ -7.8060,   5.9825],
        [ -8.6980,   6.9453],
        [ -9.0677,   6.1889],
        [ -8.9081,   6.4801],
        [ -7.4239,   5.9361],
        [ -8.7569,   6.6317],
        [ -8.6494,   7.0784],
        [ -9.4982,   4.8830],
        [ -8.3154,   6.6275],
        [ -8.3997,   6.9751],
        [ -8.5865,   7.1639],
        [ -7.4532,   5.9884]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9999e-01, 5.4730e-06],
        [6.5376e-01, 3.4624e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([4.8215e-07, 1.0000e+00], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1544, 0.1579],
         [0.8089, 0.1614]],

        [[0.7010, 0.1632],
         [0.3077, 0.3833]],

        [[0.0392, 0.1712],
         [0.4387, 0.1011]],

        [[0.6907, 0.2137],
         [0.0233, 0.4433]],

        [[0.0824, 0.2324],
         [0.5085, 0.7901]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.006127506195088984
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.01171303074670571
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 59
Adjusted Rand Index: -0.006658343736995423
Global Adjusted Rand Index: -0.00040280206055477003
Average Adjusted Rand Index: 0.0027249569557852493
Iteration 0: Loss = -21636.076367094785
Iteration 10: Loss = -10840.514926936177
Iteration 20: Loss = -10840.514926936183
1
Iteration 30: Loss = -10840.514926936197
2
Iteration 40: Loss = -10840.514926936235
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[6.7784e-16, 1.0000e+00],
        [3.1593e-15, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([3.0959e-15, 1.0000e+00])
beta: tensor([[[0.1761, 0.1646],
         [0.4251, 0.1574]],

        [[0.0178, 0.1865],
         [0.4305, 0.4046]],

        [[0.1977, 0.2354],
         [0.9097, 0.1814]],

        [[0.1521, 0.1053],
         [0.6744, 0.7123]],

        [[0.6732, 0.1290],
         [0.0459, 0.8342]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21635.487457332794
Iteration 100: Loss = -10848.45374245429
Iteration 200: Loss = -10842.742302973698
Iteration 300: Loss = -10841.57446066689
Iteration 400: Loss = -10841.08196516611
Iteration 500: Loss = -10840.809172290577
Iteration 600: Loss = -10840.65799433714
Iteration 700: Loss = -10840.561085433392
Iteration 800: Loss = -10840.492135737119
Iteration 900: Loss = -10840.438747243565
Iteration 1000: Loss = -10840.39731085378
Iteration 1100: Loss = -10840.363474494898
Iteration 1200: Loss = -10840.334787915232
Iteration 1300: Loss = -10840.309904301404
Iteration 1400: Loss = -10840.288014274132
Iteration 1500: Loss = -10840.268285959235
Iteration 1600: Loss = -10840.250309605168
Iteration 1700: Loss = -10840.233647036444
Iteration 1800: Loss = -10840.218009344726
Iteration 1900: Loss = -10840.203251931052
Iteration 2000: Loss = -10840.189250368807
Iteration 2100: Loss = -10840.176243130732
Iteration 2200: Loss = -10840.163889174724
Iteration 2300: Loss = -10840.152453216499
Iteration 2400: Loss = -10840.14210028869
Iteration 2500: Loss = -10840.132801967211
Iteration 2600: Loss = -10840.124245926081
Iteration 2700: Loss = -10840.116130249386
Iteration 2800: Loss = -10840.1084045262
Iteration 2900: Loss = -10840.10107176516
Iteration 3000: Loss = -10840.094098637743
Iteration 3100: Loss = -10840.087412447781
Iteration 3200: Loss = -10840.081011477056
Iteration 3300: Loss = -10840.07478365924
Iteration 3400: Loss = -10840.068872316202
Iteration 3500: Loss = -10840.063748123475
Iteration 3600: Loss = -10840.057584908676
Iteration 3700: Loss = -10840.052140852984
Iteration 3800: Loss = -10840.093806531837
1
Iteration 3900: Loss = -10840.0414326163
Iteration 4000: Loss = -10840.036168543082
Iteration 4100: Loss = -10840.030821034035
Iteration 4200: Loss = -10840.026890389112
Iteration 4300: Loss = -10840.019896968419
Iteration 4400: Loss = -10840.014301008605
Iteration 4500: Loss = -10840.008537253054
Iteration 4600: Loss = -10840.00257006312
Iteration 4700: Loss = -10839.996125717022
Iteration 4800: Loss = -10839.989122858298
Iteration 4900: Loss = -10839.983907880393
Iteration 5000: Loss = -10839.973062914318
Iteration 5100: Loss = -10839.963809003346
Iteration 5200: Loss = -10839.955586275524
Iteration 5300: Loss = -10839.943023746735
Iteration 5400: Loss = -10839.931981420226
Iteration 5500: Loss = -10839.92081745714
Iteration 5600: Loss = -10839.915359371827
Iteration 5700: Loss = -10839.899809187591
Iteration 5800: Loss = -10839.891299045019
Iteration 5900: Loss = -10839.884466413077
Iteration 6000: Loss = -10839.878972835917
Iteration 6100: Loss = -10839.875052057983
Iteration 6200: Loss = -10839.872121409828
Iteration 6300: Loss = -10839.870773633033
Iteration 6400: Loss = -10839.868350146135
Iteration 6500: Loss = -10839.867254436045
Iteration 6600: Loss = -10839.86646353247
Iteration 6700: Loss = -10839.866053018162
Iteration 6800: Loss = -10839.865389928991
Iteration 6900: Loss = -10839.867057519888
1
Iteration 7000: Loss = -10839.875568236434
2
Iteration 7100: Loss = -10839.883818903043
3
Iteration 7200: Loss = -10839.86421780532
Iteration 7300: Loss = -10839.867533289402
1
Iteration 7400: Loss = -10839.863745113213
Iteration 7500: Loss = -10839.873076237109
1
Iteration 7600: Loss = -10839.86326822936
Iteration 7700: Loss = -10839.86370663806
1
Iteration 7800: Loss = -10839.862852028391
Iteration 7900: Loss = -10839.862600240802
Iteration 8000: Loss = -10839.862413436014
Iteration 8100: Loss = -10839.862281180243
Iteration 8200: Loss = -10839.861930388752
Iteration 8300: Loss = -10839.861696954376
Iteration 8400: Loss = -10839.868001749337
1
Iteration 8500: Loss = -10839.861209557612
Iteration 8600: Loss = -10839.861018876236
Iteration 8700: Loss = -10840.003285709621
1
Iteration 8800: Loss = -10839.860604729945
Iteration 8900: Loss = -10839.860397202798
Iteration 9000: Loss = -10839.8602439531
Iteration 9100: Loss = -10839.860312470719
1
Iteration 9200: Loss = -10839.859837867902
Iteration 9300: Loss = -10839.859687834845
Iteration 9400: Loss = -10839.859542640974
Iteration 9500: Loss = -10839.859352418813
Iteration 9600: Loss = -10839.859181060056
Iteration 9700: Loss = -10839.859161984175
Iteration 9800: Loss = -10839.858883865198
Iteration 9900: Loss = -10839.85874399859
Iteration 10000: Loss = -10839.858683550403
Iteration 10100: Loss = -10839.858501050487
Iteration 10200: Loss = -10839.858364717644
Iteration 10300: Loss = -10839.862302971796
1
Iteration 10400: Loss = -10839.858186275553
Iteration 10500: Loss = -10839.858052229423
Iteration 10600: Loss = -10839.860270442408
1
Iteration 10700: Loss = -10839.902622023325
2
Iteration 10800: Loss = -10839.857824025807
Iteration 10900: Loss = -10840.050996297188
1
Iteration 11000: Loss = -10839.857643904255
Iteration 11100: Loss = -10839.857649483096
1
Iteration 11200: Loss = -10839.85760142139
Iteration 11300: Loss = -10839.857471590316
Iteration 11400: Loss = -10839.857635160923
1
Iteration 11500: Loss = -10839.857349231632
Iteration 11600: Loss = -10839.858289349007
1
Iteration 11700: Loss = -10839.857247810964
Iteration 11800: Loss = -10839.881600878409
1
Iteration 11900: Loss = -10839.867502319023
2
Iteration 12000: Loss = -10839.920994077838
3
Iteration 12100: Loss = -10839.857105836738
Iteration 12200: Loss = -10839.861487183256
1
Iteration 12300: Loss = -10839.857007058528
Iteration 12400: Loss = -10839.857472049052
1
Iteration 12500: Loss = -10839.856926123835
Iteration 12600: Loss = -10839.858096632228
1
Iteration 12700: Loss = -10839.85688869625
Iteration 12800: Loss = -10839.857982573525
1
Iteration 12900: Loss = -10839.856780750782
Iteration 13000: Loss = -10839.873643410916
1
Iteration 13100: Loss = -10839.856782466628
2
Iteration 13200: Loss = -10839.877709025935
3
Iteration 13300: Loss = -10839.856738409475
Iteration 13400: Loss = -10839.856765484898
1
Iteration 13500: Loss = -10839.856777655052
2
Iteration 13600: Loss = -10839.856695323911
Iteration 13700: Loss = -10839.858844322003
1
Iteration 13800: Loss = -10839.877270963721
2
Iteration 13900: Loss = -10839.856782358313
3
Iteration 14000: Loss = -10839.85667305108
Iteration 14100: Loss = -10839.861625245925
1
Iteration 14200: Loss = -10839.974755458841
2
Iteration 14300: Loss = -10839.856608918393
Iteration 14400: Loss = -10839.857618073536
1
Iteration 14500: Loss = -10839.860299241087
2
Iteration 14600: Loss = -10839.856605723106
Iteration 14700: Loss = -10839.856563242003
Iteration 14800: Loss = -10839.857016090802
1
Iteration 14900: Loss = -10839.856548526335
Iteration 15000: Loss = -10839.856762265237
1
Iteration 15100: Loss = -10839.85688483262
2
Iteration 15200: Loss = -10839.856681857436
3
Iteration 15300: Loss = -10839.8564980835
Iteration 15400: Loss = -10839.856680492852
1
Iteration 15500: Loss = -10839.856568493538
2
Iteration 15600: Loss = -10839.856987946008
3
Iteration 15700: Loss = -10839.856484259191
Iteration 15800: Loss = -10839.856662091655
1
Iteration 15900: Loss = -10840.16701954929
2
Iteration 16000: Loss = -10839.856481152645
Iteration 16100: Loss = -10839.870370733228
1
Iteration 16200: Loss = -10839.856691989027
2
Iteration 16300: Loss = -10839.856449926767
Iteration 16400: Loss = -10839.856756506602
1
Iteration 16500: Loss = -10839.856461150579
2
Iteration 16600: Loss = -10839.856918176249
3
Iteration 16700: Loss = -10839.856472061227
4
Iteration 16800: Loss = -10839.863157217218
5
Iteration 16900: Loss = -10839.85641982839
Iteration 17000: Loss = -10839.863866147332
1
Iteration 17100: Loss = -10839.856520415804
2
Iteration 17200: Loss = -10839.86115623191
3
Iteration 17300: Loss = -10839.857173456494
4
Iteration 17400: Loss = -10839.994349559687
5
Iteration 17500: Loss = -10839.856467192056
6
Iteration 17600: Loss = -10839.856429393363
7
Iteration 17700: Loss = -10839.857177764565
8
Iteration 17800: Loss = -10839.856399900531
Iteration 17900: Loss = -10839.926761930436
1
Iteration 18000: Loss = -10839.874404707329
2
Iteration 18100: Loss = -10839.996028357777
3
Iteration 18200: Loss = -10839.856475216391
4
Iteration 18300: Loss = -10839.984421101708
5
Iteration 18400: Loss = -10839.856570898311
6
Iteration 18500: Loss = -10839.856532365136
7
Iteration 18600: Loss = -10840.029980372814
8
Iteration 18700: Loss = -10839.856413356452
9
Iteration 18800: Loss = -10839.879248734662
10
Stopping early at iteration 18800 due to no improvement.
tensor([[-3.9715,  1.1503],
        [-3.6327,  1.5482],
        [-4.9189,  0.3037],
        [-3.2896,  1.8480],
        [-3.2881,  1.8683],
        [-3.3995,  1.8174],
        [-3.3632,  1.8876],
        [-3.5950,  1.6280],
        [-4.3675,  0.7546],
        [-3.6540,  1.5410],
        [-3.2735,  1.8804],
        [-3.2950,  1.8798],
        [-3.3525,  1.7985],
        [-3.2748,  1.8864],
        [-4.1751,  0.9863],
        [-3.3357,  1.8217],
        [-3.3074,  1.9165],
        [-3.6646,  1.5446],
        [-3.3415,  1.9513],
        [-3.3299,  1.8888],
        [-3.2947,  1.8894],
        [-3.6511,  1.5424],
        [-3.3238,  1.8423],
        [-3.3721,  1.8276],
        [-3.6812,  1.4767],
        [-3.8052,  1.4513],
        [-3.5535,  1.6104],
        [-3.4967,  1.6433],
        [-3.3566,  1.8234],
        [-4.0104,  1.1864],
        [-3.3227,  1.8597],
        [-3.7502,  1.4111],
        [-4.1124,  1.0476],
        [-3.3221,  1.9352],
        [-3.3317,  1.7723],
        [-3.3736,  1.8748],
        [-3.3060,  1.8722],
        [-3.3206,  1.8844],
        [-3.2726,  1.8742],
        [-3.2979,  1.9097],
        [-3.3077,  1.8568],
        [-3.4243,  1.7571],
        [-4.8979,  0.2827],
        [-3.2906,  1.8787],
        [-3.3153,  1.8404],
        [-4.0705,  1.1124],
        [-3.2719,  1.8848],
        [-3.7527,  1.5028],
        [-3.3450,  1.9377],
        [-3.7886,  1.3534],
        [-3.8323,  1.4331],
        [-4.2627,  0.9906],
        [-3.2833,  1.8803],
        [-3.3145,  1.9107],
        [-3.3164,  1.9133],
        [-3.3500,  1.8415],
        [-3.3782,  1.7997],
        [-3.4146,  1.8638],
        [-3.2745,  1.8863],
        [-3.6326,  1.5342],
        [-3.9122,  1.3436],
        [-3.6343,  1.6154],
        [-3.2968,  1.9076],
        [-3.3936,  1.8060],
        [-3.2731,  1.8709],
        [-3.3333,  1.9009],
        [-3.5582,  1.6961],
        [-3.3823,  1.8312],
        [-3.3337,  1.9332],
        [-3.4947,  1.6794],
        [-4.6443,  0.5433],
        [-4.1961,  0.9822],
        [-3.6784,  1.5426],
        [-3.7918,  1.3213],
        [-3.7952,  1.3435],
        [-3.5185,  1.6153],
        [-4.6009,  0.6207],
        [-3.9226,  1.3333],
        [-3.7879,  1.4370],
        [-3.6222,  1.5428],
        [-3.4011,  1.8225],
        [-3.6136,  1.6366],
        [-4.1484,  1.0187],
        [-3.3876,  1.7683],
        [-3.8902,  1.3810],
        [-3.5268,  1.5701],
        [-3.7067,  1.5285],
        [-3.3796,  1.7152],
        [-4.0030,  1.1578],
        [-3.5581,  1.6733],
        [-3.5052,  1.6472],
        [-3.3802,  1.8522],
        [-3.5754,  1.5942],
        [-3.8075,  1.4251],
        [-4.9144,  0.2992],
        [-3.3305,  1.8687],
        [-4.0272,  1.1096],
        [-3.6023,  1.5989],
        [-4.1839,  1.0539],
        [-3.3505,  1.9642]], dtype=torch.float64, requires_grad=True)
pi: tensor([[8.9174e-05, 9.9991e-01],
        [7.5296e-02, 9.2470e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0055, 0.9945], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1379, 0.1597],
         [0.4251, 0.1598]],

        [[0.0178, 0.1601],
         [0.4305, 0.4046]],

        [[0.1977, 0.1592],
         [0.9097, 0.1814]],

        [[0.1521, 0.1315],
         [0.6744, 0.7123]],

        [[0.6732, 0.1440],
         [0.0459, 0.8342]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -23424.431920050225
Iteration 10: Loss = -10840.14770249244
Iteration 20: Loss = -10840.14759406206
Iteration 30: Loss = -10840.147591220966
Iteration 40: Loss = -10840.147562238715
Iteration 50: Loss = -10840.147534816142
Iteration 60: Loss = -10840.147552989447
1
Iteration 70: Loss = -10840.147525014083
Iteration 80: Loss = -10840.147546127497
1
Iteration 90: Loss = -10840.147522474783
Iteration 100: Loss = -10840.147451908048
Iteration 110: Loss = -10840.147435721348
Iteration 120: Loss = -10840.14742733885
Iteration 130: Loss = -10840.147403611672
Iteration 140: Loss = -10840.147414239376
1
Iteration 150: Loss = -10840.147438171165
2
Iteration 160: Loss = -10840.147380128543
Iteration 170: Loss = -10840.147353703922
Iteration 180: Loss = -10840.147335541395
Iteration 190: Loss = -10840.147346775108
1
Iteration 200: Loss = -10840.147343789058
2
Iteration 210: Loss = -10840.147367699747
3
Stopping early at iteration 209 due to no improvement.
pi: tensor([[0.5713, 0.4287],
        [0.2655, 0.7345]], dtype=torch.float64)
alpha: tensor([0.3824, 0.6176])
beta: tensor([[[0.1571, 0.1586],
         [0.7534, 0.1576]],

        [[0.2772, 0.1601],
         [0.0902, 0.9309]],

        [[0.2728, 0.1599],
         [0.0037, 0.8128]],

        [[0.4756, 0.1523],
         [0.7405, 0.8480]],

        [[0.0574, 0.1559],
         [0.3382, 0.9332]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23424.26952378532
Iteration 100: Loss = -10971.556256871663
Iteration 200: Loss = -10921.007194411302
Iteration 300: Loss = -10892.347235814525
Iteration 400: Loss = -10871.991414540007
Iteration 500: Loss = -10860.22476893395
Iteration 600: Loss = -10856.941138237624
Iteration 700: Loss = -10855.84129433383
Iteration 800: Loss = -10853.270119593835
Iteration 900: Loss = -10851.116718181745
Iteration 1000: Loss = -10850.792344020143
Iteration 1100: Loss = -10849.126496621457
Iteration 1200: Loss = -10848.193004280092
Iteration 1300: Loss = -10847.51550581711
Iteration 1400: Loss = -10845.865589817213
Iteration 1500: Loss = -10845.78170181256
Iteration 1600: Loss = -10845.635330072446
Iteration 1700: Loss = -10845.530470679103
Iteration 1800: Loss = -10845.460027054964
Iteration 1900: Loss = -10845.323422326033
Iteration 2000: Loss = -10845.019225714595
Iteration 2100: Loss = -10844.636658565987
Iteration 2200: Loss = -10844.590740613747
Iteration 2300: Loss = -10844.56386795139
Iteration 2400: Loss = -10844.536606429643
Iteration 2500: Loss = -10844.328730949826
Iteration 2600: Loss = -10842.69185457481
Iteration 2700: Loss = -10842.606695785624
Iteration 2800: Loss = -10842.558364168155
Iteration 2900: Loss = -10842.54589969988
Iteration 3000: Loss = -10842.534647574723
Iteration 3100: Loss = -10842.522130431416
Iteration 3200: Loss = -10840.331794475083
Iteration 3300: Loss = -10840.269287028863
Iteration 3400: Loss = -10840.253900018582
Iteration 3500: Loss = -10840.245110891765
Iteration 3600: Loss = -10840.238717352953
Iteration 3700: Loss = -10840.233563879172
Iteration 3800: Loss = -10840.229106488157
Iteration 3900: Loss = -10840.22520085245
Iteration 4000: Loss = -10840.221658574976
Iteration 4100: Loss = -10840.218384721586
Iteration 4200: Loss = -10840.215415504426
Iteration 4300: Loss = -10840.212681384093
Iteration 4400: Loss = -10840.210182067558
Iteration 4500: Loss = -10840.207865946231
Iteration 4600: Loss = -10840.205711782686
Iteration 4700: Loss = -10840.203699077665
Iteration 4800: Loss = -10840.203464484302
Iteration 4900: Loss = -10840.199982493848
Iteration 5000: Loss = -10840.407040199065
1
Iteration 5100: Loss = -10840.197006949275
Iteration 5200: Loss = -10840.195118738735
Iteration 5300: Loss = -10840.193894023045
Iteration 5400: Loss = -10840.422350836903
1
Iteration 5500: Loss = -10840.190733936473
Iteration 5600: Loss = -10840.255528287144
1
Iteration 5700: Loss = -10840.193716433998
2
Iteration 5800: Loss = -10840.187037972531
Iteration 5900: Loss = -10840.185508431516
Iteration 6000: Loss = -10840.189570025514
1
Iteration 6100: Loss = -10840.182968886436
Iteration 6200: Loss = -10840.182195113497
Iteration 6300: Loss = -10840.180523694615
Iteration 6400: Loss = -10840.179857237476
Iteration 6500: Loss = -10840.17835028256
Iteration 6600: Loss = -10840.189240604785
1
Iteration 6700: Loss = -10840.176497553783
Iteration 6800: Loss = -10840.175659373695
Iteration 6900: Loss = -10840.176325299968
1
Iteration 7000: Loss = -10840.174260003158
Iteration 7100: Loss = -10840.173648877228
Iteration 7200: Loss = -10840.197999480974
1
Iteration 7300: Loss = -10840.172498385984
Iteration 7400: Loss = -10840.171883768571
Iteration 7500: Loss = -10840.184878738322
1
Iteration 7600: Loss = -10840.169279460246
Iteration 7700: Loss = -10840.157582466554
Iteration 7800: Loss = -10840.153813346484
Iteration 7900: Loss = -10840.153005858781
Iteration 8000: Loss = -10840.160310018926
1
Iteration 8100: Loss = -10840.152009456915
Iteration 8200: Loss = -10840.162841692902
1
Iteration 8300: Loss = -10840.151859152695
Iteration 8400: Loss = -10840.159812885175
1
Iteration 8500: Loss = -10840.213295926666
2
Iteration 8600: Loss = -10840.155769790714
3
Iteration 8700: Loss = -10840.15020032
Iteration 8800: Loss = -10840.150357280683
1
Iteration 8900: Loss = -10840.14985169214
Iteration 9000: Loss = -10840.149846048687
Iteration 9100: Loss = -10840.149586719624
Iteration 9200: Loss = -10840.15120651898
1
Iteration 9300: Loss = -10840.149198166217
Iteration 9400: Loss = -10840.53572233965
1
Iteration 9500: Loss = -10840.148510803223
Iteration 9600: Loss = -10840.148099630838
Iteration 9700: Loss = -10840.16164829909
1
Iteration 9800: Loss = -10840.145964629657
Iteration 9900: Loss = -10840.151125038305
1
Iteration 10000: Loss = -10840.138900032447
Iteration 10100: Loss = -10840.139171446077
1
Iteration 10200: Loss = -10840.13931376473
2
Iteration 10300: Loss = -10840.137544554365
Iteration 10400: Loss = -10840.136024753861
Iteration 10500: Loss = -10840.135690419962
Iteration 10600: Loss = -10840.133191713147
Iteration 10700: Loss = -10840.13364219369
1
Iteration 10800: Loss = -10840.161651742259
2
Iteration 10900: Loss = -10840.139311640933
3
Iteration 11000: Loss = -10840.13116323131
Iteration 11100: Loss = -10840.139647197815
1
Iteration 11200: Loss = -10840.13090433038
Iteration 11300: Loss = -10840.131182703355
1
Iteration 11400: Loss = -10840.130408977542
Iteration 11500: Loss = -10840.144138126145
1
Iteration 11600: Loss = -10840.127884670363
Iteration 11700: Loss = -10840.13062342127
1
Iteration 11800: Loss = -10840.126243837958
Iteration 11900: Loss = -10840.232976000492
1
Iteration 12000: Loss = -10840.12504707498
Iteration 12100: Loss = -10840.210385308534
1
Iteration 12200: Loss = -10840.122657013242
Iteration 12300: Loss = -10840.121600650775
Iteration 12400: Loss = -10840.129440865812
1
Iteration 12500: Loss = -10840.120459328235
Iteration 12600: Loss = -10840.594464924998
1
Iteration 12700: Loss = -10840.116979424422
Iteration 12800: Loss = -10840.117169457759
1
Iteration 12900: Loss = -10840.116375834514
Iteration 13000: Loss = -10840.11800824816
1
Iteration 13100: Loss = -10840.120923014949
2
Iteration 13200: Loss = -10840.11433169443
Iteration 13300: Loss = -10840.112398717774
Iteration 13400: Loss = -10840.112074490808
Iteration 13500: Loss = -10840.119882206483
1
Iteration 13600: Loss = -10840.115739715862
2
Iteration 13700: Loss = -10840.120594980202
3
Iteration 13800: Loss = -10840.131224743649
4
Iteration 13900: Loss = -10840.110721748113
Iteration 14000: Loss = -10840.112066962447
1
Iteration 14100: Loss = -10840.12080580517
2
Iteration 14200: Loss = -10840.01113841807
Iteration 14300: Loss = -10840.013928041333
1
Iteration 14400: Loss = -10840.168746418232
2
Iteration 14500: Loss = -10840.011078727199
Iteration 14600: Loss = -10840.011070538929
Iteration 14700: Loss = -10840.010988953016
Iteration 14800: Loss = -10840.011190632204
1
Iteration 14900: Loss = -10840.01098940359
2
Iteration 15000: Loss = -10840.011224288028
3
Iteration 15100: Loss = -10840.011528379351
4
Iteration 15200: Loss = -10840.012752418008
5
Iteration 15300: Loss = -10840.011192281101
6
Iteration 15400: Loss = -10840.018663992363
7
Iteration 15500: Loss = -10840.011007572171
8
Iteration 15600: Loss = -10840.011266413736
9
Iteration 15700: Loss = -10840.010968764665
Iteration 15800: Loss = -10840.017233710372
1
Iteration 15900: Loss = -10840.010995542601
2
Iteration 16000: Loss = -10840.011377511448
3
Iteration 16100: Loss = -10840.011475103398
4
Iteration 16200: Loss = -10840.014394670286
5
Iteration 16300: Loss = -10840.02533038066
6
Iteration 16400: Loss = -10840.092685766835
7
Iteration 16500: Loss = -10840.010987809088
8
Iteration 16600: Loss = -10840.011259539944
9
Iteration 16700: Loss = -10840.011000279239
10
Stopping early at iteration 16700 due to no improvement.
tensor([[  5.1664,  -6.9213],
        [  5.2386,  -6.7819],
        [  5.2430,  -6.7770],
        [  5.2517,  -6.8411],
        [  5.2357,  -6.8059],
        [  4.3747,  -7.6671],
        [  5.2580,  -6.7766],
        [  5.2510,  -6.7531],
        [  5.3489,  -6.7594],
        [  4.6330,  -7.3720],
        [  5.1493,  -6.8775],
        [  4.6419,  -7.3979],
        [  5.2333,  -6.8154],
        [  4.3967,  -7.6852],
        [  5.3387,  -6.7250],
        [  5.2209,  -6.8084],
        [  5.2967,  -6.7172],
        [  4.8468,  -7.1790],
        [  4.7007,  -7.2112],
        [  5.1944,  -6.8092],
        [  3.8311,  -8.2053],
        [  5.2949,  -6.7201],
        [  4.8334,  -7.2061],
        [  5.2412,  -6.7843],
        [  5.3534,  -6.7433],
        [  5.2879,  -6.6743],
        [  5.3043,  -6.7414],
        [  5.3446,  -6.7395],
        [  4.7423,  -7.3176],
        [  5.1882,  -6.8049],
        [  5.2094,  -6.8082],
        [  5.0947,  -6.9221],
        [  5.3110,  -6.7192],
        [  5.2308,  -6.7243],
        [  4.8932,  -7.2708],
        [  5.2961,  -6.7032],
        [  5.4134,  -6.8143],
        [  5.2519,  -6.7793],
        [  4.9135,  -7.1426],
        [  5.3123,  -6.7005],
        [  5.2345,  -6.8127],
        [  5.2589,  -6.7712],
        [  5.1295,  -6.9107],
        [  4.8051,  -7.2682],
        [  5.3141,  -6.7437],
        [  4.8640,  -7.1934],
        [  4.7836,  -7.3003],
        [  5.2026,  -6.7776],
        [  4.7246,  -7.1976],
        [  4.3817,  -7.7371],
        [  4.8519,  -7.1179],
        [  5.2908,  -6.6782],
        [  5.3316,  -6.7188],
        [  5.2309,  -6.7650],
        [  5.2255,  -6.7432],
        [  5.3186,  -6.7090],
        [  5.2955,  -6.7116],
        [  4.5776,  -7.3497],
        [  5.0288,  -7.0263],
        [  5.3143,  -6.7108],
        [  5.0675,  -6.8990],
        [  5.2290,  -6.7297],
        [  4.9285,  -7.1227],
        [  4.8761,  -7.1668],
        [  4.8803,  -7.2031],
        [  5.3020,  -6.7053],
        [  4.9938,  -6.9434],
        [  5.2838,  -6.6909],
        [  5.2682,  -6.6681],
        [  5.0155,  -6.9932],
        [  5.2906,  -6.7485],
        [  5.3102,  -6.7150],
        [  5.0637,  -6.9097],
        [  5.3176,  -6.8150],
        [  5.3043,  -6.7911],
        [  5.1372,  -6.8416],
        [  5.2750,  -6.6845],
        [  5.2529,  -6.6762],
        [  5.2189,  -6.7556],
        [  5.1595,  -6.9098],
        [  4.6673,  -7.3402],
        [  5.1170,  -6.8800],
        [  4.5826,  -7.4423],
        [  5.3282,  -6.7328],
        [  5.2820,  -6.6752],
        [  8.6300, -10.0197],
        [  5.2998,  -6.6872],
        [  5.3591,  -6.7760],
        [  3.8699,  -8.1866],
        [  4.9400,  -7.0543],
        [  5.1390,  -6.9232],
        [  5.2259,  -6.7152],
        [  5.0073,  -7.0499],
        [  5.2320,  -6.7372],
        [  4.7507,  -7.6257],
        [  5.2780,  -6.7194],
        [  5.0961,  -6.9959],
        [  5.1150,  -6.9503],
        [  4.9941,  -6.9979],
        [  5.2771,  -6.6745]], dtype=torch.float64, requires_grad=True)
pi: tensor([[2.8265e-01, 7.1735e-01],
        [3.3986e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 5.9372e-06], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1606, 0.1623],
         [0.7534, 0.1580]],

        [[0.2772, 0.1628],
         [0.0902, 0.9309]],

        [[0.2728, 0.1627],
         [0.0037, 0.8128]],

        [[0.4756, 0.1703],
         [0.7405, 0.8480]],

        [[0.0574, 0.0760],
         [0.3382, 0.9332]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.003622479472855591
Average Adjusted Rand Index: -0.0008569898232458489
Iteration 0: Loss = -22437.28928755015
Iteration 10: Loss = -10840.514926979666
Iteration 20: Loss = -10840.514927003282
1
Iteration 30: Loss = -10840.514927083686
2
Iteration 40: Loss = -10840.514927282795
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.6604e-11, 1.0000e+00],
        [1.6042e-11, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([1.5719e-11, 1.0000e+00])
beta: tensor([[[0.1758, 0.1646],
         [0.2824, 0.1574]],

        [[0.0950, 0.1865],
         [0.8306, 0.5449]],

        [[0.1000, 0.2354],
         [0.9452, 0.3621]],

        [[0.5723, 0.1053],
         [0.7102, 0.6973]],

        [[0.9710, 0.1282],
         [0.9743, 0.9879]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22437.051941798523
Iteration 100: Loss = -10859.494083374295
Iteration 200: Loss = -10842.751613271472
Iteration 300: Loss = -10841.191527117706
Iteration 400: Loss = -10840.754696317954
Iteration 500: Loss = -10840.55673241546
Iteration 600: Loss = -10840.444684026894
Iteration 700: Loss = -10840.376175445595
Iteration 800: Loss = -10840.328984229787
Iteration 900: Loss = -10840.295183097625
Iteration 1000: Loss = -10840.26958310756
Iteration 1100: Loss = -10840.24859811869
Iteration 1200: Loss = -10840.230517377953
Iteration 1300: Loss = -10840.214675316762
Iteration 1400: Loss = -10840.200733964375
Iteration 1500: Loss = -10840.188412065341
Iteration 1600: Loss = -10840.17764798953
Iteration 1700: Loss = -10840.168119100177
Iteration 1800: Loss = -10840.159430049409
Iteration 1900: Loss = -10840.151559506998
Iteration 2000: Loss = -10840.144316389442
Iteration 2100: Loss = -10840.137576151663
Iteration 2200: Loss = -10840.131143519824
Iteration 2300: Loss = -10840.124872147886
Iteration 2400: Loss = -10840.1187054697
Iteration 2500: Loss = -10840.112487511933
Iteration 2600: Loss = -10840.106276424305
Iteration 2700: Loss = -10840.100431430936
Iteration 2800: Loss = -10840.094989552195
Iteration 2900: Loss = -10840.089708547559
Iteration 3000: Loss = -10840.084450889926
Iteration 3100: Loss = -10840.079269043214
Iteration 3200: Loss = -10840.073980493184
Iteration 3300: Loss = -10840.068669105125
Iteration 3400: Loss = -10840.063405012595
Iteration 3500: Loss = -10840.058083681739
Iteration 3600: Loss = -10840.052785148215
Iteration 3700: Loss = -10840.04732222175
Iteration 3800: Loss = -10840.041728107208
Iteration 3900: Loss = -10840.036313963641
Iteration 4000: Loss = -10840.030846881493
Iteration 4100: Loss = -10840.025166668931
Iteration 4200: Loss = -10840.019308375377
Iteration 4300: Loss = -10840.01321314654
Iteration 4400: Loss = -10840.006639387844
Iteration 4500: Loss = -10839.999680531599
Iteration 4600: Loss = -10839.992076795783
Iteration 4700: Loss = -10839.983733032714
Iteration 4800: Loss = -10839.974602631088
Iteration 4900: Loss = -10839.964470771753
Iteration 5000: Loss = -10839.953416645365
Iteration 5100: Loss = -10839.94161594707
Iteration 5200: Loss = -10839.929278221733
Iteration 5300: Loss = -10839.916909932737
Iteration 5400: Loss = -10839.90510567143
Iteration 5500: Loss = -10839.894872163146
Iteration 5600: Loss = -10839.886479251874
Iteration 5700: Loss = -10839.87986252284
Iteration 5800: Loss = -10839.87515562033
Iteration 5900: Loss = -10839.871864204888
Iteration 6000: Loss = -10839.870520123244
Iteration 6100: Loss = -10839.871292507305
1
Iteration 6200: Loss = -10839.86824368345
Iteration 6300: Loss = -10840.030016463354
1
Iteration 6400: Loss = -10839.865320130459
Iteration 6500: Loss = -10839.865119294629
Iteration 6600: Loss = -10839.864549623817
Iteration 6700: Loss = -10839.864216715348
Iteration 6800: Loss = -10839.863915817055
Iteration 6900: Loss = -10839.864607937607
1
Iteration 7000: Loss = -10839.863459699927
Iteration 7100: Loss = -10839.87506737667
1
Iteration 7200: Loss = -10839.86296628902
Iteration 7300: Loss = -10839.862789871417
Iteration 7400: Loss = -10839.863356550872
1
Iteration 7500: Loss = -10839.86234467184
Iteration 7600: Loss = -10839.86212367572
Iteration 7700: Loss = -10839.956753906257
1
Iteration 7800: Loss = -10839.861710096764
Iteration 7900: Loss = -10839.86144271805
Iteration 8000: Loss = -10839.87417243413
1
Iteration 8100: Loss = -10839.861079303078
Iteration 8200: Loss = -10839.860881433357
Iteration 8300: Loss = -10839.868399705496
1
Iteration 8400: Loss = -10839.860505052746
Iteration 8500: Loss = -10839.860299943846
Iteration 8600: Loss = -10839.860087241472
Iteration 8700: Loss = -10839.860357659829
1
Iteration 8800: Loss = -10839.859737821575
Iteration 8900: Loss = -10839.85957016917
Iteration 9000: Loss = -10839.859649216698
1
Iteration 9100: Loss = -10839.859268387212
Iteration 9200: Loss = -10839.85933353635
1
Iteration 9300: Loss = -10839.85900670878
Iteration 9400: Loss = -10839.858845195353
Iteration 9500: Loss = -10839.867371162773
1
Iteration 9600: Loss = -10839.858603801329
Iteration 9700: Loss = -10839.858452610913
Iteration 9800: Loss = -10840.347347082954
1
Iteration 9900: Loss = -10839.858253805185
Iteration 10000: Loss = -10839.858160390322
Iteration 10100: Loss = -10839.858063254624
Iteration 10200: Loss = -10839.858192313914
1
Iteration 10300: Loss = -10839.85788906121
Iteration 10400: Loss = -10840.030026582772
1
Iteration 10500: Loss = -10839.857767260315
Iteration 10600: Loss = -10839.857635412985
Iteration 10700: Loss = -10839.865036240419
1
Iteration 10800: Loss = -10839.857523622924
Iteration 10900: Loss = -10839.857463419748
Iteration 11000: Loss = -10839.857579776875
1
Iteration 11100: Loss = -10839.85735194105
Iteration 11200: Loss = -10839.857412655194
1
Iteration 11300: Loss = -10839.857289500802
Iteration 11400: Loss = -10839.85722835696
Iteration 11500: Loss = -10839.859868907999
1
Iteration 11600: Loss = -10839.857099911856
Iteration 11700: Loss = -10839.857230457717
1
Iteration 11800: Loss = -10839.857869562282
2
Iteration 11900: Loss = -10839.857033798475
Iteration 12000: Loss = -10839.896135114695
1
Iteration 12100: Loss = -10839.856972406587
Iteration 12200: Loss = -10839.856897280242
Iteration 12300: Loss = -10839.857094183677
1
Iteration 12400: Loss = -10839.856876402831
Iteration 12500: Loss = -10840.004071252348
1
Iteration 12600: Loss = -10839.856796647098
Iteration 12700: Loss = -10839.856800311312
1
Iteration 12800: Loss = -10839.857110249228
2
Iteration 12900: Loss = -10839.856754220731
Iteration 13000: Loss = -10839.86254661021
1
Iteration 13100: Loss = -10839.856708895928
Iteration 13200: Loss = -10839.886414293036
1
Iteration 13300: Loss = -10839.856712559606
2
Iteration 13400: Loss = -10839.856715626078
3
Iteration 13500: Loss = -10839.856667336762
Iteration 13600: Loss = -10839.856636444163
Iteration 13700: Loss = -10839.860646103563
1
Iteration 13800: Loss = -10839.86144301403
2
Iteration 13900: Loss = -10839.856647034334
3
Iteration 14000: Loss = -10839.87036387921
4
Iteration 14100: Loss = -10839.85660199805
Iteration 14200: Loss = -10839.886264892195
1
Iteration 14300: Loss = -10839.870631736465
2
Iteration 14400: Loss = -10839.856549691114
Iteration 14500: Loss = -10839.856762812471
1
Iteration 14600: Loss = -10840.0851171836
2
Iteration 14700: Loss = -10839.85650885952
Iteration 14800: Loss = -10839.860726514313
1
Iteration 14900: Loss = -10839.856514370415
2
Iteration 15000: Loss = -10839.856709745263
3
Iteration 15100: Loss = -10839.87410941921
4
Iteration 15200: Loss = -10839.85659868985
5
Iteration 15300: Loss = -10839.856511714263
6
Iteration 15400: Loss = -10839.866197237909
7
Iteration 15500: Loss = -10839.856461844693
Iteration 15600: Loss = -10839.856551054023
1
Iteration 15700: Loss = -10839.857351921608
2
Iteration 15800: Loss = -10839.868367983416
3
Iteration 15900: Loss = -10839.856520310332
4
Iteration 16000: Loss = -10839.885330623956
5
Iteration 16100: Loss = -10839.856443370196
Iteration 16200: Loss = -10839.865126959266
1
Iteration 16300: Loss = -10839.856473234684
2
Iteration 16400: Loss = -10839.85669303292
3
Iteration 16500: Loss = -10839.891045335946
4
Iteration 16600: Loss = -10839.856465447805
5
Iteration 16700: Loss = -10840.017899015604
6
Iteration 16800: Loss = -10839.85646572104
7
Iteration 16900: Loss = -10839.91476207026
8
Iteration 17000: Loss = -10839.856452825217
9
Iteration 17100: Loss = -10839.863615253884
10
Stopping early at iteration 17100 due to no improvement.
tensor([[-3.3701,  1.8292],
        [-3.3643,  1.8938],
        [-4.2277,  1.0593],
        [-3.5581,  1.6552],
        [-4.9231,  0.3079],
        [-3.6367,  1.6482],
        [-3.6376,  1.6740],
        [-3.3839,  1.9024],
        [-4.2245,  0.9753],
        [-3.3636,  1.9038],
        [-4.9218,  0.3066],
        [-3.3824,  1.8695],
        [-4.0456,  1.1787],
        [-3.7073,  1.5258],
        [-3.3749,  1.8637],
        [-3.3464,  1.8857],
        [-4.2816,  1.0072],
        [-3.3420,  1.9349],
        [-4.0904,  1.2589],
        [-3.3873,  1.8982],
        [-4.9354,  0.3202],
        [-3.3725,  1.8922],
        [-3.3993,  1.8425],
        [-3.3378,  1.9316],
        [-3.3355,  1.8991],
        [-3.7247,  1.5921],
        [-3.3226,  1.9160],
        [-4.8749,  0.3400],
        [-3.4958,  1.7582],
        [-4.2729,  0.9954],
        [-3.3367,  1.9217],
        [-3.5487,  1.6885],
        [-3.4959,  1.7403],
        [-4.9681,  0.3529],
        [-3.5901,  1.5910],
        [-3.6221,  1.6872],
        [-3.6008,  1.6496],
        [-3.3958,  1.8767],
        [-3.4095,  1.8144],
        [-3.3346,  1.9412],
        [-3.3274,  1.9129],
        [-3.9819,  1.2751],
        [-3.7748,  1.4813],
        [-3.7183,  1.5271],
        [-4.1595,  1.0694],
        [-3.3221,  1.9353],
        [-3.7358,  1.4946],
        [-3.4447,  1.8702],
        [-4.9784,  0.3632],
        [-3.3042,  1.9133],
        [-3.4903,  1.8335],
        [-4.8921,  0.4213],
        [-4.1040,  1.1337],
        [-3.3380,  1.9504],
        [-3.6362,  1.6562],
        [-4.9394,  0.3242],
        [-4.1127,  1.1407],
        [-3.5620,  1.7751],
        [-3.3742,  1.8638],
        [-3.3420,  1.8980],
        [-3.3548,  1.9601],
        [-3.3486,  1.9621],
        [-3.7241,  1.5489],
        [-3.4517,  1.8175],
        [-3.4176,  1.8035],
        [-4.1411,  1.1568],
        [-3.4962,  1.8183],
        [-3.8138,  1.4676],
        [-3.6155,  1.7126],
        [-3.6486,  1.5955],
        [-3.9978,  1.2634],
        [-4.2710,  0.9829],
        [-3.4395,  1.8491],
        [-3.5248,  1.6659],
        [-3.5772,  1.6369],
        [-3.6437,  1.5665],
        [-3.4791,  1.8080],
        [-3.3860,  1.9295],
        [-3.4264,  1.8639],
        [-3.7047,  1.5354],
        [-3.6073,  1.6817],
        [-3.7476,  1.5628],
        [-4.9290,  0.3138],
        [-3.4115,  1.8186],
        [-3.4774,  1.8535],
        [-3.3797,  1.7946],
        [-3.4833,  1.8149],
        [-3.2911,  1.8824],
        [-4.1333,  1.1034],
        [-3.3469,  1.9484],
        [-3.7495,  1.4773],
        [-3.7154,  1.5807],
        [-3.3376,  1.9092],
        [-4.2687,  1.0273],
        [-3.3334,  1.9455],
        [-3.5456,  1.7244],
        [-3.4143,  1.8007],
        [-3.4387,  1.8307],
        [-4.4113,  0.8888],
        [-4.8477,  0.5227]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.7396e-04, 9.9983e-01],
        [7.3293e-02, 9.2671e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0051, 0.9949], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1374, 0.1593],
         [0.2824, 0.1600]],

        [[0.0950, 0.1600],
         [0.8306, 0.5449]],

        [[0.1000, 0.1590],
         [0.9452, 0.3621]],

        [[0.5723, 0.1310],
         [0.7102, 0.6973]],

        [[0.9710, 0.1435],
         [0.9743, 0.9879]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
10817.83581310901
new:  [-0.00040280206055477003, 0.0, -0.003622479472855591, 0.0] [0.0027249569557852493, 0.0, -0.0008569898232458489, 0.0] [10839.664880594877, 10839.879248734662, 10840.011000279239, 10839.863615253884]
prior:  [0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0] [10840.139597415327, 10840.514926936235, 10840.147367699747, 10840.514927282795]
-----------------------------------------------------------------------------------------
This iteration is 3
True Objective function: Loss = -10736.924618725014
Iteration 0: Loss = -16049.44400052609
Iteration 10: Loss = -10771.961045119324
Iteration 20: Loss = -10771.961047640792
1
Iteration 30: Loss = -10771.961052888473
2
Iteration 40: Loss = -10771.961080522246
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[2.7727e-20, 1.0000e+00],
        [3.1109e-09, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([3.0260e-09, 1.0000e+00])
beta: tensor([[[0.1961, 0.1814],
         [0.4230, 0.1558]],

        [[0.3004, 0.2696],
         [0.6902, 0.3354]],

        [[0.1048, 0.2090],
         [0.3818, 0.5529]],

        [[0.3670, 0.2544],
         [0.4029, 0.7776]],

        [[0.2028, 0.1450],
         [0.1725, 0.0518]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16711.90931640888
Iteration 100: Loss = -10775.989624598793
Iteration 200: Loss = -10773.409983605092
Iteration 300: Loss = -10772.594479559182
Iteration 400: Loss = -10772.198432525189
Iteration 500: Loss = -10772.015748719106
Iteration 600: Loss = -10771.917939202114
Iteration 700: Loss = -10771.856428490097
Iteration 800: Loss = -10771.814944632464
Iteration 900: Loss = -10771.786069825686
Iteration 1000: Loss = -10771.766143029958
Iteration 1100: Loss = -10771.75211344572
Iteration 1200: Loss = -10771.741991765064
Iteration 1300: Loss = -10771.733967464661
Iteration 1400: Loss = -10771.726980877345
Iteration 1500: Loss = -10771.720543447884
Iteration 1600: Loss = -10771.924767833632
1
Iteration 1700: Loss = -10771.70689587261
Iteration 1800: Loss = -10771.697875861673
Iteration 1900: Loss = -10771.684730895367
Iteration 2000: Loss = -10771.670860761154
Iteration 2100: Loss = -10771.61649481009
Iteration 2200: Loss = -10771.493585674345
Iteration 2300: Loss = -10771.095466448733
Iteration 2400: Loss = -10770.85884452593
Iteration 2500: Loss = -10770.737386299166
Iteration 2600: Loss = -10770.666194495665
Iteration 2700: Loss = -10770.63223818555
Iteration 2800: Loss = -10768.642757536072
Iteration 2900: Loss = -10763.082962698636
Iteration 3000: Loss = -10762.982210668331
Iteration 3100: Loss = -10763.01760724754
1
Iteration 3200: Loss = -10762.903590633694
Iteration 3300: Loss = -10762.881547465662
Iteration 3400: Loss = -10762.864439225526
Iteration 3500: Loss = -10762.84869207157
Iteration 3600: Loss = -10762.834978898534
Iteration 3700: Loss = -10762.826715280153
Iteration 3800: Loss = -10762.820406539857
Iteration 3900: Loss = -10762.813883458393
Iteration 4000: Loss = -10762.808667978903
Iteration 4100: Loss = -10762.804263296634
Iteration 4200: Loss = -10762.80493136599
1
Iteration 4300: Loss = -10762.797117903576
Iteration 4400: Loss = -10762.79417835199
Iteration 4500: Loss = -10762.791525612387
Iteration 4600: Loss = -10762.84093733029
1
Iteration 4700: Loss = -10762.787046739008
Iteration 4800: Loss = -10762.785122900117
Iteration 4900: Loss = -10762.7834065796
Iteration 5000: Loss = -10763.333364276084
1
Iteration 5100: Loss = -10762.780413790979
Iteration 5200: Loss = -10762.779081875417
Iteration 5300: Loss = -10762.777875569716
Iteration 5400: Loss = -10762.84202540504
1
Iteration 5500: Loss = -10762.775767547815
Iteration 5600: Loss = -10762.774824383801
Iteration 5700: Loss = -10762.773963603926
Iteration 5800: Loss = -10762.77329559015
Iteration 5900: Loss = -10762.772339341062
Iteration 6000: Loss = -10762.771701236115
Iteration 6100: Loss = -10763.066487060103
1
Iteration 6200: Loss = -10762.770459863554
Iteration 6300: Loss = -10762.76988909151
Iteration 6400: Loss = -10762.769399521274
Iteration 6500: Loss = -10762.77076972377
1
Iteration 6600: Loss = -10762.768491598894
Iteration 6700: Loss = -10762.768038784568
Iteration 6800: Loss = -10763.014849232051
1
Iteration 6900: Loss = -10762.767322434094
Iteration 7000: Loss = -10762.766944871162
Iteration 7100: Loss = -10762.766609561568
Iteration 7200: Loss = -10762.766403938967
Iteration 7300: Loss = -10762.766045765968
Iteration 7400: Loss = -10762.765724238894
Iteration 7500: Loss = -10763.089839926306
1
Iteration 7600: Loss = -10762.765290565982
Iteration 7700: Loss = -10762.76503222067
Iteration 7800: Loss = -10762.7648134882
Iteration 7900: Loss = -10762.764806679723
Iteration 8000: Loss = -10762.764420672016
Iteration 8100: Loss = -10762.764215925761
Iteration 8200: Loss = -10762.764144086634
Iteration 8300: Loss = -10762.763894152533
Iteration 8400: Loss = -10762.763714178527
Iteration 8500: Loss = -10762.763621003387
Iteration 8600: Loss = -10762.7633770748
Iteration 8700: Loss = -10762.763147608339
Iteration 8800: Loss = -10762.763781071439
1
Iteration 8900: Loss = -10762.762415358686
Iteration 9000: Loss = -10762.76040844838
Iteration 9100: Loss = -10762.528811588903
Iteration 9200: Loss = -10762.520257677732
Iteration 9300: Loss = -10762.520087889003
Iteration 9400: Loss = -10762.559949635746
1
Iteration 9500: Loss = -10762.51982476379
Iteration 9600: Loss = -10762.69871974874
1
Iteration 9700: Loss = -10762.519654363481
Iteration 9800: Loss = -10762.519579361217
Iteration 9900: Loss = -10762.522853617225
1
Iteration 10000: Loss = -10762.51944400463
Iteration 10100: Loss = -10762.519655949536
1
Iteration 10200: Loss = -10762.525047436768
2
Iteration 10300: Loss = -10762.52019566503
3
Iteration 10400: Loss = -10762.519178220722
Iteration 10500: Loss = -10762.519997794448
1
Iteration 10600: Loss = -10762.519420284736
2
Iteration 10700: Loss = -10762.533227133465
3
Iteration 10800: Loss = -10762.524020641516
4
Iteration 10900: Loss = -10762.519092304845
Iteration 11000: Loss = -10762.522871075365
1
Iteration 11100: Loss = -10762.518900359126
Iteration 11200: Loss = -10762.56801069698
1
Iteration 11300: Loss = -10762.518758113994
Iteration 11400: Loss = -10762.536761658206
1
Iteration 11500: Loss = -10762.518589292984
Iteration 11600: Loss = -10762.518572003459
Iteration 11700: Loss = -10762.641985257535
1
Iteration 11800: Loss = -10762.518768201136
2
Iteration 11900: Loss = -10762.5207669526
3
Iteration 12000: Loss = -10762.624866551203
4
Iteration 12100: Loss = -10762.536028363173
5
Iteration 12200: Loss = -10762.516122808844
Iteration 12300: Loss = -10762.516749487615
1
Iteration 12400: Loss = -10762.531759145226
2
Iteration 12500: Loss = -10762.516066755466
Iteration 12600: Loss = -10762.51676047253
1
Iteration 12700: Loss = -10762.525817743757
2
Iteration 12800: Loss = -10762.5197742756
3
Iteration 12900: Loss = -10762.516135861539
4
Iteration 13000: Loss = -10762.518911482943
5
Iteration 13100: Loss = -10762.515958455497
Iteration 13200: Loss = -10762.51613650888
1
Iteration 13300: Loss = -10762.516110672692
2
Iteration 13400: Loss = -10762.516379863537
3
Iteration 13500: Loss = -10762.515757875768
Iteration 13600: Loss = -10762.517001500137
1
Iteration 13700: Loss = -10762.5156583679
Iteration 13800: Loss = -10762.516536964553
1
Iteration 13900: Loss = -10762.516444880184
2
Iteration 14000: Loss = -10762.515853233486
3
Iteration 14100: Loss = -10762.515622273686
Iteration 14200: Loss = -10762.515773652487
1
Iteration 14300: Loss = -10762.515644913417
2
Iteration 14400: Loss = -10762.516469830283
3
Iteration 14500: Loss = -10762.51611546739
4
Iteration 14600: Loss = -10762.515642636772
5
Iteration 14700: Loss = -10762.519808180561
6
Iteration 14800: Loss = -10762.539340527375
7
Iteration 14900: Loss = -10762.515541818648
Iteration 15000: Loss = -10762.516846836912
1
Iteration 15100: Loss = -10762.515521161253
Iteration 15200: Loss = -10762.516026917523
1
Iteration 15300: Loss = -10762.515541794133
2
Iteration 15400: Loss = -10762.527276782192
3
Iteration 15500: Loss = -10762.518532897528
4
Iteration 15600: Loss = -10762.515528961187
5
Iteration 15700: Loss = -10762.540230230317
6
Iteration 15800: Loss = -10762.515507119413
Iteration 15900: Loss = -10762.517362268485
1
Iteration 16000: Loss = -10762.515507330629
2
Iteration 16100: Loss = -10762.515657524835
3
Iteration 16200: Loss = -10762.517322989213
4
Iteration 16300: Loss = -10762.52192276123
5
Iteration 16400: Loss = -10762.528529686131
6
Iteration 16500: Loss = -10762.516691610972
7
Iteration 16600: Loss = -10762.515531155932
8
Iteration 16700: Loss = -10762.515627188668
9
Iteration 16800: Loss = -10762.515960795363
10
Stopping early at iteration 16800 due to no improvement.
tensor([[ -9.9865,   5.3713],
        [ -2.7444,  -1.8708],
        [ -8.6831,   4.0679],
        [ -9.1597,   4.5445],
        [ -8.9224,   4.3072],
        [ -7.7500,   3.1348],
        [ -7.9550,   3.3398],
        [ -8.9086,   4.2934],
        [-10.7944,   6.1792],
        [ -8.3356,   3.7204],
        [ -5.0941,   0.4789],
        [-10.2225,   5.6073],
        [ -2.5925,  -2.0228],
        [ -4.8702,   0.2550],
        [ -6.8698,   2.2546],
        [ -6.1104,   1.4952],
        [-10.4568,   5.8416],
        [ -6.1001,   1.4849],
        [ -0.2390,  -4.3762],
        [ -4.9667,   0.3515],
        [  2.9349,  -7.5501],
        [ -8.7131,   4.0979],
        [ -8.8656,   4.2504],
        [ -8.9175,   4.3023],
        [ -7.8247,   3.2094],
        [ -6.3857,   1.7705],
        [ -9.5932,   4.9779],
        [ -7.7473,   3.1321],
        [ -6.9646,   2.3494],
        [ -2.7718,  -1.8434],
        [ -5.9558,   1.3406],
        [ -8.3204,   3.7052],
        [ -7.7347,   3.1195],
        [ -5.5298,   0.9146],
        [ -5.4248,   0.8096],
        [ -8.0884,   3.4731],
        [ -9.8866,   5.2713],
        [ -4.8051,   0.1899],
        [ -0.5242,  -4.0910],
        [ -4.5085,  -0.1067],
        [ -6.9877,   2.3724],
        [ -6.3743,   1.7591],
        [ -9.7039,   5.0887],
        [  1.6513,  -6.2665],
        [ -6.2018,   1.5866],
        [ -9.4832,   4.8680],
        [ -7.3181,   2.7029],
        [ -9.7932,   5.1780],
        [ -6.4247,   1.8095],
        [ -4.2093,  -0.4059],
        [ -8.1293,   3.5141],
        [ -9.5883,   4.9731],
        [ -5.9474,   1.3322],
        [ -8.1513,   3.5360],
        [ -9.7135,   5.0983],
        [ -5.4604,   0.8452],
        [ -8.9611,   4.3459],
        [ -7.4994,   2.8842],
        [ -4.2992,  -0.3161],
        [-10.0483,   5.4331],
        [ -7.2287,   2.6135],
        [ -7.3743,   2.7590],
        [ -9.2575,   4.6423],
        [ -9.1689,   4.5536],
        [ -6.2780,   1.6628],
        [ -7.5662,   2.9510],
        [ -5.8187,   1.2035],
        [ -6.6824,   2.0672],
        [  0.3609,  -4.9761],
        [ -9.5617,   4.9465],
        [ -8.6110,   3.9958],
        [ -4.1951,  -0.4201],
        [-10.0156,   5.4004],
        [ -9.4335,   4.8183],
        [ -7.9913,   3.3761],
        [ -6.8782,   2.2630],
        [ -9.8770,   5.2618],
        [ -5.8870,   1.2718],
        [ -7.2170,   2.6018],
        [ -8.5645,   3.9493],
        [ -9.5671,   4.9519],
        [ -8.1431,   3.5279],
        [ -6.7238,   2.1086],
        [ -7.4027,   2.7875],
        [ -8.9884,   4.3732],
        [ -8.5199,   3.9047],
        [  2.1891,  -6.8043],
        [ -7.4042,   2.7890],
        [ -8.8142,   4.1990],
        [ -4.9983,   0.3830],
        [-10.4280,   5.8128],
        [ -5.2839,   0.6687],
        [ -8.0812,   3.4660],
        [ -7.8183,   3.2031],
        [ -4.9290,   0.3137],
        [ -5.7892,   1.1740],
        [-10.2567,   5.6415],
        [ -5.2630,   0.6478],
        [  2.0903,  -6.7055],
        [ -8.0748,   3.4596]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 6.4364e-07],
        [7.6206e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0797, 0.9203], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4290, 0.1726],
         [0.4230, 0.1568]],

        [[0.3004, 0.1522],
         [0.6902, 0.3354]],

        [[0.1048, 0.1613],
         [0.3818, 0.5529]],

        [[0.3670, 0.1366],
         [0.4029, 0.7776]],

        [[0.2028, 0.1268],
         [0.1725, 0.0518]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.007389132627638759
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.015850928847493215
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.01150857271232653
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.01701445012943135
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 36
Adjusted Rand Index: 0.06239040134011877
Global Adjusted Rand Index: 0.023086860519155268
Average Adjusted Rand Index: 0.022830697131401727
Iteration 0: Loss = -19886.65209098224
Iteration 10: Loss = -10771.446885316733
Iteration 20: Loss = -10771.42136748486
Iteration 30: Loss = -10771.407546176617
Iteration 40: Loss = -10771.394461254124
Iteration 50: Loss = -10771.381967922265
Iteration 60: Loss = -10771.370071313519
Iteration 70: Loss = -10771.358748557679
Iteration 80: Loss = -10771.348058827041
Iteration 90: Loss = -10771.337935432728
Iteration 100: Loss = -10771.328344855016
Iteration 110: Loss = -10771.31927119023
Iteration 120: Loss = -10771.310714662983
Iteration 130: Loss = -10771.302563778976
Iteration 140: Loss = -10771.294856910898
Iteration 150: Loss = -10771.287560857263
Iteration 160: Loss = -10771.280593170914
Iteration 170: Loss = -10771.274000716869
Iteration 180: Loss = -10771.267777427794
Iteration 190: Loss = -10771.261827383723
Iteration 200: Loss = -10771.256173412157
Iteration 210: Loss = -10771.250765647428
Iteration 220: Loss = -10771.245681000657
Iteration 230: Loss = -10771.240756434223
Iteration 240: Loss = -10771.236076324194
Iteration 250: Loss = -10771.231592602193
Iteration 260: Loss = -10771.22728923979
Iteration 270: Loss = -10771.223202269488
Iteration 280: Loss = -10771.21930037834
Iteration 290: Loss = -10771.215550667304
Iteration 300: Loss = -10771.211951654424
Iteration 310: Loss = -10771.208479434856
Iteration 320: Loss = -10771.205142145129
Iteration 330: Loss = -10771.201973849738
Iteration 340: Loss = -10771.198921629591
Iteration 350: Loss = -10771.195966005576
Iteration 360: Loss = -10771.193096706436
Iteration 370: Loss = -10771.190383644187
Iteration 380: Loss = -10771.187746207215
Iteration 390: Loss = -10771.185176802948
Iteration 400: Loss = -10771.182729756842
Iteration 410: Loss = -10771.180361922185
Iteration 420: Loss = -10771.17808337839
Iteration 430: Loss = -10771.175842265766
Iteration 440: Loss = -10771.173716820482
Iteration 450: Loss = -10771.171643681986
Iteration 460: Loss = -10771.169649399548
Iteration 470: Loss = -10771.167686048559
Iteration 480: Loss = -10771.165795485087
Iteration 490: Loss = -10771.163983225993
Iteration 500: Loss = -10771.16224505669
Iteration 510: Loss = -10771.16049551198
Iteration 520: Loss = -10771.158879606519
Iteration 530: Loss = -10771.157233789989
Iteration 540: Loss = -10771.155653001866
Iteration 550: Loss = -10771.154165478025
Iteration 560: Loss = -10771.15264613665
Iteration 570: Loss = -10771.151219368772
Iteration 580: Loss = -10771.14983044688
Iteration 590: Loss = -10771.148462996609
Iteration 600: Loss = -10771.147144382632
Iteration 610: Loss = -10771.145844329663
Iteration 620: Loss = -10771.144600031208
Iteration 630: Loss = -10771.143363732865
Iteration 640: Loss = -10771.142184645041
Iteration 650: Loss = -10771.14102097729
Iteration 660: Loss = -10771.139888532447
Iteration 670: Loss = -10771.13878637326
Iteration 680: Loss = -10771.137720350947
Iteration 690: Loss = -10771.136641351119
Iteration 700: Loss = -10771.135637106243
Iteration 710: Loss = -10771.134599350617
Iteration 720: Loss = -10771.133634606425
Iteration 730: Loss = -10771.132704619717
Iteration 740: Loss = -10771.13180434342
Iteration 750: Loss = -10771.130867391088
Iteration 760: Loss = -10771.12996434836
Iteration 770: Loss = -10771.129132002256
Iteration 780: Loss = -10771.128242035775
Iteration 790: Loss = -10771.127403917018
Iteration 800: Loss = -10771.12661932832
Iteration 810: Loss = -10771.125787107934
Iteration 820: Loss = -10771.12502449023
Iteration 830: Loss = -10771.124271517585
Iteration 840: Loss = -10771.123472483749
Iteration 850: Loss = -10771.122759000193
Iteration 860: Loss = -10771.122035061107
Iteration 870: Loss = -10771.121357353624
Iteration 880: Loss = -10771.120668403846
Iteration 890: Loss = -10771.120013259495
Iteration 900: Loss = -10771.11932121233
Iteration 910: Loss = -10771.118705563307
Iteration 920: Loss = -10771.11805367585
Iteration 930: Loss = -10771.11740895541
Iteration 940: Loss = -10771.116795346668
Iteration 950: Loss = -10771.116185948107
Iteration 960: Loss = -10771.115598585615
Iteration 970: Loss = -10771.11501147555
Iteration 980: Loss = -10771.11444523744
Iteration 990: Loss = -10771.113887673046
Iteration 1000: Loss = -10771.11333764248
Iteration 1010: Loss = -10771.112797309414
Iteration 1020: Loss = -10771.11226182852
Iteration 1030: Loss = -10771.11171879147
Iteration 1040: Loss = -10771.1112397978
Iteration 1050: Loss = -10771.110726949631
Iteration 1060: Loss = -10771.110229832942
Iteration 1070: Loss = -10771.10972551341
Iteration 1080: Loss = -10771.109269041872
Iteration 1090: Loss = -10771.108808216866
Iteration 1100: Loss = -10771.108308376835
Iteration 1110: Loss = -10771.107875803667
Iteration 1120: Loss = -10771.107429345564
Iteration 1130: Loss = -10771.106982390005
Iteration 1140: Loss = -10771.106552123507
Iteration 1150: Loss = -10771.106111783532
Iteration 1160: Loss = -10771.105689734064
Iteration 1170: Loss = -10771.10531850663
Iteration 1180: Loss = -10771.104909161719
Iteration 1190: Loss = -10771.104489558149
Iteration 1200: Loss = -10771.104106675606
Iteration 1210: Loss = -10771.103729201008
Iteration 1220: Loss = -10771.103333143947
Iteration 1230: Loss = -10771.102937893858
Iteration 1240: Loss = -10771.102593371616
Iteration 1250: Loss = -10771.102198557934
Iteration 1260: Loss = -10771.101866423178
Iteration 1270: Loss = -10771.101509367416
Iteration 1280: Loss = -10771.101114600206
Iteration 1290: Loss = -10771.100831698464
Iteration 1300: Loss = -10771.100444520678
Iteration 1310: Loss = -10771.100162419487
Iteration 1320: Loss = -10771.099809269554
Iteration 1330: Loss = -10771.099519069727
Iteration 1340: Loss = -10771.099183294442
Iteration 1350: Loss = -10771.098877891254
Iteration 1360: Loss = -10771.098533983984
Iteration 1370: Loss = -10771.098212358935
Iteration 1380: Loss = -10771.09792692617
Iteration 1390: Loss = -10771.097655087919
Iteration 1400: Loss = -10771.0973003626
Iteration 1410: Loss = -10771.09703910969
Iteration 1420: Loss = -10771.09676636407
Iteration 1430: Loss = -10771.096470137893
Iteration 1440: Loss = -10771.096206014437
Iteration 1450: Loss = -10771.095897178215
Iteration 1460: Loss = -10771.095650641826
Iteration 1470: Loss = -10771.095378991708
Iteration 1480: Loss = -10771.095091738543
Iteration 1490: Loss = -10771.094795339168
Iteration 1500: Loss = -10771.094595926035
Iteration 1510: Loss = -10771.094319000502
Iteration 1520: Loss = -10771.09407205607
Iteration 1530: Loss = -10771.093806665958
Iteration 1540: Loss = -10771.093610427828
Iteration 1550: Loss = -10771.093294751045
Iteration 1560: Loss = -10771.09309932935
Iteration 1570: Loss = -10771.09288956527
Iteration 1580: Loss = -10771.092613966126
Iteration 1590: Loss = -10771.0924051628
Iteration 1600: Loss = -10771.092213907266
Iteration 1610: Loss = -10771.091962742034
Iteration 1620: Loss = -10771.091686068352
Iteration 1630: Loss = -10771.091489107752
Iteration 1640: Loss = -10771.091262356515
Iteration 1650: Loss = -10771.09108758514
Iteration 1660: Loss = -10771.090844497183
Iteration 1670: Loss = -10771.090638089827
Iteration 1680: Loss = -10771.090444742127
Iteration 1690: Loss = -10771.090236344648
Iteration 1700: Loss = -10771.090063370848
Iteration 1710: Loss = -10771.08983201153
Iteration 1720: Loss = -10771.089626330782
Iteration 1730: Loss = -10771.089439589969
Iteration 1740: Loss = -10771.089228664117
Iteration 1750: Loss = -10771.0890243449
Iteration 1760: Loss = -10771.08883963158
Iteration 1770: Loss = -10771.088668560189
Iteration 1780: Loss = -10771.088469546177
Iteration 1790: Loss = -10771.08831026217
Iteration 1800: Loss = -10771.08810691417
Iteration 1810: Loss = -10771.0879535196
Iteration 1820: Loss = -10771.08776214527
Iteration 1830: Loss = -10771.087598619642
Iteration 1840: Loss = -10771.087432000586
Iteration 1850: Loss = -10771.087234649303
Iteration 1860: Loss = -10771.087046150767
Iteration 1870: Loss = -10771.086908342753
Iteration 1880: Loss = -10771.086777112941
Iteration 1890: Loss = -10771.086554801366
Iteration 1900: Loss = -10771.086380990288
Iteration 1910: Loss = -10771.08625022347
Iteration 1920: Loss = -10771.086084941102
Iteration 1930: Loss = -10771.085954203856
Iteration 1940: Loss = -10771.085763863397
Iteration 1950: Loss = -10771.085664588127
Iteration 1960: Loss = -10771.085449631993
Iteration 1970: Loss = -10771.085350101155
Iteration 1980: Loss = -10771.085174831864
Iteration 1990: Loss = -10771.084985454503
Iteration 2000: Loss = -10771.084876312036
Iteration 2010: Loss = -10771.084755435026
Iteration 2020: Loss = -10771.08456805447
Iteration 2030: Loss = -10771.084448973937
Iteration 2040: Loss = -10771.08429489557
Iteration 2050: Loss = -10771.084190057052
Iteration 2060: Loss = -10771.08398725382
Iteration 2070: Loss = -10771.083899289793
Iteration 2080: Loss = -10771.083778038987
Iteration 2090: Loss = -10771.083612656701
Iteration 2100: Loss = -10771.083474209552
Iteration 2110: Loss = -10771.083376631075
Iteration 2120: Loss = -10771.083223368489
Iteration 2130: Loss = -10771.083086019311
Iteration 2140: Loss = -10771.08298098488
Iteration 2150: Loss = -10771.082815658705
Iteration 2160: Loss = -10771.082707260399
Iteration 2170: Loss = -10771.082606739394
Iteration 2180: Loss = -10771.082464122977
Iteration 2190: Loss = -10771.082304304593
Iteration 2200: Loss = -10771.082235466907
Iteration 2210: Loss = -10771.082108786153
Iteration 2220: Loss = -10771.082001552886
Iteration 2230: Loss = -10771.081856033845
Iteration 2240: Loss = -10771.081740590338
Iteration 2250: Loss = -10771.081613585222
Iteration 2260: Loss = -10771.081548086122
Iteration 2270: Loss = -10771.08146114781
Iteration 2280: Loss = -10771.081312226373
Iteration 2290: Loss = -10771.081180433699
Iteration 2300: Loss = -10771.081078719457
Iteration 2310: Loss = -10771.08096721745
Iteration 2320: Loss = -10771.080875576627
Iteration 2330: Loss = -10771.08076046821
Iteration 2340: Loss = -10771.080659451294
Iteration 2350: Loss = -10771.080546367904
Iteration 2360: Loss = -10771.08047691742
Iteration 2370: Loss = -10771.080361049844
Iteration 2380: Loss = -10771.08027151053
Iteration 2390: Loss = -10771.080154860956
Iteration 2400: Loss = -10771.080011703809
Iteration 2410: Loss = -10771.079926806411
Iteration 2420: Loss = -10771.079882966415
Iteration 2430: Loss = -10771.079753933056
Iteration 2440: Loss = -10771.079627374407
Iteration 2450: Loss = -10771.079506356109
Iteration 2460: Loss = -10771.079484681668
Iteration 2470: Loss = -10771.079336594517
Iteration 2480: Loss = -10771.079253399428
Iteration 2490: Loss = -10771.079189604303
Iteration 2500: Loss = -10771.07907796867
Iteration 2510: Loss = -10771.078992154182
Iteration 2520: Loss = -10771.0789271306
Iteration 2530: Loss = -10771.078833403291
Iteration 2540: Loss = -10771.078715298503
Iteration 2550: Loss = -10771.078641871189
Iteration 2560: Loss = -10771.078566748542
Iteration 2570: Loss = -10771.078484806263
Iteration 2580: Loss = -10771.07842208088
Iteration 2590: Loss = -10771.078307984506
Iteration 2600: Loss = -10771.078214368456
Iteration 2610: Loss = -10771.078093138107
Iteration 2620: Loss = -10771.078062258155
Iteration 2630: Loss = -10771.077972597448
Iteration 2640: Loss = -10771.077852834605
Iteration 2650: Loss = -10771.077822894536
Iteration 2660: Loss = -10771.077772365956
Iteration 2670: Loss = -10771.077627283126
Iteration 2680: Loss = -10771.07758085824
Iteration 2690: Loss = -10771.077469486434
Iteration 2700: Loss = -10771.077396956021
Iteration 2710: Loss = -10771.077355824535
Iteration 2720: Loss = -10771.077283898718
Iteration 2730: Loss = -10771.077210520189
Iteration 2740: Loss = -10771.077082659818
Iteration 2750: Loss = -10771.077028777336
Iteration 2760: Loss = -10771.07696083708
Iteration 2770: Loss = -10771.076847845234
Iteration 2780: Loss = -10771.076805130038
Iteration 2790: Loss = -10771.076710219162
Iteration 2800: Loss = -10771.076668752217
Iteration 2810: Loss = -10771.076622400562
Iteration 2820: Loss = -10771.076523524043
Iteration 2830: Loss = -10771.076460812388
Iteration 2840: Loss = -10771.076376039047
Iteration 2850: Loss = -10771.076365430112
Iteration 2860: Loss = -10771.076246362887
Iteration 2870: Loss = -10771.076192276967
Iteration 2880: Loss = -10771.076078408594
Iteration 2890: Loss = -10771.076028891956
Iteration 2900: Loss = -10771.07597004195
Iteration 2910: Loss = -10771.075924795348
Iteration 2920: Loss = -10771.075875593328
Iteration 2930: Loss = -10771.075785326075
Iteration 2940: Loss = -10771.075716499106
Iteration 2950: Loss = -10771.075649750523
Iteration 2960: Loss = -10771.075589604407
Iteration 2970: Loss = -10771.075528436013
Iteration 2980: Loss = -10771.075487165625
Iteration 2990: Loss = -10771.075416014071
Iteration 3000: Loss = -10771.075360669029
Iteration 3010: Loss = -10771.075292766991
Iteration 3020: Loss = -10771.075208577337
Iteration 3030: Loss = -10771.075157151703
Iteration 3040: Loss = -10771.075119432513
Iteration 3050: Loss = -10771.075022818644
Iteration 3060: Loss = -10771.075002193042
Iteration 3070: Loss = -10771.074943367616
Iteration 3080: Loss = -10771.074893254689
Iteration 3090: Loss = -10771.074816342836
Iteration 3100: Loss = -10771.074750521268
Iteration 3110: Loss = -10771.074674216085
Iteration 3120: Loss = -10771.074667785295
Iteration 3130: Loss = -10771.074582399944
Iteration 3140: Loss = -10771.074543302186
Iteration 3150: Loss = -10771.07448510143
Iteration 3160: Loss = -10771.074417698828
Iteration 3170: Loss = -10771.074376612301
Iteration 3180: Loss = -10771.074354553923
Iteration 3190: Loss = -10771.07425779807
Iteration 3200: Loss = -10771.074200348266
Iteration 3210: Loss = -10771.074143973614
Iteration 3220: Loss = -10771.074135765793
Iteration 3230: Loss = -10771.074036505663
Iteration 3240: Loss = -10771.073989475593
Iteration 3250: Loss = -10771.073995281711
1
Iteration 3260: Loss = -10771.073896967113
Iteration 3270: Loss = -10771.07387686778
Iteration 3280: Loss = -10771.073820672891
Iteration 3290: Loss = -10771.07381410991
Iteration 3300: Loss = -10771.073740912576
Iteration 3310: Loss = -10771.073672043785
Iteration 3320: Loss = -10771.073615331257
Iteration 3330: Loss = -10771.073595140537
Iteration 3340: Loss = -10771.073524504201
Iteration 3350: Loss = -10771.073493114101
Iteration 3360: Loss = -10771.073419029246
Iteration 3370: Loss = -10771.073403026872
Iteration 3380: Loss = -10771.073379322848
Iteration 3390: Loss = -10771.073320730326
Iteration 3400: Loss = -10771.07323523762
Iteration 3410: Loss = -10771.073199055285
Iteration 3420: Loss = -10771.073161654778
Iteration 3430: Loss = -10771.07312563102
Iteration 3440: Loss = -10771.073062622723
Iteration 3450: Loss = -10771.073041105647
Iteration 3460: Loss = -10771.073007080886
Iteration 3470: Loss = -10771.072953941135
Iteration 3480: Loss = -10771.072864631491
Iteration 3490: Loss = -10771.072835777326
Iteration 3500: Loss = -10771.072806362261
Iteration 3510: Loss = -10771.072773889184
Iteration 3520: Loss = -10771.072714715596
Iteration 3530: Loss = -10771.072723913743
1
Iteration 3540: Loss = -10771.072676173608
Iteration 3550: Loss = -10771.072624249811
Iteration 3560: Loss = -10771.072556072044
Iteration 3570: Loss = -10771.072528069155
Iteration 3580: Loss = -10771.07249939776
Iteration 3590: Loss = -10771.072432047347
Iteration 3600: Loss = -10771.072406191863
Iteration 3610: Loss = -10771.07237420505
Iteration 3620: Loss = -10771.072346267918
Iteration 3630: Loss = -10771.07227701969
Iteration 3640: Loss = -10771.072269313083
Iteration 3650: Loss = -10771.072206634275
Iteration 3660: Loss = -10771.072171335576
Iteration 3670: Loss = -10771.072168460392
Iteration 3680: Loss = -10771.072132841247
Iteration 3690: Loss = -10771.07206107955
Iteration 3700: Loss = -10771.072014552625
Iteration 3710: Loss = -10771.072010318887
Iteration 3720: Loss = -10771.07196912762
Iteration 3730: Loss = -10771.071919550142
Iteration 3740: Loss = -10771.071879824276
Iteration 3750: Loss = -10771.071860708038
Iteration 3760: Loss = -10771.071823061371
Iteration 3770: Loss = -10771.071785453476
Iteration 3780: Loss = -10771.071773057849
Iteration 3790: Loss = -10771.071683172828
Iteration 3800: Loss = -10771.071687712238
1
Iteration 3810: Loss = -10771.071681257943
Iteration 3820: Loss = -10771.071636534045
Iteration 3830: Loss = -10771.071614800376
Iteration 3840: Loss = -10771.071538576158
Iteration 3850: Loss = -10771.071493970576
Iteration 3860: Loss = -10771.071503955582
1
Iteration 3870: Loss = -10771.071465773322
Iteration 3880: Loss = -10771.071402383252
Iteration 3890: Loss = -10771.071409505776
1
Iteration 3900: Loss = -10771.071373677318
Iteration 3910: Loss = -10771.071340946584
Iteration 3920: Loss = -10771.071323181015
Iteration 3930: Loss = -10771.071298787203
Iteration 3940: Loss = -10771.071216925311
Iteration 3950: Loss = -10771.071190283907
Iteration 3960: Loss = -10771.071210016715
1
Iteration 3970: Loss = -10771.071101875346
Iteration 3980: Loss = -10771.071108154116
1
Iteration 3990: Loss = -10771.071100640906
Iteration 4000: Loss = -10771.071072813393
Iteration 4010: Loss = -10771.071038487087
Iteration 4020: Loss = -10771.071012069642
Iteration 4030: Loss = -10771.070975870569
Iteration 4040: Loss = -10771.070924550319
Iteration 4050: Loss = -10771.070897149755
Iteration 4060: Loss = -10771.070878087778
Iteration 4070: Loss = -10771.070854271857
Iteration 4080: Loss = -10771.070820125633
Iteration 4090: Loss = -10771.070747071619
Iteration 4100: Loss = -10771.070798415718
1
Iteration 4110: Loss = -10771.07075688639
2
Iteration 4120: Loss = -10771.070731198799
Iteration 4130: Loss = -10771.070715539656
Iteration 4140: Loss = -10771.070615443627
Iteration 4150: Loss = -10771.070647522523
1
Iteration 4160: Loss = -10771.070591789123
Iteration 4170: Loss = -10771.070572891764
Iteration 4180: Loss = -10771.070577861738
1
Iteration 4190: Loss = -10771.070493969795
Iteration 4200: Loss = -10771.070468225513
Iteration 4210: Loss = -10771.070475080794
1
Iteration 4220: Loss = -10771.070424873598
Iteration 4230: Loss = -10771.070396362158
Iteration 4240: Loss = -10771.070377874046
Iteration 4250: Loss = -10771.070379298873
1
Iteration 4260: Loss = -10771.070380601735
2
Iteration 4270: Loss = -10771.070288085346
Iteration 4280: Loss = -10771.070337246965
1
Iteration 4290: Loss = -10771.070280035732
Iteration 4300: Loss = -10771.070225078969
Iteration 4310: Loss = -10771.070229649553
1
Iteration 4320: Loss = -10771.070177057649
Iteration 4330: Loss = -10771.070197731175
1
Iteration 4340: Loss = -10771.07017648243
Iteration 4350: Loss = -10771.070117176794
Iteration 4360: Loss = -10771.07013631525
1
Iteration 4370: Loss = -10771.070096121059
Iteration 4380: Loss = -10771.070037574158
Iteration 4390: Loss = -10771.07003024437
Iteration 4400: Loss = -10771.070024003324
Iteration 4410: Loss = -10771.069957682152
Iteration 4420: Loss = -10771.069943057939
Iteration 4430: Loss = -10771.069934234158
Iteration 4440: Loss = -10771.069910250526
Iteration 4450: Loss = -10771.06991521895
1
Iteration 4460: Loss = -10771.069883631893
Iteration 4470: Loss = -10771.069831833973
Iteration 4480: Loss = -10771.069808626597
Iteration 4490: Loss = -10771.069829177703
1
Iteration 4500: Loss = -10771.069806786589
Iteration 4510: Loss = -10771.069769341942
Iteration 4520: Loss = -10771.069751681534
Iteration 4530: Loss = -10771.069736861622
Iteration 4540: Loss = -10771.069707540828
Iteration 4550: Loss = -10771.06968737118
Iteration 4560: Loss = -10771.069658192637
Iteration 4570: Loss = -10771.069673540835
1
Iteration 4580: Loss = -10771.069628946258
Iteration 4590: Loss = -10771.06958002262
Iteration 4600: Loss = -10771.069558581454
Iteration 4610: Loss = -10771.069603675543
1
Iteration 4620: Loss = -10771.069520635758
Iteration 4630: Loss = -10771.069529851262
1
Iteration 4640: Loss = -10771.069521807045
2
Iteration 4650: Loss = -10771.069484932797
Iteration 4660: Loss = -10771.069479044398
Iteration 4670: Loss = -10771.069436094569
Iteration 4680: Loss = -10771.069407061414
Iteration 4690: Loss = -10771.069378136206
Iteration 4700: Loss = -10771.069350825972
Iteration 4710: Loss = -10771.069398562744
1
Iteration 4720: Loss = -10771.069372688082
2
Iteration 4730: Loss = -10771.069324883945
Iteration 4740: Loss = -10771.069311439514
Iteration 4750: Loss = -10771.069276196367
Iteration 4760: Loss = -10771.069277790592
1
Iteration 4770: Loss = -10771.069239922894
Iteration 4780: Loss = -10771.06923809506
Iteration 4790: Loss = -10771.069213792169
Iteration 4800: Loss = -10771.069218074344
1
Iteration 4810: Loss = -10771.069213344168
Iteration 4820: Loss = -10771.069181810108
Iteration 4830: Loss = -10771.069164785986
Iteration 4840: Loss = -10771.069186272616
1
Iteration 4850: Loss = -10771.069107364352
Iteration 4860: Loss = -10771.069114233986
1
Iteration 4870: Loss = -10771.069126931998
2
Iteration 4880: Loss = -10771.069061323376
Iteration 4890: Loss = -10771.06904869069
Iteration 4900: Loss = -10771.069038266245
Iteration 4910: Loss = -10771.069020713143
Iteration 4920: Loss = -10771.06899530346
Iteration 4930: Loss = -10771.069028258677
1
Iteration 4940: Loss = -10771.06894670577
Iteration 4950: Loss = -10771.068932288255
Iteration 4960: Loss = -10771.068907124583
Iteration 4970: Loss = -10771.068914958245
1
Iteration 4980: Loss = -10771.068921220754
2
Iteration 4990: Loss = -10771.068891963092
Iteration 5000: Loss = -10771.06882203842
pi: tensor([[0.5872, 0.4128],
        [0.6300, 0.3700]], dtype=torch.float64)
alpha: tensor([0.6042, 0.3958])
beta: tensor([[[0.1558, 0.1601],
         [0.6845, 0.1557]],

        [[0.9536, 0.1581],
         [0.4670, 0.1756]],

        [[0.4211, 0.1583],
         [0.7623, 0.4101]],

        [[0.7458, 0.1474],
         [0.7976, 0.3335]],

        [[0.5076, 0.1547],
         [0.8817, 0.4921]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19886.51441751878
Iteration 100: Loss = -10805.203920591335
Iteration 200: Loss = -10781.739847455861
Iteration 300: Loss = -10777.5066517057
Iteration 400: Loss = -10774.536991148025
Iteration 500: Loss = -10771.98298065411
Iteration 600: Loss = -10771.601582105157
Iteration 700: Loss = -10771.417239405924
Iteration 800: Loss = -10771.318771005228
Iteration 900: Loss = -10771.25459234466
Iteration 1000: Loss = -10771.210647479544
Iteration 1100: Loss = -10771.180987179428
Iteration 1200: Loss = -10771.156310369233
Iteration 1300: Loss = -10771.135447795372
Iteration 1400: Loss = -10771.116442602735
Iteration 1500: Loss = -10771.098453572866
Iteration 1600: Loss = -10771.0808696464
Iteration 1700: Loss = -10771.063358978343
Iteration 1800: Loss = -10771.045749688701
Iteration 1900: Loss = -10771.027959406121
Iteration 2000: Loss = -10771.009523640045
Iteration 2100: Loss = -10770.989832594572
Iteration 2200: Loss = -10770.969113801275
Iteration 2300: Loss = -10770.951029232996
Iteration 2400: Loss = -10770.934870712708
Iteration 2500: Loss = -10770.920383074828
Iteration 2600: Loss = -10770.907146137979
Iteration 2700: Loss = -10770.894834042283
Iteration 2800: Loss = -10770.88313637791
Iteration 2900: Loss = -10770.871905787799
Iteration 3000: Loss = -10770.860798793437
Iteration 3100: Loss = -10770.849624830138
Iteration 3200: Loss = -10770.838372891025
Iteration 3300: Loss = -10770.826894050457
Iteration 3400: Loss = -10770.815109894635
Iteration 3500: Loss = -10770.802924859438
Iteration 3600: Loss = -10770.78942167802
Iteration 3700: Loss = -10770.757688309286
Iteration 3800: Loss = -10770.739575283184
Iteration 3900: Loss = -10770.71852682779
Iteration 4000: Loss = -10770.696766962663
Iteration 4100: Loss = -10770.678239370849
Iteration 4200: Loss = -10770.618466608175
Iteration 4300: Loss = -10770.593910346437
Iteration 4400: Loss = -10770.579746690888
Iteration 4500: Loss = -10770.572455677679
Iteration 4600: Loss = -10770.568708543053
Iteration 4700: Loss = -10770.569456020141
1
Iteration 4800: Loss = -10770.565256525755
Iteration 4900: Loss = -10770.564507952562
Iteration 5000: Loss = -10770.565501835505
1
Iteration 5100: Loss = -10770.56343449469
Iteration 5200: Loss = -10770.563154678865
Iteration 5300: Loss = -10770.562831711088
Iteration 5400: Loss = -10770.562760646779
Iteration 5500: Loss = -10770.562462375126
Iteration 5600: Loss = -10770.562362697923
Iteration 5700: Loss = -10770.565368550839
1
Iteration 5800: Loss = -10770.562036021223
Iteration 5900: Loss = -10770.567389298734
1
Iteration 6000: Loss = -10770.561797289454
Iteration 6100: Loss = -10770.562407680205
1
Iteration 6200: Loss = -10770.56158337466
Iteration 6300: Loss = -10770.561713628642
1
Iteration 6400: Loss = -10770.56492014798
2
Iteration 6500: Loss = -10770.562350501663
3
Iteration 6600: Loss = -10770.561310795229
Iteration 6700: Loss = -10770.561908293721
1
Iteration 6800: Loss = -10770.561396273755
2
Iteration 6900: Loss = -10770.56161496362
3
Iteration 7000: Loss = -10770.561189258033
Iteration 7100: Loss = -10770.56482560874
1
Iteration 7200: Loss = -10770.565356940348
2
Iteration 7300: Loss = -10770.561229189094
3
Iteration 7400: Loss = -10770.561048127389
Iteration 7500: Loss = -10770.562713409436
1
Iteration 7600: Loss = -10770.584797077325
2
Iteration 7700: Loss = -10770.562473788681
3
Iteration 7800: Loss = -10770.561056310808
4
Iteration 7900: Loss = -10770.56540444442
5
Iteration 8000: Loss = -10770.5708184264
6
Iteration 8100: Loss = -10770.581599859179
7
Iteration 8200: Loss = -10770.56020126911
Iteration 8300: Loss = -10770.56037563797
1
Iteration 8400: Loss = -10770.559861525633
Iteration 8500: Loss = -10770.559744435825
Iteration 8600: Loss = -10770.56158730704
1
Iteration 8700: Loss = -10770.559471952174
Iteration 8800: Loss = -10770.558949203203
Iteration 8900: Loss = -10770.558310627324
Iteration 9000: Loss = -10770.557347393416
Iteration 9100: Loss = -10770.592587177689
1
Iteration 9200: Loss = -10770.544805380843
Iteration 9300: Loss = -10770.591238801988
1
Iteration 9400: Loss = -10770.548653007812
2
Iteration 9500: Loss = -10770.533674808497
Iteration 9600: Loss = -10770.530186401533
Iteration 9700: Loss = -10770.521821408725
Iteration 9800: Loss = -10770.482798675212
Iteration 9900: Loss = -10767.631575094001
Iteration 10000: Loss = -10766.445428784311
Iteration 10100: Loss = -10766.425856261174
Iteration 10200: Loss = -10766.41749645654
Iteration 10300: Loss = -10766.412787130143
Iteration 10400: Loss = -10766.409764460877
Iteration 10500: Loss = -10766.407634237356
Iteration 10600: Loss = -10766.41532924556
1
Iteration 10700: Loss = -10766.404799487358
Iteration 10800: Loss = -10766.403824654639
Iteration 10900: Loss = -10766.403327561155
Iteration 11000: Loss = -10766.402440629241
Iteration 11100: Loss = -10766.401877121376
Iteration 11200: Loss = -10766.40211795848
1
Iteration 11300: Loss = -10766.401043229851
Iteration 11400: Loss = -10766.403924813732
1
Iteration 11500: Loss = -10766.400381818976
Iteration 11600: Loss = -10766.400111447076
Iteration 11700: Loss = -10766.400181755222
1
Iteration 11800: Loss = -10766.399662417562
Iteration 11900: Loss = -10766.3994500441
Iteration 12000: Loss = -10766.399494529385
1
Iteration 12100: Loss = -10766.399130443213
Iteration 12200: Loss = -10766.399007098324
Iteration 12300: Loss = -10766.424163852722
1
Iteration 12400: Loss = -10766.398730885447
Iteration 12500: Loss = -10766.398645946385
Iteration 12600: Loss = -10766.41304286646
1
Iteration 12700: Loss = -10766.398453963306
Iteration 12800: Loss = -10766.41043878636
1
Iteration 12900: Loss = -10766.398508174725
2
Iteration 13000: Loss = -10766.39822335754
Iteration 13100: Loss = -10766.398270757873
1
Iteration 13200: Loss = -10766.398123256939
Iteration 13300: Loss = -10766.477645570476
1
Iteration 13400: Loss = -10766.397989829282
Iteration 13500: Loss = -10766.39889224772
1
Iteration 13600: Loss = -10766.397968742858
Iteration 13700: Loss = -10766.45948289716
1
Iteration 13800: Loss = -10766.401756575826
2
Iteration 13900: Loss = -10766.415620568974
3
Iteration 14000: Loss = -10766.40434722632
4
Iteration 14100: Loss = -10766.596283157958
5
Iteration 14200: Loss = -10766.47806102636
6
Iteration 14300: Loss = -10766.39765185162
Iteration 14400: Loss = -10766.397740727258
1
Iteration 14500: Loss = -10766.397632043097
Iteration 14600: Loss = -10766.398056300242
1
Iteration 14700: Loss = -10766.399286246986
2
Iteration 14800: Loss = -10766.398399252354
3
Iteration 14900: Loss = -10766.398865455067
4
Iteration 15000: Loss = -10766.39768502419
5
Iteration 15100: Loss = -10766.39754023483
Iteration 15200: Loss = -10766.408525873327
1
Iteration 15300: Loss = -10766.39739303689
Iteration 15400: Loss = -10766.397482607623
1
Iteration 15500: Loss = -10766.417545829201
2
Iteration 15600: Loss = -10766.39762569828
3
Iteration 15700: Loss = -10766.397377864065
Iteration 15800: Loss = -10766.397517068317
1
Iteration 15900: Loss = -10766.47905388379
2
Iteration 16000: Loss = -10766.452276124308
3
Iteration 16100: Loss = -10766.415487906457
4
Iteration 16200: Loss = -10766.403443452136
5
Iteration 16300: Loss = -10766.404974015577
6
Iteration 16400: Loss = -10766.401961638627
7
Iteration 16500: Loss = -10766.398112046742
8
Iteration 16600: Loss = -10766.397312347397
Iteration 16700: Loss = -10766.401843026522
1
Iteration 16800: Loss = -10766.39732417846
2
Iteration 16900: Loss = -10766.39771049215
3
Iteration 17000: Loss = -10766.403324096917
4
Iteration 17100: Loss = -10766.397279836268
Iteration 17200: Loss = -10766.427517720243
1
Iteration 17300: Loss = -10766.39720698171
Iteration 17400: Loss = -10766.398738121676
1
Iteration 17500: Loss = -10766.412022532975
2
Iteration 17600: Loss = -10766.397793361832
3
Iteration 17700: Loss = -10766.39742621072
4
Iteration 17800: Loss = -10766.40079097606
5
Iteration 17900: Loss = -10766.40000619416
6
Iteration 18000: Loss = -10766.398917107794
7
Iteration 18100: Loss = -10766.451912406186
8
Iteration 18200: Loss = -10766.482601790098
9
Iteration 18300: Loss = -10766.397832535265
10
Stopping early at iteration 18300 due to no improvement.
tensor([[-4.7195,  3.3313],
        [-5.6276,  4.0426],
        [-3.4388,  1.6630],
        [-3.4550,  1.9186],
        [-3.2146,  1.8219],
        [-5.7643,  3.5625],
        [-5.3633,  2.4461],
        [-3.1687,  1.7702],
        [-4.2345,  2.3562],
        [-3.7527,  2.1967],
        [-4.7574,  3.0278],
        [-3.7923,  2.2003],
        [-5.8917,  4.0507],
        [-5.3645,  3.9711],
        [-3.5747,  2.1884],
        [ 0.7606, -2.1516],
        [-2.5509,  1.0820],
        [-3.3796,  1.9785],
        [-4.2479,  2.8615],
        [-5.9770,  3.2278],
        [-5.1819,  0.5666],
        [-4.5949,  3.0914],
        [-8.9834,  7.5962],
        [-3.1152,  1.7270],
        [-9.1910,  7.7992],
        [-7.1754,  5.4580],
        [-4.8758,  3.0646],
        [-5.9244,  4.2152],
        [-3.5468,  2.1578],
        [-5.5004,  4.0496],
        [-5.8072,  3.4958],
        [-3.5683,  1.8501],
        [-4.8601,  2.7244],
        [-2.3149,  0.5522],
        [-4.2730,  2.8252],
        [-5.0160,  3.6213],
        [-4.8426,  2.0973],
        [-5.2329,  3.7189],
        [-7.5090,  5.6352],
        [-3.5804,  1.4815],
        [-6.4754,  5.0540],
        [-4.0983,  2.6413],
        [-9.9138,  7.1700],
        [-8.6531,  5.2673],
        [-6.5855,  4.5936],
        [-7.6344,  4.1896],
        [-3.2998,  1.9134],
        [-3.5992,  2.1690],
        [-3.3218,  0.5764],
        [-0.9333, -2.3835],
        [-3.0644,  1.4532],
        [-7.5013,  3.1534],
        [-7.4864,  6.0015],
        [-3.2321,  1.5003],
        [-5.8027,  4.4122],
        [-5.7158,  3.7987],
        [-5.3752,  3.8955],
        [ 1.4269, -2.8161],
        [-4.6757,  2.7426],
        [-9.8489,  8.0160],
        [-5.2496,  3.6469],
        [-6.7775,  4.4140],
        [-4.4765,  3.0599],
        [-5.3368,  3.6338],
        [-3.0318,  1.1449],
        [-2.5748,  1.1325],
        [-7.4397,  6.0530],
        [-4.5427,  3.1229],
        [-3.6307,  1.6083],
        [-3.6051,  2.2011],
        [-9.1730,  7.5969],
        [-3.8911,  2.3501],
        [-5.7705,  2.3442],
        [-4.0353,  2.4563],
        [-4.5332,  3.0861],
        [-5.0577,  2.6633],
        [-1.0256, -0.3708],
        [-3.9394,  2.2982],
        [-5.5115,  3.1912],
        [-3.7117,  2.2349],
        [-5.0792,  2.5512],
        [-9.2036,  7.5248],
        [-3.9389,  2.2146],
        [-5.2642,  1.6750],
        [-5.0958,  3.6423],
        [-2.1752,  0.7861],
        [-4.0647,  1.8345],
        [-4.4920,  3.0713],
        [-5.5187,  4.1082],
        [-0.1202, -1.6114],
        [-3.6847,  2.1049],
        [-7.1711,  3.3064],
        [-4.0507,  2.6622],
        [-5.2174,  3.8091],
        [-5.0339,  3.1377],
        [-6.4879,  4.9364],
        [-5.6597,  3.4561],
        [-5.5035,  3.3633],
        [-1.4255, -0.6673],
        [-6.5732,  4.7780]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.7983e-06, 1.0000e+00],
        [1.0000e+00, 1.0259e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0456, 0.9544], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1528, 0.2060],
         [0.6845, 0.1601]],

        [[0.9536, 0.2221],
         [0.4670, 0.1756]],

        [[0.4211, 0.1009],
         [0.7623, 0.4101]],

        [[0.7458, 0.1195],
         [0.7976, 0.3335]],

        [[0.5076, 0.1435],
         [0.8817, 0.4921]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 4.85601903559462e-05
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.003506908785360844
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0014922741295917668
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 4.85601903559462e-05
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0015169269193791859
Global Adjusted Rand Index: -0.0017162252200383585
Average Adjusted Rand Index: 0.0007257363911720311
Iteration 0: Loss = -22969.15851306878
Iteration 10: Loss = -10771.961047171546
Iteration 20: Loss = -10771.961053984345
1
Iteration 30: Loss = -10771.961081542435
2
Iteration 40: Loss = -10771.96115672137
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[3.8101e-12, 1.0000e+00],
        [1.3097e-08, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([1.2739e-08, 1.0000e+00])
beta: tensor([[[0.1960, 0.1814],
         [0.2702, 0.1558]],

        [[0.8330, 0.2696],
         [0.0130, 0.1971]],

        [[0.4647, 0.2090],
         [0.6010, 0.4723]],

        [[0.8459, 0.2544],
         [0.6922, 0.9117]],

        [[0.4516, 0.1447],
         [0.6685, 0.1099]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22968.319059520152
Iteration 100: Loss = -10774.565524107647
Iteration 200: Loss = -10772.77756545533
Iteration 300: Loss = -10772.336299641993
Iteration 400: Loss = -10772.121966422921
Iteration 500: Loss = -10771.990585829755
Iteration 600: Loss = -10771.89975099361
Iteration 700: Loss = -10771.8351879386
Iteration 800: Loss = -10771.787512667459
Iteration 900: Loss = -10771.749103650454
Iteration 1000: Loss = -10771.716508639827
Iteration 1100: Loss = -10771.687507188459
Iteration 1200: Loss = -10771.660679109265
Iteration 1300: Loss = -10771.634844719316
Iteration 1400: Loss = -10771.608697272983
Iteration 1500: Loss = -10771.581218314284
Iteration 1600: Loss = -10771.551611375811
Iteration 1700: Loss = -10771.519582317424
Iteration 1800: Loss = -10771.486959046617
Iteration 1900: Loss = -10771.456772278783
Iteration 2000: Loss = -10771.430558322378
Iteration 2100: Loss = -10771.40694404427
Iteration 2200: Loss = -10771.383695892293
Iteration 2300: Loss = -10771.35835354239
Iteration 2400: Loss = -10771.324189889381
Iteration 2500: Loss = -10771.257324015045
Iteration 2600: Loss = -10771.219300628725
Iteration 2700: Loss = -10771.17935283101
Iteration 2800: Loss = -10771.137007739919
Iteration 2900: Loss = -10771.093292803473
Iteration 3000: Loss = -10771.050268657522
Iteration 3100: Loss = -10771.010675029785
Iteration 3200: Loss = -10770.97620278487
Iteration 3300: Loss = -10770.947495314687
Iteration 3400: Loss = -10770.92261438989
Iteration 3500: Loss = -10770.902613858336
Iteration 3600: Loss = -10770.883430351016
Iteration 3700: Loss = -10770.872008674454
Iteration 3800: Loss = -10770.847488913876
Iteration 3900: Loss = -10770.825813061001
Iteration 4000: Loss = -10770.816381517607
Iteration 4100: Loss = -10770.7854404229
Iteration 4200: Loss = -10770.764000112942
Iteration 4300: Loss = -10770.743305641932
Iteration 4400: Loss = -10770.724442592245
Iteration 4500: Loss = -10770.708195631862
Iteration 4600: Loss = -10770.695022912307
Iteration 4700: Loss = -10770.68469682035
Iteration 4800: Loss = -10770.6767320187
Iteration 4900: Loss = -10770.670579071711
Iteration 5000: Loss = -10770.666541188242
Iteration 5100: Loss = -10770.66211160479
Iteration 5200: Loss = -10770.659259134845
Iteration 5300: Loss = -10770.657167600082
Iteration 5400: Loss = -10770.65518522279
Iteration 5500: Loss = -10770.654756787926
Iteration 5600: Loss = -10770.659753749642
1
Iteration 5700: Loss = -10770.653993535183
Iteration 5800: Loss = -10770.659407212805
1
Iteration 5900: Loss = -10770.652447681798
Iteration 6000: Loss = -10770.668409997379
1
Iteration 6100: Loss = -10770.648964318709
Iteration 6200: Loss = -10770.648428386687
Iteration 6300: Loss = -10770.653413201362
1
Iteration 6400: Loss = -10770.64717869206
Iteration 6500: Loss = -10770.646472737482
Iteration 6600: Loss = -10770.645392974135
Iteration 6700: Loss = -10770.644112153761
Iteration 6800: Loss = -10770.64624729063
1
Iteration 6900: Loss = -10770.639335349788
Iteration 7000: Loss = -10770.634267614758
Iteration 7100: Loss = -10770.625182033049
Iteration 7200: Loss = -10770.607532444177
Iteration 7300: Loss = -10770.581269392751
Iteration 7400: Loss = -10770.558943917415
Iteration 7500: Loss = -10770.55159528018
Iteration 7600: Loss = -10770.541443136843
Iteration 7700: Loss = -10770.533595455874
Iteration 7800: Loss = -10770.516939492001
Iteration 7900: Loss = -10770.44429695634
Iteration 8000: Loss = -10766.846192198704
Iteration 8100: Loss = -10766.475499727338
Iteration 8200: Loss = -10766.448843149186
Iteration 8300: Loss = -10766.437020823128
Iteration 8400: Loss = -10766.430230626278
Iteration 8500: Loss = -10766.42618081635
Iteration 8600: Loss = -10766.42031063201
Iteration 8700: Loss = -10766.4285810854
1
Iteration 8800: Loss = -10766.415809797722
Iteration 8900: Loss = -10766.414376044904
Iteration 9000: Loss = -10766.423095307222
1
Iteration 9100: Loss = -10766.412207795993
Iteration 9200: Loss = -10766.411414039792
Iteration 9300: Loss = -10766.419686907277
1
Iteration 9400: Loss = -10766.462978429243
2
Iteration 9500: Loss = -10766.409466109773
Iteration 9600: Loss = -10766.4546917135
1
Iteration 9700: Loss = -10766.408802929343
Iteration 9800: Loss = -10766.408912125657
1
Iteration 9900: Loss = -10766.412881363123
2
Iteration 10000: Loss = -10766.464531368605
3
Iteration 10100: Loss = -10766.406352299698
Iteration 10200: Loss = -10766.42209959051
1
Iteration 10300: Loss = -10766.403963406441
Iteration 10400: Loss = -10766.40403378269
1
Iteration 10500: Loss = -10766.403246388767
Iteration 10600: Loss = -10766.403620061772
1
Iteration 10700: Loss = -10766.402585514903
Iteration 10800: Loss = -10766.406066699163
1
Iteration 10900: Loss = -10766.442486217267
2
Iteration 11000: Loss = -10766.402172177277
Iteration 11100: Loss = -10766.402143118912
Iteration 11200: Loss = -10766.414749734695
1
Iteration 11300: Loss = -10766.401810325786
Iteration 11400: Loss = -10766.449509499453
1
Iteration 11500: Loss = -10766.464366436263
2
Iteration 11600: Loss = -10766.401531127907
Iteration 11700: Loss = -10766.530012915717
1
Iteration 11800: Loss = -10766.401435740605
Iteration 11900: Loss = -10766.401984886857
1
Iteration 12000: Loss = -10766.401670874493
2
Iteration 12100: Loss = -10766.477340426833
3
Iteration 12200: Loss = -10766.401151353366
Iteration 12300: Loss = -10766.405913898907
1
Iteration 12400: Loss = -10766.401337449084
2
Iteration 12500: Loss = -10766.401115145974
Iteration 12600: Loss = -10766.406302796155
1
Iteration 12700: Loss = -10766.400992954219
Iteration 12800: Loss = -10766.401165255586
1
Iteration 12900: Loss = -10766.40121797624
2
Iteration 13000: Loss = -10766.401100388017
3
Iteration 13100: Loss = -10766.400777873992
Iteration 13200: Loss = -10766.414891510722
1
Iteration 13300: Loss = -10766.467515768603
2
Iteration 13400: Loss = -10766.465433834346
3
Iteration 13500: Loss = -10766.400654252988
Iteration 13600: Loss = -10766.409460868857
1
Iteration 13700: Loss = -10766.412119918179
2
Iteration 13800: Loss = -10766.402199826458
3
Iteration 13900: Loss = -10766.402243572564
4
Iteration 14000: Loss = -10766.40806718334
5
Iteration 14100: Loss = -10766.53670047995
6
Iteration 14200: Loss = -10766.402219096555
7
Iteration 14300: Loss = -10766.401061769075
8
Iteration 14400: Loss = -10766.401642603925
9
Iteration 14500: Loss = -10766.478034737784
10
Stopping early at iteration 14500 due to no improvement.
tensor([[ 3.3348e+00, -4.7214e+00],
        [ 4.0658e+00, -5.6061e+00],
        [ 8.3327e-01, -4.2814e+00],
        [ 1.9804e+00, -3.3906e+00],
        [ 1.6422e+00, -3.3909e+00],
        [ 3.9752e+00, -5.3619e+00],
        [ 2.2847e+00, -5.5311e+00],
        [ 1.7152e+00, -3.2335e+00],
        [ 2.2657e+00, -4.3239e+00],
        [ 2.1187e+00, -3.8310e+00],
        [ 2.8788e+00, -4.9108e+00],
        [ 1.9373e+00, -4.0578e+00],
        [ 4.1300e+00, -5.8330e+00],
        [ 3.8068e+00, -5.5388e+00],
        [ 1.9839e+00, -3.7786e+00],
        [-2.4597e+00,  4.4842e-01],
        [ 6.2013e-01, -3.0044e+00],
        [ 1.9032e+00, -3.4566e+00],
        [ 2.8419e+00, -4.2679e+00],
        [ 3.5434e+00, -5.6727e+00],
        [ 2.1345e+00, -3.6233e+00],
        [ 3.0771e+00, -4.6116e+00],
        [ 6.8344e+00, -8.3247e+00],
        [ 1.1374e-01, -4.7290e+00],
        [ 6.6170e+00, -8.0402e+00],
        [ 5.6617e+00, -7.2587e+00],
        [ 2.3969e+00, -5.5493e+00],
        [ 3.5198e+00, -6.6413e+00],
        [ 2.1347e+00, -3.5663e+00],
        [ 3.9042e+00, -5.6587e+00],
        [ 3.8123e+00, -5.5002e+00],
        [ 1.9797e+00, -3.4333e+00],
        [ 3.1027e+00, -4.4890e+00],
        [-1.8261e-01, -3.0425e+00],
        [ 6.1922e+00, -7.7867e+00],
        [ 3.0790e+00, -5.5695e+00],
        [ 2.7394e+00, -4.2008e+00],
        [ 3.1359e+00, -5.8215e+00],
        [ 6.1738e+00, -8.9152e+00],
        [ 1.3438e+00, -3.7124e+00],
        [ 6.5896e+00, -9.0785e+00],
        [ 2.3156e+00, -4.4292e+00],
        [ 6.7061e+00, -8.3284e+00],
        [ 6.1879e+00, -7.8461e+00],
        [ 4.8846e+00, -6.3164e+00],
        [ 6.7280e+00, -8.2118e+00],
        [ 1.8201e+00, -3.3876e+00],
        [ 2.1851e+00, -3.5788e+00],
        [ 4.0470e-01, -3.4952e+00],
        [-2.1144e+00, -6.5398e-01],
        [ 1.3842e+00, -3.1369e+00],
        [ 6.5191e+00, -8.9032e+00],
        [ 5.9732e+00, -7.4754e+00],
        [ 1.5777e+00, -3.1447e+00],
        [ 4.2146e+00, -6.0525e+00],
        [ 3.9324e+00, -5.5996e+00],
        [ 3.7765e+00, -5.5110e+00],
        [-4.3173e+00, -6.8290e-02],
        [ 2.8942e+00, -4.5282e+00],
        [ 7.0879e+00, -8.4870e+00],
        [ 3.0127e+00, -5.8933e+00],
        [ 4.9015e+00, -6.3468e+00],
        [ 3.0733e+00, -4.4658e+00],
        [ 3.6843e+00, -5.3040e+00],
        [ 1.3884e+00, -2.7847e+00],
        [ 7.9150e-01, -2.9096e+00],
        [ 5.6755e+00, -7.9369e+00],
        [ 3.1255e+00, -4.5432e+00],
        [ 1.7066e+00, -3.5240e+00],
        [ 1.5669e+00, -4.2332e+00],
        [ 6.3017e+00, -9.4133e+00],
        [ 1.1866e+00, -5.0528e+00],
        [ 2.9351e+00, -5.1918e+00],
        [ 2.3494e+00, -4.1403e+00],
        [ 3.1128e+00, -4.5099e+00],
        [ 3.1683e+00, -4.5569e+00],
        [-3.9548e-01, -1.0393e+00],
        [ 6.7453e+00, -8.8365e+00],
        [ 3.4218e+00, -5.2804e+00],
        [ 2.1416e+00, -3.8017e+00],
        [ 3.0380e+00, -4.5969e+00],
        [ 6.2170e+00, -9.4023e+00],
        [ 2.1642e+00, -3.9887e+00],
        [ 2.4392e+00, -4.5008e+00],
        [ 6.2171e+00, -7.9242e+00],
        [ 7.1800e-01, -2.2307e+00],
        [ 2.2554e+00, -3.6449e+00],
        [ 2.7050e+00, -4.8612e+00],
        [ 4.1107e+00, -5.5411e+00],
        [-1.4987e+00, -7.9831e-04],
        [ 1.1041e+00, -4.6798e+00],
        [ 4.5549e+00, -5.9527e+00],
        [ 2.6112e+00, -4.1056e+00],
        [ 5.5762e+00, -9.6920e+00],
        [ 2.6456e+00, -5.5276e+00],
        [ 3.9661e+00, -7.4747e+00],
        [ 3.1085e+00, -6.0114e+00],
        [ 3.4572e+00, -5.4133e+00],
        [-4.0705e-01, -1.1568e+00],
        [ 4.9560e+00, -6.4838e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.3089e-06, 1.0000e+00],
        [9.9999e-01, 5.4948e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9551, 0.0449], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1593, 0.2066],
         [0.2702, 0.1526]],

        [[0.8330, 0.2235],
         [0.0130, 0.1971]],

        [[0.4647, 0.1008],
         [0.6010, 0.4723]],

        [[0.8459, 0.1192],
         [0.6922, 0.9117]],

        [[0.4516, 0.1436],
         [0.6685, 0.1099]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 4.85601903559462e-05
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.003506908785360844
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0014922741295917668
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 4.85601903559462e-05
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0015169269193791859
Global Adjusted Rand Index: -0.0017162252200383585
Average Adjusted Rand Index: 0.0007257363911720311
Iteration 0: Loss = -19214.712684341604
Iteration 10: Loss = -10771.961048057809
Iteration 20: Loss = -10771.961054562795
1
Iteration 30: Loss = -10771.961084668728
2
Iteration 40: Loss = -10771.961183823467
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[2.4951e-12, 1.0000e+00],
        [1.7280e-08, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([1.6808e-08, 1.0000e+00])
beta: tensor([[[0.1961, 0.1814],
         [0.7960, 0.1558]],

        [[0.7271, 0.2696],
         [0.0605, 0.0647]],

        [[0.5964, 0.2090],
         [0.1488, 0.6727]],

        [[0.5193, 0.2544],
         [0.2338, 0.7736]],

        [[0.1279, 0.1449],
         [0.7488, 0.0996]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19214.00266817214
Iteration 100: Loss = -10773.418253180931
Iteration 200: Loss = -10772.347979378808
Iteration 300: Loss = -10771.961304327511
Iteration 400: Loss = -10771.79005364845
Iteration 500: Loss = -10771.689384075627
Iteration 600: Loss = -10771.616109965456
Iteration 700: Loss = -10771.560246841113
Iteration 800: Loss = -10771.50211423012
Iteration 900: Loss = -10771.453842001847
Iteration 1000: Loss = -10771.410974641314
Iteration 1100: Loss = -10771.378374831167
Iteration 1200: Loss = -10771.344400807102
Iteration 1300: Loss = -10771.320190166169
Iteration 1400: Loss = -10771.300043026968
Iteration 1500: Loss = -10771.28424109636
Iteration 1600: Loss = -10771.267736681228
Iteration 1700: Loss = -10771.253066990754
Iteration 1800: Loss = -10771.263389768752
1
Iteration 1900: Loss = -10771.222837131885
Iteration 2000: Loss = -10771.206174592768
Iteration 2100: Loss = -10771.187626892677
Iteration 2200: Loss = -10771.167447832398
Iteration 2300: Loss = -10771.143658890202
Iteration 2400: Loss = -10771.117871836726
Iteration 2500: Loss = -10771.0970043412
Iteration 2600: Loss = -10771.059399565005
Iteration 2700: Loss = -10771.028718498525
Iteration 2800: Loss = -10770.998435297255
Iteration 2900: Loss = -10770.970432224207
Iteration 3000: Loss = -10770.941511823083
Iteration 3100: Loss = -10770.916174267517
Iteration 3200: Loss = -10770.893075478216
Iteration 3300: Loss = -10770.87249160733
Iteration 3400: Loss = -10770.852355198589
Iteration 3500: Loss = -10770.834309770245
Iteration 3600: Loss = -10770.933618466346
1
Iteration 3700: Loss = -10770.800323059264
Iteration 3800: Loss = -10770.78425389159
Iteration 3900: Loss = -10770.768550831343
Iteration 4000: Loss = -10770.797596621032
1
Iteration 4100: Loss = -10770.737354410308
Iteration 4200: Loss = -10770.721965365077
Iteration 4300: Loss = -10770.706398448101
Iteration 4400: Loss = -10770.689963899582
Iteration 4500: Loss = -10770.673629496556
Iteration 4600: Loss = -10770.644534136345
Iteration 4700: Loss = -10770.582783654605
Iteration 4800: Loss = -10768.988282801542
Iteration 4900: Loss = -10767.443073604982
Iteration 5000: Loss = -10767.326858780789
Iteration 5100: Loss = -10767.278911465222
Iteration 5200: Loss = -10755.191078584648
Iteration 5300: Loss = -10678.183226374902
Iteration 5400: Loss = -10677.20227154414
Iteration 5500: Loss = -10677.047261674548
Iteration 5600: Loss = -10676.706551503727
Iteration 5700: Loss = -10676.672976233574
Iteration 5800: Loss = -10675.731128457412
Iteration 5900: Loss = -10675.717192267708
Iteration 6000: Loss = -10675.663666249353
Iteration 6100: Loss = -10675.58313910726
Iteration 6200: Loss = -10675.562202843894
Iteration 6300: Loss = -10675.532064385825
Iteration 6400: Loss = -10675.492549098122
Iteration 6500: Loss = -10675.479479828307
Iteration 6600: Loss = -10675.471781487848
Iteration 6700: Loss = -10675.468555950485
Iteration 6800: Loss = -10675.44992262427
Iteration 6900: Loss = -10675.448594474648
Iteration 7000: Loss = -10675.44407351805
Iteration 7100: Loss = -10675.442344066863
Iteration 7200: Loss = -10675.441812614315
Iteration 7300: Loss = -10675.441133307799
Iteration 7400: Loss = -10675.441107848748
Iteration 7500: Loss = -10675.443878767604
1
Iteration 7600: Loss = -10675.483217471949
2
Iteration 7700: Loss = -10675.450478554141
3
Iteration 7800: Loss = -10675.4453685377
4
Iteration 7900: Loss = -10675.453730955489
5
Iteration 8000: Loss = -10675.499397548563
6
Iteration 8100: Loss = -10675.435719706276
Iteration 8200: Loss = -10675.437279737398
1
Iteration 8300: Loss = -10675.433903905765
Iteration 8400: Loss = -10675.475267239644
1
Iteration 8500: Loss = -10675.433208081911
Iteration 8600: Loss = -10675.431519753522
Iteration 8700: Loss = -10675.348483856582
Iteration 8800: Loss = -10675.348175136578
Iteration 8900: Loss = -10675.350622900869
1
Iteration 9000: Loss = -10675.276271098626
Iteration 9100: Loss = -10675.276053620317
Iteration 9200: Loss = -10675.276353737258
1
Iteration 9300: Loss = -10675.275693238245
Iteration 9400: Loss = -10675.275517747818
Iteration 9500: Loss = -10675.275196003096
Iteration 9600: Loss = -10675.274695789565
Iteration 9700: Loss = -10675.27392242801
Iteration 9800: Loss = -10675.273679068108
Iteration 9900: Loss = -10675.272995457373
Iteration 10000: Loss = -10675.270751304633
Iteration 10100: Loss = -10675.26840504665
Iteration 10200: Loss = -10675.267886773046
Iteration 10300: Loss = -10675.186222300777
Iteration 10400: Loss = -10675.15553103333
Iteration 10500: Loss = -10675.155517798765
Iteration 10600: Loss = -10675.15482182639
Iteration 10700: Loss = -10675.154873733978
1
Iteration 10800: Loss = -10675.154640254168
Iteration 10900: Loss = -10675.189977270076
1
Iteration 11000: Loss = -10675.154029240048
Iteration 11100: Loss = -10675.151980367085
Iteration 11200: Loss = -10675.150266045901
Iteration 11300: Loss = -10675.149307345657
Iteration 11400: Loss = -10675.164049291945
1
Iteration 11500: Loss = -10675.147842060254
Iteration 11600: Loss = -10675.153446575127
1
Iteration 11700: Loss = -10675.147663271
Iteration 11800: Loss = -10675.149167569301
1
Iteration 11900: Loss = -10675.147953000187
2
Iteration 12000: Loss = -10675.142998821693
Iteration 12100: Loss = -10675.021274501078
Iteration 12200: Loss = -10675.02211891082
1
Iteration 12300: Loss = -10675.002549814504
Iteration 12400: Loss = -10675.002789309336
1
Iteration 12500: Loss = -10675.024891889272
2
Iteration 12600: Loss = -10674.997095115896
Iteration 12700: Loss = -10674.998156815158
1
Iteration 12800: Loss = -10675.00023872495
2
Iteration 12900: Loss = -10674.996985989132
Iteration 13000: Loss = -10675.236087256131
1
Iteration 13100: Loss = -10674.996922874861
Iteration 13200: Loss = -10674.997109645565
1
Iteration 13300: Loss = -10674.996849631138
Iteration 13400: Loss = -10675.028319558513
1
Iteration 13500: Loss = -10674.993182102135
Iteration 13600: Loss = -10675.000964792167
1
Iteration 13700: Loss = -10674.995541802058
2
Iteration 13800: Loss = -10674.992872467998
Iteration 13900: Loss = -10675.009324250539
1
Iteration 14000: Loss = -10674.993624663313
2
Iteration 14100: Loss = -10674.99249840661
Iteration 14200: Loss = -10675.01031312308
1
Iteration 14300: Loss = -10674.990008704986
Iteration 14400: Loss = -10674.995988527637
1
Iteration 14500: Loss = -10674.969170241033
Iteration 14600: Loss = -10674.962286457181
Iteration 14700: Loss = -10674.96229292013
1
Iteration 14800: Loss = -10674.96199800825
Iteration 14900: Loss = -10674.96300799662
1
Iteration 15000: Loss = -10674.961654699053
Iteration 15100: Loss = -10674.963448744069
1
Iteration 15200: Loss = -10674.961421549486
Iteration 15300: Loss = -10674.966373306635
1
Iteration 15400: Loss = -10675.053487207248
2
Iteration 15500: Loss = -10674.961268998224
Iteration 15600: Loss = -10674.963382771559
1
Iteration 15700: Loss = -10674.961259134985
Iteration 15800: Loss = -10674.961738310689
1
Iteration 15900: Loss = -10674.961275699488
2
Iteration 16000: Loss = -10674.962487470559
3
Iteration 16100: Loss = -10674.962150661804
4
Iteration 16200: Loss = -10674.961492452234
5
Iteration 16300: Loss = -10674.964747353857
6
Iteration 16400: Loss = -10674.96332082367
7
Iteration 16500: Loss = -10674.99860805873
8
Iteration 16600: Loss = -10674.960555494083
Iteration 16700: Loss = -10674.960540951088
Iteration 16800: Loss = -10674.971609093556
1
Iteration 16900: Loss = -10674.958799574993
Iteration 17000: Loss = -10674.958914551486
1
Iteration 17100: Loss = -10674.959470390206
2
Iteration 17200: Loss = -10674.994110565407
3
Iteration 17300: Loss = -10674.958685415739
Iteration 17400: Loss = -10674.958889903388
1
Iteration 17500: Loss = -10674.958621742826
Iteration 17600: Loss = -10674.958815690821
1
Iteration 17700: Loss = -10674.958073723152
Iteration 17800: Loss = -10675.014837934621
1
Iteration 17900: Loss = -10674.95804089595
Iteration 18000: Loss = -10674.969927415665
1
Iteration 18100: Loss = -10674.960277137952
2
Iteration 18200: Loss = -10674.96986798499
3
Iteration 18300: Loss = -10674.958266265956
4
Iteration 18400: Loss = -10674.958967195262
5
Iteration 18500: Loss = -10674.961423736884
6
Iteration 18600: Loss = -10674.96108645401
7
Iteration 18700: Loss = -10674.973783255411
8
Iteration 18800: Loss = -10674.963258406518
9
Iteration 18900: Loss = -10674.961250372533
10
Stopping early at iteration 18900 due to no improvement.
tensor([[ -4.9952,   3.5904],
        [  5.2496,  -7.5889],
        [ -4.8972,   2.5949],
        [ -1.5683,   0.0300],
        [ -4.4454,  -0.1698],
        [ -3.2056,   1.7667],
        [ -4.8728,   3.4101],
        [ -3.7960,   2.4093],
        [ -2.9861,   1.4214],
        [  0.5976,  -2.0835],
        [  3.0764,  -5.6319],
        [ -3.3306,   1.3523],
        [  4.6496,  -6.3391],
        [ -3.5931,   0.7669],
        [ -4.2976,   2.6653],
        [  1.3713,  -2.7864],
        [  0.9885,  -2.3767],
        [  0.4526,  -1.9759],
        [  0.2191,  -1.6227],
        [  2.9333,  -4.3658],
        [  5.4539,  -7.2302],
        [ -0.7290,  -0.6589],
        [  0.6636,  -2.0706],
        [ -3.5760,   2.1819],
        [ -3.4099,   1.2893],
        [  1.3386,  -4.4900],
        [  4.2247,  -5.6361],
        [  3.5485,  -5.5706],
        [  1.2229,  -3.2055],
        [ -4.4293,   2.5105],
        [ -2.8817,   1.4947],
        [ -3.3288,   1.1443],
        [ -6.0059,   4.5989],
        [ -1.7586,   0.0875],
        [  2.5809,  -4.1478],
        [ -5.8494,   4.0120],
        [ -0.5082,  -3.0179],
        [  1.6258,  -3.7037],
        [  5.1840,  -8.3888],
        [  8.5142, -10.3840],
        [  1.5720,  -3.3081],
        [ -6.1411,   4.1552],
        [ -0.5175,  -1.6713],
        [ -2.9313,   0.8543],
        [  1.5662,  -2.9538],
        [  1.2236,  -2.9526],
        [  2.3414,  -3.8800],
        [ -4.2141,   2.5408],
        [  5.8732,  -7.2628],
        [  3.0291,  -4.5262],
        [  2.3034,  -5.0064],
        [ -6.0927,   4.3451],
        [  0.3020,  -1.8604],
        [ -2.4203,  -0.3546],
        [  1.6482,  -3.0426],
        [  0.3654,  -1.9306],
        [ -3.5619,   2.1747],
        [  2.5708,  -5.8535],
        [  4.3389,  -5.8056],
        [ -3.7562,   1.3986],
        [  2.0663,  -4.8429],
        [  0.3087,  -1.7421],
        [  1.2826,  -2.8030],
        [ -0.5060,  -4.0031],
        [  1.9356,  -3.3778],
        [ -1.9345,   0.4371],
        [  2.6001,  -4.1994],
        [  2.5000,  -3.8986],
        [  0.2395,  -2.5167],
        [  1.4079,  -3.3150],
        [ -7.7555,   3.3951],
        [  3.0851,  -4.5135],
        [ -3.4726,   1.1948],
        [ -4.3595,   2.7532],
        [  0.2197,  -1.7182],
        [ -3.8444,   2.4390],
        [ -6.7372,   2.1220],
        [  4.0963,  -5.5073],
        [ -4.8427,   3.3777],
        [  3.1860,  -4.6079],
        [  3.2801,  -4.8345],
        [ -5.2511,   3.2363],
        [ -2.6150,   1.2185],
        [ -4.1535,   1.7049],
        [ -4.3403,   2.0952],
        [ -2.0514,   0.6531],
        [  8.4013, -10.2471],
        [  2.1259,  -3.5135],
        [ -7.3869,   5.8229],
        [  2.9287,  -4.7221],
        [  3.0001,  -5.0935],
        [ -2.6358,   0.9622],
        [ -2.7069,   0.9794],
        [ -5.2597,   1.7749],
        [  1.9511,  -4.0004],
        [  1.9239,  -3.7792],
        [ -4.2772,   2.0376],
        [  2.6841,  -5.1457],
        [  7.2695,  -8.7121],
        [  1.4036,  -2.8107]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7271, 0.2729],
        [0.2385, 0.7615]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5594, 0.4406], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2333, 0.0997],
         [0.7960, 0.1998]],

        [[0.7271, 0.1030],
         [0.0605, 0.0647]],

        [[0.5964, 0.0999],
         [0.1488, 0.6727]],

        [[0.5193, 0.0889],
         [0.2338, 0.7736]],

        [[0.1279, 0.0977],
         [0.7488, 0.0996]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721314419105764
time is 1
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721016799725718
time is 2
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7720997823854144
time is 3
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 15
Adjusted Rand Index: 0.48484848484848486
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 6
Adjusted Rand Index: 0.7720997823854144
Global Adjusted Rand Index: 0.7117589948073518
Average Adjusted Rand Index: 0.7146562343004924
Iteration 0: Loss = -28719.7444790584
Iteration 10: Loss = -10771.959681635772
Iteration 20: Loss = -10771.95439154828
Iteration 30: Loss = -10771.931605571282
Iteration 40: Loss = -10771.8704000777
Iteration 50: Loss = -10771.813205677587
Iteration 60: Loss = -10771.796476611658
Iteration 70: Loss = -10771.793372900522
Iteration 80: Loss = -10771.79268769212
Iteration 90: Loss = -10771.792465644388
Iteration 100: Loss = -10771.792380723859
Iteration 110: Loss = -10771.792416394783
1
Iteration 120: Loss = -10771.792350599559
Iteration 130: Loss = -10771.792385130919
1
Iteration 140: Loss = -10771.792363023444
2
Iteration 150: Loss = -10771.792399051526
3
Stopping early at iteration 149 due to no improvement.
pi: tensor([[9.9482e-01, 5.1837e-03],
        [1.0000e+00, 2.3728e-13]], dtype=torch.float64)
alpha: tensor([0.9948, 0.0052])
beta: tensor([[[0.1552, 0.1821],
         [0.9576, 0.2171]],

        [[0.7686, 0.2626],
         [0.9867, 0.1290]],

        [[0.2274, 0.2064],
         [0.6600, 0.4313]],

        [[0.4648, 0.2485],
         [0.6714, 0.0698]],

        [[0.6188, 0.1524],
         [0.9315, 0.2961]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28719.168897810458
Iteration 100: Loss = -10809.109008370315
Iteration 200: Loss = -10791.556592012834
Iteration 300: Loss = -10778.121651860343
Iteration 400: Loss = -10775.240339817226
Iteration 500: Loss = -10774.100071362205
Iteration 600: Loss = -10773.42807019664
Iteration 700: Loss = -10772.977550514855
Iteration 800: Loss = -10772.663630205212
Iteration 900: Loss = -10772.440265082556
Iteration 1000: Loss = -10772.281870348286
Iteration 1100: Loss = -10772.16679445743
Iteration 1200: Loss = -10772.078039537153
Iteration 1300: Loss = -10772.001611353378
Iteration 1400: Loss = -10771.91611011032
Iteration 1500: Loss = -10771.795064525422
Iteration 1600: Loss = -10771.706440406799
Iteration 1700: Loss = -10771.65470411195
Iteration 1800: Loss = -10771.625573872707
Iteration 1900: Loss = -10771.612213176084
Iteration 2000: Loss = -10771.602113907631
Iteration 2100: Loss = -10771.59298924726
Iteration 2200: Loss = -10771.58407606938
Iteration 2300: Loss = -10771.574894464742
Iteration 2400: Loss = -10771.564914010014
Iteration 2500: Loss = -10771.553966438018
Iteration 2600: Loss = -10771.543553470388
Iteration 2700: Loss = -10771.532555010854
Iteration 2800: Loss = -10771.520285477474
Iteration 2900: Loss = -10771.50659058727
Iteration 3000: Loss = -10771.491181164223
Iteration 3100: Loss = -10771.474316173184
Iteration 3200: Loss = -10771.458817186925
Iteration 3300: Loss = -10771.443360809388
Iteration 3400: Loss = -10771.427129477514
Iteration 3500: Loss = -10771.409786734768
Iteration 3600: Loss = -10771.391121122802
Iteration 3700: Loss = -10771.370924204813
Iteration 3800: Loss = -10771.348815467229
Iteration 3900: Loss = -10771.32412425542
Iteration 4000: Loss = -10771.296357339072
Iteration 4100: Loss = -10771.265430366204
Iteration 4200: Loss = -10771.231478239582
Iteration 4300: Loss = -10771.19541401589
Iteration 4400: Loss = -10771.158423051327
Iteration 4500: Loss = -10771.122334967049
Iteration 4600: Loss = -10771.088579013747
Iteration 4700: Loss = -10771.057835973801
Iteration 4800: Loss = -10771.030225810986
Iteration 4900: Loss = -10771.005430680978
Iteration 5000: Loss = -10770.98265666675
Iteration 5100: Loss = -10770.962046553495
Iteration 5200: Loss = -10770.94060136159
Iteration 5300: Loss = -10770.92047720642
Iteration 5400: Loss = -10770.900769157375
Iteration 5500: Loss = -10770.881559853276
Iteration 5600: Loss = -10770.862803483584
Iteration 5700: Loss = -10770.844528347821
Iteration 5800: Loss = -10770.826616616721
Iteration 5900: Loss = -10770.80852916572
Iteration 6000: Loss = -10770.789553609136
Iteration 6100: Loss = -10770.769251461348
Iteration 6200: Loss = -10770.745467671819
Iteration 6300: Loss = -10770.71920972161
Iteration 6400: Loss = -10770.68774472976
Iteration 6500: Loss = -10770.662465278238
Iteration 6600: Loss = -10770.62940451651
Iteration 6700: Loss = -10770.602621471622
Iteration 6800: Loss = -10770.586617463749
Iteration 6900: Loss = -10770.574292278856
Iteration 7000: Loss = -10770.567028640957
Iteration 7100: Loss = -10770.56188947289
Iteration 7200: Loss = -10770.558811124722
Iteration 7300: Loss = -10770.556948254694
Iteration 7400: Loss = -10770.5555640559
Iteration 7500: Loss = -10770.554584253348
Iteration 7600: Loss = -10770.555616353571
1
Iteration 7700: Loss = -10770.591731865476
2
Iteration 7800: Loss = -10770.551091495316
Iteration 7900: Loss = -10770.550440747702
Iteration 8000: Loss = -10770.549624431847
Iteration 8100: Loss = -10770.548944190572
Iteration 8200: Loss = -10770.5479964686
Iteration 8300: Loss = -10770.548003103226
1
Iteration 8400: Loss = -10770.547016155937
Iteration 8500: Loss = -10770.545666687436
Iteration 8600: Loss = -10770.545337647307
Iteration 8700: Loss = -10770.545407218639
1
Iteration 8800: Loss = -10770.626102045208
2
Iteration 8900: Loss = -10770.542580064579
Iteration 9000: Loss = -10770.541144848507
Iteration 9100: Loss = -10770.541792218055
1
Iteration 9200: Loss = -10770.583548803752
2
Iteration 9300: Loss = -10770.57827411224
3
Iteration 9400: Loss = -10770.531196694517
Iteration 9500: Loss = -10770.521694956715
Iteration 9600: Loss = -10770.502204440265
Iteration 9700: Loss = -10770.278866226969
Iteration 9800: Loss = -10766.551569586609
Iteration 9900: Loss = -10766.502260309451
Iteration 10000: Loss = -10766.48525350221
Iteration 10100: Loss = -10766.477450445927
Iteration 10200: Loss = -10766.47108500282
Iteration 10300: Loss = -10766.467370159939
Iteration 10400: Loss = -10766.465763968437
Iteration 10500: Loss = -10766.4629359696
Iteration 10600: Loss = -10766.587886644595
1
Iteration 10700: Loss = -10766.459757100234
Iteration 10800: Loss = -10766.481457105958
1
Iteration 10900: Loss = -10766.457818884888
Iteration 11000: Loss = -10766.465551820627
1
Iteration 11100: Loss = -10766.45637115378
Iteration 11200: Loss = -10766.468110951633
1
Iteration 11300: Loss = -10766.455433940673
Iteration 11400: Loss = -10766.461772216213
1
Iteration 11500: Loss = -10766.455531253796
2
Iteration 11600: Loss = -10766.518456702775
3
Iteration 11700: Loss = -10766.45386544449
Iteration 11800: Loss = -10766.455471621282
1
Iteration 11900: Loss = -10766.453264132973
Iteration 12000: Loss = -10766.465711500954
1
Iteration 12100: Loss = -10766.45286426466
Iteration 12200: Loss = -10766.66137627447
1
Iteration 12300: Loss = -10766.452451256633
Iteration 12400: Loss = -10766.45235735861
Iteration 12500: Loss = -10766.452523152138
1
Iteration 12600: Loss = -10766.45281921744
2
Iteration 12700: Loss = -10766.45217353307
Iteration 12800: Loss = -10766.45176677557
Iteration 12900: Loss = -10766.452464209244
1
Iteration 13000: Loss = -10766.45170848818
Iteration 13100: Loss = -10766.451594700928
Iteration 13200: Loss = -10766.456880366979
1
Iteration 13300: Loss = -10766.452249530088
2
Iteration 13400: Loss = -10766.451728204198
3
Iteration 13500: Loss = -10766.531782160755
4
Iteration 13600: Loss = -10766.451071064203
Iteration 13700: Loss = -10766.461081844527
1
Iteration 13800: Loss = -10766.459012815365
2
Iteration 13900: Loss = -10766.456602711693
3
Iteration 14000: Loss = -10766.462695333657
4
Iteration 14100: Loss = -10766.451790664596
5
Iteration 14200: Loss = -10766.454217292541
6
Iteration 14300: Loss = -10766.459546787923
7
Iteration 14400: Loss = -10766.450912970444
Iteration 14500: Loss = -10766.455015222782
1
Iteration 14600: Loss = -10766.456718198915
2
Iteration 14700: Loss = -10766.450534942256
Iteration 14800: Loss = -10766.451669985707
1
Iteration 14900: Loss = -10766.450530069289
Iteration 15000: Loss = -10766.450651800416
1
Iteration 15100: Loss = -10766.473943149565
2
Iteration 15200: Loss = -10766.451900970262
3
Iteration 15300: Loss = -10766.450676357153
4
Iteration 15400: Loss = -10766.473475128467
5
Iteration 15500: Loss = -10766.450995413623
6
Iteration 15600: Loss = -10766.452422904624
7
Iteration 15700: Loss = -10766.471742968582
8
Iteration 15800: Loss = -10766.459149991131
9
Iteration 15900: Loss = -10766.450815272616
10
Stopping early at iteration 15900 due to no improvement.
tensor([[ -8.9262,   6.4450],
        [ -6.1860,   3.6012],
        [ -3.2982,   1.9104],
        [ -3.4268,   2.0383],
        [ -9.1619,   6.5563],
        [-10.2743,   5.7677],
        [ -5.2407,   2.7089],
        [ -3.7207,   1.3030],
        [ -4.8206,   1.8706],
        [ -8.5494,   6.7420],
        [ -7.8661,   6.4678],
        [ -9.1486,   5.5890],
        [ -6.1132,   4.0022],
        [ -5.7946,   3.6792],
        [ -3.6592,   2.1834],
        [  0.6487,  -2.2736],
        [ -2.7273,   1.0097],
        [ -4.1444,   1.3212],
        [ -4.3158,   2.9087],
        [ -5.5374,   3.8075],
        [ -4.0349,   1.7809],
        [ -4.8100,   3.0004],
        [ -9.4271,   5.9865],
        [ -3.5354,   1.3938],
        [ -8.0316,   6.0705],
        [ -7.9827,   6.5857],
        [ -8.6286,   6.6046],
        [ -8.0947,   6.4522],
        [ -3.6317,   2.1588],
        [ -5.8408,   3.8532],
        [ -8.5548,   6.6315],
        [ -3.4940,   2.0485],
        [ -7.9944,   6.6072],
        [ -3.4224,  -0.4232],
        [ -7.5173,   6.0891],
        [ -7.8737,   6.4871],
        [ -4.2166,   2.8293],
        [ -5.4164,   3.6713],
        [ -8.8085,   6.5858],
        [ -3.9594,   1.1918],
        [ -8.5814,   6.6764],
        [ -9.1786,   5.1615],
        [ -9.2979,   5.4924],
        [ -7.9178,   6.2132],
        [ -6.5727,   4.7816],
        [ -7.8779,   6.4544],
        [ -7.6041,   6.1880],
        [ -7.9784,   6.4428],
        [ -2.9815,   0.9818],
        [ -0.1380,  -1.5621],
        [ -3.0459,   1.5178],
        [ -6.9583,   3.8554],
        [ -7.6818,   6.2556],
        [ -3.1244,   1.7166],
        [ -5.9083,   4.5218],
        [ -8.3450,   6.4613],
        [ -8.0541,   6.6475],
        [  1.3379,  -2.8793],
        [ -4.5310,   2.9921],
        [ -8.2998,   6.8329],
        [ -7.7896,   6.3929],
        [ -8.0307,   6.5494],
        [ -5.1928,   2.5152],
        [ -5.4597,   3.6744],
        [ -3.5074,   0.7776],
        [ -8.3250,   6.6402],
        [ -8.2736,   6.3395],
        [ -4.6855,   3.1260],
        [ -3.6363,   1.6915],
        [ -4.0264,   1.9632],
        [ -7.8929,   6.4892],
        [ -4.2152,   2.1443],
        [ -5.1324,   3.1477],
        [ -4.6608,   1.9379],
        [ -5.8581,   1.8634],
        [ -6.2266,   1.6114],
        [ -1.1380,  -0.3989],
        [ -8.9066,   5.5936],
        [ -5.1297,   3.6869],
        [ -4.0892,   1.9783],
        [ -7.9526,   6.4481],
        [ -8.1339,   6.7257],
        [ -3.7861,   2.3993],
        [ -7.6240,   6.1459],
        [ -7.9402,   6.4696],
        [ -3.1949,  -0.1348],
        [ -3.7072,   2.3026],
        [ -4.6807,   3.0062],
        [ -8.5730,   6.5176],
        [ -0.2231,  -1.6836],
        [ -7.8749,   6.4840],
        [ -6.0322,   4.6404],
        [ -8.0326,   6.5709],
        [ -9.3061,   5.3786],
        [ -4.9162,   3.3785],
        [ -6.5521,   4.9896],
        [ -5.6682,   3.5801],
        [ -7.9451,   6.4695],
        [ -1.1237,  -0.3081],
        [ -9.2810,   6.2937]], dtype=torch.float64, requires_grad=True)
pi: tensor([[7.2683e-06, 9.9999e-01],
        [1.0000e+00, 1.1336e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0442, 0.9558], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1528, 0.2065],
         [0.9576, 0.1602]],

        [[0.7686, 0.2239],
         [0.9867, 0.1290]],

        [[0.2274, 0.1001],
         [0.6600, 0.4313]],

        [[0.4648, 0.1194],
         [0.6714, 0.0698]],

        [[0.6188, 0.1432],
         [0.9315, 0.2961]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 4.85601903559462e-05
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.003506908785360844
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0014922741295917668
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 4.85601903559462e-05
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0015169269193791859
Global Adjusted Rand Index: -0.0017162252200383585
Average Adjusted Rand Index: 0.0007257363911720311
10736.924618725014
new:  [-0.0017162252200383585, -0.0017162252200383585, 0.7117589948073518, -0.0017162252200383585] [0.0007257363911720311, 0.0007257363911720311, 0.7146562343004924, 0.0007257363911720311] [10766.397832535265, 10766.478034737784, 10674.961250372533, 10766.450815272616]
prior:  [0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0] [10771.06882203842, 10771.96115672137, 10771.961183823467, 10771.792399051526]
-----------------------------------------------------------------------------------------
This iteration is 4
True Objective function: Loss = -10842.036697650363
Iteration 0: Loss = -23392.97331146888
Iteration 10: Loss = -10926.73121637141
Iteration 20: Loss = -10926.73121637141
1
Iteration 30: Loss = -10926.73121637141
2
Iteration 40: Loss = -10926.73121637141
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[9.5418e-19, 1.0000e+00],
        [2.2400e-22, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([2.2413e-22, 1.0000e+00])
beta: tensor([[[0.1992, 0.0911],
         [0.0996, 0.1595]],

        [[0.0786, 0.1332],
         [0.0373, 0.7064]],

        [[0.3494, 0.2575],
         [0.5383, 0.0459]],

        [[0.5619, 0.2249],
         [0.9595, 0.6038]],

        [[0.8151, 0.1752],
         [0.3457, 0.1360]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23509.688804832123
Iteration 100: Loss = -10929.001287508454
Iteration 200: Loss = -10928.095159256532
Iteration 300: Loss = -10927.574097244373
Iteration 400: Loss = -10927.257554166908
Iteration 500: Loss = -10927.049826634402
Iteration 600: Loss = -10926.903269868313
Iteration 700: Loss = -10926.793745336863
Iteration 800: Loss = -10926.708997768077
Iteration 900: Loss = -10926.642333526463
Iteration 1000: Loss = -10926.590407435988
Iteration 1100: Loss = -10926.552272396488
Iteration 1200: Loss = -10926.526831950789
Iteration 1300: Loss = -10926.511873942743
Iteration 1400: Loss = -10926.504157148573
Iteration 1500: Loss = -10926.500141325152
Iteration 1600: Loss = -10926.497498958093
Iteration 1700: Loss = -10926.495411974274
Iteration 1800: Loss = -10926.493497968318
Iteration 1900: Loss = -10926.491756891872
Iteration 2000: Loss = -10926.490048031845
Iteration 2100: Loss = -10926.488480181737
Iteration 2200: Loss = -10926.486954639378
Iteration 2300: Loss = -10926.485451242477
Iteration 2400: Loss = -10926.491384466137
1
Iteration 2500: Loss = -10926.482597806518
Iteration 2600: Loss = -10926.481180951481
Iteration 2700: Loss = -10926.479791178754
Iteration 2800: Loss = -10926.47860963283
Iteration 2900: Loss = -10926.47698384246
Iteration 3000: Loss = -10926.475483977893
Iteration 3100: Loss = -10926.473977775287
Iteration 3200: Loss = -10926.472343230884
Iteration 3300: Loss = -10926.470580590601
Iteration 3400: Loss = -10926.468617497134
Iteration 3500: Loss = -10926.467766135913
Iteration 3600: Loss = -10926.463951444735
Iteration 3700: Loss = -10926.460703895616
Iteration 3800: Loss = -10926.455857968223
Iteration 3900: Loss = -10926.447405271749
Iteration 4000: Loss = -10926.067389256146
Iteration 4100: Loss = -10924.794770973936
Iteration 4200: Loss = -10924.719204270365
Iteration 4300: Loss = -10924.677282604405
Iteration 4400: Loss = -10924.630036327735
Iteration 4500: Loss = -10924.566745352522
Iteration 4600: Loss = -10923.839406634475
Iteration 4700: Loss = -10923.48409799006
Iteration 4800: Loss = -10923.410718262749
Iteration 4900: Loss = -10923.381519928214
Iteration 5000: Loss = -10923.362211384532
Iteration 5100: Loss = -10923.351278287842
Iteration 5200: Loss = -10923.343637417693
Iteration 5300: Loss = -10923.337715014146
Iteration 5400: Loss = -10923.351479505984
1
Iteration 5500: Loss = -10923.329503353249
Iteration 5600: Loss = -10923.32628607642
Iteration 5700: Loss = -10923.324339212206
Iteration 5800: Loss = -10923.32034364042
Iteration 5900: Loss = -10923.318201374046
Iteration 6000: Loss = -10923.316356608117
Iteration 6100: Loss = -10923.313562208137
Iteration 6200: Loss = -10923.311808270611
Iteration 6300: Loss = -10923.310384214934
Iteration 6400: Loss = -10923.30912250997
Iteration 6500: Loss = -10923.30748098723
Iteration 6600: Loss = -10923.30532381509
Iteration 6700: Loss = -10923.303572619438
Iteration 6800: Loss = -10923.30216712398
Iteration 6900: Loss = -10923.301210550577
Iteration 7000: Loss = -10923.304435122356
1
Iteration 7100: Loss = -10923.299606203904
Iteration 7200: Loss = -10923.298845884192
Iteration 7300: Loss = -10923.358827932776
1
Iteration 7400: Loss = -10923.297026825863
Iteration 7500: Loss = -10923.296295431619
Iteration 7600: Loss = -10923.295286658624
Iteration 7700: Loss = -10923.294529530196
Iteration 7800: Loss = -10923.293922426701
Iteration 7900: Loss = -10923.293551725046
Iteration 8000: Loss = -10923.300809033262
1
Iteration 8100: Loss = -10923.292764403779
Iteration 8200: Loss = -10923.292450769513
Iteration 8300: Loss = -10923.292129205443
Iteration 8400: Loss = -10923.29197344308
Iteration 8500: Loss = -10923.291495996238
Iteration 8600: Loss = -10923.291125676544
Iteration 8700: Loss = -10923.292929147741
1
Iteration 8800: Loss = -10923.289997973066
Iteration 8900: Loss = -10923.289254803853
Iteration 9000: Loss = -10923.29784027511
1
Iteration 9100: Loss = -10923.288500447725
Iteration 9200: Loss = -10923.288281146168
Iteration 9300: Loss = -10923.8325416001
1
Iteration 9400: Loss = -10923.287964763356
Iteration 9500: Loss = -10923.287846339568
Iteration 9600: Loss = -10923.28772937115
Iteration 9700: Loss = -10923.287829033818
1
Iteration 9800: Loss = -10923.287486066669
Iteration 9900: Loss = -10923.287347336287
Iteration 10000: Loss = -10923.287451627863
1
Iteration 10100: Loss = -10923.2870881179
Iteration 10200: Loss = -10923.28696053099
Iteration 10300: Loss = -10923.297290330403
1
Iteration 10400: Loss = -10923.286369567235
Iteration 10500: Loss = -10923.286167159542
Iteration 10600: Loss = -10923.286048098686
Iteration 10700: Loss = -10923.289012083264
1
Iteration 10800: Loss = -10923.285837159852
Iteration 10900: Loss = -10923.285663620172
Iteration 11000: Loss = -10923.285995995237
1
Iteration 11100: Loss = -10923.285246326615
Iteration 11200: Loss = -10923.285131145327
Iteration 11300: Loss = -10923.285086512578
Iteration 11400: Loss = -10923.285050750232
Iteration 11500: Loss = -10923.284857281265
Iteration 11600: Loss = -10923.284706810906
Iteration 11700: Loss = -10923.284630459802
Iteration 11800: Loss = -10923.284383615464
Iteration 11900: Loss = -10923.284344510444
Iteration 12000: Loss = -10923.288035156489
1
Iteration 12100: Loss = -10923.28414989165
Iteration 12200: Loss = -10923.284058065909
Iteration 12300: Loss = -10923.346121496892
1
Iteration 12400: Loss = -10923.283864544792
Iteration 12500: Loss = -10923.283766375293
Iteration 12600: Loss = -10923.283739323464
Iteration 12700: Loss = -10923.28364838355
Iteration 12800: Loss = -10923.283578233202
Iteration 12900: Loss = -10923.28354152294
Iteration 13000: Loss = -10923.283770038526
1
Iteration 13100: Loss = -10923.285452781794
2
Iteration 13200: Loss = -10923.283322580304
Iteration 13300: Loss = -10923.28321640853
Iteration 13400: Loss = -10923.283103621252
Iteration 13500: Loss = -10923.310867918228
1
Iteration 13600: Loss = -10923.282955246872
Iteration 13700: Loss = -10923.282926047423
Iteration 13800: Loss = -10923.283651819645
1
Iteration 13900: Loss = -10923.282853244025
Iteration 14000: Loss = -10923.298265682954
1
Iteration 14100: Loss = -10923.284328637546
2
Iteration 14200: Loss = -10923.282542627108
Iteration 14300: Loss = -10923.364914507287
1
Iteration 14400: Loss = -10923.283195373313
2
Iteration 14500: Loss = -10923.285662941944
3
Iteration 14600: Loss = -10923.380165766754
4
Iteration 14700: Loss = -10923.282374904267
Iteration 14800: Loss = -10923.282271318747
Iteration 14900: Loss = -10923.3059716715
1
Iteration 15000: Loss = -10923.282847087186
2
Iteration 15100: Loss = -10923.288079380818
3
Iteration 15200: Loss = -10923.284993795916
4
Iteration 15300: Loss = -10923.282149239338
Iteration 15400: Loss = -10923.28815885928
1
Iteration 15500: Loss = -10923.317239094398
2
Iteration 15600: Loss = -10923.283745358252
3
Iteration 15700: Loss = -10923.283176353332
4
Iteration 15800: Loss = -10923.286125613477
5
Iteration 15900: Loss = -10923.282137454042
Iteration 16000: Loss = -10923.312454695591
1
Iteration 16100: Loss = -10923.281942993504
Iteration 16200: Loss = -10923.284072063845
1
Iteration 16300: Loss = -10923.282824321233
2
Iteration 16400: Loss = -10923.28339598383
3
Iteration 16500: Loss = -10923.286203551645
4
Iteration 16600: Loss = -10923.283324644326
5
Iteration 16700: Loss = -10923.285193362164
6
Iteration 16800: Loss = -10923.281964039768
7
Iteration 16900: Loss = -10923.418457895763
8
Iteration 17000: Loss = -10923.281970402713
9
Iteration 17100: Loss = -10923.376403724325
10
Stopping early at iteration 17100 due to no improvement.
tensor([[ -6.5765,   1.9613],
        [ -7.9462,   3.3310],
        [ -6.3720,   1.7567],
        [ -9.9023,   5.2870],
        [ -4.7310,   0.1158],
        [ -9.4962,   4.8810],
        [ -6.9011,   2.2859],
        [ -7.6956,   3.0804],
        [ -9.0316,   4.4164],
        [ -2.1663,  -2.4489],
        [ -7.1545,   2.5392],
        [ -6.3744,   1.7592],
        [ -9.9211,   5.3059],
        [ -9.6507,   5.0355],
        [-10.1161,   5.5008],
        [ -9.6798,   5.0646],
        [ -9.9156,   5.3003],
        [ -6.5151,   1.8999],
        [-10.0163,   5.4011],
        [ -8.4130,   3.7978],
        [ -9.6423,   5.0271],
        [ -9.8053,   5.1900],
        [ -2.0527,  -2.5625],
        [ -8.8414,   4.2262],
        [ -5.7448,   1.1296],
        [ -9.6293,   5.0141],
        [ -4.1001,  -0.5151],
        [ -3.7235,  -0.8917],
        [ -9.3647,   4.7495],
        [ -5.5735,   0.9583],
        [ -7.3109,   2.6957],
        [ -8.3763,   3.7611],
        [ -9.7183,   5.1031],
        [ -8.2635,   3.6483],
        [ -8.6247,   4.0095],
        [ -9.5349,   4.9196],
        [ -6.9087,   2.2935],
        [ -9.1809,   4.5657],
        [ -7.5426,   2.9274],
        [ -6.6880,   2.0728],
        [ -6.2495,   1.6343],
        [ -5.0461,   0.4309],
        [ -8.0673,   3.4520],
        [ -9.5695,   4.9543],
        [ -9.7210,   5.1057],
        [ -7.5462,   2.9310],
        [ -6.1087,   1.4935],
        [ -6.5926,   1.9774],
        [ -5.9712,   1.3559],
        [ -2.6676,  -1.9477],
        [ -7.6023,   2.9871],
        [ -6.4639,   1.8487],
        [ -9.8176,   5.2023],
        [ -7.9377,   3.3225],
        [ -8.9617,   4.3464],
        [ -6.1133,   1.4981],
        [-10.0697,   5.4545],
        [ -6.7155,   2.1002],
        [ -7.1978,   2.5826],
        [ -7.3184,   2.7032],
        [ -9.6141,   4.9989],
        [ -9.7057,   5.0905],
        [ -9.8277,   5.2125],
        [ -8.3669,   3.7517],
        [ -9.5412,   4.9260],
        [ -9.9068,   5.2915],
        [ -8.6323,   4.0170],
        [ -5.9831,   1.3679],
        [ -6.2901,   1.6749],
        [ -7.3371,   2.7219],
        [-10.0246,   5.4094],
        [ -8.4253,   3.8101],
        [ -5.3560,   0.7408],
        [ -9.1329,   4.5177],
        [ -8.1380,   3.5228],
        [ -4.8864,   0.2711],
        [ -6.5952,   1.9799],
        [-10.3696,   5.7543],
        [ -6.9008,   2.2856],
        [ -6.1104,   1.4952],
        [ -4.9152,   0.3000],
        [ -9.9121,   5.2969],
        [ -8.7854,   4.1702],
        [ -6.5593,   1.9440],
        [ -6.4821,   1.8669],
        [ -7.9112,   3.2960],
        [ -5.7265,   1.1113],
        [ -7.0921,   2.4768],
        [ -9.7062,   5.0910],
        [-10.0458,   5.4306],
        [-10.1249,   5.5096],
        [ -5.2665,   0.6513],
        [ -5.7346,   1.1194],
        [ -4.5874,  -0.0278],
        [ -7.1095,   2.4942],
        [ -9.8223,   5.2070],
        [ -8.3863,   3.7711],
        [ -6.7750,   2.1597],
        [ -9.8582,   5.2430],
        [ -8.1517,   3.5364]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 3.2801e-06],
        [6.2711e-03, 9.9373e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0165, 0.9835], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.5219, 0.1372],
         [0.0996, 0.1584]],

        [[0.0786, 0.1502],
         [0.0373, 0.7064]],

        [[0.3494, 0.2312],
         [0.5383, 0.0459]],

        [[0.5619, 0.2189],
         [0.9595, 0.6038]],

        [[0.8151, 0.1642],
         [0.3457, 0.1360]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007766707522784365
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
Global Adjusted Rand Index: -0.0003824099409314635
Average Adjusted Rand Index: -0.0028886639223549134
Iteration 0: Loss = -13459.199065413104
Iteration 10: Loss = -10926.459031559947
Iteration 20: Loss = -10926.38625876587
Iteration 30: Loss = -10926.359989421924
Iteration 40: Loss = -10926.346664470368
Iteration 50: Loss = -10926.338668351362
Iteration 60: Loss = -10926.3331764584
Iteration 70: Loss = -10926.329355370555
Iteration 80: Loss = -10926.32649997521
Iteration 90: Loss = -10926.324375961132
Iteration 100: Loss = -10926.322825935486
Iteration 110: Loss = -10926.321653327983
Iteration 120: Loss = -10926.320843757094
Iteration 130: Loss = -10926.320271134393
Iteration 140: Loss = -10926.319792393555
Iteration 150: Loss = -10926.31958896024
Iteration 160: Loss = -10926.31939222668
Iteration 170: Loss = -10926.319329728469
Iteration 180: Loss = -10926.319291737967
Iteration 190: Loss = -10926.319323440848
1
Iteration 200: Loss = -10926.319354965332
2
Iteration 210: Loss = -10926.31941613506
3
Stopping early at iteration 209 due to no improvement.
pi: tensor([[0.4449, 0.5551],
        [0.3624, 0.6376]], dtype=torch.float64)
alpha: tensor([0.3947, 0.6053])
beta: tensor([[[0.1618, 0.1547],
         [0.6296, 0.1580]],

        [[0.7908, 0.1583],
         [0.2515, 0.6815]],

        [[0.4287, 0.1638],
         [0.3103, 0.3178]],

        [[0.1241, 0.1609],
         [0.4949, 0.5376]],

        [[0.4088, 0.1616],
         [0.6740, 0.0244]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -13459.044662308306
Iteration 100: Loss = -10926.509833828708
Iteration 200: Loss = -10925.983221984632
Iteration 300: Loss = -10925.918340242517
Iteration 400: Loss = -10925.883680681483
Iteration 500: Loss = -10925.857939688753
Iteration 600: Loss = -10925.836700109498
Iteration 700: Loss = -10925.818120203525
Iteration 800: Loss = -10925.796789504564
Iteration 900: Loss = -10925.779222786701
Iteration 1000: Loss = -10925.769338034726
Iteration 1100: Loss = -10925.760008005573
Iteration 1200: Loss = -10925.750448182966
Iteration 1300: Loss = -10925.744396315777
Iteration 1400: Loss = -10925.732758043237
Iteration 1500: Loss = -10925.726644482105
Iteration 1600: Loss = -10925.735963213212
1
Iteration 1700: Loss = -10925.71927648166
Iteration 1800: Loss = -10925.7169441092
Iteration 1900: Loss = -10925.715137407178
Iteration 2000: Loss = -10925.715966058664
1
Iteration 2100: Loss = -10925.712536802914
Iteration 2200: Loss = -10925.711582763724
Iteration 2300: Loss = -10926.154542440996
1
Iteration 2400: Loss = -10925.710106635499
Iteration 2500: Loss = -10925.709501246949
Iteration 2600: Loss = -10925.70900880581
Iteration 2700: Loss = -10925.708952166915
Iteration 2800: Loss = -10925.708226308207
Iteration 2900: Loss = -10925.707918124572
Iteration 3000: Loss = -10925.70832307205
1
Iteration 3100: Loss = -10925.707333949302
Iteration 3200: Loss = -10925.707111119373
Iteration 3300: Loss = -10925.706875893284
Iteration 3400: Loss = -10925.706843238271
Iteration 3500: Loss = -10925.706509823827
Iteration 3600: Loss = -10925.706338503544
Iteration 3700: Loss = -10925.70968246196
1
Iteration 3800: Loss = -10925.706040114434
Iteration 3900: Loss = -10925.706190635368
1
Iteration 4000: Loss = -10925.772091897728
2
Iteration 4100: Loss = -10925.705711796692
Iteration 4200: Loss = -10925.705630126968
Iteration 4300: Loss = -10925.705757929443
1
Iteration 4400: Loss = -10925.705442281775
Iteration 4500: Loss = -10925.768237511198
1
Iteration 4600: Loss = -10925.715298459672
2
Iteration 4700: Loss = -10925.705251375757
Iteration 4800: Loss = -10925.705394256755
1
Iteration 4900: Loss = -10925.706245388043
2
Iteration 5000: Loss = -10925.704981050898
Iteration 5100: Loss = -10925.758902309808
1
Iteration 5200: Loss = -10925.703519523882
Iteration 5300: Loss = -10925.70356413536
1
Iteration 5400: Loss = -10925.70344000045
Iteration 5500: Loss = -10925.706450962618
1
Iteration 5600: Loss = -10925.70801889544
2
Iteration 5700: Loss = -10925.703146049342
Iteration 5800: Loss = -10925.744987281694
1
Iteration 5900: Loss = -10925.703074314884
Iteration 6000: Loss = -10925.70327728686
1
Iteration 6100: Loss = -10925.750292323253
2
Iteration 6200: Loss = -10925.703091135081
3
Iteration 6300: Loss = -10925.70301565131
Iteration 6400: Loss = -10925.71086672718
1
Iteration 6500: Loss = -10925.70519885471
2
Iteration 6600: Loss = -10925.703003122486
Iteration 6700: Loss = -10925.703141604832
1
Iteration 6800: Loss = -10925.755262934088
2
Iteration 6900: Loss = -10925.703097324647
3
Iteration 7000: Loss = -10925.729489925485
4
Iteration 7100: Loss = -10925.70284206155
Iteration 7200: Loss = -10925.704372660579
1
Iteration 7300: Loss = -10925.704482568843
2
Iteration 7400: Loss = -10925.704662318725
3
Iteration 7500: Loss = -10925.707882022429
4
Iteration 7600: Loss = -10925.702832106692
Iteration 7700: Loss = -10925.755885495175
1
Iteration 7800: Loss = -10925.702850474649
2
Iteration 7900: Loss = -10925.799420723108
3
Iteration 8000: Loss = -10925.702840146161
4
Iteration 8100: Loss = -10925.74170090833
5
Iteration 8200: Loss = -10925.702804644516
Iteration 8300: Loss = -10925.720556986895
1
Iteration 8400: Loss = -10925.705629347443
2
Iteration 8500: Loss = -10925.702958612412
3
Iteration 8600: Loss = -10925.70307891529
4
Iteration 8700: Loss = -10925.773177199471
5
Iteration 8800: Loss = -10925.70449857218
6
Iteration 8900: Loss = -10925.73076639136
7
Iteration 9000: Loss = -10925.702833194797
8
Iteration 9100: Loss = -10925.708811382554
9
Iteration 9200: Loss = -10925.702883308384
10
Stopping early at iteration 9200 due to no improvement.
tensor([[ 0.8868, -4.0253],
        [ 1.0487, -3.8470],
        [ 1.7534, -3.1494],
        [ 1.4670, -3.4485],
        [ 1.2730, -3.6665],
        [ 1.7287, -3.1912],
        [ 1.7355, -3.1679],
        [ 1.7575, -3.1444],
        [ 1.7430, -3.1628],
        [ 1.4883, -3.4278],
        [ 1.7144, -3.1921],
        [ 0.1594, -4.7746],
        [ 1.1030, -3.8080],
        [ 0.9934, -3.9288],
        [ 0.1501, -4.7653],
        [ 1.6539, -3.2572],
        [ 1.7175, -3.1889],
        [ 1.3480, -3.5728],
        [ 1.7471, -3.1490],
        [ 1.7585, -3.1457],
        [ 1.4092, -3.4997],
        [ 1.5692, -3.3345],
        [ 1.4518, -3.4651],
        [ 1.7130, -3.2012],
        [ 1.2906, -3.6419],
        [ 1.2162, -3.7300],
        [ 1.6501, -3.2552],
        [ 1.4497, -3.4545],
        [ 1.4624, -3.4625],
        [ 1.6947, -3.2073],
        [ 0.6706, -4.2386],
        [ 1.6776, -3.2159],
        [ 1.7466, -3.1556],
        [ 0.9788, -3.9331],
        [ 1.7350, -3.1775],
        [ 1.5999, -3.3090],
        [ 1.5583, -3.3499],
        [ 0.6301, -4.2827],
        [ 1.3518, -3.5679],
        [ 1.6915, -3.2287],
        [ 1.6624, -3.2554],
        [ 0.3045, -4.6164],
        [ 1.7051, -3.1989],
        [ 0.9654, -3.9429],
        [ 1.7599, -3.1558],
        [ 1.7707, -3.1572],
        [ 1.1942, -3.7080],
        [ 1.5208, -3.3924],
        [ 1.7284, -3.1821],
        [ 1.7076, -3.2103],
        [ 1.3117, -3.5950],
        [ 1.2310, -3.6816],
        [ 1.6745, -3.2383],
        [ 1.5512, -3.3524],
        [ 1.6446, -3.2788],
        [ 1.6342, -3.2739],
        [ 1.6361, -3.2634],
        [ 1.2148, -3.6873],
        [ 1.3688, -3.5391],
        [ 1.5362, -3.3755],
        [ 1.6074, -3.3080],
        [ 1.0819, -3.8369],
        [ 1.7399, -3.1652],
        [ 1.7289, -3.1832],
        [ 1.6356, -3.2724],
        [ 1.4459, -3.4490],
        [ 1.5550, -3.3556],
        [ 1.2017, -3.7143],
        [ 1.7073, -3.1992],
        [ 1.6070, -3.3002],
        [ 0.9174, -3.9852],
        [ 1.1711, -3.7385],
        [ 0.4992, -4.4283],
        [ 1.7446, -3.1724],
        [ 1.1760, -3.7278],
        [ 1.0591, -3.8461],
        [ 1.7537, -3.1798],
        [ 0.5951, -4.3092],
        [ 1.4277, -3.4768],
        [ 1.7375, -3.1780],
        [ 1.3879, -3.5324],
        [ 1.6914, -3.2100],
        [ 1.4671, -3.4438],
        [ 1.7208, -3.1994],
        [ 1.6058, -3.2971],
        [ 1.5502, -3.3523],
        [ 0.1500, -4.7652],
        [ 1.2206, -3.6888],
        [ 1.7253, -3.1841],
        [ 0.4005, -4.5033],
        [ 1.6206, -3.2970],
        [ 1.7434, -3.1753],
        [ 0.8379, -4.0730],
        [ 1.3231, -3.5801],
        [ 1.5388, -3.3665],
        [ 1.5476, -3.3527],
        [ 1.7225, -3.1842],
        [ 1.6732, -3.2411],
        [ 1.7754, -3.1620],
        [ 1.6514, -3.2688]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.0249, 0.9751],
        [0.0128, 0.9872]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9927, 0.0073], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1560, 0.1563],
         [0.6296, 0.1625]],

        [[0.7908, 0.1371],
         [0.2515, 0.6815]],

        [[0.4287, 0.2253],
         [0.3103, 0.3178]],

        [[0.1241, 0.1201],
         [0.4949, 0.5376]],

        [[0.4088, 0.1641],
         [0.6740, 0.0244]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00022591370147976686
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -20384.184311352605
Iteration 10: Loss = -10926.731264343454
Iteration 20: Loss = -10926.73109974516
Iteration 30: Loss = -10926.73050623111
Iteration 40: Loss = -10926.727895948263
Iteration 50: Loss = -10926.717944634796
Iteration 60: Loss = -10926.680165905806
Iteration 70: Loss = -10926.567985797312
Iteration 80: Loss = -10926.380222315167
Iteration 90: Loss = -10926.229907883993
Iteration 100: Loss = -10926.149929941588
Iteration 110: Loss = -10926.105567011058
Iteration 120: Loss = -10926.07687459473
Iteration 130: Loss = -10926.059047202869
Iteration 140: Loss = -10926.04968332932
Iteration 150: Loss = -10926.045790597349
Iteration 160: Loss = -10926.04524397561
Iteration 170: Loss = -10926.046652395766
1
Iteration 180: Loss = -10926.049285222398
2
Iteration 190: Loss = -10926.052702345329
3
Stopping early at iteration 189 due to no improvement.
pi: tensor([[0.1757, 0.8243],
        [0.0373, 0.9627]], dtype=torch.float64)
alpha: tensor([0.0428, 0.9572])
beta: tensor([[[0.2012, 0.1501],
         [0.2634, 0.1577]],

        [[0.7189, 0.1651],
         [0.5239, 0.0744]],

        [[0.1954, 0.1997],
         [0.4902, 0.7523]],

        [[0.9009, 0.2019],
         [0.3468, 0.7084]],

        [[0.6083, 0.1745],
         [0.4170, 0.2237]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20384.439494801405
Iteration 100: Loss = -10932.85140003486
Iteration 200: Loss = -10927.372884767876
Iteration 300: Loss = -10926.719062392584
Iteration 400: Loss = -10926.455894569086
Iteration 500: Loss = -10926.820403670508
1
Iteration 600: Loss = -10926.194711488635
Iteration 700: Loss = -10926.099035606683
Iteration 800: Loss = -10926.022007109807
Iteration 900: Loss = -10925.955612228245
Iteration 1000: Loss = -10925.892938207793
Iteration 1100: Loss = -10925.82884678062
Iteration 1200: Loss = -10925.87781584058
1
Iteration 1300: Loss = -10925.713137489374
Iteration 1400: Loss = -10925.650716478638
Iteration 1500: Loss = -10925.575801882089
Iteration 1600: Loss = -10925.492791775876
Iteration 1700: Loss = -10925.393937156927
Iteration 1800: Loss = -10925.288735373102
Iteration 1900: Loss = -10925.192546833048
Iteration 2000: Loss = -10925.096013919672
Iteration 2100: Loss = -10925.000119550074
Iteration 2200: Loss = -10924.907408729636
Iteration 2300: Loss = -10924.814797585257
Iteration 2400: Loss = -10924.71534678735
Iteration 2500: Loss = -10924.599169832198
Iteration 2600: Loss = -10924.453635858465
Iteration 2700: Loss = -10924.278863641806
Iteration 2800: Loss = -10924.091404361365
Iteration 2900: Loss = -10923.924398240722
Iteration 3000: Loss = -10923.791448789303
Iteration 3100: Loss = -10923.688065210188
Iteration 3200: Loss = -10923.608432333425
Iteration 3300: Loss = -10923.549584190649
Iteration 3400: Loss = -10923.504968249033
Iteration 3500: Loss = -10923.469979319107
Iteration 3600: Loss = -10923.442701057284
Iteration 3700: Loss = -10923.419003094572
Iteration 3800: Loss = -10923.400096438409
Iteration 3900: Loss = -10923.384528178054
Iteration 4000: Loss = -10923.371623774048
Iteration 4100: Loss = -10923.360870787219
Iteration 4200: Loss = -10923.351925666993
Iteration 4300: Loss = -10923.357337108973
1
Iteration 4400: Loss = -10923.338139802641
Iteration 4500: Loss = -10923.332747204904
Iteration 4600: Loss = -10923.328175632847
Iteration 4700: Loss = -10923.408449704773
1
Iteration 4800: Loss = -10923.320850371396
Iteration 4900: Loss = -10923.317888757505
Iteration 5000: Loss = -10923.315283339693
Iteration 5100: Loss = -10923.31307003218
Iteration 5200: Loss = -10923.310960377492
Iteration 5300: Loss = -10923.30914165735
Iteration 5400: Loss = -10923.310545339295
1
Iteration 5500: Loss = -10923.30602340606
Iteration 5600: Loss = -10923.304696705742
Iteration 5700: Loss = -10923.303485138118
Iteration 5800: Loss = -10923.304231477792
1
Iteration 5900: Loss = -10923.301355849471
Iteration 6000: Loss = -10923.3004427027
Iteration 6100: Loss = -10923.30843464982
1
Iteration 6200: Loss = -10923.298844535722
Iteration 6300: Loss = -10923.298099372221
Iteration 6400: Loss = -10923.297488840084
Iteration 6500: Loss = -10923.296839513961
Iteration 6600: Loss = -10923.296236083832
Iteration 6700: Loss = -10923.295678082703
Iteration 6800: Loss = -10923.449281365238
1
Iteration 6900: Loss = -10923.294727504805
Iteration 7000: Loss = -10923.294297156817
Iteration 7100: Loss = -10923.293879320525
Iteration 7200: Loss = -10923.296800414324
1
Iteration 7300: Loss = -10923.293107582049
Iteration 7400: Loss = -10923.292817396548
Iteration 7500: Loss = -10923.292947659958
1
Iteration 7600: Loss = -10923.292217638591
Iteration 7700: Loss = -10923.291844822032
Iteration 7800: Loss = -10923.291567945342
Iteration 7900: Loss = -10923.291376839461
Iteration 8000: Loss = -10923.29104444404
Iteration 8100: Loss = -10923.290774970465
Iteration 8200: Loss = -10923.291218693703
1
Iteration 8300: Loss = -10923.290368915099
Iteration 8400: Loss = -10923.290111201039
Iteration 8500: Loss = -10923.290567947379
1
Iteration 8600: Loss = -10923.289674628506
Iteration 8700: Loss = -10923.289441644432
Iteration 8800: Loss = -10923.289248831745
Iteration 8900: Loss = -10923.298059421946
1
Iteration 9000: Loss = -10923.288954997142
Iteration 9100: Loss = -10923.288817837012
Iteration 9200: Loss = -10923.288671873237
Iteration 9300: Loss = -10923.288885490572
1
Iteration 9400: Loss = -10923.28843506528
Iteration 9500: Loss = -10923.288330866118
Iteration 9600: Loss = -10923.288277338686
Iteration 9700: Loss = -10923.288223266705
Iteration 9800: Loss = -10923.288012161835
Iteration 9900: Loss = -10923.287968962375
Iteration 10000: Loss = -10923.465944027264
1
Iteration 10100: Loss = -10923.287796452001
Iteration 10200: Loss = -10923.287713486778
Iteration 10300: Loss = -10923.287643008009
Iteration 10400: Loss = -10923.287910235049
1
Iteration 10500: Loss = -10923.287530633046
Iteration 10600: Loss = -10923.287452855739
Iteration 10700: Loss = -10923.294099193557
1
Iteration 10800: Loss = -10923.287293270709
Iteration 10900: Loss = -10923.287238282777
Iteration 11000: Loss = -10923.287341852456
1
Iteration 11100: Loss = -10923.287069715763
Iteration 11200: Loss = -10923.286987158042
Iteration 11300: Loss = -10923.286915665232
Iteration 11400: Loss = -10923.289203945738
1
Iteration 11500: Loss = -10923.286798668702
Iteration 11600: Loss = -10923.286774102206
Iteration 11700: Loss = -10923.543504314484
1
Iteration 11800: Loss = -10923.286477854192
Iteration 11900: Loss = -10923.285939573116
Iteration 12000: Loss = -10923.285787807119
Iteration 12100: Loss = -10923.286051761335
1
Iteration 12200: Loss = -10923.285695813973
Iteration 12300: Loss = -10923.285668699762
Iteration 12400: Loss = -10923.287052440628
1
Iteration 12500: Loss = -10923.285579324911
Iteration 12600: Loss = -10923.285618814014
1
Iteration 12700: Loss = -10923.285765911414
2
Iteration 12800: Loss = -10923.292011110716
3
Iteration 12900: Loss = -10923.285470958335
Iteration 13000: Loss = -10923.292450251596
1
Iteration 13100: Loss = -10923.285007259372
Iteration 13200: Loss = -10923.285335947598
1
Iteration 13300: Loss = -10923.285654335403
2
Iteration 13400: Loss = -10923.286136760353
3
Iteration 13500: Loss = -10923.290042769466
4
Iteration 13600: Loss = -10923.28929375442
5
Iteration 13700: Loss = -10923.284867627759
Iteration 13800: Loss = -10923.285539804192
1
Iteration 13900: Loss = -10923.284578387213
Iteration 14000: Loss = -10923.284501243212
Iteration 14100: Loss = -10923.28405622366
Iteration 14200: Loss = -10923.284490416589
1
Iteration 14300: Loss = -10923.2992714978
2
Iteration 14400: Loss = -10923.291077645088
3
Iteration 14500: Loss = -10923.282964186927
Iteration 14600: Loss = -10923.283091122452
1
Iteration 14700: Loss = -10923.310456940277
2
Iteration 14800: Loss = -10923.283158123195
3
Iteration 14900: Loss = -10923.283329218277
4
Iteration 15000: Loss = -10923.408264032736
5
Iteration 15100: Loss = -10923.282830867898
Iteration 15200: Loss = -10923.298966108108
1
Iteration 15300: Loss = -10923.283008494189
2
Iteration 15400: Loss = -10923.283306219206
3
Iteration 15500: Loss = -10923.28449783825
4
Iteration 15600: Loss = -10923.312953457724
5
Iteration 15700: Loss = -10923.28285198976
6
Iteration 15800: Loss = -10923.28683441046
7
Iteration 15900: Loss = -10923.285580962962
8
Iteration 16000: Loss = -10923.283588747523
9
Iteration 16100: Loss = -10923.321114646727
10
Stopping early at iteration 16100 due to no improvement.
tensor([[ -5.0562,   3.4820],
        [ -8.9991,   7.3061],
        [ -4.9045,   3.2412],
        [ -8.3039,   6.9146],
        [ -3.2331,   1.6382],
        [ -8.2026,   5.7490],
        [ -5.8185,   3.3809],
        [ -9.0348,   7.3195],
        [ -9.5776,   6.6883],
        [ -0.8157,  -1.0900],
        [ -5.6022,   4.1016],
        [ -4.8489,   3.2781],
        [ -8.5619,   4.0599],
        [ -7.5275,   6.0811],
        [ -8.7275,   7.0716],
        [ -9.1466,   5.3954],
        [ -8.5530,   7.0368],
        [ -5.2466,   3.1775],
        [ -8.8856,   5.0692],
        [ -8.4371,   7.0315],
        [ -8.7921,   7.0930],
        [ -8.3776,   6.9913],
        [ -0.7242,  -1.2230],
        [ -8.2709,   6.6813],
        [ -4.1385,   2.7502],
        [ -8.3707,   6.6085],
        [ -2.5935,   0.9667],
        [ -2.2527,   0.5575],
        [ -8.1198,   6.6097],
        [ -5.1539,   1.3706],
        [ -5.7009,   4.3146],
        [ -6.6206,   4.0578],
        [ -8.4481,   6.7506],
        [ -9.0906,   7.7032],
        [ -8.9151,   6.3674],
        [ -7.8672,   6.4401],
        [ -5.5994,   3.6135],
        [ -7.5748,   6.1759],
        [ -5.9270,   4.5406],
        [ -5.3156,   3.4488],
        [ -4.7041,   3.1736],
        [ -3.6599,   1.8335],
        [ -6.8954,   4.6732],
        [ -6.7415,   4.9515],
        [ -9.3610,   5.0443],
        [ -8.5348,   6.3473],
        [ -4.7682,   2.8514],
        [ -5.4227,   3.1589],
        [ -4.7522,   2.5909],
        [ -1.0707,  -0.3330],
        [ -8.7325,   7.1901],
        [ -5.1579,   3.1558],
        [ -7.1334,   4.2146],
        [-10.0115,   6.1515],
        [ -6.2216,   4.5894],
        [ -8.5256,   7.1007],
        [ -9.5431,   6.3923],
        [ -5.1905,   3.6336],
        [ -6.5805,   3.2144],
        [ -6.1168,   3.9174],
        [ -8.5128,   6.9235],
        [ -8.0978,   6.4504],
        [ -7.3252,   5.4654],
        [ -8.5151,   6.4001],
        [ -6.5626,   4.2750],
        [ -5.9565,   4.5235],
        [ -7.1222,   5.5820],
        [ -4.7474,   2.6052],
        [ -5.3788,   2.5802],
        [ -7.3444,   2.7292],
        [ -7.4182,   5.4817],
        [ -8.5149,   6.0918],
        [ -3.7808,   2.3148],
        [ -8.3253,   6.7160],
        [ -8.4846,   6.2969],
        [ -3.9517,   1.1844],
        [ -5.0778,   3.5109],
        [ -8.8859,   7.2110],
        [ -5.7530,   3.4439],
        [ -4.4955,   3.1034],
        [ -3.3164,   1.8948],
        [ -9.0370,   6.8532],
        [ -8.1250,   6.3690],
        [ -5.3031,   3.2150],
        [ -5.8378,   2.5230],
        [ -6.3953,   4.8236],
        [ -4.2415,   2.6112],
        [ -8.7134,   6.0161],
        [-10.2985,   5.6833],
        [ -9.2990,   6.5168],
        [ -8.4707,   6.3645],
        [ -3.6716,   2.2292],
        [ -4.4100,   2.4349],
        [ -3.0461,   1.4875],
        [ -5.8398,   3.7776],
        [ -8.2579,   6.8435],
        [ -7.0602,   5.1361],
        [ -8.9008,   6.7143],
        [ -9.7792,   5.1639],
        [ -7.0226,   5.6142]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9999e-01, 5.3451e-06],
        [6.1212e-03, 9.9388e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0167, 0.9833], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.5227, 0.1362],
         [0.2634, 0.1599]],

        [[0.7189, 0.1484],
         [0.5239, 0.0744]],

        [[0.1954, 0.2337],
         [0.4902, 0.7523]],

        [[0.9009, 0.2181],
         [0.3468, 0.7084]],

        [[0.6083, 0.1645],
         [0.4170, 0.2237]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007766707522784365
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
Global Adjusted Rand Index: -0.0003824099409314635
Average Adjusted Rand Index: -0.0028886639223549134
Iteration 0: Loss = -16693.53300237966
Iteration 10: Loss = -10926.293744007375
Iteration 20: Loss = -10926.056782742768
Iteration 30: Loss = -10926.05148771608
Iteration 40: Loss = -10926.050395171167
Iteration 50: Loss = -10926.046949894086
Iteration 60: Loss = -10926.045011924476
Iteration 70: Loss = -10926.045091990713
1
Iteration 80: Loss = -10926.046794145968
2
Iteration 90: Loss = -10926.04953978766
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.9646, 0.0354],
        [0.8236, 0.1764]], dtype=torch.float64)
alpha: tensor([0.9593, 0.0407])
beta: tensor([[[0.1578, 0.1493],
         [0.9385, 0.2015]],

        [[0.4631, 0.1648],
         [0.7587, 0.1707]],

        [[0.6617, 0.2007],
         [0.2191, 0.6597]],

        [[0.6992, 0.2030],
         [0.6927, 0.2603]],

        [[0.9314, 0.1745],
         [0.1492, 0.1222]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16693.83727465678
Iteration 100: Loss = -10929.146513988417
Iteration 200: Loss = -10927.631135162706
Iteration 300: Loss = -10926.956454763389
Iteration 400: Loss = -10926.578451716829
Iteration 500: Loss = -10926.321296062544
Iteration 600: Loss = -10926.095758451194
Iteration 700: Loss = -10925.874867584711
Iteration 800: Loss = -10925.682177525287
Iteration 900: Loss = -10925.51156435917
Iteration 1000: Loss = -10925.328037750822
Iteration 1100: Loss = -10925.147210557237
Iteration 1200: Loss = -10924.996924628165
Iteration 1300: Loss = -10924.87812051786
Iteration 1400: Loss = -10924.782520174196
Iteration 1500: Loss = -10924.699011030327
Iteration 1600: Loss = -10924.619603764166
Iteration 1700: Loss = -10924.541208384802
Iteration 1800: Loss = -10924.456811680346
Iteration 1900: Loss = -10924.356471832089
Iteration 2000: Loss = -10924.22518787866
Iteration 2100: Loss = -10924.065473344532
Iteration 2200: Loss = -10923.914219145949
Iteration 2300: Loss = -10923.797828033303
Iteration 2400: Loss = -10923.716496287312
Iteration 2500: Loss = -10923.659837168932
Iteration 2600: Loss = -10923.61957050219
Iteration 2700: Loss = -10923.58940430193
Iteration 2800: Loss = -10923.562872279625
Iteration 2900: Loss = -10923.529101832988
Iteration 3000: Loss = -10923.493839391218
Iteration 3100: Loss = -10923.420952305883
Iteration 3200: Loss = -10923.388874543458
Iteration 3300: Loss = -10923.426523462991
1
Iteration 3400: Loss = -10923.351709441502
Iteration 3500: Loss = -10923.340579968797
Iteration 3600: Loss = -10923.332360076018
Iteration 3700: Loss = -10923.327187032723
Iteration 3800: Loss = -10923.320971301466
Iteration 3900: Loss = -10923.31686368018
Iteration 4000: Loss = -10923.313369154397
Iteration 4100: Loss = -10923.312100849194
Iteration 4200: Loss = -10923.307107820567
Iteration 4300: Loss = -10923.304137271765
Iteration 4400: Loss = -10923.34724912005
1
Iteration 4500: Loss = -10923.299806073965
Iteration 4600: Loss = -10923.29844236348
Iteration 4700: Loss = -10923.29726666703
Iteration 4800: Loss = -10923.297576608244
1
Iteration 4900: Loss = -10923.295236124863
Iteration 5000: Loss = -10923.294366390879
Iteration 5100: Loss = -10923.298098896328
1
Iteration 5200: Loss = -10923.292834848966
Iteration 5300: Loss = -10923.292094112256
Iteration 5400: Loss = -10923.291410113714
Iteration 5500: Loss = -10923.303812254771
1
Iteration 5600: Loss = -10923.29026111239
Iteration 5700: Loss = -10923.28974229488
Iteration 5800: Loss = -10923.317645481264
1
Iteration 5900: Loss = -10923.288817237762
Iteration 6000: Loss = -10923.288362578349
Iteration 6100: Loss = -10923.287982686968
Iteration 6200: Loss = -10923.288314815905
1
Iteration 6300: Loss = -10923.287243414628
Iteration 6400: Loss = -10923.286874172762
Iteration 6500: Loss = -10923.36477881887
1
Iteration 6600: Loss = -10923.28628219708
Iteration 6700: Loss = -10923.286005841735
Iteration 6800: Loss = -10923.285793597837
Iteration 6900: Loss = -10923.285499012767
Iteration 7000: Loss = -10923.28528656179
Iteration 7100: Loss = -10923.285051384852
Iteration 7200: Loss = -10923.28536543235
1
Iteration 7300: Loss = -10923.284653254792
Iteration 7400: Loss = -10923.284463105882
Iteration 7500: Loss = -10923.31540650573
1
Iteration 7600: Loss = -10923.28414190421
Iteration 7700: Loss = -10923.283997027824
Iteration 7800: Loss = -10923.283858885103
Iteration 7900: Loss = -10923.285749276765
1
Iteration 8000: Loss = -10923.283614687089
Iteration 8100: Loss = -10923.283463928567
Iteration 8200: Loss = -10923.283338193825
Iteration 8300: Loss = -10923.283761997407
1
Iteration 8400: Loss = -10923.283160598352
Iteration 8500: Loss = -10923.283035383001
Iteration 8600: Loss = -10923.286431699504
1
Iteration 8700: Loss = -10923.282882270374
Iteration 8800: Loss = -10923.28278316447
Iteration 8900: Loss = -10923.282694372749
Iteration 9000: Loss = -10923.282675978717
Iteration 9100: Loss = -10923.282534600223
Iteration 9200: Loss = -10923.282487429793
Iteration 9300: Loss = -10923.28295133994
1
Iteration 9400: Loss = -10923.282355728006
Iteration 9500: Loss = -10923.282323111966
Iteration 9600: Loss = -10923.28226414274
Iteration 9700: Loss = -10923.282720464564
1
Iteration 9800: Loss = -10923.28216502466
Iteration 9900: Loss = -10923.282129891704
Iteration 10000: Loss = -10923.283076575704
1
Iteration 10100: Loss = -10923.282041840877
Iteration 10200: Loss = -10923.282088798165
1
Iteration 10300: Loss = -10923.281963980007
Iteration 10400: Loss = -10923.283373598006
1
Iteration 10500: Loss = -10923.281875809254
Iteration 10600: Loss = -10923.282422985594
1
Iteration 10700: Loss = -10923.283535891087
2
Iteration 10800: Loss = -10923.281829409692
Iteration 10900: Loss = -10923.28312965128
1
Iteration 11000: Loss = -10923.282343598456
2
Iteration 11100: Loss = -10923.281811856281
Iteration 11200: Loss = -10923.285164054441
1
Iteration 11300: Loss = -10923.281724081053
Iteration 11400: Loss = -10923.281785223295
1
Iteration 11500: Loss = -10923.281650412702
Iteration 11600: Loss = -10923.283180068802
1
Iteration 11700: Loss = -10923.284276486496
2
Iteration 11800: Loss = -10923.281856230858
3
Iteration 11900: Loss = -10923.285381627209
4
Iteration 12000: Loss = -10923.28163855537
Iteration 12100: Loss = -10923.285610148261
1
Iteration 12200: Loss = -10923.281501663063
Iteration 12300: Loss = -10923.282116427166
1
Iteration 12400: Loss = -10923.532725928735
2
Iteration 12500: Loss = -10923.282126545368
3
Iteration 12600: Loss = -10923.409020540312
4
Iteration 12700: Loss = -10923.281423739127
Iteration 12800: Loss = -10923.281425746707
1
Iteration 12900: Loss = -10923.281430891313
2
Iteration 13000: Loss = -10923.281442331294
3
Iteration 13100: Loss = -10923.28680745333
4
Iteration 13200: Loss = -10923.281408649613
Iteration 13300: Loss = -10923.2870069403
1
Iteration 13400: Loss = -10923.281611011307
2
Iteration 13500: Loss = -10923.281703423576
3
Iteration 13600: Loss = -10923.333995384952
4
Iteration 13700: Loss = -10923.28568143381
5
Iteration 13800: Loss = -10923.284422965547
6
Iteration 13900: Loss = -10923.281373192212
Iteration 14000: Loss = -10923.282650507872
1
Iteration 14100: Loss = -10923.28747460622
2
Iteration 14200: Loss = -10923.281329709933
Iteration 14300: Loss = -10923.283760896831
1
Iteration 14400: Loss = -10923.309698516045
2
Iteration 14500: Loss = -10923.28251371481
3
Iteration 14600: Loss = -10923.282221374819
4
Iteration 14700: Loss = -10923.289699906234
5
Iteration 14800: Loss = -10923.281324593674
Iteration 14900: Loss = -10923.28129415395
Iteration 15000: Loss = -10923.281557330654
1
Iteration 15100: Loss = -10923.281510291401
2
Iteration 15200: Loss = -10923.318388609057
3
Iteration 15300: Loss = -10923.281306932053
4
Iteration 15400: Loss = -10923.281445071058
5
Iteration 15500: Loss = -10923.28142705963
6
Iteration 15600: Loss = -10923.281267943375
Iteration 15700: Loss = -10923.343067523409
1
Iteration 15800: Loss = -10923.28201038051
2
Iteration 15900: Loss = -10923.28135877896
3
Iteration 16000: Loss = -10923.28137296362
4
Iteration 16100: Loss = -10923.288499013368
5
Iteration 16200: Loss = -10923.281301859453
6
Iteration 16300: Loss = -10923.289093414756
7
Iteration 16400: Loss = -10923.281704761876
8
Iteration 16500: Loss = -10923.281462135024
9
Iteration 16600: Loss = -10923.423149123359
10
Stopping early at iteration 16600 due to no improvement.
tensor([[  3.4424,  -5.0850],
        [  4.4941,  -6.7653],
        [  3.3578,  -4.7588],
        [  5.4362,  -7.2643],
        [  1.5910,  -3.2418],
        [  4.2255,  -6.8563],
        [  3.2504,  -5.9255],
        [  4.1131,  -6.6463],
        [  6.0562,  -7.4643],
        [ -1.0547,  -0.7553],
        [  4.1521,  -5.5400],
        [  3.1139,  -4.9937],
        [  5.4861,  -7.1055],
        [  5.9717,  -7.3590],
        [  5.6303, -10.2455],
        [  6.5459,  -8.7855],
        [  7.2482,  -8.6376],
        [  3.0335,  -5.3658],
        [  6.2780,  -7.6910],
        [  5.3917,  -6.8131],
        [  7.1109,  -8.9367],
        [  5.7735,  -7.2136],
        [ -0.9714,  -0.4394],
        [  5.6777,  -7.3804],
        [  2.2953,  -4.5623],
        [  5.0413,  -6.5935],
        [  0.9907,  -2.5845],
        [  0.4931,  -2.3303],
        [  6.5797,  -7.9758],
        [  2.0765,  -4.4578],
        [  4.2832,  -5.7209],
        [  4.1251,  -6.5269],
        [  5.7333,  -7.1213],
        [  4.6995,  -7.2196],
        [  5.6175,  -7.0199],
        [  4.3268,  -5.7322],
        [  3.8496,  -5.3313],
        [  6.0939,  -7.6636],
        [  3.6196,  -6.8231],
        [  2.0665,  -6.6817],
        [  3.1836,  -4.6645],
        [  0.4241,  -5.0393],
        [  4.1140,  -7.3937],
        [  4.7471,  -6.9334],
        [  4.1847,  -6.1778],
        [  4.4810,  -5.8806],
        [  2.4703,  -5.1420],
        [  3.5100,  -5.0458],
        [  2.6387,  -4.6791],
        [ -0.4293,  -1.1393],
        [  4.5834,  -5.9775],
        [  1.9521,  -6.3601],
        [  4.9575,  -6.3743],
        [  3.7100,  -7.5387],
        [  4.2598,  -6.5228],
        [  3.0161,  -4.5734],
        [  7.1384,  -8.5416],
        [  3.0698,  -5.7366],
        [  4.1801,  -5.5800],
        [  4.1130,  -5.9027],
        [  6.9339,  -8.5038],
        [  6.5773,  -7.9704],
        [  5.6354,  -7.1489],
        [  5.1881,  -6.9296],
        [  4.5730,  -6.2453],
        [  3.4969,  -6.9571],
        [  5.6206,  -7.0440],
        [  2.8964,  -4.4331],
        [  2.9342,  -5.0127],
        [  3.3510,  -6.6985],
        [  5.6372,  -7.2226],
        [  4.9883,  -7.2562],
        [  1.8579,  -4.2161],
        [  6.1520,  -7.5391],
        [  5.1375,  -6.5243],
        [  1.8580,  -3.2907],
        [  3.5835,  -4.9765],
        [  6.6568,  -8.9837],
        [  3.6136,  -5.5563],
        [  2.9508,  -4.6349],
        [  0.3996,  -4.7902],
        [  5.7848,  -7.4626],
        [  4.1927,  -8.8080],
        [  3.0015,  -5.4841],
        [  3.1989,  -5.1312],
        [  4.8677,  -6.3247],
        [  2.6776,  -4.1480],
        [  3.8170,  -5.7326],
        [  4.6319,  -6.0260],
        [  6.6724,  -8.2621],
        [  7.2871,  -8.8132],
        [  1.9508,  -3.9480],
        [  2.7141,  -4.1058],
        [  0.2706,  -4.2781],
        [  4.0699,  -5.5250],
        [  7.3152,  -9.6358],
        [  5.1977,  -6.9485],
        [  3.4726,  -5.4485],
        [  6.2585,  -8.0292],
        [  4.9883,  -6.6990]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9373e-01, 6.2721e-03],
        [2.0496e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9836, 0.0164], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1584, 0.1378],
         [0.9385, 0.5211]],

        [[0.4631, 0.1500],
         [0.7587, 0.1707]],

        [[0.6617, 0.2315],
         [0.2191, 0.6597]],

        [[0.6992, 0.2180],
         [0.6927, 0.2603]],

        [[0.9314, 0.1649],
         [0.1492, 0.1222]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0007766707522784365
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.008484848484848486
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
Global Adjusted Rand Index: -0.0003824099409314635
Average Adjusted Rand Index: -0.0028886639223549134
Iteration 0: Loss = -18844.28596557808
Iteration 10: Loss = -10926.682591160286
Iteration 20: Loss = -10926.446708374588
Iteration 30: Loss = -10926.266910461058
Iteration 40: Loss = -10926.170351063789
Iteration 50: Loss = -10926.11796035669
Iteration 60: Loss = -10926.08526938403
Iteration 70: Loss = -10926.064180568565
Iteration 80: Loss = -10926.05224007035
Iteration 90: Loss = -10926.046717389187
Iteration 100: Loss = -10926.045149712441
Iteration 110: Loss = -10926.045993553225
1
Iteration 120: Loss = -10926.048253591527
2
Iteration 130: Loss = -10926.051396720699
3
Stopping early at iteration 129 due to no improvement.
pi: tensor([[0.1760, 0.8240],
        [0.0365, 0.9635]], dtype=torch.float64)
alpha: tensor([0.0420, 0.9580])
beta: tensor([[[0.2013, 0.1498],
         [0.9406, 0.1577]],

        [[0.0484, 0.1650],
         [0.1009, 0.2441]],

        [[0.8958, 0.2001],
         [0.3564, 0.4312]],

        [[0.7155, 0.2023],
         [0.8702, 0.3298]],

        [[0.3391, 0.1745],
         [0.9118, 0.6471]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18844.04484593589
Iteration 100: Loss = -10928.007015803138
Iteration 200: Loss = -10926.82152195831
Iteration 300: Loss = -10926.534164566174
Iteration 400: Loss = -10926.410356112896
Iteration 500: Loss = -10926.315898713281
Iteration 600: Loss = -10926.222643602881
Iteration 700: Loss = -10926.130680389466
Iteration 800: Loss = -10926.045787901654
Iteration 900: Loss = -10925.97421067402
Iteration 1000: Loss = -10925.916226939124
Iteration 1100: Loss = -10925.867396253509
Iteration 1200: Loss = -10925.81779953592
Iteration 1300: Loss = -10925.757266406026
Iteration 1400: Loss = -10925.678241679678
Iteration 1500: Loss = -10925.593154089482
Iteration 1600: Loss = -10925.487154731365
Iteration 1700: Loss = -10925.36139098976
Iteration 1800: Loss = -10925.229821167655
Iteration 1900: Loss = -10925.109792856727
Iteration 2000: Loss = -10924.993553660544
Iteration 2100: Loss = -10924.886335577703
Iteration 2200: Loss = -10924.784050324028
Iteration 2300: Loss = -10924.680962987479
Iteration 2400: Loss = -10924.56394498462
Iteration 2500: Loss = -10924.425319443657
Iteration 2600: Loss = -10924.246530361643
Iteration 2700: Loss = -10924.034125255186
Iteration 2800: Loss = -10923.844431906598
Iteration 2900: Loss = -10923.704867945315
Iteration 3000: Loss = -10923.60702509197
Iteration 3100: Loss = -10923.537771191144
Iteration 3200: Loss = -10923.487281948574
Iteration 3300: Loss = -10923.449206898653
Iteration 3400: Loss = -10923.41979050977
Iteration 3500: Loss = -10923.396577530128
Iteration 3600: Loss = -10923.378099206691
Iteration 3700: Loss = -10923.363457012169
Iteration 3800: Loss = -10923.376587559744
1
Iteration 3900: Loss = -10923.34255745866
Iteration 4000: Loss = -10923.33507594632
Iteration 4100: Loss = -10923.328940240464
Iteration 4200: Loss = -10923.32421425043
Iteration 4300: Loss = -10923.31978375771
Iteration 4400: Loss = -10923.316260530766
Iteration 4500: Loss = -10923.324574813478
1
Iteration 4600: Loss = -10923.310750037552
Iteration 4700: Loss = -10923.30856067294
Iteration 4800: Loss = -10923.315371040782
1
Iteration 4900: Loss = -10923.304916398272
Iteration 5000: Loss = -10923.303408117676
Iteration 5100: Loss = -10923.302102689873
Iteration 5200: Loss = -10923.301516050828
Iteration 5300: Loss = -10923.299830434924
Iteration 5400: Loss = -10923.29882426627
Iteration 5500: Loss = -10923.387558333583
1
Iteration 5600: Loss = -10923.29716051945
Iteration 5700: Loss = -10923.296405037134
Iteration 5800: Loss = -10923.295718314457
Iteration 5900: Loss = -10923.29550194608
Iteration 6000: Loss = -10923.294502962124
Iteration 6100: Loss = -10923.29397039501
Iteration 6200: Loss = -10923.743814281703
1
Iteration 6300: Loss = -10923.293037741198
Iteration 6400: Loss = -10923.29257947918
Iteration 6500: Loss = -10923.292143643532
Iteration 6600: Loss = -10923.294553039334
1
Iteration 6700: Loss = -10923.291400739827
Iteration 6800: Loss = -10923.291079347278
Iteration 6900: Loss = -10923.292005224077
1
Iteration 7000: Loss = -10923.290424866298
Iteration 7100: Loss = -10923.290140760004
Iteration 7200: Loss = -10923.417070078078
1
Iteration 7300: Loss = -10923.289573252245
Iteration 7400: Loss = -10923.289308006239
Iteration 7500: Loss = -10923.289076473602
Iteration 7600: Loss = -10923.28885112814
Iteration 7700: Loss = -10923.288341637754
Iteration 7800: Loss = -10923.28787623021
Iteration 7900: Loss = -10923.302858494966
1
Iteration 8000: Loss = -10923.287360903907
Iteration 8100: Loss = -10923.287203622594
Iteration 8200: Loss = -10923.399675794446
1
Iteration 8300: Loss = -10923.286867314893
Iteration 8400: Loss = -10923.286730423084
Iteration 8500: Loss = -10923.286626017703
Iteration 8600: Loss = -10923.287332236057
1
Iteration 8700: Loss = -10923.286367542249
Iteration 8800: Loss = -10923.28621321542
Iteration 8900: Loss = -10923.286562739448
1
Iteration 9000: Loss = -10923.285998586687
Iteration 9100: Loss = -10923.285913412734
Iteration 9200: Loss = -10923.837476418123
1
Iteration 9300: Loss = -10923.285760993464
Iteration 9400: Loss = -10923.28562725771
Iteration 9500: Loss = -10923.285531385893
Iteration 9600: Loss = -10923.287560253022
1
Iteration 9700: Loss = -10923.285341087418
Iteration 9800: Loss = -10923.28526655634
Iteration 9900: Loss = -10923.35451108832
1
Iteration 10000: Loss = -10923.285089990972
Iteration 10100: Loss = -10923.284878352026
Iteration 10200: Loss = -10923.2847379837
Iteration 10300: Loss = -10923.28508648165
1
Iteration 10400: Loss = -10923.284594485396
Iteration 10500: Loss = -10923.284470105282
Iteration 10600: Loss = -10923.300216884769
1
Iteration 10700: Loss = -10923.284207219467
Iteration 10800: Loss = -10923.284078437486
Iteration 10900: Loss = -10923.284027959522
Iteration 11000: Loss = -10923.28411484742
1
Iteration 11100: Loss = -10923.283944146038
Iteration 11200: Loss = -10923.283894794016
Iteration 11300: Loss = -10923.28431605781
1
Iteration 11400: Loss = -10923.283851947217
Iteration 11500: Loss = -10923.283824162612
Iteration 11600: Loss = -10923.28535148188
1
Iteration 11700: Loss = -10923.283757516525
Iteration 11800: Loss = -10923.283749866365
Iteration 11900: Loss = -10923.283731522808
Iteration 12000: Loss = -10923.375083773613
1
Iteration 12100: Loss = -10923.28367792293
Iteration 12200: Loss = -10923.283676730249
Iteration 12300: Loss = -10923.283648721585
Iteration 12400: Loss = -10923.284037498284
1
Iteration 12500: Loss = -10923.283582173275
Iteration 12600: Loss = -10923.283541722478
Iteration 12700: Loss = -10923.317175857612
1
Iteration 12800: Loss = -10923.284576536795
2
Iteration 12900: Loss = -10923.283434126892
Iteration 13000: Loss = -10923.295959218593
1
Iteration 13100: Loss = -10923.283408217092
Iteration 13200: Loss = -10923.283429310559
1
Iteration 13300: Loss = -10923.284637934312
2
Iteration 13400: Loss = -10923.2837458831
3
Iteration 13500: Loss = -10923.285322709176
4
Iteration 13600: Loss = -10923.283939606177
5
Iteration 13700: Loss = -10923.315333082435
6
Iteration 13800: Loss = -10923.28518823892
7
Iteration 13900: Loss = -10923.306781222427
8
Iteration 14000: Loss = -10923.283263884614
Iteration 14100: Loss = -10923.28499873733
1
Iteration 14200: Loss = -10923.283328533294
2
Iteration 14300: Loss = -10923.284403057345
3
Iteration 14400: Loss = -10923.28420764273
4
Iteration 14500: Loss = -10923.284084927936
5
Iteration 14600: Loss = -10923.283259322718
Iteration 14700: Loss = -10923.385604157535
1
Iteration 14800: Loss = -10923.283382319609
2
Iteration 14900: Loss = -10923.283224515042
Iteration 15000: Loss = -10923.284433776858
1
Iteration 15100: Loss = -10923.283299904757
2
Iteration 15200: Loss = -10923.283866096426
3
Iteration 15300: Loss = -10923.396769578765
4
Iteration 15400: Loss = -10923.283113371568
Iteration 15500: Loss = -10923.28307480682
Iteration 15600: Loss = -10923.283088298453
1
Iteration 15700: Loss = -10923.288957525183
2
Iteration 15800: Loss = -10923.283078102459
3
Iteration 15900: Loss = -10923.284055218664
4
Iteration 16000: Loss = -10923.28307528994
5
Iteration 16100: Loss = -10923.300355717845
6
Iteration 16200: Loss = -10923.28303700174
Iteration 16300: Loss = -10923.283224181567
1
Iteration 16400: Loss = -10923.283219805895
2
Iteration 16500: Loss = -10923.283385657749
3
Iteration 16600: Loss = -10923.286613954539
4
Iteration 16700: Loss = -10923.282941351368
Iteration 16800: Loss = -10923.283116350703
1
Iteration 16900: Loss = -10923.28302697691
2
Iteration 17000: Loss = -10923.295484256103
3
Iteration 17100: Loss = -10923.288192631226
4
Iteration 17200: Loss = -10923.283093592792
5
Iteration 17300: Loss = -10923.283158394537
6
Iteration 17400: Loss = -10923.283204917336
7
Iteration 17500: Loss = -10923.283489069505
8
Iteration 17600: Loss = -10923.283268068291
9
Iteration 17700: Loss = -10923.283019228078
10
Stopping early at iteration 17700 due to no improvement.
tensor([[ -5.0256,   3.5108],
        [ -6.4817,   4.7990],
        [ -4.7646,   3.3634],
        [ -7.0642,   5.6690],
        [ -3.7818,   1.0647],
        [ -6.3103,   4.7970],
        [-10.6268,   6.1925],
        [ -6.1296,   4.6477],
        [ -9.2265,   7.3423],
        [ -1.2730,  -1.5511],
        [ -5.9884,   3.7137],
        [ -4.7865,   3.3352],
        [ -7.0390,   5.6058],
        [ -8.7704,   7.0640],
        [ -9.2119,   7.6449],
        [ -9.5512,   6.3207],
        [ -9.1537,   7.6291],
        [ -4.9037,   3.5136],
        [ -9.0341,   7.6359],
        [ -9.1295,   7.5268],
        [ -9.2733,   7.8846],
        [ -9.4153,   7.3962],
        [ -0.7190,  -1.2284],
        [ -8.8940,   6.6287],
        [ -4.1850,   2.6918],
        [ -7.2600,   4.3880],
        [ -2.4789,   1.0860],
        [ -2.1328,   0.6769],
        [ -8.8607,   7.1702],
        [ -4.5425,   1.9766],
        [ -5.7270,   4.2886],
        [ -6.0746,   4.5953],
        [ -8.1073,   4.7654],
        [ -6.6584,   5.2624],
        [ -9.1669,   7.1720],
        [ -6.0902,   3.9779],
        [ -5.4725,   3.7264],
        [ -7.8191,   6.2238],
        [ -5.9249,   4.5379],
        [ -5.0961,   3.6670],
        [ -4.6346,   3.2315],
        [ -3.6408,   1.8366],
        [ -8.0752,   3.4600],
        [ -8.2686,   6.8226],
        [ -5.9797,   4.4038],
        [ -5.9204,   4.4531],
        [ -4.7708,   2.8342],
        [ -5.1703,   3.4002],
        [ -4.4050,   2.9259],
        [ -1.9857,  -1.2611],
        [ -5.9973,   4.5880],
        [ -6.4609,   1.8456],
        [ -6.4228,   4.9181],
        [ -6.3252,   4.9389],
        [ -7.7120,   3.0968],
        [ -4.7964,   2.7991],
        [ -9.6077,   6.9975],
        [ -5.1813,   3.6316],
        [ -5.9342,   3.8449],
        [ -8.7715,   7.3533],
        [ -8.8789,   7.3725],
        [ -8.5923,   7.1902],
        [ -8.8661,   7.2302],
        [ -6.8887,   5.2440],
        [ -9.0443,   6.6070],
        [ -6.0541,   4.4155],
        [ -7.2802,   5.3842],
        [ -4.4758,   2.8667],
        [ -4.7096,   3.2353],
        [ -7.0029,   3.0635],
        [ -9.4672,   7.7414],
        [ -7.5980,   4.6582],
        [ -5.0239,   1.0579],
        [ -7.5607,   6.1315],
        [ -6.7675,   4.9071],
        [ -3.4312,   1.7042],
        [ -4.9900,   3.5834],
        [-10.8918,   6.3078],
        [ -5.2877,   3.9012],
        [ -4.5006,   3.0963],
        [ -3.6113,   1.5924],
        [ -9.3022,   7.6978],
        [ -8.6501,   6.9394],
        [ -4.9476,   3.5529],
        [ -5.1891,   3.1586],
        [ -6.5394,   4.6731],
        [ -4.1498,   2.6823],
        [ -5.6079,   3.9615],
        [ -6.2543,   4.4225],
        [ -8.9492,   7.4889],
        [-10.0021,   7.3240],
        [ -3.6480,   2.2491],
        [ -4.8526,   1.9847],
        [ -3.2809,   1.2571],
        [ -5.4981,   4.1109],
        [ -9.3036,   7.6658],
        [ -7.0375,   5.1311],
        [ -5.3306,   3.6031],
        [ -9.3740,   7.1283],
        [ -7.1680,   4.5302]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 1.7650e-06],
        [6.2157e-03, 9.9378e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0165, 0.9835], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.5218, 0.1370],
         [0.9406, 0.1591]],

        [[0.0484, 0.1490],
         [0.1009, 0.2441]],

        [[0.8958, 0.2325],
         [0.3564, 0.4312]],

        [[0.7155, 0.2190],
         [0.8702, 0.3298]],

        [[0.3391, 0.1642],
         [0.9118, 0.6471]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007766707522784365
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
Global Adjusted Rand Index: -0.0003824099409314635
Average Adjusted Rand Index: -0.0028886639223549134
10842.036697650363
new:  [0.00022591370147976686, -0.0003824099409314635, -0.0003824099409314635, -0.0003824099409314635] [0.0, -0.0028886639223549134, -0.0028886639223549134, -0.0028886639223549134] [10925.702883308384, 10923.321114646727, 10923.423149123359, 10923.283019228078]
prior:  [0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0] [10926.31941613506, 10926.052702345329, 10926.04953978766, 10926.051396720699]
-----------------------------------------------------------------------------------------
This iteration is 5
True Objective function: Loss = -11003.77711673985
Iteration 0: Loss = -31429.57657062477
Iteration 10: Loss = -11115.446472399519
Iteration 20: Loss = -11115.020004302643
Iteration 30: Loss = -11114.887686962615
Iteration 40: Loss = -11114.377877814974
Iteration 50: Loss = -11113.833637583355
Iteration 60: Loss = -11113.508808813596
Iteration 70: Loss = -11113.214117201622
Iteration 80: Loss = -11112.93813283875
Iteration 90: Loss = -11112.682153594265
Iteration 100: Loss = -11112.45187441959
Iteration 110: Loss = -11112.251545976576
Iteration 120: Loss = -11112.081495875122
Iteration 130: Loss = -11111.939186096151
Iteration 140: Loss = -11111.821133908841
Iteration 150: Loss = -11111.723664253004
Iteration 160: Loss = -11111.643316283835
Iteration 170: Loss = -11111.577272754206
Iteration 180: Loss = -11111.523073100707
Iteration 190: Loss = -11111.478828659896
Iteration 200: Loss = -11111.442757068153
Iteration 210: Loss = -11111.413549192981
Iteration 220: Loss = -11111.390054158144
Iteration 230: Loss = -11111.371412470984
Iteration 240: Loss = -11111.356756049037
Iteration 250: Loss = -11111.345393074194
Iteration 260: Loss = -11111.33679691329
Iteration 270: Loss = -11111.33050358925
Iteration 280: Loss = -11111.32610645192
Iteration 290: Loss = -11111.323217681611
Iteration 300: Loss = -11111.321609119113
Iteration 310: Loss = -11111.32100035726
Iteration 320: Loss = -11111.32118719538
1
Iteration 330: Loss = -11111.322021801923
2
Iteration 340: Loss = -11111.323329824563
3
Stopping early at iteration 339 due to no improvement.
pi: tensor([[0.6214, 0.3786],
        [0.5046, 0.4954]], dtype=torch.float64)
alpha: tensor([0.5726, 0.4274])
beta: tensor([[[0.1387, 0.1680],
         [0.0145, 0.2076]],

        [[0.1278, 0.1653],
         [0.3003, 0.5295]],

        [[0.6276, 0.1523],
         [0.7142, 0.3581]],

        [[0.9109, 0.1686],
         [0.9405, 0.0349]],

        [[0.7943, 0.1707],
         [0.0078, 0.2894]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.023108518696371746
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 63
Adjusted Rand Index: 0.05881238795385571
time is 2
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 70
Adjusted Rand Index: 0.15310162780466344
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 67
Adjusted Rand Index: 0.10727875591880681
time is 4
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 76
Adjusted Rand Index: 0.263030303030303
Global Adjusted Rand Index: 0.11393913484196316
Average Adjusted Rand Index: 0.12106631868080013
tensor([[-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31413.30117066082
Iteration 100: Loss = -11123.437599476383
Iteration 200: Loss = -11120.306965871305
Iteration 300: Loss = -11119.135632540896
Iteration 400: Loss = -11118.604093960435
Iteration 500: Loss = -11118.307039432706
Iteration 600: Loss = -11118.101970604928
Iteration 700: Loss = -11117.940423881651
Iteration 800: Loss = -11117.810261258099
Iteration 900: Loss = -11117.703555870004
Iteration 1000: Loss = -11117.631126092376
Iteration 1100: Loss = -11117.59339876201
Iteration 1200: Loss = -11117.577150411313
Iteration 1300: Loss = -11117.568395437433
Iteration 1400: Loss = -11117.561864446981
Iteration 1500: Loss = -11117.556289627753
Iteration 1600: Loss = -11117.55124973959
Iteration 1700: Loss = -11117.546492195994
Iteration 1800: Loss = -11117.541916531221
Iteration 1900: Loss = -11117.537431867631
Iteration 2000: Loss = -11117.53295079863
Iteration 2100: Loss = -11117.528400868203
Iteration 2200: Loss = -11117.523794773297
Iteration 2300: Loss = -11117.519211117344
Iteration 2400: Loss = -11117.514596199264
Iteration 2500: Loss = -11117.510071003264
Iteration 2600: Loss = -11117.505883663865
Iteration 2700: Loss = -11117.501956260998
Iteration 2800: Loss = -11117.498464826876
Iteration 2900: Loss = -11117.495347371614
Iteration 3000: Loss = -11117.492665322072
Iteration 3100: Loss = -11117.4901892009
Iteration 3200: Loss = -11117.4879262524
Iteration 3300: Loss = -11117.48583826997
Iteration 3400: Loss = -11117.483834729443
Iteration 3500: Loss = -11117.48189598373
Iteration 3600: Loss = -11117.48004022565
Iteration 3700: Loss = -11117.478169839265
Iteration 3800: Loss = -11117.476323654639
Iteration 3900: Loss = -11117.474551952073
Iteration 4000: Loss = -11117.472660585907
Iteration 4100: Loss = -11117.470802395153
Iteration 4200: Loss = -11117.4687324584
Iteration 4300: Loss = -11117.46617740762
Iteration 4400: Loss = -11117.462404388176
Iteration 4500: Loss = -11117.457696907064
Iteration 4600: Loss = -11117.414369644306
Iteration 4700: Loss = -11117.251935970306
Iteration 4800: Loss = -11117.218121034251
Iteration 4900: Loss = -11117.203002235985
Iteration 5000: Loss = -11117.196233789204
Iteration 5100: Loss = -11117.20438796105
1
Iteration 5200: Loss = -11117.188274112357
Iteration 5300: Loss = -11117.184732973574
Iteration 5400: Loss = -11117.1838105391
Iteration 5500: Loss = -11117.170141526043
Iteration 5600: Loss = -11117.108177449538
Iteration 5700: Loss = -11116.69579620874
Iteration 5800: Loss = -11116.591244818976
Iteration 5900: Loss = -11116.552901927898
Iteration 6000: Loss = -11116.53402723164
Iteration 6100: Loss = -11116.521795283243
Iteration 6200: Loss = -11116.514398092913
Iteration 6300: Loss = -11116.51348450933
Iteration 6400: Loss = -11116.505786710513
Iteration 6500: Loss = -11116.503161541365
Iteration 6600: Loss = -11116.505507319602
1
Iteration 6700: Loss = -11116.499560181604
Iteration 6800: Loss = -11116.498347425513
Iteration 6900: Loss = -11116.498884891871
1
Iteration 7000: Loss = -11116.496457312842
Iteration 7100: Loss = -11116.49574011108
Iteration 7200: Loss = -11116.49618667572
1
Iteration 7300: Loss = -11116.494618529114
Iteration 7400: Loss = -11116.49414432719
Iteration 7500: Loss = -11116.496070989086
1
Iteration 7600: Loss = -11116.493449842686
Iteration 7700: Loss = -11116.49312536846
Iteration 7800: Loss = -11116.510014148813
1
Iteration 7900: Loss = -11116.492581116154
Iteration 8000: Loss = -11116.492323744467
Iteration 8100: Loss = -11116.50206704954
1
Iteration 8200: Loss = -11116.491902704005
Iteration 8300: Loss = -11116.491805855387
Iteration 8400: Loss = -11116.5210076607
1
Iteration 8500: Loss = -11116.491511224694
Iteration 8600: Loss = -11116.49136764608
Iteration 8700: Loss = -11116.502976783058
1
Iteration 8800: Loss = -11116.491137800347
Iteration 8900: Loss = -11116.491024124993
Iteration 9000: Loss = -11116.495849222822
1
Iteration 9100: Loss = -11116.490925994616
Iteration 9200: Loss = -11116.490785470598
Iteration 9300: Loss = -11116.592176091428
1
Iteration 9400: Loss = -11116.49064508807
Iteration 9500: Loss = -11116.490629707681
Iteration 9600: Loss = -11116.492159084617
1
Iteration 9700: Loss = -11116.490465565726
Iteration 9800: Loss = -11116.49041670433
Iteration 9900: Loss = -11116.490725672757
1
Iteration 10000: Loss = -11116.490314781351
Iteration 10100: Loss = -11116.492665912065
1
Iteration 10200: Loss = -11116.490244387514
Iteration 10300: Loss = -11116.49626216346
1
Iteration 10400: Loss = -11116.49020104834
Iteration 10500: Loss = -11116.490138169476
Iteration 10600: Loss = -11116.499096220945
1
Iteration 10700: Loss = -11116.490071604007
Iteration 10800: Loss = -11116.518531476851
1
Iteration 10900: Loss = -11116.490009637413
Iteration 11000: Loss = -11116.489989777852
Iteration 11100: Loss = -11116.490363885423
1
Iteration 11200: Loss = -11116.492205368795
2
Iteration 11300: Loss = -11116.489961127978
Iteration 11400: Loss = -11116.48997318158
1
Iteration 11500: Loss = -11116.489863568166
Iteration 11600: Loss = -11116.491924552807
1
Iteration 11700: Loss = -11116.489814917593
Iteration 11800: Loss = -11116.522068910932
1
Iteration 11900: Loss = -11116.489789226727
Iteration 12000: Loss = -11116.490704833814
1
Iteration 12100: Loss = -11116.489767512254
Iteration 12200: Loss = -11116.489782032035
1
Iteration 12300: Loss = -11116.48995328803
2
Iteration 12400: Loss = -11116.489718066592
Iteration 12500: Loss = -11116.491253359487
1
Iteration 12600: Loss = -11116.489714353507
Iteration 12700: Loss = -11116.501399795523
1
Iteration 12800: Loss = -11116.489736362459
2
Iteration 12900: Loss = -11116.489690212218
Iteration 13000: Loss = -11116.505510559988
1
Iteration 13100: Loss = -11116.489660242712
Iteration 13200: Loss = -11116.489681568672
1
Iteration 13300: Loss = -11116.498900476414
2
Iteration 13400: Loss = -11116.489651936483
Iteration 13500: Loss = -11116.562776224908
1
Iteration 13600: Loss = -11116.489632774272
Iteration 13700: Loss = -11116.494112821136
1
Iteration 13800: Loss = -11116.493933484113
2
Iteration 13900: Loss = -11116.489648267727
3
Iteration 14000: Loss = -11116.73160947786
4
Iteration 14100: Loss = -11116.489617755533
Iteration 14200: Loss = -11116.490858430996
1
Iteration 14300: Loss = -11116.4896056719
Iteration 14400: Loss = -11116.550552806017
1
Iteration 14500: Loss = -11116.63636790094
2
Iteration 14600: Loss = -11116.490109728707
3
Iteration 14700: Loss = -11116.48978986491
4
Iteration 14800: Loss = -11116.508221563947
5
Iteration 14900: Loss = -11116.489558219006
Iteration 15000: Loss = -11116.49006244491
1
Iteration 15100: Loss = -11116.553293150173
2
Iteration 15200: Loss = -11116.489616423009
3
Iteration 15300: Loss = -11116.490345175853
4
Iteration 15400: Loss = -11116.490033264741
5
Iteration 15500: Loss = -11116.514612819501
6
Iteration 15600: Loss = -11116.489529603365
Iteration 15700: Loss = -11116.49058939027
1
Iteration 15800: Loss = -11116.511343165792
2
Iteration 15900: Loss = -11116.489568875633
3
Iteration 16000: Loss = -11116.489652073007
4
Iteration 16100: Loss = -11116.490337708101
5
Iteration 16200: Loss = -11116.491752432894
6
Iteration 16300: Loss = -11116.49284821708
7
Iteration 16400: Loss = -11116.489581866828
8
Iteration 16500: Loss = -11116.492393002602
9
Iteration 16600: Loss = -11116.489535585326
10
Stopping early at iteration 16600 due to no improvement.
tensor([[-1.7267, -2.8885],
        [-1.6663, -2.9490],
        [-1.5063, -3.1089],
        [-1.3775, -3.2378],
        [-1.4703, -3.1450],
        [-3.0803, -1.5350],
        [-1.5326, -3.0827],
        [-1.6982, -2.9170],
        [-1.2835, -3.3317],
        [-1.4437, -3.1715],
        [-1.3364, -3.2788],
        [-1.3910, -3.2242],
        [-1.3358, -3.2794],
        [-1.2907, -3.3245],
        [-1.3485, -3.2667],
        [-1.3553, -3.2599],
        [-1.4106, -3.2046],
        [-1.4662, -3.1490],
        [-1.4870, -3.1282],
        [-1.4699, -3.1453],
        [-1.3805, -3.2347],
        [-1.6092, -3.0061],
        [-1.8053, -2.8099],
        [-1.2445, -3.3707],
        [-1.6179, -2.9973],
        [-1.4796, -3.1356],
        [-1.4973, -3.1179],
        [-1.6822, -2.9331],
        [-1.3703, -3.2450],
        [-1.2922, -3.3230],
        [-1.4615, -3.1537],
        [-1.5786, -3.0366],
        [-1.5082, -3.1070],
        [-1.6374, -2.9778],
        [-1.4226, -3.1927],
        [-1.4664, -3.1488],
        [-1.3015, -3.3137],
        [-1.6098, -3.0054],
        [-1.4194, -3.1958],
        [-1.5364, -3.0788],
        [-1.4796, -3.1356],
        [-1.4826, -3.1326],
        [-1.3723, -3.2429],
        [-1.5351, -3.0801],
        [-1.6418, -2.9734],
        [-1.6406, -2.9747],
        [-1.4949, -3.1203],
        [-1.5918, -3.0234],
        [-1.2597, -3.3555],
        [-1.4682, -3.1470],
        [-1.4716, -3.1437],
        [-1.3610, -3.2543],
        [-1.3001, -3.3152],
        [-1.3342, -3.2810],
        [-1.3011, -3.3141],
        [-1.3262, -3.2890],
        [-1.4108, -3.2044],
        [-2.0739, -2.5414],
        [-1.4461, -3.1692],
        [-1.4870, -3.1282],
        [-1.5186, -3.0966],
        [-1.8396, -2.7756],
        [-1.6113, -3.0039],
        [-1.4713, -3.1439],
        [-1.4569, -3.1584],
        [-1.5674, -3.0478],
        [-1.2758, -3.3394],
        [-1.4885, -3.1267],
        [-1.4119, -3.2033],
        [-1.3810, -3.2342],
        [-1.8587, -2.7565],
        [-2.2228, -2.3924],
        [-1.2206, -3.3946],
        [-1.3244, -3.2908],
        [-1.5599, -3.0553],
        [-1.7117, -2.9036],
        [-1.5171, -3.0981],
        [-1.4441, -3.1712],
        [-1.2520, -3.3632],
        [-1.6272, -2.9880],
        [-1.4189, -3.1963],
        [-1.3565, -3.2587],
        [-1.5452, -3.0700],
        [-1.5684, -3.0469],
        [-1.3829, -3.2323],
        [-1.3177, -3.2975],
        [-1.6992, -2.9160],
        [-1.3508, -3.2644],
        [-1.5465, -3.0687],
        [-1.6612, -2.9541],
        [-1.3024, -3.3128],
        [-1.6278, -2.9874],
        [-1.7589, -2.8563],
        [-1.6248, -2.9904],
        [-1.7430, -2.8722],
        [-1.4208, -3.1944],
        [-1.5068, -3.1084],
        [-1.6052, -3.0101],
        [-1.4007, -3.2146],
        [-1.8588, -2.7564]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 6.6730e-08],
        [7.7723e-01, 2.2277e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8223, 0.1777], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1640, 0.1717],
         [0.0145, 0.1842]],

        [[0.1278, 0.2118],
         [0.3003, 0.5295]],

        [[0.6276, 0.2882],
         [0.7142, 0.3581]],

        [[0.9109, 0.1773],
         [0.9405, 0.0349]],

        [[0.7943, 0.9030],
         [0.0078, 0.2894]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0006863796963546269
Average Adjusted Rand Index: -0.0004293504388350744
Iteration 0: Loss = -29690.18658542952
Iteration 10: Loss = -11117.774804416815
Iteration 20: Loss = -11117.774804488872
1
Iteration 30: Loss = -11117.774815749877
2
Iteration 40: Loss = -11117.77477602599
Iteration 50: Loss = -11117.742933044252
Iteration 60: Loss = -11116.278212478248
Iteration 70: Loss = -11115.26671976229
Iteration 80: Loss = -11114.964049522705
Iteration 90: Loss = -11114.870242571073
Iteration 100: Loss = -11114.307446731586
Iteration 110: Loss = -11113.78691179875
Iteration 120: Loss = -11113.467802936153
Iteration 130: Loss = -11113.175988459487
Iteration 140: Loss = -11112.902465442749
Iteration 150: Loss = -11112.64954218999
Iteration 160: Loss = -11112.423177519482
Iteration 170: Loss = -11112.22698924902
Iteration 180: Loss = -11112.060804291385
Iteration 190: Loss = -11111.92198554286
Iteration 200: Loss = -11111.806925093428
Iteration 210: Loss = -11111.711910887256
Iteration 220: Loss = -11111.633645817636
Iteration 230: Loss = -11111.569347843968
Iteration 240: Loss = -11111.516606671728
Iteration 250: Loss = -11111.473507138497
Iteration 260: Loss = -11111.438426067743
Iteration 270: Loss = -11111.410087955703
Iteration 280: Loss = -11111.387325322357
Iteration 290: Loss = -11111.369239393698
Iteration 300: Loss = -11111.355041228962
Iteration 310: Loss = -11111.344109978281
Iteration 320: Loss = -11111.335853037926
Iteration 330: Loss = -11111.329792871406
Iteration 340: Loss = -11111.32557847362
Iteration 350: Loss = -11111.322976391142
Iteration 360: Loss = -11111.321462677253
Iteration 370: Loss = -11111.320959335873
Iteration 380: Loss = -11111.321240972473
1
Iteration 390: Loss = -11111.322169180772
2
Iteration 400: Loss = -11111.323502935973
3
Stopping early at iteration 399 due to no improvement.
pi: tensor([[0.4959, 0.5041],
        [0.3791, 0.6209]], dtype=torch.float64)
alpha: tensor([0.4279, 0.5721])
beta: tensor([[[0.2075, 0.1680],
         [0.0688, 0.1387]],

        [[0.9168, 0.1653],
         [0.3844, 0.4553]],

        [[0.6133, 0.1522],
         [0.8658, 0.8339]],

        [[0.9672, 0.1686],
         [0.4626, 0.2551]],

        [[0.1084, 0.1706],
         [0.4265, 0.6061]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.03072883317374626
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.04861756599398833
time is 2
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 30
Adjusted Rand Index: 0.15310162780466344
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 33
Adjusted Rand Index: 0.10727875591880681
time is 4
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 24
Adjusted Rand Index: 0.263030303030303
Global Adjusted Rand Index: 0.11393313593040864
Average Adjusted Rand Index: 0.12055141718430158
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29647.407539957905
Iteration 100: Loss = -11174.520941679868
Iteration 200: Loss = -11141.299477075629
Iteration 300: Loss = -11127.30489884015
Iteration 400: Loss = -11121.560226124278
Iteration 500: Loss = -11120.33515411266
Iteration 600: Loss = -11119.626930813658
Iteration 700: Loss = -11119.16233335327
Iteration 800: Loss = -11118.836796188647
Iteration 900: Loss = -11118.597360727215
Iteration 1000: Loss = -11118.413693800167
Iteration 1100: Loss = -11118.268773596508
Iteration 1200: Loss = -11118.151508255132
Iteration 1300: Loss = -11118.053976483736
Iteration 1400: Loss = -11117.969063119714
Iteration 1500: Loss = -11117.88475358613
Iteration 1600: Loss = -11117.71885041344
Iteration 1700: Loss = -11116.915639473067
Iteration 1800: Loss = -11116.581481829191
Iteration 1900: Loss = -11116.38186816887
Iteration 2000: Loss = -11116.09946135949
Iteration 2100: Loss = -11115.966221021952
Iteration 2200: Loss = -11115.866986459709
Iteration 2300: Loss = -11115.816441233286
Iteration 2400: Loss = -11115.76878246932
Iteration 2500: Loss = -11115.646247417963
Iteration 2600: Loss = -11115.579513598852
Iteration 2700: Loss = -11115.521405724545
Iteration 2800: Loss = -11115.362968835345
Iteration 2900: Loss = -11115.132882442143
Iteration 3000: Loss = -11114.936031482133
Iteration 3100: Loss = -11114.785907907406
Iteration 3200: Loss = -11114.684520590685
Iteration 3300: Loss = -11114.39702772397
Iteration 3400: Loss = -11113.727488032711
Iteration 3500: Loss = -11106.637447503123
Iteration 3600: Loss = -11103.994991409432
Iteration 3700: Loss = -11101.610339126339
Iteration 3800: Loss = -11095.757020994668
Iteration 3900: Loss = -11093.673678071036
Iteration 4000: Loss = -11091.63814155863
Iteration 4100: Loss = -11087.870360363242
Iteration 4200: Loss = -11082.505025871818
Iteration 4300: Loss = -11048.826094541026
Iteration 4400: Loss = -11040.841552790773
Iteration 4500: Loss = -11032.475764621573
Iteration 4600: Loss = -11032.27660274584
Iteration 4700: Loss = -11031.359564156544
Iteration 4800: Loss = -11031.209708165497
Iteration 4900: Loss = -11028.253505483224
Iteration 5000: Loss = -11019.628470092202
Iteration 5100: Loss = -11017.975997145657
Iteration 5200: Loss = -11014.391381761392
Iteration 5300: Loss = -11014.376078598863
Iteration 5400: Loss = -11014.364251052444
Iteration 5500: Loss = -11008.684501576601
Iteration 5600: Loss = -11002.879025366114
Iteration 5700: Loss = -10997.70231449552
Iteration 5800: Loss = -10997.680591298056
Iteration 5900: Loss = -10997.672538622432
Iteration 6000: Loss = -10997.66418158594
Iteration 6100: Loss = -10997.655037848657
Iteration 6200: Loss = -10997.640734949993
Iteration 6300: Loss = -10997.612430105373
Iteration 6400: Loss = -10997.50548301768
Iteration 6500: Loss = -10997.28902310754
Iteration 6600: Loss = -10996.953519620703
Iteration 6700: Loss = -10996.844287702841
Iteration 6800: Loss = -10996.804275769942
Iteration 6900: Loss = -10996.756512082282
Iteration 7000: Loss = -10993.925862849246
Iteration 7100: Loss = -10993.913345956104
Iteration 7200: Loss = -10993.910655022855
Iteration 7300: Loss = -10993.91446145214
1
Iteration 7400: Loss = -10993.882099621987
Iteration 7500: Loss = -10993.838279705331
Iteration 7600: Loss = -10993.830782939802
Iteration 7700: Loss = -10993.829895833622
Iteration 7800: Loss = -10993.828143898052
Iteration 7900: Loss = -10993.830697866068
1
Iteration 8000: Loss = -10991.67527106846
Iteration 8100: Loss = -10991.675365977633
1
Iteration 8200: Loss = -10991.668100174038
Iteration 8300: Loss = -10991.647663259813
Iteration 8400: Loss = -10991.621169803548
Iteration 8500: Loss = -10991.472754368824
Iteration 8600: Loss = -10991.473074374713
1
Iteration 8700: Loss = -10991.471404895528
Iteration 8800: Loss = -10991.470837845405
Iteration 8900: Loss = -10991.429763155127
Iteration 9000: Loss = -10991.413596637181
Iteration 9100: Loss = -10991.41351641168
Iteration 9200: Loss = -10991.413628975755
1
Iteration 9300: Loss = -10991.412986874466
Iteration 9400: Loss = -10991.411653649435
Iteration 9500: Loss = -10991.40626916428
Iteration 9600: Loss = -10991.556694040895
1
Iteration 9700: Loss = -10985.6785697473
Iteration 9800: Loss = -10985.722377549415
1
Iteration 9900: Loss = -10985.677632937764
Iteration 10000: Loss = -10985.677445605466
Iteration 10100: Loss = -10985.677260735913
Iteration 10200: Loss = -10985.676201421107
Iteration 10300: Loss = -10985.68081086516
1
Iteration 10400: Loss = -10985.67662146987
2
Iteration 10500: Loss = -10985.675849386687
Iteration 10600: Loss = -10985.675831773347
Iteration 10700: Loss = -10985.675379469492
Iteration 10800: Loss = -10985.676760931517
1
Iteration 10900: Loss = -10985.676441628668
2
Iteration 11000: Loss = -10985.790164726435
3
Iteration 11100: Loss = -10985.676065410016
4
Iteration 11200: Loss = -10985.656022122064
Iteration 11300: Loss = -10985.65616112694
1
Iteration 11400: Loss = -10985.68718253386
2
Iteration 11500: Loss = -10985.669343183015
3
Iteration 11600: Loss = -10985.659765689237
4
Iteration 11700: Loss = -10985.67078728117
5
Iteration 11800: Loss = -10985.639839369083
Iteration 11900: Loss = -10985.634860871807
Iteration 12000: Loss = -10985.627656274797
Iteration 12100: Loss = -10985.627401119427
Iteration 12200: Loss = -10985.627478534887
1
Iteration 12300: Loss = -10985.661289164773
2
Iteration 12400: Loss = -10985.627101506614
Iteration 12500: Loss = -10985.626972475038
Iteration 12600: Loss = -10985.636953799334
1
Iteration 12700: Loss = -10985.64109015468
2
Iteration 12800: Loss = -10985.628316338718
3
Iteration 12900: Loss = -10985.654559503997
4
Iteration 13000: Loss = -10985.613857595625
Iteration 13100: Loss = -10985.613848119383
Iteration 13200: Loss = -10985.665552211389
1
Iteration 13300: Loss = -10985.651758588348
2
Iteration 13400: Loss = -10985.613207950588
Iteration 13500: Loss = -10985.615911342196
1
Iteration 13600: Loss = -10985.62444846714
2
Iteration 13700: Loss = -10985.625982635127
3
Iteration 13800: Loss = -10985.619803714215
4
Iteration 13900: Loss = -10984.646140875739
Iteration 14000: Loss = -10984.669539186143
1
Iteration 14100: Loss = -10984.77721921307
2
Iteration 14200: Loss = -10984.650494610181
3
Iteration 14300: Loss = -10984.64503931832
Iteration 14400: Loss = -10984.641109099664
Iteration 14500: Loss = -10984.57931546389
Iteration 14600: Loss = -10984.567063951225
Iteration 14700: Loss = -10984.566641174148
Iteration 14800: Loss = -10984.571995131882
1
Iteration 14900: Loss = -10984.570674125769
2
Iteration 15000: Loss = -10984.565913672379
Iteration 15100: Loss = -10984.551466779241
Iteration 15200: Loss = -10984.5456267546
Iteration 15300: Loss = -10984.547146062401
1
Iteration 15400: Loss = -10984.573669102829
2
Iteration 15500: Loss = -10984.6032335521
3
Iteration 15600: Loss = -10984.543588135675
Iteration 15700: Loss = -10984.543624922662
1
Iteration 15800: Loss = -10984.561962016227
2
Iteration 15900: Loss = -10984.580241328102
3
Iteration 16000: Loss = -10984.54576218835
4
Iteration 16100: Loss = -10984.543532748563
Iteration 16200: Loss = -10984.548332608467
1
Iteration 16300: Loss = -10984.624461750642
2
Iteration 16400: Loss = -10984.540061324082
Iteration 16500: Loss = -10984.53999978849
Iteration 16600: Loss = -10984.54070839473
1
Iteration 16700: Loss = -10984.571416996514
2
Iteration 16800: Loss = -10984.543048237585
3
Iteration 16900: Loss = -10984.539914718824
Iteration 17000: Loss = -10984.553566804627
1
Iteration 17100: Loss = -10984.539916549582
2
Iteration 17200: Loss = -10984.54006069345
3
Iteration 17300: Loss = -10984.578877165764
4
Iteration 17400: Loss = -10984.542751902287
5
Iteration 17500: Loss = -10984.549565174402
6
Iteration 17600: Loss = -10984.540301386252
7
Iteration 17700: Loss = -10984.540068838638
8
Iteration 17800: Loss = -10984.73384172123
9
Iteration 17900: Loss = -10984.50593646042
Iteration 18000: Loss = -10984.49261548673
Iteration 18100: Loss = -10984.47008037367
Iteration 18200: Loss = -10984.470481263807
1
Iteration 18300: Loss = -10984.47463760586
2
Iteration 18400: Loss = -10984.47053979388
3
Iteration 18500: Loss = -10984.523464409334
4
Iteration 18600: Loss = -10984.471744934737
5
Iteration 18700: Loss = -10984.469925821186
Iteration 18800: Loss = -10984.477313237183
1
Iteration 18900: Loss = -10984.469847381208
Iteration 19000: Loss = -10984.469915231584
1
Iteration 19100: Loss = -10984.469818722524
Iteration 19200: Loss = -10984.470518744913
1
Iteration 19300: Loss = -10984.477714772773
2
Iteration 19400: Loss = -10984.46926356862
Iteration 19500: Loss = -10984.475972253933
1
Iteration 19600: Loss = -10984.463161722111
Iteration 19700: Loss = -10984.46573819598
1
Iteration 19800: Loss = -10984.514843046436
2
Iteration 19900: Loss = -10984.48642158595
3
tensor([[-5.2591,  0.6695],
        [-6.0517,  4.5892],
        [-4.9269,  3.3069],
        [-2.0169,  0.5767],
        [-3.4453,  1.6943],
        [ 0.6476, -2.1285],
        [-3.8272,  2.4279],
        [-5.9179,  4.5315],
        [-2.8129,  1.1235],
        [-3.1581,  1.7700],
        [-0.4829, -0.9952],
        [-1.8022,  0.0971],
        [-0.8791, -1.8584],
        [-2.4924, -0.0481],
        [ 0.9199, -2.3183],
        [ 4.4732, -5.9155],
        [-0.1505, -2.0061],
        [-3.7525,  1.5945],
        [-4.4863,  1.7748],
        [-3.2348,  1.8401],
        [-4.6942,  2.8725],
        [-3.8060,  2.3034],
        [-4.5945,  3.1480],
        [-1.3724, -0.0933],
        [ 1.7092, -3.3112],
        [-3.3745,  0.1589],
        [-3.2817,  0.8075],
        [-6.5473,  5.0117],
        [-3.3816,  1.7130],
        [ 0.1249, -1.5113],
        [-5.5644,  1.8823],
        [-4.8516,  3.1847],
        [-4.6103,  3.2236],
        [ 0.3671, -1.7672],
        [-2.7441,  1.1572],
        [-2.9770,  0.7332],
        [-2.5926,  1.0458],
        [-6.0048,  4.3483],
        [-2.5972,  0.5326],
        [ 0.8085, -3.3489],
        [ 1.9197, -3.7891],
        [-2.4208,  1.0160],
        [-2.2847,  0.8168],
        [-3.5965,  1.9130],
        [-3.9348,  2.0463],
        [-5.0519,  3.6018],
        [-4.6758,  2.9089],
        [-4.6686,  0.0534],
        [-1.0175, -0.3701],
        [-3.5352,  2.0671],
        [-5.9254,  4.3958],
        [ 2.4272, -4.0371],
        [-3.0270,  0.4717],
        [-3.4247,  1.0819],
        [-2.6482,  1.1027],
        [ 0.6675, -2.2964],
        [ 0.3850, -1.9142],
        [-3.2865,  0.5954],
        [-4.4249,  2.9030],
        [-4.2922,  2.8894],
        [-6.1927,  4.0973],
        [-2.2195,  0.8245],
        [-4.1862, -0.2440],
        [ 1.9509, -3.7269],
        [-2.3447,  0.6454],
        [-2.2180,  0.8302],
        [-0.9009, -0.7826],
        [-4.2984,  2.5702],
        [-0.3317, -1.3818],
        [-2.2564,  0.8546],
        [ 2.0258, -3.4145],
        [-4.7965,  3.4055],
        [-1.4792, -1.2765],
        [ 0.1775, -3.0993],
        [-3.2329,  1.3573],
        [-4.7736,  2.6401],
        [-5.1054,  3.1313],
        [-4.3734,  2.4754],
        [ 0.5556, -2.0054],
        [-2.2376,  0.7620],
        [ 2.4868, -4.4388],
        [ 0.7040, -2.1152],
        [-2.5039,  0.1627],
        [-3.2402,  1.7862],
        [-4.4728,  3.0839],
        [-5.9642,  1.6529],
        [-3.9906,  2.3198],
        [-1.2870, -1.1247],
        [-3.1939,  1.7000],
        [ 1.3496, -3.1841],
        [-3.0847,  1.3870],
        [-3.8664,  2.0331],
        [-6.1006,  1.4853],
        [-5.6003,  4.2140],
        [ 3.2070, -5.8420],
        [-3.1123,  1.1128],
        [-3.4082,  0.9393],
        [-7.3069,  4.9707],
        [ 0.4062, -1.8241],
        [-4.6522,  3.0756]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7462, 0.2538],
        [0.3454, 0.6546]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2515, 0.7485], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2583, 0.1079],
         [0.0688, 0.1991]],

        [[0.9168, 0.1041],
         [0.3844, 0.4553]],

        [[0.6133, 0.0888],
         [0.8658, 0.8339]],

        [[0.9672, 0.0938],
         [0.4626, 0.2551]],

        [[0.1084, 0.1112],
         [0.4265, 0.6061]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0166980783118736
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080650161849864
time is 2
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 12
Adjusted Rand Index: 0.5732932095726908
Global Adjusted Rand Index: 0.4722909793948044
Average Adjusted Rand Index: 0.6557716046127698
Iteration 0: Loss = -15541.784308325266
Iteration 10: Loss = -11111.75975946102
Iteration 20: Loss = -11111.362920649512
Iteration 30: Loss = -11111.325846340971
Iteration 40: Loss = -11111.322949017018
Iteration 50: Loss = -11111.324116831871
1
Iteration 60: Loss = -11111.325858444266
2
Iteration 70: Loss = -11111.327958269276
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.5033, 0.4967],
        [0.3877, 0.6123]], dtype=torch.float64)
alpha: tensor([0.4370, 0.5630])
beta: tensor([[[0.2065, 0.1674],
         [0.0553, 0.1383]],

        [[0.6019, 0.1646],
         [0.8949, 0.1932]],

        [[0.0490, 0.1515],
         [0.8675, 0.7989]],

        [[0.2928, 0.1681],
         [0.6734, 0.3890]],

        [[0.3553, 0.1699],
         [0.7420, 0.8332]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.00014102694055273746
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 36
Adjusted Rand Index: 0.06944091551974574
time is 2
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 28
Adjusted Rand Index: 0.18669926650366747
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 32
Adjusted Rand Index: 0.12116660824748324
time is 4
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 23
Adjusted Rand Index: 0.28446967526581246
Global Adjusted Rand Index: 0.116634589870439
Average Adjusted Rand Index: 0.13238349849545233
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -15541.509138621384
Iteration 100: Loss = -11124.092807153662
Iteration 200: Loss = -11116.176868515862
Iteration 300: Loss = -11115.259504323412
Iteration 400: Loss = -11114.778703223079
Iteration 500: Loss = -11114.636446436929
Iteration 600: Loss = -11114.587097809257
Iteration 700: Loss = -11114.558726994863
Iteration 800: Loss = -11114.539126568614
Iteration 900: Loss = -11114.523090882032
Iteration 1000: Loss = -11114.511331174184
Iteration 1100: Loss = -11114.503664648044
Iteration 1200: Loss = -11114.4979547277
Iteration 1300: Loss = -11114.493387069862
Iteration 1400: Loss = -11114.489776054024
Iteration 1500: Loss = -11114.486834299976
Iteration 1600: Loss = -11114.484332416481
Iteration 1700: Loss = -11114.482031786978
Iteration 1800: Loss = -11114.479944765724
Iteration 1900: Loss = -11114.477803267711
Iteration 2000: Loss = -11114.475434178277
Iteration 2100: Loss = -11114.47169120834
Iteration 2200: Loss = -11114.41538750534
Iteration 2300: Loss = -11114.398634974477
Iteration 2400: Loss = -11114.382078919123
Iteration 2500: Loss = -11114.375005460905
Iteration 2600: Loss = -11114.35839297832
Iteration 2700: Loss = -11114.272592602894
Iteration 2800: Loss = -11114.219023983584
Iteration 2900: Loss = -11114.035187802941
Iteration 3000: Loss = -11113.88907310942
Iteration 3100: Loss = -11113.827212465583
Iteration 3200: Loss = -11113.781307510788
Iteration 3300: Loss = -11113.760637466632
Iteration 3400: Loss = -11113.751314390129
Iteration 3500: Loss = -11113.741077603898
Iteration 3600: Loss = -11112.693415267742
Iteration 3700: Loss = -11110.778660298729
Iteration 3800: Loss = -11110.774752024758
Iteration 3900: Loss = -11110.759788561027
Iteration 4000: Loss = -11103.391656461377
Iteration 4100: Loss = -11103.211865986683
Iteration 4200: Loss = -11102.99971110228
Iteration 4300: Loss = -11102.933878516993
Iteration 4400: Loss = -11102.855007524795
Iteration 4500: Loss = -11102.828084123974
Iteration 4600: Loss = -11102.7669914531
Iteration 4700: Loss = -11102.726227391024
Iteration 4800: Loss = -11102.715271301751
Iteration 4900: Loss = -11102.693972539717
Iteration 5000: Loss = -11102.697052886178
1
Iteration 5100: Loss = -11102.683090003578
Iteration 5200: Loss = -11102.645276525633
Iteration 5300: Loss = -11102.742986607565
1
Iteration 5400: Loss = -10991.905192339182
Iteration 5500: Loss = -10990.85864219715
Iteration 5600: Loss = -10990.78683742349
Iteration 5700: Loss = -10990.631740029527
Iteration 5800: Loss = -10990.514719299601
Iteration 5900: Loss = -10990.497524739429
Iteration 6000: Loss = -10989.219655284704
Iteration 6100: Loss = -10989.092765887277
Iteration 6200: Loss = -10989.084249234384
Iteration 6300: Loss = -10989.074596500952
Iteration 6400: Loss = -10989.067875093744
Iteration 6500: Loss = -10989.063569594999
Iteration 6600: Loss = -10989.061434066436
Iteration 6700: Loss = -10989.038200780877
Iteration 6800: Loss = -10989.035908274805
Iteration 6900: Loss = -10989.030523322735
Iteration 7000: Loss = -10989.049233602054
1
Iteration 7100: Loss = -10989.025410576978
Iteration 7200: Loss = -10989.020287732734
Iteration 7300: Loss = -10989.018462346925
Iteration 7400: Loss = -10989.018424329579
Iteration 7500: Loss = -10989.077207923512
1
Iteration 7600: Loss = -10989.016000892192
Iteration 7700: Loss = -10989.055203417272
1
Iteration 7800: Loss = -10989.012766806893
Iteration 7900: Loss = -10989.012427426795
Iteration 8000: Loss = -10987.960785199253
Iteration 8100: Loss = -10987.878550005833
Iteration 8200: Loss = -10987.891016579802
1
Iteration 8300: Loss = -10987.861177846542
Iteration 8400: Loss = -10987.851497359112
Iteration 8500: Loss = -10987.841209187176
Iteration 8600: Loss = -10987.837028367072
Iteration 8700: Loss = -10987.836736680758
Iteration 8800: Loss = -10987.833548094723
Iteration 8900: Loss = -10987.791587359234
Iteration 9000: Loss = -10987.782427029353
Iteration 9100: Loss = -10987.781991678468
Iteration 9200: Loss = -10987.781445905266
Iteration 9300: Loss = -10987.781149273796
Iteration 9400: Loss = -10987.78545406245
1
Iteration 9500: Loss = -10987.780525847586
Iteration 9600: Loss = -10987.801569742336
1
Iteration 9700: Loss = -10987.779082427824
Iteration 9800: Loss = -10987.778335513904
Iteration 9900: Loss = -10987.77847531157
1
Iteration 10000: Loss = -10987.792965183748
2
Iteration 10100: Loss = -10987.778358915906
3
Iteration 10200: Loss = -10987.778809094118
4
Iteration 10300: Loss = -10987.777498373038
Iteration 10400: Loss = -10987.777769790502
1
Iteration 10500: Loss = -10987.825046968692
2
Iteration 10600: Loss = -10987.801091992005
3
Iteration 10700: Loss = -10987.777257629068
Iteration 10800: Loss = -10987.77734683768
1
Iteration 10900: Loss = -10987.779649225064
2
Iteration 11000: Loss = -10987.932701850017
3
Iteration 11100: Loss = -10987.778441385944
4
Iteration 11200: Loss = -10987.795688744269
5
Iteration 11300: Loss = -10987.788165199387
6
Iteration 11400: Loss = -10987.776291300133
Iteration 11500: Loss = -10987.845792590186
1
Iteration 11600: Loss = -10987.776159953715
Iteration 11700: Loss = -10987.77633750644
1
Iteration 11800: Loss = -10987.788538741706
2
Iteration 11900: Loss = -10987.782091694431
3
Iteration 12000: Loss = -10987.77630289314
4
Iteration 12100: Loss = -10987.776271029283
5
Iteration 12200: Loss = -10987.781926861777
6
Iteration 12300: Loss = -10987.792742967025
7
Iteration 12400: Loss = -10987.775882291138
Iteration 12500: Loss = -10987.776518691342
1
Iteration 12600: Loss = -10987.77555963393
Iteration 12700: Loss = -10987.775224282757
Iteration 12800: Loss = -10987.775103270898
Iteration 12900: Loss = -10987.755517259706
Iteration 13000: Loss = -10987.755110526998
Iteration 13100: Loss = -10987.760221374785
1
Iteration 13200: Loss = -10987.758626183357
2
Iteration 13300: Loss = -10987.767817166308
3
Iteration 13400: Loss = -10987.753683611647
Iteration 13500: Loss = -10987.753865457751
1
Iteration 13600: Loss = -10987.92437071586
2
Iteration 13700: Loss = -10987.753624912086
Iteration 13800: Loss = -10987.772802021676
1
Iteration 13900: Loss = -10987.75426714006
2
Iteration 14000: Loss = -10987.736394916383
Iteration 14100: Loss = -10987.73660865983
1
Iteration 14200: Loss = -10987.737267860519
2
Iteration 14300: Loss = -10987.736420629335
3
Iteration 14400: Loss = -10987.743634357319
4
Iteration 14500: Loss = -10987.738563935774
5
Iteration 14600: Loss = -10987.736549617714
6
Iteration 14700: Loss = -10987.736486423311
7
Iteration 14800: Loss = -10984.127265771855
Iteration 14900: Loss = -10984.099994939052
Iteration 15000: Loss = -10984.100698684977
1
Iteration 15100: Loss = -10984.240727283355
2
Iteration 15200: Loss = -10984.096175782443
Iteration 15300: Loss = -10984.096644700336
1
Iteration 15400: Loss = -10984.224994700844
2
Iteration 15500: Loss = -10984.094152209209
Iteration 15600: Loss = -10984.034862136581
Iteration 15700: Loss = -10984.03535016553
1
Iteration 15800: Loss = -10984.034712068356
Iteration 15900: Loss = -10984.160033995395
1
Iteration 16000: Loss = -10984.035879321187
2
Iteration 16100: Loss = -10984.034660803107
Iteration 16200: Loss = -10984.034842137575
1
Iteration 16300: Loss = -10984.039811698383
2
Iteration 16400: Loss = -10984.028506234734
Iteration 16500: Loss = -10984.027286362854
Iteration 16600: Loss = -10984.027569489308
1
Iteration 16700: Loss = -10984.027683465445
2
Iteration 16800: Loss = -10984.027259982795
Iteration 16900: Loss = -10984.027241926266
Iteration 17000: Loss = -10984.03251153154
1
Iteration 17100: Loss = -10984.02725179061
2
Iteration 17200: Loss = -10984.123046838766
3
Iteration 17300: Loss = -10984.027208600839
Iteration 17400: Loss = -10984.027406261812
1
Iteration 17500: Loss = -10984.03613517947
2
Iteration 17600: Loss = -10984.109629366214
3
Iteration 17700: Loss = -10984.24376911133
4
Iteration 17800: Loss = -10984.027239072555
5
Iteration 17900: Loss = -10984.02791035774
6
Iteration 18000: Loss = -10984.02859116232
7
Iteration 18100: Loss = -10984.027236828593
8
Iteration 18200: Loss = -10984.087399489934
9
Iteration 18300: Loss = -10984.027189837925
Iteration 18400: Loss = -10984.033864198153
1
Iteration 18500: Loss = -10984.031787256064
2
Iteration 18600: Loss = -10984.02809163443
3
Iteration 18700: Loss = -10984.027315611158
4
Iteration 18800: Loss = -10984.027679989942
5
Iteration 18900: Loss = -10984.03055271229
6
Iteration 19000: Loss = -10984.027227701908
7
Iteration 19100: Loss = -10984.027685830375
8
Iteration 19200: Loss = -10984.106127952311
9
Iteration 19300: Loss = -10984.058650597923
10
Stopping early at iteration 19300 due to no improvement.
tensor([[-4.2474e+00,  1.6818e+00],
        [-6.0244e+00,  4.6114e+00],
        [-4.9140e+00,  3.2978e+00],
        [-1.9907e+00,  6.0078e-01],
        [-3.2990e+00,  1.8449e+00],
        [ 6.7959e-01, -2.0747e+00],
        [-5.4564e+00,  8.4115e-01],
        [-6.3306e+00,  4.0582e+00],
        [-2.7186e+00,  1.2236e+00],
        [-3.2053e+00,  1.7186e+00],
        [-4.3080e-01, -9.6918e-01],
        [-1.6597e+00,  1.8875e-01],
        [-9.3226e-01, -1.9025e+00],
        [-2.1694e+00,  3.1616e-01],
        [ 9.2278e-01, -2.3121e+00],
        [ 2.2263e+00, -3.7186e+00],
        [-1.7356e-01, -2.0020e+00],
        [-3.7893e+00,  1.5446e+00],
        [-3.8273e+00,  2.4341e+00],
        [-3.2717e+00,  1.8077e+00],
        [-4.6600e+00,  2.9015e+00],
        [-3.7488e+00,  2.3552e+00],
        [-5.0974e+00,  2.6195e+00],
        [-1.3659e+00, -1.5409e-01],
        [ 1.4930e+00, -3.4848e+00],
        [-4.0898e+00, -5.2540e-01],
        [-2.7986e+00,  1.3000e+00],
        [-5.9897e+00,  4.5638e+00],
        [-3.2347e+00,  1.8461e+00],
        [ 9.3659e-02, -1.5281e+00],
        [-4.5889e+00,  2.8590e+00],
        [-4.7400e+00,  3.3087e+00],
        [-4.9146e+00,  2.9225e+00],
        [-8.9350e-01, -3.0131e+00],
        [-2.7042e+00,  1.2109e+00],
        [-2.5890e+00,  1.1282e+00],
        [-2.6559e+00,  9.5803e-01],
        [-6.0104e+00,  4.3325e+00],
        [-2.2686e+00,  8.7694e-01],
        [ 9.6610e-01, -3.1687e+00],
        [ 2.0909e+00, -3.6121e+00],
        [-2.6790e+00,  7.8088e-01],
        [-3.2451e+00, -1.6459e-01],
        [-3.5230e+00,  2.0112e+00],
        [-3.6846e+00,  2.2919e+00],
        [-5.8085e+00,  2.7521e+00],
        [-5.1027e+00,  2.4844e+00],
        [-3.1568e+00,  1.5749e+00],
        [-1.0204e+00, -3.6765e-01],
        [-3.5327e+00,  2.0675e+00],
        [-5.8249e+00,  4.4061e+00],
        [ 2.4400e+00, -4.0052e+00],
        [-2.8054e+00,  7.0399e-01],
        [-4.4325e+00,  8.5388e-02],
        [-2.8325e+00,  9.2022e-01],
        [ 4.9617e-01, -2.4582e+00],
        [ 4.2914e-01, -1.8621e+00],
        [-2.6294e+00,  1.2354e+00],
        [-4.3941e+00,  2.9453e+00],
        [-4.4513e+00,  2.7386e+00],
        [-5.8374e+00,  4.4463e+00],
        [-2.4105e+00,  6.1735e-01],
        [-2.7121e+00,  1.2461e+00],
        [ 1.7205e+00, -3.9280e+00],
        [-2.6588e+00,  3.5067e-01],
        [-2.2541e+00,  8.0846e-01],
        [-1.4075e+00, -1.2822e+00],
        [-4.5147e+00,  2.3313e+00],
        [-7.1643e-01, -1.7583e+00],
        [-2.2676e+00,  8.1414e-01],
        [ 8.0130e-01, -4.6099e+00],
        [-5.0511e+00,  3.1492e+00],
        [-8.0752e-01, -5.8212e-01],
        [ 8.7943e-01, -2.3785e+00],
        [-3.3281e+00,  1.2687e+00],
        [-4.4380e+00,  2.9383e+00],
        [-4.8135e+00,  3.4266e+00],
        [-4.3712e+00,  2.4467e+00],
        [-4.0021e-01, -2.9512e+00],
        [-2.9884e+00,  1.2443e-02],
        [ 2.6202e+00, -4.3067e+00],
        [-8.9549e-01, -3.7197e+00],
        [-2.1904e+00,  4.9210e-01],
        [-3.7998e+00,  1.2347e+00],
        [-4.4752e+00,  3.0503e+00],
        [-4.6716e+00,  3.0052e+00],
        [-3.9407e+00,  2.3253e+00],
        [-1.0576e+00, -8.8993e-01],
        [-3.9654e+00,  9.4224e-01],
        [ 1.2292e+00, -3.2903e+00],
        [-2.9808e+00,  1.5094e+00],
        [-4.7441e+00,  1.1715e+00],
        [-4.7359e+00,  2.8512e+00],
        [-9.5936e+00,  7.5856e+00],
        [ 3.0871e+00, -5.9311e+00],
        [-3.1329e+00,  1.0953e+00],
        [-3.4164e+00,  9.3758e-01],
        [-6.8318e+00,  5.4427e+00],
        [-8.7721e-03, -2.1808e+00],
        [-5.8173e+00,  1.9084e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7488, 0.2512],
        [0.3478, 0.6522]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2535, 0.7465], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2570, 0.1079],
         [0.0553, 0.1999]],

        [[0.6019, 0.1039],
         [0.8949, 0.1932]],

        [[0.0490, 0.0884],
         [0.8675, 0.7989]],

        [[0.2928, 0.0936],
         [0.6734, 0.3890]],

        [[0.3553, 0.1107],
         [0.7420, 0.8332]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0166980783118736
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080650161849864
time is 2
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 12
Adjusted Rand Index: 0.5732932095726908
Global Adjusted Rand Index: 0.4722909793948044
Average Adjusted Rand Index: 0.6557716046127698
Iteration 0: Loss = -28220.61226456714
Iteration 10: Loss = -11113.73491116164
Iteration 20: Loss = -11111.640774889245
Iteration 30: Loss = -11111.381575926005
Iteration 40: Loss = -11111.35072587419
Iteration 50: Loss = -11111.33998403065
Iteration 60: Loss = -11111.332781805088
Iteration 70: Loss = -11111.327681511939
Iteration 80: Loss = -11111.324198213984
Iteration 90: Loss = -11111.322104614977
Iteration 100: Loss = -11111.32112786535
Iteration 110: Loss = -11111.321029640403
Iteration 120: Loss = -11111.32159532485
1
Iteration 130: Loss = -11111.322743045417
2
Iteration 140: Loss = -11111.324288998258
3
Stopping early at iteration 139 due to no improvement.
pi: tensor([[0.6191, 0.3809],
        [0.5025, 0.4975]], dtype=torch.float64)
alpha: tensor([0.5702, 0.4298])
beta: tensor([[[0.1386, 0.1679],
         [0.6853, 0.2073]],

        [[0.3057, 0.1651],
         [0.3472, 0.3968]],

        [[0.5376, 0.1521],
         [0.4073, 0.2392]],

        [[0.2883, 0.1685],
         [0.4775, 0.3447]],

        [[0.6640, 0.1705],
         [0.4840, 0.6482]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.01609150833656456
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 63
Adjusted Rand Index: 0.05862072510300676
time is 2
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 70
Adjusted Rand Index: 0.15310162780466344
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 67
Adjusted Rand Index: 0.10727875591880681
time is 4
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 77
Adjusted Rand Index: 0.28446967526581246
Global Adjusted Rand Index: 0.11392162427804196
Average Adjusted Rand Index: 0.12391245848577079
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28220.235178638723
Iteration 100: Loss = -11133.111441942036
Iteration 200: Loss = -11116.021942086642
Iteration 300: Loss = -11111.48992037793
Iteration 400: Loss = -11109.710973946892
Iteration 500: Loss = -11108.57841625139
Iteration 600: Loss = -11107.83659379675
Iteration 700: Loss = -11107.439294651134
Iteration 800: Loss = -11107.165424152025
Iteration 900: Loss = -11106.966470642754
Iteration 1000: Loss = -11106.816316844943
Iteration 1100: Loss = -11106.699472207974
Iteration 1200: Loss = -11106.603321638595
Iteration 1300: Loss = -11106.527052190693
Iteration 1400: Loss = -11106.467188962833
Iteration 1500: Loss = -11106.417590928919
Iteration 1600: Loss = -11106.376004198384
Iteration 1700: Loss = -11106.340994025777
Iteration 1800: Loss = -11106.310733672108
Iteration 1900: Loss = -11106.284801052605
Iteration 2000: Loss = -11106.264530747267
Iteration 2100: Loss = -11106.24262575021
Iteration 2200: Loss = -11106.225245039019
Iteration 2300: Loss = -11106.210032796813
Iteration 2400: Loss = -11106.19579792273
Iteration 2500: Loss = -11106.182667320407
Iteration 2600: Loss = -11106.169396116835
Iteration 2700: Loss = -11106.150943013316
Iteration 2800: Loss = -11106.141858717181
Iteration 2900: Loss = -11106.134049233346
Iteration 3000: Loss = -11106.126956072447
Iteration 3100: Loss = -11106.126959972036
1
Iteration 3200: Loss = -11106.114680547065
Iteration 3300: Loss = -11106.109351663941
Iteration 3400: Loss = -11106.122301967174
1
Iteration 3500: Loss = -11106.100107941205
Iteration 3600: Loss = -11106.096036601486
Iteration 3700: Loss = -11106.092501968958
Iteration 3800: Loss = -11106.088877465761
Iteration 3900: Loss = -11106.088030276247
Iteration 4000: Loss = -11106.082776729445
Iteration 4100: Loss = -11106.080069993444
Iteration 4200: Loss = -11106.077942740325
Iteration 4300: Loss = -11106.075322723707
Iteration 4400: Loss = -11106.07318247258
Iteration 4500: Loss = -11106.071549757507
Iteration 4600: Loss = -11106.06936069252
Iteration 4700: Loss = -11106.067684726002
Iteration 4800: Loss = -11106.066524278855
Iteration 4900: Loss = -11106.06463829612
Iteration 5000: Loss = -11106.063549448265
Iteration 5100: Loss = -11106.061975830222
Iteration 5200: Loss = -11106.060772873643
Iteration 5300: Loss = -11106.062789171552
1
Iteration 5400: Loss = -11106.058684849942
Iteration 5500: Loss = -11106.057742733017
Iteration 5600: Loss = -11106.056953684225
Iteration 5700: Loss = -11106.05618039694
Iteration 5800: Loss = -11106.055510095046
Iteration 5900: Loss = -11106.054843023685
Iteration 6000: Loss = -11106.069995393089
1
Iteration 6100: Loss = -11106.053721632925
Iteration 6200: Loss = -11106.054529115449
1
Iteration 6300: Loss = -11106.052691058032
Iteration 6400: Loss = -11106.05280228331
1
Iteration 6500: Loss = -11106.052141976963
Iteration 6600: Loss = -11106.057617721226
1
Iteration 6700: Loss = -11106.05073924852
Iteration 6800: Loss = -11106.050448428123
Iteration 6900: Loss = -11106.049619874264
Iteration 7000: Loss = -11106.04899620879
Iteration 7100: Loss = -11106.048441311328
Iteration 7200: Loss = -11106.046442076778
Iteration 7300: Loss = -11106.04277041773
Iteration 7400: Loss = -11106.018397937114
Iteration 7500: Loss = -11104.392505554099
Iteration 7600: Loss = -11104.285657861168
Iteration 7700: Loss = -11104.229088842108
Iteration 7800: Loss = -11104.20449332828
Iteration 7900: Loss = -11103.487140242214
Iteration 8000: Loss = -11103.352380052691
Iteration 8100: Loss = -11103.337278507011
Iteration 8200: Loss = -11103.115734822079
Iteration 8300: Loss = -11103.046260210547
Iteration 8400: Loss = -11102.956590899876
Iteration 8500: Loss = -11102.745551639708
Iteration 8600: Loss = -11102.498988918524
Iteration 8700: Loss = -11002.04438707442
Iteration 8800: Loss = -10992.508698559664
Iteration 8900: Loss = -10992.218615484637
Iteration 9000: Loss = -10991.69008303483
Iteration 9100: Loss = -10991.58110391504
Iteration 9200: Loss = -10991.549460916334
Iteration 9300: Loss = -10991.540898739508
Iteration 9400: Loss = -10991.534268368681
Iteration 9500: Loss = -10991.515673367638
Iteration 9600: Loss = -10991.513257755592
Iteration 9700: Loss = -10991.51287476988
Iteration 9800: Loss = -10991.487910660348
Iteration 9900: Loss = -10991.649222704162
1
Iteration 10000: Loss = -10991.45965949056
Iteration 10100: Loss = -10991.444514500896
Iteration 10200: Loss = -10991.407908978708
Iteration 10300: Loss = -10991.355797673394
Iteration 10400: Loss = -10991.342223009278
Iteration 10500: Loss = -10991.275287829423
Iteration 10600: Loss = -10991.264484853447
Iteration 10700: Loss = -10991.257266315755
Iteration 10800: Loss = -10991.296306724067
1
Iteration 10900: Loss = -10991.255764931142
Iteration 11000: Loss = -10991.27487632144
1
Iteration 11100: Loss = -10991.246718566641
Iteration 11200: Loss = -10991.247001439506
1
Iteration 11300: Loss = -10991.24226420408
Iteration 11400: Loss = -10991.23806442745
Iteration 11500: Loss = -10991.230621717466
Iteration 11600: Loss = -10991.229409362842
Iteration 11700: Loss = -10991.227681182614
Iteration 11800: Loss = -10991.232603499386
1
Iteration 11900: Loss = -10990.92300638409
Iteration 12000: Loss = -10985.511112132002
Iteration 12100: Loss = -10974.743373126857
Iteration 12200: Loss = -10970.064447334003
Iteration 12300: Loss = -10968.22437139377
Iteration 12400: Loss = -10967.61792224328
Iteration 12500: Loss = -10967.597977642678
Iteration 12600: Loss = -10965.78282989157
Iteration 12700: Loss = -10965.720737012649
Iteration 12800: Loss = -10965.682090771577
Iteration 12900: Loss = -10960.148323360929
Iteration 13000: Loss = -10959.058786731146
Iteration 13100: Loss = -10959.071082136754
1
Iteration 13200: Loss = -10958.296439162019
Iteration 13300: Loss = -10958.157844751837
Iteration 13400: Loss = -10958.150010707248
Iteration 13500: Loss = -10958.14864341765
Iteration 13600: Loss = -10958.192268270457
1
Iteration 13700: Loss = -10958.133956393654
Iteration 13800: Loss = -10958.136362990304
1
Iteration 13900: Loss = -10958.35286923132
2
Iteration 14000: Loss = -10958.13622691179
3
Iteration 14100: Loss = -10958.136047963724
4
Iteration 14200: Loss = -10958.144337652664
5
Iteration 14300: Loss = -10958.192491331223
6
Iteration 14400: Loss = -10958.130322950763
Iteration 14500: Loss = -10958.129198059962
Iteration 14600: Loss = -10958.132478534626
1
Iteration 14700: Loss = -10958.137049454288
2
Iteration 14800: Loss = -10958.126529109744
Iteration 14900: Loss = -10958.29356509722
1
Iteration 15000: Loss = -10958.1266386992
2
Iteration 15100: Loss = -10958.127279010701
3
Iteration 15200: Loss = -10958.126990340585
4
Iteration 15300: Loss = -10958.12593499024
Iteration 15400: Loss = -10958.13047005751
1
Iteration 15500: Loss = -10958.135396088523
2
Iteration 15600: Loss = -10958.125158259425
Iteration 15700: Loss = -10958.12477912792
Iteration 15800: Loss = -10958.125285851145
1
Iteration 15900: Loss = -10958.14113038662
2
Iteration 16000: Loss = -10958.129153747734
3
Iteration 16100: Loss = -10958.124353572211
Iteration 16200: Loss = -10958.125542686135
1
Iteration 16300: Loss = -10958.12895208888
2
Iteration 16400: Loss = -10958.12351998186
Iteration 16500: Loss = -10958.124359249161
1
Iteration 16600: Loss = -10958.12457045078
2
Iteration 16700: Loss = -10958.124229860747
3
Iteration 16800: Loss = -10958.123585612118
4
Iteration 16900: Loss = -10958.123417150508
Iteration 17000: Loss = -10958.151810689867
1
Iteration 17100: Loss = -10958.123373103794
Iteration 17200: Loss = -10958.130236040613
1
Iteration 17300: Loss = -10958.135343169744
2
Iteration 17400: Loss = -10958.12338533577
3
Iteration 17500: Loss = -10958.131480523562
4
Iteration 17600: Loss = -10958.154353417704
5
Iteration 17700: Loss = -10958.123367288772
Iteration 17800: Loss = -10958.12349355009
1
Iteration 17900: Loss = -10958.3231426574
2
Iteration 18000: Loss = -10958.123314424132
Iteration 18100: Loss = -10958.126499718202
1
Iteration 18200: Loss = -10958.278988886681
2
Iteration 18300: Loss = -10958.124253150312
3
Iteration 18400: Loss = -10958.16534530868
4
Iteration 18500: Loss = -10958.126288239526
5
Iteration 18600: Loss = -10958.14466518849
6
Iteration 18700: Loss = -10958.13865406651
7
Iteration 18800: Loss = -10958.124570513839
8
Iteration 18900: Loss = -10958.120000777855
Iteration 19000: Loss = -10958.131528636703
1
Iteration 19100: Loss = -10958.119974559324
Iteration 19200: Loss = -10958.1204622112
1
Iteration 19300: Loss = -10958.139900353395
2
Iteration 19400: Loss = -10958.123277664161
3
Iteration 19500: Loss = -10958.12035440117
4
Iteration 19600: Loss = -10958.131302098818
5
Iteration 19700: Loss = -10958.12092949436
6
Iteration 19800: Loss = -10958.120510454833
7
Iteration 19900: Loss = -10958.127328072018
8
tensor([[-2.9858e+00,  1.5762e+00],
        [-3.6648e+00,  2.0380e+00],
        [ 3.5094e+00, -4.9193e+00],
        [ 3.2018e+00, -5.1122e+00],
        [-3.7409e+00,  2.1840e+00],
        [ 2.1598e+00, -4.3060e+00],
        [-3.6495e+00,  3.4577e-01],
        [-1.6321e+00,  2.4516e-01],
        [ 2.5580e+00, -4.2658e+00],
        [ 2.6556e-01, -1.7608e+00],
        [ 2.8598e+00, -5.3763e+00],
        [-3.8035e+00, -8.1168e-01],
        [ 1.6556e+00, -3.5816e+00],
        [ 3.5473e+00, -5.0443e+00],
        [ 2.1050e+00, -3.5665e+00],
        [-2.1126e+00,  4.7641e-02],
        [-3.4049e+00,  7.1019e-01],
        [-4.7387e+00,  2.8949e+00],
        [-4.6227e+00,  2.9446e+00],
        [-2.9972e-01, -1.1274e+00],
        [-7.3821e+00,  5.5633e+00],
        [-4.5416e+00,  1.5288e+00],
        [-3.2937e+00,  1.7214e+00],
        [ 2.2344e+00, -4.4052e+00],
        [ 1.1038e+00, -3.6881e+00],
        [ 6.1293e-01, -3.6436e+00],
        [-5.0637e+00,  7.8831e-01],
        [-5.5781e+00,  2.1366e+00],
        [ 2.7787e+00, -5.2128e+00],
        [ 1.1504e+00, -4.1778e+00],
        [-4.8361e+00,  3.3812e+00],
        [ 2.1270e+00, -3.9745e+00],
        [-2.3629e+00,  8.3211e-01],
        [ 4.2799e+00, -5.7002e+00],
        [-1.7491e+00,  2.1326e-01],
        [-7.4009e-02, -1.3522e+00],
        [ 2.1498e+00, -4.1902e+00],
        [ 6.1781e-01, -2.0117e+00],
        [ 2.7051e+00, -4.1611e+00],
        [ 5.5553e+00, -7.2261e+00],
        [ 2.0201e-01, -4.5684e+00],
        [-2.2685e+00,  8.7211e-01],
        [ 1.8738e+00, -3.9603e+00],
        [-5.4115e+00,  4.0182e+00],
        [-3.6638e+00,  1.8730e+00],
        [ 7.1252e-01, -2.0990e+00],
        [ 2.2501e+00, -3.6954e+00],
        [-6.0189e-03, -2.0918e+00],
        [ 7.8592e-01, -2.4936e+00],
        [-5.6002e+00,  4.1317e+00],
        [ 1.4444e+00, -2.9296e+00],
        [ 2.2596e+00, -4.2650e+00],
        [ 4.3848e+00, -5.8677e+00],
        [-2.2575e+00,  6.8388e-01],
        [-4.0522e+00,  1.7982e+00],
        [ 9.5330e-01, -2.3595e+00],
        [ 2.3357e-01, -2.1767e+00],
        [-4.8231e+00,  3.4017e+00],
        [-7.0050e+00,  4.3980e+00],
        [ 2.5037e+00, -4.3029e+00],
        [-3.8614e+00,  2.0210e+00],
        [ 2.0130e+00, -5.1862e+00],
        [-2.1694e+00, -1.3002e+00],
        [ 2.3004e+00, -3.8105e+00],
        [ 3.6270e+00, -5.0395e+00],
        [-6.3437e+00,  4.9573e+00],
        [-8.6440e-01, -6.2040e-01],
        [ 1.4856e+00, -3.0969e+00],
        [-5.0514e+00,  3.5563e+00],
        [ 2.0437e+00, -5.3353e+00],
        [-5.1368e+00,  2.8286e+00],
        [-5.7473e+00,  4.3499e+00],
        [ 1.8627e+00, -3.5159e+00],
        [ 1.3187e+00, -3.8696e+00],
        [-4.4222e+00,  2.6479e+00],
        [-3.6855e+00,  1.2146e+00],
        [ 5.3063e+00, -6.9979e+00],
        [ 1.1006e+00, -2.6764e+00],
        [-1.0821e+00, -3.3931e-01],
        [-2.8041e+00, -2.3380e-02],
        [ 1.5218e+00, -3.2009e+00],
        [-3.4375e+00, -4.0849e-01],
        [ 2.9193e+00, -4.3274e+00],
        [-8.1169e+00,  4.1608e+00],
        [ 6.5242e-01, -3.2559e+00],
        [-3.4846e+00,  5.0904e-02],
        [ 1.9988e+00, -5.7079e+00],
        [ 5.4991e-01, -2.2507e+00],
        [ 3.4181e+00, -4.8707e+00],
        [ 2.9583e+00, -4.3459e+00],
        [-6.2705e-01, -1.9717e+00],
        [-5.5072e+00,  2.3525e+00],
        [-7.0935e+00,  5.3900e+00],
        [ 2.5148e+00, -5.1770e+00],
        [ 9.6026e-01, -4.8171e+00],
        [-5.0221e+00,  2.3474e+00],
        [ 3.1680e+00, -6.8363e+00],
        [ 2.1720e+00, -3.5916e+00],
        [-2.3771e+00,  3.8796e-01],
        [-6.2326e+00,  4.8265e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7235, 0.2765],
        [0.2178, 0.7822]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5543, 0.4457], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2014, 0.1084],
         [0.6853, 0.2573]],

        [[0.3057, 0.1035],
         [0.3472, 0.3968]],

        [[0.5376, 0.0892],
         [0.4073, 0.2392]],

        [[0.2883, 0.0936],
         [0.4775, 0.3447]],

        [[0.6640, 0.1116],
         [0.4840, 0.6482]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 1
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 2
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 89
Adjusted Rand Index: 0.6044273961975466
Global Adjusted Rand Index: 0.8387330980292153
Average Adjusted Rand Index: 0.8421740001626924
Iteration 0: Loss = -22801.710847525457
Iteration 10: Loss = -11114.711396315055
Iteration 20: Loss = -11114.57242761172
Iteration 30: Loss = -11114.535996011491
Iteration 40: Loss = -11114.398872149679
Iteration 50: Loss = -11113.146654403381
Iteration 60: Loss = -11112.552585720656
Iteration 70: Loss = -11112.32544034654
Iteration 80: Loss = -11112.143169913867
Iteration 90: Loss = -11111.990545315084
Iteration 100: Loss = -11111.863673173206
Iteration 110: Loss = -11111.758805916743
Iteration 120: Loss = -11111.672243119408
Iteration 130: Loss = -11111.60101567683
Iteration 140: Loss = -11111.542564025262
Iteration 150: Loss = -11111.494730547893
Iteration 160: Loss = -11111.45568591749
Iteration 170: Loss = -11111.423980571592
Iteration 180: Loss = -11111.398474112555
Iteration 190: Loss = -11111.378034073234
Iteration 200: Loss = -11111.361935828421
Iteration 210: Loss = -11111.349396611231
Iteration 220: Loss = -11111.339777387739
Iteration 230: Loss = -11111.332698011738
Iteration 240: Loss = -11111.327588876926
Iteration 250: Loss = -11111.324139831322
Iteration 260: Loss = -11111.32212316223
Iteration 270: Loss = -11111.321130354194
Iteration 280: Loss = -11111.321025515754
Iteration 290: Loss = -11111.321628695892
1
Iteration 300: Loss = -11111.322764649974
2
Iteration 310: Loss = -11111.324344997312
3
Stopping early at iteration 309 due to no improvement.
pi: tensor([[0.6190, 0.3810],
        [0.5025, 0.4975]], dtype=torch.float64)
alpha: tensor([0.5701, 0.4299])
beta: tensor([[[0.1386, 0.1679],
         [0.8917, 0.2073]],

        [[0.2848, 0.1651],
         [0.4695, 0.8160]],

        [[0.3295, 0.1521],
         [0.6888, 0.0811]],

        [[0.1821, 0.1685],
         [0.4597, 0.4631]],

        [[0.3715, 0.1705],
         [0.6536, 0.3036]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.01609150833656456
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 63
Adjusted Rand Index: 0.05862072510300676
time is 2
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 70
Adjusted Rand Index: 0.15310162780466344
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 67
Adjusted Rand Index: 0.10727875591880681
time is 4
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 77
Adjusted Rand Index: 0.28446967526581246
Global Adjusted Rand Index: 0.11392162427804196
Average Adjusted Rand Index: 0.12391245848577079
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22690.79505004409
Iteration 100: Loss = -11121.114473106694
Iteration 200: Loss = -11116.383362754967
Iteration 300: Loss = -11115.550801179452
Iteration 400: Loss = -11114.907525006862
Iteration 500: Loss = -11114.410616846035
Iteration 600: Loss = -11113.863442086895
Iteration 700: Loss = -11113.027915241266
Iteration 800: Loss = -11110.602141940226
Iteration 900: Loss = -11091.109139696116
Iteration 1000: Loss = -11047.586846395006
Iteration 1100: Loss = -11013.08967171872
Iteration 1200: Loss = -11008.852916083113
Iteration 1300: Loss = -11008.62577072918
Iteration 1400: Loss = -11004.092581336734
Iteration 1500: Loss = -11003.913510000322
Iteration 1600: Loss = -11000.8373818325
Iteration 1700: Loss = -11000.792602515208
Iteration 1800: Loss = -11000.753244495405
Iteration 1900: Loss = -11000.736236011757
Iteration 2000: Loss = -11000.679063969852
Iteration 2100: Loss = -11000.617604404804
Iteration 2200: Loss = -10999.95538998654
Iteration 2300: Loss = -10998.071263440188
Iteration 2400: Loss = -10998.053139495953
Iteration 2500: Loss = -10998.01970211793
Iteration 2600: Loss = -10998.013934587652
Iteration 2700: Loss = -10998.010024889736
Iteration 2800: Loss = -10998.007069406267
Iteration 2900: Loss = -10998.004328251447
Iteration 3000: Loss = -10998.00175486943
Iteration 3100: Loss = -10997.99902020844
Iteration 3200: Loss = -10997.99580901236
Iteration 3300: Loss = -10997.990832841017
Iteration 3400: Loss = -10997.973097274651
Iteration 3500: Loss = -10996.079501216696
Iteration 3600: Loss = -10996.01102176163
Iteration 3700: Loss = -10996.001370886106
Iteration 3800: Loss = -10995.103356831718
Iteration 3900: Loss = -10995.10023686317
Iteration 4000: Loss = -10995.099181917769
Iteration 4100: Loss = -10995.098441240782
Iteration 4200: Loss = -10995.09760725578
Iteration 4300: Loss = -10995.096464539989
Iteration 4400: Loss = -10995.095577392867
Iteration 4500: Loss = -10995.094156105062
Iteration 4600: Loss = -10995.090109522012
Iteration 4700: Loss = -10995.075938252226
Iteration 4800: Loss = -10995.073591437511
Iteration 4900: Loss = -10995.07130547582
Iteration 5000: Loss = -10995.069310082688
Iteration 5100: Loss = -10995.066373016049
Iteration 5200: Loss = -10995.057812942006
Iteration 5300: Loss = -10995.01580158234
Iteration 5400: Loss = -10994.921752348993
Iteration 5500: Loss = -10994.782495418343
Iteration 5600: Loss = -10994.525010662719
Iteration 5700: Loss = -10994.2134275107
Iteration 5800: Loss = -10994.210541196166
Iteration 5900: Loss = -10994.209060094472
Iteration 6000: Loss = -10994.206517769293
Iteration 6100: Loss = -10994.173037505687
Iteration 6200: Loss = -10994.172322223358
Iteration 6300: Loss = -10994.170562219131
Iteration 6400: Loss = -10994.166569127028
Iteration 6500: Loss = -10993.992734306956
Iteration 6600: Loss = -10993.985186587102
Iteration 6700: Loss = -10964.792958261078
Iteration 6800: Loss = -10964.689286606323
Iteration 6900: Loss = -10964.595145375375
Iteration 7000: Loss = -10964.583947093925
Iteration 7100: Loss = -10964.58893227754
1
Iteration 7200: Loss = -10958.320406114013
Iteration 7300: Loss = -10958.29368248879
Iteration 7400: Loss = -10958.293185898072
Iteration 7500: Loss = -10958.295945149723
1
Iteration 7600: Loss = -10958.292914341295
Iteration 7700: Loss = -10958.292841228442
Iteration 7800: Loss = -10958.292043178159
Iteration 7900: Loss = -10958.290401067869
Iteration 8000: Loss = -10958.289995680938
Iteration 8100: Loss = -10958.308570496394
1
Iteration 8200: Loss = -10958.289907319586
Iteration 8300: Loss = -10958.289848217191
Iteration 8400: Loss = -10958.435703583775
1
Iteration 8500: Loss = -10958.289535305896
Iteration 8600: Loss = -10958.28921538949
Iteration 8700: Loss = -10958.293710817801
1
Iteration 8800: Loss = -10958.27273217819
Iteration 8900: Loss = -10958.272346361235
Iteration 9000: Loss = -10958.293611610765
1
Iteration 9100: Loss = -10958.27196221762
Iteration 9200: Loss = -10958.271795468489
Iteration 9300: Loss = -10958.280401503946
1
Iteration 9400: Loss = -10958.271724875643
Iteration 9500: Loss = -10958.27172563835
1
Iteration 9600: Loss = -10958.27765859846
2
Iteration 9700: Loss = -10958.271664299957
Iteration 9800: Loss = -10958.271686454696
1
Iteration 9900: Loss = -10958.441423387323
2
Iteration 10000: Loss = -10958.271590204991
Iteration 10100: Loss = -10958.271494016748
Iteration 10200: Loss = -10958.299539489457
1
Iteration 10300: Loss = -10958.271403175422
Iteration 10400: Loss = -10958.276707709134
1
Iteration 10500: Loss = -10958.280271154032
2
Iteration 10600: Loss = -10958.271532698453
3
Iteration 10700: Loss = -10958.272348646511
4
Iteration 10800: Loss = -10958.271258159006
Iteration 10900: Loss = -10958.273975622616
1
Iteration 11000: Loss = -10958.183614011634
Iteration 11100: Loss = -10958.191389535325
1
Iteration 11200: Loss = -10958.183347478996
Iteration 11300: Loss = -10958.177788857001
Iteration 11400: Loss = -10958.178651180047
1
Iteration 11500: Loss = -10958.17798106588
2
Iteration 11600: Loss = -10958.182272447566
3
Iteration 11700: Loss = -10958.177522120279
Iteration 11800: Loss = -10958.177584469875
1
Iteration 11900: Loss = -10958.235770294683
2
Iteration 12000: Loss = -10958.190866913428
3
Iteration 12100: Loss = -10958.176877359028
Iteration 12200: Loss = -10958.177239523235
1
Iteration 12300: Loss = -10958.183450970158
2
Iteration 12400: Loss = -10958.177785729955
3
Iteration 12500: Loss = -10958.170327054315
Iteration 12600: Loss = -10958.174632077875
1
Iteration 12700: Loss = -10958.425589730985
2
Iteration 12800: Loss = -10958.16609230573
Iteration 12900: Loss = -10958.17252401132
1
Iteration 13000: Loss = -10958.167868479271
2
Iteration 13100: Loss = -10958.166262890583
3
Iteration 13200: Loss = -10958.16609216916
Iteration 13300: Loss = -10958.173696496902
1
Iteration 13400: Loss = -10958.166001578978
Iteration 13500: Loss = -10958.166257487497
1
Iteration 13600: Loss = -10958.185498031851
2
Iteration 13700: Loss = -10958.165873030504
Iteration 13800: Loss = -10958.165141867474
Iteration 13900: Loss = -10958.159202417202
Iteration 14000: Loss = -10958.159285663201
1
Iteration 14100: Loss = -10958.173796215347
2
Iteration 14200: Loss = -10958.157823949112
Iteration 14300: Loss = -10958.187494734746
1
Iteration 14400: Loss = -10958.172064358665
2
Iteration 14500: Loss = -10958.16089946229
3
Iteration 14600: Loss = -10958.157813851956
Iteration 14700: Loss = -10958.159339950815
1
Iteration 14800: Loss = -10958.158419736068
2
Iteration 14900: Loss = -10958.364606468485
3
Iteration 15000: Loss = -10958.160041387993
4
Iteration 15100: Loss = -10958.167960573031
5
Iteration 15200: Loss = -10958.287427228752
6
Iteration 15300: Loss = -10958.157722696948
Iteration 15400: Loss = -10958.158553012465
1
Iteration 15500: Loss = -10958.157703445897
Iteration 15600: Loss = -10958.161340977324
1
Iteration 15700: Loss = -10958.156304137668
Iteration 15800: Loss = -10958.13353298179
Iteration 15900: Loss = -10958.12139766505
Iteration 16000: Loss = -10958.124382981325
1
Iteration 16100: Loss = -10958.12312232751
2
Iteration 16200: Loss = -10958.12137030389
Iteration 16300: Loss = -10958.13565462642
1
Iteration 16400: Loss = -10958.124518734117
2
Iteration 16500: Loss = -10958.123837842359
3
Iteration 16600: Loss = -10958.21909495226
4
Iteration 16700: Loss = -10958.121357326454
Iteration 16800: Loss = -10958.127472599765
1
Iteration 16900: Loss = -10958.12183519005
2
Iteration 17000: Loss = -10958.121387334686
3
Iteration 17100: Loss = -10958.121346038262
Iteration 17200: Loss = -10958.121496676788
1
Iteration 17300: Loss = -10958.121373629252
2
Iteration 17400: Loss = -10958.12957364636
3
Iteration 17500: Loss = -10958.121436741945
4
Iteration 17600: Loss = -10958.145275403203
5
Iteration 17700: Loss = -10958.12442952505
6
Iteration 17800: Loss = -10958.12146586494
7
Iteration 17900: Loss = -10958.120279691015
Iteration 18000: Loss = -10958.121175815375
1
Iteration 18100: Loss = -10958.136702900701
2
Iteration 18200: Loss = -10958.144397320331
3
Iteration 18300: Loss = -10958.12021986237
Iteration 18400: Loss = -10958.123944087518
1
Iteration 18500: Loss = -10958.133275026496
2
Iteration 18600: Loss = -10958.120707384422
3
Iteration 18700: Loss = -10958.120244934835
4
Iteration 18800: Loss = -10958.402019933386
5
Iteration 18900: Loss = -10958.12022260613
6
Iteration 19000: Loss = -10958.126921171954
7
Iteration 19100: Loss = -10958.126704515442
8
Iteration 19200: Loss = -10958.120215144443
Iteration 19300: Loss = -10958.120266594484
1
Iteration 19400: Loss = -10958.265760726288
2
Iteration 19500: Loss = -10958.12024308172
3
Iteration 19600: Loss = -10958.121243864178
4
Iteration 19700: Loss = -10958.120243638888
5
Iteration 19800: Loss = -10958.120192733933
Iteration 19900: Loss = -10958.120290799596
1
tensor([[-3.0417,  1.5198],
        [-3.7709,  1.9318],
        [ 3.2112, -5.2172],
        [ 3.1959, -5.1181],
        [-3.7774,  2.1470],
        [ 2.2130, -4.2528],
        [-3.4311,  0.5637],
        [-1.7075,  0.1692],
        [ 2.6747, -4.1492],
        [-0.1489, -2.1756],
        [ 3.0066, -5.2295],
        [-2.9784,  0.0130],
        [ 0.9807, -4.2569],
        [ 3.1868, -5.4048],
        [ 2.1191, -3.5526],
        [-1.7993,  0.3605],
        [-2.7513,  1.3634],
        [-4.8040,  2.8293],
        [-4.5271,  3.0400],
        [-0.3323, -1.1603],
        [-7.8115,  5.1387],
        [-3.8135,  2.2566],
        [-3.3335,  1.6812],
        [ 2.4793, -4.1603],
        [ 1.0579, -3.7343],
        [-0.1792, -4.4360],
        [-3.7700,  2.0817],
        [-4.5553,  3.1590],
        [ 3.2392, -4.7521],
        [ 1.5321, -3.7965],
        [-4.8105,  3.4066],
        [ 2.3400, -3.7611],
        [-2.5075,  0.6870],
        [ 4.2908, -5.6894],
        [-2.2878, -0.3259],
        [-0.1186, -1.3972],
        [ 1.9831, -4.3570],
        [ 0.6040, -2.0258],
        [ 2.7392, -4.1268],
        [ 4.8052, -7.9762],
        [ 1.1544, -3.6161],
        [-2.2634,  0.8767],
        [ 2.1672, -3.6669],
        [-5.7704,  3.6593],
        [-4.2659,  1.2704],
        [ 0.4019, -2.4096],
        [ 1.1085, -4.8367],
        [ 0.1042, -1.9819],
        [ 0.8963, -2.3836],
        [-5.9362,  3.7956],
        [ 1.1598, -3.2143],
        [ 2.3718, -4.1530],
        [ 4.4335, -5.8200],
        [-2.7624,  0.1785],
        [-5.0685,  0.7817],
        [ 0.9136, -2.3995],
        [-0.1618, -2.5723],
        [-4.8257,  3.3989],
        [-8.5597,  7.0698],
        [ 2.5193, -4.2870],
        [-4.3923,  1.4899],
        [ 2.4405, -4.7582],
        [-1.4599, -0.5912],
        [ 2.2502, -3.8605],
        [ 2.9294, -5.7370],
        [-6.7342,  4.5667],
        [-0.9327, -0.6892],
        [ 1.3763, -3.2064],
        [-5.3188,  3.2894],
        [ 2.3775, -5.0016],
        [-4.9295,  3.0356],
        [-5.7548,  4.3424],
        [ 1.5909, -3.7880],
        [ 1.8119, -3.3767],
        [-4.2587,  2.8111],
        [-3.3522,  1.5475],
        [ 5.4184, -6.8861],
        [ 0.1024, -3.6749],
        [-1.1182, -0.3759],
        [-3.6977, -0.9175],
        [ 1.5955, -3.1273],
        [-2.6078,  0.4208],
        [ 2.7413, -4.5052],
        [-7.1250,  5.1531],
        [ 1.1290, -2.7797],
        [-2.6498,  0.8851],
        [ 3.1527, -4.5539],
        [ 0.5434, -2.2575],
        [ 2.5776, -5.7110],
        [ 2.8761, -4.4278],
        [-0.0222, -1.3672],
        [-4.6769,  3.1826],
        [-6.7193,  4.3234],
        [ 3.0618, -4.6295],
        [ 2.1515, -3.6258],
        [-5.4655,  1.9038],
        [ 3.4237, -6.5805],
        [ 0.5740, -5.1892],
        [-2.3681,  0.3963],
        [-6.2338,  4.8253]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7235, 0.2765],
        [0.2178, 0.7822]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5542, 0.4458], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2015, 0.1084],
         [0.8917, 0.2572]],

        [[0.2848, 0.1035],
         [0.4695, 0.8160]],

        [[0.3295, 0.0892],
         [0.6888, 0.0811]],

        [[0.1821, 0.0933],
         [0.4597, 0.4631]],

        [[0.3715, 0.1117],
         [0.6536, 0.3036]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 1
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 2
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 89
Adjusted Rand Index: 0.6044273961975466
Global Adjusted Rand Index: 0.8387330980292153
Average Adjusted Rand Index: 0.8421740001626924
11003.77711673985
new:  [0.4722909793948044, 0.4722909793948044, 0.8387330980292153, 0.8387330980292153] [0.6557716046127698, 0.6557716046127698, 0.8421740001626924, 0.8421740001626924] [10984.46304989956, 10984.058650597923, 10958.120045065052, 10958.120834900808]
prior:  [0.11393313593040864, 0.116634589870439, 0.11392162427804196, 0.11392162427804196] [0.12055141718430158, 0.13238349849545233, 0.12391245848577079, 0.12391245848577079] [11111.323502935973, 11111.327958269276, 11111.324288998258, 11111.324344997312]
-----------------------------------------------------------------------------------------
This iteration is 6
True Objective function: Loss = -10833.238846087863
Iteration 0: Loss = -16770.68564883542
Iteration 10: Loss = -10929.230506808695
Iteration 20: Loss = -10929.230494461697
Iteration 30: Loss = -10929.230537619853
1
Iteration 40: Loss = -10929.230486498782
Iteration 50: Loss = -10929.230558512596
1
Iteration 60: Loss = -10929.230535405357
2
Iteration 70: Loss = -10929.23053044063
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[1.1498e-06, 1.0000e+00],
        [9.8157e-01, 1.8434e-02]], dtype=torch.float64)
alpha: tensor([0.4953, 0.5047])
beta: tensor([[[1.5955e-01, 1.5480e-01],
         [5.8375e-01, 1.5957e-01]],

        [[8.8303e-01, 1.6700e-01],
         [2.5126e-01, 2.6003e-01]],

        [[1.0089e-01, 1.6060e-01],
         [2.6832e-01, 9.2483e-01]],

        [[8.3854e-01, 1.5740e-01],
         [8.9279e-04, 4.3744e-01]],

        [[6.1585e-02, 1.5800e-01],
         [7.9274e-01, 6.6758e-01]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 63
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16847.81968448634
Iteration 100: Loss = -10932.118986915153
Iteration 200: Loss = -10930.581546365114
Iteration 300: Loss = -10929.135304852896
Iteration 400: Loss = -10928.576997986393
Iteration 500: Loss = -10928.289817909623
Iteration 600: Loss = -10928.084056971578
Iteration 700: Loss = -10927.753374473716
Iteration 800: Loss = -10927.398330404683
Iteration 900: Loss = -10927.098388085422
Iteration 1000: Loss = -10926.822457808448
Iteration 1100: Loss = -10926.591663936464
Iteration 1200: Loss = -10926.36456626862
Iteration 1300: Loss = -10926.239587272385
Iteration 1400: Loss = -10926.063987624082
Iteration 1500: Loss = -10925.9149524055
Iteration 1600: Loss = -10925.835478545921
Iteration 1700: Loss = -10925.797060037257
Iteration 1800: Loss = -10925.75689535152
Iteration 1900: Loss = -10925.733082914401
Iteration 2000: Loss = -10925.723079944704
Iteration 2100: Loss = -10925.716741562783
Iteration 2200: Loss = -10925.712528406893
Iteration 2300: Loss = -10925.708849123714
Iteration 2400: Loss = -10925.70737229063
Iteration 2500: Loss = -10925.680868227932
Iteration 2600: Loss = -10925.678558141108
Iteration 2700: Loss = -10925.67718997084
Iteration 2800: Loss = -10925.674138751174
Iteration 2900: Loss = -10925.671998831316
Iteration 3000: Loss = -10925.669646168057
Iteration 3100: Loss = -10925.667062546408
Iteration 3200: Loss = -10925.674174162286
1
Iteration 3300: Loss = -10925.660606557107
Iteration 3400: Loss = -10925.656493925064
Iteration 3500: Loss = -10925.651092145012
Iteration 3600: Loss = -10925.645466834496
Iteration 3700: Loss = -10925.635589151714
Iteration 3800: Loss = -10925.62502991018
Iteration 3900: Loss = -10925.61278066003
Iteration 4000: Loss = -10925.601730207889
Iteration 4100: Loss = -10925.59479614079
Iteration 4200: Loss = -10925.59148967824
Iteration 4300: Loss = -10925.588774605383
Iteration 4400: Loss = -10925.587540792312
Iteration 4500: Loss = -10925.586793692823
Iteration 4600: Loss = -10925.586324720867
Iteration 4700: Loss = -10925.590857356754
1
Iteration 4800: Loss = -10925.585481940983
Iteration 4900: Loss = -10925.584890867796
Iteration 5000: Loss = -10925.584465799066
Iteration 5100: Loss = -10925.584392735354
Iteration 5200: Loss = -10925.585653070935
1
Iteration 5300: Loss = -10925.585172679817
2
Iteration 5400: Loss = -10925.584143397722
Iteration 5500: Loss = -10925.583264056855
Iteration 5600: Loss = -10925.583442547992
1
Iteration 5700: Loss = -10925.582770400864
Iteration 5800: Loss = -10925.584817671488
1
Iteration 5900: Loss = -10925.586672115487
2
Iteration 6000: Loss = -10925.60278354064
3
Iteration 6100: Loss = -10925.582828556508
4
Iteration 6200: Loss = -10925.58252659738
Iteration 6300: Loss = -10925.594114081683
1
Iteration 6400: Loss = -10925.589316578264
2
Iteration 6500: Loss = -10925.586134824931
3
Iteration 6600: Loss = -10925.584517106552
4
Iteration 6700: Loss = -10925.525344938836
Iteration 6800: Loss = -10925.531950296017
1
Iteration 6900: Loss = -10925.525604020684
2
Iteration 7000: Loss = -10925.550236601053
3
Iteration 7100: Loss = -10925.524801255477
Iteration 7200: Loss = -10925.630854586369
1
Iteration 7300: Loss = -10925.524741763225
Iteration 7400: Loss = -10925.523322372579
Iteration 7500: Loss = -10925.52371720925
1
Iteration 7600: Loss = -10925.530621640964
2
Iteration 7700: Loss = -10925.523114595551
Iteration 7800: Loss = -10925.538662201609
1
Iteration 7900: Loss = -10925.523060503003
Iteration 8000: Loss = -10925.604791862543
1
Iteration 8100: Loss = -10925.523167838997
2
Iteration 8200: Loss = -10925.524017736487
3
Iteration 8300: Loss = -10925.523708992978
4
Iteration 8400: Loss = -10925.529868385805
5
Iteration 8500: Loss = -10925.52276673776
Iteration 8600: Loss = -10925.522944111255
1
Iteration 8700: Loss = -10925.523760700848
2
Iteration 8800: Loss = -10925.523556014501
3
Iteration 8900: Loss = -10925.522717207761
Iteration 9000: Loss = -10925.529513727712
1
Iteration 9100: Loss = -10925.68718154857
2
Iteration 9200: Loss = -10925.52258052629
Iteration 9300: Loss = -10925.524809424882
1
Iteration 9400: Loss = -10925.524212179324
2
Iteration 9500: Loss = -10925.525134787196
3
Iteration 9600: Loss = -10925.522698099181
4
Iteration 9700: Loss = -10925.522559068459
Iteration 9800: Loss = -10925.576640927688
1
Iteration 9900: Loss = -10925.522620500176
2
Iteration 10000: Loss = -10925.522704339659
3
Iteration 10100: Loss = -10925.525322933929
4
Iteration 10200: Loss = -10925.522729834342
5
Iteration 10300: Loss = -10925.522959113385
6
Iteration 10400: Loss = -10925.52354744477
7
Iteration 10500: Loss = -10925.541427908798
8
Iteration 10600: Loss = -10925.522729848704
9
Iteration 10700: Loss = -10925.524902619109
10
Stopping early at iteration 10700 due to no improvement.
tensor([[-4.0139, -0.6013],
        [-3.3984, -1.2168],
        [-4.2591, -0.3562],
        [-4.1226, -0.4926],
        [-3.2227, -1.3926],
        [-3.7091, -0.9061],
        [-4.3598, -0.2554],
        [-4.8470,  0.2318],
        [-3.1391, -1.4761],
        [-4.2410, -0.3742],
        [-3.1668, -1.4484],
        [-3.5778, -1.0374],
        [-3.0765, -1.5387],
        [-3.0890, -1.5262],
        [-4.6399,  0.0247],
        [-3.8660, -0.7493],
        [-4.2528, -0.3624],
        [-3.9046, -0.7106],
        [-3.8019, -0.8134],
        [-4.2122, -0.4030],
        [-3.4565, -1.1587],
        [-4.0154, -0.5999],
        [-3.8192, -0.7960],
        [-3.8317, -0.7835],
        [-4.1148, -0.5004],
        [-3.7995, -0.8157],
        [-4.1102, -0.5051],
        [-3.2126, -1.4026],
        [-4.1577, -0.4575],
        [-3.9094, -0.7058],
        [-3.1593, -1.4559],
        [-3.7241, -0.8911],
        [-3.8166, -0.7987],
        [-3.9578, -0.6574],
        [-3.9061, -0.7091],
        [-3.5379, -1.0773],
        [-3.8215, -0.7938],
        [-4.2856, -0.3296],
        [-4.6072, -0.0080],
        [-3.3754, -1.2398],
        [-4.2610, -0.3542],
        [-3.8797, -0.7355],
        [-3.0769, -1.5383],
        [-3.6747, -0.9405],
        [-4.2906, -0.3246],
        [-4.3001, -0.3151],
        [-3.2902, -1.3250],
        [-3.4799, -1.1353],
        [-4.3733, -0.2419],
        [-4.1698, -0.4454],
        [-4.7150,  0.0998],
        [-3.8070, -0.8082],
        [-4.6739,  0.0586],
        [-3.4173, -1.1979],
        [-3.4239, -1.1913],
        [-3.5178, -1.0974],
        [-3.4814, -1.1338],
        [-3.1436, -1.4717],
        [-3.6694, -0.9458],
        [-4.1896, -0.4256],
        [-3.0644, -1.5508],
        [-3.8907, -0.7245],
        [-3.1189, -1.4963],
        [-4.2120, -0.4032],
        [-3.3150, -1.3002],
        [-3.3238, -1.2914],
        [-4.4818, -0.1334],
        [-3.5508, -1.0644],
        [-3.9168, -0.6984],
        [-3.3309, -1.2843],
        [-3.3993, -1.2159],
        [-3.2794, -1.3358],
        [-4.7266,  0.1113],
        [-3.9089, -0.7063],
        [-3.6396, -0.9756],
        [-3.6407, -0.9745],
        [-4.4155, -0.1997],
        [-3.3184, -1.2968],
        [-3.1035, -1.5117],
        [-4.4866, -0.1286],
        [-4.4860, -0.1292],
        [-4.7434,  0.1282],
        [-3.6836, -0.9316],
        [-4.5544, -0.0608],
        [-3.6962, -0.9190],
        [-4.1455, -0.4697],
        [-4.5980, -0.0172],
        [-3.9774, -0.6379],
        [-3.2272, -1.3881],
        [-3.3150, -1.3002],
        [-3.6219, -0.9933],
        [-3.0689, -1.5463],
        [-3.7899, -0.8253],
        [-3.6273, -0.9879],
        [-3.0495, -1.5658],
        [-4.2526, -0.3626],
        [-3.8934, -0.7218],
        [-2.9738, -1.6414],
        [-4.3475, -0.2677],
        [-3.7932, -0.8220]], dtype=torch.float64, requires_grad=True)
pi: tensor([[5.2188e-05, 9.9995e-01],
        [1.4572e-01, 8.5428e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0665, 0.9335], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[2.4780e-01, 1.8215e-01],
         [5.8375e-01, 1.5174e-01]],

        [[8.8303e-01, 1.9840e-01],
         [2.5126e-01, 2.6003e-01]],

        [[1.0089e-01, 2.0600e-01],
         [2.6832e-01, 9.2483e-01]],

        [[8.3854e-01, 1.9205e-01],
         [8.9279e-04, 4.3744e-01]],

        [[6.1585e-02, 1.7291e-01],
         [7.9274e-01, 6.6758e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 59
Adjusted Rand Index: -0.02789997666760923
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.010101010101010102
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.006467401572035518
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.005556983500553775
Average Adjusted Rand Index: -0.008893677668130972
Iteration 0: Loss = -29619.593575775005
Iteration 10: Loss = -10930.031028178719
Iteration 20: Loss = -10930.031038335972
1
Iteration 30: Loss = -10930.030046369593
Iteration 40: Loss = -10928.930709247088
Iteration 50: Loss = -10926.797344465476
Iteration 60: Loss = -10926.455269748945
Iteration 70: Loss = -10926.362166805113
Iteration 80: Loss = -10926.328594375533
Iteration 90: Loss = -10926.314619106919
Iteration 100: Loss = -10926.308371277557
Iteration 110: Loss = -10926.305505194188
Iteration 120: Loss = -10926.304161426298
Iteration 130: Loss = -10926.303498808255
Iteration 140: Loss = -10926.303247225678
Iteration 150: Loss = -10926.303038521828
Iteration 160: Loss = -10926.302992522667
Iteration 170: Loss = -10926.302987998479
Iteration 180: Loss = -10926.302971673991
Iteration 190: Loss = -10926.302935817705
Iteration 200: Loss = -10926.302912174217
Iteration 210: Loss = -10926.302956650059
1
Iteration 220: Loss = -10926.302940569482
2
Iteration 230: Loss = -10926.302917655086
3
Stopping early at iteration 229 due to no improvement.
pi: tensor([[9.7145e-01, 2.8553e-02],
        [1.0000e+00, 3.3546e-47]], dtype=torch.float64)
alpha: tensor([0.9709, 0.0291])
beta: tensor([[[0.1592, 0.0865],
         [0.0983, 0.1396]],

        [[0.3717, 0.2540],
         [0.1087, 0.4326]],

        [[0.7442, 0.2303],
         [0.2201, 0.8505]],

        [[0.7616, 0.1218],
         [0.1182, 0.9534]],

        [[0.9669, 0.1551],
         [0.8187, 0.0789]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 39
Adjusted Rand Index: -0.015623423336712405
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0005549176189843577
Average Adjusted Rand Index: -0.002235795778453592
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29618.931850733115
Iteration 100: Loss = -10934.764882903984
Iteration 200: Loss = -10932.016170806242
Iteration 300: Loss = -10931.12746050081
Iteration 400: Loss = -10930.654877441177
Iteration 500: Loss = -10930.218349501261
Iteration 600: Loss = -10929.550671152225
Iteration 700: Loss = -10929.283790545858
Iteration 800: Loss = -10929.112801231297
Iteration 900: Loss = -10928.96405624513
Iteration 1000: Loss = -10928.814052589985
Iteration 1100: Loss = -10928.675473962272
Iteration 1200: Loss = -10928.565587620295
Iteration 1300: Loss = -10928.477117685534
Iteration 1400: Loss = -10928.401142851262
Iteration 1500: Loss = -10928.33203456688
Iteration 1600: Loss = -10928.26559950723
Iteration 1700: Loss = -10928.19868997389
Iteration 1800: Loss = -10928.128360392022
Iteration 1900: Loss = -10928.05077713212
Iteration 2000: Loss = -10927.956997317511
Iteration 2100: Loss = -10927.810784640089
Iteration 2200: Loss = -10927.43010525223
Iteration 2300: Loss = -10927.035375842883
Iteration 2400: Loss = -10926.637941083027
Iteration 2500: Loss = -10926.411419092114
Iteration 2600: Loss = -10926.239237378264
Iteration 2700: Loss = -10926.10335856546
Iteration 2800: Loss = -10926.000225498967
Iteration 2900: Loss = -10925.925686592305
Iteration 3000: Loss = -10925.873291572287
Iteration 3100: Loss = -10925.83650880819
Iteration 3200: Loss = -10925.809819126433
Iteration 3300: Loss = -10925.789417538532
Iteration 3400: Loss = -10925.772908795509
Iteration 3500: Loss = -10925.758844513122
Iteration 3600: Loss = -10925.746466569788
Iteration 3700: Loss = -10925.735233413752
Iteration 3800: Loss = -10925.724898619424
Iteration 3900: Loss = -10925.715230059546
Iteration 4000: Loss = -10925.706153434023
Iteration 4100: Loss = -10925.697722200357
Iteration 4200: Loss = -10925.689893026767
Iteration 4300: Loss = -10925.682678330988
Iteration 4400: Loss = -10925.676172062793
Iteration 4500: Loss = -10925.67029199828
Iteration 4600: Loss = -10925.665146777095
Iteration 4700: Loss = -10925.660705999979
Iteration 4800: Loss = -10925.656895478214
Iteration 4900: Loss = -10925.653629352295
Iteration 5000: Loss = -10925.650964013026
Iteration 5100: Loss = -10925.648704790634
Iteration 5200: Loss = -10925.646819612712
Iteration 5300: Loss = -10925.645244721514
Iteration 5400: Loss = -10925.643952459124
Iteration 5500: Loss = -10925.642822418991
Iteration 5600: Loss = -10925.641871291586
Iteration 5700: Loss = -10925.640963975915
Iteration 5800: Loss = -10925.64023724784
Iteration 5900: Loss = -10925.63954848739
Iteration 6000: Loss = -10925.638974035764
Iteration 6100: Loss = -10925.638378567963
Iteration 6200: Loss = -10925.637871493429
Iteration 6300: Loss = -10925.637378575284
Iteration 6400: Loss = -10925.636956871718
Iteration 6500: Loss = -10925.636518354811
Iteration 6600: Loss = -10925.636143788439
Iteration 6700: Loss = -10925.635795799322
Iteration 6800: Loss = -10925.635483685248
Iteration 6900: Loss = -10925.63515924434
Iteration 7000: Loss = -10925.634866434068
Iteration 7100: Loss = -10925.634589663081
Iteration 7200: Loss = -10925.634285134269
Iteration 7300: Loss = -10925.634065057291
Iteration 7400: Loss = -10925.633791996775
Iteration 7500: Loss = -10925.63360808924
Iteration 7600: Loss = -10925.633338381282
Iteration 7700: Loss = -10925.633180106357
Iteration 7800: Loss = -10925.632945367248
Iteration 7900: Loss = -10925.63277040107
Iteration 8000: Loss = -10925.632688050646
Iteration 8100: Loss = -10925.632452755528
Iteration 8200: Loss = -10925.632353042678
Iteration 8300: Loss = -10925.633888081797
1
Iteration 8400: Loss = -10925.632019773391
Iteration 8500: Loss = -10925.63190140702
Iteration 8600: Loss = -10925.633668893564
1
Iteration 8700: Loss = -10925.645839279481
2
Iteration 8800: Loss = -10925.645489486282
3
Iteration 8900: Loss = -10925.65080780357
4
Iteration 9000: Loss = -10925.631300273923
Iteration 9100: Loss = -10925.633240848256
1
Iteration 9200: Loss = -10925.63115112806
Iteration 9300: Loss = -10925.65192600733
1
Iteration 9400: Loss = -10925.630939764764
Iteration 9500: Loss = -10925.630889818345
Iteration 9600: Loss = -10925.63103929286
1
Iteration 9700: Loss = -10925.630725261783
Iteration 9800: Loss = -10925.70860077403
1
Iteration 9900: Loss = -10925.630593965194
Iteration 10000: Loss = -10925.630496752845
Iteration 10100: Loss = -10925.631276284792
1
Iteration 10200: Loss = -10925.63041193355
Iteration 10300: Loss = -10925.630328137078
Iteration 10400: Loss = -10925.630325126964
Iteration 10500: Loss = -10925.630235728171
Iteration 10600: Loss = -10925.630185579694
Iteration 10700: Loss = -10925.630391753692
1
Iteration 10800: Loss = -10925.630102882871
Iteration 10900: Loss = -10925.630076889222
Iteration 11000: Loss = -10925.631496862812
1
Iteration 11100: Loss = -10925.629991563212
Iteration 11200: Loss = -10925.629957379257
Iteration 11300: Loss = -10925.695192515994
1
Iteration 11400: Loss = -10925.629912545208
Iteration 11500: Loss = -10925.6298503337
Iteration 11600: Loss = -10925.653790460972
1
Iteration 11700: Loss = -10925.62979864663
Iteration 11800: Loss = -10925.6298065621
1
Iteration 11900: Loss = -10925.630057271732
2
Iteration 12000: Loss = -10925.62972875349
Iteration 12100: Loss = -10925.629741261127
1
Iteration 12200: Loss = -10925.637822576527
2
Iteration 12300: Loss = -10925.629618785824
Iteration 12400: Loss = -10925.629634668338
1
Iteration 12500: Loss = -10925.651159877305
2
Iteration 12600: Loss = -10925.629620940681
3
Iteration 12700: Loss = -10925.644239270296
4
Iteration 12800: Loss = -10925.6296316819
5
Iteration 12900: Loss = -10925.629587847221
Iteration 13000: Loss = -10925.632922667395
1
Iteration 13100: Loss = -10925.62954632834
Iteration 13200: Loss = -10925.62951913434
Iteration 13300: Loss = -10925.673868728336
1
Iteration 13400: Loss = -10925.629523824655
2
Iteration 13500: Loss = -10925.62949470532
Iteration 13600: Loss = -10925.643815591613
1
Iteration 13700: Loss = -10925.629463311643
Iteration 13800: Loss = -10925.67299727646
1
Iteration 13900: Loss = -10925.629426946154
Iteration 14000: Loss = -10925.62943670966
1
Iteration 14100: Loss = -10925.629502539112
2
Iteration 14200: Loss = -10925.629441166215
3
Iteration 14300: Loss = -10925.751462681417
4
Iteration 14400: Loss = -10925.629411443453
Iteration 14500: Loss = -10925.629401894035
Iteration 14600: Loss = -10925.646701543106
1
Iteration 14700: Loss = -10925.62939281701
Iteration 14800: Loss = -10925.629438153239
1
Iteration 14900: Loss = -10925.631934563207
2
Iteration 15000: Loss = -10925.629342018701
Iteration 15100: Loss = -10925.632844525033
1
Iteration 15200: Loss = -10925.629386270482
2
Iteration 15300: Loss = -10925.629351634725
3
Iteration 15400: Loss = -10925.62943528157
4
Iteration 15500: Loss = -10925.629345554917
5
Iteration 15600: Loss = -10925.720607046978
6
Iteration 15700: Loss = -10925.6293498767
7
Iteration 15800: Loss = -10925.629348699704
8
Iteration 15900: Loss = -10925.630801117148
9
Iteration 16000: Loss = -10925.629346226655
10
Stopping early at iteration 16000 due to no improvement.
tensor([[ 0.6618, -2.5901],
        [ 2.4111, -4.1594],
        [-0.0283, -2.1858],
        [ 0.5448, -2.0598],
        [ 1.6931, -4.7417],
        [ 1.4142, -2.9943],
        [-0.0462, -1.4218],
        [-1.2329, -0.2741],
        [ 2.3898, -5.4367],
        [ 0.3324, -1.7256],
        [ 2.5302, -4.9194],
        [ 2.1058, -3.8486],
        [ 2.2562, -5.4779],
        [ 3.0167, -4.4845],
        [-0.9175, -1.1515],
        [ 1.2430, -2.7437],
        [-0.3372, -2.3165],
        [ 1.0932, -2.5888],
        [-0.4897, -3.6098],
        [ 0.4143, -2.2177],
        [ 2.0708, -3.6086],
        [ 0.9712, -2.7542],
        [ 1.8227, -3.2594],
        [ 1.4068, -2.8060],
        [ 1.1685, -3.5413],
        [ 1.1618, -3.2764],
        [ 1.1509, -2.5690],
        [ 2.3574, -4.7761],
        [-0.0766, -2.0311],
        [ 0.4466, -2.3825],
        [ 2.0684, -5.1838],
        [ 0.7247, -3.2714],
        [ 1.1871, -2.7343],
        [ 1.5359, -3.0786],
        [ 0.5567, -2.1205],
        [ 1.7660, -3.3577],
        [ 0.6704, -2.8381],
        [ 0.1685, -2.4551],
        [-2.2675, -2.3106],
        [ 2.1514, -3.9264],
        [ 0.0237, -1.7711],
        [ 1.9520, -3.3410],
        [ 1.7960, -5.6851],
        [ 1.6542, -3.1230],
        [-0.1236, -1.9816],
        [ 3.5593, -5.3268],
        [ 2.6142, -4.0005],
        [ 2.0737, -3.6555],
        [-0.0966, -1.4007],
        [ 0.4128, -2.6211],
        [-1.3608, -0.6026],
        [ 0.6238, -3.0182],
        [-1.0066, -1.7381],
        [ 2.1141, -3.5316],
        [ 1.8810, -4.0868],
        [ 1.9797, -3.4895],
        [ 2.6112, -4.5284],
        [ 2.3520, -5.3365],
        [ 1.4508, -3.4996],
        [ 0.0226, -1.6299],
        [ 2.3816, -5.2426],
        [ 1.1011, -2.5451],
        [ 2.8470, -4.2444],
        [ 0.2411, -1.6476],
        [ 2.6899, -4.1927],
        [ 2.8273, -4.2204],
        [ 0.9182, -2.3048],
        [ 1.7673, -3.1652],
        [ 0.8545, -2.6024],
        [ 2.2286, -3.9684],
        [ 2.2383, -3.7842],
        [ 1.6684, -4.7432],
        [ 0.4594, -1.9174],
        [ 1.1025, -3.0861],
        [ 1.7227, -3.7173],
        [ 1.2634, -4.2773],
        [-0.3485, -1.6902],
        [ 2.3963, -3.7975],
        [ 2.1673, -5.1353],
        [-0.4517, -2.4949],
        [-0.3154, -2.1975],
        [-1.0783, -0.7205],
        [ 1.5930, -3.0873],
        [-1.0187, -1.6428],
        [ 1.5511, -3.3424],
        [ 0.5610, -2.3206],
        [-2.3615, -2.2538],
        [-0.6402, -3.5829],
        [ 1.6685, -5.2124],
        [ 2.0525, -4.4722],
        [ 1.3029, -4.1781],
        [ 2.9761, -4.3626],
        [ 1.4465, -2.8740],
        [ 1.7911, -3.5940],
        [ 3.2136, -4.7471],
        [ 0.4158, -1.9737],
        [ 0.4934, -2.7642],
        [ 3.4243, -4.8759],
        [ 0.2109, -1.5989],
        [ 1.2986, -2.7161]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.7843e-01, 2.1569e-02],
        [9.9999e-01, 7.6118e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9258, 0.0742], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1622, 0.0977],
         [0.0983, 0.1030]],

        [[0.3717, 0.2660],
         [0.1087, 0.4326]],

        [[0.7442, 0.2324],
         [0.2201, 0.8505]],

        [[0.7616, 0.1154],
         [0.1182, 0.9534]],

        [[0.9669, 0.1529],
         [0.8187, 0.0789]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.022626262626262626
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 38
Adjusted Rand Index: -0.008233622622776837
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0025027289519074284
Average Adjusted Rand Index: 0.002878528000697158
Iteration 0: Loss = -17645.774287888027
Iteration 10: Loss = -10927.005027258521
Iteration 20: Loss = -10926.706781403613
Iteration 30: Loss = -10926.496663060707
Iteration 40: Loss = -10926.364574842088
Iteration 50: Loss = -10926.289843035553
Iteration 60: Loss = -10926.250405396542
Iteration 70: Loss = -10926.230733089185
Iteration 80: Loss = -10926.221713649202
Iteration 90: Loss = -10926.218238949683
Iteration 100: Loss = -10926.217737258587
Iteration 110: Loss = -10926.218778447688
1
Iteration 120: Loss = -10926.220652878319
2
Iteration 130: Loss = -10926.222910886523
3
Stopping early at iteration 129 due to no improvement.
pi: tensor([[0.0821, 0.9179],
        [0.1246, 0.8754]], dtype=torch.float64)
alpha: tensor([0.1187, 0.8813])
beta: tensor([[[0.2343, 0.1750],
         [0.3783, 0.1504]],

        [[0.5070, 0.2002],
         [0.2024, 0.6463]],

        [[0.8450, 0.2048],
         [0.7778, 0.4574]],

        [[0.9243, 0.1903],
         [0.1293, 0.3598]],

        [[0.6652, 0.1699],
         [0.0471, 0.5149]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 59
Adjusted Rand Index: -0.02789997666760923
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.009696969696969697
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.004936296329625465
Average Adjusted Rand Index: -0.008543252070615024
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17645.587200971
Iteration 100: Loss = -10941.820632003644
Iteration 200: Loss = -10929.2750924542
Iteration 300: Loss = -10927.661534059898
Iteration 400: Loss = -10926.70833661787
Iteration 500: Loss = -10926.245166394569
Iteration 600: Loss = -10926.002189033898
Iteration 700: Loss = -10925.864977982996
Iteration 800: Loss = -10925.780521859182
Iteration 900: Loss = -10925.726236014772
Iteration 1000: Loss = -10925.691272160237
Iteration 1100: Loss = -10925.66845096043
Iteration 1200: Loss = -10925.652784776894
Iteration 1300: Loss = -10925.641269808975
Iteration 1400: Loss = -10925.632096677673
Iteration 1500: Loss = -10925.624454425459
Iteration 1600: Loss = -10925.617956336826
Iteration 1700: Loss = -10925.612247900528
Iteration 1800: Loss = -10925.6072683838
Iteration 1900: Loss = -10925.602768531793
Iteration 2000: Loss = -10925.59869561117
Iteration 2100: Loss = -10925.594879338241
Iteration 2200: Loss = -10925.591219095651
Iteration 2300: Loss = -10925.587692177269
Iteration 2400: Loss = -10925.584100412923
Iteration 2500: Loss = -10925.58036741317
Iteration 2600: Loss = -10925.576401043749
Iteration 2700: Loss = -10925.571990977089
Iteration 2800: Loss = -10925.566941568264
Iteration 2900: Loss = -10925.561058002471
Iteration 3000: Loss = -10925.55418906346
Iteration 3100: Loss = -10925.546146518267
Iteration 3200: Loss = -10925.536924531283
Iteration 3300: Loss = -10925.52656459057
Iteration 3400: Loss = -10925.513559602405
Iteration 3500: Loss = -10925.433009801212
Iteration 3600: Loss = -10925.405402504644
Iteration 3700: Loss = -10925.380887506992
Iteration 3800: Loss = -10925.361478950677
Iteration 3900: Loss = -10925.352103362291
Iteration 4000: Loss = -10925.339262785415
Iteration 4100: Loss = -10925.335549094827
Iteration 4200: Loss = -10925.328877947404
Iteration 4300: Loss = -10925.326881081202
Iteration 4400: Loss = -10925.356420761947
1
Iteration 4500: Loss = -10925.323948199015
Iteration 4600: Loss = -10925.323714553118
Iteration 4700: Loss = -10925.35164970948
1
Iteration 4800: Loss = -10925.320971525103
Iteration 4900: Loss = -10925.25200433397
Iteration 5000: Loss = -10925.169284419899
Iteration 5100: Loss = -10925.167768416946
Iteration 5200: Loss = -10925.17058074232
1
Iteration 5300: Loss = -10925.166393735315
Iteration 5400: Loss = -10925.166546040991
1
Iteration 5500: Loss = -10925.165385441562
Iteration 5600: Loss = -10925.165702225724
1
Iteration 5700: Loss = -10925.164558553744
Iteration 5800: Loss = -10925.168689888676
1
Iteration 5900: Loss = -10925.163801167051
Iteration 6000: Loss = -10925.16349550753
Iteration 6100: Loss = -10925.16363342846
1
Iteration 6200: Loss = -10925.162887919321
Iteration 6300: Loss = -10925.349025584283
1
Iteration 6400: Loss = -10925.162384141668
Iteration 6500: Loss = -10925.162149823467
Iteration 6600: Loss = -10925.166278514373
1
Iteration 6700: Loss = -10925.161709359743
Iteration 6800: Loss = -10925.16147949748
Iteration 6900: Loss = -10925.377764875166
1
Iteration 7000: Loss = -10925.161179299674
Iteration 7100: Loss = -10925.160960657075
Iteration 7200: Loss = -10925.217255509726
1
Iteration 7300: Loss = -10925.160643916
Iteration 7400: Loss = -10925.160516622334
Iteration 7500: Loss = -10925.164017974364
1
Iteration 7600: Loss = -10925.160252349222
Iteration 7700: Loss = -10925.160120963394
Iteration 7800: Loss = -10925.177854165955
1
Iteration 7900: Loss = -10925.159936758286
Iteration 8000: Loss = -10925.159770900753
Iteration 8100: Loss = -10925.162024120846
1
Iteration 8200: Loss = -10925.159651990893
Iteration 8300: Loss = -10925.159536310857
Iteration 8400: Loss = -10925.162126853207
1
Iteration 8500: Loss = -10925.15986910376
2
Iteration 8600: Loss = -10925.159302497776
Iteration 8700: Loss = -10925.159813160773
1
Iteration 8800: Loss = -10925.159332631103
2
Iteration 8900: Loss = -10925.177955756983
3
Iteration 9000: Loss = -10925.165632680244
4
Iteration 9100: Loss = -10925.183393171106
5
Iteration 9200: Loss = -10925.158913755577
Iteration 9300: Loss = -10925.158975392094
1
Iteration 9400: Loss = -10925.162702063833
2
Iteration 9500: Loss = -10925.169939767722
3
Iteration 9600: Loss = -10925.159157527243
4
Iteration 9700: Loss = -10925.165692495837
5
Iteration 9800: Loss = -10925.158688634549
Iteration 9900: Loss = -10925.28629219991
1
Iteration 10000: Loss = -10925.158607460811
Iteration 10100: Loss = -10925.15857596873
Iteration 10200: Loss = -10925.15867567251
1
Iteration 10300: Loss = -10925.159233294717
2
Iteration 10400: Loss = -10925.160455739928
3
Iteration 10500: Loss = -10925.400974758852
4
Iteration 10600: Loss = -10925.162704965738
5
Iteration 10700: Loss = -10925.179942369565
6
Iteration 10800: Loss = -10925.158566257549
Iteration 10900: Loss = -10925.165651661637
1
Iteration 11000: Loss = -10925.158588379009
2
Iteration 11100: Loss = -10925.158790193927
3
Iteration 11200: Loss = -10925.158261606544
Iteration 11300: Loss = -10925.289996646023
1
Iteration 11400: Loss = -10925.1582029635
Iteration 11500: Loss = -10925.158333308094
1
Iteration 11600: Loss = -10925.165520820405
2
Iteration 11700: Loss = -10925.158188979463
Iteration 11800: Loss = -10925.159183610791
1
Iteration 11900: Loss = -10925.159134145882
2
Iteration 12000: Loss = -10925.159290907548
3
Iteration 12100: Loss = -10925.160061424705
4
Iteration 12200: Loss = -10925.159196345594
5
Iteration 12300: Loss = -10925.159373391301
6
Iteration 12400: Loss = -10925.165659649014
7
Iteration 12500: Loss = -10925.162501337845
8
Iteration 12600: Loss = -10925.17534762773
9
Iteration 12700: Loss = -10925.158056541572
Iteration 12800: Loss = -10925.16611572749
1
Iteration 12900: Loss = -10925.160197267182
2
Iteration 13000: Loss = -10925.158087905478
3
Iteration 13100: Loss = -10925.471976773075
4
Iteration 13200: Loss = -10925.162483874301
5
Iteration 13300: Loss = -10925.165337910943
6
Iteration 13400: Loss = -10925.158659042929
7
Iteration 13500: Loss = -10925.162807134697
8
Iteration 13600: Loss = -10925.158418479688
9
Iteration 13700: Loss = -10925.158651301888
10
Stopping early at iteration 13700 due to no improvement.
tensor([[-3.4651,  1.9400],
        [-5.7193,  3.9717],
        [-3.0385,  1.3023],
        [-3.0748,  1.5681],
        [-5.7551,  3.6981],
        [-4.2125,  2.7606],
        [-2.3155,  0.8655],
        [-0.8489, -0.6264],
        [-6.2986,  4.8943],
        [-3.8461,  0.1085],
        [-6.7649,  4.1886],
        [-5.2302,  3.7705],
        [-6.6164,  4.4116],
        [-6.1847,  4.6978],
        [-2.2918, -0.4994],
        [-3.8872,  2.4732],
        [-2.7534,  1.1717],
        [-5.0747,  0.9618],
        [-3.4425,  1.9255],
        [-3.1746,  1.6051],
        [-5.0438,  3.6474],
        [-3.7936,  2.3767],
        [-5.5809,  2.6328],
        [-4.0732,  2.6745],
        [-4.6575,  3.2666],
        [-4.6888,  2.1949],
        [-3.9080,  2.3695],
        [-5.8766,  4.4425],
        [-2.6921,  1.3014],
        [-3.0132,  1.5650],
        [-7.1031,  3.2514],
        [-3.8658,  2.4699],
        [-4.9277,  1.4922],
        [-5.3805,  2.1534],
        [-3.4634,  0.7541],
        [-4.6698,  2.9782],
        [-4.7889,  1.2330],
        [-3.1155,  1.7268],
        [-1.3438, -0.0442],
        [-5.4171,  3.9224],
        [-2.6662,  1.1265],
        [-5.1978,  3.2423],
        [-6.5422,  4.3597],
        [-5.6194,  1.6470],
        [-2.8753,  0.9695],
        [-6.8278,  4.8927],
        [-5.7903,  3.8330],
        [-5.2192,  3.5899],
        [-2.2664,  0.8800],
        [-3.4624,  1.9589],
        [-2.4593, -2.0048],
        [-3.8770,  2.4659],
        [-2.5194,  0.1030],
        [-5.7587,  2.5752],
        [-5.1480,  3.7262],
        [-5.7088,  2.4737],
        [-6.1848,  4.4845],
        [-6.9941,  4.0880],
        [-4.6707,  2.8834],
        [-2.4871,  1.0039],
        [-6.7092,  4.2648],
        [-3.8257,  2.1869],
        [-5.9515,  4.1904],
        [-2.7510,  1.0084],
        [-6.4977,  3.7387],
        [-6.3299,  4.0420],
        [-3.7178,  2.2862],
        [-5.2319,  2.5276],
        [-4.4464,  1.3313],
        [-5.4186,  3.6949],
        [-5.1791,  3.7124],
        [-5.7715,  3.3664],
        [-3.0247,  1.4698],
        [-5.3212,  1.4994],
        [-5.5488,  2.7410],
        [-6.3137,  2.3856],
        [-2.3077,  0.9074],
        [-5.4028,  3.9470],
        [-6.4037,  4.0561],
        [-2.9721,  1.2187],
        [-2.8900,  1.2203],
        [-1.7799, -0.7664],
        [-5.3970,  1.9474],
        [-2.9798, -0.5561],
        [-4.5631,  2.9520],
        [-3.2846,  1.8939],
        [-1.3445, -0.0939],
        [-3.1306,  1.7024],
        [-5.7328,  4.1671],
        [-6.0912,  3.9254],
        [-5.3352,  2.9488],
        [-6.0335,  4.6469],
        [-4.1112,  2.6881],
        [-5.2425,  2.9917],
        [-6.6997,  4.6485],
        [-2.9996,  1.6130],
        [-3.3871,  1.7370],
        [-6.6596,  5.0805],
        [-2.6591,  1.2230],
        [-4.0171,  2.6084]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.6419e-05, 9.9998e-01],
        [8.5065e-02, 9.1494e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0230, 0.9770], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2582, 0.0828],
         [0.3783, 0.1560]],

        [[0.5070, 0.2104],
         [0.2024, 0.6463]],

        [[0.8450, 0.2159],
         [0.7778, 0.4574]],

        [[0.9243, 0.2020],
         [0.1293, 0.3598]],

        [[0.6652, 0.1710],
         [0.0471, 0.5149]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 61
Adjusted Rand Index: -0.015623423336712405
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0029139835017840316
Average Adjusted Rand Index: -0.005010292448297414
Iteration 0: Loss = -16490.902328298263
Iteration 10: Loss = -10927.15599798825
Iteration 20: Loss = -10926.54928969167
Iteration 30: Loss = -10926.479618878999
Iteration 40: Loss = -10926.47217443678
Iteration 50: Loss = -10926.469711753007
Iteration 60: Loss = -10926.467611147473
Iteration 70: Loss = -10926.465416024217
Iteration 80: Loss = -10926.462956365507
Iteration 90: Loss = -10926.460337325887
Iteration 100: Loss = -10926.45745283793
Iteration 110: Loss = -10926.454405169487
Iteration 120: Loss = -10926.451080661356
Iteration 130: Loss = -10926.447534372908
Iteration 140: Loss = -10926.443745324952
Iteration 150: Loss = -10926.439644248225
Iteration 160: Loss = -10926.435416217848
Iteration 170: Loss = -10926.430903342874
Iteration 180: Loss = -10926.426187778987
Iteration 190: Loss = -10926.421197526006
Iteration 200: Loss = -10926.416026278957
Iteration 210: Loss = -10926.410629191285
Iteration 220: Loss = -10926.404951701299
Iteration 230: Loss = -10926.399076224847
Iteration 240: Loss = -10926.393061317427
Iteration 250: Loss = -10926.386871550854
Iteration 260: Loss = -10926.380510033043
Iteration 270: Loss = -10926.373996970508
Iteration 280: Loss = -10926.367312672797
Iteration 290: Loss = -10926.360571351841
Iteration 300: Loss = -10926.353737572495
Iteration 310: Loss = -10926.346795568199
Iteration 320: Loss = -10926.339865544187
Iteration 330: Loss = -10926.332902584669
Iteration 340: Loss = -10926.326058720793
Iteration 350: Loss = -10926.319216098384
Iteration 360: Loss = -10926.312489381913
Iteration 370: Loss = -10926.305953956127
Iteration 380: Loss = -10926.29953926863
Iteration 390: Loss = -10926.293381850866
Iteration 400: Loss = -10926.2874746818
Iteration 410: Loss = -10926.28186184609
Iteration 420: Loss = -10926.276587115775
Iteration 430: Loss = -10926.271632718628
Iteration 440: Loss = -10926.267022707021
Iteration 450: Loss = -10926.262853393657
Iteration 460: Loss = -10926.259058629585
Iteration 470: Loss = -10926.255713959124
Iteration 480: Loss = -10926.252733646517
Iteration 490: Loss = -10926.25013738311
Iteration 500: Loss = -10926.247880139012
Iteration 510: Loss = -10926.24607464885
Iteration 520: Loss = -10926.244545208836
Iteration 530: Loss = -10926.243280522693
Iteration 540: Loss = -10926.242258091059
Iteration 550: Loss = -10926.241484754182
Iteration 560: Loss = -10926.240891398918
Iteration 570: Loss = -10926.240408595224
Iteration 580: Loss = -10926.240086811
Iteration 590: Loss = -10926.239830173372
Iteration 600: Loss = -10926.23966967897
Iteration 610: Loss = -10926.239553502457
Iteration 620: Loss = -10926.239436739044
Iteration 630: Loss = -10926.239431185491
Iteration 640: Loss = -10926.239379772656
Iteration 650: Loss = -10926.23935871119
Iteration 660: Loss = -10926.239371243708
1
Iteration 670: Loss = -10926.239359835308
2
Iteration 680: Loss = -10926.23935327433
Iteration 690: Loss = -10926.23934075522
Iteration 700: Loss = -10926.2393462534
1
Iteration 710: Loss = -10926.239350705759
2
Iteration 720: Loss = -10926.239359434821
3
Stopping early at iteration 719 due to no improvement.
pi: tensor([[0.5008, 0.4992],
        [0.5045, 0.4955]], dtype=torch.float64)
alpha: tensor([0.5011, 0.4989])
beta: tensor([[[0.1322, 0.1474],
         [0.5801, 0.1944]],

        [[0.8726, 0.1596],
         [0.5097, 0.0758]],

        [[0.7424, 0.1594],
         [0.5223, 0.4863]],

        [[0.0309, 0.1559],
         [0.8041, 0.9698]],

        [[0.8921, 0.1577],
         [0.2947, 0.2870]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 72
Adjusted Rand Index: 0.18545454545454546
time is 1
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 80
Adjusted Rand Index: 0.353778413555379
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.01575757575757576
time is 3
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 68
Adjusted Rand Index: 0.12079645497608049
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.022626262626262626
Global Adjusted Rand Index: 0.11932350744033172
Average Adjusted Rand Index: 0.13968265047396866
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16490.78296149327
Iteration 100: Loss = -10987.39723563316
Iteration 200: Loss = -10941.348841458657
Iteration 300: Loss = -10929.539960208469
Iteration 400: Loss = -10928.776202283188
Iteration 500: Loss = -10928.437806024893
Iteration 600: Loss = -10928.235466707096
Iteration 700: Loss = -10928.09295205238
Iteration 800: Loss = -10927.992015186459
Iteration 900: Loss = -10927.915713721239
Iteration 1000: Loss = -10927.854789047029
Iteration 1100: Loss = -10927.803674165016
Iteration 1200: Loss = -10927.763467143814
Iteration 1300: Loss = -10927.73076614211
Iteration 1400: Loss = -10927.712393853511
Iteration 1500: Loss = -10927.676694727343
Iteration 1600: Loss = -10927.65436663889
Iteration 1700: Loss = -10927.640290191675
Iteration 1800: Loss = -10927.627657097522
Iteration 1900: Loss = -10927.607421721088
Iteration 2000: Loss = -10927.5725362047
Iteration 2100: Loss = -10927.558399958345
Iteration 2200: Loss = -10927.550767310517
Iteration 2300: Loss = -10927.54435369321
Iteration 2400: Loss = -10927.538686294512
Iteration 2500: Loss = -10927.548027945451
1
Iteration 2600: Loss = -10927.529571083656
Iteration 2700: Loss = -10927.52579883133
Iteration 2800: Loss = -10927.523562620616
Iteration 2900: Loss = -10927.519135818631
Iteration 3000: Loss = -10927.515617437499
Iteration 3100: Loss = -10927.5128054735
Iteration 3200: Loss = -10927.508875589072
Iteration 3300: Loss = -10927.506132668299
Iteration 3400: Loss = -10927.50453684888
Iteration 3500: Loss = -10927.500857852994
Iteration 3600: Loss = -10927.49834643507
Iteration 3700: Loss = -10927.496032147432
Iteration 3800: Loss = -10927.4863093265
Iteration 3900: Loss = -10927.523221949448
1
Iteration 4000: Loss = -10927.48248445277
Iteration 4100: Loss = -10927.48085858223
Iteration 4200: Loss = -10927.479563161816
Iteration 4300: Loss = -10927.476980723437
Iteration 4400: Loss = -10927.474465842153
Iteration 4500: Loss = -10927.469371555808
Iteration 4600: Loss = -10927.4612758408
Iteration 4700: Loss = -10927.458165253265
Iteration 4800: Loss = -10927.457067362035
Iteration 4900: Loss = -10927.453794094912
Iteration 5000: Loss = -10927.450433461468
Iteration 5100: Loss = -10927.449560944198
Iteration 5200: Loss = -10927.45836889054
1
Iteration 5300: Loss = -10927.446429527252
Iteration 5400: Loss = -10927.586318312826
1
Iteration 5500: Loss = -10927.43997206908
Iteration 5600: Loss = -10927.437937854178
Iteration 5700: Loss = -10927.431253957322
Iteration 5800: Loss = -10927.428822825243
Iteration 5900: Loss = -10927.462148228842
1
Iteration 6000: Loss = -10927.43830333508
2
Iteration 6100: Loss = -10927.431424114504
3
Iteration 6200: Loss = -10927.425703750303
Iteration 6300: Loss = -10927.424846352736
Iteration 6400: Loss = -10927.424486051408
Iteration 6500: Loss = -10927.425517878713
1
Iteration 6600: Loss = -10927.423939960205
Iteration 6700: Loss = -10927.424121353004
1
Iteration 6800: Loss = -10927.42328369348
Iteration 6900: Loss = -10927.425046114238
1
Iteration 7000: Loss = -10927.42329809493
2
Iteration 7100: Loss = -10927.423075429157
Iteration 7200: Loss = -10927.422345578621
Iteration 7300: Loss = -10927.422183574843
Iteration 7400: Loss = -10927.43744253212
1
Iteration 7500: Loss = -10927.418353659026
Iteration 7600: Loss = -10927.400347916288
Iteration 7700: Loss = -10927.401844975635
1
Iteration 7800: Loss = -10927.599901957285
2
Iteration 7900: Loss = -10927.399806271695
Iteration 8000: Loss = -10927.400647952612
1
Iteration 8100: Loss = -10927.39972123399
Iteration 8200: Loss = -10927.39933966887
Iteration 8300: Loss = -10927.400805210138
1
Iteration 8400: Loss = -10927.401166701387
2
Iteration 8500: Loss = -10927.438299031757
3
Iteration 8600: Loss = -10927.400793773479
4
Iteration 8700: Loss = -10927.398977506868
Iteration 8800: Loss = -10927.415566944293
1
Iteration 8900: Loss = -10927.464538205557
2
Iteration 9000: Loss = -10927.406668918149
3
Iteration 9100: Loss = -10927.398396419805
Iteration 9200: Loss = -10927.39877179556
1
Iteration 9300: Loss = -10927.445863989482
2
Iteration 9400: Loss = -10927.427487048722
3
Iteration 9500: Loss = -10927.390628197161
Iteration 9600: Loss = -10927.390819957436
1
Iteration 9700: Loss = -10927.409619152484
2
Iteration 9800: Loss = -10927.391440742394
3
Iteration 9900: Loss = -10927.430902807528
4
Iteration 10000: Loss = -10927.389866556368
Iteration 10100: Loss = -10927.391094459928
1
Iteration 10200: Loss = -10927.389805631215
Iteration 10300: Loss = -10927.389846896638
1
Iteration 10400: Loss = -10927.389976707183
2
Iteration 10500: Loss = -10927.389845988198
3
Iteration 10600: Loss = -10927.421307493385
4
Iteration 10700: Loss = -10927.416724552759
5
Iteration 10800: Loss = -10927.394218537416
6
Iteration 10900: Loss = -10927.39080755011
7
Iteration 11000: Loss = -10927.389742599527
Iteration 11100: Loss = -10927.390056527065
1
Iteration 11200: Loss = -10927.62124066024
2
Iteration 11300: Loss = -10927.388709220237
Iteration 11400: Loss = -10927.478871214948
1
Iteration 11500: Loss = -10927.383929657688
Iteration 11600: Loss = -10927.436289373001
1
Iteration 11700: Loss = -10927.383939278383
2
Iteration 11800: Loss = -10927.384328167745
3
Iteration 11900: Loss = -10927.481608114278
4
Iteration 12000: Loss = -10927.38376110503
Iteration 12100: Loss = -10927.386391801301
1
Iteration 12200: Loss = -10927.383214803653
Iteration 12300: Loss = -10927.38505719277
1
Iteration 12400: Loss = -10927.38315222679
Iteration 12500: Loss = -10927.383724745094
1
Iteration 12600: Loss = -10927.383694751124
2
Iteration 12700: Loss = -10927.396616570017
3
Iteration 12800: Loss = -10927.385275576547
4
Iteration 12900: Loss = -10927.384875449956
5
Iteration 13000: Loss = -10927.382405148497
Iteration 13100: Loss = -10927.381448077129
Iteration 13200: Loss = -10927.381465328113
1
Iteration 13300: Loss = -10927.383524723136
2
Iteration 13400: Loss = -10927.381119548907
Iteration 13500: Loss = -10927.38283008309
1
Iteration 13600: Loss = -10927.39790717915
2
Iteration 13700: Loss = -10927.381945765703
3
Iteration 13800: Loss = -10927.381094398002
Iteration 13900: Loss = -10927.657323005447
1
Iteration 14000: Loss = -10927.381087087522
Iteration 14100: Loss = -10927.473675430621
1
Iteration 14200: Loss = -10927.381071667436
Iteration 14300: Loss = -10927.389331602495
1
Iteration 14400: Loss = -10927.401037701506
2
Iteration 14500: Loss = -10927.381122464385
3
Iteration 14600: Loss = -10927.442673725844
4
Iteration 14700: Loss = -10927.395608006906
5
Iteration 14800: Loss = -10927.398414983027
6
Iteration 14900: Loss = -10927.416321062035
7
Iteration 15000: Loss = -10927.381705324487
8
Iteration 15100: Loss = -10927.381033266673
Iteration 15200: Loss = -10927.558979036941
1
Iteration 15300: Loss = -10927.381013245504
Iteration 15400: Loss = -10927.421378245412
1
Iteration 15500: Loss = -10927.38086259289
Iteration 15600: Loss = -10927.381461824087
1
Iteration 15700: Loss = -10927.390922900562
2
Iteration 15800: Loss = -10927.381024417879
3
Iteration 15900: Loss = -10927.39348433511
4
Iteration 16000: Loss = -10927.387252320465
5
Iteration 16100: Loss = -10927.383046503828
6
Iteration 16200: Loss = -10927.380852108345
Iteration 16300: Loss = -10927.38107350578
1
Iteration 16400: Loss = -10927.381892595435
2
Iteration 16500: Loss = -10927.393897616552
3
Iteration 16600: Loss = -10927.380855264288
4
Iteration 16700: Loss = -10927.381505112362
5
Iteration 16800: Loss = -10927.40485894136
6
Iteration 16900: Loss = -10927.385671913567
7
Iteration 17000: Loss = -10927.397413288694
8
Iteration 17100: Loss = -10927.396302012166
9
Iteration 17200: Loss = -10927.383038280781
10
Stopping early at iteration 17200 due to no improvement.
tensor([[ 1.9889, -3.3787],
        [ 1.2486, -2.7028],
        [ 1.9602, -3.3799],
        [ 1.9308, -3.3502],
        [ 1.2196, -2.6117],
        [ 1.4132, -2.9536],
        [ 0.5504, -5.1657],
        [ 2.2859, -4.2343],
        [ 0.8019, -2.2292],
        [ 1.5980, -3.9165],
        [ 0.1770, -2.9748],
        [ 1.1833, -2.5859],
        [ 0.8699, -2.7736],
        [ 0.6272, -2.6734],
        [ 2.3547, -3.7692],
        [ 0.9687, -3.6905],
        [ 1.6350, -3.8568],
        [ 1.7188, -3.1126],
        [ 0.3187, -4.2541],
        [ 1.3556, -3.0866],
        [ 1.1889, -3.0917],
        [ 1.6735, -3.0919],
        [ 0.9610, -2.4003],
        [ 1.4895, -3.0845],
        [ 0.9895, -2.9514],
        [ 1.5750, -3.0676],
        [ 1.3033, -2.7007],
        [ 0.6560, -2.8842],
        [ 1.9070, -3.3356],
        [ 1.5466, -3.2795],
        [ 0.7700, -2.7707],
        [ 1.4762, -2.8758],
        [ 1.0059, -2.4730],
        [ 1.1007, -2.8339],
        [ 1.2295, -3.5856],
        [ 1.5304, -2.9531],
        [ 1.5240, -3.0421],
        [ 1.3867, -3.4670],
        [ 1.9375, -3.7432],
        [ 1.0335, -2.6534],
        [ 1.8999, -3.5930],
        [ 0.4638, -2.7520],
        [ 0.9181, -2.4126],
        [ 1.2510, -2.6498],
        [ 1.8271, -3.6509],
        [-1.6412,  0.2549],
        [ 0.3724, -3.3773],
        [ 0.9663, -2.9524],
        [ 2.1084, -3.5854],
        [ 1.7909, -3.2028],
        [ 2.3324, -4.0516],
        [ 1.2492, -3.3643],
        [ 2.0267, -3.4404],
        [ 0.8112, -3.4551],
        [ 1.2657, -2.6706],
        [ 0.5066, -3.6845],
        [ 0.3465, -2.7020],
        [-0.3062, -3.8650],
        [ 0.9359, -3.6465],
        [ 2.0441, -3.4875],
        [ 0.4938, -2.8512],
        [ 1.1483, -3.7218],
        [ 0.8444, -2.4478],
        [ 1.8335, -3.3446],
        [ 0.4936, -2.5412],
        [ 0.8471, -2.3820],
        [ 1.0367, -2.7389],
        [ 0.8550, -3.3162],
        [ 0.8405, -3.9899],
        [ 1.0115, -2.7139],
        [ 1.2497, -2.7050],
        [ 0.6157, -2.8575],
        [ 0.3316, -1.8626],
        [ 1.1793, -3.3620],
        [ 1.3303, -2.7287],
        [ 0.9282, -3.1644],
        [ 2.1361, -3.5380],
        [ 0.3051, -3.4163],
        [ 0.9917, -2.6034],
        [ 1.9084, -3.2961],
        [ 1.4382, -3.7917],
        [ 2.4344, -3.9058],
        [ 1.6013, -3.0985],
        [ 2.1939, -3.7040],
        [ 0.7689, -3.6147],
        [ 0.6325, -4.1159],
        [ 2.5315, -3.9179],
        [ 1.3675, -3.4381],
        [ 1.1493, -2.6881],
        [ 0.0433, -3.3975],
        [ 1.2977, -2.7750],
        [ 0.8818, -2.4273],
        [ 1.6060, -2.9942],
        [ 1.0215, -3.0263],
        [ 0.8392, -2.2272],
        [ 1.6006, -3.5162],
        [ 1.4135, -3.4184],
        [ 1.0008, -2.4146],
        [ 2.0173, -3.4674],
        [ 1.4979, -3.0855]], dtype=torch.float64, requires_grad=True)
pi: tensor([[5.8101e-07, 1.0000e+00],
        [4.5905e-01, 5.4095e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9745, 0.0255], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1543, 0.1863],
         [0.5801, 0.1652]],

        [[0.8726, 0.2847],
         [0.5097, 0.0758]],

        [[0.7424, 0.1616],
         [0.5223, 0.4863]],

        [[0.0309, 0.1537],
         [0.8041, 0.9698]],

        [[0.8921, 0.1581],
         [0.2947, 0.2870]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 62
Adjusted Rand Index: -0.008233622622776837
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00045868142037170875
Average Adjusted Rand Index: -0.00043460331243415503
Iteration 0: Loss = -36171.09655243046
Iteration 10: Loss = -10926.496626426728
Iteration 20: Loss = -10926.295597227856
Iteration 30: Loss = -10926.251760877343
Iteration 40: Loss = -10926.231296708269
Iteration 50: Loss = -10926.221915316266
Iteration 60: Loss = -10926.218279361661
Iteration 70: Loss = -10926.21770093115
Iteration 80: Loss = -10926.218716658033
1
Iteration 90: Loss = -10926.220556478149
2
Iteration 100: Loss = -10926.222811842266
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.8755, 0.1245],
        [0.9180, 0.0820]], dtype=torch.float64)
alpha: tensor([0.8814, 0.1186])
beta: tensor([[[0.1504, 0.1750],
         [0.8548, 0.2343]],

        [[0.4761, 0.2002],
         [0.1049, 0.2494]],

        [[0.1361, 0.2049],
         [0.8469, 0.5108]],

        [[0.7491, 0.1903],
         [0.5035, 0.0994]],

        [[0.8162, 0.1699],
         [0.6700, 0.8142]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: -0.02789997666760923
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.009696969696969697
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.005119313988496187
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.004936296329625465
Average Adjusted Rand Index: -0.008543252070615024
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36171.21957354983
Iteration 100: Loss = -11041.566677769806
Iteration 200: Loss = -11009.561086209755
Iteration 300: Loss = -10989.790563562728
Iteration 400: Loss = -10975.709027417948
Iteration 500: Loss = -10960.212618186446
Iteration 600: Loss = -10952.61084228913
Iteration 700: Loss = -10944.89332045938
Iteration 800: Loss = -10939.94688547717
Iteration 900: Loss = -10936.9443706212
Iteration 1000: Loss = -10931.88300995818
Iteration 1100: Loss = -10931.016127667155
Iteration 1200: Loss = -10930.305859441653
Iteration 1300: Loss = -10929.836575668965
Iteration 1400: Loss = -10929.515828403377
Iteration 1500: Loss = -10929.278049643195
Iteration 1600: Loss = -10929.09393404465
Iteration 1700: Loss = -10928.947724149859
Iteration 1800: Loss = -10928.829572908433
Iteration 1900: Loss = -10928.732619381533
Iteration 2000: Loss = -10928.651537883634
Iteration 2100: Loss = -10928.582666656244
Iteration 2200: Loss = -10928.523461908648
Iteration 2300: Loss = -10928.4719893992
Iteration 2400: Loss = -10928.427007213048
Iteration 2500: Loss = -10928.38736869331
Iteration 2600: Loss = -10928.352185773128
Iteration 2700: Loss = -10928.320962018961
Iteration 2800: Loss = -10928.293008144285
Iteration 2900: Loss = -10928.267913758971
Iteration 3000: Loss = -10928.245360385383
Iteration 3100: Loss = -10928.224900623714
Iteration 3200: Loss = -10928.206323669274
Iteration 3300: Loss = -10928.189464192354
Iteration 3400: Loss = -10928.174069712632
Iteration 3500: Loss = -10928.15997432978
Iteration 3600: Loss = -10928.14703341753
Iteration 3700: Loss = -10928.135138583611
Iteration 3800: Loss = -10928.124247975702
Iteration 3900: Loss = -10928.11422154707
Iteration 4000: Loss = -10928.104925060645
Iteration 4100: Loss = -10928.096362487844
Iteration 4200: Loss = -10928.088421812685
Iteration 4300: Loss = -10928.081079478685
Iteration 4400: Loss = -10928.074232904724
Iteration 4500: Loss = -10928.067850058715
Iteration 4600: Loss = -10928.061965728117
Iteration 4700: Loss = -10928.056439163485
Iteration 4800: Loss = -10928.051298336672
Iteration 4900: Loss = -10928.046472640817
Iteration 5000: Loss = -10928.042002908061
Iteration 5100: Loss = -10928.037785898206
Iteration 5200: Loss = -10928.033870804344
Iteration 5300: Loss = -10928.030201275375
Iteration 5400: Loss = -10928.02678793702
Iteration 5500: Loss = -10928.02453156039
Iteration 5600: Loss = -10928.020457937813
Iteration 5700: Loss = -10928.017654591942
Iteration 5800: Loss = -10928.014945724626
Iteration 5900: Loss = -10928.012880147555
Iteration 6000: Loss = -10928.010110960015
Iteration 6100: Loss = -10928.00786521986
Iteration 6200: Loss = -10928.323063473877
1
Iteration 6300: Loss = -10928.003778827559
Iteration 6400: Loss = -10928.00188710858
Iteration 6500: Loss = -10928.000186100639
Iteration 6600: Loss = -10928.013144402252
1
Iteration 6700: Loss = -10927.996991159847
Iteration 6800: Loss = -10927.99547148927
Iteration 6900: Loss = -10927.99410043112
Iteration 7000: Loss = -10927.993043254148
Iteration 7100: Loss = -10927.991539296607
Iteration 7200: Loss = -10927.990395539164
Iteration 7300: Loss = -10928.111378919573
1
Iteration 7400: Loss = -10927.988247415407
Iteration 7500: Loss = -10927.987279508525
Iteration 7600: Loss = -10927.986368102862
Iteration 7700: Loss = -10928.010875544953
1
Iteration 7800: Loss = -10927.984667629995
Iteration 7900: Loss = -10927.983867712757
Iteration 8000: Loss = -10927.98324425259
Iteration 8100: Loss = -10927.98242250773
Iteration 8200: Loss = -10927.981762022579
Iteration 8300: Loss = -10927.981156040094
Iteration 8400: Loss = -10927.98124043079
1
Iteration 8500: Loss = -10927.980009870469
Iteration 8600: Loss = -10927.979536928644
Iteration 8700: Loss = -10928.07213690782
1
Iteration 8800: Loss = -10927.978564689935
Iteration 8900: Loss = -10927.97818017668
Iteration 9000: Loss = -10927.977690732032
Iteration 9100: Loss = -10927.978298449596
1
Iteration 9200: Loss = -10927.9771744159
Iteration 9300: Loss = -10927.976569420147
Iteration 9400: Loss = -10927.976376851038
Iteration 9500: Loss = -10927.975951941866
Iteration 9600: Loss = -10927.975652158466
Iteration 9700: Loss = -10927.981171596233
1
Iteration 9800: Loss = -10927.975104963845
Iteration 9900: Loss = -10928.017885444733
1
Iteration 10000: Loss = -10927.974768815931
Iteration 10100: Loss = -10927.99652882623
1
Iteration 10200: Loss = -10927.974201992005
Iteration 10300: Loss = -10927.975265532052
1
Iteration 10400: Loss = -10927.973853646294
Iteration 10500: Loss = -10927.978523358996
1
Iteration 10600: Loss = -10927.973455021449
Iteration 10700: Loss = -10928.022361367144
1
Iteration 10800: Loss = -10927.973163800807
Iteration 10900: Loss = -10927.973695590925
1
Iteration 11000: Loss = -10927.973045454193
Iteration 11100: Loss = -10927.972747497906
Iteration 11200: Loss = -10927.9742129006
1
Iteration 11300: Loss = -10927.972521158725
Iteration 11400: Loss = -10927.972385488529
Iteration 11500: Loss = -10927.972463614617
1
Iteration 11600: Loss = -10927.97220138652
Iteration 11700: Loss = -10927.972383901275
1
Iteration 11800: Loss = -10927.972073816152
Iteration 11900: Loss = -10927.971920822029
Iteration 12000: Loss = -10927.971847318024
Iteration 12100: Loss = -10927.97209413923
1
Iteration 12200: Loss = -10927.97166508279
Iteration 12300: Loss = -10927.97160029525
Iteration 12400: Loss = -10927.974333057871
1
Iteration 12500: Loss = -10927.971478044321
Iteration 12600: Loss = -10927.971413649435
Iteration 12700: Loss = -10928.040156467508
1
Iteration 12800: Loss = -10927.971318524242
Iteration 12900: Loss = -10927.9712769838
Iteration 13000: Loss = -10927.971221662014
Iteration 13100: Loss = -10927.975962325714
1
Iteration 13200: Loss = -10927.971081587044
Iteration 13300: Loss = -10927.971032248968
Iteration 13400: Loss = -10927.971299279756
1
Iteration 13500: Loss = -10927.971022146898
Iteration 13600: Loss = -10927.97108356853
1
Iteration 13700: Loss = -10927.973179109818
2
Iteration 13800: Loss = -10927.970990622027
Iteration 13900: Loss = -10927.970913771993
Iteration 14000: Loss = -10927.970890254768
Iteration 14100: Loss = -10927.974241651838
1
Iteration 14200: Loss = -10928.020207850808
2
Iteration 14300: Loss = -10927.973538757436
3
Iteration 14400: Loss = -10928.025546032719
4
Iteration 14500: Loss = -10927.970791497943
Iteration 14600: Loss = -10927.974378925715
1
Iteration 14700: Loss = -10927.970725918236
Iteration 14800: Loss = -10927.970722748792
Iteration 14900: Loss = -10927.970769720872
1
Iteration 15000: Loss = -10927.970685720125
Iteration 15100: Loss = -10927.986098038305
1
Iteration 15200: Loss = -10927.970857618928
2
Iteration 15300: Loss = -10927.971541863888
3
Iteration 15400: Loss = -10927.948490780362
Iteration 15500: Loss = -10927.942574691158
Iteration 15600: Loss = -10927.93782265583
Iteration 15700: Loss = -10927.930075319811
Iteration 15800: Loss = -10927.92446575123
Iteration 15900: Loss = -10927.92345831621
Iteration 16000: Loss = -10927.923779441804
1
Iteration 16100: Loss = -10927.92588508947
2
Iteration 16200: Loss = -10927.921160941693
Iteration 16300: Loss = -10927.921474560635
1
Iteration 16400: Loss = -10927.919423913658
Iteration 16500: Loss = -10927.917676485313
Iteration 16600: Loss = -10927.915323643141
Iteration 16700: Loss = -10927.917860812733
1
Iteration 16800: Loss = -10927.912816298687
Iteration 16900: Loss = -10927.913950304259
1
Iteration 17000: Loss = -10927.910942263657
Iteration 17100: Loss = -10927.912847356178
1
Iteration 17200: Loss = -10927.90956480536
Iteration 17300: Loss = -10927.909192475903
Iteration 17400: Loss = -10927.908486313385
Iteration 17500: Loss = -10927.917615243505
1
Iteration 17600: Loss = -10927.931659145957
2
Iteration 17700: Loss = -10927.905900000595
Iteration 17800: Loss = -10927.906914953383
1
Iteration 17900: Loss = -10927.90694524114
2
Iteration 18000: Loss = -10927.905535893195
Iteration 18100: Loss = -10927.907552306126
1
Iteration 18200: Loss = -10927.905711539279
2
Iteration 18300: Loss = -10927.907621272916
3
Iteration 18400: Loss = -10927.93057633206
4
Iteration 18500: Loss = -10927.910914705279
5
Iteration 18600: Loss = -10927.908601482588
6
Iteration 18700: Loss = -10927.905255773534
Iteration 18800: Loss = -10927.914785468927
1
Iteration 18900: Loss = -10927.917457397894
2
Iteration 19000: Loss = -10927.930757441236
3
Iteration 19100: Loss = -10927.90583416909
4
Iteration 19200: Loss = -10927.906202509346
5
Iteration 19300: Loss = -10927.906186923776
6
Iteration 19400: Loss = -10927.90557951568
7
Iteration 19500: Loss = -10927.91139477287
8
Iteration 19600: Loss = -10927.942303350528
9
Iteration 19700: Loss = -10927.964558946564
10
Stopping early at iteration 19700 due to no improvement.
tensor([[ -7.6359,   6.2280],
        [ -9.5272,   7.9890],
        [ -7.6560,   6.1782],
        [ -7.7760,   5.9724],
        [ -7.9283,   5.4414],
        [ -7.6431,   6.0257],
        [ -8.1895,   5.9944],
        [ -8.0616,   6.0230],
        [ -7.3393,   5.8903],
        [ -7.7633,   6.1061],
        [ -7.4450,   5.7476],
        [ -9.6169,   7.4949],
        [ -7.8192,   5.3158],
        [ -7.4012,   5.7945],
        [ -9.1088,   7.7182],
        [ -7.6121,   6.1283],
        [ -7.9194,   6.1472],
        [ -7.7490,   6.0810],
        [ -8.5980,   5.2001],
        [ -9.3959,   4.7807],
        [ -7.4162,   6.0246],
        [ -7.5577,   6.1703],
        [ -8.8789,   6.6882],
        [ -7.6298,   6.0984],
        [ -7.5316,   6.1426],
        [ -7.5612,   6.0749],
        [ -8.2830,   5.3541],
        [ -7.4076,   5.9107],
        [ -7.7024,   6.2127],
        [ -7.5849,   6.0939],
        [ -8.7938,   4.3971],
        [ -8.3537,   5.1057],
        [ -8.0337,   5.7931],
        [ -7.6576,   6.0129],
        [ -8.0666,   5.7800],
        [ -7.7713,   5.8143],
        [ -7.7304,   6.0261],
        [ -8.3694,   6.9828],
        [-10.1532,   8.3484],
        [ -7.6032,   5.8240],
        [ -7.6686,   6.2489],
        [ -7.6575,   6.0759],
        [ -7.3328,   5.8556],
        [ -7.5797,   5.9707],
        [ -7.8958,   6.1746],
        [ -9.5687,   4.9545],
        [ -7.3812,   5.9938],
        [ -7.4663,   5.9525],
        [ -7.9218,   6.2297],
        [ -8.1011,   5.8376],
        [ -9.5932,   7.7677],
        [ -7.7755,   5.8514],
        [ -8.4833,   5.8616],
        [ -7.7038,   5.8095],
        [ -8.0131,   5.5287],
        [ -7.4734,   6.0753],
        [ -7.4725,   5.9306],
        [ -7.3420,   5.9039],
        [ -7.5473,   6.0830],
        [ -7.8854,   6.1136],
        [-10.1078,   8.5957],
        [ -7.7628,   5.8716],
        [ -7.4123,   5.8023],
        [ -8.1823,   5.8344],
        [ -9.6734,   7.2035],
        [ -8.2712,   4.9401],
        [ -8.2237,   5.5990],
        [ -7.7053,   5.8928],
        [ -7.8576,   5.9862],
        [ -7.5086,   5.8298],
        [ -7.4912,   5.9822],
        [ -7.3565,   5.9613],
        [ -8.0311,   6.4966],
        [ -7.8676,   5.9087],
        [ -7.4800,   6.0662],
        [ -7.8867,   5.6693],
        [ -8.2808,   6.7491],
        [ -7.4985,   5.8278],
        [ -8.1780,   5.1146],
        [ -7.8119,   6.2864],
        [ -8.4066,   5.7439],
        [ -9.0054,   5.4522],
        [ -7.5155,   6.0694],
        [ -7.7491,   6.3468],
        [ -8.1539,   5.3313],
        [ -9.6518,   8.1001],
        [ -7.8006,   6.3994],
        [ -7.5822,   6.1878],
        [ -8.2485,   4.9386],
        [ -7.3458,   5.9549],
        [ -7.8947,   5.5823],
        [ -7.4555,   5.7077],
        [ -7.8663,   5.6903],
        [ -7.5038,   5.9974],
        [ -8.9705,   7.5163],
        [ -7.6466,   6.2595],
        [ -9.8524,   8.4660],
        [ -7.2917,   5.8546],
        [ -7.7097,   6.1993],
        [ -7.5734,   6.1300]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9976, 0.0024],
        [0.9709, 0.0291]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0719e-06, 1.0000e+00], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1612, 0.1680],
         [0.8548, 0.1555]],

        [[0.4761, 0.2492],
         [0.1049, 0.2494]],

        [[0.1361, 0.2329],
         [0.8469, 0.5108]],

        [[0.7491, 0.1085],
         [0.5035, 0.0994]],

        [[0.8162, 0.1421],
         [0.6700, 0.8142]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 38
Adjusted Rand Index: -0.008233622622776837
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0021054296496262255
Average Adjusted Rand Index: -0.0016467245245553673
10833.238846087863
new:  [0.0025027289519074284, -0.0029139835017840316, -0.00045868142037170875, -0.0021054296496262255] [0.002878528000697158, -0.005010292448297414, -0.00043460331243415503, -0.0016467245245553673] [10925.629346226655, 10925.158651301888, 10927.383038280781, 10927.964558946564]
prior:  [-0.0005549176189843577, -0.004936296329625465, 0.11932350744033172, -0.004936296329625465] [-0.002235795778453592, -0.008543252070615024, 0.13968265047396866, -0.008543252070615024] [10926.302917655086, 10926.222910886523, 10926.239359434821, 10926.222811842266]
-----------------------------------------------------------------------------------------
This iteration is 7
True Objective function: Loss = -10809.327774846859
Iteration 0: Loss = -17405.33384092193
Iteration 10: Loss = -10926.441802996891
Iteration 20: Loss = -10926.045524175526
Iteration 30: Loss = -10925.480791137923
Iteration 40: Loss = -10925.422981664782
Iteration 50: Loss = -10925.400981120914
Iteration 60: Loss = -10925.399859740897
Iteration 70: Loss = -10925.401051099958
1
Iteration 80: Loss = -10925.401824210892
2
Iteration 90: Loss = -10925.402209640648
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.2359, 0.7641],
        [0.0150, 0.9850]], dtype=torch.float64)
alpha: tensor([0.0192, 0.9808])
beta: tensor([[[0.2076, 0.2213],
         [0.8092, 0.1588]],

        [[0.3874, 0.2141],
         [0.9669, 0.4088]],

        [[0.9810, 0.1044],
         [0.2194, 0.0694]],

        [[0.4387, 0.2153],
         [0.5429, 0.7707]],

        [[0.4113, 0.1127],
         [0.0897, 0.4389]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17605.53007700798
Iteration 100: Loss = -10931.789729440758
Iteration 200: Loss = -10928.799340147356
Iteration 300: Loss = -10927.787927945667
Iteration 400: Loss = -10927.284781160439
Iteration 500: Loss = -10926.9912464965
Iteration 600: Loss = -10926.816952845336
Iteration 700: Loss = -10926.724753124246
Iteration 800: Loss = -10926.674357022117
Iteration 900: Loss = -10926.64457907728
Iteration 1000: Loss = -10926.62477053819
Iteration 1100: Loss = -10926.609813513822
Iteration 1200: Loss = -10926.597571622298
Iteration 1300: Loss = -10926.586991016507
Iteration 1400: Loss = -10926.577461798845
Iteration 1500: Loss = -10926.568527417996
Iteration 1600: Loss = -10926.559788705508
Iteration 1700: Loss = -10926.550520706272
Iteration 1800: Loss = -10926.53950142072
Iteration 1900: Loss = -10926.523365607422
Iteration 2000: Loss = -10926.491022731227
Iteration 2100: Loss = -10926.432031726637
Iteration 2200: Loss = -10926.401615879464
Iteration 2300: Loss = -10926.38767003302
Iteration 2400: Loss = -10926.379038083396
Iteration 2500: Loss = -10926.372412340867
Iteration 2600: Loss = -10926.36677094537
Iteration 2700: Loss = -10926.361683000165
Iteration 2800: Loss = -10926.356959398965
Iteration 2900: Loss = -10926.352321833745
Iteration 3000: Loss = -10926.34752291855
Iteration 3100: Loss = -10926.34202243346
Iteration 3200: Loss = -10926.334480382455
Iteration 3300: Loss = -10926.31899309905
Iteration 3400: Loss = -10926.06040799611
Iteration 3500: Loss = -10925.17099629411
Iteration 3600: Loss = -10925.086630440741
Iteration 3700: Loss = -10925.051794790023
Iteration 3800: Loss = -10925.033475956505
Iteration 3900: Loss = -10925.01808339358
Iteration 4000: Loss = -10924.999077888966
Iteration 4100: Loss = -10924.642783478132
Iteration 4200: Loss = -10921.566246833521
Iteration 4300: Loss = -10921.53955209348
Iteration 4400: Loss = -10921.501147431662
Iteration 4500: Loss = -10921.479055119455
Iteration 4600: Loss = -10921.038666152912
Iteration 4700: Loss = -10918.88609224964
Iteration 4800: Loss = -10918.854826378029
Iteration 4900: Loss = -10918.839600903784
Iteration 5000: Loss = -10918.836528194255
Iteration 5100: Loss = -10918.834367485106
Iteration 5200: Loss = -10918.832929233653
Iteration 5300: Loss = -10918.831965949203
Iteration 5400: Loss = -10918.83227909718
1
Iteration 5500: Loss = -10918.830723669445
Iteration 5600: Loss = -10918.82932775759
Iteration 5700: Loss = -10918.842150571823
1
Iteration 5800: Loss = -10918.828099793902
Iteration 5900: Loss = -10918.827605524179
Iteration 6000: Loss = -10918.827413874169
Iteration 6100: Loss = -10918.826606001046
Iteration 6200: Loss = -10918.8262392157
Iteration 6300: Loss = -10918.826112408633
Iteration 6400: Loss = -10918.82566390199
Iteration 6500: Loss = -10918.825362334412
Iteration 6600: Loss = -10918.82517550398
Iteration 6700: Loss = -10918.82483313732
Iteration 6800: Loss = -10918.824575368608
Iteration 6900: Loss = -10918.829269661637
1
Iteration 7000: Loss = -10918.824212245132
Iteration 7100: Loss = -10918.824011603385
Iteration 7200: Loss = -10918.824283806356
1
Iteration 7300: Loss = -10918.823698327675
Iteration 7400: Loss = -10918.823585436729
Iteration 7500: Loss = -10918.82349084333
Iteration 7600: Loss = -10918.824782497313
1
Iteration 7700: Loss = -10918.825558590459
2
Iteration 7800: Loss = -10918.877855078948
3
Iteration 7900: Loss = -10918.822977334492
Iteration 8000: Loss = -10918.827848000132
1
Iteration 8100: Loss = -10918.822719461074
Iteration 8200: Loss = -10918.822636108749
Iteration 8300: Loss = -10918.822740046033
1
Iteration 8400: Loss = -10918.822566950637
Iteration 8500: Loss = -10918.822402130543
Iteration 8600: Loss = -10918.82261445427
1
Iteration 8700: Loss = -10918.826784037505
2
Iteration 8800: Loss = -10918.823227643637
3
Iteration 8900: Loss = -10918.822127214109
Iteration 9000: Loss = -10918.822071876237
Iteration 9100: Loss = -10918.821937585459
Iteration 9200: Loss = -10918.821916145414
Iteration 9300: Loss = -10918.821906847195
Iteration 9400: Loss = -10918.833465227925
1
Iteration 9500: Loss = -10918.821727269957
Iteration 9600: Loss = -10918.821971897523
1
Iteration 9700: Loss = -10918.821681020385
Iteration 9800: Loss = -10918.821611710864
Iteration 9900: Loss = -10918.821759422422
1
Iteration 10000: Loss = -10918.821545978431
Iteration 10100: Loss = -10918.823374591642
1
Iteration 10200: Loss = -10918.822886008664
2
Iteration 10300: Loss = -10918.821446880755
Iteration 10400: Loss = -10918.82292751519
1
Iteration 10500: Loss = -10918.821389023526
Iteration 10600: Loss = -10918.830063734322
1
Iteration 10700: Loss = -10918.821367581739
Iteration 10800: Loss = -10918.826202561739
1
Iteration 10900: Loss = -10918.821311203
Iteration 11000: Loss = -10918.825160684646
1
Iteration 11100: Loss = -10918.821232179249
Iteration 11200: Loss = -10918.82405640819
1
Iteration 11300: Loss = -10918.82124841311
2
Iteration 11400: Loss = -10918.82137487723
3
Iteration 11500: Loss = -10918.826762045577
4
Iteration 11600: Loss = -10918.821485366947
5
Iteration 11700: Loss = -10918.83168306704
6
Iteration 11800: Loss = -10918.821100653044
Iteration 11900: Loss = -10918.824325099411
1
Iteration 12000: Loss = -10918.821098350309
Iteration 12100: Loss = -10918.872582432661
1
Iteration 12200: Loss = -10918.82107158052
Iteration 12300: Loss = -10918.821034839997
Iteration 12400: Loss = -10918.82142596375
1
Iteration 12500: Loss = -10918.821031874326
Iteration 12600: Loss = -10918.876127712292
1
Iteration 12700: Loss = -10918.820981183244
Iteration 12800: Loss = -10918.82224027653
1
Iteration 12900: Loss = -10918.82113030803
2
Iteration 13000: Loss = -10918.825213189659
3
Iteration 13100: Loss = -10918.821028846667
4
Iteration 13200: Loss = -10918.863516147761
5
Iteration 13300: Loss = -10918.821174334416
6
Iteration 13400: Loss = -10918.876735653079
7
Iteration 13500: Loss = -10918.821038097052
8
Iteration 13600: Loss = -10918.836804107263
9
Iteration 13700: Loss = -10918.821043150574
10
Stopping early at iteration 13700 due to no improvement.
tensor([[-3.7054, -0.9099],
        [-3.6622, -0.9530],
        [-3.0410, -1.5742],
        [-3.2938, -1.3214],
        [-3.5416, -1.0736],
        [-3.5458, -1.0695],
        [-3.6549, -0.9603],
        [-3.5630, -1.0522],
        [-3.5951, -1.0201],
        [-3.3511, -1.2642],
        [-3.6966, -0.9186],
        [-3.6330, -0.9822],
        [-3.6551, -0.9602],
        [-3.6651, -0.9501],
        [-3.3356, -1.2796],
        [-3.6936, -0.9216],
        [-3.3390, -1.2762],
        [-3.5639, -1.0513],
        [-1.4394, -3.1759],
        [-3.6814, -0.9338],
        [-3.6210, -0.9942],
        [-3.6892, -0.9260],
        [-3.2741, -1.3412],
        [-2.0785, -2.5367],
        [-3.7645, -0.8507],
        [-3.0795, -1.5357],
        [-3.4985, -1.1167],
        [-2.9866, -1.6287],
        [-3.6603, -0.9550],
        [-3.0449, -1.5703],
        [-0.5979, -4.0173],
        [-3.3070, -1.3082],
        [-3.5940, -1.0213],
        [-3.6434, -0.9718],
        [-3.1009, -1.5143],
        [-3.6026, -1.0126],
        [-3.5229, -1.0923],
        [-3.6410, -0.9742],
        [-3.7695, -0.8457],
        [-3.7739, -0.8413],
        [-3.6857, -0.9295],
        [-3.6834, -0.9318],
        [-3.3572, -1.2580],
        [-3.7292, -0.8861],
        [-3.6926, -0.9226],
        [-3.6664, -0.9488],
        [-3.5633, -1.0519],
        [-3.5660, -1.0492],
        [-3.0044, -1.6108],
        [-3.4989, -1.1163],
        [-3.7265, -0.8887],
        [-3.5725, -1.0427],
        [-3.6410, -0.9742],
        [-3.3733, -1.2419],
        [-3.3408, -1.2744],
        [-3.5955, -1.0197],
        [-3.7003, -0.9150],
        [-3.5235, -1.0917],
        [-3.4264, -1.1888],
        [-3.3833, -1.2319],
        [-3.5047, -1.1105],
        [-3.1698, -1.4454],
        [-3.6584, -0.9569],
        [-1.9762, -2.6390],
        [-3.3209, -1.2943],
        [-3.5335, -1.0817],
        [-3.6274, -0.9878],
        [-3.6077, -1.0075],
        [-3.5870, -1.0282],
        [-3.7156, -0.8996],
        [-3.3288, -1.2865],
        [-3.6309, -0.9844],
        [-3.3618, -1.2534],
        [-3.6213, -0.9939],
        [-3.6081, -1.0072],
        [-3.6711, -0.9441],
        [-2.6359, -1.9793],
        [-3.6320, -0.9833],
        [-1.6004, -3.0149],
        [-3.5640, -1.0512],
        [-3.7437, -0.8716],
        [-3.6572, -0.9580],
        [-3.6201, -0.9951],
        [-3.3759, -1.2394],
        [-3.6149, -1.0003],
        [-3.7064, -0.9088],
        [-3.6157, -0.9995],
        [-3.2881, -1.3271],
        [-3.6373, -0.9780],
        [-3.3866, -1.2286],
        [-3.6595, -0.9557],
        [-3.6724, -0.9428],
        [-3.6586, -0.9566],
        [-3.5047, -1.1105],
        [-3.5284, -1.0869],
        [-3.6843, -0.9310],
        [-3.6741, -0.9412],
        [-3.5652, -1.0501],
        [ 1.3694, -5.9846],
        [-3.6527, -0.9625]], dtype=torch.float64, requires_grad=True)
pi: tensor([[6.1819e-01, 3.8181e-01],
        [7.5765e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1304, 0.8696], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1634, 0.1582],
         [0.8092, 0.1611]],

        [[0.3874, 0.2051],
         [0.9669, 0.4088]],

        [[0.9810, 0.0833],
         [0.2194, 0.0694]],

        [[0.4387, 0.2498],
         [0.5429, 0.7707]],

        [[0.4113, 0.0944],
         [0.0897, 0.4389]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.0019209323236274677
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.0006803382252891437
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.004166919759460609
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
Global Adjusted Rand Index: -0.0003346061575015009
Average Adjusted Rand Index: -0.0014856235194590558
Iteration 0: Loss = -17389.62547141252
Iteration 10: Loss = -10926.107397287602
Iteration 20: Loss = -10926.008138975436
Iteration 30: Loss = -10925.93118513181
Iteration 40: Loss = -10925.854158381073
Iteration 50: Loss = -10925.808812930141
Iteration 60: Loss = -10925.779667864366
Iteration 70: Loss = -10925.755254657775
Iteration 80: Loss = -10925.734168281255
Iteration 90: Loss = -10925.716226903614
Iteration 100: Loss = -10925.700723265629
Iteration 110: Loss = -10925.687342945184
Iteration 120: Loss = -10925.675423239514
Iteration 130: Loss = -10925.664726814599
Iteration 140: Loss = -10925.654907317055
Iteration 150: Loss = -10925.64584473345
Iteration 160: Loss = -10925.637390555474
Iteration 170: Loss = -10925.629383854493
Iteration 180: Loss = -10925.621776651622
Iteration 190: Loss = -10925.614527706317
Iteration 200: Loss = -10925.607470870442
Iteration 210: Loss = -10925.600616292128
Iteration 220: Loss = -10925.59398572639
Iteration 230: Loss = -10925.587479201638
Iteration 240: Loss = -10925.581067561847
Iteration 250: Loss = -10925.574789299859
Iteration 260: Loss = -10925.568518350594
Iteration 270: Loss = -10925.56231167499
Iteration 280: Loss = -10925.556147288058
Iteration 290: Loss = -10925.55002101916
Iteration 300: Loss = -10925.54381037401
Iteration 310: Loss = -10925.537683648421
Iteration 320: Loss = -10925.531502361653
Iteration 330: Loss = -10925.52527532478
Iteration 340: Loss = -10925.519055539016
Iteration 350: Loss = -10925.512755480506
Iteration 360: Loss = -10925.506372090505
Iteration 370: Loss = -10925.500011272537
Iteration 380: Loss = -10925.493512252111
Iteration 390: Loss = -10925.486940266126
Iteration 400: Loss = -10925.480328750085
Iteration 410: Loss = -10925.473646155211
Iteration 420: Loss = -10925.4667819333
Iteration 430: Loss = -10925.459932127647
Iteration 440: Loss = -10925.452861375088
Iteration 450: Loss = -10925.4457641154
Iteration 460: Loss = -10925.43852636732
Iteration 470: Loss = -10925.431202038797
Iteration 480: Loss = -10925.423751940369
Iteration 490: Loss = -10925.416157885835
Iteration 500: Loss = -10925.408436512713
Iteration 510: Loss = -10925.400618541518
Iteration 520: Loss = -10925.392598234326
Iteration 530: Loss = -10925.384496044388
Iteration 540: Loss = -10925.37618028852
Iteration 550: Loss = -10925.367790375816
Iteration 560: Loss = -10925.359182495251
Iteration 570: Loss = -10925.350443129646
Iteration 580: Loss = -10925.34149439329
Iteration 590: Loss = -10925.332465449928
Iteration 600: Loss = -10925.323173987652
Iteration 610: Loss = -10925.313757251224
Iteration 620: Loss = -10925.304157008457
Iteration 630: Loss = -10925.29440576084
Iteration 640: Loss = -10925.284384988661
Iteration 650: Loss = -10925.274192864474
Iteration 660: Loss = -10925.263792157504
Iteration 670: Loss = -10925.25320809078
Iteration 680: Loss = -10925.242475268053
Iteration 690: Loss = -10925.231469754179
Iteration 700: Loss = -10925.220232533853
Iteration 710: Loss = -10925.208826730539
Iteration 720: Loss = -10925.19711619194
Iteration 730: Loss = -10925.185178701224
Iteration 740: Loss = -10925.173021650086
Iteration 750: Loss = -10925.160605777457
Iteration 760: Loss = -10925.147912180186
Iteration 770: Loss = -10925.135009005944
Iteration 780: Loss = -10925.121802718264
Iteration 790: Loss = -10925.108352897669
Iteration 800: Loss = -10925.094612603785
Iteration 810: Loss = -10925.08055131952
Iteration 820: Loss = -10925.066181350334
Iteration 830: Loss = -10925.05152028916
Iteration 840: Loss = -10925.03650863519
Iteration 850: Loss = -10925.021152279396
Iteration 860: Loss = -10925.005519188146
Iteration 870: Loss = -10924.989411029428
Iteration 880: Loss = -10924.973025005882
Iteration 890: Loss = -10924.956219509353
Iteration 900: Loss = -10924.939051467436
Iteration 910: Loss = -10924.921401305051
Iteration 920: Loss = -10924.9033315556
Iteration 930: Loss = -10924.88478751166
Iteration 940: Loss = -10924.865786165972
Iteration 950: Loss = -10924.846261150687
Iteration 960: Loss = -10924.826221220568
Iteration 970: Loss = -10924.805643105401
Iteration 980: Loss = -10924.784381789825
Iteration 990: Loss = -10924.762584879678
Iteration 1000: Loss = -10924.740034327959
Iteration 1010: Loss = -10924.716819703392
Iteration 1020: Loss = -10924.692825988202
Iteration 1030: Loss = -10924.667976518327
Iteration 1040: Loss = -10924.642305207064
Iteration 1050: Loss = -10924.615572605548
Iteration 1060: Loss = -10924.587794205227
Iteration 1070: Loss = -10924.558918008648
Iteration 1080: Loss = -10924.528660040967
Iteration 1090: Loss = -10924.496994327224
Iteration 1100: Loss = -10924.463789514983
Iteration 1110: Loss = -10924.428634457458
Iteration 1120: Loss = -10924.3914327019
Iteration 1130: Loss = -10924.35183377432
Iteration 1140: Loss = -10924.309352991564
Iteration 1150: Loss = -10924.263484607465
Iteration 1160: Loss = -10924.213544553399
Iteration 1170: Loss = -10924.15844146962
Iteration 1180: Loss = -10924.096916496112
Iteration 1190: Loss = -10924.02685454745
Iteration 1200: Loss = -10923.945234692068
Iteration 1210: Loss = -10923.847160943293
Iteration 1220: Loss = -10923.724009897318
Iteration 1230: Loss = -10923.56010178521
Iteration 1240: Loss = -10923.32450720011
Iteration 1250: Loss = -10922.959691427142
Iteration 1260: Loss = -10922.415479645582
Iteration 1270: Loss = -10921.890086697169
Iteration 1280: Loss = -10921.725212873494
Iteration 1290: Loss = -10921.74226234736
1
Iteration 1300: Loss = -10921.774682739675
2
Iteration 1310: Loss = -10921.795177391043
3
Stopping early at iteration 1309 due to no improvement.
pi: tensor([[0.9379, 0.0621],
        [0.9723, 0.0277]], dtype=torch.float64)
alpha: tensor([0.9391, 0.0609])
beta: tensor([[[0.1662, 0.1144],
         [0.6517, 0.0929]],

        [[0.6620, 0.0777],
         [0.4904, 0.6718]],

        [[0.9965, 0.1046],
         [0.9016, 0.6366]],

        [[0.9190, 0.1304],
         [0.3091, 0.0098]],

        [[0.4356, 0.1172],
         [0.1950, 0.3068]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.00485601903559462
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0002897541819023893
Average Adjusted Rand Index: 0.0005064848866166887
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17389.59829422907
Iteration 100: Loss = -10938.032310780034
Iteration 200: Loss = -10928.194501486461
Iteration 300: Loss = -10926.960769748366
Iteration 400: Loss = -10926.581034986151
Iteration 500: Loss = -10926.41785733658
Iteration 600: Loss = -10926.29184451862
Iteration 700: Loss = -10926.152153961079
Iteration 800: Loss = -10925.939613803524
Iteration 900: Loss = -10924.551638623547
Iteration 1000: Loss = -10922.976654027469
Iteration 1100: Loss = -10921.966642728752
Iteration 1200: Loss = -10920.926608702048
Iteration 1300: Loss = -10920.202378804994
Iteration 1400: Loss = -10919.764494218001
Iteration 1500: Loss = -10919.48198819725
Iteration 1600: Loss = -10919.345949099054
Iteration 1700: Loss = -10919.271280360692
Iteration 1800: Loss = -10919.209394599837
Iteration 1900: Loss = -10919.050377094245
Iteration 2000: Loss = -10919.006678817354
Iteration 2100: Loss = -10918.982869836307
Iteration 2200: Loss = -10918.964678978624
Iteration 2300: Loss = -10918.949660054117
Iteration 2400: Loss = -10918.936915225098
Iteration 2500: Loss = -10918.92595403617
Iteration 2600: Loss = -10918.916412337423
Iteration 2700: Loss = -10918.90805033081
Iteration 2800: Loss = -10918.900680344686
Iteration 2900: Loss = -10918.894219978194
Iteration 3000: Loss = -10918.888467907087
Iteration 3100: Loss = -10918.883340125967
Iteration 3200: Loss = -10918.878736813307
Iteration 3300: Loss = -10918.874581298218
Iteration 3400: Loss = -10918.870828776404
Iteration 3500: Loss = -10918.867406910407
Iteration 3600: Loss = -10918.86427711634
Iteration 3700: Loss = -10918.861415904163
Iteration 3800: Loss = -10918.85941245252
Iteration 3900: Loss = -10918.856364482717
Iteration 4000: Loss = -10918.85412561867
Iteration 4100: Loss = -10918.852219103506
Iteration 4200: Loss = -10918.96532919723
1
Iteration 4300: Loss = -10918.84834578407
Iteration 4400: Loss = -10918.846693194691
Iteration 4500: Loss = -10918.84536395494
Iteration 4600: Loss = -10918.84372214511
Iteration 4700: Loss = -10918.842497096844
Iteration 4800: Loss = -10918.84117556882
Iteration 4900: Loss = -10918.8399645772
Iteration 5000: Loss = -10918.83883648074
Iteration 5100: Loss = -10918.83824598378
Iteration 5200: Loss = -10918.836844300817
Iteration 5300: Loss = -10918.83595677921
Iteration 5400: Loss = -10918.90864902349
1
Iteration 5500: Loss = -10918.83418212967
Iteration 5600: Loss = -10918.833398632758
Iteration 5700: Loss = -10918.833610915277
1
Iteration 5800: Loss = -10918.832008545463
Iteration 5900: Loss = -10918.831336647736
Iteration 6000: Loss = -10918.830740214069
Iteration 6100: Loss = -10918.830376190539
Iteration 6200: Loss = -10918.82967007939
Iteration 6300: Loss = -10918.829213815881
Iteration 6400: Loss = -10918.828784587922
Iteration 6500: Loss = -10918.828354550295
Iteration 6600: Loss = -10918.827969932256
Iteration 6700: Loss = -10918.851813470348
1
Iteration 6800: Loss = -10918.827233889808
Iteration 6900: Loss = -10918.826924410494
Iteration 7000: Loss = -10918.827472846011
1
Iteration 7100: Loss = -10918.826322903331
Iteration 7200: Loss = -10918.826041808816
Iteration 7300: Loss = -10918.826215799922
1
Iteration 7400: Loss = -10918.825613292343
Iteration 7500: Loss = -10918.825250621452
Iteration 7600: Loss = -10918.825051918753
Iteration 7700: Loss = -10918.825733565553
1
Iteration 7800: Loss = -10918.87810023554
2
Iteration 7900: Loss = -10918.82446778392
Iteration 8000: Loss = -10918.830866708362
1
Iteration 8100: Loss = -10918.824058314494
Iteration 8200: Loss = -10918.826977069588
1
Iteration 8300: Loss = -10918.82376361144
Iteration 8400: Loss = -10918.914831424443
1
Iteration 8500: Loss = -10918.823496439038
Iteration 8600: Loss = -10918.823335364556
Iteration 8700: Loss = -10918.837842287387
1
Iteration 8800: Loss = -10918.823111434072
Iteration 8900: Loss = -10918.83071230845
1
Iteration 9000: Loss = -10918.822909862534
Iteration 9100: Loss = -10918.822779588187
Iteration 9200: Loss = -10918.82361198991
1
Iteration 9300: Loss = -10918.82766420597
2
Iteration 9400: Loss = -10918.822489831395
Iteration 9500: Loss = -10918.82246912253
Iteration 9600: Loss = -10918.822340753884
Iteration 9700: Loss = -10918.822625532985
1
Iteration 9800: Loss = -10918.82217395274
Iteration 9900: Loss = -10918.82304474842
1
Iteration 10000: Loss = -10918.822091839746
Iteration 10100: Loss = -10918.822643407038
1
Iteration 10200: Loss = -10918.822037153763
Iteration 10300: Loss = -10918.836847806753
1
Iteration 10400: Loss = -10918.821879616144
Iteration 10500: Loss = -10918.885418565997
1
Iteration 10600: Loss = -10918.82174618783
Iteration 10700: Loss = -10918.868957396646
1
Iteration 10800: Loss = -10918.821708805695
Iteration 10900: Loss = -10918.821668038923
Iteration 11000: Loss = -10918.821728984423
1
Iteration 11100: Loss = -10918.821558313524
Iteration 11200: Loss = -10918.82175071741
1
Iteration 11300: Loss = -10918.82146957277
Iteration 11400: Loss = -10918.828757595538
1
Iteration 11500: Loss = -10918.821855554657
2
Iteration 11600: Loss = -10918.821575485947
3
Iteration 11700: Loss = -10918.821488629601
4
Iteration 11800: Loss = -10918.842843808401
5
Iteration 11900: Loss = -10918.821442669669
Iteration 12000: Loss = -10918.82159931673
1
Iteration 12100: Loss = -10918.851258498697
2
Iteration 12200: Loss = -10918.821286858614
Iteration 12300: Loss = -10918.82231795302
1
Iteration 12400: Loss = -10918.824115873686
2
Iteration 12500: Loss = -10918.821236897025
Iteration 12600: Loss = -10918.842670176273
1
Iteration 12700: Loss = -10918.821249255265
2
Iteration 12800: Loss = -10918.821478807413
3
Iteration 12900: Loss = -10918.821331491406
4
Iteration 13000: Loss = -10918.821157880537
Iteration 13100: Loss = -10918.824352482516
1
Iteration 13200: Loss = -10918.821154225652
Iteration 13300: Loss = -10918.821126730036
Iteration 13400: Loss = -10918.821634484988
1
Iteration 13500: Loss = -10918.82106495192
Iteration 13600: Loss = -10918.821221233691
1
Iteration 13700: Loss = -10918.830198994227
2
Iteration 13800: Loss = -10918.821135523025
3
Iteration 13900: Loss = -10918.82289417555
4
Iteration 14000: Loss = -10918.838768156795
5
Iteration 14100: Loss = -10918.821288206376
6
Iteration 14200: Loss = -10918.845209044443
7
Iteration 14300: Loss = -10918.821936666363
8
Iteration 14400: Loss = -10918.821120439465
9
Iteration 14500: Loss = -10918.857080306636
10
Stopping early at iteration 14500 due to no improvement.
tensor([[-3.5753, -0.7680],
        [-2.0704,  0.6461],
        [-1.6335, -0.1751],
        [-1.9352,  0.0487],
        [-2.0614,  0.4054],
        [-2.1156,  0.3723],
        [-2.5223,  0.1821],
        [-3.5666, -1.0486],
        [-1.9762,  0.5896],
        [-1.7657,  0.3114],
        [-2.5764,  0.2057],
        [-2.4345,  0.2198],
        [-3.2768, -0.5916],
        [-2.1109,  0.5954],
        [-2.1242, -0.0569],
        [-2.2273,  0.5357],
        [-1.7281,  0.3261],
        [-1.9799,  0.5232],
        [-0.1411, -1.8666],
        [-2.1688,  0.5742],
        [-2.4674,  0.1667],
        [-2.0917,  0.6679],
        [-1.6705,  0.2551],
        [-0.4600, -0.9266],
        [-2.3943,  0.5305],
        [-2.0145, -0.4801],
        [-2.1837,  0.1936],
        [-1.7652, -0.3955],
        [-2.2729,  0.4235],
        [-1.8154, -0.3302],
        [ 0.5958, -2.8356],
        [-1.8902,  0.0990],
        [-2.3091,  0.2575],
        [-2.0860,  0.5959],
        [-1.4914,  0.1048],
        [-2.1925,  0.3891],
        [-2.0022,  0.4225],
        [-2.2694,  0.3890],
        [-2.4587,  0.4769],
        [-2.1904,  0.7539],
        [-2.4219,  0.3440],
        [-2.3145,  0.4438],
        [-1.7491,  0.3543],
        [-2.1910,  0.6636],
        [-2.5804,  0.1833],
        [-2.5506,  0.1640],
        [-2.4181,  0.1055],
        [-2.1403,  0.3818],
        [-1.7022, -0.2978],
        [-1.9563,  0.4176],
        [-2.1098,  0.7235],
        [-3.5677, -1.0475],
        [-2.0688,  0.5997],
        [-1.7703,  0.3730],
        [-2.0563,  0.0203],
        [-2.2333,  0.3330],
        [-2.1180,  0.6753],
        [-2.0102,  0.4140],
        [-1.9670,  0.2813],
        [-2.2013, -0.0595],
        [-1.9163,  0.4791],
        [-1.6274,  0.1067],
        [-2.8570, -0.1459],
        [-1.3827, -2.0352],
        [-2.5379, -0.5208],
        [-1.9458,  0.4965],
        [-2.1382,  0.5062],
        [-2.0430,  0.5678],
        [-2.4013,  0.1505],
        [-2.7917,  0.0358],
        [-1.9222,  0.1110],
        [-2.1183,  0.5268],
        [-2.2919, -0.1745],
        [-3.2210, -0.6031],
        [-2.0515,  0.5402],
        [-2.3213,  0.4078],
        [-2.1863, -1.5211],
        [-2.0579,  0.5982],
        [-0.0392, -1.4634],
        [-3.3381, -0.8184],
        [-2.8322,  0.0417],
        [-2.0915,  0.5986],
        [-2.3585,  0.2588],
        [-1.9579,  0.1865],
        [-2.9250, -0.3194],
        [-2.4515,  0.3450],
        [-2.7338, -0.1272],
        [-1.7379,  0.2142],
        [-2.0575,  0.5939],
        [-2.3986, -0.2287],
        [-2.0613,  0.6519],
        [-2.7280, -0.0052],
        [-2.1121,  0.5815],
        [-3.5000, -1.1152],
        [-1.9234,  0.5088],
        [-2.0876,  0.6646],
        [-2.7964, -0.0534],
        [-2.0021,  0.5182],
        [ 2.7636, -4.5943],
        [-2.0349,  0.6486]], dtype=torch.float64, requires_grad=True)
pi: tensor([[6.1823e-01, 3.8177e-01],
        [4.6811e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1302, 0.8698], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1635, 0.1589],
         [0.6517, 0.1603]],

        [[0.6620, 0.2059],
         [0.4904, 0.6718]],

        [[0.9965, 0.0833],
         [0.9016, 0.6366]],

        [[0.9190, 0.2496],
         [0.3091, 0.0098]],

        [[0.4356, 0.0945],
         [0.1950, 0.3068]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.0019209323236274677
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.0006803382252891437
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.004166919759460609
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
Global Adjusted Rand Index: -0.0003346061575015009
Average Adjusted Rand Index: -0.0014856235194590558
Iteration 0: Loss = -19758.625041543368
Iteration 10: Loss = -10926.731174071612
Iteration 20: Loss = -10926.731132641586
Iteration 30: Loss = -10926.72985881258
Iteration 40: Loss = -10926.709701518912
Iteration 50: Loss = -10926.589664752402
Iteration 60: Loss = -10926.121081966096
Iteration 70: Loss = -10925.614521119232
Iteration 80: Loss = -10925.4541236548
Iteration 90: Loss = -10925.417064598541
Iteration 100: Loss = -10925.407174545398
Iteration 110: Loss = -10925.4040916914
Iteration 120: Loss = -10925.403014686464
Iteration 130: Loss = -10925.402653702224
Iteration 140: Loss = -10925.402490539649
Iteration 150: Loss = -10925.402438242243
Iteration 160: Loss = -10925.402429400914
Iteration 170: Loss = -10925.402422350751
Iteration 180: Loss = -10925.402427818079
1
Iteration 190: Loss = -10925.402423160616
2
Iteration 200: Loss = -10925.402429379477
3
Stopping early at iteration 199 due to no improvement.
pi: tensor([[0.9850, 0.0150],
        [0.7640, 0.2360]], dtype=torch.float64)
alpha: tensor([0.9809, 0.0191])
beta: tensor([[[0.1588, 0.2214],
         [0.7796, 0.2076]],

        [[0.2006, 0.2141],
         [0.5843, 0.5094]],

        [[0.1233, 0.1044],
         [0.9323, 0.3327]],

        [[0.5137, 0.2153],
         [0.2407, 0.7198]],

        [[0.9859, 0.1126],
         [0.5065, 0.9005]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19758.2636906244
Iteration 100: Loss = -10928.652076098988
Iteration 200: Loss = -10927.171420240777
Iteration 300: Loss = -10926.760125339268
Iteration 400: Loss = -10926.495126251708
Iteration 500: Loss = -10926.253060472929
Iteration 600: Loss = -10926.016688086462
Iteration 700: Loss = -10925.767702210918
Iteration 800: Loss = -10925.587470498105
Iteration 900: Loss = -10925.337006661219
Iteration 1000: Loss = -10924.772876462537
Iteration 1100: Loss = -10922.232623489059
Iteration 1200: Loss = -10921.669040769064
Iteration 1300: Loss = -10921.443576474407
Iteration 1400: Loss = -10921.323609617328
Iteration 1500: Loss = -10921.252117426318
Iteration 1600: Loss = -10921.204380716468
Iteration 1700: Loss = -10920.964787660816
Iteration 1800: Loss = -10920.92122911301
Iteration 1900: Loss = -10920.898748167863
Iteration 2000: Loss = -10920.885886560896
Iteration 2100: Loss = -10920.878138821041
Iteration 2200: Loss = -10920.872004321634
Iteration 2300: Loss = -10920.866132447127
Iteration 2400: Loss = -10920.85445014817
Iteration 2500: Loss = -10920.793705077736
Iteration 2600: Loss = -10920.788821080183
Iteration 2700: Loss = -10920.784909203525
Iteration 2800: Loss = -10920.78054186755
Iteration 2900: Loss = -10920.77628609218
Iteration 3000: Loss = -10920.774390646995
Iteration 3100: Loss = -10920.772957370385
Iteration 3200: Loss = -10920.771929914394
Iteration 3300: Loss = -10920.771140700084
Iteration 3400: Loss = -10920.77047782795
Iteration 3500: Loss = -10920.769894906774
Iteration 3600: Loss = -10920.769384008834
Iteration 3700: Loss = -10920.768912357475
Iteration 3800: Loss = -10920.76847235041
Iteration 3900: Loss = -10920.768050252047
Iteration 4000: Loss = -10920.767583728248
Iteration 4100: Loss = -10920.767150155469
Iteration 4200: Loss = -10920.766740415755
Iteration 4300: Loss = -10920.766417070385
Iteration 4400: Loss = -10920.766166939644
Iteration 4500: Loss = -10920.765981897597
Iteration 4600: Loss = -10920.765787813223
Iteration 4700: Loss = -10920.765658293674
Iteration 4800: Loss = -10920.76551013205
Iteration 4900: Loss = -10920.76537666396
Iteration 5000: Loss = -10920.765227859738
Iteration 5100: Loss = -10920.765108547796
Iteration 5200: Loss = -10920.765003728327
Iteration 5300: Loss = -10920.764919846031
Iteration 5400: Loss = -10920.764850336654
Iteration 5500: Loss = -10920.764746530185
Iteration 5600: Loss = -10920.764673744605
Iteration 5700: Loss = -10920.764605554
Iteration 5800: Loss = -10920.764572889906
Iteration 5900: Loss = -10920.764447454556
Iteration 6000: Loss = -10920.764336574663
Iteration 6100: Loss = -10920.764224983306
Iteration 6200: Loss = -10920.76418611604
Iteration 6300: Loss = -10920.764184937398
Iteration 6400: Loss = -10920.764147852939
Iteration 6500: Loss = -10920.764074530895
Iteration 6600: Loss = -10920.764041751127
Iteration 6700: Loss = -10920.76403070628
Iteration 6800: Loss = -10920.764561450378
1
Iteration 6900: Loss = -10920.764492835666
2
Iteration 7000: Loss = -10920.76394191141
Iteration 7100: Loss = -10920.76395733056
1
Iteration 7200: Loss = -10920.862934756422
2
Iteration 7300: Loss = -10920.763787206239
Iteration 7400: Loss = -10920.832381493008
1
Iteration 7500: Loss = -10920.763762157314
Iteration 7600: Loss = -10920.763761259917
Iteration 7700: Loss = -10920.764233090233
1
Iteration 7800: Loss = -10920.763711085163
Iteration 7900: Loss = -10920.763665081604
Iteration 8000: Loss = -10920.86200339244
1
Iteration 8100: Loss = -10920.763667623303
2
Iteration 8200: Loss = -10920.763660330429
Iteration 8300: Loss = -10920.787358931973
1
Iteration 8400: Loss = -10920.763641418876
Iteration 8500: Loss = -10920.763646285
1
Iteration 8600: Loss = -10920.900122698342
2
Iteration 8700: Loss = -10920.763649283557
3
Iteration 8800: Loss = -10920.763653395192
4
Iteration 8900: Loss = -10920.8631247385
5
Iteration 9000: Loss = -10920.763617262375
Iteration 9100: Loss = -10920.763590186203
Iteration 9200: Loss = -10920.765409065552
1
Iteration 9300: Loss = -10920.763603858559
2
Iteration 9400: Loss = -10920.763595546225
3
Iteration 9500: Loss = -10920.76669255116
4
Iteration 9600: Loss = -10920.763582993945
Iteration 9700: Loss = -10920.764507798656
1
Iteration 9800: Loss = -10920.764214553139
2
Iteration 9900: Loss = -10920.763618764197
3
Iteration 10000: Loss = -10920.76358925507
4
Iteration 10100: Loss = -10920.77338967582
5
Iteration 10200: Loss = -10920.763671321287
6
Iteration 10300: Loss = -10920.76361349166
7
Iteration 10400: Loss = -10920.778013950248
8
Iteration 10500: Loss = -10920.763813431911
9
Iteration 10600: Loss = -10920.770334892171
10
Stopping early at iteration 10600 due to no improvement.
tensor([[-0.1514, -1.9582],
        [ 0.9270, -3.0479],
        [ 0.7821, -2.4620],
        [ 0.5008, -2.1395],
        [ 1.3897, -2.7761],
        [-0.5955, -2.2893],
        [ 0.9598, -2.3588],
        [ 1.2806, -3.2296],
        [ 2.3515, -3.7413],
        [ 2.8026, -4.7303],
        [ 1.1799, -3.0908],
        [ 0.9047, -2.2922],
        [ 3.0712, -4.4845],
        [ 1.6973, -4.8617],
        [ 0.6366, -2.5304],
        [ 1.5069, -4.5434],
        [ 1.0404, -3.9643],
        [ 2.6789, -4.5344],
        [ 0.6363, -2.1964],
        [ 2.0761, -3.5449],
        [ 1.5577, -3.0247],
        [ 2.1970, -3.6089],
        [ 1.6089, -3.0566],
        [ 0.9788, -2.6151],
        [ 0.9972, -2.4710],
        [ 1.3600, -3.8387],
        [ 0.8270, -3.9504],
        [-1.6043, -3.0109],
        [ 2.1948, -4.1618],
        [ 1.5064, -3.3637],
        [ 3.0140, -4.4072],
        [ 2.9537, -4.3842],
        [ 2.1949, -3.7428],
        [ 1.2016, -2.5932],
        [ 1.7196, -3.1227],
        [ 1.7073, -3.4356],
        [ 1.5997, -4.1563],
        [ 2.2187, -3.8608],
        [ 0.5237, -1.9236],
        [ 0.5442, -1.9551],
        [ 0.7082, -3.3464],
        [ 0.2597, -4.3562],
        [ 0.1631, -3.3076],
        [ 0.3182, -1.7628],
        [ 1.1917, -4.1367],
        [ 1.2456, -2.7394],
        [-1.3007, -0.8327],
        [ 1.4960, -2.8918],
        [ 1.1165, -2.6577],
        [ 1.7372, -3.4536],
        [ 1.8046, -3.9876],
        [ 2.5198, -3.9063],
        [ 1.3092, -3.2781],
        [ 0.2991, -2.0411],
        [ 0.5631, -3.1970],
        [ 2.1191, -5.3580],
        [ 0.9383, -2.3287],
        [ 0.6731, -4.9146],
        [-0.7277, -3.3663],
        [ 0.8548, -5.4701],
        [ 1.6571, -3.2217],
        [ 1.5299, -2.9871],
        [ 0.4116, -2.7440],
        [-0.2899, -1.6354],
        [ 2.0237, -4.2319],
        [ 2.3007, -3.6929],
        [ 0.6818, -3.0011],
        [ 0.8935, -2.6538],
        [ 2.2852, -3.6722],
        [ 0.5651, -2.1561],
        [ 1.8309, -4.8632],
        [ 1.3364, -3.2039],
        [ 0.9264, -2.3527],
        [ 2.3927, -3.8501],
        [ 1.3228, -4.1368],
        [ 1.9596, -3.3616],
        [-0.3524, -4.2629],
        [ 0.9197, -2.5971],
        [ 1.8878, -3.3765],
        [ 1.0578, -2.4453],
        [ 1.5501, -3.7258],
        [ 2.1740, -3.9167],
        [ 1.4469, -3.8185],
        [ 1.6911, -3.1891],
        [ 2.3754, -3.9106],
        [ 1.2406, -3.4996],
        [ 3.4470, -4.8976],
        [ 1.6260, -3.0326],
        [ 1.9216, -3.8467],
        [-1.0037, -2.4403],
        [ 0.4376, -2.5916],
        [ 1.9373, -3.6506],
        [ 1.1806, -4.2469],
        [ 2.5350, -4.1901],
        [ 1.3927, -4.9483],
        [ 1.1768, -3.3259],
        [ 0.1763, -2.6557],
        [ 1.0153, -3.1795],
        [-0.2004, -1.3377],
        [ 1.4297, -3.4827]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9677, 0.0323],
        [0.6212, 0.3788]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9665, 0.0335], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1601, 0.2135],
         [0.7796, 0.3773]],

        [[0.2006, 0.2076],
         [0.5843, 0.5094]],

        [[0.1233, 0.0758],
         [0.9323, 0.3327]],

        [[0.5137, 0.2217],
         [0.2407, 0.7198]],

        [[0.9859, 0.1737],
         [0.5065, 0.9005]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.002427096100870114
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.007431593898976406
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.008484848484848486
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00015340735879710383
Average Adjusted Rand Index: 0.0006082138535855413
Iteration 0: Loss = -23140.142831190882
Iteration 10: Loss = -10926.021026368768
Iteration 20: Loss = -10925.925956632685
Iteration 30: Loss = -10925.78577677651
Iteration 40: Loss = -10925.528030264997
Iteration 50: Loss = -10925.430642051097
Iteration 60: Loss = -10925.433493851946
1
Iteration 70: Loss = -10925.438511995539
2
Iteration 80: Loss = -10925.442225526313
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.2230, 0.7770],
        [0.0198, 0.9802]], dtype=torch.float64)
alpha: tensor([0.0248, 0.9752])
beta: tensor([[[0.2219, 0.2138],
         [0.4990, 0.1584]],

        [[0.2102, 0.2117],
         [0.5308, 0.1323]],

        [[0.3719, 0.1056],
         [0.0731, 0.2806]],

        [[0.0497, 0.2130],
         [0.8373, 0.6821]],

        [[0.3589, 0.1521],
         [0.9392, 0.4491]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23139.45386694953
Iteration 100: Loss = -10969.425416019456
Iteration 200: Loss = -10905.010119746905
Iteration 300: Loss = -10860.595498642984
Iteration 400: Loss = -10827.617194476408
Iteration 500: Loss = -10817.606741561143
Iteration 600: Loss = -10813.855649447441
Iteration 700: Loss = -10813.678126914194
Iteration 800: Loss = -10813.575700967885
Iteration 900: Loss = -10813.45087386889
Iteration 1000: Loss = -10813.420339966377
Iteration 1100: Loss = -10813.400167006967
Iteration 1200: Loss = -10813.38220344063
Iteration 1300: Loss = -10813.368406398975
Iteration 1400: Loss = -10813.35763274319
Iteration 1500: Loss = -10813.33803460392
Iteration 1600: Loss = -10813.198937593153
Iteration 1700: Loss = -10813.189395916379
Iteration 1800: Loss = -10813.158058086017
Iteration 1900: Loss = -10813.071741058515
Iteration 2000: Loss = -10813.062549515394
Iteration 2100: Loss = -10813.058685697824
Iteration 2200: Loss = -10813.055555168583
Iteration 2300: Loss = -10813.05173016513
Iteration 2400: Loss = -10813.04718200153
Iteration 2500: Loss = -10812.987355861082
Iteration 2600: Loss = -10812.965912386215
Iteration 2700: Loss = -10812.964485644794
Iteration 2800: Loss = -10812.963279679148
Iteration 2900: Loss = -10812.96192883802
Iteration 3000: Loss = -10812.958565879264
Iteration 3100: Loss = -10812.945728953
Iteration 3200: Loss = -10812.943900077515
Iteration 3300: Loss = -10812.942815211269
Iteration 3400: Loss = -10812.941891541439
Iteration 3500: Loss = -10812.940774492765
Iteration 3600: Loss = -10812.939455912443
Iteration 3700: Loss = -10812.938234677245
Iteration 3800: Loss = -10812.937003648456
Iteration 3900: Loss = -10812.926036528532
Iteration 4000: Loss = -10812.892652433364
Iteration 4100: Loss = -10812.892229961411
Iteration 4200: Loss = -10812.89188257331
Iteration 4300: Loss = -10812.891580023761
Iteration 4400: Loss = -10812.891290778876
Iteration 4500: Loss = -10812.890994494475
Iteration 4600: Loss = -10812.890687360357
Iteration 4700: Loss = -10812.890398067346
Iteration 4800: Loss = -10812.890204812129
Iteration 4900: Loss = -10812.890033749398
Iteration 5000: Loss = -10812.890159118819
1
Iteration 5100: Loss = -10812.889384249314
Iteration 5200: Loss = -10812.888790993497
Iteration 5300: Loss = -10812.890139753486
1
Iteration 5400: Loss = -10812.888249969354
Iteration 5500: Loss = -10812.88901937748
1
Iteration 5600: Loss = -10812.888357672371
2
Iteration 5700: Loss = -10812.89170901272
3
Iteration 5800: Loss = -10812.88776079854
Iteration 5900: Loss = -10812.88800735626
1
Iteration 6000: Loss = -10812.88769852464
Iteration 6100: Loss = -10812.891254419268
1
Iteration 6200: Loss = -10812.886201590238
Iteration 6300: Loss = -10812.885375095451
Iteration 6400: Loss = -10809.722182818521
Iteration 6500: Loss = -10808.645141892022
Iteration 6600: Loss = -10808.645586067092
1
Iteration 6700: Loss = -10808.641137631448
Iteration 6800: Loss = -10808.640455656065
Iteration 6900: Loss = -10808.634646514167
Iteration 7000: Loss = -10805.658497158087
Iteration 7100: Loss = -10805.29260837161
Iteration 7200: Loss = -10805.279348765858
Iteration 7300: Loss = -10805.270858530164
Iteration 7400: Loss = -10805.27611673086
1
Iteration 7500: Loss = -10805.270495103963
Iteration 7600: Loss = -10805.280758138917
1
Iteration 7700: Loss = -10805.273977973251
2
Iteration 7800: Loss = -10805.269783949494
Iteration 7900: Loss = -10805.269667360373
Iteration 8000: Loss = -10805.247094796217
Iteration 8100: Loss = -10805.24388323147
Iteration 8200: Loss = -10805.241572206936
Iteration 8300: Loss = -10805.243758247247
1
Iteration 8400: Loss = -10805.239932509325
Iteration 8500: Loss = -10805.230670048752
Iteration 8600: Loss = -10805.226409516585
Iteration 8700: Loss = -10805.235583897214
1
Iteration 8800: Loss = -10805.225272133823
Iteration 8900: Loss = -10805.224726720917
Iteration 9000: Loss = -10805.230614848217
1
Iteration 9100: Loss = -10805.22433682894
Iteration 9200: Loss = -10805.251916608882
1
Iteration 9300: Loss = -10805.22311876341
Iteration 9400: Loss = -10805.220686183371
Iteration 9500: Loss = -10805.21596419212
Iteration 9600: Loss = -10805.216611162024
1
Iteration 9700: Loss = -10805.22409891096
2
Iteration 9800: Loss = -10805.217836010963
3
Iteration 9900: Loss = -10805.225795425313
4
Iteration 10000: Loss = -10805.216492504242
5
Iteration 10100: Loss = -10805.216656069251
6
Iteration 10200: Loss = -10805.214768141605
Iteration 10300: Loss = -10805.215141326882
1
Iteration 10400: Loss = -10805.21426833274
Iteration 10500: Loss = -10805.214489038435
1
Iteration 10600: Loss = -10805.215325168348
2
Iteration 10700: Loss = -10805.221548321097
3
Iteration 10800: Loss = -10805.219429987446
4
Iteration 10900: Loss = -10805.214145539585
Iteration 11000: Loss = -10805.215283380512
1
Iteration 11100: Loss = -10805.214115283738
Iteration 11200: Loss = -10805.208473996063
Iteration 11300: Loss = -10805.15303518842
Iteration 11400: Loss = -10805.147348266708
Iteration 11500: Loss = -10805.15886728687
1
Iteration 11600: Loss = -10805.131884930282
Iteration 11700: Loss = -10805.13066796914
Iteration 11800: Loss = -10805.14564744645
1
Iteration 11900: Loss = -10805.131977661793
2
Iteration 12000: Loss = -10805.130194555866
Iteration 12100: Loss = -10805.132652051023
1
Iteration 12200: Loss = -10805.13627966109
2
Iteration 12300: Loss = -10805.128916501513
Iteration 12400: Loss = -10805.141320987297
1
Iteration 12500: Loss = -10805.146137599171
2
Iteration 12600: Loss = -10805.12837840391
Iteration 12700: Loss = -10805.128127766438
Iteration 12800: Loss = -10805.312193178925
1
Iteration 12900: Loss = -10805.127965826367
Iteration 13000: Loss = -10805.127665363376
Iteration 13100: Loss = -10805.13804626555
1
Iteration 13200: Loss = -10805.130967647628
2
Iteration 13300: Loss = -10805.128042411301
3
Iteration 13400: Loss = -10805.12900097926
4
Iteration 13500: Loss = -10805.226799013017
5
Iteration 13600: Loss = -10805.124176132958
Iteration 13700: Loss = -10805.137541812024
1
Iteration 13800: Loss = -10805.127313503172
2
Iteration 13900: Loss = -10805.230803477081
3
Iteration 14000: Loss = -10805.125885793366
4
Iteration 14100: Loss = -10805.129311435745
5
Iteration 14200: Loss = -10805.133850236665
6
Iteration 14300: Loss = -10805.124510966907
7
Iteration 14400: Loss = -10805.125241647442
8
Iteration 14500: Loss = -10805.144335178426
9
Iteration 14600: Loss = -10805.13134113908
10
Stopping early at iteration 14600 due to no improvement.
tensor([[-4.9253,  3.2533],
        [-4.1430,  1.8022],
        [ 0.9147, -5.1597],
        [-7.8116,  5.6942],
        [-6.4802,  3.8822],
        [-3.6185,  1.1659],
        [-8.9568,  5.4261],
        [-4.7986,  3.3062],
        [-3.6530,  2.2623],
        [ 3.4430, -5.0868],
        [-1.7366, -2.1451],
        [-3.8237,  2.0218],
        [ 0.4606, -1.8562],
        [ 1.7738, -3.5703],
        [-5.0325,  1.5283],
        [ 0.6173, -2.0790],
        [-2.6277, -0.1318],
        [ 2.6660, -4.7649],
        [ 1.3896, -2.7778],
        [-0.1580, -3.1609],
        [-2.6371,  1.2093],
        [ 1.4668, -3.6980],
        [ 1.1048, -2.6772],
        [-5.3751,  1.4654],
        [ 1.1119, -2.4984],
        [-2.7764,  0.9634],
        [-1.9723,  0.2085],
        [-4.8527,  3.0493],
        [ 2.0441, -3.7036],
        [ 1.4228, -2.8145],
        [ 1.4063, -4.2486],
        [ 0.1961, -4.8113],
        [ 0.8561, -3.8386],
        [-4.5590,  2.5267],
        [-4.6601,  1.7012],
        [ 1.7292, -3.3511],
        [ 1.0651, -2.9291],
        [ 1.5909, -4.4557],
        [ 4.5020, -6.2114],
        [-6.4117,  4.9711],
        [-3.8746,  1.9049],
        [ 0.5609, -2.1045],
        [-5.4432,  1.9553],
        [-9.2782,  7.1227],
        [ 1.7640, -3.1656],
        [-6.9351,  2.3199],
        [-6.6983,  5.0579],
        [-8.8374,  7.1406],
        [-0.7002, -1.1871],
        [-7.2882,  3.7704],
        [ 1.6717, -3.3094],
        [ 3.4964, -4.9075],
        [ 2.2680, -5.7282],
        [-5.2071,  3.5295],
        [ 0.9919, -2.5847],
        [ 0.8433, -2.7661],
        [ 2.4033, -4.0056],
        [-0.4008, -1.1233],
        [-8.6112,  6.6961],
        [-3.0331,  1.4996],
        [ 4.0446, -7.8735],
        [-3.9393,  2.5074],
        [-4.5317,  2.3910],
        [-0.9310, -1.8167],
        [-1.2742, -0.3459],
        [-0.2012, -1.6618],
        [-5.7453,  4.3518],
        [-8.4345,  3.8193],
        [-0.3970, -1.3771],
        [-1.9903, -0.6565],
        [ 1.2128, -3.5955],
        [-5.4609,  3.0936],
        [-4.7803,  3.1680],
        [-0.3389, -1.2539],
        [ 1.8307, -3.8044],
        [ 2.1970, -3.5990],
        [-0.0338, -4.5814],
        [-4.1930,  1.0094],
        [-3.1026,  0.7247],
        [ 0.1080, -2.7096],
        [-3.5405,  0.5956],
        [ 2.4395, -3.8493],
        [-3.3937,  0.7146],
        [-1.6896,  0.2168],
        [-3.5856,  1.7897],
        [-6.9170,  4.6811],
        [ 0.2334, -2.8069],
        [ 0.9810, -3.0300],
        [ 0.8225, -4.3518],
        [-7.9488,  6.4386],
        [-4.7606,  1.7900],
        [-5.7616,  3.8142],
        [-1.5555, -0.0266],
        [ 2.3268, -3.8325],
        [ 1.8738, -3.6273],
        [ 0.2428, -1.6448],
        [-6.4783,  3.0310],
        [-1.6689,  0.2690],
        [-2.9928,  1.5566],
        [ 3.0204, -4.9483]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.2928, 0.7072],
        [0.7759, 0.2241]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4733, 0.5267], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2208, 0.1014],
         [0.4990, 0.2295]],

        [[0.2102, 0.0966],
         [0.5308, 0.1323]],

        [[0.3719, 0.0942],
         [0.0731, 0.2806]],

        [[0.0497, 0.1014],
         [0.8373, 0.6821]],

        [[0.3589, 0.0944],
         [0.9392, 0.4491]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080650161849864
time is 1
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080682750429576
time is 2
tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 3
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
Global Adjusted Rand Index: 0.033408942442542
Average Adjusted Rand Index: 0.8526779788641567
Iteration 0: Loss = -30310.88724437467
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.0281,    nan]],

        [[0.2971,    nan],
         [0.5523, 0.8298]],

        [[0.3740,    nan],
         [0.7010, 0.0655]],

        [[0.4673,    nan],
         [0.3535, 0.2121]],

        [[0.1851,    nan],
         [0.4003, 0.7313]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30309.879416924086
Iteration 100: Loss = -10934.294934642017
Iteration 200: Loss = -10931.135870879742
Iteration 300: Loss = -10929.709739181793
Iteration 400: Loss = -10928.865427493922
Iteration 500: Loss = -10928.31846776289
Iteration 600: Loss = -10927.940836977106
Iteration 700: Loss = -10927.668823267695
Iteration 800: Loss = -10927.465919993436
Iteration 900: Loss = -10927.310196230514
Iteration 1000: Loss = -10927.18803548983
Iteration 1100: Loss = -10927.090251784939
Iteration 1200: Loss = -10927.010584227475
Iteration 1300: Loss = -10926.944713736637
Iteration 1400: Loss = -10926.889544007729
Iteration 1500: Loss = -10926.842800447528
Iteration 1600: Loss = -10926.802840432321
Iteration 1700: Loss = -10926.768354532756
Iteration 1800: Loss = -10926.738295943504
Iteration 1900: Loss = -10926.711885668648
Iteration 2000: Loss = -10926.688563832891
Iteration 2100: Loss = -10926.667820028799
Iteration 2200: Loss = -10926.64937972736
Iteration 2300: Loss = -10926.63270959261
Iteration 2400: Loss = -10926.617664145699
Iteration 2500: Loss = -10926.603902426401
Iteration 2600: Loss = -10926.591286547326
Iteration 2700: Loss = -10926.579621624343
Iteration 2800: Loss = -10926.568733977496
Iteration 2900: Loss = -10926.558428457198
Iteration 3000: Loss = -10926.548717404341
Iteration 3100: Loss = -10926.539313547295
Iteration 3200: Loss = -10926.530146311605
Iteration 3300: Loss = -10926.520997306934
Iteration 3400: Loss = -10926.511713742177
Iteration 3500: Loss = -10926.501924718825
Iteration 3600: Loss = -10926.632548434653
1
Iteration 3700: Loss = -10926.47868404317
Iteration 3800: Loss = -10926.463486735165
Iteration 3900: Loss = -10926.444334834625
Iteration 4000: Loss = -10926.422586179171
Iteration 4100: Loss = -10926.386889518812
Iteration 4200: Loss = -10926.341647866846
Iteration 4300: Loss = -10926.72168419641
1
Iteration 4400: Loss = -10926.05666923428
Iteration 4500: Loss = -10925.564078907375
Iteration 4600: Loss = -10924.762543370965
Iteration 4700: Loss = -10923.191064972196
Iteration 4800: Loss = -10921.629328440364
Iteration 4900: Loss = -10921.216731615015
Iteration 5000: Loss = -10921.135646287996
Iteration 5100: Loss = -10921.08625141818
Iteration 5200: Loss = -10921.041879785093
Iteration 5300: Loss = -10920.823292218496
Iteration 5400: Loss = -10920.810803481589
Iteration 5500: Loss = -10920.775437790731
Iteration 5600: Loss = -10920.751503061685
Iteration 5700: Loss = -10920.801288283808
1
Iteration 5800: Loss = -10920.729617630508
Iteration 5900: Loss = -10920.73033198493
1
Iteration 6000: Loss = -10920.717437324583
Iteration 6100: Loss = -10920.71187400097
Iteration 6200: Loss = -10920.641943437238
Iteration 6300: Loss = -10920.637674502244
Iteration 6400: Loss = -10920.660162456636
1
Iteration 6500: Loss = -10920.632387201735
Iteration 6600: Loss = -10920.629640399029
Iteration 6700: Loss = -10920.64859233873
1
Iteration 6800: Loss = -10920.62637137968
Iteration 6900: Loss = -10920.62526556793
Iteration 7000: Loss = -10920.62454986616
Iteration 7100: Loss = -10920.623224041614
Iteration 7200: Loss = -10920.626067026853
1
Iteration 7300: Loss = -10920.621091184725
Iteration 7400: Loss = -10920.61938463934
Iteration 7500: Loss = -10920.616759851715
Iteration 7600: Loss = -10920.605351846829
Iteration 7700: Loss = -10920.494750960272
Iteration 7800: Loss = -10920.462854856509
Iteration 7900: Loss = -10920.445306114512
Iteration 8000: Loss = -10920.43275148911
Iteration 8100: Loss = -10920.416185629774
Iteration 8200: Loss = -10920.402507834595
Iteration 8300: Loss = -10920.396108131585
Iteration 8400: Loss = -10920.38476377961
Iteration 8500: Loss = -10920.379852334763
Iteration 8600: Loss = -10920.382808491855
1
Iteration 8700: Loss = -10920.370566118649
Iteration 8800: Loss = -10920.367245489948
Iteration 8900: Loss = -10920.456360200546
1
Iteration 9000: Loss = -10920.364271200759
Iteration 9100: Loss = -10920.363453948838
Iteration 9200: Loss = -10920.36361555252
1
Iteration 9300: Loss = -10920.362409984538
Iteration 9400: Loss = -10920.362060209163
Iteration 9500: Loss = -10920.363339219806
1
Iteration 9600: Loss = -10920.361613324343
Iteration 9700: Loss = -10920.36143692065
Iteration 9800: Loss = -10920.361600891229
1
Iteration 9900: Loss = -10920.361210342306
Iteration 10000: Loss = -10920.371343430961
1
Iteration 10100: Loss = -10920.361118265038
Iteration 10200: Loss = -10920.3609508779
Iteration 10300: Loss = -10920.361065161276
1
Iteration 10400: Loss = -10920.360880561562
Iteration 10500: Loss = -10920.361263524255
1
Iteration 10600: Loss = -10920.360908589915
2
Iteration 10700: Loss = -10920.360753476165
Iteration 10800: Loss = -10920.360700461011
Iteration 10900: Loss = -10920.36072023611
1
Iteration 11000: Loss = -10920.360646643012
Iteration 11100: Loss = -10920.360973023626
1
Iteration 11200: Loss = -10920.36059673746
Iteration 11300: Loss = -10920.361353720633
1
Iteration 11400: Loss = -10920.389554980853
2
Iteration 11500: Loss = -10920.360345269175
Iteration 11600: Loss = -10920.362867801212
1
Iteration 11700: Loss = -10920.360286851428
Iteration 11800: Loss = -10920.541622877634
1
Iteration 11900: Loss = -10920.360277040989
Iteration 12000: Loss = -10920.360281746656
1
Iteration 12100: Loss = -10920.360645802117
2
Iteration 12200: Loss = -10920.360588187064
3
Iteration 12300: Loss = -10920.360925125737
4
Iteration 12400: Loss = -10920.360500243
5
Iteration 12500: Loss = -10920.455265415621
6
Iteration 12600: Loss = -10920.472387574273
7
Iteration 12700: Loss = -10920.361271908485
8
Iteration 12800: Loss = -10920.360384280115
9
Iteration 12900: Loss = -10920.44907197483
10
Stopping early at iteration 12900 due to no improvement.
tensor([[-1.9430,  0.2199],
        [-3.0112,  1.5113],
        [-2.5441,  1.0093],
        [-2.5407,  0.5262],
        [-3.1719,  1.5274],
        [-1.9151,  0.2163],
        [-2.7468,  1.0339],
        [-3.7856,  1.2369],
        [-4.3855,  2.2718],
        [-4.9423,  3.2587],
        [-3.9543,  0.8098],
        [-3.0440,  0.6205],
        [-5.0071,  3.2049],
        [-4.5767,  2.5915],
        [-3.0664,  0.6628],
        [-4.0274,  2.6031],
        [-3.6983,  1.8456],
        [-4.9362,  2.9191],
        [-2.5847,  0.3599],
        [-3.8634,  2.2985],
        [-3.3768,  1.7374],
        [-3.9087,  2.4819],
        [-3.5608,  1.3829],
        [-3.1127,  0.6046],
        [-3.4923,  0.4870],
        [-3.8088,  1.9456],
        [-3.3232,  1.8885],
        [-1.6102,  0.2236],
        [-4.2238,  2.7600],
        [-3.4524,  1.9782],
        [-4.7143,  3.2302],
        [-4.9639,  3.4922],
        [-4.9232,  1.5965],
        [-3.0062,  1.3150],
        [-3.4284,  1.9813],
        [-4.2048,  1.4541],
        [-4.2262,  2.1115],
        [-4.0474,  2.6372],
        [-2.1566,  0.7347],
        [-2.2469,  0.6817],
        [-3.0647,  1.5214],
        [-4.1490,  1.0167],
        [-2.7456,  1.1652],
        [-1.9356,  0.5486],
        [-3.8813,  1.9811],
        [-4.5314, -0.0839],
        [-0.6782, -0.9020],
        [-3.2581,  1.6606],
        [-3.3421,  0.9966],
        [-3.6252,  2.1437],
        [-4.1604,  2.2238],
        [-4.4112,  2.6278],
        [-3.4274,  1.7007],
        [-2.1750,  0.5989],
        [-3.0477,  1.1605],
        [-4.7816,  3.3741],
        [-2.7119,  0.9496],
        [-3.9842,  2.2028],
        [-3.0151,  0.0626],
        [-4.9011,  2.0508],
        [-3.6153,  1.8303],
        [-3.9312,  1.1659],
        [-3.0497,  0.5250],
        [-1.5527,  0.1654],
        [-4.5752,  2.2495],
        [-4.0529,  2.4992],
        [-2.8623,  1.3165],
        [-3.2631,  0.8188],
        [-4.0885,  2.4816],
        [-2.3079,  0.8599],
        [-4.5281,  2.8100],
        [-3.3118,  1.7393],
        [-2.5380,  1.1121],
        [-4.5353,  2.2560],
        [-4.4059,  1.5713],
        [-3.6563,  2.2333],
        [-3.1089,  1.3751],
        [-3.5746,  0.4347],
        [-3.3794,  1.9727],
        [-2.6486,  1.1799],
        [-3.7498,  2.1129],
        [-5.6516,  1.0364],
        [-3.5999,  2.1769],
        [-3.8850,  1.5025],
        [-4.4370,  2.3995],
        [-3.5374,  1.7181],
        [-5.6319,  3.4099],
        [-3.5399,  1.6432],
        [-4.0318,  2.3239],
        [-1.6549,  0.1688],
        [-4.0381, -0.5772],
        [-3.8570,  2.3278],
        [-3.7142,  2.2532],
        [-4.4273,  2.9280],
        [-4.4405,  2.5289],
        [-3.5229,  1.4837],
        [-2.3521,  0.9597],
        [-3.9578,  0.7211],
        [-1.2392, -0.1502],
        [-4.7316,  0.7316]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4200, 0.5800],
        [0.0238, 0.9762]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0247, 0.9753], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4198, 0.2177],
         [0.0281, 0.1617]],

        [[0.2971, 0.2069],
         [0.5523, 0.8298]],

        [[0.3740, 0.0748],
         [0.7010, 0.0655]],

        [[0.4673, 0.2301],
         [0.3535, 0.2121]],

        [[0.1851, 0.1024],
         [0.4003, 0.7313]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.007431593898976406
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: -0.0002174212737586244
Average Adjusted Rand Index: -0.0001684076785394205
10809.327774846859
new:  [-0.0003346061575015009, -0.00015340735879710383, 0.033408942442542, -0.0002174212737586244] [-0.0014856235194590558, 0.0006082138535855413, 0.8526779788641567, -0.0001684076785394205] [10918.857080306636, 10920.770334892171, 10805.13134113908, 10920.44907197483]
prior:  [0.0002897541819023893, 0.0, 0.0, 0.0] [0.0005064848866166887, 0.0, 0.0, 0.0] [10921.795177391043, 10925.402429379477, 10925.442225526313, nan]
-----------------------------------------------------------------------------------------
This iteration is 8
True Objective function: Loss = -10951.77092898354
Iteration 0: Loss = -22045.26196415933
Iteration 10: Loss = -10990.526255099849
Iteration 20: Loss = -10990.013821735845
Iteration 30: Loss = -10989.846800855405
Iteration 40: Loss = -10989.704268785152
Iteration 50: Loss = -10989.599276813484
Iteration 60: Loss = -10989.62899815294
1
Iteration 70: Loss = -10989.669100918067
2
Iteration 80: Loss = -10989.707794399139
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[3.5522e-20, 1.0000e+00],
        [3.2454e-02, 9.6755e-01]], dtype=torch.float64)
alpha: tensor([0.0290, 0.9710])
beta: tensor([[[0.1456, 0.3008],
         [0.2419, 0.1607]],

        [[0.3018, 0.1600],
         [0.4627, 0.9811]],

        [[0.5767, 0.1005],
         [0.9069, 0.4662]],

        [[0.6485, 0.1201],
         [0.9225, 0.8476]],

        [[0.2497, 0.2120],
         [0.5896, 0.0354]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0011260433799580283
Average Adjusted Rand Index: 0.000488518314825395
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21978.80852596654
Iteration 100: Loss = -10998.78300036226
Iteration 200: Loss = -10995.547381772078
Iteration 300: Loss = -10992.770539230167
Iteration 400: Loss = -10991.819690925513
Iteration 500: Loss = -10991.280046780586
Iteration 600: Loss = -10991.01532758542
Iteration 700: Loss = -10990.874376058375
Iteration 800: Loss = -10990.778806931925
Iteration 900: Loss = -10990.708970065658
Iteration 1000: Loss = -10990.700325834627
Iteration 1100: Loss = -10990.615483348458
Iteration 1200: Loss = -10990.583405056363
Iteration 1300: Loss = -10990.557730501045
Iteration 1400: Loss = -10990.537592588753
Iteration 1500: Loss = -10990.519837413742
Iteration 1600: Loss = -10990.505616788134
Iteration 1700: Loss = -10990.810949577006
1
Iteration 1800: Loss = -10990.4833902059
Iteration 1900: Loss = -10990.474601418968
Iteration 2000: Loss = -10990.466979196246
Iteration 2100: Loss = -10990.461732711012
Iteration 2200: Loss = -10990.454442968763
Iteration 2300: Loss = -10990.44916503541
Iteration 2400: Loss = -10990.456029404377
1
Iteration 2500: Loss = -10990.440274825693
Iteration 2600: Loss = -10990.436375664092
Iteration 2700: Loss = -10990.432950812357
Iteration 2800: Loss = -10990.429782000243
Iteration 2900: Loss = -10990.426685741435
Iteration 3000: Loss = -10990.423885256076
Iteration 3100: Loss = -10990.426067598697
1
Iteration 3200: Loss = -10990.41884708259
Iteration 3300: Loss = -10990.416509303888
Iteration 3400: Loss = -10990.414309155904
Iteration 3500: Loss = -10990.412756709486
Iteration 3600: Loss = -10990.410195481378
Iteration 3700: Loss = -10990.408260352782
Iteration 3800: Loss = -10990.406427911386
Iteration 3900: Loss = -10990.404816567836
Iteration 4000: Loss = -10990.402767126958
Iteration 4100: Loss = -10990.401048114169
Iteration 4200: Loss = -10990.560543946956
1
Iteration 4300: Loss = -10990.397659860258
Iteration 4400: Loss = -10990.39597747349
Iteration 4500: Loss = -10990.394375988757
Iteration 4600: Loss = -10990.393357628083
Iteration 4700: Loss = -10990.39118134307
Iteration 4800: Loss = -10990.389574565406
Iteration 4900: Loss = -10990.387980345118
Iteration 5000: Loss = -10990.387228331487
Iteration 5100: Loss = -10990.38483839023
Iteration 5200: Loss = -10990.383239577628
Iteration 5300: Loss = -10990.657008962067
1
Iteration 5400: Loss = -10990.380144084029
Iteration 5500: Loss = -10990.378524036852
Iteration 5600: Loss = -10990.376957495488
Iteration 5700: Loss = -10990.379942310245
1
Iteration 5800: Loss = -10990.373854452751
Iteration 5900: Loss = -10990.372301660147
Iteration 6000: Loss = -10990.370760118083
Iteration 6100: Loss = -10990.369347807109
Iteration 6200: Loss = -10990.367659097605
Iteration 6300: Loss = -10990.366160570835
Iteration 6400: Loss = -10990.364600444946
Iteration 6500: Loss = -10990.363117336825
Iteration 6600: Loss = -10990.361786771951
Iteration 6700: Loss = -10990.360144933895
Iteration 6800: Loss = -10990.358674204157
Iteration 6900: Loss = -10990.359901538675
1
Iteration 7000: Loss = -10990.355802471175
Iteration 7100: Loss = -10990.354413051196
Iteration 7200: Loss = -10990.370701871932
1
Iteration 7300: Loss = -10990.351745028169
Iteration 7400: Loss = -10990.35034479414
Iteration 7500: Loss = -10990.349085290647
Iteration 7600: Loss = -10990.359909309247
1
Iteration 7700: Loss = -10990.346602111249
Iteration 7800: Loss = -10990.345347170856
Iteration 7900: Loss = -10990.344148427124
Iteration 8000: Loss = -10990.343136527435
Iteration 8100: Loss = -10990.341914958091
Iteration 8200: Loss = -10990.340851288878
Iteration 8300: Loss = -10990.33981679378
Iteration 8400: Loss = -10990.338970939141
Iteration 8500: Loss = -10990.337870643767
Iteration 8600: Loss = -10990.33694600753
Iteration 8700: Loss = -10990.336615413778
Iteration 8800: Loss = -10990.335188620656
Iteration 8900: Loss = -10990.334351875399
Iteration 9000: Loss = -10990.333555836296
Iteration 9100: Loss = -10990.335730391813
1
Iteration 9200: Loss = -10990.331990101871
Iteration 9300: Loss = -10990.331292076144
Iteration 9400: Loss = -10990.352292655898
1
Iteration 9500: Loss = -10990.32989280182
Iteration 9600: Loss = -10990.32916633543
Iteration 9700: Loss = -10990.355211441423
1
Iteration 9800: Loss = -10990.326678551295
Iteration 9900: Loss = -10990.319673191327
Iteration 10000: Loss = -10990.316656202582
Iteration 10100: Loss = -10990.315562524169
Iteration 10200: Loss = -10990.314624707873
Iteration 10300: Loss = -10990.31348595954
Iteration 10400: Loss = -10990.31404284454
1
Iteration 10500: Loss = -10988.791502727481
Iteration 10600: Loss = -10988.775088797067
Iteration 10700: Loss = -10988.772256007329
Iteration 10800: Loss = -10988.768685066454
Iteration 10900: Loss = -10988.767721936942
Iteration 11000: Loss = -10988.767330573926
Iteration 11100: Loss = -10988.769440812905
1
Iteration 11200: Loss = -10988.766791030552
Iteration 11300: Loss = -10988.76646177312
Iteration 11400: Loss = -10988.766265818653
Iteration 11500: Loss = -10988.781909902777
1
Iteration 11600: Loss = -10988.765862003951
Iteration 11700: Loss = -10988.765989074105
1
Iteration 11800: Loss = -10988.77218993932
2
Iteration 11900: Loss = -10988.765287324
Iteration 12000: Loss = -10988.765459872628
1
Iteration 12100: Loss = -10988.765345949309
2
Iteration 12200: Loss = -10988.76468974612
Iteration 12300: Loss = -10988.765953508115
1
Iteration 12400: Loss = -10988.764271513885
Iteration 12500: Loss = -10988.764026460292
Iteration 12600: Loss = -10988.763991173693
Iteration 12700: Loss = -10988.76406755036
1
Iteration 12800: Loss = -10988.763223267822
Iteration 12900: Loss = -10988.763004357823
Iteration 13000: Loss = -10988.762580588538
Iteration 13100: Loss = -10988.762442118357
Iteration 13200: Loss = -10988.762603996229
1
Iteration 13300: Loss = -10988.76152369515
Iteration 13400: Loss = -10988.761223260433
Iteration 13500: Loss = -10988.76084643996
Iteration 13600: Loss = -10988.836849055377
1
Iteration 13700: Loss = -10988.760433445796
Iteration 13800: Loss = -10988.911680419116
1
Iteration 13900: Loss = -10988.759956697357
Iteration 14000: Loss = -10988.760786731627
1
Iteration 14100: Loss = -10988.760046739719
2
Iteration 14200: Loss = -10988.760661832892
3
Iteration 14300: Loss = -10988.81580931318
4
Iteration 14400: Loss = -10988.759858443045
Iteration 14500: Loss = -10988.85234619808
1
Iteration 14600: Loss = -10988.759790948123
Iteration 14700: Loss = -10988.817089380464
1
Iteration 14800: Loss = -10988.760080108214
2
Iteration 14900: Loss = -10988.78687212543
3
Iteration 15000: Loss = -10988.76094674006
4
Iteration 15100: Loss = -10988.75974071907
Iteration 15200: Loss = -10988.761330276955
1
Iteration 15300: Loss = -10988.759672137761
Iteration 15400: Loss = -10988.760475106676
1
Iteration 15500: Loss = -10988.759990147862
2
Iteration 15600: Loss = -10988.75966221255
Iteration 15700: Loss = -10988.7598442773
1
Iteration 15800: Loss = -10988.759653810554
Iteration 15900: Loss = -10988.761058366144
1
Iteration 16000: Loss = -10988.759672500839
2
Iteration 16100: Loss = -10988.759838144942
3
Iteration 16200: Loss = -10988.759630089851
Iteration 16300: Loss = -10988.810921362123
1
Iteration 16400: Loss = -10988.759630599297
2
Iteration 16500: Loss = -10988.7608091505
3
Iteration 16600: Loss = -10988.759718870471
4
Iteration 16700: Loss = -10988.760102479162
5
Iteration 16800: Loss = -10988.759577824034
Iteration 16900: Loss = -10988.761818504343
1
Iteration 17000: Loss = -10988.75974241341
2
Iteration 17100: Loss = -10988.759645144763
3
Iteration 17200: Loss = -10988.75959187525
4
Iteration 17300: Loss = -10988.760097720993
5
Iteration 17400: Loss = -10988.759578480041
6
Iteration 17500: Loss = -10988.761039637458
7
Iteration 17600: Loss = -10988.759572806524
Iteration 17700: Loss = -10988.760201634066
1
Iteration 17800: Loss = -10988.759587375393
2
Iteration 17900: Loss = -10988.75979012351
3
Iteration 18000: Loss = -10988.759581530936
4
Iteration 18100: Loss = -10988.759691092315
5
Iteration 18200: Loss = -10988.759614463243
6
Iteration 18300: Loss = -10988.759562667126
Iteration 18400: Loss = -10988.759742658356
1
Iteration 18500: Loss = -10988.759575517899
2
Iteration 18600: Loss = -10988.760612613996
3
Iteration 18700: Loss = -10988.759617384903
4
Iteration 18800: Loss = -10988.759672747521
5
Iteration 18900: Loss = -10988.759570764938
6
Iteration 19000: Loss = -10988.75962634854
7
Iteration 19100: Loss = -10988.75956515957
8
Iteration 19200: Loss = -10988.763195134889
9
Iteration 19300: Loss = -10988.759901383155
10
Stopping early at iteration 19300 due to no improvement.
tensor([[-10.7545,   6.1393],
        [-11.5921,   6.9769],
        [-11.2653,   6.6500],
        [-11.2485,   6.6333],
        [-12.2727,   7.6575],
        [-12.2562,   7.6410],
        [ -8.9578,   4.3425],
        [-11.4898,   6.8746],
        [ -8.1326,   3.5174],
        [-11.4012,   6.7860],
        [-10.8678,   6.2526],
        [ -4.8816,   0.2663],
        [-11.7989,   7.1837],
        [-11.0716,   6.4564],
        [-11.4681,   6.8529],
        [-11.0409,   6.4256],
        [ -7.1444,   2.5292],
        [-11.3371,   6.7219],
        [-11.4829,   6.8677],
        [-10.9069,   6.2917],
        [-11.1269,   6.5117],
        [-11.0388,   6.4236],
        [-12.3064,   7.6912],
        [-12.3198,   7.7046],
        [-12.0333,   7.4181],
        [-11.0606,   6.4454],
        [-10.9133,   6.2981],
        [-11.4108,   6.7955],
        [ -8.5469,   3.9316],
        [-11.2259,   6.6107],
        [-10.9905,   6.3752],
        [-11.5290,   6.9138],
        [-12.4769,   7.8616],
        [-11.4090,   6.7938],
        [ -4.3379,  -0.2773],
        [-10.8544,   6.2392],
        [-11.3268,   6.7116],
        [-11.4566,   6.8414],
        [-11.3290,   6.7138],
        [-11.3601,   6.7448],
        [-11.3020,   6.6868],
        [-10.9102,   6.2950],
        [-12.2469,   7.6317],
        [ -8.2781,   3.6629],
        [-11.5854,   6.9702],
        [ -6.1466,   1.5314],
        [-11.3067,   6.6915],
        [-11.3392,   6.7240],
        [-12.2932,   7.6780],
        [-11.6211,   7.0059],
        [-11.5146,   6.8993],
        [-10.8692,   6.2540],
        [-12.6435,   8.0283],
        [-10.9036,   6.2884],
        [-11.1120,   6.4968],
        [-11.4815,   6.8663],
        [-11.1278,   6.5126],
        [ -7.9522,   3.3370],
        [-11.5959,   6.9807],
        [-12.5339,   7.9187],
        [  0.7703,  -5.3856],
        [-11.0398,   6.4246],
        [-12.4381,   7.8229],
        [-11.3070,   6.6918],
        [-10.0892,   5.4740],
        [-11.1772,   6.5619],
        [-11.5836,   6.9683],
        [-11.1940,   6.5788],
        [-12.3002,   7.6850],
        [-11.3575,   6.7423],
        [-10.8651,   6.2498],
        [-11.1737,   6.5585],
        [-12.1746,   7.5594],
        [-11.3474,   6.7322],
        [-11.0147,   6.3995],
        [-12.3023,   7.6870],
        [ -9.8440,   5.2288],
        [ -9.9246,   5.3093],
        [-11.9303,   7.3151],
        [-11.3033,   6.6881],
        [-12.2389,   7.6236],
        [-12.0611,   7.4459],
        [-11.5332,   6.9179],
        [-11.4855,   6.8703],
        [ -7.0374,   2.4222],
        [-11.6913,   7.0761],
        [-10.7360,   6.1208],
        [ -8.6735,   4.0583],
        [-12.2102,   7.5950],
        [-10.7922,   6.1769],
        [-12.5045,   7.8893],
        [-11.6650,   7.0498],
        [-10.6948,   6.0795],
        [-12.4690,   7.8538],
        [ -9.5633,   4.9480],
        [-12.0751,   7.4599],
        [-11.9992,   7.3840],
        [-11.6749,   7.0597],
        [-11.3882,   6.7730],
        [ -8.2446,   3.6293]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 1.2733e-06],
        [1.5289e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0102, 0.9898], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1453, 0.3318],
         [0.2419, 0.1623]],

        [[0.3018, 0.1525],
         [0.4627, 0.9811]],

        [[0.5767, 0.1913],
         [0.9069, 0.4662]],

        [[0.6485, 0.1413],
         [0.9225, 0.8476]],

        [[0.2497, 0.1027],
         [0.5896, 0.0354]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 41
Adjusted Rand Index: -0.006658343736995423
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
Global Adjusted Rand Index: 0.0017702717390684977
Average Adjusted Rand Index: -0.0002538021515822506
Iteration 0: Loss = -17205.89048792662
Iteration 10: Loss = -10990.187081684795
Iteration 20: Loss = -10990.295065926512
1
Iteration 30: Loss = -10990.400803271496
2
Iteration 40: Loss = -10990.488392871388
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.0031, 0.9969],
        [0.0509, 0.9491]], dtype=torch.float64)
alpha: tensor([0.0440, 0.9560])
beta: tensor([[[0.2083, 0.2820],
         [0.8000, 0.1583]],

        [[0.5331, 0.1860],
         [0.0542, 0.0162]],

        [[0.7845, 0.1711],
         [0.1044, 0.9827]],

        [[0.1821, 0.1435],
         [0.4201, 0.2063]],

        [[0.1478, 0.2098],
         [0.0780, 0.2316]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0011260433799580283
Average Adjusted Rand Index: 0.000488518314825395
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17205.884040036435
Iteration 100: Loss = -11001.647328949528
Iteration 200: Loss = -10993.026622909865
Iteration 300: Loss = -10992.031644329614
Iteration 400: Loss = -10991.305845765044
Iteration 500: Loss = -10990.534760206852
Iteration 600: Loss = -10990.048486332775
Iteration 700: Loss = -10989.831129119615
Iteration 800: Loss = -10989.713286188578
Iteration 900: Loss = -10989.648855032832
Iteration 1000: Loss = -10989.604941536109
Iteration 1100: Loss = -10989.566835193224
Iteration 1200: Loss = -10989.546728834903
Iteration 1300: Loss = -10989.530289321816
Iteration 1400: Loss = -10989.511393387937
Iteration 1500: Loss = -10989.487061510528
Iteration 1600: Loss = -10989.477519288825
Iteration 1700: Loss = -10989.470967419593
Iteration 1800: Loss = -10989.46590643325
Iteration 1900: Loss = -10989.461608266243
Iteration 2000: Loss = -10989.457168898321
Iteration 2100: Loss = -10989.448790574128
Iteration 2200: Loss = -10989.429012570028
Iteration 2300: Loss = -10989.425137147748
Iteration 2400: Loss = -10989.422615650334
Iteration 2500: Loss = -10989.420507503957
Iteration 2600: Loss = -10989.418735298908
Iteration 2700: Loss = -10989.417148541925
Iteration 2800: Loss = -10989.415289540973
Iteration 2900: Loss = -10989.409561185394
Iteration 3000: Loss = -10989.367762571446
Iteration 3100: Loss = -10989.36520417436
Iteration 3200: Loss = -10989.363749865068
Iteration 3300: Loss = -10989.364097138878
1
Iteration 3400: Loss = -10989.36198301474
Iteration 3500: Loss = -10989.361381365092
Iteration 3600: Loss = -10989.361133940445
Iteration 3700: Loss = -10989.360467689547
Iteration 3800: Loss = -10989.360093043439
Iteration 3900: Loss = -10989.359783014916
Iteration 4000: Loss = -10989.359430705419
Iteration 4100: Loss = -10989.359105797646
Iteration 4200: Loss = -10989.35883268287
Iteration 4300: Loss = -10989.358599428226
Iteration 4400: Loss = -10989.362372841048
1
Iteration 4500: Loss = -10989.358055543904
Iteration 4600: Loss = -10989.357821151229
Iteration 4700: Loss = -10989.358233877592
1
Iteration 4800: Loss = -10989.357408102269
Iteration 4900: Loss = -10989.357127588262
Iteration 5000: Loss = -10989.361982682427
1
Iteration 5100: Loss = -10989.356762474501
Iteration 5200: Loss = -10989.356544401606
Iteration 5300: Loss = -10989.35639171464
Iteration 5400: Loss = -10989.356225325977
Iteration 5500: Loss = -10989.356011226335
Iteration 5600: Loss = -10989.356964872946
1
Iteration 5700: Loss = -10989.355688232434
Iteration 5800: Loss = -10989.356202632416
1
Iteration 5900: Loss = -10989.355419695621
Iteration 6000: Loss = -10989.437291658638
1
Iteration 6100: Loss = -10989.35512173399
Iteration 6200: Loss = -10989.355008232427
Iteration 6300: Loss = -10989.379466130902
1
Iteration 6400: Loss = -10989.354744505283
Iteration 6500: Loss = -10989.354616291525
Iteration 6600: Loss = -10989.517390183582
1
Iteration 6700: Loss = -10989.354440208157
Iteration 6800: Loss = -10989.354307120699
Iteration 6900: Loss = -10989.356228355311
1
Iteration 7000: Loss = -10989.354134689605
Iteration 7100: Loss = -10989.35404826142
Iteration 7200: Loss = -10989.353924730527
Iteration 7300: Loss = -10989.354037807534
1
Iteration 7400: Loss = -10989.353822833304
Iteration 7500: Loss = -10989.35372066954
Iteration 7600: Loss = -10989.353722462572
1
Iteration 7700: Loss = -10989.353591665427
Iteration 7800: Loss = -10989.35352093933
Iteration 7900: Loss = -10989.354392985315
1
Iteration 8000: Loss = -10989.353430171397
Iteration 8100: Loss = -10989.353343666302
Iteration 8200: Loss = -10989.357775401924
1
Iteration 8300: Loss = -10989.353252953491
Iteration 8400: Loss = -10989.35321906642
Iteration 8500: Loss = -10989.353452206191
1
Iteration 8600: Loss = -10989.353087915995
Iteration 8700: Loss = -10989.353032295685
Iteration 8800: Loss = -10989.358929743832
1
Iteration 8900: Loss = -10989.352989689269
Iteration 9000: Loss = -10989.352938563165
Iteration 9100: Loss = -10989.37262086683
1
Iteration 9200: Loss = -10989.352884928101
Iteration 9300: Loss = -10989.35284190302
Iteration 9400: Loss = -10989.499979160337
1
Iteration 9500: Loss = -10989.352790214802
Iteration 9600: Loss = -10989.352767613025
Iteration 9700: Loss = -10989.352745555734
Iteration 9800: Loss = -10989.352743234773
Iteration 9900: Loss = -10989.352697377864
Iteration 10000: Loss = -10989.352676776327
Iteration 10100: Loss = -10989.35330246003
1
Iteration 10200: Loss = -10989.352643410648
Iteration 10300: Loss = -10989.35276715219
1
Iteration 10400: Loss = -10989.35268447485
2
Iteration 10500: Loss = -10989.548829159852
3
Iteration 10600: Loss = -10989.352543586963
Iteration 10700: Loss = -10989.446315503228
1
Iteration 10800: Loss = -10989.352541610036
Iteration 10900: Loss = -10989.352498534789
Iteration 11000: Loss = -10989.35266654512
1
Iteration 11100: Loss = -10989.352490446923
Iteration 11200: Loss = -10989.436941386974
1
Iteration 11300: Loss = -10989.352455816372
Iteration 11400: Loss = -10989.353613293459
1
Iteration 11500: Loss = -10989.381678062411
2
Iteration 11600: Loss = -10989.352437757883
Iteration 11700: Loss = -10989.352460854881
1
Iteration 11800: Loss = -10989.352402854458
Iteration 11900: Loss = -10989.352360288378
Iteration 12000: Loss = -10989.352368530243
1
Iteration 12100: Loss = -10989.352384930951
2
Iteration 12200: Loss = -10989.352348675091
Iteration 12300: Loss = -10989.352379889351
1
Iteration 12400: Loss = -10989.352398726907
2
Iteration 12500: Loss = -10989.37199001858
3
Iteration 12600: Loss = -10989.352317895156
Iteration 12700: Loss = -10989.442420744854
1
Iteration 12800: Loss = -10989.352310746508
Iteration 12900: Loss = -10989.411106175043
1
Iteration 13000: Loss = -10989.352369885024
2
Iteration 13100: Loss = -10989.352404883626
3
Iteration 13200: Loss = -10989.352984816831
4
Iteration 13300: Loss = -10989.358744710773
5
Iteration 13400: Loss = -10989.35236775184
6
Iteration 13500: Loss = -10989.352901399468
7
Iteration 13600: Loss = -10989.352303239217
Iteration 13700: Loss = -10989.352953505417
1
Iteration 13800: Loss = -10989.416399550917
2
Iteration 13900: Loss = -10989.352285742296
Iteration 14000: Loss = -10989.39948837797
1
Iteration 14100: Loss = -10989.352292042518
2
Iteration 14200: Loss = -10989.352933600088
3
Iteration 14300: Loss = -10989.354293813436
4
Iteration 14400: Loss = -10989.386498649601
5
Iteration 14500: Loss = -10989.352271278782
Iteration 14600: Loss = -10989.529864405513
1
Iteration 14700: Loss = -10989.352277163962
2
Iteration 14800: Loss = -10989.352347997661
3
Iteration 14900: Loss = -10989.352274289333
4
Iteration 15000: Loss = -10989.352740675833
5
Iteration 15100: Loss = -10989.352276437368
6
Iteration 15200: Loss = -10989.381736517687
7
Iteration 15300: Loss = -10989.352258773863
Iteration 15400: Loss = -10989.373783956034
1
Iteration 15500: Loss = -10989.352263405317
2
Iteration 15600: Loss = -10989.352688807861
3
Iteration 15700: Loss = -10989.352372131816
4
Iteration 15800: Loss = -10989.353472199167
5
Iteration 15900: Loss = -10989.352781714084
6
Iteration 16000: Loss = -10989.352255127496
Iteration 16100: Loss = -10989.352553803654
1
Iteration 16200: Loss = -10989.352337447915
2
Iteration 16300: Loss = -10989.354798468079
3
Iteration 16400: Loss = -10989.35230673505
4
Iteration 16500: Loss = -10989.352340600833
5
Iteration 16600: Loss = -10989.352212416234
Iteration 16700: Loss = -10989.352824387615
1
Iteration 16800: Loss = -10989.35232442895
2
Iteration 16900: Loss = -10989.354473106412
3
Iteration 17000: Loss = -10989.387843175053
4
Iteration 17100: Loss = -10989.354006203635
5
Iteration 17200: Loss = -10989.357738794375
6
Iteration 17300: Loss = -10989.352220308056
7
Iteration 17400: Loss = -10989.37390029063
8
Iteration 17500: Loss = -10989.352224258118
9
Iteration 17600: Loss = -10989.353407575121
10
Stopping early at iteration 17600 due to no improvement.
tensor([[-8.2960,  6.7678],
        [-7.1132,  5.7033],
        [-6.7538,  2.6530],
        [-4.7104,  3.0602],
        [-6.5020,  4.5678],
        [-6.0836,  3.8517],
        [-4.5321,  3.1305],
        [-6.6155,  4.9835],
        [-5.4247,  3.7250],
        [-5.7780,  4.2921],
        [-3.7523,  2.1735],
        [-2.6815,  0.7380],
        [-4.0382,  2.5137],
        [-7.3117,  5.1973],
        [-7.0087,  4.2387],
        [-4.0472,  2.5545],
        [-4.1245,  1.8511],
        [-5.7525,  3.5833],
        [-6.2425,  4.8558],
        [-2.9909,  1.5917],
        [-5.2195,  1.9054],
        [-5.8255,  4.4392],
        [-6.8555,  4.7766],
        [-6.7959,  5.1208],
        [-5.6786,  4.2921],
        [-4.4598,  2.3552],
        [-3.6719,  2.2613],
        [-7.0091,  5.5193],
        [-5.4105,  3.9321],
        [-5.5285,  2.7069],
        [-4.1994,  2.6386],
        [-7.4700,  5.2881],
        [-7.9556,  6.5693],
        [-5.0040,  3.4897],
        [-0.8608, -0.6804],
        [-3.9123,  1.9008],
        [-5.3676,  3.7694],
        [-6.3470,  3.5883],
        [-4.8389,  3.4403],
        [-7.7239,  3.1087],
        [-4.5385,  3.1335],
        [-4.3292, -0.2295],
        [-5.8099,  4.4149],
        [-4.5152,  2.9079],
        [-7.3461,  5.9537],
        [-4.9502,  1.8506],
        [-5.2710,  3.8610],
        [-5.4004,  3.9597],
        [-7.0620,  5.6649],
        [-8.0384,  6.4946],
        [-6.1975,  3.5989],
        [-4.4996,  1.4535],
        [-9.6742,  8.0809],
        [-3.7013,  2.2741],
        [-5.1364,  2.5229],
        [-5.4360,  3.9884],
        [-4.4468,  3.0587],
        [-5.7431,  3.3585],
        [-7.5248,  6.1039],
        [-8.5390,  6.3719],
        [ 1.4673, -3.3125],
        [-4.1656,  2.5943],
        [-7.8748,  6.3700],
        [-6.0335,  4.1455],
        [-5.8497,  4.3439],
        [-4.4132,  2.7737],
        [-8.1782,  6.3793],
        [-6.2926,  1.6837],
        [-6.4822,  4.5832],
        [-5.5090,  3.8991],
        [-3.7083,  2.0761],
        [-4.1471,  2.6728],
        [-5.3259,  3.7402],
        [-5.8088,  4.4003],
        [-5.1357,  2.6147],
        [-6.6203,  5.0821],
        [-5.8633,  4.4077],
        [-6.1182,  4.6315],
        [-5.0610,  3.4264],
        [-4.5454,  3.0976],
        [-6.5459,  4.5303],
        [-5.4072,  2.8564],
        [-6.9471,  5.5491],
        [-7.0378,  5.5401],
        [-4.8060,  3.1807],
        [-8.6985,  7.2556],
        [-3.0210,  1.5565],
        [-4.7329,  3.3229],
        [-5.6911,  3.7283],
        [-5.4227,  3.6417],
        [-7.9189,  6.2415],
        [-7.2249,  5.5360],
        [-4.8761,  1.4233],
        [-8.6016,  4.8510],
        [-6.7669,  3.4209],
        [-5.1614,  3.3930],
        [-5.2739,  2.9232],
        [-7.4927,  6.1037],
        [-6.2371,  4.3562],
        [-4.1049,  2.5160]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.8691e-06, 9.9999e-01],
        [5.1224e-02, 9.4878e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0155, 0.9845], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2020, 0.3098],
         [0.8000, 0.1607]],

        [[0.5331, 0.1817],
         [0.0542, 0.0162]],

        [[0.7845, 0.1691],
         [0.1044, 0.9827]],

        [[0.1821, 0.1372],
         [0.4201, 0.2063]],

        [[0.1478, 0.2100],
         [0.0780, 0.2316]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0005469993372002862
Average Adjusted Rand Index: 0.0001633280491905654
Iteration 0: Loss = -22553.656024747943
Iteration 10: Loss = -10992.260850848594
Iteration 20: Loss = -10992.163369937598
Iteration 30: Loss = -10992.146399458214
Iteration 40: Loss = -10992.130543912184
Iteration 50: Loss = -10992.111862264344
Iteration 60: Loss = -10992.090269159698
Iteration 70: Loss = -10992.065863428852
Iteration 80: Loss = -10992.038672671759
Iteration 90: Loss = -10992.008644226245
Iteration 100: Loss = -10991.975605941909
Iteration 110: Loss = -10991.939387123859
Iteration 120: Loss = -10991.899810138146
Iteration 130: Loss = -10991.85664586095
Iteration 140: Loss = -10991.809657503043
Iteration 150: Loss = -10991.758598618011
Iteration 160: Loss = -10991.703402224162
Iteration 170: Loss = -10991.644142386811
Iteration 180: Loss = -10991.581064490272
Iteration 190: Loss = -10991.514778583622
Iteration 200: Loss = -10991.446173441427
Iteration 210: Loss = -10991.376245092002
Iteration 220: Loss = -10991.306184063644
Iteration 230: Loss = -10991.236927809348
Iteration 240: Loss = -10991.169182308937
Iteration 250: Loss = -10991.10361997966
Iteration 260: Loss = -10991.040977585255
Iteration 270: Loss = -10990.982006512904
Iteration 280: Loss = -10990.927849672606
Iteration 290: Loss = -10990.879612417119
Iteration 300: Loss = -10990.83844875841
Iteration 310: Loss = -10990.80497474665
Iteration 320: Loss = -10990.779459726926
Iteration 330: Loss = -10990.761368790349
Iteration 340: Loss = -10990.749681179028
Iteration 350: Loss = -10990.742950266591
Iteration 360: Loss = -10990.739749274857
Iteration 370: Loss = -10990.73876996409
Iteration 380: Loss = -10990.738879924138
1
Iteration 390: Loss = -10990.73952595883
2
Iteration 400: Loss = -10990.740296897895
3
Stopping early at iteration 399 due to no improvement.
pi: tensor([[7.4190e-04, 9.9926e-01],
        [6.0051e-02, 9.3995e-01]], dtype=torch.float64)
alpha: tensor([0.0535, 0.9465])
beta: tensor([[[0.2325, 0.2488],
         [0.0848, 0.1572]],

        [[0.1697, 0.1906],
         [0.7964, 0.0362]],

        [[0.2710, 0.1768],
         [0.0034, 0.8383]],

        [[0.9090, 0.1548],
         [0.4911, 0.4737]],

        [[0.3327, 0.2088],
         [0.4259, 0.7051]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0011260433799580283
Average Adjusted Rand Index: 0.000488518314825395
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22553.427204815314
Iteration 100: Loss = -11150.215994586268
Iteration 200: Loss = -11114.41059073449
Iteration 300: Loss = -11080.32818011679
Iteration 400: Loss = -11045.744684776395
Iteration 500: Loss = -11013.70773549702
Iteration 600: Loss = -11001.32199023882
Iteration 700: Loss = -10995.976765711912
Iteration 800: Loss = -10994.803224795283
Iteration 900: Loss = -10994.173777000287
Iteration 1000: Loss = -10993.820912329234
Iteration 1100: Loss = -10993.569088956547
Iteration 1200: Loss = -10993.37644361467
Iteration 1300: Loss = -10993.168235451752
Iteration 1400: Loss = -10992.12533238816
Iteration 1500: Loss = -10989.797824270036
Iteration 1600: Loss = -10989.02014460781
Iteration 1700: Loss = -10988.64501687154
Iteration 1800: Loss = -10988.436454010798
Iteration 1900: Loss = -10988.323423877308
Iteration 2000: Loss = -10988.25403531921
Iteration 2100: Loss = -10988.205878349936
Iteration 2200: Loss = -10988.170414743854
Iteration 2300: Loss = -10988.143053508444
Iteration 2400: Loss = -10988.121241676197
Iteration 2500: Loss = -10988.103434431256
Iteration 2600: Loss = -10988.088419029946
Iteration 2700: Loss = -10988.075350810595
Iteration 2800: Loss = -10988.063728348103
Iteration 2900: Loss = -10988.053798701794
Iteration 3000: Loss = -10988.045549811974
Iteration 3100: Loss = -10988.038288939795
Iteration 3200: Loss = -10988.031757215438
Iteration 3300: Loss = -10988.02574774253
Iteration 3400: Loss = -10988.020111044754
Iteration 3500: Loss = -10988.01470524281
Iteration 3600: Loss = -10988.009444016134
Iteration 3700: Loss = -10988.004539801475
Iteration 3800: Loss = -10988.000249849203
Iteration 3900: Loss = -10987.996930870722
Iteration 4000: Loss = -10987.994472818746
Iteration 4100: Loss = -10987.992395631725
Iteration 4200: Loss = -10987.990569180514
Iteration 4300: Loss = -10988.099045925648
1
Iteration 4400: Loss = -10987.987502502738
Iteration 4500: Loss = -10987.986158800397
Iteration 4600: Loss = -10987.98488815802
Iteration 4700: Loss = -10987.988067096258
1
Iteration 4800: Loss = -10987.982577059893
Iteration 4900: Loss = -10987.981505012858
Iteration 5000: Loss = -10988.029536245722
1
Iteration 5100: Loss = -10987.979469338903
Iteration 5200: Loss = -10987.978530837489
Iteration 5300: Loss = -10987.978388392374
Iteration 5400: Loss = -10987.97664155405
Iteration 5500: Loss = -10987.975736361748
Iteration 5600: Loss = -10987.974918542115
Iteration 5700: Loss = -10987.97436274024
Iteration 5800: Loss = -10987.97345925938
Iteration 5900: Loss = -10987.972771052626
Iteration 6000: Loss = -10988.035429257638
1
Iteration 6100: Loss = -10987.971346944061
Iteration 6200: Loss = -10987.970080672208
Iteration 6300: Loss = -10988.000053160851
1
Iteration 6400: Loss = -10987.96691327142
Iteration 6500: Loss = -10987.985277525797
1
Iteration 6600: Loss = -10987.966035489151
Iteration 6700: Loss = -10987.981158508494
1
Iteration 6800: Loss = -10988.001964173127
2
Iteration 6900: Loss = -10987.964824409366
Iteration 7000: Loss = -10987.995008815913
1
Iteration 7100: Loss = -10987.964212768173
Iteration 7200: Loss = -10988.223979533166
1
Iteration 7300: Loss = -10987.96362009072
Iteration 7400: Loss = -10987.963345338901
Iteration 7500: Loss = -10987.964802275594
1
Iteration 7600: Loss = -10987.963510303176
2
Iteration 7700: Loss = -10987.96262906449
Iteration 7800: Loss = -10987.96298736421
1
Iteration 7900: Loss = -10987.962178720418
Iteration 8000: Loss = -10987.962211184285
1
Iteration 8100: Loss = -10988.009385627774
2
Iteration 8200: Loss = -10987.962111219998
Iteration 8300: Loss = -10987.970235420687
1
Iteration 8400: Loss = -10987.96488057835
2
Iteration 8500: Loss = -10987.974567144016
3
Iteration 8600: Loss = -10987.960770971813
Iteration 8700: Loss = -10988.029411477015
1
Iteration 8800: Loss = -10987.960227397516
Iteration 8900: Loss = -10988.143516095708
1
Iteration 9000: Loss = -10987.959792111466
Iteration 9100: Loss = -10987.959615451226
Iteration 9200: Loss = -10987.959515850136
Iteration 9300: Loss = -10987.95920351564
Iteration 9400: Loss = -10987.959771502254
1
Iteration 9500: Loss = -10987.957928122703
Iteration 9600: Loss = -10987.972457175047
1
Iteration 9700: Loss = -10987.952508264441
Iteration 9800: Loss = -10988.03003115427
1
Iteration 9900: Loss = -10987.952290011988
Iteration 10000: Loss = -10987.952946555866
1
Iteration 10100: Loss = -10987.952003812406
Iteration 10200: Loss = -10987.951913752784
Iteration 10300: Loss = -10987.95184614324
Iteration 10400: Loss = -10987.951828113497
Iteration 10500: Loss = -10987.951679019185
Iteration 10600: Loss = -10987.9517476981
1
Iteration 10700: Loss = -10987.951553526424
Iteration 10800: Loss = -10988.143255074274
1
Iteration 10900: Loss = -10987.95145875273
Iteration 11000: Loss = -10987.951433050423
Iteration 11100: Loss = -10988.225475156067
1
Iteration 11200: Loss = -10987.951296109612
Iteration 11300: Loss = -10987.951293922064
Iteration 11400: Loss = -10987.951383592386
1
Iteration 11500: Loss = -10987.95234210261
2
Iteration 11600: Loss = -10988.007444274856
3
Iteration 11700: Loss = -10987.950973731386
Iteration 11800: Loss = -10987.9512982826
1
Iteration 11900: Loss = -10987.951065607107
2
Iteration 12000: Loss = -10987.950830843774
Iteration 12100: Loss = -10988.022196952295
1
Iteration 12200: Loss = -10987.95075180575
Iteration 12300: Loss = -10987.962048369393
1
Iteration 12400: Loss = -10987.950694491783
Iteration 12500: Loss = -10987.9540005306
1
Iteration 12600: Loss = -10987.950661614428
Iteration 12700: Loss = -10987.95060769959
Iteration 12800: Loss = -10987.955822987864
1
Iteration 12900: Loss = -10987.950600460012
Iteration 13000: Loss = -10987.950590024479
Iteration 13100: Loss = -10987.954611574869
1
Iteration 13200: Loss = -10987.950571897723
Iteration 13300: Loss = -10987.951034275928
1
Iteration 13400: Loss = -10987.95058830304
2
Iteration 13500: Loss = -10987.9505073183
Iteration 13600: Loss = -10987.95877952642
1
Iteration 13700: Loss = -10987.950546780876
2
Iteration 13800: Loss = -10987.951040378119
3
Iteration 13900: Loss = -10987.950487013775
Iteration 14000: Loss = -10987.950540683358
1
Iteration 14100: Loss = -10987.950487478382
2
Iteration 14200: Loss = -10987.951199024283
3
Iteration 14300: Loss = -10987.950443925143
Iteration 14400: Loss = -10987.950580333625
1
Iteration 14500: Loss = -10987.950456154947
2
Iteration 14600: Loss = -10987.956677663406
3
Iteration 14700: Loss = -10987.950418175542
Iteration 14800: Loss = -10987.950411973396
Iteration 14900: Loss = -10987.984285173981
1
Iteration 15000: Loss = -10987.950419018442
2
Iteration 15100: Loss = -10987.950392251589
Iteration 15200: Loss = -10987.966295835631
1
Iteration 15300: Loss = -10987.950420254323
2
Iteration 15400: Loss = -10987.950399587526
3
Iteration 15500: Loss = -10988.339066708222
4
Iteration 15600: Loss = -10987.950372601952
Iteration 15700: Loss = -10987.950379370022
1
Iteration 15800: Loss = -10987.954319972376
2
Iteration 15900: Loss = -10987.950364990957
Iteration 16000: Loss = -10987.95035426012
Iteration 16100: Loss = -10987.96797730497
1
Iteration 16200: Loss = -10987.95038198507
2
Iteration 16300: Loss = -10987.950368097587
3
Iteration 16400: Loss = -10987.95163732404
4
Iteration 16500: Loss = -10987.950356722755
5
Iteration 16600: Loss = -10987.950363145026
6
Iteration 16700: Loss = -10987.950445743749
7
Iteration 16800: Loss = -10987.950333533061
Iteration 16900: Loss = -10988.392869049698
1
Iteration 17000: Loss = -10987.950349896391
2
Iteration 17100: Loss = -10987.950342044149
3
Iteration 17200: Loss = -10988.065088817331
4
Iteration 17300: Loss = -10987.950318657602
Iteration 17400: Loss = -10987.950332987295
1
Iteration 17500: Loss = -10987.95233184654
2
Iteration 17600: Loss = -10987.950400470732
3
Iteration 17700: Loss = -10987.950324885556
4
Iteration 17800: Loss = -10987.950358563201
5
Iteration 17900: Loss = -10987.950918396971
6
Iteration 18000: Loss = -10987.95034025525
7
Iteration 18100: Loss = -10987.95032907066
8
Iteration 18200: Loss = -10987.950569075372
9
Iteration 18300: Loss = -10987.950326775905
10
Stopping early at iteration 18300 due to no improvement.
tensor([[  9.4222, -10.8178],
        [  8.7513, -10.1511],
        [  7.7511,  -9.4545],
        [  8.8880, -11.3717],
        [  9.0009, -10.7984],
        [  8.5368, -11.1541],
        [  8.1703,  -9.5593],
        [  9.2876, -12.7696],
        [  8.2261, -10.6589],
        [  9.0379, -10.4303],
        [  8.5811, -10.2734],
        [  7.9821, -10.1097],
        [  5.0586,  -9.0017],
        [  8.4953,  -9.9780],
        [  8.4951, -10.2785],
        [  7.9813,  -9.3805],
        [  8.3414, -11.3136],
        [  8.0792,  -9.5344],
        [  9.1830, -10.6319],
        [  8.2952,  -9.7437],
        [  8.6421, -10.5762],
        [  8.2408, -10.7656],
        [  7.2401,  -8.6521],
        [  7.4851, -10.6403],
        [  9.2447, -10.6440],
        [  9.0387, -10.4954],
        [  7.9832,  -9.4534],
        [  9.5689, -11.2660],
        [  8.9326, -10.3195],
        [  8.5284, -10.4344],
        [  8.7316, -10.1844],
        [  9.0383, -11.1559],
        [  9.0234, -11.8181],
        [  9.2230, -11.0705],
        [  7.7761,  -9.5992],
        [  8.1761, -10.6232],
        [  7.5012, -11.5408],
        [  8.4631,  -9.8586],
        [  8.2385,  -9.8799],
        [  9.2132, -10.6698],
        [  8.4795,  -9.9122],
        [  7.3043,  -9.0012],
        [  9.2102, -10.8674],
        [  7.9804, -12.1378],
        [  8.5150,  -9.9134],
        [  8.0883,  -9.5018],
        [  8.8810, -10.3612],
        [  8.0624,  -9.5097],
        [  9.4986, -11.1951],
        [  9.4985, -11.5284],
        [  8.6694, -10.3105],
        [  8.2041,  -9.7051],
        [  9.0709, -10.4690],
        [  9.1894, -10.6407],
        [  8.1407, -10.4610],
        [  8.5804,  -9.9773],
        [  8.6963, -10.3444],
        [  8.5530, -10.0729],
        [  8.7268, -11.8042],
        [  7.3391, -11.9543],
        [  8.1219,  -9.5226],
        [  8.0418,  -9.6463],
        [  9.4523, -10.8545],
        [  9.3859, -10.7815],
        [  8.5668,  -9.9541],
        [  8.8679, -10.3412],
        [  8.6388, -10.2012],
        [  8.4423,  -9.9044],
        [  8.4558, -10.0047],
        [  8.6239, -11.3073],
        [  9.0001, -11.0241],
        [  7.9256, -11.7553],
        [  9.1284, -10.9392],
        [  9.3400, -11.3613],
        [  7.7352,  -9.3233],
        [  9.4533, -11.3105],
        [  8.5495,  -9.9733],
        [  9.0511, -10.9057],
        [  9.0531, -10.6776],
        [  9.0484, -10.4417],
        [  9.0281, -10.7133],
        [  7.6760,  -9.0790],
        [  9.0887, -10.5553],
        [  8.3260,  -9.8207],
        [  8.6105, -10.9438],
        [  7.9476, -10.2237],
        [  8.1459, -10.2301],
        [  8.4689,  -9.9628],
        [  8.6474, -10.0351],
        [  9.0352, -10.4217],
        [  9.4725, -11.2378],
        [  8.1070,  -9.5174],
        [  8.1961,  -9.6317],
        [  9.0081, -10.9648],
        [  9.3123, -12.1452],
        [  8.2847, -10.1033],
        [  9.1218, -12.2440],
        [  8.8472, -10.3907],
        [  7.9275, -10.8066],
        [  7.9466,  -9.3331]], dtype=torch.float64, requires_grad=True)
pi: tensor([[2.3591e-02, 9.7641e-01],
        [1.0000e+00, 1.3574e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 2.5633e-08], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1686, 0.1958],
         [0.0848, 0.1616]],

        [[0.1697, 0.1426],
         [0.7964, 0.0362]],

        [[0.2710, 0.0914],
         [0.0034, 0.8383]],

        [[0.9090, 0.1036],
         [0.4911, 0.4737]],

        [[0.3327, 0.1104],
         [0.4259, 0.7051]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 58
Adjusted Rand Index: -0.012494332208171696
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
Global Adjusted Rand Index: -0.002020008845009144
Average Adjusted Rand Index: -0.003417031662930823
Iteration 0: Loss = -22508.137983305365
Iteration 10: Loss = -10991.46867241109
Iteration 20: Loss = -10990.924593318767
Iteration 30: Loss = -10989.990322826503
Iteration 40: Loss = -10989.635704022274
Iteration 50: Loss = -10989.580668202956
Iteration 60: Loss = -10989.586262681913
1
Iteration 70: Loss = -10989.617092683393
2
Iteration 80: Loss = -10989.658080197443
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[9.6983e-01, 3.0169e-02],
        [1.0000e+00, 2.7532e-13]], dtype=torch.float64)
alpha: tensor([0.9728, 0.0272])
beta: tensor([[[0.1605, 0.3018],
         [0.2128, 0.1537]],

        [[0.4455, 0.1716],
         [0.7055, 0.6317]],

        [[0.6614, 0.0997],
         [0.6721, 0.1737]],

        [[0.7810, 0.1194],
         [0.4627, 0.8495]],

        [[0.7917, 0.2138],
         [0.1227, 0.5372]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0011260433799580283
Average Adjusted Rand Index: 0.000488518314825395
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22507.606393900467
Iteration 100: Loss = -10997.387341647947
Iteration 200: Loss = -10983.273780008072
Iteration 300: Loss = -10980.615123299467
Iteration 400: Loss = -10980.213121320761
Iteration 500: Loss = -10980.014080110426
Iteration 600: Loss = -10979.890160524117
Iteration 700: Loss = -10979.786439016427
Iteration 800: Loss = -10979.671161097762
Iteration 900: Loss = -10979.55258491302
Iteration 1000: Loss = -10979.448871979423
Iteration 1100: Loss = -10979.36132056007
Iteration 1200: Loss = -10979.280791599578
Iteration 1300: Loss = -10979.20427502601
Iteration 1400: Loss = -10979.131171812485
Iteration 1500: Loss = -10979.062729850364
Iteration 1600: Loss = -10978.961730166266
Iteration 1700: Loss = -10978.788132646534
Iteration 1800: Loss = -10978.685156718466
Iteration 1900: Loss = -10978.624822665884
Iteration 2000: Loss = -10978.586893933047
Iteration 2100: Loss = -10978.56168932588
Iteration 2200: Loss = -10978.544079225541
Iteration 2300: Loss = -10978.53126300102
Iteration 2400: Loss = -10978.521596199003
Iteration 2500: Loss = -10978.514018714479
Iteration 2600: Loss = -10978.508002884577
Iteration 2700: Loss = -10978.503043033315
Iteration 2800: Loss = -10978.498939358744
Iteration 2900: Loss = -10978.49554167622
Iteration 3000: Loss = -10978.492645262926
Iteration 3100: Loss = -10978.490125996213
Iteration 3200: Loss = -10978.487972735415
Iteration 3300: Loss = -10978.486072312096
Iteration 3400: Loss = -10978.484397463659
Iteration 3500: Loss = -10978.48290866778
Iteration 3600: Loss = -10978.481597788417
Iteration 3700: Loss = -10978.480400807972
Iteration 3800: Loss = -10978.479332830962
Iteration 3900: Loss = -10978.478352013659
Iteration 4000: Loss = -10978.477458118337
Iteration 4100: Loss = -10978.476631124327
Iteration 4200: Loss = -10978.47591220805
Iteration 4300: Loss = -10978.475215637378
Iteration 4400: Loss = -10978.474558070176
Iteration 4500: Loss = -10978.474021471055
Iteration 4600: Loss = -10978.47344755242
Iteration 4700: Loss = -10978.472944150953
Iteration 4800: Loss = -10978.472443805751
Iteration 4900: Loss = -10978.471935831096
Iteration 5000: Loss = -10978.471348798554
Iteration 5100: Loss = -10978.46973447377
Iteration 5200: Loss = -10978.465814879875
Iteration 5300: Loss = -10978.465302491939
Iteration 5400: Loss = -10978.464963960598
Iteration 5500: Loss = -10978.464681413046
Iteration 5600: Loss = -10978.464361422448
Iteration 5700: Loss = -10978.464279527048
Iteration 5800: Loss = -10978.463877126618
Iteration 5900: Loss = -10978.463655861575
Iteration 6000: Loss = -10978.463458155904
Iteration 6100: Loss = -10978.467657207937
1
Iteration 6200: Loss = -10978.463060777969
Iteration 6300: Loss = -10978.46290759609
Iteration 6400: Loss = -10978.462694313585
Iteration 6500: Loss = -10978.46256481298
Iteration 6600: Loss = -10978.462791951406
1
Iteration 6700: Loss = -10978.462284148587
Iteration 6800: Loss = -10978.46214968131
Iteration 6900: Loss = -10978.462073172086
Iteration 7000: Loss = -10978.461872624737
Iteration 7100: Loss = -10978.46185229143
Iteration 7200: Loss = -10978.461649437457
Iteration 7300: Loss = -10978.46169181987
1
Iteration 7400: Loss = -10978.46148863966
Iteration 7500: Loss = -10978.46232927818
1
Iteration 7600: Loss = -10978.461276612796
Iteration 7700: Loss = -10978.46255691644
1
Iteration 7800: Loss = -10978.461175183511
Iteration 7900: Loss = -10978.461057805594
Iteration 8000: Loss = -10978.513086980598
1
Iteration 8100: Loss = -10978.460938364062
Iteration 8200: Loss = -10978.460878553986
Iteration 8300: Loss = -10978.460951780446
1
Iteration 8400: Loss = -10978.460769867066
Iteration 8500: Loss = -10978.491390342599
1
Iteration 8600: Loss = -10978.46064520831
Iteration 8700: Loss = -10978.460580419343
Iteration 8800: Loss = -10978.461552997449
1
Iteration 8900: Loss = -10978.460522039382
Iteration 9000: Loss = -10978.460447510028
Iteration 9100: Loss = -10978.460569004847
1
Iteration 9200: Loss = -10978.460387572188
Iteration 9300: Loss = -10978.46257292465
1
Iteration 9400: Loss = -10978.460450699877
2
Iteration 9500: Loss = -10978.59431105761
3
Iteration 9600: Loss = -10978.460686925317
4
Iteration 9700: Loss = -10978.498349875086
5
Iteration 9800: Loss = -10978.460278730034
Iteration 9900: Loss = -10978.460606548719
1
Iteration 10000: Loss = -10978.460139243058
Iteration 10100: Loss = -10978.46113072416
1
Iteration 10200: Loss = -10978.460079762533
Iteration 10300: Loss = -10978.462580127407
1
Iteration 10400: Loss = -10978.460105492295
2
Iteration 10500: Loss = -10978.526709209184
3
Iteration 10600: Loss = -10978.460014407678
Iteration 10700: Loss = -10978.460001113672
Iteration 10800: Loss = -10978.594301827083
1
Iteration 10900: Loss = -10978.459977261284
Iteration 11000: Loss = -10978.461272163278
1
Iteration 11100: Loss = -10978.485490845844
2
Iteration 11200: Loss = -10978.459923052837
Iteration 11300: Loss = -10978.459929263923
1
Iteration 11400: Loss = -10978.459927150257
2
Iteration 11500: Loss = -10978.459910775395
Iteration 11600: Loss = -10978.462213593082
1
Iteration 11700: Loss = -10978.460718904133
2
Iteration 11800: Loss = -10978.459845378886
Iteration 11900: Loss = -10978.45993837833
1
Iteration 12000: Loss = -10978.459837069984
Iteration 12100: Loss = -10978.537268736325
1
Iteration 12200: Loss = -10978.459841399806
2
Iteration 12300: Loss = -10978.47805420365
3
Iteration 12400: Loss = -10978.459790817958
Iteration 12500: Loss = -10978.460026941422
1
Iteration 12600: Loss = -10978.46236597858
2
Iteration 12700: Loss = -10978.459819076403
3
Iteration 12800: Loss = -10978.459899966872
4
Iteration 12900: Loss = -10978.696551356308
5
Iteration 13000: Loss = -10978.459735267057
Iteration 13100: Loss = -10978.472779243139
1
Iteration 13200: Loss = -10978.459755834883
2
Iteration 13300: Loss = -10978.467403040666
3
Iteration 13400: Loss = -10978.46323345798
4
Iteration 13500: Loss = -10978.459751796312
5
Iteration 13600: Loss = -10978.460046321225
6
Iteration 13700: Loss = -10978.459839298408
7
Iteration 13800: Loss = -10978.549404870619
8
Iteration 13900: Loss = -10978.459756529495
9
Iteration 14000: Loss = -10978.460090608256
10
Stopping early at iteration 14000 due to no improvement.
tensor([[ 0.1742, -1.5662],
        [ 1.8565, -3.3307],
        [-0.5853, -3.2204],
        [ 1.5146, -3.2392],
        [-2.0224,  0.3149],
        [ 1.9933, -3.4039],
        [-4.9653,  3.3428],
        [ 1.4179, -4.6186],
        [-2.2633, -0.3237],
        [ 0.8771, -2.3921],
        [ 3.4455, -4.8356],
        [-5.7266,  3.6822],
        [-6.9846,  5.1702],
        [-1.4071, -0.1716],
        [ 1.6425, -3.6868],
        [ 0.0579, -1.4794],
        [ 2.5105, -3.9036],
        [ 1.7573, -3.4651],
        [ 0.8192, -2.3411],
        [ 1.5040, -3.6580],
        [-4.4965,  3.0684],
        [-2.7036,  0.1883],
        [-3.1957,  1.8073],
        [ 3.3120, -5.2791],
        [-3.4069,  0.2972],
        [ 3.8562, -5.2606],
        [ 1.0981, -5.7133],
        [-2.5887, -1.1668],
        [ 2.5503, -5.2866],
        [ 3.2038, -6.6555],
        [-2.8676,  1.1646],
        [ 0.9332, -2.4625],
        [-1.5951,  0.2086],
        [ 3.1932, -4.5836],
        [-5.6504,  4.1855],
        [-3.7192,  1.8473],
        [ 2.8023, -4.3872],
        [ 0.8827, -2.4973],
        [-2.1414,  0.4677],
        [ 2.8609, -5.7254],
        [ 2.0847, -6.5731],
        [-6.1069,  4.4912],
        [ 1.9858, -4.0567],
        [-1.9880,  0.1390],
        [-1.4050, -2.6148],
        [ 2.9736, -4.8049],
        [ 2.1530, -3.7431],
        [ 2.5834, -5.9064],
        [-0.5133, -4.1019],
        [ 4.4945, -6.0704],
        [-4.6177,  2.4160],
        [ 2.0890, -3.8336],
        [ 0.5593, -5.1746],
        [ 1.9372, -3.6418],
        [-2.8197,  0.7820],
        [ 4.2740, -5.8338],
        [-2.4164,  0.9647],
        [-3.3162,  0.1512],
        [-0.2322, -3.5293],
        [-0.2610, -2.4213],
        [-6.8761,  4.2136],
        [ 0.8199, -2.2275],
        [ 3.1067, -4.5358],
        [ 4.0333, -6.7926],
        [ 1.3161, -2.9660],
        [-4.2900,  2.4851],
        [ 0.6567, -2.0933],
        [-4.9129,  3.2086],
        [ 0.7369, -2.2680],
        [ 1.0606, -2.6590],
        [-2.5818,  1.1531],
        [ 2.7903, -4.4442],
        [-2.6063,  1.2175],
        [-1.9719,  0.0590],
        [-2.4017,  1.0153],
        [-1.1501, -0.5335],
        [ 3.6649, -5.0774],
        [-3.6240,  2.2375],
        [ 0.1881, -2.2336],
        [ 2.1676, -3.9131],
        [ 2.8994, -4.6072],
        [-4.0775,  2.2327],
        [ 2.9268, -4.3812],
        [-0.5919, -0.9253],
        [ 0.4436, -2.7449],
        [ 2.4629, -4.9903],
        [-5.8059,  4.1885],
        [-4.6122,  3.1498],
        [-3.1076,  0.2906],
        [-1.9250,  0.5353],
        [ 0.4881, -2.0664],
        [ 3.2954, -4.9644],
        [-5.8900,  4.1436],
        [-0.7746, -1.7758],
        [ 1.8476, -3.2394],
        [-2.6584,  1.2587],
        [-5.1116,  3.6772],
        [ 0.6191, -5.2344],
        [-3.7457,  2.3589],
        [-4.3366,  2.8646]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.7427e-01, 2.5735e-02],
        [1.0000e+00, 1.5275e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5877, 0.4123], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1629, 0.1134],
         [0.2128, 0.2962]],

        [[0.4455, 0.2384],
         [0.7055, 0.6317]],

        [[0.6614, 0.0946],
         [0.6721, 0.1737]],

        [[0.7810, 0.1136],
         [0.4627, 0.8495]],

        [[0.7917, 0.2216],
         [0.1227, 0.5372]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 92
Adjusted Rand Index: 0.702691493950761
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.05054876473825637
Average Adjusted Rand Index: 0.14087174434073815
Iteration 0: Loss = -13736.745964388025
Iteration 10: Loss = -10992.177698875093
Iteration 20: Loss = -10992.094367299971
Iteration 30: Loss = -10992.061553244288
Iteration 40: Loss = -10992.032429430083
Iteration 50: Loss = -10992.001369877524
Iteration 60: Loss = -10991.967550549876
Iteration 70: Loss = -10991.930604740814
Iteration 80: Loss = -10991.890189287024
Iteration 90: Loss = -10991.846122635237
Iteration 100: Loss = -10991.798224184402
Iteration 110: Loss = -10991.746187850567
Iteration 120: Loss = -10991.690042237798
Iteration 130: Loss = -10991.629881621422
Iteration 140: Loss = -10991.566049412753
Iteration 150: Loss = -10991.4991334099
Iteration 160: Loss = -10991.430105602489
Iteration 170: Loss = -10991.360051064545
Iteration 180: Loss = -10991.290139956782
Iteration 190: Loss = -10991.221122608767
Iteration 200: Loss = -10991.15384369394
Iteration 210: Loss = -10991.088872453565
Iteration 220: Loss = -10991.026993210002
Iteration 230: Loss = -10990.96906139912
Iteration 240: Loss = -10990.916157053642
Iteration 250: Loss = -10990.869484225823
Iteration 260: Loss = -10990.830020759087
Iteration 270: Loss = -10990.798410563539
Iteration 280: Loss = -10990.774677507394
Iteration 290: Loss = -10990.758136331138
Iteration 300: Loss = -10990.747742570537
Iteration 310: Loss = -10990.74196425734
Iteration 320: Loss = -10990.73934315517
Iteration 330: Loss = -10990.738700877759
Iteration 340: Loss = -10990.739038014546
1
Iteration 350: Loss = -10990.739747559988
2
Iteration 360: Loss = -10990.74044522703
3
Stopping early at iteration 359 due to no improvement.
pi: tensor([[9.3994e-01, 6.0060e-02],
        [9.9929e-01, 7.0693e-04]], dtype=torch.float64)
alpha: tensor([0.9465, 0.0535])
beta: tensor([[[0.1572, 0.2488],
         [0.7446, 0.2325]],

        [[0.7142, 0.1906],
         [0.4748, 0.5720]],

        [[0.5774, 0.1768],
         [0.6594, 0.9072]],

        [[0.4872, 0.1548],
         [0.5912, 0.1122]],

        [[0.3617, 0.2088],
         [0.1671, 0.3425]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0011260433799580283
Average Adjusted Rand Index: 0.000488518314825395
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -13736.60230852045
Iteration 100: Loss = -11002.326879709046
Iteration 200: Loss = -10992.823680333251
Iteration 300: Loss = -10991.831700044137
Iteration 400: Loss = -10991.160092367765
Iteration 500: Loss = -10990.313319525922
Iteration 600: Loss = -10989.972196559702
Iteration 700: Loss = -10989.795922493438
Iteration 800: Loss = -10989.65013105235
Iteration 900: Loss = -10989.580525447407
Iteration 1000: Loss = -10989.531759526728
Iteration 1100: Loss = -10989.500242715578
Iteration 1200: Loss = -10989.481452066679
Iteration 1300: Loss = -10989.463874613497
Iteration 1400: Loss = -10989.440430083288
Iteration 1500: Loss = -10989.431907860027
Iteration 1600: Loss = -10989.412614552222
Iteration 1700: Loss = -10989.407730939402
Iteration 1800: Loss = -10989.40520683637
Iteration 1900: Loss = -10989.403261244679
Iteration 2000: Loss = -10989.40164332057
Iteration 2100: Loss = -10989.400289909767
Iteration 2200: Loss = -10989.401483715032
1
Iteration 2300: Loss = -10989.39750051941
Iteration 2400: Loss = -10989.39103504279
Iteration 2500: Loss = -10989.392779930828
1
Iteration 2600: Loss = -10989.387845561618
Iteration 2700: Loss = -10989.38731936863
Iteration 2800: Loss = -10989.413986038902
1
Iteration 2900: Loss = -10989.38640357763
Iteration 3000: Loss = -10989.385977278633
Iteration 3100: Loss = -10989.42224303515
1
Iteration 3200: Loss = -10989.384928676838
Iteration 3300: Loss = -10989.364166639472
Iteration 3400: Loss = -10989.358584216998
Iteration 3500: Loss = -10989.358174950397
Iteration 3600: Loss = -10989.357876403801
Iteration 3700: Loss = -10989.357630344353
Iteration 3800: Loss = -10989.357665792964
1
Iteration 3900: Loss = -10989.357233292454
Iteration 4000: Loss = -10989.357026280577
Iteration 4100: Loss = -10989.371090974037
1
Iteration 4200: Loss = -10989.35667299031
Iteration 4300: Loss = -10989.356466677584
Iteration 4400: Loss = -10989.356329003906
Iteration 4500: Loss = -10989.356170104493
Iteration 4600: Loss = -10989.35597755031
Iteration 4700: Loss = -10989.355818146398
Iteration 4800: Loss = -10989.405972942428
1
Iteration 4900: Loss = -10989.355497910094
Iteration 5000: Loss = -10989.355395247774
Iteration 5100: Loss = -10989.355256164408
Iteration 5200: Loss = -10989.356197333345
1
Iteration 5300: Loss = -10989.354995341877
Iteration 5400: Loss = -10989.35483430528
Iteration 5500: Loss = -10989.400848505222
1
Iteration 5600: Loss = -10989.354637430373
Iteration 5700: Loss = -10989.354531532199
Iteration 5800: Loss = -10989.400363089864
1
Iteration 5900: Loss = -10989.354330170416
Iteration 6000: Loss = -10989.354242899273
Iteration 6100: Loss = -10989.36730981166
1
Iteration 6200: Loss = -10989.354039620168
Iteration 6300: Loss = -10989.353972318715
Iteration 6400: Loss = -10989.361392129093
1
Iteration 6500: Loss = -10989.35383387116
Iteration 6600: Loss = -10989.353695485961
Iteration 6700: Loss = -10989.3609117547
1
Iteration 6800: Loss = -10989.353608979553
Iteration 6900: Loss = -10989.353499152989
Iteration 7000: Loss = -10989.440310895718
1
Iteration 7100: Loss = -10989.353412531776
Iteration 7200: Loss = -10989.353362545653
Iteration 7300: Loss = -10989.353345204025
Iteration 7400: Loss = -10989.353295381543
Iteration 7500: Loss = -10989.353187885394
Iteration 7600: Loss = -10989.353162730024
Iteration 7700: Loss = -10989.354086196003
1
Iteration 7800: Loss = -10989.353084667762
Iteration 7900: Loss = -10989.35304280491
Iteration 8000: Loss = -10989.353192425022
1
Iteration 8100: Loss = -10989.352920300074
Iteration 8200: Loss = -10989.353074928329
1
Iteration 8300: Loss = -10989.352940651379
2
Iteration 8400: Loss = -10989.35289712434
Iteration 8500: Loss = -10989.53223022391
1
Iteration 8600: Loss = -10989.35281963347
Iteration 8700: Loss = -10989.352782412305
Iteration 8800: Loss = -10989.354118744639
1
Iteration 8900: Loss = -10989.352740891456
Iteration 9000: Loss = -10989.352698218298
Iteration 9100: Loss = -10989.365074638998
1
Iteration 9200: Loss = -10989.367787370176
2
Iteration 9300: Loss = -10989.56726485844
3
Iteration 9400: Loss = -10989.359317695018
4
Iteration 9500: Loss = -10989.355351754291
5
Iteration 9600: Loss = -10989.369047936876
6
Iteration 9700: Loss = -10989.352779020825
7
Iteration 9800: Loss = -10989.352761491815
8
Iteration 9900: Loss = -10989.353645431893
9
Iteration 10000: Loss = -10989.352629623132
Iteration 10100: Loss = -10989.354139791729
1
Iteration 10200: Loss = -10989.369739544993
2
Iteration 10300: Loss = -10989.35246226821
Iteration 10400: Loss = -10989.353371078769
1
Iteration 10500: Loss = -10989.35285265406
2
Iteration 10600: Loss = -10989.55789242522
3
Iteration 10700: Loss = -10989.352423762446
Iteration 10800: Loss = -10989.357554305101
1
Iteration 10900: Loss = -10989.35238315461
Iteration 11000: Loss = -10989.3523808494
Iteration 11100: Loss = -10989.352611277724
1
Iteration 11200: Loss = -10989.352362360934
Iteration 11300: Loss = -10989.446971635747
1
Iteration 11400: Loss = -10989.352359783457
Iteration 11500: Loss = -10989.35236878665
1
Iteration 11600: Loss = -10989.352364648508
2
Iteration 11700: Loss = -10989.352435433211
3
Iteration 11800: Loss = -10989.352325775466
Iteration 11900: Loss = -10989.360495437415
1
Iteration 12000: Loss = -10989.35233700695
2
Iteration 12100: Loss = -10989.352298702579
Iteration 12200: Loss = -10989.356156327185
1
Iteration 12300: Loss = -10989.360668890615
2
Iteration 12400: Loss = -10989.36441104345
3
Iteration 12500: Loss = -10989.361856412224
4
Iteration 12600: Loss = -10989.365042436972
5
Iteration 12700: Loss = -10989.352320301585
6
Iteration 12800: Loss = -10989.355073506398
7
Iteration 12900: Loss = -10989.394352520203
8
Iteration 13000: Loss = -10989.35231517085
9
Iteration 13100: Loss = -10989.354012283726
10
Stopping early at iteration 13100 due to no improvement.
tensor([[-8.3116,  6.8778],
        [-7.1449,  5.7301],
        [-5.4749,  3.9323],
        [-4.9188,  2.8504],
        [-6.4728,  4.5993],
        [-6.0440,  3.8887],
        [-6.0237,  1.6353],
        [-6.4932,  5.1042],
        [-5.2896,  3.8588],
        [-5.7934,  4.2750],
        [-4.4791,  1.4452],
        [-2.4693,  0.9483],
        [-4.0674,  2.4818],
        [-6.9848,  5.5245],
        [-6.6988,  4.5509],
        [-5.1469,  1.4539],
        [-5.1525,  0.8215],
        [-5.6857,  3.6489],
        [-6.2470,  4.8505],
        [-2.9985,  1.5814],
        [-4.5177,  2.6042],
        [-6.0507,  4.2125],
        [-6.6385,  4.9922],
        [-6.8236,  5.1031],
        [-6.3800,  3.5867],
        [-4.1129,  2.7023],
        [-3.7835,  2.1482],
        [-6.9795,  5.5929],
        [-5.7025,  3.6397],
        [-4.9675,  3.2666],
        [-4.3264,  2.5104],
        [-7.1542,  5.6068],
        [-8.1935,  6.3931],
        [-5.7487,  2.7420],
        [-0.9212, -0.7422],
        [-3.6482,  2.1638],
        [-5.6201,  3.5164],
        [-5.9366,  3.9963],
        [-4.8340,  3.4439],
        [-6.9721,  3.8604],
        [-4.5775,  3.0936],
        [-3.3733,  0.7252],
        [-5.8435,  4.3822],
        [-5.5717,  1.8501],
        [-7.4247,  5.9011],
        [-4.1020,  2.6975],
        [-5.2603,  3.8705],
        [-5.4254,  3.9337],
        [-7.0878,  5.6548],
        [-7.9983,  6.4459],
        [-5.8214,  3.9741],
        [-3.6823,  2.2691],
        [-9.0941,  7.5993],
        [-4.2132,  1.7605],
        [-4.7699,  2.8881],
        [-5.4412,  3.9794],
        [-4.5846,  2.9207],
        [-5.2827,  3.8154],
        [-7.7808,  5.8440],
        [-8.5443,  6.4436],
        [ 1.6210, -3.1603],
        [-4.2463,  2.5119],
        [-8.2129,  6.1065],
        [-7.2101,  2.9694],
        [-6.1906,  4.0021],
        [-5.2403,  1.9442],
        [-8.2536,  6.3298],
        [-4.7001,  3.2750],
        [-7.2109,  3.8533],
        [-5.4341,  3.9724],
        [-4.6912,  1.0920],
        [-4.2930,  2.5258],
        [-5.2319,  3.8337],
        [-5.8066,  4.3998],
        [-5.1008,  2.6497],
        [-6.5452,  5.1582],
        [-6.4991,  3.7696],
        [-6.1700,  4.5777],
        [-4.9739,  3.5123],
        [-4.6715,  2.9701],
        [-6.4128,  4.6624],
        [-5.1911,  3.0721],
        [-7.6255,  4.8679],
        [-6.9909,  5.6045],
        [-5.3915,  2.5931],
        [-8.4591,  7.0502],
        [-3.6205,  0.9542],
        [-4.8866,  3.1649],
        [-5.4925,  3.9274],
        [-5.5126,  3.5496],
        [-8.0256,  6.1494],
        [-7.1849,  5.5716],
        [-4.0387,  2.2581],
        [-7.4482,  6.0010],
        [-6.0687,  4.1194],
        [-5.5669,  2.9878],
        [-4.8836,  3.3126],
        [-7.7430,  5.9253],
        [-6.0124,  4.5783],
        [-4.2238,  2.3959]], dtype=torch.float64, requires_grad=True)
pi: tensor([[6.3233e-05, 9.9994e-01],
        [5.1353e-02, 9.4865e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0155, 0.9845], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2022, 0.3098],
         [0.7446, 0.1607]],

        [[0.7142, 0.1818],
         [0.4748, 0.5720]],

        [[0.5774, 0.1692],
         [0.6594, 0.9072]],

        [[0.4872, 0.1374],
         [0.5912, 0.1122]],

        [[0.3617, 0.2101],
         [0.1671, 0.3425]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0005469993372002862
Average Adjusted Rand Index: 0.0001633280491905654
10951.77092898354
new:  [0.0005469993372002862, -0.002020008845009144, 0.05054876473825637, 0.0005469993372002862] [0.0001633280491905654, -0.003417031662930823, 0.14087174434073815, 0.0001633280491905654] [10989.353407575121, 10987.950326775905, 10978.460090608256, 10989.354012283726]
prior:  [0.0011260433799580283, 0.0011260433799580283, 0.0011260433799580283, 0.0011260433799580283] [0.000488518314825395, 0.000488518314825395, 0.000488518314825395, 0.000488518314825395] [10990.488392871388, 10990.740296897895, 10989.658080197443, 10990.74044522703]
-----------------------------------------------------------------------------------------
This iteration is 9
True Objective function: Loss = -10846.484065894521
Iteration 0: Loss = -28459.627983174054
Iteration 10: Loss = -10885.38232220648
Iteration 20: Loss = -10885.381935622861
Iteration 30: Loss = -10885.380292402962
Iteration 40: Loss = -10885.375386859474
Iteration 50: Loss = -10885.36004724567
Iteration 60: Loss = -10885.314728154643
Iteration 70: Loss = -10885.207167762705
Iteration 80: Loss = -10885.036434313395
Iteration 90: Loss = -10884.868682401007
Iteration 100: Loss = -10884.748174200886
Iteration 110: Loss = -10884.669992195568
Iteration 120: Loss = -10884.619955235088
Iteration 130: Loss = -10884.588125823588
Iteration 140: Loss = -10884.568537326497
Iteration 150: Loss = -10884.55750839209
Iteration 160: Loss = -10884.552767947916
Iteration 170: Loss = -10884.55282813188
1
Iteration 180: Loss = -10884.556821864824
2
Iteration 190: Loss = -10884.563959508141
3
Stopping early at iteration 189 due to no improvement.
pi: tensor([[9.1266e-01, 8.7339e-02],
        [1.0000e+00, 2.1622e-15]], dtype=torch.float64)
alpha: tensor([0.9207, 0.0793])
beta: tensor([[[0.1550, 0.1530],
         [0.3493, 0.2022]],

        [[0.2222, 0.1761],
         [0.5916, 0.3811]],

        [[0.8483, 0.1888],
         [0.0698, 0.3581]],

        [[0.6198, 0.1880],
         [0.9683, 0.1506]],

        [[0.6264, 0.1756],
         [0.3773, 0.6251]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 63
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28144.128230969487
Iteration 100: Loss = -10887.784577769771
Iteration 200: Loss = -10886.19141221413
Iteration 300: Loss = -10885.874723568963
Iteration 400: Loss = -10885.689860767949
Iteration 500: Loss = -10885.539064749559
Iteration 600: Loss = -10885.368280876144
Iteration 700: Loss = -10885.177317570791
Iteration 800: Loss = -10885.164759877967
Iteration 900: Loss = -10885.10958043025
Iteration 1000: Loss = -10885.128829184587
1
Iteration 1100: Loss = -10885.095776992444
Iteration 1200: Loss = -10885.090133812024
Iteration 1300: Loss = -10885.090408217178
1
Iteration 1400: Loss = -10885.080764096092
Iteration 1500: Loss = -10885.076645121648
Iteration 1600: Loss = -10885.085053481698
1
Iteration 1700: Loss = -10885.069068399427
Iteration 1800: Loss = -10885.065343851016
Iteration 1900: Loss = -10885.318924589235
1
Iteration 2000: Loss = -10885.05717218112
Iteration 2100: Loss = -10885.052192986073
Iteration 2200: Loss = -10885.04581135836
Iteration 2300: Loss = -10885.047662394745
1
Iteration 2400: Loss = -10885.022081437683
Iteration 2500: Loss = -10884.99262032888
Iteration 2600: Loss = -10884.924278731156
Iteration 2700: Loss = -10884.786658954257
Iteration 2800: Loss = -10884.701047255516
Iteration 2900: Loss = -10884.4055262204
Iteration 3000: Loss = -10883.986421346075
Iteration 3100: Loss = -10883.787408941631
Iteration 3200: Loss = -10883.707885848522
Iteration 3300: Loss = -10883.680512345783
Iteration 3400: Loss = -10883.680986196265
1
Iteration 3500: Loss = -10883.655805332794
Iteration 3600: Loss = -10883.647388290914
Iteration 3700: Loss = -10883.655921892352
1
Iteration 3800: Loss = -10883.633540341083
Iteration 3900: Loss = -10883.643669556473
1
Iteration 4000: Loss = -10883.632406468605
Iteration 4100: Loss = -10883.633419503189
1
Iteration 4200: Loss = -10883.663572965314
2
Iteration 4300: Loss = -10883.610340185283
Iteration 4400: Loss = -10883.610012387298
Iteration 4500: Loss = -10883.603979141439
Iteration 4600: Loss = -10883.746679027721
1
Iteration 4700: Loss = -10883.598769844164
Iteration 4800: Loss = -10883.59643322175
Iteration 4900: Loss = -10883.594574711786
Iteration 5000: Loss = -10883.592439567095
Iteration 5100: Loss = -10883.616503437175
1
Iteration 5200: Loss = -10883.733172215
2
Iteration 5300: Loss = -10883.587593650836
Iteration 5400: Loss = -10883.58648439613
Iteration 5500: Loss = -10883.647546608912
1
Iteration 5600: Loss = -10883.583762073105
Iteration 5700: Loss = -10883.78185705587
1
Iteration 5800: Loss = -10883.581628518492
Iteration 5900: Loss = -10883.5835440079
1
Iteration 6000: Loss = -10883.579805045427
Iteration 6100: Loss = -10883.581127370999
1
Iteration 6200: Loss = -10883.57819186504
Iteration 6300: Loss = -10883.5899189943
1
Iteration 6400: Loss = -10883.576848933131
Iteration 6500: Loss = -10883.576180121327
Iteration 6600: Loss = -10883.577802038231
1
Iteration 6700: Loss = -10883.575062370279
Iteration 6800: Loss = -10883.576182618126
1
Iteration 6900: Loss = -10883.57504333839
Iteration 7000: Loss = -10883.573732992098
Iteration 7100: Loss = -10883.643779550834
1
Iteration 7200: Loss = -10883.573011342478
Iteration 7300: Loss = -10883.572389924924
Iteration 7400: Loss = -10883.576726761365
1
Iteration 7500: Loss = -10883.640687102736
2
Iteration 7600: Loss = -10883.57142814026
Iteration 7700: Loss = -10883.784262667632
1
Iteration 7800: Loss = -10883.570836499252
Iteration 7900: Loss = -10883.578386208717
1
Iteration 8000: Loss = -10883.578483357824
2
Iteration 8100: Loss = -10883.570100392157
Iteration 8200: Loss = -10883.57082925219
1
Iteration 8300: Loss = -10883.599134390517
2
Iteration 8400: Loss = -10883.569465234257
Iteration 8500: Loss = -10883.58598225884
1
Iteration 8600: Loss = -10883.56908784592
Iteration 8700: Loss = -10883.569006610498
Iteration 8800: Loss = -10883.570633094709
1
Iteration 8900: Loss = -10883.568639921545
Iteration 9000: Loss = -10883.603876415895
1
Iteration 9100: Loss = -10883.584835011108
2
Iteration 9200: Loss = -10883.568201189182
Iteration 9300: Loss = -10883.574086277675
1
Iteration 9400: Loss = -10883.570366690097
2
Iteration 9500: Loss = -10883.591890163721
3
Iteration 9600: Loss = -10883.608427224055
4
Iteration 9700: Loss = -10883.567676110757
Iteration 9800: Loss = -10883.568258193653
1
Iteration 9900: Loss = -10883.583454118007
2
Iteration 10000: Loss = -10883.567404769654
Iteration 10100: Loss = -10883.567522734049
1
Iteration 10200: Loss = -10883.569364150258
2
Iteration 10300: Loss = -10883.568822140796
3
Iteration 10400: Loss = -10883.635638857602
4
Iteration 10500: Loss = -10883.579008323146
5
Iteration 10600: Loss = -10883.568159974779
6
Iteration 10700: Loss = -10883.566951428076
Iteration 10800: Loss = -10883.567667988525
1
Iteration 10900: Loss = -10883.574974159576
2
Iteration 11000: Loss = -10883.566746100534
Iteration 11100: Loss = -10883.567769882035
1
Iteration 11200: Loss = -10883.575018338295
2
Iteration 11300: Loss = -10883.566714645454
Iteration 11400: Loss = -10883.566637927684
Iteration 11500: Loss = -10883.593583711649
1
Iteration 11600: Loss = -10883.566494437915
Iteration 11700: Loss = -10883.566538284815
1
Iteration 11800: Loss = -10883.566557301581
2
Iteration 11900: Loss = -10883.7296298775
3
Iteration 12000: Loss = -10883.566384799524
Iteration 12100: Loss = -10883.566584878157
1
Iteration 12200: Loss = -10883.775702834217
2
Iteration 12300: Loss = -10883.566326800183
Iteration 12400: Loss = -10883.567465808643
1
Iteration 12500: Loss = -10883.566244362799
Iteration 12600: Loss = -10883.56837980638
1
Iteration 12700: Loss = -10883.566293524518
2
Iteration 12800: Loss = -10883.56627029378
3
Iteration 12900: Loss = -10883.64661931227
4
Iteration 13000: Loss = -10883.566144370281
Iteration 13100: Loss = -10883.56647946759
1
Iteration 13200: Loss = -10883.574902521998
2
Iteration 13300: Loss = -10883.56608747931
Iteration 13400: Loss = -10883.566163739693
1
Iteration 13500: Loss = -10883.566040346686
Iteration 13600: Loss = -10883.566147550075
1
Iteration 13700: Loss = -10883.566109693445
2
Iteration 13800: Loss = -10883.585611226954
3
Iteration 13900: Loss = -10883.571763597089
4
Iteration 14000: Loss = -10883.65467176862
5
Iteration 14100: Loss = -10883.566261073753
6
Iteration 14200: Loss = -10883.575225469105
7
Iteration 14300: Loss = -10883.569832161118
8
Iteration 14400: Loss = -10883.581255802947
9
Iteration 14500: Loss = -10883.566069334114
10
Stopping early at iteration 14500 due to no improvement.
tensor([[ 3.6092e-01, -4.9761e+00],
        [ 1.2259e-01, -4.7378e+00],
        [ 2.0040e-01, -4.8156e+00],
        [ 3.2993e-01, -4.9451e+00],
        [ 2.6309e-01, -4.8783e+00],
        [ 2.9532e-02, -4.6448e+00],
        [ 4.1627e-01, -5.0315e+00],
        [ 2.9363e-01, -4.9088e+00],
        [ 1.8610e-01, -4.8013e+00],
        [ 1.5576e-01, -4.7710e+00],
        [ 2.2662e-01, -4.8418e+00],
        [ 1.8318e-01, -4.7984e+00],
        [ 4.3375e-01, -5.0490e+00],
        [ 1.0427e-01, -4.7195e+00],
        [ 3.1483e-01, -4.9300e+00],
        [ 1.7895e-01, -4.7942e+00],
        [ 3.8365e-01, -4.9989e+00],
        [ 2.7007e-01, -4.8853e+00],
        [ 3.7518e-01, -4.9904e+00],
        [ 1.2165e-01, -4.7369e+00],
        [-1.9138e-02, -4.5961e+00],
        [ 1.2347e-01, -4.7387e+00],
        [ 4.1426e-01, -5.0295e+00],
        [ 1.8681e-01, -4.8020e+00],
        [ 4.1992e-01, -5.0351e+00],
        [ 1.4991e-01, -4.7651e+00],
        [ 2.2173e-01, -4.8369e+00],
        [ 1.7201e-01, -4.7872e+00],
        [ 1.1438e-01, -4.7296e+00],
        [ 1.9585e-01, -4.8111e+00],
        [ 1.8401e-01, -4.7992e+00],
        [ 2.7524e-01, -4.8905e+00],
        [ 1.8378e-01, -4.7990e+00],
        [ 2.1299e-01, -4.8282e+00],
        [ 2.8184e-01, -4.8971e+00],
        [ 2.8770e-01, -4.9029e+00],
        [ 2.9487e-01, -4.9101e+00],
        [ 2.7276e-01, -4.8880e+00],
        [ 3.2117e-01, -4.9364e+00],
        [ 1.8727e-01, -4.8025e+00],
        [ 5.6856e-02, -4.6721e+00],
        [ 1.7358e-01, -4.7888e+00],
        [ 1.5976e-01, -4.7750e+00],
        [ 2.7503e-01, -4.8902e+00],
        [ 4.0367e-01, -5.0189e+00],
        [ 2.9518e-01, -4.9104e+00],
        [ 1.9697e-01, -4.8122e+00],
        [ 8.2219e-02, -4.6974e+00],
        [ 3.5066e-01, -4.9659e+00],
        [ 1.0025e-01, -4.7155e+00],
        [ 2.3037e-01, -4.8456e+00],
        [-3.4094e-02, -4.5811e+00],
        [-4.1128e-02, -4.5741e+00],
        [ 2.2815e-01, -4.8434e+00],
        [ 3.7682e-01, -4.9920e+00],
        [ 2.3472e-01, -4.8499e+00],
        [ 1.5752e-01, -4.7727e+00],
        [-7.6546e-02, -4.5387e+00],
        [-9.7574e-03, -4.6055e+00],
        [ 5.4030e-02, -4.6693e+00],
        [ 8.4525e-02, -4.6997e+00],
        [ 2.5336e-01, -4.8686e+00],
        [ 3.1163e-01, -4.9269e+00],
        [ 2.1145e-01, -4.8267e+00],
        [ 2.1021e-03, -4.6173e+00],
        [ 2.5607e-01, -4.8713e+00],
        [ 1.5248e-01, -4.7677e+00],
        [ 2.9593e-01, -4.9111e+00],
        [ 1.7501e-01, -4.7902e+00],
        [ 3.6212e-01, -4.9773e+00],
        [ 5.0703e-01, -5.1223e+00],
        [ 2.3855e-01, -4.8538e+00],
        [ 2.7226e-01, -4.8875e+00],
        [ 1.7213e-01, -4.7873e+00],
        [-3.2418e-03, -4.6120e+00],
        [ 3.2418e-01, -4.9394e+00],
        [ 2.5483e-01, -4.8701e+00],
        [ 1.2701e-01, -4.7422e+00],
        [ 1.7531e-02, -4.6328e+00],
        [ 1.8224e-01, -4.7975e+00],
        [ 2.6449e-01, -4.8797e+00],
        [-2.2406e-01, -4.3912e+00],
        [ 3.1143e-01, -4.9266e+00],
        [ 4.6392e-02, -4.6616e+00],
        [ 2.7662e-01, -4.8918e+00],
        [ 1.6148e-01, -4.7767e+00],
        [ 1.2347e-01, -4.7387e+00],
        [ 2.5149e-01, -4.8667e+00],
        [ 1.8381e-01, -4.7990e+00],
        [ 1.7501e-01, -4.7902e+00],
        [ 2.8533e-01, -4.9006e+00],
        [ 9.6111e-02, -4.7113e+00],
        [ 4.1912e-01, -5.0343e+00],
        [ 3.7390e-01, -4.9891e+00],
        [ 2.0700e-01, -4.8222e+00],
        [ 1.2258e-01, -4.7378e+00],
        [ 9.6163e-02, -4.7114e+00],
        [ 1.3534e-01, -4.7506e+00],
        [ 1.1830e-01, -4.7335e+00],
        [ 2.0673e-01, -4.8220e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[7.7195e-01, 2.2805e-01],
        [9.9992e-01, 8.4023e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9933, 0.0067], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1527, 0.1609],
         [0.3493, 0.2038]],

        [[0.2222, 0.1726],
         [0.5916, 0.3811]],

        [[0.8483, 0.1813],
         [0.0698, 0.3581]],

        [[0.6198, 0.1779],
         [0.9683, 0.1506]],

        [[0.6264, 0.1729],
         [0.3773, 0.6251]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 61
Adjusted Rand Index: -0.015623423336712405
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.002468638218550053
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -0.0013373888019515253
Average Adjusted Rand Index: -0.0030247842003192633
Iteration 0: Loss = -19127.91743897346
Iteration 10: Loss = -10885.159663741271
Iteration 20: Loss = -10884.931959406951
Iteration 30: Loss = -10884.788950635322
Iteration 40: Loss = -10884.695812710475
Iteration 50: Loss = -10884.636421309217
Iteration 60: Loss = -10884.598599189369
Iteration 70: Loss = -10884.57486758898
Iteration 80: Loss = -10884.560852918888
Iteration 90: Loss = -10884.553900093812
Iteration 100: Loss = -10884.552297036968
Iteration 110: Loss = -10884.554881442504
1
Iteration 120: Loss = -10884.560927901151
2
Iteration 130: Loss = -10884.56973910326
3
Stopping early at iteration 129 due to no improvement.
pi: tensor([[9.0782e-01, 9.2185e-02],
        [1.0000e+00, 2.0598e-08]], dtype=torch.float64)
alpha: tensor([0.9167, 0.0833])
beta: tensor([[[0.1549, 0.1535],
         [0.9438, 0.2009]],

        [[0.8487, 0.1757],
         [0.9498, 0.7201]],

        [[0.9648, 0.1878],
         [0.9489, 0.3909]],

        [[0.6180, 0.1867],
         [0.8422, 0.0629]],

        [[0.5534, 0.1751],
         [0.8828, 0.2846]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 63
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19127.67490484978
Iteration 100: Loss = -10890.887338361368
Iteration 200: Loss = -10885.582341916379
Iteration 300: Loss = -10885.12588738136
Iteration 400: Loss = -10884.94543351195
Iteration 500: Loss = -10884.831968624094
Iteration 600: Loss = -10884.745629613424
Iteration 700: Loss = -10884.67095811948
Iteration 800: Loss = -10884.603146872561
Iteration 900: Loss = -10884.538610279844
Iteration 1000: Loss = -10884.473300181266
Iteration 1100: Loss = -10884.40766846164
Iteration 1200: Loss = -10884.343643528446
Iteration 1300: Loss = -10884.280219150745
Iteration 1400: Loss = -10884.220386655274
Iteration 1500: Loss = -10884.165285711433
Iteration 1600: Loss = -10884.113538386402
Iteration 1700: Loss = -10884.063083547522
Iteration 1800: Loss = -10884.013373942385
Iteration 1900: Loss = -10883.964504774376
Iteration 2000: Loss = -10883.915788461563
Iteration 2100: Loss = -10883.86757198218
Iteration 2200: Loss = -10883.819471318797
Iteration 2300: Loss = -10883.772750142369
Iteration 2400: Loss = -10883.727333730247
Iteration 2500: Loss = -10883.687437964298
Iteration 2600: Loss = -10883.653899449444
Iteration 2700: Loss = -10883.628990574074
Iteration 2800: Loss = -10883.611401539103
Iteration 2900: Loss = -10883.600543467055
Iteration 3000: Loss = -10883.59293808396
Iteration 3100: Loss = -10883.58896139505
Iteration 3200: Loss = -10883.585860248364
Iteration 3300: Loss = -10883.592133289665
1
Iteration 3400: Loss = -10883.582859340986
Iteration 3500: Loss = -10883.687862658031
1
Iteration 3600: Loss = -10883.581062922794
Iteration 3700: Loss = -10883.580394797456
Iteration 3800: Loss = -10883.579827708134
Iteration 3900: Loss = -10883.579113026555
Iteration 4000: Loss = -10883.578909419542
Iteration 4100: Loss = -10883.578093485481
Iteration 4200: Loss = -10883.577589478322
Iteration 4300: Loss = -10883.595232844202
1
Iteration 4400: Loss = -10883.576642375738
Iteration 4500: Loss = -10883.576203108647
Iteration 4600: Loss = -10883.668763242222
1
Iteration 4700: Loss = -10883.575349893585
Iteration 4800: Loss = -10883.574942423087
Iteration 4900: Loss = -10883.764180784003
1
Iteration 5000: Loss = -10883.574242817176
Iteration 5100: Loss = -10883.573874891814
Iteration 5200: Loss = -10883.995058831924
1
Iteration 5300: Loss = -10883.573190000781
Iteration 5400: Loss = -10883.572858304715
Iteration 5500: Loss = -10883.611905490914
1
Iteration 5600: Loss = -10883.57228825361
Iteration 5700: Loss = -10883.571962504258
Iteration 5800: Loss = -10883.573369432112
1
Iteration 5900: Loss = -10883.571444579307
Iteration 6000: Loss = -10883.57120685566
Iteration 6100: Loss = -10883.576479965028
1
Iteration 6200: Loss = -10883.570710085029
Iteration 6300: Loss = -10883.570484115118
Iteration 6400: Loss = -10883.574370165747
1
Iteration 6500: Loss = -10883.570061165727
Iteration 6600: Loss = -10883.569883599119
Iteration 6700: Loss = -10883.573957639821
1
Iteration 6800: Loss = -10883.56947289262
Iteration 6900: Loss = -10883.611579024297
1
Iteration 7000: Loss = -10883.569152320708
Iteration 7100: Loss = -10883.569014842573
Iteration 7200: Loss = -10883.57034554443
1
Iteration 7300: Loss = -10883.568719027659
Iteration 7400: Loss = -10883.568579482293
Iteration 7500: Loss = -10883.568489779218
Iteration 7600: Loss = -10883.568310955767
Iteration 7700: Loss = -10883.57556084841
1
Iteration 7800: Loss = -10883.568090334431
Iteration 7900: Loss = -10883.56795787408
Iteration 8000: Loss = -10883.572002695753
1
Iteration 8100: Loss = -10883.567781471718
Iteration 8200: Loss = -10883.573703732549
1
Iteration 8300: Loss = -10883.568241365872
2
Iteration 8400: Loss = -10883.570204211843
3
Iteration 8500: Loss = -10883.658187925013
4
Iteration 8600: Loss = -10883.598996916935
5
Iteration 8700: Loss = -10883.567340413272
Iteration 8800: Loss = -10883.567171192706
Iteration 8900: Loss = -10883.567635346817
1
Iteration 9000: Loss = -10883.569649293657
2
Iteration 9100: Loss = -10883.56703873594
Iteration 9200: Loss = -10883.56760844508
1
Iteration 9300: Loss = -10883.569944218092
2
Iteration 9400: Loss = -10883.566817383107
Iteration 9500: Loss = -10883.568317640107
1
Iteration 9600: Loss = -10883.629123430857
2
Iteration 9700: Loss = -10883.566680265381
Iteration 9800: Loss = -10883.569042536023
1
Iteration 9900: Loss = -10883.566572856538
Iteration 10000: Loss = -10883.567190737385
1
Iteration 10100: Loss = -10883.567824149544
2
Iteration 10200: Loss = -10883.710313980588
3
Iteration 10300: Loss = -10883.566567229473
Iteration 10400: Loss = -10883.566442319849
Iteration 10500: Loss = -10883.592664771126
1
Iteration 10600: Loss = -10883.566391080134
Iteration 10700: Loss = -10883.56639371403
1
Iteration 10800: Loss = -10883.576163198139
2
Iteration 10900: Loss = -10883.566309896036
Iteration 11000: Loss = -10883.632385669744
1
Iteration 11100: Loss = -10883.566236462113
Iteration 11200: Loss = -10883.683586472804
1
Iteration 11300: Loss = -10883.566248872585
2
Iteration 11400: Loss = -10883.566360829618
3
Iteration 11500: Loss = -10883.56945520019
4
Iteration 11600: Loss = -10883.566178908068
Iteration 11700: Loss = -10883.572064664424
1
Iteration 11800: Loss = -10883.566106481105
Iteration 11900: Loss = -10883.566204054063
1
Iteration 12000: Loss = -10883.589055996019
2
Iteration 12100: Loss = -10883.566040227503
Iteration 12200: Loss = -10883.581121042685
1
Iteration 12300: Loss = -10883.633567782323
2
Iteration 12400: Loss = -10883.56603211215
Iteration 12500: Loss = -10883.566937640748
1
Iteration 12600: Loss = -10883.57453103708
2
Iteration 12700: Loss = -10883.565980069876
Iteration 12800: Loss = -10883.566056361682
1
Iteration 12900: Loss = -10883.565967862625
Iteration 13000: Loss = -10883.56634752956
1
Iteration 13100: Loss = -10883.565959687072
Iteration 13200: Loss = -10883.588384891898
1
Iteration 13300: Loss = -10883.56592273622
Iteration 13400: Loss = -10883.566331993969
1
Iteration 13500: Loss = -10883.566118942364
2
Iteration 13600: Loss = -10883.565962795448
3
Iteration 13700: Loss = -10883.572104351195
4
Iteration 13800: Loss = -10883.56591360944
Iteration 13900: Loss = -10883.566641732712
1
Iteration 14000: Loss = -10883.592738126512
2
Iteration 14100: Loss = -10883.565926813983
3
Iteration 14200: Loss = -10883.88437691451
4
Iteration 14300: Loss = -10883.565915532703
5
Iteration 14400: Loss = -10883.567909048834
6
Iteration 14500: Loss = -10883.565850118233
Iteration 14600: Loss = -10883.565994261691
1
Iteration 14700: Loss = -10883.566987933298
2
Iteration 14800: Loss = -10883.709014639773
3
Iteration 14900: Loss = -10883.565822221935
Iteration 15000: Loss = -10883.567334037727
1
Iteration 15100: Loss = -10883.583253728018
2
Iteration 15200: Loss = -10883.5658523535
3
Iteration 15300: Loss = -10883.769124173503
4
Iteration 15400: Loss = -10883.565838883564
5
Iteration 15500: Loss = -10883.567099961827
6
Iteration 15600: Loss = -10883.56585945758
7
Iteration 15700: Loss = -10883.566558838347
8
Iteration 15800: Loss = -10883.642852398694
9
Iteration 15900: Loss = -10883.602836823757
10
Stopping early at iteration 15900 due to no improvement.
tensor([[ 1.7691, -3.5425],
        [ 1.6960, -3.1522],
        [ 1.7783, -3.2212],
        [ 1.3852, -3.8551],
        [ 1.7808, -3.3377],
        [ 1.3770, -3.2846],
        [ 1.5067, -3.9144],
        [ 1.2284, -3.9382],
        [ 1.5296, -3.4457],
        [ 1.7568, -3.1560],
        [ 1.5151, -3.5277],
        [ 1.7373, -3.2221],
        [ 2.0315, -3.4227],
        [ 1.6638, -3.1475],
        [ 1.9125, -3.3062],
        [ 1.7849, -3.1724],
        [ 1.9832, -3.3720],
        [ 1.5908, -3.5398],
        [ 1.9194, -3.4163],
        [ 1.6961, -3.1496],
        [ 1.0656, -3.5006],
        [ 1.0316, -3.8188],
        [ 1.8257, -3.5905],
        [ 1.7404, -3.2300],
        [ 1.6229, -3.7988],
        [ 1.6817, -3.2108],
        [ 1.7346, -3.3022],
        [ 0.7516, -4.1963],
        [ 1.0516, -3.7772],
        [ 1.7218, -3.2653],
        [ 1.7823, -3.1845],
        [ 1.5378, -3.5929],
        [ 1.2227, -3.7425],
        [ 1.7024, -3.3130],
        [ 1.5170, -3.6269],
        [ 1.8811, -3.2781],
        [ 1.8819, -3.2953],
        [ 1.4452, -3.6898],
        [ 1.9242, -3.3118],
        [ 1.7894, -3.1799],
        [ 1.6489, -3.0681],
        [ 1.0913, -3.8556],
        [ 1.5756, -3.3410],
        [ 0.6863, -4.4432],
        [ 1.8360, -3.5551],
        [ 1.8055, -3.3727],
        [ 1.0297, -3.9543],
        [ 1.6832, -3.0858],
        [ 1.4074, -3.8865],
        [ 1.6869, -3.1151],
        [ 1.7974, -3.2651],
        [ 1.2165, -3.3204],
        [ 1.5363, -2.9862],
        [ 1.7666, -3.2922],
        [ 1.6440, -3.7012],
        [ 1.7587, -3.3002],
        [ 0.7701, -4.1456],
        [ 1.4983, -2.9536],
        [ 1.5923, -2.9933],
        [ 1.4276, -3.2850],
        [ 1.5854, -3.1874],
        [ 1.6591, -3.4430],
        [ 1.9017, -3.3108],
        [ 1.5455, -3.4587],
        [ 1.3407, -3.2679],
        [ 1.7977, -3.3042],
        [ 0.4600, -4.4452],
        [ 1.8060, -3.3711],
        [ 1.7813, -3.1735],
        [ 1.8957, -3.4179],
        [ 2.1030, -3.4989],
        [ 1.3750, -3.6882],
        [ 1.7483, -3.3779],
        [ 1.7708, -3.1666],
        [ 1.1099, -3.4890],
        [ 1.8542, -3.3842],
        [ 1.6763, -3.4201],
        [ 1.3394, -3.5196],
        [ 1.3483, -3.2916],
        [ 1.7592, -3.1970],
        [ 1.6336, -3.4886],
        [ 1.3834, -2.7737],
        [ 1.0081, -4.1993],
        [ 1.5134, -3.1837],
        [ 0.9657, -4.1765],
        [ 1.4564, -3.4719],
        [ 1.6872, -3.1566],
        [ 1.2550, -3.8553],
        [ 1.7781, -3.1873],
        [ 1.7512, -3.1911],
        [ 1.5998, -3.5521],
        [ 1.6624, -3.1297],
        [ 1.9684, -3.4565],
        [ 1.9263, -3.4076],
        [ 1.7900, -3.2161],
        [ 1.6923, -3.1571],
        [ 1.4862, -3.3099],
        [ 1.6394, -3.2339],
        [ 0.8919, -3.9446],
        [ 1.6967, -3.3045]], dtype=torch.float64, requires_grad=True)
pi: tensor([[7.7068e-01, 2.2932e-01],
        [9.9998e-01, 2.2675e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9931, 0.0069], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1520, 0.1616],
         [0.9438, 0.2045]],

        [[0.8487, 0.1730],
         [0.9498, 0.7201]],

        [[0.9648, 0.1819],
         [0.9489, 0.3909]],

        [[0.6180, 0.1785],
         [0.8422, 0.0629]],

        [[0.5534, 0.1734],
         [0.8828, 0.2846]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 61
Adjusted Rand Index: -0.015623423336712405
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.002468638218550053
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -0.0013373888019515253
Average Adjusted Rand Index: -0.0030247842003192633
Iteration 0: Loss = -21676.584119621515
Iteration 10: Loss = -10884.618409656458
Iteration 20: Loss = -10884.576976402628
Iteration 30: Loss = -10884.562193709122
Iteration 40: Loss = -10884.557113175306
Iteration 50: Loss = -10884.557781503996
1
Iteration 60: Loss = -10884.562462364733
2
Iteration 70: Loss = -10884.569980278458
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.0136, 0.9864],
        [0.0946, 0.9054]], dtype=torch.float64)
alpha: tensor([0.0864, 0.9136])
beta: tensor([[[0.2009, 0.1540],
         [0.0566, 0.1548]],

        [[0.5920, 0.1757],
         [0.1178, 0.4803]],

        [[0.9426, 0.1874],
         [0.0817, 0.7780]],

        [[0.4492, 0.1863],
         [0.0367, 0.8738]],

        [[0.1707, 0.1750],
         [0.9675, 0.0378]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 37
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21676.33986614887
Iteration 100: Loss = -10899.362899437825
Iteration 200: Loss = -10886.33368515676
Iteration 300: Loss = -10885.60392652352
Iteration 400: Loss = -10885.336134444527
Iteration 500: Loss = -10885.168788010616
Iteration 600: Loss = -10885.102629902529
Iteration 700: Loss = -10885.052127434197
Iteration 800: Loss = -10885.010226649238
Iteration 900: Loss = -10884.964839004017
Iteration 1000: Loss = -10884.902157758073
Iteration 1100: Loss = -10884.82598353483
Iteration 1200: Loss = -10884.754654246957
Iteration 1300: Loss = -10884.693284240673
Iteration 1400: Loss = -10884.629439566092
Iteration 1500: Loss = -10884.560983343565
Iteration 1600: Loss = -10884.482679144967
Iteration 1700: Loss = -10884.390567006263
Iteration 1800: Loss = -10884.29463095865
Iteration 1900: Loss = -10884.21104653758
Iteration 2000: Loss = -10884.14392804486
Iteration 2100: Loss = -10884.08403546497
Iteration 2200: Loss = -10884.029883117604
Iteration 2300: Loss = -10883.980221934766
Iteration 2400: Loss = -10883.929936870098
Iteration 2500: Loss = -10883.866905069126
Iteration 2600: Loss = -10883.792378073505
Iteration 2700: Loss = -10883.758277621662
Iteration 2800: Loss = -10883.736766044469
Iteration 2900: Loss = -10883.686021317897
Iteration 3000: Loss = -10883.67323104338
Iteration 3100: Loss = -10883.66797634284
Iteration 3200: Loss = -10883.66545606038
Iteration 3300: Loss = -10883.663835167663
Iteration 3400: Loss = -10883.700020511724
1
Iteration 3500: Loss = -10883.661715022852
Iteration 3600: Loss = -10883.660858914107
Iteration 3700: Loss = -10883.673367927007
1
Iteration 3800: Loss = -10883.659281129085
Iteration 3900: Loss = -10883.658603070739
Iteration 4000: Loss = -10883.657981288487
Iteration 4100: Loss = -10883.657259989439
Iteration 4200: Loss = -10883.656534165415
Iteration 4300: Loss = -10883.653591815435
Iteration 4400: Loss = -10883.582558721188
Iteration 4500: Loss = -10883.57734250966
Iteration 4600: Loss = -10883.576020132852
Iteration 4700: Loss = -10883.575371952344
Iteration 4800: Loss = -10883.57559301699
1
Iteration 4900: Loss = -10883.573945864378
Iteration 5000: Loss = -10883.573795685466
Iteration 5100: Loss = -10883.573238198655
Iteration 5200: Loss = -10883.572999788177
Iteration 5300: Loss = -10883.574428258762
1
Iteration 5400: Loss = -10883.572509821552
Iteration 5500: Loss = -10883.5722885548
Iteration 5600: Loss = -10883.572092876924
Iteration 5700: Loss = -10883.57186397009
Iteration 5800: Loss = -10883.571684755037
Iteration 5900: Loss = -10883.5714097315
Iteration 6000: Loss = -10883.605071880567
1
Iteration 6100: Loss = -10883.570988353993
Iteration 6200: Loss = -10883.570783605866
Iteration 6300: Loss = -10883.57057172568
Iteration 6400: Loss = -10883.570242022246
Iteration 6500: Loss = -10883.634115603709
1
Iteration 6600: Loss = -10883.569471697501
Iteration 6700: Loss = -10883.569275715421
Iteration 6800: Loss = -10883.569813672077
1
Iteration 6900: Loss = -10883.568978407966
Iteration 7000: Loss = -10883.597263175583
1
Iteration 7100: Loss = -10883.568711194417
Iteration 7200: Loss = -10883.569889847442
1
Iteration 7300: Loss = -10883.568676484821
Iteration 7400: Loss = -10883.56900697336
1
Iteration 7500: Loss = -10883.569545559747
2
Iteration 7600: Loss = -10883.573560225974
3
Iteration 7700: Loss = -10883.586304377703
4
Iteration 7800: Loss = -10883.56859575128
Iteration 7900: Loss = -10883.568039219215
Iteration 8000: Loss = -10883.569457423142
1
Iteration 8100: Loss = -10883.625752954971
2
Iteration 8200: Loss = -10883.567861364501
Iteration 8300: Loss = -10883.570121227174
1
Iteration 8400: Loss = -10883.567720924613
Iteration 8500: Loss = -10883.579967947599
1
Iteration 8600: Loss = -10883.567584262628
Iteration 8700: Loss = -10883.638135010102
1
Iteration 8800: Loss = -10883.56751725202
Iteration 8900: Loss = -10883.7702099667
1
Iteration 9000: Loss = -10883.567436854057
Iteration 9100: Loss = -10883.57246891041
1
Iteration 9200: Loss = -10883.567291751451
Iteration 9300: Loss = -10883.567685580003
1
Iteration 9400: Loss = -10883.567193464625
Iteration 9500: Loss = -10883.567171142819
Iteration 9600: Loss = -10883.567082747202
Iteration 9700: Loss = -10883.569698014677
1
Iteration 9800: Loss = -10883.566988915201
Iteration 9900: Loss = -10883.567220092991
1
Iteration 10000: Loss = -10883.566882568719
Iteration 10100: Loss = -10883.567108184721
1
Iteration 10200: Loss = -10883.813803014073
2
Iteration 10300: Loss = -10883.566675959564
Iteration 10400: Loss = -10883.758851297232
1
Iteration 10500: Loss = -10883.566728040054
2
Iteration 10600: Loss = -10883.571291880797
3
Iteration 10700: Loss = -10883.566448318743
Iteration 10800: Loss = -10883.567065988784
1
Iteration 10900: Loss = -10883.578632920286
2
Iteration 11000: Loss = -10883.566229452508
Iteration 11100: Loss = -10883.566709573337
1
Iteration 11200: Loss = -10883.567332228238
2
Iteration 11300: Loss = -10883.570779294938
3
Iteration 11400: Loss = -10883.57136585366
4
Iteration 11500: Loss = -10883.566451412073
5
Iteration 11600: Loss = -10883.566483016102
6
Iteration 11700: Loss = -10883.589950560168
7
Iteration 11800: Loss = -10883.593627184984
8
Iteration 11900: Loss = -10883.67468722184
9
Iteration 12000: Loss = -10883.6089170661
10
Stopping early at iteration 12000 due to no improvement.
tensor([[-3.4094e+00,  1.9322e+00],
        [-3.1955e+00,  1.6747e+00],
        [-3.2229e+00,  1.8022e+00],
        [-3.4855e+00,  1.7927e+00],
        [-3.3200e+00,  1.8305e+00],
        [-3.1923e+00,  1.4905e+00],
        [-3.8163e+00,  1.6361e+00],
        [-3.3506e+00,  1.8581e+00],
        [-3.3016e+00,  1.6946e+00],
        [-3.1871e+00,  1.7497e+00],
        [-3.7542e+00,  1.3219e+00],
        [-3.3963e+00,  1.5924e+00],
        [-3.4709e+00,  2.0121e+00],
        [-3.1826e+00,  1.6506e+00],
        [-4.0176e+00,  1.2348e+00],
        [-3.1883e+00,  1.7939e+00],
        [-3.5130e+00,  1.8764e+00],
        [-3.3916e+00,  1.7701e+00],
        [-3.7578e+00,  1.6097e+00],
        [-3.2117e+00,  1.6562e+00],
        [-3.9519e+00,  6.3288e-01],
        [-3.1322e+00,  1.7403e+00],
        [-3.4784e+00,  1.9692e+00],
        [-3.1970e+00,  1.8018e+00],
        [-3.7505e+00,  1.7122e+00],
        [-3.3367e+00,  1.5853e+00],
        [-3.5547e+00,  1.5120e+00],
        [-3.3959e+00,  1.5718e+00],
        [-3.1688e+00,  1.6822e+00],
        [-3.2246e+00,  1.7911e+00],
        [-3.3649e+00,  1.6273e+00],
        [-3.3256e+00,  1.8462e+00],
        [-3.2643e+00,  1.7268e+00],
        [-3.5080e+00,  1.5402e+00],
        [-3.3094e+00,  1.8719e+00],
        [-3.5850e+00,  1.6100e+00],
        [-3.4055e+00,  1.8038e+00],
        [-3.2940e+00,  1.8719e+00],
        [-3.5139e+00,  1.7547e+00],
        [-3.7625e+00,  1.2348e+00],
        [-3.0624e+00,  1.6760e+00],
        [-3.3523e+00,  1.6188e+00],
        [-3.3185e+00,  1.6251e+00],
        [-3.2880e+00,  1.8810e+00],
        [-3.8281e+00,  1.5952e+00],
        [-3.2997e+00,  1.9100e+00],
        [-3.2013e+00,  1.8137e+00],
        [-3.0926e+00,  1.6961e+00],
        [-3.5085e+00,  1.8156e+00],
        [-3.8428e+00,  9.8149e-01],
        [-3.8798e+00,  1.2048e+00],
        [-3.9269e+00,  6.3064e-01],
        [-3.0267e+00,  1.5181e+00],
        [-3.2982e+00,  1.7843e+00],
        [-3.3847e+00,  1.9954e+00],
        [-3.2387e+00,  1.8508e+00],
        [-3.1845e+00,  1.7532e+00],
        [-3.4140e+00,  1.0603e+00],
        [-3.2777e+00,  1.3288e+00],
        [-3.2896e+00,  1.4422e+00],
        [-3.1256e+00,  1.6676e+00],
        [-3.4619e+00,  1.6700e+00],
        [-3.4508e+00,  1.7928e+00],
        [-3.2271e+00,  1.8171e+00],
        [-3.2757e+00,  1.3537e+00],
        [-4.8759e+00,  2.6068e-01],
        [-3.1735e+00,  1.7547e+00],
        [-3.3956e+00,  1.8145e+00],
        [-3.2270e+00,  1.7500e+00],
        [-3.7769e+00,  1.5723e+00],
        [-3.6375e+00,  1.9961e+00],
        [-3.2797e+00,  1.8177e+00],
        [-3.8163e+00,  1.3494e+00],
        [-3.3236e+00,  1.6428e+00],
        [-3.0257e+00,  1.5952e+00],
        [-3.4189e+00,  1.8499e+00],
        [-3.2590e+00,  1.8721e+00],
        [-3.1368e+00,  1.7415e+00],
        [-3.1335e+00,  1.5268e+00],
        [-3.2458e+00,  1.7409e+00],
        [-4.1844e+00,  9.6908e-01],
        [-4.1895e+00, -4.6251e-03],
        [-3.3313e+00,  1.9081e+00],
        [-3.1305e+00,  1.5865e+00],
        [-3.3073e+00,  1.8665e+00],
        [-3.1757e+00,  1.7760e+00],
        [-3.2295e+00,  1.6410e+00],
        [-3.3377e+00,  1.7982e+00],
        [-3.3635e+00,  1.6291e+00],
        [-3.4588e+00,  1.5135e+00],
        [-3.2901e+00,  1.9035e+00],
        [-3.2333e+00,  1.5816e+00],
        [-3.5650e+00,  1.8922e+00],
        [-3.6927e+00,  1.6744e+00],
        [-4.3498e+00,  6.8906e-01],
        [-4.1724e+00,  6.9562e-01],
        [-3.1639e+00,  1.6527e+00],
        [-3.5547e+00,  1.3376e+00],
        [-3.1385e+00,  1.7222e+00],
        [-3.2393e+00,  1.7952e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.2023e-04, 9.9988e-01],
        [2.2705e-01, 7.7295e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0067, 0.9933], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2038, 0.1606],
         [0.0566, 0.1529]],

        [[0.5920, 0.1724],
         [0.1178, 0.4803]],

        [[0.9426, 0.1811],
         [0.0817, 0.7780]],

        [[0.4492, 0.1778],
         [0.0367, 0.8738]],

        [[0.1707, 0.1727],
         [0.9675, 0.0378]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 39
Adjusted Rand Index: -0.015623423336712405
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.002468638218550053
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -0.0013373888019515253
Average Adjusted Rand Index: -0.0030247842003192633
Iteration 0: Loss = -26111.292739041437
Iteration 10: Loss = -10885.376313653847
Iteration 20: Loss = -10885.362427272286
Iteration 30: Loss = -10885.321595258323
Iteration 40: Loss = -10885.2214228779
Iteration 50: Loss = -10885.054299298863
Iteration 60: Loss = -10884.882967933283
Iteration 70: Loss = -10884.75756914728
Iteration 80: Loss = -10884.676031641611
Iteration 90: Loss = -10884.623773498803
Iteration 100: Loss = -10884.590585924801
Iteration 110: Loss = -10884.570014666644
Iteration 120: Loss = -10884.558273058496
Iteration 130: Loss = -10884.552954397646
Iteration 140: Loss = -10884.552652210454
Iteration 150: Loss = -10884.556312021095
1
Iteration 160: Loss = -10884.563187295653
2
Iteration 170: Loss = -10884.572678234208
3
Stopping early at iteration 169 due to no improvement.
pi: tensor([[2.6760e-16, 1.0000e+00],
        [9.4441e-02, 9.0556e-01]], dtype=torch.float64)
alpha: tensor([0.0852, 0.9148])
beta: tensor([[[0.2003, 0.1536],
         [0.1047, 0.1549]],

        [[0.0797, 0.1755],
         [0.3095, 0.3930]],

        [[0.6176, 0.1874],
         [0.0510, 0.6841]],

        [[0.5064, 0.1861],
         [0.8675, 0.0072]],

        [[0.1675, 0.1749],
         [0.5819, 0.3153]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 37
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26001.65976285245
Iteration 100: Loss = -10958.638855083509
Iteration 200: Loss = -10907.618732208088
Iteration 300: Loss = -10890.232712818193
Iteration 400: Loss = -10887.921455788523
Iteration 500: Loss = -10886.950591977957
Iteration 600: Loss = -10886.408025485669
Iteration 700: Loss = -10886.066816395858
Iteration 800: Loss = -10885.83092054585
Iteration 900: Loss = -10885.65140280619
Iteration 1000: Loss = -10885.502859241613
Iteration 1100: Loss = -10885.363987634513
Iteration 1200: Loss = -10885.203992988936
Iteration 1300: Loss = -10885.027000093949
Iteration 1400: Loss = -10884.871522551755
Iteration 1500: Loss = -10884.733907731925
Iteration 1600: Loss = -10884.609745983704
Iteration 1700: Loss = -10884.520837509948
Iteration 1800: Loss = -10884.463306082727
Iteration 1900: Loss = -10884.415982075425
Iteration 2000: Loss = -10884.37235975666
Iteration 2100: Loss = -10884.334546337965
Iteration 2200: Loss = -10884.302730994465
Iteration 2300: Loss = -10884.276691683057
Iteration 2400: Loss = -10884.254256887989
Iteration 2500: Loss = -10884.235307628847
Iteration 2600: Loss = -10884.2199832031
Iteration 2700: Loss = -10884.207173369407
Iteration 2800: Loss = -10884.196047077057
Iteration 2900: Loss = -10884.186388076338
Iteration 3000: Loss = -10884.177610851448
Iteration 3100: Loss = -10884.22985019955
1
Iteration 3200: Loss = -10884.162098465677
Iteration 3300: Loss = -10884.15532094332
Iteration 3400: Loss = -10884.148983456556
Iteration 3500: Loss = -10884.14549697818
Iteration 3600: Loss = -10884.13741234943
Iteration 3700: Loss = -10884.132199596927
Iteration 3800: Loss = -10884.127177505026
Iteration 3900: Loss = -10884.123266109515
Iteration 4000: Loss = -10884.117666265278
Iteration 4100: Loss = -10884.113110382194
Iteration 4200: Loss = -10884.470245377068
1
Iteration 4300: Loss = -10884.104374096165
Iteration 4400: Loss = -10884.100160337666
Iteration 4500: Loss = -10884.09602208433
Iteration 4600: Loss = -10884.095790159567
Iteration 4700: Loss = -10884.087880402769
Iteration 4800: Loss = -10884.083919514627
Iteration 4900: Loss = -10884.080040977857
Iteration 5000: Loss = -10884.076587850777
Iteration 5100: Loss = -10884.072649964755
Iteration 5200: Loss = -10884.06916087117
Iteration 5300: Loss = -10884.074126207834
1
Iteration 5400: Loss = -10884.062865092468
Iteration 5500: Loss = -10884.060030477624
Iteration 5600: Loss = -10884.058526708008
Iteration 5700: Loss = -10884.055094488971
Iteration 5800: Loss = -10884.052950024863
Iteration 5900: Loss = -10884.050999641537
Iteration 6000: Loss = -10884.049206108764
Iteration 6100: Loss = -10884.047554887149
Iteration 6200: Loss = -10884.04608000269
Iteration 6300: Loss = -10884.113388986425
1
Iteration 6400: Loss = -10884.043500840085
Iteration 6500: Loss = -10884.04247647753
Iteration 6600: Loss = -10884.041516087715
Iteration 6700: Loss = -10884.04099447133
Iteration 6800: Loss = -10884.039870028284
Iteration 6900: Loss = -10884.039221099882
Iteration 7000: Loss = -10884.28717404755
1
Iteration 7100: Loss = -10884.03806467041
Iteration 7200: Loss = -10884.037551888261
Iteration 7300: Loss = -10884.037109521165
Iteration 7400: Loss = -10884.041373583934
1
Iteration 7500: Loss = -10884.036245173495
Iteration 7600: Loss = -10884.035909058028
Iteration 7700: Loss = -10884.035645060098
Iteration 7800: Loss = -10884.035385513045
Iteration 7900: Loss = -10884.035110273559
Iteration 8000: Loss = -10884.034846948507
Iteration 8100: Loss = -10884.045920160559
1
Iteration 8200: Loss = -10884.034368228164
Iteration 8300: Loss = -10884.034203731811
Iteration 8400: Loss = -10884.311366120137
1
Iteration 8500: Loss = -10884.033845310949
Iteration 8600: Loss = -10884.033647641518
Iteration 8700: Loss = -10884.033456488423
Iteration 8800: Loss = -10884.03349114351
1
Iteration 8900: Loss = -10884.032703988965
Iteration 9000: Loss = -10884.032578220558
Iteration 9100: Loss = -10884.032447054824
Iteration 9200: Loss = -10884.037236706681
1
Iteration 9300: Loss = -10884.031641363574
Iteration 9400: Loss = -10884.037750859039
1
Iteration 9500: Loss = -10884.031479816007
Iteration 9600: Loss = -10884.037409755352
1
Iteration 9700: Loss = -10884.031330828704
Iteration 9800: Loss = -10884.031836717284
1
Iteration 9900: Loss = -10884.031349740306
2
Iteration 10000: Loss = -10884.03133259202
3
Iteration 10100: Loss = -10884.031666541483
4
Iteration 10200: Loss = -10884.031191964006
Iteration 10300: Loss = -10884.031257308772
1
Iteration 10400: Loss = -10884.052465366405
2
Iteration 10500: Loss = -10884.035002391001
3
Iteration 10600: Loss = -10884.066362162219
4
Iteration 10700: Loss = -10884.03106223106
Iteration 10800: Loss = -10884.031018368241
Iteration 10900: Loss = -10884.030991244575
Iteration 11000: Loss = -10884.037553465618
1
Iteration 11100: Loss = -10884.033835539185
2
Iteration 11200: Loss = -10884.122080257854
3
Iteration 11300: Loss = -10884.034417575294
4
Iteration 11400: Loss = -10884.030903927138
Iteration 11500: Loss = -10884.031048416049
1
Iteration 11600: Loss = -10884.03155051227
2
Iteration 11700: Loss = -10884.030876483921
Iteration 11800: Loss = -10884.030221423627
Iteration 11900: Loss = -10884.05090064241
1
Iteration 12000: Loss = -10884.045971640979
2
Iteration 12100: Loss = -10884.029848789556
Iteration 12200: Loss = -10884.034198564246
1
Iteration 12300: Loss = -10884.029543959014
Iteration 12400: Loss = -10884.029797353956
1
Iteration 12500: Loss = -10884.029200315246
Iteration 12600: Loss = -10884.03551048313
1
Iteration 12700: Loss = -10884.028560429228
Iteration 12800: Loss = -10884.029208094762
1
Iteration 12900: Loss = -10884.027744531728
Iteration 13000: Loss = -10884.080728215515
1
Iteration 13100: Loss = -10884.032689134734
2
Iteration 13200: Loss = -10884.029657235853
3
Iteration 13300: Loss = -10884.030289780552
4
Iteration 13400: Loss = -10884.055569537973
5
Iteration 13500: Loss = -10884.042717803053
6
Iteration 13600: Loss = -10884.026724773157
Iteration 13700: Loss = -10884.027693664815
1
Iteration 13800: Loss = -10884.0323686478
2
Iteration 13900: Loss = -10884.147032007802
3
Iteration 14000: Loss = -10884.026303613382
Iteration 14100: Loss = -10884.027010888616
1
Iteration 14200: Loss = -10884.027283306603
2
Iteration 14300: Loss = -10884.026319223787
3
Iteration 14400: Loss = -10884.026401645588
4
Iteration 14500: Loss = -10884.12072854526
5
Iteration 14600: Loss = -10884.060690138218
6
Iteration 14700: Loss = -10884.026267470977
Iteration 14800: Loss = -10884.03665109225
1
Iteration 14900: Loss = -10884.026246158839
Iteration 15000: Loss = -10884.026845823506
1
Iteration 15100: Loss = -10884.030102148641
2
Iteration 15200: Loss = -10884.042644160378
3
Iteration 15300: Loss = -10884.026251514693
4
Iteration 15400: Loss = -10884.026620531691
5
Iteration 15500: Loss = -10884.02625180229
6
Iteration 15600: Loss = -10884.026487704923
7
Iteration 15700: Loss = -10884.072134601603
8
Iteration 15800: Loss = -10884.02628363409
9
Iteration 15900: Loss = -10884.027628721706
10
Stopping early at iteration 15900 due to no improvement.
tensor([[ 2.3009, -3.6879],
        [ 2.0990, -3.8999],
        [ 2.2609, -3.7346],
        [ 0.6849, -5.3001],
        [ 2.2324, -3.7620],
        [ 2.2184, -3.7796],
        [ 2.1569, -3.8305],
        [ 2.2278, -3.7624],
        [ 1.4928, -4.5049],
        [ 2.3047, -3.6925],
        [ 2.2700, -3.7220],
        [ 2.2154, -3.7763],
        [ 1.1631, -4.8186],
        [ 2.2311, -3.7672],
        [ 2.2051, -3.7868],
        [ 2.2815, -3.7144],
        [ 1.3372, -4.6549],
        [ 2.0118, -3.9785],
        [ 1.9397, -4.0429],
        [ 2.1401, -3.8590],
        [ 1.9338, -4.0653],
        [ 2.2546, -3.7441],
        [ 2.2939, -3.6937],
        [ 1.8934, -4.1021],
        [ 1.3702, -4.6223],
        [ 2.0051, -3.9863],
        [ 2.2206, -3.7714],
        [ 1.8852, -4.1142],
        [ 1.7818, -4.2127],
        [ 1.6594, -4.3339],
        [ 2.2918, -3.7037],
        [ 0.8959, -5.0946],
        [ 2.0597, -3.9356],
        [ 2.2199, -3.7727],
        [ 2.0508, -3.9353],
        [ 1.3345, -4.6515],
        [ 2.2654, -3.7203],
        [ 0.6866, -5.3018],
        [ 2.2291, -3.7697],
        [ 0.7934, -5.2001],
        [ 2.2983, -3.6998],
        [ 2.2655, -3.7322],
        [ 2.1573, -3.8376],
        [ 2.2992, -3.6865],
        [ 2.2182, -3.7623],
        [ 2.2938, -3.6919],
        [ 1.4377, -4.5516],
        [ 1.1333, -4.8675],
        [ 2.2796, -3.7175],
        [ 2.2875, -3.7091],
        [ 2.1984, -3.8017],
        [ 2.3076, -3.6957],
        [ 1.7364, -4.2644],
        [ 2.2713, -3.7290],
        [ 2.3055, -3.6921],
        [ 1.8696, -4.1183],
        [ 1.2517, -4.7455],
        [ 2.2306, -3.7719],
        [ 2.3096, -3.6963],
        [ 2.2060, -3.7917],
        [ 1.7518, -4.2470],
        [ 2.2659, -3.7300],
        [ 2.2782, -3.7102],
        [ 1.9147, -4.0749],
        [ 2.2172, -3.7846],
        [ 2.1152, -3.8746],
        [ 1.5218, -4.4738],
        [ 1.7064, -4.2778],
        [ 2.1393, -3.8622],
        [ 2.3024, -3.6930],
        [ 2.0804, -3.9081],
        [ 2.1301, -3.8577],
        [ 2.0849, -3.9039],
        [ 2.0403, -3.9508],
        [ 2.2504, -3.7541],
        [ 2.2889, -3.7017],
        [ 2.2442, -3.7457],
        [ 2.2726, -3.7286],
        [ 2.1753, -3.8243],
        [ 2.3025, -3.6892],
        [ 1.7496, -4.2452],
        [ 2.2636, -3.7490],
        [ 1.6551, -4.3263],
        [ 2.3060, -3.6944],
        [ 2.1946, -3.7954],
        [ 2.2634, -3.7381],
        [ 2.2245, -3.7698],
        [ 2.1252, -3.8858],
        [ 2.2040, -3.7915],
        [ 2.0442, -3.9468],
        [ 2.2779, -3.7151],
        [ 1.7995, -4.1967],
        [ 2.1431, -3.8442],
        [ 2.2899, -3.6969],
        [ 2.1903, -3.7988],
        [ 2.2207, -3.7784],
        [ 2.0949, -3.9036],
        [ 2.2148, -3.7824],
        [ 2.2862, -3.7092],
        [ 2.2974, -3.6916]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.0154, 0.9846],
        [0.0111, 0.9889]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9975, 0.0025], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1536, 0.1533],
         [0.1047, 0.1611]],

        [[0.0797, 0.1700],
         [0.3095, 0.3930]],

        [[0.6176, 0.2088],
         [0.0510, 0.6841]],

        [[0.5064, 0.2258],
         [0.8675, 0.0072]],

        [[0.1675, 0.1616],
         [0.5819, 0.3153]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 10%|█         | 10/100 [7:48:06<67:11:14, 2687.50s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 11%|█         | 11/100 [8:37:25<68:29:44, 2770.62s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 12%|█▏        | 12/100 [9:15:48<64:14:35, 2628.14s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 13%|█▎        | 13/100 [10:05:58<66:18:43, 2743.95s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 14%|█▍        | 14/100 [10:44:45<62:32:22, 2617.94s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 15%|█▌        | 15/100 [11:25:32<60:35:57, 2566.56s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 16%|█▌        | 16/100 [12:14:10<62:21:14, 2672.32s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 17%|█▋        | 17/100 [12:56:57<60:52:52, 2640.63s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 18%|█▊        | 18/100 [13:31:32<56:16:28, 2470.60s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 19%|█▉        | 19/100 [14:09:57<54:28:20, 2420.99s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 20%|██        | 20/100 [14:58:24<57:02:13, 2566.67s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 37
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.004162497788148053
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -68019.69888853612
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.7902,    nan]],

        [[0.8181,    nan],
         [0.5658, 0.0315]],

        [[0.9924,    nan],
         [0.6597, 0.8599]],

        [[0.5185,    nan],
         [0.8814, 0.5726]],

        [[0.6880,    nan],
         [0.5063, 0.5112]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 63
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -68013.66287371216
Iteration 100: Loss = -10935.502074686094
Iteration 200: Loss = -10908.468561586786
Iteration 300: Loss = -10898.767053021384
Iteration 400: Loss = -10894.084571763597
Iteration 500: Loss = -10891.45177941257
Iteration 600: Loss = -10889.817454648424
Iteration 700: Loss = -10888.730502920153
Iteration 800: Loss = -10887.970102072362
Iteration 900: Loss = -10887.417134983078
Iteration 1000: Loss = -10887.002618083196
Iteration 1100: Loss = -10886.684273974626
Iteration 1200: Loss = -10886.434814326827
Iteration 1300: Loss = -10886.236077098341
Iteration 1400: Loss = -10886.075492376
Iteration 1500: Loss = -10885.944145627536
Iteration 1600: Loss = -10885.835698313873
Iteration 1700: Loss = -10885.74532735681
Iteration 1800: Loss = -10885.669502575054
Iteration 1900: Loss = -10885.605460742723
Iteration 2000: Loss = -10885.551109205406
Iteration 2100: Loss = -10885.504695293863
Iteration 2200: Loss = -10885.46492217454
Iteration 2300: Loss = -10885.430790420874
Iteration 2400: Loss = -10885.401375299174
Iteration 2500: Loss = -10885.375937432556
Iteration 2600: Loss = -10885.353900749613
Iteration 2700: Loss = -10885.334793107371
Iteration 2800: Loss = -10885.318154612769
Iteration 2900: Loss = -10885.303603594653
Iteration 3000: Loss = -10885.290877944093
Iteration 3100: Loss = -10885.279582216723
Iteration 3200: Loss = -10885.269636011806
Iteration 3300: Loss = -10885.260749608538
Iteration 3400: Loss = -10885.252784732731
Iteration 3500: Loss = -10885.245591073786
Iteration 3600: Loss = -10885.23905134628
Iteration 3700: Loss = -10885.233121077197
Iteration 3800: Loss = -10885.227668707059
Iteration 3900: Loss = -10885.222693491412
Iteration 4000: Loss = -10885.21802831725
Iteration 4100: Loss = -10885.21373886737
Iteration 4200: Loss = -10885.20973704669
Iteration 4300: Loss = -10885.205986060484
Iteration 4400: Loss = -10885.202519691971
Iteration 4500: Loss = -10885.199302218709
Iteration 4600: Loss = -10885.196261913336
Iteration 4700: Loss = -10885.193413981158
Iteration 4800: Loss = -10885.190717991483
Iteration 4900: Loss = -10885.18819063333
Iteration 5000: Loss = -10885.18589493387
Iteration 5100: Loss = -10885.183654665874
Iteration 5200: Loss = -10885.181596505261
Iteration 5300: Loss = -10885.179581955988
Iteration 5400: Loss = -10885.177733983159
Iteration 5500: Loss = -10885.175994541149
Iteration 5600: Loss = -10885.174274209363
Iteration 5700: Loss = -10885.17271533415
Iteration 5800: Loss = -10885.171212736093
Iteration 5900: Loss = -10885.169796300197
Iteration 6000: Loss = -10885.168457111997
Iteration 6100: Loss = -10885.167176097031
Iteration 6200: Loss = -10885.165982239883
Iteration 6300: Loss = -10885.164844850206
Iteration 6400: Loss = -10885.16374224427
Iteration 6500: Loss = -10885.16266587161
Iteration 6600: Loss = -10885.161721543687
Iteration 6700: Loss = -10885.160766320761
Iteration 6800: Loss = -10885.15989296072
Iteration 6900: Loss = -10885.15901610944
Iteration 7000: Loss = -10885.15824772813
Iteration 7100: Loss = -10885.157435943633
Iteration 7200: Loss = -10885.156688550382
Iteration 7300: Loss = -10885.155948526242
Iteration 7400: Loss = -10885.155301299346
Iteration 7500: Loss = -10885.15467805797
Iteration 7600: Loss = -10885.154069253027
Iteration 7700: Loss = -10885.153446714548
Iteration 7800: Loss = -10885.152852069476
Iteration 7900: Loss = -10885.152331010293
Iteration 8000: Loss = -10885.151774496479
Iteration 8100: Loss = -10885.151254213302
Iteration 8200: Loss = -10885.150756075926
Iteration 8300: Loss = -10885.150273648807
Iteration 8400: Loss = -10885.149802323163
Iteration 8500: Loss = -10885.149347525952
Iteration 8600: Loss = -10885.148872495845
Iteration 8700: Loss = -10885.148456314433
Iteration 8800: Loss = -10885.148086384526
Iteration 8900: Loss = -10885.147693340556
Iteration 9000: Loss = -10885.147293907323
Iteration 9100: Loss = -10885.146967723695
Iteration 9200: Loss = -10885.146636361982
Iteration 9300: Loss = -10885.14630077843
Iteration 9400: Loss = -10885.146031219876
Iteration 9500: Loss = -10885.145696432446
Iteration 9600: Loss = -10885.145383622783
Iteration 9700: Loss = -10885.145104381934
Iteration 9800: Loss = -10885.144793195497
Iteration 9900: Loss = -10885.144531195234
Iteration 10000: Loss = -10885.448217007503
1
Iteration 10100: Loss = -10885.143953044442
Iteration 10200: Loss = -10885.1436615549
Iteration 10300: Loss = -10885.143328419577
Iteration 10400: Loss = -10885.142979635806
Iteration 10500: Loss = -10885.142453856923
Iteration 10600: Loss = -10885.141923925148
Iteration 10700: Loss = -10885.141159782323
Iteration 10800: Loss = -10885.140233352931
Iteration 10900: Loss = -10885.139184132086
Iteration 11000: Loss = -10885.138219458308
Iteration 11100: Loss = -10885.139848941179
1
Iteration 11200: Loss = -10885.136575858218
Iteration 11300: Loss = -10885.135981780879
Iteration 11400: Loss = -10885.135531441607
Iteration 11500: Loss = -10885.135174263003
Iteration 11600: Loss = -10885.134680150466
Iteration 11700: Loss = -10885.13409280654
Iteration 11800: Loss = -10885.271808597705
1
Iteration 11900: Loss = -10885.133464813422
Iteration 12000: Loss = -10885.133169478055
Iteration 12100: Loss = -10885.13291299031
Iteration 12200: Loss = -10885.13270102095
Iteration 12300: Loss = -10885.132314594774
Iteration 12400: Loss = -10885.132007124043
Iteration 12500: Loss = -10885.131877385498
Iteration 12600: Loss = -10885.131329681719
Iteration 12700: Loss = -10885.130962885716
Iteration 12800: Loss = -10885.130532333487
Iteration 12900: Loss = -10885.355810660116
1
Iteration 13000: Loss = -10885.129538666846
Iteration 13100: Loss = -10885.128950834536
Iteration 13200: Loss = -10885.128228818665
Iteration 13300: Loss = -10885.12812435863
Iteration 13400: Loss = -10885.12637312745
Iteration 13500: Loss = -10885.125059092275
Iteration 13600: Loss = -10885.222471784498
1
Iteration 13700: Loss = -10885.120881163046
Iteration 13800: Loss = -10885.259826858557
1
Iteration 13900: Loss = -10885.109767957667
Iteration 14000: Loss = -10885.091170931757
Iteration 14100: Loss = -10884.71938328696
Iteration 14200: Loss = -10884.189329069526
Iteration 14300: Loss = -10883.930005130447
Iteration 14400: Loss = -10883.745070817871
Iteration 14500: Loss = -10883.642321011852
Iteration 14600: Loss = -10883.57198524968
Iteration 14700: Loss = -10883.603290439723
1
Iteration 14800: Loss = -10883.568755476377
Iteration 14900: Loss = -10883.568465437667
Iteration 15000: Loss = -10883.568704651347
1
Iteration 15100: Loss = -10883.568344971156
Iteration 15200: Loss = -10883.676629169437
1
Iteration 15300: Loss = -10883.567600900396
Iteration 15400: Loss = -10883.568406115823
1
Iteration 15500: Loss = -10883.60536382249
2
Iteration 15600: Loss = -10883.567428471506
Iteration 15700: Loss = -10883.575214238386
1
Iteration 15800: Loss = -10883.567273258266
Iteration 15900: Loss = -10883.56751344215
1
Iteration 16000: Loss = -10883.56748507662
2
Iteration 16100: Loss = -10883.56772838927
3
Iteration 16200: Loss = -10883.570127973326
4
Iteration 16300: Loss = -10883.592567479822
5
Iteration 16400: Loss = -10883.597379465582
6
Iteration 16500: Loss = -10883.575483358685
7
Iteration 16600: Loss = -10883.628061545127
8
Iteration 16700: Loss = -10883.567056027929
Iteration 16800: Loss = -10883.573194483874
1
Iteration 16900: Loss = -10883.567017446683
Iteration 17000: Loss = -10883.572362687151
1
Iteration 17100: Loss = -10883.69185511162
2
Iteration 17200: Loss = -10883.567236280029
3
Iteration 17300: Loss = -10883.566948101838
Iteration 17400: Loss = -10883.567582629825
1
Iteration 17500: Loss = -10883.567080584757
2
Iteration 17600: Loss = -10883.566963715413
3
Iteration 17700: Loss = -10883.572095443627
4
Iteration 17800: Loss = -10883.572881707252
5
Iteration 17900: Loss = -10883.598452980077
6
Iteration 18000: Loss = -10883.568357530297
7
Iteration 18100: Loss = -10883.58877130054
8
Iteration 18200: Loss = -10883.662927998668
9
Iteration 18300: Loss = -10883.56902905647
10
Stopping early at iteration 18300 due to no improvement.
tensor([[ 3.3578, -4.8516],
        [ 2.8012, -5.0655],
        [ 2.8711, -5.1046],
        [ 3.3561, -4.7551],
        [ 2.8838, -5.1960],
        [ 3.0834, -4.5990],
        [ 3.2679, -5.0309],
        [ 3.2215, -4.8815],
        [ 3.2358, -4.7347],
        [ 3.1342, -4.7754],
        [ 3.2986, -4.6895],
        [ 2.2400, -5.6688],
        [ 2.2645, -6.0156],
        [ 3.1647, -4.6630],
        [ 3.3675, -4.7771],
        [ 3.2548, -4.6788],
        [ 2.7971, -5.4796],
        [ 3.2925, -4.7650],
        [ 2.9904, -5.1902],
        [ 3.2192, -4.6450],
        [ 3.0551, -4.5527],
        [ 3.0857, -4.7772],
        [ 3.4325, -4.8654],
        [ 3.2742, -4.6744],
        [ 3.1149, -5.2319],
        [ 2.4053, -5.4323],
        [ 3.1039, -4.8772],
        [ 3.2248, -4.7329],
        [ 3.0087, -4.7976],
        [ 3.0233, -4.9247],
        [ 2.8853, -5.0602],
        [ 3.2911, -4.7716],
        [ 3.2723, -4.6715],
        [ 2.7172, -5.2408],
        [ 3.2967, -4.7403],
        [ 1.7166, -6.3318],
        [ 3.3160, -4.7449],
        [ 3.3245, -4.7141],
        [ 1.8745, -6.3556],
        [ 3.1765, -4.7540],
        [ 3.1087, -4.6224],
        [ 3.2790, -4.6683],
        [ 2.6415, -5.2562],
        [ 3.3170, -4.7072],
        [ 3.3364, -4.8798],
        [ 3.3332, -4.7292],
        [ 2.6599, -5.2490],
        [ 3.2063, -4.5974],
        [ 3.0012, -5.2722],
        [ 3.1326, -4.6704],
        [ 3.2817, -4.7931],
        [ 2.8843, -4.7336],
        [ 3.0950, -4.4887],
        [ 3.1327, -4.9369],
        [ 1.8547, -6.4700],
        [ 2.7934, -5.1735],
        [ 2.8114, -5.0994],
        [ 3.0218, -4.5107],
        [ 3.0826, -4.5970],
        [ 3.0276, -4.7042],
        [ 3.1149, -4.6756],
        [ 3.3475, -4.7339],
        [ 3.2717, -4.8463],
        [ 3.0666, -4.8727],
        [ 3.1038, -4.5689],
        [ 3.2952, -4.7293],
        [ 2.4198, -5.4680],
        [ 3.3208, -4.7245],
        [ 2.6262, -5.3567],
        [ 2.7923, -5.4839],
        [ 3.1396, -5.3447],
        [ 2.8768, -5.0989],
        [ 2.9484, -5.0906],
        [ 3.2446, -4.6357],
        [ 3.1440, -4.5327],
        [ 3.3121, -4.8456],
        [ 3.1948, -4.8268],
        [ 2.7994, -5.0927],
        [ 2.7433, -4.9338],
        [ 3.0525, -4.8501],
        [ 3.3479, -4.7375],
        [ 2.7592, -4.5843],
        [ 3.1411, -4.9184],
        [ 3.1052, -4.6270],
        [ 3.3376, -4.7279],
        [ 3.1993, -4.7616],
        [ 3.0057, -4.8180],
        [ 3.0964, -5.1206],
        [ 3.1440, -4.8002],
        [ 3.2411, -4.6459],
        [ 1.7459, -6.3611],
        [ 2.8520, -4.9399],
        [ 2.7776, -5.5281],
        [ 3.3947, -4.8190],
        [ 1.9944, -5.9329],
        [ 2.0346, -5.8311],
        [ 3.1232, -4.6870],
        [ 3.1542, -4.7154],
        [ 2.7364, -5.0825],
        [ 2.8040, -5.1223]], dtype=torch.float64, requires_grad=True)
pi: tensor([[7.7779e-01, 2.2221e-01],
        [9.9994e-01, 6.0364e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9965e-01, 3.5020e-04], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1529, 0.1585],
         [0.7902, 0.2044]],

        [[0.8181, 0.1730],
         [0.5658, 0.0315]],

        [[0.9924, 0.1817],
         [0.6597, 0.8599]],

        [[0.5185, 0.1784],
         [0.8814, 0.5726]],

        [[0.6880, 0.1733],
         [0.5063, 0.5112]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 61
Adjusted Rand Index: -0.015623423336712405
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.002468638218550053
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -0.0013373888019515253
Average Adjusted Rand Index: -0.0030247842003192633
10846.484065894521
new:  [-0.0013373888019515253, -0.0013373888019515253, 0.004162497788148053, -0.0013373888019515253] [-0.0030247842003192633, -0.0030247842003192633, 0.0, -0.0030247842003192633] [10883.602836823757, 10883.6089170661, 10884.027628721706, 10883.56902905647]
prior:  [0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0] [10884.56973910326, 10884.569980278458, 10884.572678234208, nan]
-----------------------------------------------------------------------------------------
This iteration is 10
True Objective function: Loss = -10765.478268091018
Iteration 0: Loss = -17480.388622571714
Iteration 10: Loss = -10852.772311867797
Iteration 20: Loss = -10852.447635424609
Iteration 30: Loss = -10851.805703702725
Iteration 40: Loss = -10851.79036710912
Iteration 50: Loss = -10851.792252069728
1
Iteration 60: Loss = -10851.792977675588
2
Iteration 70: Loss = -10851.793155222647
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.0095, 0.9905],
        [0.0330, 0.9670]], dtype=torch.float64)
alpha: tensor([0.0324, 0.9676])
beta: tensor([[[0.1244, 0.1335],
         [0.9932, 0.1559]],

        [[0.8432, 0.2821],
         [0.8052, 0.5907]],

        [[0.0796, 0.2555],
         [0.8567, 0.5825]],

        [[0.9038, 0.0881],
         [0.2964, 0.3257]],

        [[0.9667, 0.1920],
         [0.9411, 0.3866]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0005519328215568777
Average Adjusted Rand Index: 4.489343139188126e-06
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17854.162886532515
Iteration 100: Loss = -10860.536165510244
Iteration 200: Loss = -10859.451636720489
Iteration 300: Loss = -10859.095654252544
Iteration 400: Loss = -10858.821022584336
Iteration 500: Loss = -10857.998072031105
Iteration 600: Loss = -10857.751901835252
Iteration 700: Loss = -10857.409173168713
Iteration 800: Loss = -10856.903922868592
Iteration 900: Loss = -10856.07725708945
Iteration 1000: Loss = -10855.545989543569
Iteration 1100: Loss = -10855.217580236158
Iteration 1200: Loss = -10855.064950940317
Iteration 1300: Loss = -10854.983966732043
Iteration 1400: Loss = -10854.947319044231
Iteration 1500: Loss = -10854.926804976962
Iteration 1600: Loss = -10854.912496428886
Iteration 1700: Loss = -10854.901035262536
Iteration 1800: Loss = -10854.89173855072
Iteration 1900: Loss = -10854.884158628978
Iteration 2000: Loss = -10854.877897647297
Iteration 2100: Loss = -10854.872609339229
Iteration 2200: Loss = -10854.868062853902
Iteration 2300: Loss = -10854.864116546796
Iteration 2400: Loss = -10854.8607228865
Iteration 2500: Loss = -10854.857679716773
Iteration 2600: Loss = -10854.855021428259
Iteration 2700: Loss = -10854.85261417809
Iteration 2800: Loss = -10854.85046639732
Iteration 2900: Loss = -10854.848562967647
Iteration 3000: Loss = -10854.846785154301
Iteration 3100: Loss = -10854.845236288083
Iteration 3200: Loss = -10854.843769855099
Iteration 3300: Loss = -10854.842416439173
Iteration 3400: Loss = -10854.84119382574
Iteration 3500: Loss = -10854.840061772864
Iteration 3600: Loss = -10854.83902174465
Iteration 3700: Loss = -10854.838049204702
Iteration 3800: Loss = -10854.837095393088
Iteration 3900: Loss = -10854.83624578546
Iteration 4000: Loss = -10854.83546471211
Iteration 4100: Loss = -10854.834682497698
Iteration 4200: Loss = -10854.833991118221
Iteration 4300: Loss = -10854.833300118677
Iteration 4400: Loss = -10854.832586574219
Iteration 4500: Loss = -10854.831923857562
Iteration 4600: Loss = -10854.83127755638
Iteration 4700: Loss = -10854.830617014046
Iteration 4800: Loss = -10854.829932690775
Iteration 4900: Loss = -10854.830050199113
1
Iteration 5000: Loss = -10854.833366231336
2
Iteration 5100: Loss = -10854.827381277108
Iteration 5200: Loss = -10854.827212247144
Iteration 5300: Loss = -10854.824888215771
Iteration 5400: Loss = -10854.822965359464
Iteration 5500: Loss = -10854.830757702219
1
Iteration 5600: Loss = -10854.813812238446
Iteration 5700: Loss = -10854.761466580696
Iteration 5800: Loss = -10854.522172526096
Iteration 5900: Loss = -10854.117221005632
Iteration 6000: Loss = -10854.013810900748
Iteration 6100: Loss = -10853.868776989002
Iteration 6200: Loss = -10853.85119528387
Iteration 6300: Loss = -10853.838553717025
Iteration 6400: Loss = -10853.832764956936
Iteration 6500: Loss = -10853.829350862788
Iteration 6600: Loss = -10853.828862567774
Iteration 6700: Loss = -10853.825308615671
Iteration 6800: Loss = -10853.824046878499
Iteration 6900: Loss = -10853.82530541878
1
Iteration 7000: Loss = -10853.822047044181
Iteration 7100: Loss = -10853.820960463458
Iteration 7200: Loss = -10853.820076620905
Iteration 7300: Loss = -10853.819357657201
Iteration 7400: Loss = -10853.818917984707
Iteration 7500: Loss = -10853.818547564842
Iteration 7600: Loss = -10853.818116850427
Iteration 7700: Loss = -10853.817806819721
Iteration 7800: Loss = -10853.828538260217
1
Iteration 7900: Loss = -10853.814080621944
Iteration 8000: Loss = -10853.81118112559
Iteration 8100: Loss = -10853.818149229493
1
Iteration 8200: Loss = -10853.810639413754
Iteration 8300: Loss = -10853.810163007534
Iteration 8400: Loss = -10854.327529280408
1
Iteration 8500: Loss = -10853.796542820839
Iteration 8600: Loss = -10853.793221303056
Iteration 8700: Loss = -10853.792597468488
Iteration 8800: Loss = -10853.792401626213
Iteration 8900: Loss = -10853.79218952481
Iteration 9000: Loss = -10853.792078966535
Iteration 9100: Loss = -10853.792021725876
Iteration 9200: Loss = -10853.791845225938
Iteration 9300: Loss = -10853.791749284952
Iteration 9400: Loss = -10853.792004801535
1
Iteration 9500: Loss = -10853.79161203904
Iteration 9600: Loss = -10853.791505560033
Iteration 9700: Loss = -10853.791974927562
1
Iteration 9800: Loss = -10853.791382065738
Iteration 9900: Loss = -10853.791329572005
Iteration 10000: Loss = -10853.791278724197
Iteration 10100: Loss = -10853.791174768956
Iteration 10200: Loss = -10853.791194286563
1
Iteration 10300: Loss = -10853.79073979178
Iteration 10400: Loss = -10853.884669075309
1
Iteration 10500: Loss = -10853.790421504013
Iteration 10600: Loss = -10853.790280746072
Iteration 10700: Loss = -10853.79011194256
Iteration 10800: Loss = -10853.79025607591
1
Iteration 10900: Loss = -10853.79006804219
Iteration 11000: Loss = -10853.790040059856
Iteration 11100: Loss = -10853.798383689867
1
Iteration 11200: Loss = -10853.789907530043
Iteration 11300: Loss = -10853.788966968712
Iteration 11400: Loss = -10853.810330728733
1
Iteration 11500: Loss = -10853.78866357131
Iteration 11600: Loss = -10853.788645913277
Iteration 11700: Loss = -10853.838135278093
1
Iteration 11800: Loss = -10853.78861792467
Iteration 11900: Loss = -10853.788553683273
Iteration 12000: Loss = -10853.808757984969
1
Iteration 12100: Loss = -10853.788505192333
Iteration 12200: Loss = -10853.788489261846
Iteration 12300: Loss = -10853.789814758027
1
Iteration 12400: Loss = -10853.774449600038
Iteration 12500: Loss = -10853.774317369014
Iteration 12600: Loss = -10853.777784950349
1
Iteration 12700: Loss = -10853.774207956063
Iteration 12800: Loss = -10853.774186300947
Iteration 12900: Loss = -10853.775089687515
1
Iteration 13000: Loss = -10853.774100908086
Iteration 13100: Loss = -10853.774090484452
Iteration 13200: Loss = -10853.780754576235
1
Iteration 13300: Loss = -10853.774026273291
Iteration 13400: Loss = -10853.774041601666
1
Iteration 13500: Loss = -10853.777900554152
2
Iteration 13600: Loss = -10853.774018591494
Iteration 13700: Loss = -10853.77392421261
Iteration 13800: Loss = -10853.77260448986
Iteration 13900: Loss = -10853.772600577395
Iteration 14000: Loss = -10853.772604764488
1
Iteration 14100: Loss = -10853.77079943921
Iteration 14200: Loss = -10853.772934231563
1
Iteration 14300: Loss = -10853.773412348468
2
Iteration 14400: Loss = -10853.770087978084
Iteration 14500: Loss = -10853.77278146193
1
Iteration 14600: Loss = -10853.770133091099
2
Iteration 14700: Loss = -10853.770153607564
3
Iteration 14800: Loss = -10853.770212399264
4
Iteration 14900: Loss = -10853.770123600994
5
Iteration 15000: Loss = -10853.832911579082
6
Iteration 15100: Loss = -10853.77009405096
7
Iteration 15200: Loss = -10853.769693991577
Iteration 15300: Loss = -10853.769485125325
Iteration 15400: Loss = -10853.771482758437
1
Iteration 15500: Loss = -10853.77775181783
2
Iteration 15600: Loss = -10853.769470571651
Iteration 15700: Loss = -10853.790502153683
1
Iteration 15800: Loss = -10853.769359968204
Iteration 15900: Loss = -10853.889718152306
1
Iteration 16000: Loss = -10853.769270921779
Iteration 16100: Loss = -10853.769125338575
Iteration 16200: Loss = -10853.76956400687
1
Iteration 16300: Loss = -10853.772206417576
2
Iteration 16400: Loss = -10853.769494098438
3
Iteration 16500: Loss = -10853.769811181279
4
Iteration 16600: Loss = -10853.769180938523
5
Iteration 16700: Loss = -10853.769194918226
6
Iteration 16800: Loss = -10853.965302235216
7
Iteration 16900: Loss = -10853.76909233877
Iteration 17000: Loss = -10853.771823645411
1
Iteration 17100: Loss = -10853.768788093173
Iteration 17200: Loss = -10853.771730333001
1
Iteration 17300: Loss = -10853.766340197326
Iteration 17400: Loss = -10853.77917038765
1
Iteration 17500: Loss = -10853.769757905673
2
Iteration 17600: Loss = -10853.89577878262
3
Iteration 17700: Loss = -10853.766263728998
Iteration 17800: Loss = -10853.770044735593
1
Iteration 17900: Loss = -10853.766276686947
2
Iteration 18000: Loss = -10854.000115120645
3
Iteration 18100: Loss = -10853.766213461822
Iteration 18200: Loss = -10853.77090755499
1
Iteration 18300: Loss = -10853.766211526163
Iteration 18400: Loss = -10853.807521618295
1
Iteration 18500: Loss = -10853.765996226222
Iteration 18600: Loss = -10853.767123285463
1
Iteration 18700: Loss = -10853.765851223063
Iteration 18800: Loss = -10853.766188534604
1
Iteration 18900: Loss = -10853.775251233546
2
Iteration 19000: Loss = -10853.765842287094
Iteration 19100: Loss = -10853.77524734647
1
Iteration 19200: Loss = -10853.767073464045
2
Iteration 19300: Loss = -10853.766641279666
3
Iteration 19400: Loss = -10853.766381854219
4
Iteration 19500: Loss = -10853.765983436017
5
Iteration 19600: Loss = -10853.773449632408
6
Iteration 19700: Loss = -10853.765858612478
7
Iteration 19800: Loss = -10853.848042543235
8
Iteration 19900: Loss = -10853.766327057388
9
tensor([[ -5.2019,   0.5866],
        [ -6.3252,   1.7100],
        [-10.3449,   5.7297],
        [ -7.5720,   2.9568],
        [-10.5080,   5.8928],
        [-10.4254,   5.8102],
        [-10.5677,   5.9525],
        [ -5.2072,   0.5920],
        [ -4.7118,   0.0965],
        [ -4.4083,  -0.2070],
        [-10.6270,   6.0118],
        [-10.3894,   5.7742],
        [-10.6782,   6.0629],
        [-10.1936,   5.5784],
        [ -4.8350,   0.2198],
        [-10.6792,   6.0640],
        [ -5.2941,   0.6789],
        [-10.2239,   5.6087],
        [-10.2532,   5.6380],
        [ -5.7407,   1.1255],
        [ -5.6064,   0.9912],
        [ -4.6825,   0.0673],
        [-10.4554,   5.8402],
        [ -7.8611,   3.2459],
        [ -4.4033,  -0.2119],
        [ -0.1793,  -4.4359],
        [ -6.2789,   1.6637],
        [ -7.6145,   2.9993],
        [ -9.4963,   4.8810],
        [ -4.4547,  -0.1606],
        [-10.2198,   5.6046],
        [ -7.6789,   3.0637],
        [ -7.5197,   2.9045],
        [ -5.9360,   1.3207],
        [-10.6828,   6.0675],
        [ -7.2089,   2.5937],
        [ -4.2785,  -0.3367],
        [ -4.1590,  -0.4562],
        [ -4.4815,  -0.1337],
        [-10.8063,   6.1911],
        [-10.7287,   6.1135],
        [ -5.7671,   1.1519],
        [ -7.1294,   2.5142],
        [ -5.5694,   0.9542],
        [ -6.4622,   1.8470],
        [-10.8269,   6.2117],
        [ -4.6790,   0.0638],
        [ -9.9060,   5.2908],
        [ -7.6627,   3.0475],
        [-10.8645,   6.2493],
        [-10.6333,   6.0181],
        [-10.5558,   5.9406],
        [ -8.4503,   3.8351],
        [ -6.3191,   1.7039],
        [-10.2713,   5.6561],
        [-10.9284,   6.3132],
        [ -4.7677,   0.1525],
        [ -7.5905,   2.9752],
        [ -3.9053,  -0.7099],
        [ -6.3235,   1.7083],
        [ -4.2363,  -0.3789],
        [-10.3920,   5.7768],
        [-10.6677,   6.0525],
        [-10.6340,   6.0188],
        [-10.4362,   5.8210],
        [ -0.3619,  -4.2533],
        [ -3.1325,  -1.4827],
        [ -5.8760,   1.2608],
        [-10.7178,   6.1026],
        [-10.5968,   5.9816],
        [-10.4367,   5.8215],
        [ -6.2748,   1.6596],
        [ -6.0815,   1.4662],
        [ -5.9427,   1.3275],
        [ -4.9472,   0.3320],
        [-10.3979,   5.7826],
        [ -5.2428,   0.6276],
        [-10.5622,   5.9469],
        [-10.6869,   6.0717],
        [ -7.5180,   2.9028],
        [ -4.3432,  -0.2720],
        [ -5.9519,   1.3367],
        [-10.7012,   6.0860],
        [ -4.6725,   0.0573],
        [-10.2245,   5.6093],
        [ -3.4198,  -1.1954],
        [ -6.6183,   2.0030],
        [ -4.1094,  -0.5058],
        [-10.7371,   6.1219],
        [ -7.0892,   2.4740],
        [ -6.8843,   2.2691],
        [ -6.8518,   2.2365],
        [ -4.8241,   0.2089],
        [ -5.7514,   1.1362],
        [ -6.5257,   1.9105],
        [ -2.0752,  -2.5401],
        [ -6.3770,   1.7618],
        [-10.8169,   6.2017],
        [-11.0020,   6.3867],
        [-10.6673,   6.0520]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 1.6646e-06],
        [8.7143e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0312, 0.9688], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2623, 0.1455],
         [0.9932, 0.1575]],

        [[0.8432, 0.2777],
         [0.8052, 0.5907]],

        [[0.0796, 0.1796],
         [0.8567, 0.5825]],

        [[0.9038, 0.1592],
         [0.2964, 0.3257]],

        [[0.9667, 0.1811],
         [0.9411, 0.3866]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: -0.0041478895259491316
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.002056465888134684
Global Adjusted Rand Index: 0.00021560637723492266
Average Adjusted Rand Index: -0.0018303052465242532
Iteration 0: Loss = -20078.371144383476
Iteration 10: Loss = -10858.820856396003
Iteration 20: Loss = -10855.569730697816
Iteration 30: Loss = -10851.874264326987
Iteration 40: Loss = -10851.856705002398
Iteration 50: Loss = -10851.793318494992
Iteration 60: Loss = -10851.793347976543
1
Iteration 70: Loss = -10851.793203663578
Iteration 80: Loss = -10851.793132051036
Iteration 90: Loss = -10851.793159594612
1
Iteration 100: Loss = -10851.793197491368
2
Iteration 110: Loss = -10851.793206453776
3
Stopping early at iteration 109 due to no improvement.
pi: tensor([[0.9670, 0.0330],
        [0.9906, 0.0094]], dtype=torch.float64)
alpha: tensor([0.9676, 0.0324])
beta: tensor([[[0.1559, 0.1335],
         [0.2567, 0.1244]],

        [[0.1178, 0.2821],
         [0.0751, 0.3207]],

        [[0.9190, 0.2555],
         [0.2758, 0.3746]],

        [[0.6330, 0.0881],
         [0.6631, 0.7323]],

        [[0.8153, 0.1920],
         [0.2122, 0.2805]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.002764517368444376
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0005519328215568777
Average Adjusted Rand Index: 4.489343139188126e-06
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20077.85022085439
Iteration 100: Loss = -10861.174609000485
Iteration 200: Loss = -10859.419628939324
Iteration 300: Loss = -10856.46577119225
Iteration 400: Loss = -10854.421649201377
Iteration 500: Loss = -10853.528633985921
Iteration 600: Loss = -10853.096905956772
Iteration 700: Loss = -10852.77914291797
Iteration 800: Loss = -10852.506459049035
Iteration 900: Loss = -10852.277381763024
Iteration 1000: Loss = -10852.099437060775
Iteration 1100: Loss = -10851.966838649301
Iteration 1200: Loss = -10851.868192865246
Iteration 1300: Loss = -10851.793585635029
Iteration 1400: Loss = -10851.736099796053
Iteration 1500: Loss = -10851.690991807902
Iteration 1600: Loss = -10851.655092007233
Iteration 1700: Loss = -10851.626153270536
Iteration 1800: Loss = -10851.60260001043
Iteration 1900: Loss = -10851.583189750065
Iteration 2000: Loss = -10851.567061778356
Iteration 2100: Loss = -10851.553545365794
Iteration 2200: Loss = -10851.542067304177
Iteration 2300: Loss = -10851.532287356864
Iteration 2400: Loss = -10851.52381672803
Iteration 2500: Loss = -10851.51646671613
Iteration 2600: Loss = -10851.509953749082
Iteration 2700: Loss = -10851.504138186852
Iteration 2800: Loss = -10851.498915076314
Iteration 2900: Loss = -10851.494197010621
Iteration 3000: Loss = -10851.489894167811
Iteration 3100: Loss = -10851.485956804438
Iteration 3200: Loss = -10851.48224563998
Iteration 3300: Loss = -10851.47883232149
Iteration 3400: Loss = -10851.475683641043
Iteration 3500: Loss = -10851.472767789299
Iteration 3600: Loss = -10851.470113431178
Iteration 3700: Loss = -10851.467642441185
Iteration 3800: Loss = -10851.465392952581
Iteration 3900: Loss = -10851.463345367682
Iteration 4000: Loss = -10851.461430214147
Iteration 4100: Loss = -10851.45965454882
Iteration 4200: Loss = -10851.458010349123
Iteration 4300: Loss = -10851.456489125667
Iteration 4400: Loss = -10851.455011368691
Iteration 4500: Loss = -10851.453605504477
Iteration 4600: Loss = -10851.452278416195
Iteration 4700: Loss = -10851.450983836903
Iteration 4800: Loss = -10851.44975017562
Iteration 4900: Loss = -10851.44857764563
Iteration 5000: Loss = -10851.447420622395
Iteration 5100: Loss = -10851.446319737874
Iteration 5200: Loss = -10851.445223283687
Iteration 5300: Loss = -10851.444161500669
Iteration 5400: Loss = -10851.443102585774
Iteration 5500: Loss = -10851.442113735713
Iteration 5600: Loss = -10851.44119635384
Iteration 5700: Loss = -10851.440263918028
Iteration 5800: Loss = -10851.439330817533
Iteration 5900: Loss = -10851.438916810283
Iteration 6000: Loss = -10851.439873870579
1
Iteration 6100: Loss = -10851.436747662383
Iteration 6200: Loss = -10851.436585427959
Iteration 6300: Loss = -10851.43546586352
Iteration 6400: Loss = -10851.434245164199
Iteration 6500: Loss = -10851.433551016551
Iteration 6600: Loss = -10851.432630080699
Iteration 6700: Loss = -10851.43182185273
Iteration 6800: Loss = -10851.431529589665
Iteration 6900: Loss = -10851.430024072395
Iteration 7000: Loss = -10851.433337278195
1
Iteration 7100: Loss = -10851.427815062052
Iteration 7200: Loss = -10851.426152987613
Iteration 7300: Loss = -10851.45525743325
1
Iteration 7400: Loss = -10851.418435477952
Iteration 7500: Loss = -10851.403046483849
Iteration 7600: Loss = -10851.3943109417
Iteration 7700: Loss = -10851.307131660866
Iteration 7800: Loss = -10851.282734700982
Iteration 7900: Loss = -10851.277949778578
Iteration 8000: Loss = -10851.277439603724
Iteration 8100: Loss = -10851.293917043795
1
Iteration 8200: Loss = -10851.272039411293
Iteration 8300: Loss = -10851.290384614695
1
Iteration 8400: Loss = -10851.271356192156
Iteration 8500: Loss = -10851.418977440702
1
Iteration 8600: Loss = -10851.27087774394
Iteration 8700: Loss = -10851.270728035935
Iteration 8800: Loss = -10851.270811831257
1
Iteration 8900: Loss = -10851.270447924695
Iteration 9000: Loss = -10851.270182143699
Iteration 9100: Loss = -10851.271592640187
1
Iteration 9200: Loss = -10851.296300714692
2
Iteration 9300: Loss = -10851.281848180612
3
Iteration 9400: Loss = -10851.270018294228
Iteration 9500: Loss = -10851.26965132092
Iteration 9600: Loss = -10851.26982130937
1
Iteration 9700: Loss = -10851.486570386092
2
Iteration 9800: Loss = -10851.270373661804
3
Iteration 9900: Loss = -10851.274283746796
4
Iteration 10000: Loss = -10851.269186218522
Iteration 10100: Loss = -10851.287306534716
1
Iteration 10200: Loss = -10851.269070897037
Iteration 10300: Loss = -10851.27197013301
1
Iteration 10400: Loss = -10851.268930551478
Iteration 10500: Loss = -10851.281602249837
1
Iteration 10600: Loss = -10851.269460445934
2
Iteration 10700: Loss = -10851.279375702406
3
Iteration 10800: Loss = -10851.268813687419
Iteration 10900: Loss = -10851.268669805197
Iteration 11000: Loss = -10851.268989412516
1
Iteration 11100: Loss = -10851.268630205836
Iteration 11200: Loss = -10851.27177663789
1
Iteration 11300: Loss = -10851.268739579327
2
Iteration 11400: Loss = -10851.268515719556
Iteration 11500: Loss = -10851.294791593002
1
Iteration 11600: Loss = -10851.268450423313
Iteration 11700: Loss = -10851.26853628038
1
Iteration 11800: Loss = -10851.268388674513
Iteration 11900: Loss = -10851.268525698993
1
Iteration 12000: Loss = -10851.26841510132
2
Iteration 12100: Loss = -10851.268276051102
Iteration 12200: Loss = -10851.268541546546
1
Iteration 12300: Loss = -10851.26824084473
Iteration 12400: Loss = -10851.269203248985
1
Iteration 12500: Loss = -10851.26820437052
Iteration 12600: Loss = -10851.268304519472
1
Iteration 12700: Loss = -10851.271443937916
2
Iteration 12800: Loss = -10851.284386741723
3
Iteration 12900: Loss = -10851.268247918777
4
Iteration 13000: Loss = -10851.268146258828
Iteration 13100: Loss = -10851.268435131846
1
Iteration 13200: Loss = -10851.268102195974
Iteration 13300: Loss = -10851.26963827812
1
Iteration 13400: Loss = -10851.268051808542
Iteration 13500: Loss = -10851.345147513053
1
Iteration 13600: Loss = -10851.26805527024
2
Iteration 13700: Loss = -10851.268033644987
Iteration 13800: Loss = -10851.26869224654
1
Iteration 13900: Loss = -10851.268014832745
Iteration 14000: Loss = -10851.268065248112
1
Iteration 14100: Loss = -10851.434142427437
2
Iteration 14200: Loss = -10851.270205734
3
Iteration 14300: Loss = -10851.505254616459
4
Iteration 14400: Loss = -10851.267978172817
Iteration 14500: Loss = -10851.273373900714
1
Iteration 14600: Loss = -10851.267966431267
Iteration 14700: Loss = -10851.269878635176
1
Iteration 14800: Loss = -10851.26833120753
2
Iteration 14900: Loss = -10851.275040971455
3
Iteration 15000: Loss = -10851.279637414575
4
Iteration 15100: Loss = -10851.312211746173
5
Iteration 15200: Loss = -10851.313436686705
6
Iteration 15300: Loss = -10851.268890069055
7
Iteration 15400: Loss = -10851.26793363442
Iteration 15500: Loss = -10851.273181199405
1
Iteration 15600: Loss = -10851.296296797185
2
Iteration 15700: Loss = -10851.267917313176
Iteration 15800: Loss = -10851.272471200973
1
Iteration 15900: Loss = -10851.26883493689
2
Iteration 16000: Loss = -10851.268770094712
3
Iteration 16100: Loss = -10851.283855966229
4
Iteration 16200: Loss = -10851.268168725184
5
Iteration 16300: Loss = -10851.268224199686
6
Iteration 16400: Loss = -10851.267863777515
Iteration 16500: Loss = -10851.26819934293
1
Iteration 16600: Loss = -10851.267997431472
2
Iteration 16700: Loss = -10851.267897214047
3
Iteration 16800: Loss = -10851.26954523028
4
Iteration 16900: Loss = -10851.27170295337
5
Iteration 17000: Loss = -10851.267847861705
Iteration 17100: Loss = -10851.27226252564
1
Iteration 17200: Loss = -10851.290104820348
2
Iteration 17300: Loss = -10851.268783477304
3
Iteration 17400: Loss = -10851.26786965962
4
Iteration 17500: Loss = -10851.28890832261
5
Iteration 17600: Loss = -10851.27263348462
6
Iteration 17700: Loss = -10851.27031716643
7
Iteration 17800: Loss = -10851.275813710035
8
Iteration 17900: Loss = -10851.328946857386
9
Iteration 18000: Loss = -10851.269210053202
10
Stopping early at iteration 18000 due to no improvement.
tensor([[-0.7602, -1.1977],
        [-0.7840, -1.1896],
        [-0.5969, -1.0346],
        [-0.4338, -0.9713],
        [-0.7612, -1.2762],
        [-1.0708, -1.7445],
        [-0.4548, -0.9959],
        [-0.5204, -0.9828],
        [-0.7139, -1.2086],
        [-0.5712, -0.9706],
        [-1.4760, -1.8832],
        [-0.3564, -1.0301],
        [-0.5881, -0.9659],
        [-0.7803, -1.1946],
        [-0.6278, -0.9618],
        [-0.7552, -1.1338],
        [-1.2358, -1.7626],
        [-0.8903, -1.2712],
        [-0.9260, -1.4705],
        [-0.8681, -1.2491],
        [-0.8667, -1.2448],
        [-0.6942, -1.2311],
        [-0.7718, -1.2576],
        [-0.4452, -0.9833],
        [-0.4803, -0.9184],
        [ 1.0520, -2.5270],
        [-0.9233, -1.3817],
        [-0.5744, -0.9238],
        [-0.8551, -1.1937],
        [-0.9472, -1.3609],
        [-0.5081, -0.9164],
        [-1.5557, -1.9893],
        [-0.7258, -1.0772],
        [-2.0087, -2.6065],
        [-1.1121, -1.6496],
        [-0.8725, -1.3573],
        [-0.4994, -0.8869],
        [-1.4054, -1.9805],
        [-0.4799, -0.9986],
        [-0.5991, -1.1096],
        [-0.9834, -1.4150],
        [-1.3365, -1.6958],
        [-0.7365, -0.9866],
        [-0.6392, -1.0794],
        [-1.4907, -2.1129],
        [-1.3504, -1.8622],
        [-0.7139, -1.0971],
        [-0.6115, -0.9437],
        [-0.5604, -0.8850],
        [-0.3350, -1.0859],
        [-0.7345, -1.0881],
        [-0.3721, -1.0160],
        [-0.6860, -1.0371],
        [-0.7698, -1.2279],
        [-0.5706, -1.0114],
        [-0.4257, -0.9880],
        [-0.4663, -1.0349],
        [-0.9130, -1.4034],
        [-0.4311, -0.9790],
        [-0.4367, -1.0534],
        [-0.6784, -0.9916],
        [-1.3234, -1.8124],
        [-0.5192, -0.9252],
        [-0.4012, -0.9954],
        [-0.6604, -1.0937],
        [ 1.1677, -3.0750],
        [-0.4352, -0.9717],
        [-0.4700, -1.0124],
        [-1.3014, -1.7593],
        [-1.2300, -1.7189],
        [-0.4550, -0.9445],
        [-0.8431, -1.2509],
        [-0.9619, -1.5261],
        [-0.4339, -0.9613],
        [-0.6241, -0.9251],
        [-0.7915, -1.3359],
        [-0.4535, -1.0492],
        [-0.5272, -1.0691],
        [-0.5676, -0.9742],
        [-1.7622, -2.1697],
        [-0.5405, -0.8727],
        [-0.7181, -1.3136],
        [-0.5329, -0.9372],
        [-0.4566, -0.9559],
        [-0.9213, -1.2213],
        [-0.4396, -1.0313],
        [-1.0535, -1.4849],
        [-0.6383, -1.1351],
        [-0.4783, -0.9105],
        [-0.6988, -1.1862],
        [-0.7477, -1.2044],
        [-0.5717, -0.9822],
        [-0.8629, -1.4163],
        [-0.6749, -1.2500],
        [-0.8093, -1.2955],
        [-0.4968, -1.7971],
        [-0.4255, -0.9630],
        [-0.4569, -0.9690],
        [-1.0061, -1.6229],
        [-0.5922, -1.2645]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.5810e-01, 4.1900e-02],
        [1.0000e+00, 3.9867e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6239, 0.3761], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1577, 0.1538],
         [0.2567, 0.1513]],

        [[0.1178, 0.2846],
         [0.0751, 0.3207]],

        [[0.9190, 0.2501],
         [0.2758, 0.3746]],

        [[0.6330, 0.0904],
         [0.6631, 0.7323]],

        [[0.8153, 0.1925],
         [0.2122, 0.2805]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0008065306474943187
Average Adjusted Rand Index: -0.00018703005664877213
Iteration 0: Loss = -28478.040007330954
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.7449,    nan]],

        [[0.2969,    nan],
         [0.1456, 0.7442]],

        [[0.9089,    nan],
         [0.7351, 0.7121]],

        [[0.3498,    nan],
         [0.4778, 0.8807]],

        [[0.5619,    nan],
         [0.6576, 0.8178]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28475.349081699344
Iteration 100: Loss = -10865.202406968208
Iteration 200: Loss = -10862.52857981203
Iteration 300: Loss = -10861.308439598044
Iteration 400: Loss = -10860.564678214922
Iteration 500: Loss = -10860.072803447001
Iteration 600: Loss = -10859.723323014689
Iteration 700: Loss = -10859.433912401439
Iteration 800: Loss = -10858.767247116113
Iteration 900: Loss = -10858.146917534967
Iteration 1000: Loss = -10857.78888221338
Iteration 1100: Loss = -10857.52255490981
Iteration 1200: Loss = -10857.287713589743
Iteration 1300: Loss = -10856.975287271503
Iteration 1400: Loss = -10855.765189528458
Iteration 1500: Loss = -10855.052680329727
Iteration 1600: Loss = -10854.641700658818
Iteration 1700: Loss = -10854.286752464363
Iteration 1800: Loss = -10853.928428379264
Iteration 1900: Loss = -10853.562778889154
Iteration 2000: Loss = -10853.214404537472
Iteration 2100: Loss = -10852.898106824916
Iteration 2200: Loss = -10852.621735663568
Iteration 2300: Loss = -10852.389050409673
Iteration 2400: Loss = -10852.19617172016
Iteration 2500: Loss = -10852.038536819606
Iteration 2600: Loss = -10851.912493262364
Iteration 2700: Loss = -10851.814155244145
Iteration 2800: Loss = -10851.738799883407
Iteration 2900: Loss = -10851.68150722765
Iteration 3000: Loss = -10851.638218408894
Iteration 3100: Loss = -10851.605601477424
Iteration 3200: Loss = -10851.581130764984
Iteration 3300: Loss = -10851.56276214559
Iteration 3400: Loss = -10851.549013226471
Iteration 3500: Loss = -10851.538526955048
Iteration 3600: Loss = -10851.53037428568
Iteration 3700: Loss = -10851.52396094724
Iteration 3800: Loss = -10851.518751024656
Iteration 3900: Loss = -10851.514400225044
Iteration 4000: Loss = -10851.510648063393
Iteration 4100: Loss = -10851.507309247585
Iteration 4200: Loss = -10851.504326569737
Iteration 4300: Loss = -10851.501538207196
Iteration 4400: Loss = -10851.498978087602
Iteration 4500: Loss = -10851.496481133174
Iteration 4600: Loss = -10851.4941216861
Iteration 4700: Loss = -10851.491854284503
Iteration 4800: Loss = -10851.489647186272
Iteration 4900: Loss = -10851.487457888115
Iteration 5000: Loss = -10851.485368799384
Iteration 5100: Loss = -10851.483369140415
Iteration 5200: Loss = -10851.481202964085
Iteration 5300: Loss = -10851.479135576961
Iteration 5400: Loss = -10851.748023213224
1
Iteration 5500: Loss = -10851.475080091455
Iteration 5600: Loss = -10851.473078141245
Iteration 5700: Loss = -10851.471077380733
Iteration 5800: Loss = -10851.471211165079
1
Iteration 5900: Loss = -10851.467217626841
Iteration 6000: Loss = -10851.465381567796
Iteration 6100: Loss = -10851.463652007324
Iteration 6200: Loss = -10851.462463676837
Iteration 6300: Loss = -10851.460442131669
Iteration 6400: Loss = -10851.458989351628
Iteration 6500: Loss = -10851.457504402533
Iteration 6600: Loss = -10851.457024319354
Iteration 6700: Loss = -10851.454831527179
Iteration 6800: Loss = -10851.453564562593
Iteration 6900: Loss = -10852.006070948948
1
Iteration 7000: Loss = -10851.45102349094
Iteration 7100: Loss = -10851.449802873978
Iteration 7200: Loss = -10851.44855859396
Iteration 7300: Loss = -10851.448546401542
Iteration 7400: Loss = -10851.446149985708
Iteration 7500: Loss = -10851.444969979639
Iteration 7600: Loss = -10851.463589912628
1
Iteration 7700: Loss = -10851.442686475973
Iteration 7800: Loss = -10851.441536449654
Iteration 7900: Loss = -10851.440508681811
Iteration 8000: Loss = -10851.444327257415
1
Iteration 8100: Loss = -10851.438374511843
Iteration 8200: Loss = -10851.437322433554
Iteration 8300: Loss = -10851.436654049981
Iteration 8400: Loss = -10851.435482022283
Iteration 8500: Loss = -10851.43448249847
Iteration 8600: Loss = -10851.433602234029
Iteration 8700: Loss = -10851.432701013813
Iteration 8800: Loss = -10851.43180238351
Iteration 8900: Loss = -10851.430872984378
Iteration 9000: Loss = -10851.431075598046
1
Iteration 9100: Loss = -10851.428880826807
Iteration 9200: Loss = -10851.427623848604
Iteration 9300: Loss = -10851.427717643588
1
Iteration 9400: Loss = -10851.422799543689
Iteration 9500: Loss = -10851.415190026992
Iteration 9600: Loss = -10851.445621601873
1
Iteration 9700: Loss = -10851.332166687247
Iteration 9800: Loss = -10851.286371163189
Iteration 9900: Loss = -10851.284723111157
Iteration 10000: Loss = -10851.317873937802
1
Iteration 10100: Loss = -10851.274236341329
Iteration 10200: Loss = -10851.271743701345
Iteration 10300: Loss = -10851.279844232093
1
Iteration 10400: Loss = -10851.27104004433
Iteration 10500: Loss = -10851.27368262121
1
Iteration 10600: Loss = -10851.270590789914
Iteration 10700: Loss = -10851.27152675088
1
Iteration 10800: Loss = -10851.270324575744
Iteration 10900: Loss = -10851.270125859364
Iteration 11000: Loss = -10851.27002909535
Iteration 11100: Loss = -10851.269822104727
Iteration 11200: Loss = -10851.270024588395
1
Iteration 11300: Loss = -10851.271766907796
2
Iteration 11400: Loss = -10851.411238760942
3
Iteration 11500: Loss = -10851.272213391498
4
Iteration 11600: Loss = -10851.26949632672
Iteration 11700: Loss = -10851.27380078436
1
Iteration 11800: Loss = -10851.286961735106
2
Iteration 11900: Loss = -10851.275697763404
3
Iteration 12000: Loss = -10851.26916065609
Iteration 12100: Loss = -10851.269098093462
Iteration 12200: Loss = -10851.275175366362
1
Iteration 12300: Loss = -10851.269762782227
2
Iteration 12400: Loss = -10851.270491692609
3
Iteration 12500: Loss = -10851.28866010423
4
Iteration 12600: Loss = -10851.268792979083
Iteration 12700: Loss = -10851.268734271183
Iteration 12800: Loss = -10851.268798198525
1
Iteration 12900: Loss = -10851.268599582787
Iteration 13000: Loss = -10851.26909435002
1
Iteration 13100: Loss = -10851.268542292173
Iteration 13200: Loss = -10851.270106823182
1
Iteration 13300: Loss = -10851.268925740074
2
Iteration 13400: Loss = -10851.269160068918
3
Iteration 13500: Loss = -10851.271512236608
4
Iteration 13600: Loss = -10851.270732545898
5
Iteration 13700: Loss = -10851.268575958333
6
Iteration 13800: Loss = -10851.26876693412
7
Iteration 13900: Loss = -10851.271757731281
8
Iteration 14000: Loss = -10851.268331084446
Iteration 14100: Loss = -10851.283054602485
1
Iteration 14200: Loss = -10851.26821649169
Iteration 14300: Loss = -10851.27323291807
1
Iteration 14400: Loss = -10851.268175473366
Iteration 14500: Loss = -10851.268739779693
1
Iteration 14600: Loss = -10851.27204777477
2
Iteration 14700: Loss = -10851.285153584236
3
Iteration 14800: Loss = -10851.268134999173
Iteration 14900: Loss = -10851.38035583605
1
Iteration 15000: Loss = -10851.268118888165
Iteration 15100: Loss = -10851.282683808671
1
Iteration 15200: Loss = -10851.268264995591
2
Iteration 15300: Loss = -10851.268051834362
Iteration 15400: Loss = -10851.269550867248
1
Iteration 15500: Loss = -10851.268048469126
Iteration 15600: Loss = -10851.29394376817
1
Iteration 15700: Loss = -10851.268034662902
Iteration 15800: Loss = -10851.268396894282
1
Iteration 15900: Loss = -10851.280376985167
2
Iteration 16000: Loss = -10851.268120230861
3
Iteration 16100: Loss = -10851.268159259647
4
Iteration 16200: Loss = -10851.274558113946
5
Iteration 16300: Loss = -10851.275618814987
6
Iteration 16400: Loss = -10851.268159139703
7
Iteration 16500: Loss = -10851.269785772402
8
Iteration 16600: Loss = -10851.269128285458
9
Iteration 16700: Loss = -10851.421511900844
10
Stopping early at iteration 16700 due to no improvement.
tensor([[-0.9175, -0.4929],
        [-0.9725, -0.5824],
        [-0.9301, -0.5028],
        [-1.1561, -0.6052],
        [-1.0564, -0.5303],
        [-1.0988, -0.4075],
        [-1.0401, -0.4849],
        [-0.9764, -0.5188],
        [-0.9465, -0.4499],
        [-0.9237, -0.5407],
        [-1.7353, -1.3435],
        [-1.3980, -0.7067],
        [-1.5222, -1.1604],
        [-0.8979, -0.4990],
        [-0.9565, -0.6393],
        [-2.4888, -2.1265],
        [-1.1459, -0.6102],
        [-1.4220, -1.0575],
        [-1.4464, -0.8880],
        [-0.9891, -0.6245],
        [-1.4223, -1.0601],
        [-1.1242, -0.5742],
        [-1.5628, -1.0751],
        [-1.0417, -0.4903],
        [-0.9449, -0.5237],
        [-2.5085,  1.0507],
        [-1.2230, -0.7726],
        [-1.0661, -0.7326],
        [-1.0098, -0.6881],
        [-0.8928, -0.4937],
        [-0.9134, -0.5201],
        [-2.0453, -1.6245],
        [-0.9707, -0.6356],
        [-2.6149, -2.0004],
        [-1.1559, -0.6050],
        [-1.3169, -0.8307],
        [-0.9774, -0.6068],
        [-1.2989, -0.7084],
        [-1.3635, -0.8354],
        [-0.9548, -0.4353],
        [-0.9759, -0.5575],
        [-0.9068, -0.5643],
        [-0.8130, -0.5795],
        [-1.4446, -1.0175],
        [-1.3443, -0.7050],
        [-1.0462, -0.5260],
        [-0.8824, -0.5157],
        [-0.8670, -0.5516],
        [-2.2089, -1.9007],
        [-1.0832, -0.3136],
        [-1.2028, -0.8659],
        [-1.4307, -0.7694],
        [-1.0807, -0.7458],
        [-0.9975, -0.5476],
        [-1.1131, -0.6854],
        [-1.2471, -0.6696],
        [-1.4411, -0.8609],
        [-0.9500, -0.4532],
        [-1.2984, -0.7451],
        [-1.0101, -0.3763],
        [-1.2083, -0.9117],
        [-1.2482, -0.7561],
        [-1.1968, -0.8062],
        [-1.2347, -0.6239],
        [-1.0833, -0.6631],
        [-2.9125,  1.3038],
        [-2.3850, -1.8406],
        [-1.1959, -0.6399],
        [-1.2011, -0.7518],
        [-1.0689, -0.5758],
        [-1.1817, -0.6888],
        [-0.9793, -0.5863],
        [-1.2944, -0.7148],
        [-0.9694, -0.4350],
        [-1.8751, -1.5909],
        [-0.9746, -0.4174],
        [-1.1061, -0.4941],
        [-0.9887, -0.4328],
        [-0.9039, -0.5124],
        [-1.6636, -1.2710],
        [-0.9926, -0.6773],
        [-1.0707, -0.4591],
        [-0.9275, -0.5384],
        [-1.8160, -1.3102],
        [-1.0657, -0.7828],
        [-0.9965, -0.4177],
        [-1.2852, -0.8670],
        [-1.0961, -0.5908],
        [-1.3122, -0.8930],
        [-1.1566, -0.6661],
        [-0.9188, -0.4711],
        [-1.8653, -1.4692],
        [-1.2205, -0.6548],
        [-1.1484, -0.5578],
        [-0.9580, -0.4701],
        [-1.3475, -0.0664],
        [-0.9700, -0.4195],
        [-0.9713, -0.4498],
        [-1.1855, -0.5517],
        [-1.3215, -0.6308]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.9626e-05, 9.9998e-01],
        [4.2352e-02, 9.5765e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3746, 0.6254], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1524, 0.1547],
         [0.7449, 0.1565]],

        [[0.2969, 0.2823],
         [0.1456, 0.7442]],

        [[0.9089, 0.2489],
         [0.7351, 0.7121]],

        [[0.3498, 0.0906],
         [0.4778, 0.8807]],

        [[0.5619, 0.1933],
         [0.6576, 0.8178]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0008065306474943187
Average Adjusted Rand Index: -0.00018703005664877213
Iteration 0: Loss = -23626.40781241897
Iteration 10: Loss = -10852.63461806586
Iteration 20: Loss = -10852.26421672961
Iteration 30: Loss = -10851.801095571242
Iteration 40: Loss = -10851.79050475364
Iteration 50: Loss = -10851.79235694932
1
Iteration 60: Loss = -10851.793001198565
2
Iteration 70: Loss = -10851.793155168381
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.9670, 0.0330],
        [0.9905, 0.0095]], dtype=torch.float64)
alpha: tensor([0.9676, 0.0324])
beta: tensor([[[0.1559, 0.1335],
         [0.8129, 0.1244]],

        [[0.2399, 0.2821],
         [0.4896, 0.4619]],

        [[0.1884, 0.2555],
         [0.4548, 0.1728]],

        [[0.4304, 0.0881],
         [0.9107, 0.9328]],

        [[0.9431, 0.1920],
         [0.7682, 0.6576]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.002764517368444376
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0005519328215568777
Average Adjusted Rand Index: 4.489343139188126e-06
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23625.823074574706
Iteration 100: Loss = -10882.303756645104
Iteration 200: Loss = -10864.499603119706
Iteration 300: Loss = -10861.72068378571
Iteration 400: Loss = -10857.037722740537
Iteration 500: Loss = -10855.883293043007
Iteration 600: Loss = -10855.23121725741
Iteration 700: Loss = -10854.700364442011
Iteration 800: Loss = -10854.337806912648
Iteration 900: Loss = -10854.034234974117
Iteration 1000: Loss = -10853.784294790814
Iteration 1100: Loss = -10853.586290705058
Iteration 1200: Loss = -10853.408824392229
Iteration 1300: Loss = -10853.223752320999
Iteration 1400: Loss = -10853.06241621734
Iteration 1500: Loss = -10852.934663006707
Iteration 1600: Loss = -10852.82794023058
Iteration 1700: Loss = -10852.738297963135
Iteration 1800: Loss = -10852.659793525321
Iteration 1900: Loss = -10852.588218045496
Iteration 2000: Loss = -10852.525252741849
Iteration 2100: Loss = -10852.466653081168
Iteration 2200: Loss = -10852.406824823833
Iteration 2300: Loss = -10852.350276568093
Iteration 2400: Loss = -10852.310586340865
Iteration 2500: Loss = -10852.279283909322
Iteration 2600: Loss = -10852.246336622009
Iteration 2700: Loss = -10852.206782736546
Iteration 2800: Loss = -10852.134446042026
Iteration 2900: Loss = -10851.978437163722
Iteration 3000: Loss = -10851.75810894434
Iteration 3100: Loss = -10851.614931639488
Iteration 3200: Loss = -10851.549257116973
Iteration 3300: Loss = -10851.517629972543
Iteration 3400: Loss = -10851.500235081929
Iteration 3500: Loss = -10851.489365120866
Iteration 3600: Loss = -10851.481842590516
Iteration 3700: Loss = -10851.4761058507
Iteration 3800: Loss = -10851.471562000776
Iteration 3900: Loss = -10851.467843368211
Iteration 4000: Loss = -10851.464734232948
Iteration 4100: Loss = -10851.462031270437
Iteration 4200: Loss = -10851.45973603436
Iteration 4300: Loss = -10851.457694129214
Iteration 4400: Loss = -10851.455844913558
Iteration 4500: Loss = -10851.454122410887
Iteration 4600: Loss = -10851.452520967792
Iteration 4700: Loss = -10851.451047282804
Iteration 4800: Loss = -10851.449623083237
Iteration 4900: Loss = -10851.448282343568
Iteration 5000: Loss = -10851.447076256336
Iteration 5100: Loss = -10851.445924360249
Iteration 5200: Loss = -10851.44488984111
Iteration 5300: Loss = -10851.443869878507
Iteration 5400: Loss = -10851.4430002459
Iteration 5500: Loss = -10851.442197876957
Iteration 5600: Loss = -10851.4414536214
Iteration 5700: Loss = -10851.440756698446
Iteration 5800: Loss = -10851.440146786497
Iteration 5900: Loss = -10851.439607091228
Iteration 6000: Loss = -10851.439097316406
Iteration 6100: Loss = -10851.438619407936
Iteration 6200: Loss = -10851.44319712686
1
Iteration 6300: Loss = -10851.437987954461
Iteration 6400: Loss = -10851.43750106522
Iteration 6500: Loss = -10851.437182034755
Iteration 6600: Loss = -10851.436775955637
Iteration 6700: Loss = -10851.587700154778
1
Iteration 6800: Loss = -10851.43622382328
Iteration 6900: Loss = -10851.43594377916
Iteration 7000: Loss = -10851.437639403994
1
Iteration 7100: Loss = -10851.435357133447
Iteration 7200: Loss = -10851.43417536174
Iteration 7300: Loss = -10851.429455891335
Iteration 7400: Loss = -10851.42617117531
Iteration 7500: Loss = -10851.423185235122
Iteration 7600: Loss = -10851.418533292337
Iteration 7700: Loss = -10851.360915029776
Iteration 7800: Loss = -10851.280909509864
Iteration 7900: Loss = -10851.27261986157
Iteration 8000: Loss = -10851.271451772429
Iteration 8100: Loss = -10851.271827454004
1
Iteration 8200: Loss = -10851.27070132602
Iteration 8300: Loss = -10851.270861460132
1
Iteration 8400: Loss = -10851.379733534013
2
Iteration 8500: Loss = -10851.269934007989
Iteration 8600: Loss = -10851.329658784589
1
Iteration 8700: Loss = -10851.269623835424
Iteration 8800: Loss = -10851.302207075181
1
Iteration 8900: Loss = -10851.269419405087
Iteration 9000: Loss = -10851.26998550047
1
Iteration 9100: Loss = -10851.269262099413
Iteration 9200: Loss = -10851.282371290052
1
Iteration 9300: Loss = -10851.276277352214
2
Iteration 9400: Loss = -10851.272408037295
3
Iteration 9500: Loss = -10851.274497951037
4
Iteration 9600: Loss = -10851.383122876225
5
Iteration 9700: Loss = -10851.269732778865
6
Iteration 9800: Loss = -10851.272038552912
7
Iteration 9900: Loss = -10851.275031961966
8
Iteration 10000: Loss = -10851.268775607161
Iteration 10100: Loss = -10851.416479481406
1
Iteration 10200: Loss = -10851.268661477125
Iteration 10300: Loss = -10851.269378695795
1
Iteration 10400: Loss = -10851.268600157993
Iteration 10500: Loss = -10851.268636202856
1
Iteration 10600: Loss = -10851.268616720274
2
Iteration 10700: Loss = -10851.27241090562
3
Iteration 10800: Loss = -10851.268460570507
Iteration 10900: Loss = -10851.271022611107
1
Iteration 11000: Loss = -10851.26839618211
Iteration 11100: Loss = -10851.268570463633
1
Iteration 11200: Loss = -10851.26938585708
2
Iteration 11300: Loss = -10851.269253615426
3
Iteration 11400: Loss = -10851.272343697128
4
Iteration 11500: Loss = -10851.279391229658
5
Iteration 11600: Loss = -10851.613586369058
6
Iteration 11700: Loss = -10851.268815673708
7
Iteration 11800: Loss = -10851.35636082042
8
Iteration 11900: Loss = -10851.297583669311
9
Iteration 12000: Loss = -10851.383812392065
10
Stopping early at iteration 12000 due to no improvement.
tensor([[-1.0163, -1.4602],
        [-0.6245, -1.0380],
        [-1.1137, -1.5555],
        [-0.4530, -0.9793],
        [-0.5071, -1.0121],
        [-0.5978, -1.2553],
        [-0.7241, -1.2525],
        [-0.6002, -1.0632],
        [-0.4918, -0.9836],
        [-0.8339, -1.2418],
        [-0.4870, -0.9020],
        [-1.5665, -2.2241],
        [-0.5465, -0.9327],
        [-1.0171, -1.4395],
        [-1.0324, -1.3750],
        [-0.5031, -0.8901],
        [-0.4356, -0.9545],
        [-0.5071, -0.8963],
        [-0.5743, -1.1055],
        [-0.7577, -1.1468],
        [-0.5009, -0.8875],
        [-0.7530, -1.2790],
        [-0.5234, -1.0074],
        [-0.7616, -1.2881],
        [-0.7192, -1.1654],
        [ 0.9937, -2.5647],
        [-1.0252, -1.4877],
        [-0.6554, -1.0133],
        [-0.5193, -0.8670],
        [-0.7539, -1.1752],
        [-0.7868, -1.2033],
        [-0.4836, -0.9232],
        [-0.5845, -0.9445],
        [-0.6133, -1.1958],
        [-0.9511, -1.4774],
        [-0.4915, -0.9749],
        [-0.4954, -0.8911],
        [-0.4349, -0.9958],
        [-0.6757, -1.1874],
        [-0.5912, -1.0953],
        [-0.6099, -1.0487],
        [-0.5095, -0.8773],
        [-0.5646, -0.8227],
        [-0.5990, -1.0459],
        [-0.9961, -1.6026],
        [-1.2064, -1.7123],
        [-0.5739, -0.9653],
        [-0.7093, -1.0503],
        [-0.5538, -0.8868],
        [-0.4515, -1.1863],
        [-1.8957, -2.2578],
        [-0.7706, -1.3986],
        [-0.5539, -0.9135],
        [-0.4634, -0.9254],
        [-0.4877, -0.9346],
        [-0.5762, -1.1254],
        [-0.5183, -1.0767],
        [-0.5877, -1.0711],
        [-0.4295, -0.9730],
        [-0.4633, -1.0646],
        [-1.1866, -1.5082],
        [-0.5238, -1.0091],
        [-0.5328, -0.9466],
        [-0.6117, -1.1908],
        [-0.4746, -0.9142],
        [ 1.2918, -2.9040],
        [-0.4667, -0.9966],
        [-0.5024, -1.0333],
        [-0.4996, -0.9619],
        [-1.3142, -1.7984],
        [-1.5117, -1.9970],
        [-0.5649, -0.9807],
        [-0.7486, -1.2994],
        [-0.7006, -1.2226],
        [-0.6406, -0.9498],
        [-0.8364, -1.3701],
        [-0.4035, -0.9838],
        [-0.5408, -1.0699],
        [-0.5396, -0.9540],
        [-0.5107, -0.9261],
        [-2.1371, -2.4781],
        [-0.7193, -1.2994],
        [-0.6585, -1.0705],
        [-0.8686, -1.3599],
        [-0.5622, -0.8704],
        [-0.5002, -1.0958],
        [-0.5848, -1.0233],
        [-0.4572, -0.9448],
        [-0.9909, -1.4300],
        [-0.5069, -0.9908],
        [-0.5084, -0.9702],
        [-0.7075, -1.1247],
        [-0.4276, -0.9703],
        [-0.6004, -1.1617],
        [-0.4509, -0.9355],
        [-0.1699, -1.4744],
        [-0.6057, -1.1323],
        [-1.3253, -1.8294],
        [-0.6383, -1.2396],
        [-0.8250, -1.4815]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.5812e-01, 4.1882e-02],
        [9.9995e-01, 5.4504e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6210, 0.3790], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1587, 0.1531],
         [0.8129, 0.1509]],

        [[0.2399, 0.2862],
         [0.4896, 0.4619]],

        [[0.1884, 0.2503],
         [0.4548, 0.1728]],

        [[0.4304, 0.0906],
         [0.9107, 0.9328]],

        [[0.9431, 0.1912],
         [0.7682, 0.6576]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0008065306474943187
Average Adjusted Rand Index: -0.00018703005664877213
Iteration 0: Loss = -36113.26442262329
Iteration 10: Loss = -10858.820648195035
Iteration 20: Loss = -10852.550460290136
Iteration 30: Loss = -10851.861138140954
Iteration 40: Loss = -10851.79439233336
Iteration 50: Loss = -10851.79354621177
Iteration 60: Loss = -10851.793288646923
Iteration 70: Loss = -10851.793215991742
Iteration 80: Loss = -10851.793226965465
1
Iteration 90: Loss = -10851.793203684163
Iteration 100: Loss = -10851.79320899353
1
Iteration 110: Loss = -10851.793192959405
Iteration 120: Loss = -10851.793197601155
1
Iteration 130: Loss = -10851.793200759083
2
Iteration 140: Loss = -10851.793200383898
3
Stopping early at iteration 139 due to no improvement.
pi: tensor([[0.9670, 0.0330],
        [0.9906, 0.0094]], dtype=torch.float64)
alpha: tensor([0.9676, 0.0324])
beta: tensor([[[0.1559, 0.1335],
         [0.8521, 0.1244]],

        [[0.6265, 0.2821],
         [0.9361, 0.6231]],

        [[0.7362, 0.2555],
         [0.2406, 0.3311]],

        [[0.9864, 0.0881],
         [0.9667, 0.8075]],

        [[0.6668, 0.1920],
         [0.5064, 0.7177]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.002764517368444376
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0005519328215568777
Average Adjusted Rand Index: 4.489343139188126e-06
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36112.1986719973
Iteration 100: Loss = -10985.105781408163
Iteration 200: Loss = -10948.611010831579
Iteration 300: Loss = -10922.291110364262
Iteration 400: Loss = -10909.354959791059
Iteration 500: Loss = -10894.84742670882
Iteration 600: Loss = -10886.885305888842
Iteration 700: Loss = -10880.872124466097
Iteration 800: Loss = -10869.350332827398
Iteration 900: Loss = -10865.929243426086
Iteration 1000: Loss = -10865.312740366775
Iteration 1100: Loss = -10861.35189266834
Iteration 1200: Loss = -10860.257742844964
Iteration 1300: Loss = -10859.964623193393
Iteration 1400: Loss = -10859.75883642098
Iteration 1500: Loss = -10859.602973687179
Iteration 1600: Loss = -10859.479395356808
Iteration 1700: Loss = -10859.378670249851
Iteration 1800: Loss = -10859.29569346469
Iteration 1900: Loss = -10859.225599487334
Iteration 2000: Loss = -10859.165467449822
Iteration 2100: Loss = -10859.113281258167
Iteration 2200: Loss = -10859.067592718102
Iteration 2300: Loss = -10859.02726864069
Iteration 2400: Loss = -10858.991452858365
Iteration 2500: Loss = -10858.959414885916
Iteration 2600: Loss = -10858.930694308985
Iteration 2700: Loss = -10858.904832362643
Iteration 2800: Loss = -10858.881448492673
Iteration 2900: Loss = -10858.860380376942
Iteration 3000: Loss = -10858.84126609827
Iteration 3100: Loss = -10858.823889780193
Iteration 3200: Loss = -10858.807867605232
Iteration 3300: Loss = -10858.79320886341
Iteration 3400: Loss = -10858.779696290158
Iteration 3500: Loss = -10858.767178079706
Iteration 3600: Loss = -10858.755548464738
Iteration 3700: Loss = -10858.744666576113
Iteration 3800: Loss = -10858.734584598185
Iteration 3900: Loss = -10858.725099470428
Iteration 4000: Loss = -10858.716184330946
Iteration 4100: Loss = -10858.70777707974
Iteration 4200: Loss = -10858.699719946177
Iteration 4300: Loss = -10858.692045829917
Iteration 4400: Loss = -10858.684655431545
Iteration 4500: Loss = -10858.67732994821
Iteration 4600: Loss = -10858.670041989553
Iteration 4700: Loss = -10858.66244510741
Iteration 4800: Loss = -10858.65424065164
Iteration 4900: Loss = -10858.64456154525
Iteration 5000: Loss = -10858.631455313538
Iteration 5100: Loss = -10858.609646633497
Iteration 5200: Loss = -10858.563584607271
Iteration 5300: Loss = -10858.491224055997
Iteration 5400: Loss = -10858.44643657417
Iteration 5500: Loss = -10858.410244486147
Iteration 5600: Loss = -10856.200257656783
Iteration 5700: Loss = -10855.7255133689
Iteration 5800: Loss = -10855.66405678463
Iteration 5900: Loss = -10855.627258558694
Iteration 6000: Loss = -10855.601717459978
Iteration 6100: Loss = -10855.585283383882
Iteration 6200: Loss = -10855.576840180902
Iteration 6300: Loss = -10855.565288289425
Iteration 6400: Loss = -10855.567143349563
1
Iteration 6500: Loss = -10855.555596367736
Iteration 6600: Loss = -10855.549926943695
Iteration 6700: Loss = -10855.546673809531
Iteration 6800: Loss = -10855.545037226495
Iteration 6900: Loss = -10855.54163493825
Iteration 7000: Loss = -10855.54087192171
Iteration 7100: Loss = -10855.580435221773
1
Iteration 7200: Loss = -10855.536250184528
Iteration 7300: Loss = -10855.53497027034
Iteration 7400: Loss = -10855.537758194192
1
Iteration 7500: Loss = -10855.533244915125
Iteration 7600: Loss = -10855.558472420542
1
Iteration 7700: Loss = -10855.531824013719
Iteration 7800: Loss = -10855.529409206518
Iteration 7900: Loss = -10855.5289288557
Iteration 8000: Loss = -10855.527799582971
Iteration 8100: Loss = -10855.530976170185
1
Iteration 8200: Loss = -10855.526370546671
Iteration 8300: Loss = -10855.52928140404
1
Iteration 8400: Loss = -10855.525149829275
Iteration 8500: Loss = -10855.524543682386
Iteration 8600: Loss = -10855.524040084307
Iteration 8700: Loss = -10855.523468315714
Iteration 8800: Loss = -10855.588818673274
1
Iteration 8900: Loss = -10855.522647890946
Iteration 9000: Loss = -10855.5222334701
Iteration 9100: Loss = -10855.521870223729
Iteration 9200: Loss = -10855.521514021522
Iteration 9300: Loss = -10855.521389382808
Iteration 9400: Loss = -10855.520905572474
Iteration 9500: Loss = -10855.520523367983
Iteration 9600: Loss = -10855.520235545553
Iteration 9700: Loss = -10855.521409319086
1
Iteration 9800: Loss = -10855.51970390048
Iteration 9900: Loss = -10855.519414556482
Iteration 10000: Loss = -10855.524178996684
1
Iteration 10100: Loss = -10855.519050566534
Iteration 10200: Loss = -10855.518802254292
Iteration 10300: Loss = -10855.51857042921
Iteration 10400: Loss = -10855.518408901402
Iteration 10500: Loss = -10855.51821667637
Iteration 10600: Loss = -10855.518028954439
Iteration 10700: Loss = -10855.518134369418
1
Iteration 10800: Loss = -10855.517654789906
Iteration 10900: Loss = -10855.517517254466
Iteration 11000: Loss = -10855.594996668551
1
Iteration 11100: Loss = -10855.517280290245
Iteration 11200: Loss = -10855.517176975683
Iteration 11300: Loss = -10855.517163558901
Iteration 11400: Loss = -10855.51713752945
Iteration 11500: Loss = -10855.526227549843
1
Iteration 11600: Loss = -10855.516748151924
Iteration 11700: Loss = -10855.51690308962
1
Iteration 11800: Loss = -10855.516574058689
Iteration 11900: Loss = -10855.51660253985
1
Iteration 12000: Loss = -10855.516431707129
Iteration 12100: Loss = -10855.523695285247
1
Iteration 12200: Loss = -10855.516256749252
Iteration 12300: Loss = -10855.51626240385
1
Iteration 12400: Loss = -10855.516216112048
Iteration 12500: Loss = -10855.516076286336
Iteration 12600: Loss = -10855.522343739989
1
Iteration 12700: Loss = -10855.515978109655
Iteration 12800: Loss = -10855.515916409022
Iteration 12900: Loss = -10855.579239076054
1
Iteration 13000: Loss = -10855.515791401287
Iteration 13100: Loss = -10855.515758863083
Iteration 13200: Loss = -10855.530161256376
1
Iteration 13300: Loss = -10855.644169829704
2
Iteration 13400: Loss = -10855.51564811916
Iteration 13500: Loss = -10855.516844105483
1
Iteration 13600: Loss = -10855.516724818255
2
Iteration 13700: Loss = -10855.515699466678
3
Iteration 13800: Loss = -10855.515523889395
Iteration 13900: Loss = -10855.600241457698
1
Iteration 14000: Loss = -10855.51565901649
2
Iteration 14100: Loss = -10855.515434691091
Iteration 14200: Loss = -10855.626443065414
1
Iteration 14300: Loss = -10855.515399072669
Iteration 14400: Loss = -10855.70422588462
1
Iteration 14500: Loss = -10855.515350957043
Iteration 14600: Loss = -10855.824007994928
1
Iteration 14700: Loss = -10855.515318669173
Iteration 14800: Loss = -10855.51527658127
Iteration 14900: Loss = -10855.5154516323
1
Iteration 15000: Loss = -10855.515185685706
Iteration 15100: Loss = -10855.543599040657
1
Iteration 15200: Loss = -10855.515023095533
Iteration 15300: Loss = -10853.944840165317
Iteration 15400: Loss = -10853.939336652991
Iteration 15500: Loss = -10853.848800197004
Iteration 15600: Loss = -10852.706024486097
Iteration 15700: Loss = -10852.772392259207
1
Iteration 15800: Loss = -10852.705625962271
Iteration 15900: Loss = -10852.705591875061
Iteration 16000: Loss = -10852.565977171449
Iteration 16100: Loss = -10852.504664568592
Iteration 16200: Loss = -10852.453399637428
Iteration 16300: Loss = -10852.315610491172
Iteration 16400: Loss = -10852.453306613543
1
Iteration 16500: Loss = -10852.173906354445
Iteration 16600: Loss = -10852.171184764447
Iteration 16700: Loss = -10852.11439579163
Iteration 16800: Loss = -10851.97435505165
Iteration 16900: Loss = -10851.747851261289
Iteration 17000: Loss = -10851.736804585233
Iteration 17100: Loss = -10851.709439417431
Iteration 17200: Loss = -10851.639239037524
Iteration 17300: Loss = -10851.609032627164
Iteration 17400: Loss = -10851.57621954053
Iteration 17500: Loss = -10851.566394682834
Iteration 17600: Loss = -10851.558694847614
Iteration 17700: Loss = -10851.533414287504
Iteration 17800: Loss = -10851.741585297857
1
Iteration 17900: Loss = -10851.515523670787
Iteration 18000: Loss = -10851.511520584052
Iteration 18100: Loss = -10851.495043523539
Iteration 18200: Loss = -10851.48721126871
Iteration 18300: Loss = -10851.759859073016
1
Iteration 18400: Loss = -10851.476267739987
Iteration 18500: Loss = -10851.655857920037
1
Iteration 18600: Loss = -10851.472471242112
Iteration 18700: Loss = -10851.501387510534
1
Iteration 18800: Loss = -10851.469810935114
Iteration 18900: Loss = -10851.47245577728
1
Iteration 19000: Loss = -10851.477408500752
2
Iteration 19100: Loss = -10851.477416391968
3
Iteration 19200: Loss = -10851.467637851702
Iteration 19300: Loss = -10851.468171327393
1
Iteration 19400: Loss = -10851.467544368328
Iteration 19500: Loss = -10851.467012435825
Iteration 19600: Loss = -10851.516605955761
1
Iteration 19700: Loss = -10851.466428569396
Iteration 19800: Loss = -10851.478903977817
1
Iteration 19900: Loss = -10851.466374631957
tensor([[  5.6463,  -7.1277],
        [  5.4733,  -7.1817],
        [  5.6691,  -7.0854],
        [  5.9234,  -7.3217],
        [  5.8141,  -7.2727],
        [  6.2086,  -7.6163],
        [  5.8635,  -7.3745],
        [  4.8315,  -8.0855],
        [  5.4203,  -7.5821],
        [  5.5771,  -6.9638],
        [  5.2824,  -7.3785],
        [  5.3642,  -8.5432],
        [  5.4278,  -7.0871],
        [  5.4924,  -7.1592],
        [  4.6668,  -7.6159],
        [  4.7634,  -7.7691],
        [  5.8755,  -7.2672],
        [  5.4049,  -7.1207],
        [  5.7902,  -7.4439],
        [  5.0542,  -7.4856],
        [  4.4947,  -8.0318],
        [  5.9093,  -7.3457],
        [  5.6682,  -7.3230],
        [  5.5209,  -7.7253],
        [  5.5794,  -6.9874],
        [  6.9541,  -8.4238],
        [  5.3692,  -7.5004],
        [  5.2770,  -7.1098],
        [  5.3865,  -6.8614],
        [  5.5745,  -7.0191],
        [  4.6667,  -7.9735],
        [  5.4844,  -7.2973],
        [  5.3648,  -7.0207],
        [  5.8761,  -7.5992],
        [  5.9155,  -7.3316],
        [  5.6813,  -7.3376],
        [  5.3896,  -7.0574],
        [  5.9384,  -7.4207],
        [  5.7432,  -7.4059],
        [  8.7457, -10.1481],
        [  5.6617,  -7.1055],
        [  5.4906,  -6.8781],
        [  7.8694,  -9.9477],
        [  5.6584,  -7.1078],
        [  5.2933,  -8.3212],
        [  5.8055,  -7.3597],
        [  4.8646,  -7.6538],
        [  5.4387,  -6.8517],
        [  4.6447,  -7.6515],
        [  6.3431,  -7.8342],
        [  5.2807,  -7.1174],
        [  5.9501,  -7.6349],
        [  8.6321, -10.3499],
        [  5.7458,  -7.1405],
        [  5.6863,  -7.0922],
        [  5.8474,  -7.5408],
        [  5.2917,  -8.0143],
        [  5.8021,  -7.1985],
        [  5.8747,  -7.2975],
        [  6.0561,  -7.4555],
        [  3.7818,  -8.3970],
        [  4.9653,  -8.0759],
        [  4.6369,  -8.0300],
        [  6.0472,  -7.4436],
        [  5.6910,  -7.0812],
        [  7.1454, -10.5980],
        [  5.8783,  -7.2751],
        [  5.7044,  -7.5218],
        [  5.7274,  -7.1602],
        [  5.8107,  -7.1976],
        [  5.5633,  -7.4779],
        [  5.3873,  -7.2703],
        [  6.0009,  -7.3873],
        [  5.8783,  -7.2646],
        [  5.3721,  -6.7739],
        [  7.8208, -10.7761],
        [  6.0524,  -7.4421],
        [  5.9364,  -7.3259],
        [  5.4100,  -7.2363],
        [  4.3949,  -8.2779],
        [  5.1735,  -7.0945],
        [  5.9842,  -7.4537],
        [  5.6243,  -7.0129],
        [  5.2256,  -7.8199],
        [  4.8921,  -7.2751],
        [  5.1647,  -7.8153],
        [  5.6713,  -7.0955],
        [  4.2154,  -8.8306],
        [  5.5921,  -7.1805],
        [  5.7621,  -7.2564],
        [  4.2870,  -8.6316],
        [  5.6274,  -7.0147],
        [  5.4804,  -7.8140],
        [  5.7390,  -7.6226],
        [  5.8019,  -7.2121],
        [  6.0903,  -7.5886],
        [  5.9264,  -7.3277],
        [  8.6057, -10.0731],
        [  5.7455,  -7.8914],
        [  5.3235,  -8.5312]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.6460e-01, 3.5396e-02],
        [1.0000e+00, 1.1387e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 2.4412e-06], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1569, 0.1411],
         [0.8521, 0.1932]],

        [[0.6265, 0.2768],
         [0.9361, 0.6231]],

        [[0.7362, 0.2500],
         [0.2406, 0.3311]],

        [[0.9864, 0.0865],
         [0.9667, 0.8075]],

        [[0.6668, 0.1973],
         [0.5064, 0.7177]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.002764517368444376
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0005519328215568777
Average Adjusted Rand Index: 4.489343139188126e-06
10765.478268091018
new:  [0.0008065306474943187, 0.0008065306474943187, 0.0008065306474943187, 0.0005519328215568777] [-0.00018703005664877213, -0.00018703005664877213, -0.00018703005664877213, 4.489343139188126e-06] [10851.269210053202, 10851.421511900844, 10851.383812392065, 10851.466371232047]
prior:  [0.0005519328215568777, 0.0, 0.0005519328215568777, 0.0005519328215568777] [4.489343139188126e-06, 0.0, 4.489343139188126e-06, 4.489343139188126e-06] [10851.793206453776, nan, 10851.793155168381, 10851.793200383898]
-----------------------------------------------------------------------------------------
This iteration is 11
True Objective function: Loss = -10852.630324507372
Iteration 0: Loss = -33435.04376436794
Iteration 10: Loss = -10888.320848228517
Iteration 20: Loss = -10886.509162773647
Iteration 30: Loss = -10886.405002313879
Iteration 40: Loss = -10886.172063923372
Iteration 50: Loss = -10885.66906087168
Iteration 60: Loss = -10885.660318087026
Iteration 70: Loss = -10885.641938610155
Iteration 80: Loss = -10885.512595625147
Iteration 90: Loss = -10885.414747670404
Iteration 100: Loss = -10885.387173232662
Iteration 110: Loss = -10885.377497895715
Iteration 120: Loss = -10885.371938193177
Iteration 130: Loss = -10885.367967922764
Iteration 140: Loss = -10885.36488351877
Iteration 150: Loss = -10885.362493305774
Iteration 160: Loss = -10885.360494595188
Iteration 170: Loss = -10885.358967515805
Iteration 180: Loss = -10885.357727714376
Iteration 190: Loss = -10885.356753233145
Iteration 200: Loss = -10885.356030735393
Iteration 210: Loss = -10885.3554687959
Iteration 220: Loss = -10885.355124763691
Iteration 230: Loss = -10885.354974509015
Iteration 240: Loss = -10885.354972194149
Iteration 250: Loss = -10885.35513624182
1
Iteration 260: Loss = -10885.355476967728
2
Iteration 270: Loss = -10885.35589667037
3
Stopping early at iteration 269 due to no improvement.
pi: tensor([[0.8876, 0.1124],
        [0.8460, 0.1540]], dtype=torch.float64)
alpha: tensor([0.8837, 0.1163])
beta: tensor([[[0.1525, 0.1636],
         [0.8161, 0.1975]],

        [[0.4599, 0.2155],
         [0.2710, 0.2084]],

        [[0.8402, 0.1748],
         [0.7669, 0.4863]],

        [[0.9725, 0.1578],
         [0.9290, 0.6960]],

        [[0.9953, 0.1826],
         [0.2169, 0.9211]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.006738692547152536
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0023374197449582577
Average Adjusted Rand Index: -0.0013477385094305071
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33081.1298367688
Iteration 100: Loss = -10890.754004761258
Iteration 200: Loss = -10889.688089116311
Iteration 300: Loss = -10889.298781742727
Iteration 400: Loss = -10889.03747072767
Iteration 500: Loss = -10888.82106281918
Iteration 600: Loss = -10888.556891231467
Iteration 700: Loss = -10888.083713765083
Iteration 800: Loss = -10887.54435807391
Iteration 900: Loss = -10886.942111554707
Iteration 1000: Loss = -10886.136084332136
Iteration 1100: Loss = -10885.48856861855
Iteration 1200: Loss = -10885.268342003314
Iteration 1300: Loss = -10885.172438964464
Iteration 1400: Loss = -10885.103333603543
Iteration 1500: Loss = -10885.044353069512
Iteration 1600: Loss = -10884.990641998347
Iteration 1700: Loss = -10884.939925255929
Iteration 1800: Loss = -10884.892027092916
Iteration 1900: Loss = -10884.846820394707
Iteration 2000: Loss = -10884.804508741125
Iteration 2100: Loss = -10884.765424835046
Iteration 2200: Loss = -10884.728905716262
Iteration 2300: Loss = -10884.694774387108
Iteration 2400: Loss = -10884.663622231405
Iteration 2500: Loss = -10884.635005830878
Iteration 2600: Loss = -10884.60942711444
Iteration 2700: Loss = -10884.584428056083
Iteration 2800: Loss = -10884.562199217957
Iteration 2900: Loss = -10884.54298116602
Iteration 3000: Loss = -10884.523787451864
Iteration 3100: Loss = -10884.507006542673
Iteration 3200: Loss = -10884.491378486526
Iteration 3300: Loss = -10884.47790182844
Iteration 3400: Loss = -10884.466119193628
Iteration 3500: Loss = -10884.453360281865
Iteration 3600: Loss = -10884.4426674262
Iteration 3700: Loss = -10884.433115610236
Iteration 3800: Loss = -10884.424839380501
Iteration 3900: Loss = -10884.416496997716
Iteration 4000: Loss = -10884.408215082574
Iteration 4100: Loss = -10884.400966987823
Iteration 4200: Loss = -10884.394099727908
Iteration 4300: Loss = -10884.38765073906
Iteration 4400: Loss = -10884.38119616811
Iteration 4500: Loss = -10884.373742020782
Iteration 4600: Loss = -10884.365663978544
Iteration 4700: Loss = -10884.356666074398
Iteration 4800: Loss = -10884.345348998204
Iteration 4900: Loss = -10884.330492182597
Iteration 5000: Loss = -10884.308687977662
Iteration 5100: Loss = -10884.269420770555
Iteration 5200: Loss = -10884.189835844174
Iteration 5300: Loss = -10884.069193874971
Iteration 5400: Loss = -10884.012292687019
Iteration 5500: Loss = -10883.97121389232
Iteration 5600: Loss = -10883.957839424762
Iteration 5700: Loss = -10883.952177219397
Iteration 5800: Loss = -10883.94854196494
Iteration 5900: Loss = -10883.946839282424
Iteration 6000: Loss = -10883.946557167732
Iteration 6100: Loss = -10883.944326933224
Iteration 6200: Loss = -10883.942782870103
Iteration 6300: Loss = -10883.941602710685
Iteration 6400: Loss = -10883.94037539396
Iteration 6500: Loss = -10883.939219760685
Iteration 6600: Loss = -10883.940075456723
1
Iteration 6700: Loss = -10883.940805004946
2
Iteration 6800: Loss = -10883.93588255081
Iteration 6900: Loss = -10883.934992830005
Iteration 7000: Loss = -10883.944616190407
1
Iteration 7100: Loss = -10883.933405490598
Iteration 7200: Loss = -10883.9327493984
Iteration 7300: Loss = -10884.094329271456
1
Iteration 7400: Loss = -10883.931615474143
Iteration 7500: Loss = -10883.931648932841
1
Iteration 7600: Loss = -10883.930820330754
Iteration 7700: Loss = -10883.930498078207
Iteration 7800: Loss = -10883.93017005942
Iteration 7900: Loss = -10883.929893119579
Iteration 8000: Loss = -10883.929662080234
Iteration 8100: Loss = -10883.957753535386
1
Iteration 8200: Loss = -10883.929204772121
Iteration 8300: Loss = -10883.92904152055
Iteration 8400: Loss = -10884.0163372646
1
Iteration 8500: Loss = -10883.928721593284
Iteration 8600: Loss = -10883.928632460047
Iteration 8700: Loss = -10883.928656182394
1
Iteration 8800: Loss = -10883.951624705547
2
Iteration 8900: Loss = -10883.932756113078
3
Iteration 9000: Loss = -10883.928206527275
Iteration 9100: Loss = -10883.934939221312
1
Iteration 9200: Loss = -10883.927968385766
Iteration 9300: Loss = -10883.93975248933
1
Iteration 9400: Loss = -10883.927949823137
Iteration 9500: Loss = -10883.927800043824
Iteration 9600: Loss = -10883.934762670853
1
Iteration 9700: Loss = -10883.927694757538
Iteration 9800: Loss = -10883.929608594848
1
Iteration 9900: Loss = -10883.927727916078
2
Iteration 10000: Loss = -10884.211727010423
3
Iteration 10100: Loss = -10883.927614530385
Iteration 10200: Loss = -10883.927441193353
Iteration 10300: Loss = -10883.927646543021
1
Iteration 10400: Loss = -10883.927359447256
Iteration 10500: Loss = -10883.927420371096
1
Iteration 10600: Loss = -10883.927359273965
Iteration 10700: Loss = -10884.009059509532
1
Iteration 10800: Loss = -10883.927242297177
Iteration 10900: Loss = -10883.927712635345
1
Iteration 11000: Loss = -10883.927199521893
Iteration 11100: Loss = -10883.927151932443
Iteration 11200: Loss = -10883.931815207467
1
Iteration 11300: Loss = -10883.92707812998
Iteration 11400: Loss = -10884.045048685584
1
Iteration 11500: Loss = -10883.927088978538
2
Iteration 11600: Loss = -10883.927135712838
3
Iteration 11700: Loss = -10883.999813314864
4
Iteration 11800: Loss = -10883.926979485599
Iteration 11900: Loss = -10883.984123026386
1
Iteration 12000: Loss = -10883.926974584801
Iteration 12100: Loss = -10883.943313303409
1
Iteration 12200: Loss = -10883.94805273264
2
Iteration 12300: Loss = -10883.928518318466
3
Iteration 12400: Loss = -10883.926903083737
Iteration 12500: Loss = -10883.929019180598
1
Iteration 12600: Loss = -10883.926860838088
Iteration 12700: Loss = -10883.9391075633
1
Iteration 12800: Loss = -10883.926848106945
Iteration 12900: Loss = -10883.972184109294
1
Iteration 13000: Loss = -10883.926855921398
2
Iteration 13100: Loss = -10883.927034994802
3
Iteration 13200: Loss = -10883.969781713986
4
Iteration 13300: Loss = -10883.926822279394
Iteration 13400: Loss = -10883.92682289142
1
Iteration 13500: Loss = -10883.927084390967
2
Iteration 13600: Loss = -10883.926874577113
3
Iteration 13700: Loss = -10883.949933794687
4
Iteration 13800: Loss = -10883.926783609897
Iteration 13900: Loss = -10883.927116298955
1
Iteration 14000: Loss = -10883.926810529321
2
Iteration 14100: Loss = -10883.926776815517
Iteration 14200: Loss = -10883.982497847901
1
Iteration 14300: Loss = -10883.926746541067
Iteration 14400: Loss = -10883.926753297306
1
Iteration 14500: Loss = -10883.927218612627
2
Iteration 14600: Loss = -10883.926799009301
3
Iteration 14700: Loss = -10883.926745329363
Iteration 14800: Loss = -10883.926730220837
Iteration 14900: Loss = -10883.928677864935
1
Iteration 15000: Loss = -10883.926722482533
Iteration 15100: Loss = -10883.928764543549
1
Iteration 15200: Loss = -10883.926714238307
Iteration 15300: Loss = -10883.926751943352
1
Iteration 15400: Loss = -10883.92762707954
2
Iteration 15500: Loss = -10883.96783372902
3
Iteration 15600: Loss = -10883.92674051802
4
Iteration 15700: Loss = -10883.934183094134
5
Iteration 15800: Loss = -10884.003618370192
6
Iteration 15900: Loss = -10884.035563823154
7
Iteration 16000: Loss = -10884.04697159115
8
Iteration 16100: Loss = -10883.92674726424
9
Iteration 16200: Loss = -10883.926777379676
10
Stopping early at iteration 16200 due to no improvement.
tensor([[-4.6539e+00,  3.8691e-02],
        [-4.6582e+00,  4.2976e-02],
        [-4.5932e+00, -2.2056e-02],
        [-4.6499e+00,  3.4700e-02],
        [-4.6740e+00,  5.8741e-02],
        [-4.6695e+00,  5.4279e-02],
        [-4.6315e+00,  1.6289e-02],
        [-4.6801e+00,  6.4916e-02],
        [-4.6835e+00,  6.8325e-02],
        [-4.6130e+00, -2.2257e-03],
        [-4.6456e+00,  3.0364e-02],
        [-4.6906e+00,  7.5398e-02],
        [-4.6754e+00,  6.0194e-02],
        [-4.6620e+00,  4.6812e-02],
        [-4.6496e+00,  3.4332e-02],
        [-4.6701e+00,  5.4888e-02],
        [-4.6442e+00,  2.8961e-02],
        [-4.6803e+00,  6.5098e-02],
        [-4.6854e+00,  7.0219e-02],
        [-4.6725e+00,  5.7262e-02],
        [-4.6783e+00,  6.3052e-02],
        [-4.6547e+00,  3.9440e-02],
        [-4.6548e+00,  3.9604e-02],
        [-4.6703e+00,  5.5034e-02],
        [-4.6397e+00,  2.4480e-02],
        [-4.6596e+00,  4.4347e-02],
        [-4.6340e+00,  1.8818e-02],
        [-4.6170e+00,  1.7591e-03],
        [-4.6518e+00,  3.6577e-02],
        [-4.6637e+00,  4.8434e-02],
        [-4.6907e+00,  7.5482e-02],
        [-4.6537e+00,  3.8481e-02],
        [-4.6648e+00,  4.9590e-02],
        [-4.6032e+00, -1.1975e-02],
        [-4.6531e+00,  3.7860e-02],
        [-4.6906e+00,  7.5331e-02],
        [-4.6548e+00,  3.9552e-02],
        [-4.6450e+00,  2.9822e-02],
        [-4.6166e+00,  1.4117e-03],
        [-4.6665e+00,  5.1233e-02],
        [-4.6374e+00,  2.2132e-02],
        [-4.6560e+00,  4.0828e-02],
        [-4.6804e+00,  6.5208e-02],
        [-4.6701e+00,  5.4889e-02],
        [-4.6310e+00,  1.5821e-02],
        [-4.6652e+00,  4.9966e-02],
        [-4.6541e+00,  3.8838e-02],
        [-4.6390e+00,  2.3748e-02],
        [-4.7109e+00,  9.5701e-02],
        [-4.6595e+00,  4.4255e-02],
        [-4.6601e+00,  4.4856e-02],
        [-4.6687e+00,  5.3430e-02],
        [-4.6805e+00,  6.5233e-02],
        [-4.6701e+00,  5.4910e-02],
        [-4.6332e+00,  1.7964e-02],
        [-4.6273e+00,  1.2043e-02],
        [-4.6467e+00,  3.1465e-02],
        [-4.6601e+00,  4.4832e-02],
        [-4.6303e+00,  1.5042e-02],
        [-4.6744e+00,  5.9148e-02],
        [-4.6431e+00,  2.7874e-02],
        [-4.6750e+00,  5.9768e-02],
        [-4.6397e+00,  2.4509e-02],
        [-4.6498e+00,  3.4586e-02],
        [-4.6497e+00,  3.4523e-02],
        [-4.6855e+00,  7.0282e-02],
        [-4.6498e+00,  3.4618e-02],
        [-4.6845e+00,  6.9295e-02],
        [-4.6688e+00,  5.3575e-02],
        [-4.6446e+00,  2.9355e-02],
        [-4.6496e+00,  3.4344e-02],
        [-4.6347e+00,  1.9450e-02],
        [-4.6498e+00,  3.4628e-02],
        [-4.6498e+00,  3.4533e-02],
        [-4.6700e+00,  5.4778e-02],
        [-4.6753e+00,  6.0120e-02],
        [-4.6703e+00,  5.5117e-02],
        [-4.6550e+00,  3.9796e-02],
        [-4.6476e+00,  3.2341e-02],
        [-4.6516e+00,  3.6414e-02],
        [-4.6907e+00,  7.5442e-02],
        [-4.6800e+00,  6.4824e-02],
        [-4.6947e+00,  7.9489e-02],
        [-4.6855e+00,  7.0263e-02],
        [-4.6601e+00,  4.4885e-02],
        [-4.6804e+00,  6.5194e-02],
        [-4.6450e+00,  2.9816e-02],
        [-4.6649e+00,  4.9697e-02],
        [-4.6684e+00,  5.3219e-02],
        [-4.6804e+00,  6.5226e-02],
        [-4.6538e+00,  3.8573e-02],
        [-4.6517e+00,  3.6459e-02],
        [-4.6414e+00,  2.6131e-02],
        [-4.6696e+00,  5.4364e-02],
        [-4.6482e+00,  3.3024e-02],
        [-4.6805e+00,  6.5243e-02],
        [-4.6855e+00,  7.0314e-02],
        [-4.6901e+00,  7.4871e-02],
        [-4.6663e+00,  5.1107e-02],
        [-4.6854e+00,  7.0181e-02]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9478, 0.0522],
        [0.9552, 0.0448]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0090, 0.9910], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1623, 0.1562],
         [0.8161, 0.1550]],

        [[0.4599, 0.2442],
         [0.2710, 0.2084]],

        [[0.8402, 0.1650],
         [0.7669, 0.4863]],

        [[0.9725, 0.1302],
         [0.9290, 0.6960]],

        [[0.9953, 0.0818],
         [0.2169, 0.9211]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.006467401572035518
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0070073061126442035
Global Adjusted Rand Index: -0.001959166440327788
Average Adjusted Rand Index: -0.0026949415369359442
Iteration 0: Loss = -45531.32011485595
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.8037,    nan]],

        [[0.3053,    nan],
         [0.7553, 0.1817]],

        [[0.4795,    nan],
         [0.0109, 0.5540]],

        [[0.8081,    nan],
         [0.8191, 0.6073]],

        [[0.3325,    nan],
         [0.5688, 0.2569]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -45527.35401455806
Iteration 100: Loss = -10926.853698164028
Iteration 200: Loss = -10905.566357711812
Iteration 300: Loss = -10898.03194263361
Iteration 400: Loss = -10893.324767472288
Iteration 500: Loss = -10891.683771850061
Iteration 600: Loss = -10890.734115837773
Iteration 700: Loss = -10890.134274099331
Iteration 800: Loss = -10889.73979788737
Iteration 900: Loss = -10889.470857314414
Iteration 1000: Loss = -10889.277572954357
Iteration 1100: Loss = -10889.132430439335
Iteration 1200: Loss = -10889.019795674367
Iteration 1300: Loss = -10888.931287008667
Iteration 1400: Loss = -10888.86072153308
Iteration 1500: Loss = -10888.803057647288
Iteration 1600: Loss = -10888.754139434626
Iteration 1700: Loss = -10888.711137267575
Iteration 1800: Loss = -10888.671925156446
Iteration 1900: Loss = -10888.635261325613
Iteration 2000: Loss = -10888.600319012063
Iteration 2100: Loss = -10888.566930098308
Iteration 2200: Loss = -10888.535579582427
Iteration 2300: Loss = -10888.50695169165
Iteration 2400: Loss = -10888.481271943381
Iteration 2500: Loss = -10888.458348486502
Iteration 2600: Loss = -10888.437801164459
Iteration 2700: Loss = -10888.419331527528
Iteration 2800: Loss = -10888.402491684985
Iteration 2900: Loss = -10888.38718565036
Iteration 3000: Loss = -10888.373123816846
Iteration 3100: Loss = -10888.36002587414
Iteration 3200: Loss = -10888.347863916664
Iteration 3300: Loss = -10888.336257335135
Iteration 3400: Loss = -10888.325120923098
Iteration 3500: Loss = -10888.314448621008
Iteration 3600: Loss = -10888.30438573137
Iteration 3700: Loss = -10888.294691418947
Iteration 3800: Loss = -10888.28515200641
Iteration 3900: Loss = -10888.275650759722
Iteration 4000: Loss = -10888.266074696096
Iteration 4100: Loss = -10888.256299335082
Iteration 4200: Loss = -10888.246171489827
Iteration 4300: Loss = -10888.235522608496
Iteration 4400: Loss = -10888.22417158567
Iteration 4500: Loss = -10888.21189448978
Iteration 4600: Loss = -10888.198458728171
Iteration 4700: Loss = -10888.183496322892
Iteration 4800: Loss = -10888.166460998998
Iteration 4900: Loss = -10888.146899133577
Iteration 5000: Loss = -10888.262621625076
1
Iteration 5100: Loss = -10888.095965976694
Iteration 5200: Loss = -10888.061519898223
Iteration 5300: Loss = -10888.01729985439
Iteration 5400: Loss = -10887.958297350859
Iteration 5500: Loss = -10887.874884221163
Iteration 5600: Loss = -10887.74988459061
Iteration 5700: Loss = -10887.550638596882
Iteration 5800: Loss = -10887.221323824318
Iteration 5900: Loss = -10886.76155646046
Iteration 6000: Loss = -10886.332431315066
Iteration 6100: Loss = -10886.047626536465
Iteration 6200: Loss = -10885.809394186255
Iteration 6300: Loss = -10885.548288922764
Iteration 6400: Loss = -10885.336715031288
Iteration 6500: Loss = -10885.215591310374
Iteration 6600: Loss = -10885.219381904151
1
Iteration 6700: Loss = -10885.242909089056
2
Iteration 6800: Loss = -10885.090131538807
Iteration 6900: Loss = -10885.055543497157
Iteration 7000: Loss = -10885.057484031151
1
Iteration 7100: Loss = -10884.996837685863
Iteration 7200: Loss = -10884.98440874306
Iteration 7300: Loss = -10884.969729043101
Iteration 7400: Loss = -10884.967266257223
Iteration 7500: Loss = -10884.954633327901
Iteration 7600: Loss = -10884.94941158447
Iteration 7700: Loss = -10884.943917272507
Iteration 7800: Loss = -10884.9394248903
Iteration 7900: Loss = -10884.936978287962
Iteration 8000: Loss = -10884.936279941538
Iteration 8100: Loss = -10884.934349737545
Iteration 8200: Loss = -10884.932839276451
Iteration 8300: Loss = -10885.405439738117
1
Iteration 8400: Loss = -10884.92896999939
Iteration 8500: Loss = -10884.926455939696
Iteration 8600: Loss = -10884.92331558061
Iteration 8700: Loss = -10884.91938040046
Iteration 8800: Loss = -10884.914318125744
Iteration 8900: Loss = -10884.90778436345
Iteration 9000: Loss = -10884.901352515924
Iteration 9100: Loss = -10884.888808255348
Iteration 9200: Loss = -10884.876111020914
Iteration 9300: Loss = -10884.862399741422
Iteration 9400: Loss = -10884.84542655225
Iteration 9500: Loss = -10884.826862781525
Iteration 9600: Loss = -10884.793367999928
Iteration 9700: Loss = -10884.75651921727
Iteration 9800: Loss = -10884.73786685104
Iteration 9900: Loss = -10884.869829531417
1
Iteration 10000: Loss = -10884.725410037274
Iteration 10100: Loss = -10884.72383517716
Iteration 10200: Loss = -10884.721618193793
Iteration 10300: Loss = -10884.73303995694
1
Iteration 10400: Loss = -10884.720171030307
Iteration 10500: Loss = -10884.719708137074
Iteration 10600: Loss = -10884.7198179717
1
Iteration 10700: Loss = -10884.718987615888
Iteration 10800: Loss = -10884.718703277369
Iteration 10900: Loss = -10884.71990521452
1
Iteration 11000: Loss = -10884.718114482264
Iteration 11100: Loss = -10884.717832714037
Iteration 11200: Loss = -10884.7175799124
Iteration 11300: Loss = -10884.71732846215
Iteration 11400: Loss = -10884.720859461662
1
Iteration 11500: Loss = -10884.717106154483
Iteration 11600: Loss = -10885.00456211554
1
Iteration 11700: Loss = -10884.716850755902
Iteration 11800: Loss = -10884.717201648646
1
Iteration 11900: Loss = -10884.716442207588
Iteration 12000: Loss = -10884.716207394596
Iteration 12100: Loss = -10884.72092126443
1
Iteration 12200: Loss = -10884.7158947636
Iteration 12300: Loss = -10884.774400789745
1
Iteration 12400: Loss = -10884.740543479085
2
Iteration 12500: Loss = -10884.715449885445
Iteration 12600: Loss = -10884.714743954748
Iteration 12700: Loss = -10884.81750669931
1
Iteration 12800: Loss = -10884.71295799823
Iteration 12900: Loss = -10884.712669367429
Iteration 13000: Loss = -10884.706884470992
Iteration 13100: Loss = -10884.686067871207
Iteration 13200: Loss = -10884.49349192093
Iteration 13300: Loss = -10884.31538590725
Iteration 13400: Loss = -10884.013351369964
Iteration 13500: Loss = -10884.013341103173
Iteration 13600: Loss = -10884.172785447065
1
Iteration 13700: Loss = -10884.013368795797
2
Iteration 13800: Loss = -10884.03081641559
3
Iteration 13900: Loss = -10884.01337057636
4
Iteration 14000: Loss = -10884.01344186701
5
Iteration 14100: Loss = -10884.013362656944
6
Iteration 14200: Loss = -10884.035374078921
7
Iteration 14300: Loss = -10884.013370266846
8
Iteration 14400: Loss = -10884.030725074248
9
Iteration 14500: Loss = -10884.013346024216
10
Stopping early at iteration 14500 due to no improvement.
tensor([[-0.9224, -0.8981],
        [-0.5665, -0.8564],
        [-1.8955,  0.2426],
        [-0.4803, -1.5525],
        [-0.2416, -1.1899],
        [-0.2577, -1.3123],
        [-0.9443, -0.4494],
        [-0.5953, -1.8594],
        [ 0.0182, -1.5207],
        [-1.5699, -0.7753],
        [-0.9072, -0.6729],
        [ 0.1230, -2.0594],
        [-1.0094, -3.0211],
        [-1.0285, -1.2899],
        [-0.8332, -0.9130],
        [-0.4793, -1.6787],
        [-2.4692, -1.9959],
        [ 0.1035, -1.4911],
        [ 0.0406, -1.4590],
        [-0.2763, -1.1921],
        [-0.4760, -1.4408],
        [-1.4094, -1.6243],
        [-0.3301, -1.0922],
        [-0.0965, -1.3965],
        [-0.4644, -1.0595],
        [-1.7445, -2.2268],
        [-1.0324, -0.5774],
        [-2.1985, -0.5644],
        [-0.7341, -0.8219],
        [-0.6392, -1.4107],
        [-0.1716, -1.6266],
        [-0.6104, -0.8140],
        [-0.4574, -1.0710],
        [-1.3186, -1.2135],
        [-0.7158, -0.7923],
        [-0.8173, -3.2743],
        [-0.8017, -0.9634],
        [-1.5102, -1.2928],
        [-0.8568, -0.6471],
        [-0.6250, -0.9948],
        [-1.2672, -0.6684],
        [-0.7875, -0.7808],
        [-1.4295, -3.1857],
        [-0.0996, -1.5356],
        [-0.6705, -1.0348],
        [-0.1842, -1.7111],
        [-0.6888, -0.9982],
        [-0.9977, -0.9905],
        [ 0.7974, -2.3177],
        [-1.2467, -1.9790],
        [-0.2600, -1.6602],
        [-0.2653, -1.2173],
        [ 0.1377, -1.5264],
        [-0.4156, -1.2090],
        [-1.1629, -0.3193],
        [-0.9887, -0.4407],
        [-0.7428, -0.8028],
        [-0.1191, -1.2942],
        [-1.2075, -0.1873],
        [-1.0207, -2.3989],
        [-1.2068, -0.5660],
        [-0.1234, -1.6171],
        [-1.0246, -1.0330],
        [-0.6018, -1.1601],
        [-0.4684, -0.9300],
        [ 0.1941, -1.6951],
        [-0.3892, -1.0649],
        [ 0.0145, -1.4621],
        [-0.2753, -1.1477],
        [-0.7396, -0.6779],
        [-1.9382, -2.6770],
        [-0.8196, -1.0032],
        [-0.2549, -1.1627],
        [-0.6496, -1.1536],
        [-0.0379, -1.3511],
        [ 0.2085, -1.6252],
        [-0.1989, -1.7594],
        [-1.0458, -2.2207],
        [-1.3786, -1.0266],
        [-0.9594, -0.8842],
        [ 0.4035, -1.9193],
        [-0.3202, -2.1895],
        [-0.2034, -1.5935],
        [-0.0548, -1.8676],
        [-0.6729, -1.9808],
        [ 0.1748, -1.6537],
        [-1.0028, -0.9924],
        [-1.3542, -2.5797],
        [-0.7393, -1.0799],
        [-0.2332, -1.7022],
        [-0.7133, -0.8637],
        [-0.9516, -1.3230],
        [-1.0049, -0.4914],
        [-0.4778, -1.3439],
        [-0.7339, -0.9192],
        [-0.8992, -2.8944],
        [ 0.2911, -1.6826],
        [ 0.1604, -1.5895],
        [-0.4599, -1.2062],
        [ 0.1285, -1.5311]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7963, 0.2037],
        [0.2695, 0.7305]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6468, 0.3532], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1392, 0.1610],
         [0.8037, 0.1958]],

        [[0.3053, 0.1731],
         [0.7553, 0.1817]],

        [[0.4795, 0.1687],
         [0.0109, 0.5540]],

        [[0.8081, 0.1531],
         [0.8191, 0.6073]],

        [[0.3325, 0.1639],
         [0.5688, 0.2569]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 63
Adjusted Rand Index: 0.059154975056103586
time is 1
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 71
Adjusted Rand Index: 0.16825678020003848
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 62
Adjusted Rand Index: 0.04768577403151745
time is 3
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 64
Adjusted Rand Index: 0.06982753052128415
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.022913377152719128
Global Adjusted Rand Index: 0.07429099710834049
Average Adjusted Rand Index: 0.07356768739233256
Iteration 0: Loss = -30741.23191884317
Iteration 10: Loss = -10888.697304266392
Iteration 20: Loss = -10888.69730426642
1
Iteration 30: Loss = -10888.697304267582
2
Iteration 40: Loss = -10888.697304319736
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.7302e-18, 1.0000e+00],
        [2.4035e-12, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([2.3453e-12, 1.0000e+00])
beta: tensor([[[0.1463, 0.0742],
         [0.8792, 0.1586]],

        [[0.8279, 0.2585],
         [0.8390, 0.4035]],

        [[0.4941, 0.1736],
         [0.8406, 0.1353]],

        [[0.1769, 0.0769],
         [0.7873, 0.3565]],

        [[0.2333, 0.2050],
         [0.4749, 0.9592]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30740.352629070385
Iteration 100: Loss = -10900.685208515762
Iteration 200: Loss = -10891.310539751148
Iteration 300: Loss = -10889.328613931448
Iteration 400: Loss = -10888.749204666847
Iteration 500: Loss = -10888.434863369199
Iteration 600: Loss = -10888.242049950177
Iteration 700: Loss = -10888.109737367133
Iteration 800: Loss = -10888.01206519878
Iteration 900: Loss = -10887.92898564512
Iteration 1000: Loss = -10887.851451313296
Iteration 1100: Loss = -10887.775212687482
Iteration 1200: Loss = -10887.697824282886
Iteration 1300: Loss = -10887.616697139803
Iteration 1400: Loss = -10887.529344687311
Iteration 1500: Loss = -10887.433097365552
Iteration 1600: Loss = -10887.324608848205
Iteration 1700: Loss = -10887.199886828908
Iteration 1800: Loss = -10887.054273630103
Iteration 1900: Loss = -10886.882806401067
Iteration 2000: Loss = -10886.680719214484
Iteration 2100: Loss = -10886.445227903998
Iteration 2200: Loss = -10886.178732628217
Iteration 2300: Loss = -10885.893872670416
Iteration 2400: Loss = -10885.620475848798
Iteration 2500: Loss = -10885.406516571393
Iteration 2600: Loss = -10885.275544443997
Iteration 2700: Loss = -10885.151937688468
Iteration 2800: Loss = -10885.108148954112
Iteration 2900: Loss = -10885.078809126939
Iteration 3000: Loss = -10885.057116383909
Iteration 3100: Loss = -10885.039638936563
Iteration 3200: Loss = -10885.02473470729
Iteration 3300: Loss = -10885.012306361534
Iteration 3400: Loss = -10885.002583184221
Iteration 3500: Loss = -10884.99411934044
Iteration 3600: Loss = -10884.985517205592
Iteration 3700: Loss = -10884.974231889639
Iteration 3800: Loss = -10884.958117389895
Iteration 3900: Loss = -10884.951674507376
Iteration 4000: Loss = -10884.946645834478
Iteration 4100: Loss = -10884.9425909413
Iteration 4200: Loss = -10884.939420480558
Iteration 4300: Loss = -10884.936868708097
Iteration 4400: Loss = -10884.93468399704
Iteration 4500: Loss = -10884.932478239762
Iteration 4600: Loss = -10884.930534709703
Iteration 4700: Loss = -10884.928855961007
Iteration 4800: Loss = -10884.927145316628
Iteration 4900: Loss = -10884.925190777725
Iteration 5000: Loss = -10884.923061203308
Iteration 5100: Loss = -10884.920571719145
Iteration 5200: Loss = -10884.917614724824
Iteration 5300: Loss = -10884.91432568895
Iteration 5400: Loss = -10884.910366555585
Iteration 5500: Loss = -10884.905768209968
Iteration 5600: Loss = -10884.900328415204
Iteration 5700: Loss = -10884.89399459294
Iteration 5800: Loss = -10884.88660568865
Iteration 5900: Loss = -10884.878072856181
Iteration 6000: Loss = -10884.869052924563
Iteration 6100: Loss = -10884.85910753519
Iteration 6200: Loss = -10884.847770806778
Iteration 6300: Loss = -10884.833706928619
Iteration 6400: Loss = -10884.81426141712
Iteration 6500: Loss = -10884.787520875843
Iteration 6600: Loss = -10884.76065996265
Iteration 6700: Loss = -10884.74341019037
Iteration 6800: Loss = -10884.734660837736
Iteration 6900: Loss = -10884.730877946451
Iteration 7000: Loss = -10884.727225244482
Iteration 7100: Loss = -10884.725192950544
Iteration 7200: Loss = -10884.72368094505
Iteration 7300: Loss = -10884.722499634025
Iteration 7400: Loss = -10884.721488460891
Iteration 7500: Loss = -10884.720732383605
Iteration 7600: Loss = -10884.720047277948
Iteration 7700: Loss = -10884.719419990517
Iteration 7800: Loss = -10884.718814803386
Iteration 7900: Loss = -10884.719047776203
1
Iteration 8000: Loss = -10884.716893454623
Iteration 8100: Loss = -10884.723429268934
1
Iteration 8200: Loss = -10884.715835687544
Iteration 8300: Loss = -10884.715181034997
Iteration 8400: Loss = -10884.714843494972
Iteration 8500: Loss = -10884.714500289909
Iteration 8600: Loss = -10884.71428316742
Iteration 8700: Loss = -10884.714268931606
Iteration 8800: Loss = -10884.721207150054
1
Iteration 8900: Loss = -10884.71804515389
2
Iteration 9000: Loss = -10884.752844858267
3
Iteration 9100: Loss = -10884.711587785952
Iteration 9200: Loss = -10884.711467968182
Iteration 9300: Loss = -10884.707596775683
Iteration 9400: Loss = -10884.701697820548
Iteration 9500: Loss = -10884.676322826408
Iteration 9600: Loss = -10884.587295669202
Iteration 9700: Loss = -10884.165047106688
Iteration 9800: Loss = -10884.0167467896
Iteration 9900: Loss = -10884.013351084463
Iteration 10000: Loss = -10884.018917637008
1
Iteration 10100: Loss = -10884.013372829824
2
Iteration 10200: Loss = -10884.013394806476
3
Iteration 10300: Loss = -10884.077055603519
4
Iteration 10400: Loss = -10884.013354784853
5
Iteration 10500: Loss = -10884.01446232004
6
Iteration 10600: Loss = -10884.013340614465
Iteration 10700: Loss = -10884.0145012809
1
Iteration 10800: Loss = -10884.013368020307
2
Iteration 10900: Loss = -10884.013793381302
3
Iteration 11000: Loss = -10884.241616737318
4
Iteration 11100: Loss = -10884.013364258797
5
Iteration 11200: Loss = -10884.01365559187
6
Iteration 11300: Loss = -10884.021745290436
7
Iteration 11400: Loss = -10884.01338681085
8
Iteration 11500: Loss = -10884.013463724488
9
Iteration 11600: Loss = -10884.715642555535
10
Stopping early at iteration 11600 due to no improvement.
tensor([[-0.9529, -0.9911],
        [-0.9627, -0.6895],
        [-0.1037, -2.2178],
        [-1.2607, -0.1967],
        [-1.8349, -0.8995],
        [-1.2225, -0.1722],
        [-1.0816, -1.5848],
        [-1.3324, -0.0799],
        [-1.6919, -0.1599],
        [-0.5637, -1.3702],
        [-1.0418, -1.2930],
        [-1.8563,  0.3194],
        [-1.8467,  0.1604],
        [-1.0810, -0.8344],
        [-1.1442, -1.0757],
        [-1.3558, -0.1640],
        [-0.5165, -0.9962],
        [-2.0417, -0.4539],
        [-1.4379,  0.0493],
        [-1.3990, -0.4952],
        [-1.1768, -0.2240],
        [-0.8441, -0.6414],
        [-1.0880, -0.3328],
        [-1.3427, -0.0447],
        [-1.5546, -0.9672],
        [-1.0425, -0.5658],
        [-0.4648, -0.9242],
        [-0.5846, -2.2108],
        [-0.9626, -0.8885],
        [-1.3278, -0.5689],
        [-1.4232,  0.0150],
        [-0.9357, -0.7443],
        [-0.9934, -0.3939],
        [-0.6509, -0.7777],
        [-1.0364, -0.9715],
        [-3.5312, -1.0840],
        [-0.9454, -0.7930],
        [-1.3321, -1.5623],
        [-0.6125, -0.8338],
        [-0.8840, -0.5318],
        [-0.4097, -1.0194],
        [-0.6975, -0.7163],
        [-2.0201, -0.2721],
        [-1.5945, -0.1653],
        [-0.8746, -0.5304],
        [-1.8350, -0.3165],
        [-0.9851, -0.6840],
        [-1.0356, -1.0499],
        [-2.2997,  0.8091],
        [-1.0852, -0.3596],
        [-2.8309, -1.4385],
        [-1.3322, -0.3883],
        [-1.5471,  0.1090],
        [-1.0859, -0.3019],
        [-1.4421, -2.2992],
        [-0.6431, -1.2081],
        [-0.7469, -0.6993],
        [-1.4823, -0.3140],
        [-0.5425, -1.5699],
        [-1.8769, -0.5040],
        [-0.3698, -1.0226],
        [-1.7454, -0.2600],
        [-0.7193, -0.7200],
        [-1.0341, -0.4781],
        [-0.9575, -0.5028],
        [-2.0067, -0.1251],
        [-1.0334, -0.3613],
        [-1.6033, -0.1377],
        [-1.5659, -0.7080],
        [-0.6878, -0.7596],
        [-1.4067, -0.6762],
        [-1.0359, -0.8585],
        [-1.4563, -0.5560],
        [-1.2815, -0.7852],
        [-1.3537, -0.0467],
        [-1.6258,  0.2036],
        [-1.7323, -0.1804],
        [-1.2966, -0.1226],
        [-1.1792, -1.5429],
        [-1.7021, -1.7928],
        [-2.0573,  0.2576],
        [-2.0871, -0.2256],
        [-1.4902, -0.1185],
        [-1.6043,  0.2043],
        [-1.5976, -0.2950],
        [-1.7101,  0.1119],
        [-0.7320, -0.7520],
        [-1.3305, -0.1057],
        [-0.9939, -0.6706],
        [-1.6694, -0.2099],
        [-0.9251, -0.7874],
        [-0.8756, -0.5191],
        [-0.4427, -0.9644],
        [-1.1584, -0.2971],
        [-0.9753, -0.8006],
        [-2.0373, -0.0465],
        [-1.9025,  0.0646],
        [-2.5800, -0.8440],
        [-2.3065, -1.5752],
        [-1.5415,  0.1063]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7236, 0.2764],
        [0.1960, 0.8040]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3421, 0.6579], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1977, 0.1633],
         [0.8792, 0.1412]],

        [[0.8279, 0.1758],
         [0.8390, 0.4035]],

        [[0.4941, 0.1710],
         [0.8406, 0.1353]],

        [[0.1769, 0.1554],
         [0.7873, 0.3565]],

        [[0.2333, 0.1663],
         [0.4749, 0.9592]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 36
Adjusted Rand Index: 0.06998044845260952
time is 1
tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 29
Adjusted Rand Index: 0.16825678020003848
time is 2
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 38
Adjusted Rand Index: 0.04768577403151745
time is 3
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 36
Adjusted Rand Index: 0.06982753052128415
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.022913377152719128
Global Adjusted Rand Index: 0.07652031526428464
Average Adjusted Rand Index: 0.07573278207163375
Iteration 0: Loss = -18658.30045654458
Iteration 10: Loss = -10888.391325740202
Iteration 20: Loss = -10886.582175744834
Iteration 30: Loss = -10886.390697475672
Iteration 40: Loss = -10886.220407407467
Iteration 50: Loss = -10885.67243116899
Iteration 60: Loss = -10885.659733713132
Iteration 70: Loss = -10885.636102130884
Iteration 80: Loss = -10885.501211117706
Iteration 90: Loss = -10885.41117095555
Iteration 100: Loss = -10885.38627232889
Iteration 110: Loss = -10885.377106342476
Iteration 120: Loss = -10885.371672751497
Iteration 130: Loss = -10885.36777453794
Iteration 140: Loss = -10885.364741403928
Iteration 150: Loss = -10885.362354358764
Iteration 160: Loss = -10885.360429516033
Iteration 170: Loss = -10885.358892011496
Iteration 180: Loss = -10885.357628120479
Iteration 190: Loss = -10885.35670748805
Iteration 200: Loss = -10885.355985110227
Iteration 210: Loss = -10885.35545673212
Iteration 220: Loss = -10885.355127379802
Iteration 230: Loss = -10885.354991484435
Iteration 240: Loss = -10885.355017693779
1
Iteration 250: Loss = -10885.355131207667
2
Iteration 260: Loss = -10885.355467817342
3
Stopping early at iteration 259 due to no improvement.
pi: tensor([[0.1515, 0.8485],
        [0.1111, 0.8889]], dtype=torch.float64)
alpha: tensor([0.1148, 0.8852])
beta: tensor([[[0.1972, 0.1635],
         [0.8308, 0.1526]],

        [[0.0953, 0.2161],
         [0.0220, 0.6319]],

        [[0.0719, 0.1748],
         [0.2474, 0.4468]],

        [[0.0683, 0.1576],
         [0.7176, 0.6199]],

        [[0.7956, 0.1827],
         [0.8400, 0.6354]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.006738692547152536
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0023374197449582577
Average Adjusted Rand Index: -0.0013477385094305071
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18658.076806178608
Iteration 100: Loss = -10890.355235142217
Iteration 200: Loss = -10887.71470704297
Iteration 300: Loss = -10886.959543870535
Iteration 400: Loss = -10886.383845474835
Iteration 500: Loss = -10885.859549417315
Iteration 600: Loss = -10885.530422897873
Iteration 700: Loss = -10885.321829272238
Iteration 800: Loss = -10885.188877928467
Iteration 900: Loss = -10885.108634744372
Iteration 1000: Loss = -10885.061832087571
Iteration 1100: Loss = -10885.031260129681
Iteration 1200: Loss = -10885.009860702638
Iteration 1300: Loss = -10884.994117325481
Iteration 1400: Loss = -10884.981987445637
Iteration 1500: Loss = -10884.972127987045
Iteration 1600: Loss = -10884.963928547697
Iteration 1700: Loss = -10884.957009470108
Iteration 1800: Loss = -10884.95109006609
Iteration 1900: Loss = -10884.945961003528
Iteration 2000: Loss = -10884.941590857434
Iteration 2100: Loss = -10885.388597862117
1
Iteration 2200: Loss = -10884.933924894343
Iteration 2300: Loss = -10884.93065795838
Iteration 2400: Loss = -10884.927736656833
Iteration 2500: Loss = -10884.925598776616
Iteration 2600: Loss = -10884.922009183992
Iteration 2700: Loss = -10884.919140718173
Iteration 2800: Loss = -10884.9192468052
1
Iteration 2900: Loss = -10884.912895392992
Iteration 3000: Loss = -10884.909440228541
Iteration 3100: Loss = -10884.991358526378
1
Iteration 3200: Loss = -10884.901474127408
Iteration 3300: Loss = -10884.896855840367
Iteration 3400: Loss = -10884.891779164971
Iteration 3500: Loss = -10884.886010095182
Iteration 3600: Loss = -10884.88162470954
Iteration 3700: Loss = -10884.883823982409
1
Iteration 3800: Loss = -10884.864488930636
Iteration 3900: Loss = -10884.855375987741
Iteration 4000: Loss = -10884.87985970802
1
Iteration 4100: Loss = -10884.833041322634
Iteration 4200: Loss = -10884.813078534718
Iteration 4300: Loss = -10884.77856429421
Iteration 4400: Loss = -10884.439360406172
Iteration 4500: Loss = -10884.136651433837
Iteration 4600: Loss = -10884.03971862393
Iteration 4700: Loss = -10884.007936980624
Iteration 4800: Loss = -10883.995873358626
Iteration 4900: Loss = -10884.004409549267
1
Iteration 5000: Loss = -10883.985719143642
Iteration 5100: Loss = -10883.999235570305
1
Iteration 5200: Loss = -10883.981830741865
Iteration 5300: Loss = -10884.28904303115
1
Iteration 5400: Loss = -10883.979866836922
Iteration 5500: Loss = -10883.979213152386
Iteration 5600: Loss = -10883.981914403856
1
Iteration 5700: Loss = -10883.978314679955
Iteration 5800: Loss = -10883.978010781231
Iteration 5900: Loss = -10883.984302885452
1
Iteration 6000: Loss = -10883.977536683738
Iteration 6100: Loss = -10883.977361509329
Iteration 6200: Loss = -10883.977815384547
1
Iteration 6300: Loss = -10883.977070822028
Iteration 6400: Loss = -10883.976934682447
Iteration 6500: Loss = -10883.976948509118
1
Iteration 6600: Loss = -10883.976744176158
Iteration 6700: Loss = -10883.983880696895
1
Iteration 6800: Loss = -10883.976577960751
Iteration 6900: Loss = -10883.976543398712
Iteration 7000: Loss = -10883.978325880342
1
Iteration 7100: Loss = -10883.976399275665
Iteration 7200: Loss = -10883.976392430408
Iteration 7300: Loss = -10883.976394154888
1
Iteration 7400: Loss = -10883.976301942565
Iteration 7500: Loss = -10884.028773320073
1
Iteration 7600: Loss = -10883.976272343029
Iteration 7700: Loss = -10883.976231746848
Iteration 7800: Loss = -10884.018148235014
1
Iteration 7900: Loss = -10883.976156427168
Iteration 8000: Loss = -10883.976161572884
1
Iteration 8100: Loss = -10883.978061014453
2
Iteration 8200: Loss = -10883.976161417277
3
Iteration 8300: Loss = -10883.976126673298
Iteration 8400: Loss = -10883.97652007599
1
Iteration 8500: Loss = -10883.976088880177
Iteration 8600: Loss = -10884.019564534045
1
Iteration 8700: Loss = -10883.976055723368
Iteration 8800: Loss = -10883.976041464828
Iteration 8900: Loss = -10884.00242900683
1
Iteration 9000: Loss = -10883.976019318852
Iteration 9100: Loss = -10883.976037099734
1
Iteration 9200: Loss = -10883.976634196513
2
Iteration 9300: Loss = -10883.980597572543
3
Iteration 9400: Loss = -10883.976092235145
4
Iteration 9500: Loss = -10883.976129846027
5
Iteration 9600: Loss = -10883.978765573162
6
Iteration 9700: Loss = -10883.976142163117
7
Iteration 9800: Loss = -10883.976070290377
8
Iteration 9900: Loss = -10883.985144247197
9
Iteration 10000: Loss = -10883.975996584759
Iteration 10100: Loss = -10883.996234344138
1
Iteration 10200: Loss = -10883.976706241172
2
Iteration 10300: Loss = -10883.976017853325
3
Iteration 10400: Loss = -10883.977560310035
4
Iteration 10500: Loss = -10883.976714789353
5
Iteration 10600: Loss = -10884.001316058595
6
Iteration 10700: Loss = -10883.975986204176
Iteration 10800: Loss = -10883.977440539638
1
Iteration 10900: Loss = -10883.981507071663
2
Iteration 11000: Loss = -10883.976001896915
3
Iteration 11100: Loss = -10883.994166002743
4
Iteration 11200: Loss = -10884.026838245953
5
Iteration 11300: Loss = -10883.97656849277
6
Iteration 11400: Loss = -10884.01159901916
7
Iteration 11500: Loss = -10883.980742109121
8
Iteration 11600: Loss = -10883.976613357156
9
Iteration 11700: Loss = -10883.976296748315
10
Stopping early at iteration 11700 due to no improvement.
tensor([[-4.9905,  2.0844],
        [-4.8423,  3.3831],
        [-0.5090, -2.2447],
        [-4.1602,  2.5973],
        [-5.7977,  4.3426],
        [-6.1049,  4.4223],
        [-2.8300,  1.1281],
        [-6.7801,  5.1885],
        [-6.6926,  4.9905],
        [-3.7644,  2.1397],
        [-4.0098,  2.5929],
        [-6.7922,  5.3878],
        [-6.1129,  4.3882],
        [-4.9724,  3.3472],
        [-4.0230,  2.5481],
        [-5.5375,  4.1512],
        [-3.6543,  2.2386],
        [-6.9611,  4.2284],
        [-6.9181,  5.1581],
        [-5.8782,  4.1920],
        [-6.1436,  4.7080],
        [-4.8726,  2.4379],
        [-5.0043,  3.4012],
        [-5.8397,  3.8857],
        [-3.4573,  1.7345],
        [-4.7007,  3.3110],
        [-2.9089,  1.5013],
        [-2.7443, -1.1481],
        [-5.4100,  1.4856],
        [-5.5418,  4.0929],
        [-7.0607,  5.6231],
        [-5.9256,  1.3103],
        [-5.4355,  3.3312],
        [-5.0557,  3.2814],
        [-4.3630,  2.7037],
        [-7.9911,  4.7355],
        [-4.4784,  3.0541],
        [-4.6129,  1.3942],
        [-4.4354,  2.5426],
        [-5.4747,  3.6289],
        [-3.9197,  1.7811],
        [-4.5246,  3.0500],
        [-6.5903,  4.6591],
        [-5.8695,  4.0175],
        [-5.6806,  4.2829],
        [-6.0450,  3.9200],
        [-5.0154,  3.3915],
        [-3.4458,  1.7288],
        [-7.5213,  6.1218],
        [-4.7246,  3.3236],
        [-4.9111,  3.3598],
        [-6.6577,  3.7709],
        [-7.9565,  3.3413],
        [-7.5085,  3.2855],
        [-3.2436,  1.8362],
        [-4.4103,  2.1243],
        [-4.3031,  2.8811],
        [-5.5095,  3.9261],
        [-3.3726,  1.9019],
        [-6.0125,  4.4476],
        [-4.0201,  2.6299],
        [-6.0892,  4.5095],
        [-3.9537,  2.1893],
        [-4.3868,  2.3037],
        [-4.2772,  2.3157],
        [-7.2422,  5.6896],
        [-4.8479,  3.0592],
        [-7.1574,  4.6215],
        [-5.4149,  4.0188],
        [-5.8089,  1.1937],
        [-4.6064,  2.9993],
        [-3.4584,  2.0036],
        [-6.2548,  1.6396],
        [-5.6381,  2.0299],
        [-5.7484,  3.9152],
        [-6.6758,  3.7946],
        [-5.6216,  4.1386],
        [-4.9461,  3.5094],
        [-4.0412,  2.0650],
        [-4.6247,  3.0778],
        [-7.1184,  5.5447],
        [-6.4039,  4.7491],
        [-7.7110,  5.0175],
        [-6.5607,  5.0622],
        [-4.8559,  3.4032],
        [-6.9547,  5.3212],
        [-4.1745,  2.7664],
        [-5.3470,  3.6100],
        [-6.4383,  4.0581],
        [-6.5379,  4.8479],
        [-4.5575,  2.5870],
        [-4.8815,  3.2610],
        [-3.9173,  2.4399],
        [-6.1311,  4.3119],
        [-3.9229,  2.4477],
        [-6.4003,  4.8462],
        [-8.1238,  4.8220],
        [-6.9673,  5.1614],
        [-5.4210,  3.9583],
        [-7.1625,  5.2370]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4184, 0.5816],
        [0.1306, 0.8694]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0112, 0.9888], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2042, 0.2772],
         [0.8308, 0.1524]],

        [[0.0953, 0.2087],
         [0.0220, 0.6319]],

        [[0.0719, 0.1760],
         [0.2474, 0.4468]],

        [[0.0683, 0.1616],
         [0.7176, 0.6199]],

        [[0.7956, 0.1784],
         [0.8400, 0.6354]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.006738692547152536
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
Global Adjusted Rand Index: 0.00234217628298956
Average Adjusted Rand Index: -0.0012446639897738744
Iteration 0: Loss = -32756.021281794405
Iteration 10: Loss = -10888.697308635623
Iteration 20: Loss = -10888.697408260367
1
Iteration 30: Loss = -10888.697106954696
Iteration 40: Loss = -10888.6864221498
Iteration 50: Loss = -10888.304492221336
Iteration 60: Loss = -10886.647470474141
Iteration 70: Loss = -10886.391102463169
Iteration 80: Loss = -10886.309793127626
Iteration 90: Loss = -10885.682824712985
Iteration 100: Loss = -10885.660063851185
Iteration 110: Loss = -10885.666239306847
1
Iteration 120: Loss = -10885.670367437335
2
Iteration 130: Loss = -10885.673391943234
3
Stopping early at iteration 129 due to no improvement.
pi: tensor([[2.6121e-07, 1.0000e+00],
        [8.8719e-02, 9.1128e-01]], dtype=torch.float64)
alpha: tensor([0.0811, 0.9189])
beta: tensor([[[0.1882, 0.1568],
         [0.4897, 0.1549]],

        [[0.6296, 0.2253],
         [0.6366, 0.8058]],

        [[0.9510, 0.1732],
         [0.7523, 0.9999]],

        [[0.3128, 0.1513],
         [0.4222, 0.5691]],

        [[0.4158, 0.1832],
         [0.3560, 0.7194]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0070073061126442035
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0018058126460340001
Average Adjusted Rand Index: -0.0014014612225288407
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -32694.546330601286
Iteration 100: Loss = -10898.301975055945
Iteration 200: Loss = -10892.231921126804
Iteration 300: Loss = -10889.3456237005
Iteration 400: Loss = -10888.460964861686
Iteration 500: Loss = -10888.104060819784
Iteration 600: Loss = -10887.917721821968
Iteration 700: Loss = -10887.783717025623
Iteration 800: Loss = -10887.673887300165
Iteration 900: Loss = -10887.570110837194
Iteration 1000: Loss = -10887.460374133329
Iteration 1100: Loss = -10887.326756837549
Iteration 1200: Loss = -10887.156900690621
Iteration 1300: Loss = -10886.976516653089
Iteration 1400: Loss = -10886.797870031165
Iteration 1500: Loss = -10886.603826962619
Iteration 1600: Loss = -10886.380299670625
Iteration 1700: Loss = -10886.116204357724
Iteration 1800: Loss = -10885.803126949377
Iteration 1900: Loss = -10885.441534819352
Iteration 2000: Loss = -10885.066911848877
Iteration 2100: Loss = -10884.766603642642
Iteration 2200: Loss = -10884.582725279693
Iteration 2300: Loss = -10884.477859004417
Iteration 2400: Loss = -10884.408715988106
Iteration 2500: Loss = -10884.362239970284
Iteration 2600: Loss = -10884.32790709903
Iteration 2700: Loss = -10884.298035034022
Iteration 2800: Loss = -10884.271165518405
Iteration 2900: Loss = -10884.247785757248
Iteration 3000: Loss = -10884.229563503159
Iteration 3100: Loss = -10884.215324538056
Iteration 3200: Loss = -10884.203793345074
Iteration 3300: Loss = -10884.19382628694
Iteration 3400: Loss = -10884.184592697156
Iteration 3500: Loss = -10884.17579942838
Iteration 3600: Loss = -10884.167131188746
Iteration 3700: Loss = -10884.158351901295
Iteration 3800: Loss = -10884.149283349207
Iteration 3900: Loss = -10884.139677159477
Iteration 4000: Loss = -10884.12888809767
Iteration 4100: Loss = -10884.11622153535
Iteration 4200: Loss = -10884.100978626324
Iteration 4300: Loss = -10884.083192197671
Iteration 4400: Loss = -10884.06328081329
Iteration 4500: Loss = -10884.043395011153
Iteration 4600: Loss = -10884.026259405959
Iteration 4700: Loss = -10884.013151359242
Iteration 4800: Loss = -10884.004769903422
Iteration 4900: Loss = -10883.99956675302
Iteration 5000: Loss = -10883.99624471945
Iteration 5100: Loss = -10883.993672878705
Iteration 5200: Loss = -10883.991639182737
Iteration 5300: Loss = -10883.990001432074
Iteration 5400: Loss = -10883.988551844257
Iteration 5500: Loss = -10883.987626165563
Iteration 5600: Loss = -10883.986484878022
Iteration 5700: Loss = -10883.985996676676
Iteration 5800: Loss = -10883.985113980569
Iteration 5900: Loss = -10883.984604997373
Iteration 6000: Loss = -10883.983979305413
Iteration 6100: Loss = -10883.99107608212
1
Iteration 6200: Loss = -10883.98282039429
Iteration 6300: Loss = -10883.982356741366
Iteration 6400: Loss = -10883.98223248124
Iteration 6500: Loss = -10883.98167671041
Iteration 6600: Loss = -10883.984648909543
1
Iteration 6700: Loss = -10883.981057766876
Iteration 6800: Loss = -10883.980820998286
Iteration 6900: Loss = -10883.980405264496
Iteration 7000: Loss = -10883.980008704044
Iteration 7100: Loss = -10883.979573980478
Iteration 7200: Loss = -10883.978790001878
Iteration 7300: Loss = -10883.982902155096
1
Iteration 7400: Loss = -10883.98559073871
2
Iteration 7500: Loss = -10883.978399732525
Iteration 7600: Loss = -10883.978442010579
1
Iteration 7700: Loss = -10883.98651085917
2
Iteration 7800: Loss = -10883.980568522631
3
Iteration 7900: Loss = -10883.981074497768
4
Iteration 8000: Loss = -10883.982078500932
5
Iteration 8100: Loss = -10883.99547670169
6
Iteration 8200: Loss = -10883.977666146335
Iteration 8300: Loss = -10883.977982796041
1
Iteration 8400: Loss = -10883.983198640743
2
Iteration 8500: Loss = -10883.978076396914
3
Iteration 8600: Loss = -10883.978703743454
4
Iteration 8700: Loss = -10883.992756833873
5
Iteration 8800: Loss = -10883.97900025356
6
Iteration 8900: Loss = -10883.98047028166
7
Iteration 9000: Loss = -10883.982520778827
8
Iteration 9100: Loss = -10883.97725847879
Iteration 9200: Loss = -10883.977298841646
1
Iteration 9300: Loss = -10883.97722799544
Iteration 9400: Loss = -10883.97719820147
Iteration 9500: Loss = -10883.982243939223
1
Iteration 9600: Loss = -10883.982030829953
2
Iteration 9700: Loss = -10884.02009491204
3
Iteration 9800: Loss = -10883.977142881626
Iteration 9900: Loss = -10883.986663018413
1
Iteration 10000: Loss = -10884.00572775729
2
Iteration 10100: Loss = -10883.97715148387
3
Iteration 10200: Loss = -10883.977087700276
Iteration 10300: Loss = -10884.037948650888
1
Iteration 10400: Loss = -10883.976912906297
Iteration 10500: Loss = -10883.993950565879
1
Iteration 10600: Loss = -10883.976303025373
Iteration 10700: Loss = -10883.976319706167
1
Iteration 10800: Loss = -10883.976280291432
Iteration 10900: Loss = -10883.979339205
1
Iteration 11000: Loss = -10883.97627621885
Iteration 11100: Loss = -10883.999132209454
1
Iteration 11200: Loss = -10883.982708021958
2
Iteration 11300: Loss = -10884.125231111595
3
Iteration 11400: Loss = -10883.978154408034
4
Iteration 11500: Loss = -10883.976145017796
Iteration 11600: Loss = -10883.981184084747
1
Iteration 11700: Loss = -10884.106265481234
2
Iteration 11800: Loss = -10883.978663954042
3
Iteration 11900: Loss = -10883.976165481961
4
Iteration 12000: Loss = -10883.977487277494
5
Iteration 12100: Loss = -10883.980574604559
6
Iteration 12200: Loss = -10883.976538279527
7
Iteration 12300: Loss = -10884.087993408795
8
Iteration 12400: Loss = -10884.00207541407
9
Iteration 12500: Loss = -10884.024496319968
10
Stopping early at iteration 12500 due to no improvement.
tensor([[-4.5279,  2.5493],
        [-4.8038,  3.4168],
        [-0.0269, -1.7676],
        [-4.4593,  2.2965],
        [-5.7769,  4.3636],
        [-6.1356,  4.6609],
        [-2.9326,  1.0285],
        [-6.7449,  5.3481],
        [-6.5076,  5.1031],
        [-4.3431,  1.5575],
        [-4.0197,  2.5837],
        [-6.9431,  5.5479],
        [-5.9537,  4.5455],
        [-4.9838,  3.3299],
        [-4.1626,  2.4143],
        [-6.6251,  3.0641],
        [-3.9491,  1.9476],
        [-6.3070,  4.9111],
        [-6.7138,  5.3207],
        [-6.3005,  4.2544],
        [-6.2615,  4.5901],
        [-4.8815,  2.4287],
        [-5.4232,  2.9813],
        [-5.5879,  4.1111],
        [-3.5784,  1.6186],
        [-4.8203,  3.1872],
        [-3.4666,  0.9439],
        [-1.8696, -0.2758],
        [-4.2064,  2.6907],
        [-5.8781,  3.7563],
        [-7.3807,  5.3039],
        [-4.4228,  2.8153],
        [-5.7246,  3.0372],
        [-5.6064,  2.7236],
        [-4.2301,  2.8371],
        [-6.9876,  5.1922],
        [-4.4617,  3.0681],
        [-4.1683,  1.8391],
        [-4.5182,  2.4550],
        [-5.6249,  3.4758],
        [-3.5449,  2.1562],
        [-4.5634,  3.0044],
        [-6.3149,  4.8324],
        [-5.6402,  4.2526],
        [-7.9422,  4.2264],
        [-5.7886,  4.1953],
        [-4.8962,  3.5096],
        [-3.4363,  1.7426],
        [-7.8834,  5.2879],
        [-5.1675,  2.8767],
        [-4.8469,  3.4287],
        [-6.0407,  4.3892],
        [-6.8202,  4.4329],
        [-6.1601,  4.7514],
        [-3.2423,  1.8330],
        [-4.2070,  2.3256],
        [-4.2962,  2.8913],
        [-5.8350,  3.6322],
        [-3.5337,  1.7373],
        [-6.4334,  4.3440],
        [-4.0181,  2.6305],
        [-6.0292,  4.6082],
        [-3.9297,  2.2166],
        [-4.0530,  2.6410],
        [-4.8393,  1.7583],
        [-7.0576,  5.6156],
        [-4.8057,  3.1012],
        [-7.6294,  4.2725],
        [-6.1097,  3.3226],
        [-5.8105,  1.1952],
        [-4.5270,  3.0821],
        [-3.6940,  1.7734],
        [-4.6887,  3.2090],
        [-4.9200,  2.7515],
        [-5.5317,  4.1308],
        [-6.1170,  4.3543],
        [-6.1529,  3.5927],
        [-5.2639,  3.1914],
        [-3.7550,  2.3520],
        [-4.6720,  3.0263],
        [-7.0868,  5.6319],
        [-6.3280,  4.8477],
        [-8.0306,  4.8251],
        [-6.8792,  4.9174],
        [-5.6861,  2.5728],
        [-6.8178,  5.4174],
        [-4.1964,  2.7451],
        [-5.6819,  3.2732],
        [-6.6745,  3.8291],
        [-6.3934,  5.0046],
        [-4.3151,  2.8337],
        [-5.0112,  3.1252],
        [-3.8744,  2.4862],
        [-7.1743,  3.2740],
        [-4.1879,  2.1853],
        [-6.9907,  4.2568],
        [-6.9841,  5.5707],
        [-8.6007,  3.9855],
        [-5.5109,  3.8704],
        [-6.9426,  5.3619]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4200, 0.5800],
        [0.1295, 0.8705]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0111, 0.9889], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2039, 0.2773],
         [0.4897, 0.1528]],

        [[0.6296, 0.2098],
         [0.6366, 0.8058]],

        [[0.9510, 0.1755],
         [0.7523, 0.9999]],

        [[0.3128, 0.1612],
         [0.4222, 0.5691]],

        [[0.4158, 0.1779],
         [0.3560, 0.7194]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.006738692547152536
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
Global Adjusted Rand Index: 0.00234217628298956
Average Adjusted Rand Index: -0.0012446639897738744
10852.630324507372
new:  [0.07429099710834049, 0.07652031526428464, 0.00234217628298956, 0.00234217628298956] [0.07356768739233256, 0.07573278207163375, -0.0012446639897738744, -0.0012446639897738744] [10884.013346024216, 10884.715642555535, 10883.976296748315, 10884.024496319968]
prior:  [0.0, 0.0, 0.0023374197449582577, 0.0018058126460340001] [0.0, 0.0, -0.0013477385094305071, -0.0014014612225288407] [nan, 10888.697304319736, 10885.355467817342, 10885.673391943234]
-----------------------------------------------------------------------------------------
This iteration is 12
True Objective function: Loss = -10873.378638192185
Iteration 0: Loss = -28801.6677722376
Iteration 10: Loss = -10980.12575026955
Iteration 20: Loss = -10978.516533133734
Iteration 30: Loss = -10978.007068491508
Iteration 40: Loss = -10977.681078204443
Iteration 50: Loss = -10977.31815295851
Iteration 60: Loss = -10975.453117598605
Iteration 70: Loss = -10975.452675610299
Iteration 80: Loss = -10975.455092333725
1
Iteration 90: Loss = -10975.455293791787
2
Iteration 100: Loss = -10975.455319332304
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.9833, 0.0167],
        [0.9788, 0.0212]], dtype=torch.float64)
alpha: tensor([0.9830, 0.0170])
beta: tensor([[[0.1607, 0.1383],
         [0.8101, 0.1141]],

        [[0.0815, 0.1511],
         [0.3667, 0.0242]],

        [[0.8844, 0.0691],
         [0.1266, 0.9265]],

        [[0.3618, 0.2055],
         [0.5278, 0.8192]],

        [[0.1840, 0.3392],
         [0.4898, 0.7843]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -0.00028920796095122244
Average Adjusted Rand Index: 0.0004951898005778471
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28551.754228054437
Iteration 100: Loss = -10987.429581300214
Iteration 200: Loss = -10984.35812133809
Iteration 300: Loss = -10983.450824993371
Iteration 400: Loss = -10983.090380076026
Iteration 500: Loss = -10982.898174768192
Iteration 600: Loss = -10982.755715314832
Iteration 700: Loss = -10982.629317095176
Iteration 800: Loss = -10982.530446261875
Iteration 900: Loss = -10982.473057359392
Iteration 1000: Loss = -10982.45338456907
Iteration 1100: Loss = -10982.443389519913
Iteration 1200: Loss = -10982.434471840832
Iteration 1300: Loss = -10982.425429931489
Iteration 1400: Loss = -10982.415447860285
Iteration 1500: Loss = -10982.403175373262
Iteration 1600: Loss = -10982.386486514066
Iteration 1700: Loss = -10982.359414456241
Iteration 1800: Loss = -10982.310551946872
Iteration 1900: Loss = -10982.25251017378
Iteration 2000: Loss = -10982.090840806173
Iteration 2100: Loss = -10981.998950001056
Iteration 2200: Loss = -10981.920043641
Iteration 2300: Loss = -10976.93390215896
Iteration 2400: Loss = -10976.89093278602
Iteration 2500: Loss = -10976.868463397785
Iteration 2600: Loss = -10976.8507105422
Iteration 2700: Loss = -10976.83540830633
Iteration 2800: Loss = -10976.821957093664
Iteration 2900: Loss = -10976.817555798532
Iteration 3000: Loss = -10976.796917943826
Iteration 3100: Loss = -10976.783293066866
Iteration 3200: Loss = -10976.764614401693
Iteration 3300: Loss = -10976.717742797275
Iteration 3400: Loss = -10976.4001930992
Iteration 3500: Loss = -10975.846169721417
Iteration 3600: Loss = -10975.700385801312
Iteration 3700: Loss = -10975.599903249144
Iteration 3800: Loss = -10975.548729686396
Iteration 3900: Loss = -10975.499799693964
Iteration 4000: Loss = -10975.46770856779
Iteration 4100: Loss = -10975.426800836862
Iteration 4200: Loss = -10975.37031489177
Iteration 4300: Loss = -10975.332752121521
Iteration 4400: Loss = -10975.296586966368
Iteration 4500: Loss = -10975.258293263321
Iteration 4600: Loss = -10975.229800140683
Iteration 4700: Loss = -10975.194641332797
Iteration 4800: Loss = -10975.152060077062
Iteration 4900: Loss = -10975.09564730033
Iteration 5000: Loss = -10975.006260670189
Iteration 5100: Loss = -10974.888022976664
Iteration 5200: Loss = -10974.55209295062
Iteration 5300: Loss = -10974.363163744592
Iteration 5400: Loss = -10974.260353703656
Iteration 5500: Loss = -10974.211599537102
Iteration 5600: Loss = -10974.187946662862
Iteration 5700: Loss = -10974.178655308537
Iteration 5800: Loss = -10974.162382556478
Iteration 5900: Loss = -10974.18859704544
1
Iteration 6000: Loss = -10974.143108235405
Iteration 6100: Loss = -10974.138989779898
Iteration 6200: Loss = -10974.146736870727
1
Iteration 6300: Loss = -10974.133061948329
Iteration 6400: Loss = -10974.151372160739
1
Iteration 6500: Loss = -10974.129107686384
Iteration 6600: Loss = -10974.127569405073
Iteration 6700: Loss = -10974.12645424519
Iteration 6800: Loss = -10974.125029076637
Iteration 6900: Loss = -10974.211001123469
1
Iteration 7000: Loss = -10974.1231652299
Iteration 7100: Loss = -10974.12239259151
Iteration 7200: Loss = -10974.137508326467
1
Iteration 7300: Loss = -10974.121133542043
Iteration 7400: Loss = -10974.120585164585
Iteration 7500: Loss = -10974.132427471255
1
Iteration 7600: Loss = -10974.119617352077
Iteration 7700: Loss = -10974.119236179207
Iteration 7800: Loss = -10974.119082436528
Iteration 7900: Loss = -10974.11851744189
Iteration 8000: Loss = -10974.118245224661
Iteration 8100: Loss = -10974.11798376609
Iteration 8200: Loss = -10974.117739634921
Iteration 8300: Loss = -10974.117450735635
Iteration 8400: Loss = -10974.138638285554
1
Iteration 8500: Loss = -10974.117001568547
Iteration 8600: Loss = -10974.116834820961
Iteration 8700: Loss = -10974.12403952646
1
Iteration 8800: Loss = -10974.116500452561
Iteration 8900: Loss = -10974.116361146682
Iteration 9000: Loss = -10974.118774605877
1
Iteration 9100: Loss = -10974.116056788778
Iteration 9200: Loss = -10974.244606022423
1
Iteration 9300: Loss = -10974.11581861902
Iteration 9400: Loss = -10974.115708115809
Iteration 9500: Loss = -10974.116448153372
1
Iteration 9600: Loss = -10974.115526552116
Iteration 9700: Loss = -10974.11544825856
Iteration 9800: Loss = -10974.117585702821
1
Iteration 9900: Loss = -10974.115248702758
Iteration 10000: Loss = -10974.115200290686
Iteration 10100: Loss = -10974.12412661896
1
Iteration 10200: Loss = -10974.11506262324
Iteration 10300: Loss = -10974.114966122886
Iteration 10400: Loss = -10974.121996774295
1
Iteration 10500: Loss = -10974.1148688141
Iteration 10600: Loss = -10974.114819759274
Iteration 10700: Loss = -10974.198567586793
1
Iteration 10800: Loss = -10974.1147337504
Iteration 10900: Loss = -10974.114701169614
Iteration 11000: Loss = -10974.11466589183
Iteration 11100: Loss = -10974.117781267747
1
Iteration 11200: Loss = -10974.114573945983
Iteration 11300: Loss = -10974.114794067047
1
Iteration 11400: Loss = -10974.114522772632
Iteration 11500: Loss = -10974.114464000513
Iteration 11600: Loss = -10974.114461229552
Iteration 11700: Loss = -10974.114519541012
1
Iteration 11800: Loss = -10974.114378068027
Iteration 11900: Loss = -10974.177731083597
1
Iteration 12000: Loss = -10974.122010716255
2
Iteration 12100: Loss = -10974.160765793986
3
Iteration 12200: Loss = -10974.114360659756
Iteration 12300: Loss = -10974.114318211363
Iteration 12400: Loss = -10974.16306559169
1
Iteration 12500: Loss = -10974.114238874276
Iteration 12600: Loss = -10974.121882990416
1
Iteration 12700: Loss = -10974.114234843624
Iteration 12800: Loss = -10974.115173296059
1
Iteration 12900: Loss = -10974.114186014964
Iteration 13000: Loss = -10974.11423936493
1
Iteration 13100: Loss = -10974.12521208473
2
Iteration 13200: Loss = -10974.114134035844
Iteration 13300: Loss = -10974.172618096982
1
Iteration 13400: Loss = -10974.114111322717
Iteration 13500: Loss = -10974.11459094385
1
Iteration 13600: Loss = -10974.114107416028
Iteration 13700: Loss = -10974.114087382672
Iteration 13800: Loss = -10974.114384865214
1
Iteration 13900: Loss = -10974.114061064985
Iteration 14000: Loss = -10974.129194633808
1
Iteration 14100: Loss = -10974.114045022076
Iteration 14200: Loss = -10974.11403833601
Iteration 14300: Loss = -10974.114111237108
1
Iteration 14400: Loss = -10974.11401578354
Iteration 14500: Loss = -10974.115929560021
1
Iteration 14600: Loss = -10974.11401919451
2
Iteration 14700: Loss = -10974.567212188827
3
Iteration 14800: Loss = -10974.114010179554
Iteration 14900: Loss = -10974.114026444557
1
Iteration 15000: Loss = -10974.137011955954
2
Iteration 15100: Loss = -10974.113951900361
Iteration 15200: Loss = -10974.115348028081
1
Iteration 15300: Loss = -10974.229925325179
2
Iteration 15400: Loss = -10974.119214937613
3
Iteration 15500: Loss = -10974.11399823851
4
Iteration 15600: Loss = -10974.124347057656
5
Iteration 15700: Loss = -10974.114825639123
6
Iteration 15800: Loss = -10974.114918586722
7
Iteration 15900: Loss = -10974.11398747897
8
Iteration 16000: Loss = -10974.114133381083
9
Iteration 16100: Loss = -10974.122337880734
10
Stopping early at iteration 16100 due to no improvement.
tensor([[-6.0968,  1.4816],
        [-6.1121,  1.4969],
        [-6.1003,  1.4851],
        [-6.0585,  1.4433],
        [-6.0506,  1.4354],
        [-6.1453,  1.5301],
        [-6.1278,  1.5126],
        [-6.0863,  1.4711],
        [-6.1220,  1.5068],
        [-6.2211,  1.6059],
        [-6.1950,  1.5797],
        [-6.1120,  1.4968],
        [-6.1477,  1.5325],
        [-6.0604,  1.4452],
        [-6.0705,  1.4553],
        [-6.0252,  1.4100],
        [-6.1519,  1.5367],
        [-6.1277,  1.5125],
        [-6.0686,  1.4534],
        [-6.1439,  1.5287],
        [-6.0217,  1.4065],
        [-6.1291,  1.5139],
        [-5.9711,  1.3559],
        [-6.1219,  1.5067],
        [-6.1177,  1.5025],
        [-6.0771,  1.4618],
        [-6.0461,  1.4309],
        [-6.2156,  1.6004],
        [-6.1429,  1.5277],
        [-6.0806,  1.4654],
        [-6.0981,  1.4828],
        [-6.1039,  1.4886],
        [-6.0476,  1.4324],
        [-6.0698,  1.4546],
        [-6.0915,  1.4763],
        [-6.1229,  1.5077],
        [-6.1105,  1.4953],
        [-6.1176,  1.5023],
        [-6.1737,  1.5585],
        [-6.1582,  1.5430],
        [-6.0819,  1.4667],
        [-6.1610,  1.5458],
        [-6.5999,  1.9847],
        [-6.0243,  1.4091],
        [-6.0825,  1.4673],
        [-6.1392,  1.5240],
        [-6.1395,  1.5242],
        [-6.1174,  1.5021],
        [-6.0797,  1.4645],
        [-6.0805,  1.4653],
        [-6.0888,  1.4736],
        [-6.1107,  1.4954],
        [-6.1496,  1.5344],
        [-6.0746,  1.4594],
        [-6.0856,  1.4703],
        [-6.0566,  1.4413],
        [-6.1454,  1.5302],
        [-6.0960,  1.4808],
        [-6.0545,  1.4393],
        [-6.0405,  1.4253],
        [-6.1492,  1.5340],
        [-6.0757,  1.4604],
        [-6.1240,  1.5087],
        [-6.4092,  1.7940],
        [-6.1467,  1.5315],
        [-6.0820,  1.4668],
        [-6.0983,  1.4831],
        [-6.1335,  1.5182],
        [-6.1617,  1.5465],
        [-6.0931,  1.4778],
        [-6.0663,  1.4510],
        [-6.1258,  1.5105],
        [-6.1235,  1.5083],
        [-6.0110,  1.3958],
        [-6.1142,  1.4990],
        [-6.0611,  1.4458],
        [-6.2231,  1.6079],
        [-6.1100,  1.4948],
        [-6.1251,  1.5099],
        [-6.1402,  1.5250],
        [-6.1353,  1.5200],
        [-6.0719,  1.4566],
        [-6.0735,  1.4583],
        [-6.1281,  1.5128],
        [-6.1022,  1.4870],
        [-6.1368,  1.5216],
        [-6.0841,  1.4689],
        [-6.1092,  1.4940],
        [-6.1430,  1.5278],
        [-6.1005,  1.4853],
        [-6.2366,  1.6214],
        [-6.0291,  1.4138],
        [-6.1204,  1.5052],
        [-6.0468,  1.4316],
        [-6.0971,  1.4819],
        [-6.0709,  1.4557],
        [-6.1301,  1.5149],
        [-6.0989,  1.4837],
        [-6.0380,  1.4227],
        [-6.0312,  1.4160]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9906, 0.0094],
        [0.8267, 0.1733]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([5.0050e-04, 9.9950e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1650, 0.1596],
         [0.8101, 0.1565]],

        [[0.0815, 0.1574],
         [0.3667, 0.0242]],

        [[0.8844, 0.0815],
         [0.1266, 0.9265]],

        [[0.3618, 0.2143],
         [0.5278, 0.8192]],

        [[0.1840, 0.3454],
         [0.4898, 0.7843]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: -0.011374456256342739
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -0.0017672282072931302
Average Adjusted Rand Index: -0.0017797014506907005
Iteration 0: Loss = -27193.213789006873
Iteration 10: Loss = -10977.341611476517
Iteration 20: Loss = -10977.182311264816
Iteration 30: Loss = -10975.465125982893
Iteration 40: Loss = -10975.455291170136
Iteration 50: Loss = -10975.455303357674
1
Iteration 60: Loss = -10975.455295480127
2
Iteration 70: Loss = -10975.455333429078
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.0212, 0.9788],
        [0.0167, 0.9833]], dtype=torch.float64)
alpha: tensor([0.0170, 0.9830])
beta: tensor([[[0.1141, 0.1383],
         [0.8114, 0.1607]],

        [[0.0274, 0.1511],
         [0.0661, 0.8862]],

        [[0.0456, 0.0691],
         [0.0749, 0.4899]],

        [[0.4640, 0.2055],
         [0.6246, 0.4866]],

        [[0.3852, 0.3392],
         [0.8203, 0.5202]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -0.00028920796095122244
Average Adjusted Rand Index: 0.0004951898005778471
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27193.635521804477
Iteration 100: Loss = -11059.618160007836
Iteration 200: Loss = -11028.454294856707
Iteration 300: Loss = -11010.235837133703
Iteration 400: Loss = -10993.525868986017
Iteration 500: Loss = -10990.12144130775
Iteration 600: Loss = -10983.69633540771
Iteration 700: Loss = -10980.358486328383
Iteration 800: Loss = -10979.601584777572
Iteration 900: Loss = -10979.16935854692
Iteration 1000: Loss = -10978.892633808575
Iteration 1100: Loss = -10978.684024457052
Iteration 1200: Loss = -10978.452210805543
Iteration 1300: Loss = -10977.938868869318
Iteration 1400: Loss = -10977.586460084513
Iteration 1500: Loss = -10977.412056626265
Iteration 1600: Loss = -10977.29716946354
Iteration 1700: Loss = -10977.214530447656
Iteration 1800: Loss = -10977.150983680625
Iteration 1900: Loss = -10977.099560155963
Iteration 2000: Loss = -10977.05699769531
Iteration 2100: Loss = -10977.021073227812
Iteration 2200: Loss = -10976.99011162179
Iteration 2300: Loss = -10976.962417401499
Iteration 2400: Loss = -10976.936709366973
Iteration 2500: Loss = -10976.913203825883
Iteration 2600: Loss = -10976.894162571192
Iteration 2700: Loss = -10976.877730426786
Iteration 2800: Loss = -10976.86264507919
Iteration 2900: Loss = -10976.848599012099
Iteration 3000: Loss = -10976.835270295272
Iteration 3100: Loss = -10976.822595090294
Iteration 3200: Loss = -10976.810479357575
Iteration 3300: Loss = -10976.79881642532
Iteration 3400: Loss = -10976.78708348178
Iteration 3500: Loss = -10976.774897415831
Iteration 3600: Loss = -10976.762354054781
Iteration 3700: Loss = -10976.749244121174
Iteration 3800: Loss = -10976.734733067216
Iteration 3900: Loss = -10976.719130797894
Iteration 4000: Loss = -10976.704128539513
Iteration 4100: Loss = -10976.689344641141
Iteration 4200: Loss = -10976.673826610799
Iteration 4300: Loss = -10976.662848324939
Iteration 4400: Loss = -10976.655346159718
Iteration 4500: Loss = -10976.649894364873
Iteration 4600: Loss = -10976.645584291968
Iteration 4700: Loss = -10976.64214361196
Iteration 4800: Loss = -10976.63914947188
Iteration 4900: Loss = -10976.636449275036
Iteration 5000: Loss = -10976.633714656204
Iteration 5100: Loss = -10976.629525123903
Iteration 5200: Loss = -10976.578209460857
Iteration 5300: Loss = -10976.489754549712
Iteration 5400: Loss = -10976.31341993338
Iteration 5500: Loss = -10976.141893983082
Iteration 5600: Loss = -10976.079678890033
Iteration 5700: Loss = -10976.050406083605
Iteration 5800: Loss = -10976.032738223847
Iteration 5900: Loss = -10976.020115560756
Iteration 6000: Loss = -10976.00960049922
Iteration 6100: Loss = -10975.992690833345
Iteration 6200: Loss = -10975.80046049667
Iteration 6300: Loss = -10975.73373316211
Iteration 6400: Loss = -10975.600209260212
Iteration 6500: Loss = -10975.5962284482
Iteration 6600: Loss = -10975.59229738653
Iteration 6700: Loss = -10975.589260640407
Iteration 6800: Loss = -10975.571087417122
Iteration 6900: Loss = -10975.507244573273
Iteration 7000: Loss = -10975.436864309539
Iteration 7100: Loss = -10975.3880450246
Iteration 7200: Loss = -10975.31859029124
Iteration 7300: Loss = -10975.268731486634
Iteration 7400: Loss = -10975.217968665125
Iteration 7500: Loss = -10975.17151796769
Iteration 7600: Loss = -10975.131166485882
Iteration 7700: Loss = -10975.071213908192
Iteration 7800: Loss = -10975.05071129053
Iteration 7900: Loss = -10975.003621006003
Iteration 8000: Loss = -10974.949072454432
Iteration 8100: Loss = -10974.919221596734
Iteration 8200: Loss = -10974.901077942706
Iteration 8300: Loss = -10974.875727890854
Iteration 8400: Loss = -10974.839382217937
Iteration 8500: Loss = -10974.821659942201
Iteration 8600: Loss = -10974.805151751656
Iteration 8700: Loss = -10974.817232604486
1
Iteration 8800: Loss = -10974.767332037283
Iteration 8900: Loss = -10974.74068843471
Iteration 9000: Loss = -10974.720819179642
Iteration 9100: Loss = -10974.702910351949
Iteration 9200: Loss = -10974.695764560207
Iteration 9300: Loss = -10974.689631037247
Iteration 9400: Loss = -10974.677324287326
Iteration 9500: Loss = -10974.663293407859
Iteration 9600: Loss = -10974.65625331441
Iteration 9700: Loss = -10974.658596631805
1
Iteration 9800: Loss = -10974.653276657147
Iteration 9900: Loss = -10974.648323881342
Iteration 10000: Loss = -10974.635215911285
Iteration 10100: Loss = -10974.63216416729
Iteration 10200: Loss = -10974.629269449157
Iteration 10300: Loss = -10974.634218843985
1
Iteration 10400: Loss = -10974.626836825035
Iteration 10500: Loss = -10974.626971040507
1
Iteration 10600: Loss = -10974.628254089228
2
Iteration 10700: Loss = -10974.626445370941
Iteration 10800: Loss = -10974.691419144136
1
Iteration 10900: Loss = -10974.669211718921
2
Iteration 11000: Loss = -10974.628783482849
3
Iteration 11100: Loss = -10974.630583831708
4
Iteration 11200: Loss = -10974.626751181979
5
Iteration 11300: Loss = -10974.632060367265
6
Iteration 11400: Loss = -10974.625718292771
Iteration 11500: Loss = -10974.625967192862
1
Iteration 11600: Loss = -10974.62562763955
Iteration 11700: Loss = -10974.625477296624
Iteration 11800: Loss = -10974.625612286936
1
Iteration 11900: Loss = -10974.625532689117
2
Iteration 12000: Loss = -10974.625502198605
3
Iteration 12100: Loss = -10974.625484445252
4
Iteration 12200: Loss = -10974.625228095307
Iteration 12300: Loss = -10974.625407227231
1
Iteration 12400: Loss = -10974.62520163061
Iteration 12500: Loss = -10974.625313518782
1
Iteration 12600: Loss = -10974.625215441958
2
Iteration 12700: Loss = -10974.62524489644
3
Iteration 12800: Loss = -10974.626714838465
4
Iteration 12900: Loss = -10974.625082726996
Iteration 13000: Loss = -10974.632566283315
1
Iteration 13100: Loss = -10974.625063751593
Iteration 13200: Loss = -10974.637938715086
1
Iteration 13300: Loss = -10974.62504535335
Iteration 13400: Loss = -10974.628408167679
1
Iteration 13500: Loss = -10974.624989497906
Iteration 13600: Loss = -10974.651421448518
1
Iteration 13700: Loss = -10974.62495362978
Iteration 13800: Loss = -10974.624943296305
Iteration 13900: Loss = -10974.624968070226
1
Iteration 14000: Loss = -10974.624991180308
2
Iteration 14100: Loss = -10974.661992154826
3
Iteration 14200: Loss = -10974.640844064797
4
Iteration 14300: Loss = -10974.629776578218
5
Iteration 14400: Loss = -10974.67686380002
6
Iteration 14500: Loss = -10974.624941274356
Iteration 14600: Loss = -10974.628787432193
1
Iteration 14700: Loss = -10974.627278643544
2
Iteration 14800: Loss = -10974.624920381852
Iteration 14900: Loss = -10974.627080034621
1
Iteration 15000: Loss = -10974.624938536603
2
Iteration 15100: Loss = -10974.680039375662
3
Iteration 15200: Loss = -10974.624856311015
Iteration 15300: Loss = -10974.642972836642
1
Iteration 15400: Loss = -10974.626069729791
2
Iteration 15500: Loss = -10974.624764245858
Iteration 15600: Loss = -10974.6281746323
1
Iteration 15700: Loss = -10974.660336113513
2
Iteration 15800: Loss = -10974.624846235065
3
Iteration 15900: Loss = -10974.636603873976
4
Iteration 16000: Loss = -10974.640039964417
5
Iteration 16100: Loss = -10974.624927344696
6
Iteration 16200: Loss = -10974.625207789551
7
Iteration 16300: Loss = -10974.632112966865
8
Iteration 16400: Loss = -10974.624724741729
Iteration 16500: Loss = -10974.719398460904
1
Iteration 16600: Loss = -10974.62474439021
2
Iteration 16700: Loss = -10974.702174148035
3
Iteration 16800: Loss = -10974.624766703133
4
Iteration 16900: Loss = -10974.627344293409
5
Iteration 17000: Loss = -10974.625530410653
6
Iteration 17100: Loss = -10974.645051295593
7
Iteration 17200: Loss = -10974.650001331805
8
Iteration 17300: Loss = -10974.62485437495
9
Iteration 17400: Loss = -10974.625282709554
10
Stopping early at iteration 17400 due to no improvement.
tensor([[ 3.5282, -5.1212],
        [ 3.6052, -5.0345],
        [ 3.6208, -5.0257],
        [ 3.3833, -5.2647],
        [ 3.6343, -5.0233],
        [ 3.6268, -5.0137],
        [ 3.6214, -5.0241],
        [ 3.6323, -5.0188],
        [ 3.6286, -5.0172],
        [ 3.4967, -5.1481],
        [ 3.6147, -5.0113],
        [ 3.5320, -5.1120],
        [ 2.8987, -5.7462],
        [ 3.5406, -5.1124],
        [ 3.6033, -5.0521],
        [ 3.0649, -5.5928],
        [ 3.4003, -5.2376],
        [ 3.3140, -5.3319],
        [ 2.5564, -6.0997],
        [ 3.5049, -5.1554],
        [ 3.4920, -5.1711],
        [ 3.6080, -5.0318],
        [ 3.2964, -5.3721],
        [ 3.4301, -5.2102],
        [ 3.1270, -5.5203],
        [ 3.0964, -5.5552],
        [ 3.6343, -5.0245],
        [ 3.5691, -5.0732],
        [ 3.4895, -5.1535],
        [ 3.5794, -5.0698],
        [ 3.6313, -5.0200],
        [ 3.0500, -5.5959],
        [ 3.1803, -5.4812],
        [ 2.2640, -6.3870],
        [ 2.7786, -5.8693],
        [ 3.4588, -5.1929],
        [ 3.5633, -5.0818],
        [ 3.6113, -5.0307],
        [ 3.5462, -5.0897],
        [ 3.5853, -5.0532],
        [ 3.4808, -5.1686],
        [ 3.5275, -5.1006],
        [ 3.6380, -5.0244],
        [ 2.8496, -5.8139],
        [ 3.6235, -5.0275],
        [ 3.6259, -5.0165],
        [ 3.6216, -5.0260],
        [ 3.3557, -5.2925],
        [ 3.0743, -5.5933],
        [ 3.6307, -5.0256],
        [ 3.5383, -5.1115],
        [ 3.3319, -5.3351],
        [ 3.5730, -5.0580],
        [ 3.4804, -5.1742],
        [ 3.6300, -5.0208],
        [ 3.5309, -5.1269],
        [ 3.1509, -5.4901],
        [ 3.2068, -5.4445],
        [ 3.0616, -5.5881],
        [ 3.6281, -5.0525],
        [ 3.6120, -5.0310],
        [ 3.3332, -5.3373],
        [ 3.4949, -5.1419],
        [ 2.8786, -5.7750],
        [ 3.2764, -5.3537],
        [ 3.5136, -5.1391],
        [ 2.0889, -6.5778],
        [ 3.4511, -5.1916],
        [ 3.5987, -5.0316],
        [ 2.9375, -5.7171],
        [ 3.0011, -5.6553],
        [ 3.6088, -5.0221],
        [ 3.6157, -5.0176],
        [ 3.5718, -5.0945],
        [ 3.5777, -5.0683],
        [ 2.1481, -6.5082],
        [ 3.6190, -5.0303],
        [ 3.2708, -5.3756],
        [ 3.4940, -5.1483],
        [ 3.5490, -5.0903],
        [ 3.5854, -5.0593],
        [ 3.3703, -5.2825],
        [ 3.5605, -5.0964],
        [ 3.5828, -5.0578],
        [ 3.5534, -5.0904],
        [ 2.9050, -5.7404],
        [ 3.5419, -5.1109],
        [ 2.9933, -5.6581],
        [ 2.3789, -6.2665],
        [ 3.0206, -5.6229],
        [ 3.2142, -5.4390],
        [ 3.2082, -5.4472],
        [ 3.6231, -5.0235],
        [ 3.6350, -5.0213],
        [ 3.6124, -5.0285],
        [ 3.2856, -5.3726],
        [ 3.0163, -5.6247],
        [ 3.3089, -5.3367],
        [ 2.9596, -5.7013],
        [ 3.6149, -5.0407]], dtype=torch.float64, requires_grad=True)
pi: tensor([[3.7966e-06, 1.0000e+00],
        [1.8376e-02, 9.8162e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9982e-01, 1.7527e-04], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1568, 0.1565],
         [0.8114, 0.1637]],

        [[0.0274, 0.1510],
         [0.0661, 0.8862]],

        [[0.0456, 0.0698],
         [0.0749, 0.4899]],

        [[0.4640, 0.2021],
         [0.6246, 0.4866]],

        [[0.3852, 0.3369],
         [0.8203, 0.5202]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -0.0016965008955278312
Average Adjusted Rand Index: 0.0004951898005778471
Iteration 0: Loss = -22623.71784876289
Iteration 10: Loss = -10981.55558413987
Iteration 20: Loss = -10981.47054922909
Iteration 30: Loss = -10981.451708188568
Iteration 40: Loss = -10981.445548893518
Iteration 50: Loss = -10981.442882607062
Iteration 60: Loss = -10981.44147409826
Iteration 70: Loss = -10981.440546705415
Iteration 80: Loss = -10981.43983303947
Iteration 90: Loss = -10981.439225787417
Iteration 100: Loss = -10981.438645859589
Iteration 110: Loss = -10981.438055599621
Iteration 120: Loss = -10981.43738898259
Iteration 130: Loss = -10981.436695701841
Iteration 140: Loss = -10981.435833777205
Iteration 150: Loss = -10981.434790003
Iteration 160: Loss = -10981.433394001637
Iteration 170: Loss = -10981.43141516917
Iteration 180: Loss = -10981.428438443885
Iteration 190: Loss = -10981.423601612469
Iteration 200: Loss = -10981.415122945931
Iteration 210: Loss = -10981.399770664577
Iteration 220: Loss = -10981.370608623374
Iteration 230: Loss = -10981.317178380667
Iteration 240: Loss = -10981.231238279117
Iteration 250: Loss = -10981.126941559747
Iteration 260: Loss = -10981.043567691706
Iteration 270: Loss = -10981.003809989668
Iteration 280: Loss = -10980.998784817937
Iteration 290: Loss = -10981.011745141843
1
Iteration 300: Loss = -10981.032278539165
2
Iteration 310: Loss = -10981.055183555287
3
Stopping early at iteration 309 due to no improvement.
pi: tensor([[0.5928, 0.4072],
        [0.4657, 0.5343]], dtype=torch.float64)
alpha: tensor([0.5367, 0.4633])
beta: tensor([[[0.1477, 0.1556],
         [0.2640, 0.1775]],

        [[0.1907, 0.1591],
         [0.0796, 0.3256]],

        [[0.4797, 0.1566],
         [0.3660, 0.2807]],

        [[0.4700, 0.1661],
         [0.0210, 0.3411]],

        [[0.3023, 0.1689],
         [0.9222, 0.7568]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 64
Adjusted Rand Index: 0.07054753493148173
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: -0.0021162535833247854
time is 2
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 69
Adjusted Rand Index: 0.1360960505219871
time is 3
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 64
Adjusted Rand Index: 0.06909090909090909
time is 4
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.016536399457399393
Global Adjusted Rand Index: 0.05577673195100744
Average Adjusted Rand Index: 0.0580309280836905
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22623.351395326463
Iteration 100: Loss = -11089.403407461981
Iteration 200: Loss = -11013.944132419214
Iteration 300: Loss = -10985.709968288425
Iteration 400: Loss = -10980.764675900286
Iteration 500: Loss = -10979.842891679975
Iteration 600: Loss = -10979.33191173032
Iteration 700: Loss = -10978.99492218113
Iteration 800: Loss = -10978.755308443775
Iteration 900: Loss = -10978.577737797084
Iteration 1000: Loss = -10978.442407737164
Iteration 1100: Loss = -10978.33701723733
Iteration 1200: Loss = -10978.25340785728
Iteration 1300: Loss = -10978.18602738861
Iteration 1400: Loss = -10978.130902471972
Iteration 1500: Loss = -10978.085356763695
Iteration 1600: Loss = -10978.047186259693
Iteration 1700: Loss = -10978.014924702775
Iteration 1800: Loss = -10977.987417198548
Iteration 1900: Loss = -10977.96375605346
Iteration 2000: Loss = -10977.943260628064
Iteration 2100: Loss = -10977.92537510952
Iteration 2200: Loss = -10977.909742272619
Iteration 2300: Loss = -10977.895883191108
Iteration 2400: Loss = -10977.883613698114
Iteration 2500: Loss = -10977.872735987268
Iteration 2600: Loss = -10977.862988316148
Iteration 2700: Loss = -10977.85423792265
Iteration 2800: Loss = -10977.846370933534
Iteration 2900: Loss = -10977.839211661543
Iteration 3000: Loss = -10977.832810021922
Iteration 3100: Loss = -10977.826908719448
Iteration 3200: Loss = -10977.821612652177
Iteration 3300: Loss = -10977.816712323136
Iteration 3400: Loss = -10977.812284019192
Iteration 3500: Loss = -10977.808164298653
Iteration 3600: Loss = -10977.804416076991
Iteration 3700: Loss = -10977.800931711106
Iteration 3800: Loss = -10977.797772617025
Iteration 3900: Loss = -10977.794791178561
Iteration 4000: Loss = -10977.792062088947
Iteration 4100: Loss = -10977.78957617118
Iteration 4200: Loss = -10977.787235491169
Iteration 4300: Loss = -10977.785031221732
Iteration 4400: Loss = -10977.78301103148
Iteration 4500: Loss = -10977.781130990024
Iteration 4600: Loss = -10977.779352080337
Iteration 4700: Loss = -10977.777724323965
Iteration 4800: Loss = -10977.776199796397
Iteration 4900: Loss = -10977.774776304628
Iteration 5000: Loss = -10977.773410005482
Iteration 5100: Loss = -10977.77218301996
Iteration 5200: Loss = -10977.77094635833
Iteration 5300: Loss = -10977.7698770169
Iteration 5400: Loss = -10977.768858573072
Iteration 5500: Loss = -10977.767899038054
Iteration 5600: Loss = -10977.766973929656
Iteration 5700: Loss = -10977.766142781176
Iteration 5800: Loss = -10977.765297253256
Iteration 5900: Loss = -10977.764550893446
Iteration 6000: Loss = -10977.763850126852
Iteration 6100: Loss = -10977.763200568317
Iteration 6200: Loss = -10977.762530496217
Iteration 6300: Loss = -10977.761923860231
Iteration 6400: Loss = -10977.761378005493
Iteration 6500: Loss = -10977.760869380798
Iteration 6600: Loss = -10977.760350242708
Iteration 6700: Loss = -10977.759858087444
Iteration 6800: Loss = -10977.759429437221
Iteration 6900: Loss = -10977.759006136259
Iteration 7000: Loss = -10977.786640920365
1
Iteration 7100: Loss = -10977.75821336091
Iteration 7200: Loss = -10977.757839347341
Iteration 7300: Loss = -10977.757895054501
1
Iteration 7400: Loss = -10977.757653718885
Iteration 7500: Loss = -10977.756871185435
Iteration 7600: Loss = -10977.756541341474
Iteration 7700: Loss = -10977.852956193523
1
Iteration 7800: Loss = -10977.755932358512
Iteration 7900: Loss = -10977.755717691682
Iteration 8000: Loss = -10977.982855767785
1
Iteration 8100: Loss = -10977.755018917813
Iteration 8200: Loss = -10977.75459764031
Iteration 8300: Loss = -10977.75369708465
Iteration 8400: Loss = -10977.753884499709
1
Iteration 8500: Loss = -10977.748094192671
Iteration 8600: Loss = -10977.746941928972
Iteration 8700: Loss = -10977.74695965337
1
Iteration 8800: Loss = -10977.745988172475
Iteration 8900: Loss = -10977.745379652899
Iteration 9000: Loss = -10977.745149428449
Iteration 9100: Loss = -10977.753598434554
1
Iteration 9200: Loss = -10977.744689420082
Iteration 9300: Loss = -10977.744416231018
Iteration 9400: Loss = -10977.744277140777
Iteration 9500: Loss = -10977.747405235677
1
Iteration 9600: Loss = -10977.743975508003
Iteration 9700: Loss = -10977.743848960508
Iteration 9800: Loss = -10977.763511495092
1
Iteration 9900: Loss = -10977.743584482907
Iteration 10000: Loss = -10977.74347532041
Iteration 10100: Loss = -10977.743393383196
Iteration 10200: Loss = -10977.74390690269
1
Iteration 10300: Loss = -10977.743231087608
Iteration 10400: Loss = -10977.743114975143
Iteration 10500: Loss = -10977.743100650667
Iteration 10600: Loss = -10977.743217343628
1
Iteration 10700: Loss = -10977.742961399694
Iteration 10800: Loss = -10977.742843492484
Iteration 10900: Loss = -10977.934793519753
1
Iteration 11000: Loss = -10977.742739941434
Iteration 11100: Loss = -10977.742684910418
Iteration 11200: Loss = -10977.742669795283
Iteration 11300: Loss = -10977.758123962103
1
Iteration 11400: Loss = -10977.74255605104
Iteration 11500: Loss = -10977.74254650858
Iteration 11600: Loss = -10977.742479815686
Iteration 11700: Loss = -10977.753154372156
1
Iteration 11800: Loss = -10977.742401174946
Iteration 11900: Loss = -10977.74236690979
Iteration 12000: Loss = -10977.742338175025
Iteration 12100: Loss = -10977.752083045654
1
Iteration 12200: Loss = -10977.742282115772
Iteration 12300: Loss = -10977.742252121534
Iteration 12400: Loss = -10977.74225406742
1
Iteration 12500: Loss = -10977.744052360147
2
Iteration 12600: Loss = -10977.742226949229
Iteration 12700: Loss = -10977.742200235232
Iteration 12800: Loss = -10977.742177645514
Iteration 12900: Loss = -10977.753915439174
1
Iteration 13000: Loss = -10977.742136148418
Iteration 13100: Loss = -10977.742119528599
Iteration 13200: Loss = -10977.744592256478
1
Iteration 13300: Loss = -10977.742158277717
2
Iteration 13400: Loss = -10977.742111611251
Iteration 13500: Loss = -10977.813637010708
1
Iteration 13600: Loss = -10977.742089854682
Iteration 13700: Loss = -10977.742246455357
1
Iteration 13800: Loss = -10977.742371144828
2
Iteration 13900: Loss = -10977.742040787072
Iteration 14000: Loss = -10977.762651212772
1
Iteration 14100: Loss = -10977.742025817748
Iteration 14200: Loss = -10977.742012882949
Iteration 14300: Loss = -10977.744806343928
1
Iteration 14400: Loss = -10977.742005114469
Iteration 14500: Loss = -10977.741971620673
Iteration 14600: Loss = -10977.90726394816
1
Iteration 14700: Loss = -10977.741977703621
2
Iteration 14800: Loss = -10977.741975516456
3
Iteration 14900: Loss = -10978.080415464241
4
Iteration 15000: Loss = -10977.741986170695
5
Iteration 15100: Loss = -10977.74196118692
Iteration 15200: Loss = -10977.741973273263
1
Iteration 15300: Loss = -10977.761033362645
2
Iteration 15400: Loss = -10977.741975441508
3
Iteration 15500: Loss = -10977.74196483326
4
Iteration 15600: Loss = -10977.74196798985
5
Iteration 15700: Loss = -10977.749526462301
6
Iteration 15800: Loss = -10977.741977209605
7
Iteration 15900: Loss = -10977.741965045128
8
Iteration 16000: Loss = -10977.741961945767
9
Iteration 16100: Loss = -10977.745823045601
10
Stopping early at iteration 16100 due to no improvement.
tensor([[  7.5480,  -9.4905],
        [  9.0148, -11.3357],
        [  9.0083, -10.7999],
        [  9.2206, -10.6110],
        [  7.9048, -11.2146],
        [  8.6103, -10.1980],
        [  8.2403, -12.8555],
        [  8.1336, -10.0800],
        [  7.6883,  -9.9462],
        [  8.5313, -10.7809],
        [  8.4556,  -9.8694],
        [  6.9149, -11.0484],
        [  7.8879,  -9.5162],
        [  8.5391,  -9.9712],
        [  8.2445, -11.1609],
        [  7.2515, -11.8667],
        [  7.9219,  -9.5635],
        [  8.4695, -10.2874],
        [  8.3012,  -9.7239],
        [  8.4025, -11.5038],
        [  8.4513,  -9.8812],
        [  8.8862, -10.2731],
        [  8.1859,  -9.6357],
        [  8.2329,  -9.6440],
        [  8.0999, -10.1524],
        [  8.3170,  -9.9017],
        [  8.2739, -10.0136],
        [  6.6501, -11.2653],
        [  7.3486, -10.2328],
        [  8.8602, -10.7627],
        [  8.0959,  -9.8634],
        [  8.6934, -10.1131],
        [  8.3684,  -9.7621],
        [  7.8760,  -9.9475],
        [  8.3181,  -9.7322],
        [  8.2093,  -9.6983],
        [  8.2276,  -9.6743],
        [  8.7410, -10.2345],
        [  9.0287, -10.5253],
        [  8.4771, -10.2792],
        [  8.6664, -10.7504],
        [  8.6724, -10.4629],
        [  7.5415,  -9.0619],
        [  7.6845, -10.3685],
        [  8.5181, -11.3048],
        [  8.6994, -10.1957],
        [  8.1989, -11.1933],
        [  8.5893, -10.0877],
        [  8.0694,  -9.4688],
        [  7.8801,  -9.7070],
        [  9.3732, -10.8005],
        [  8.2041,  -9.5975],
        [  8.2863, -10.7702],
        [  8.6403, -10.1155],
        [  8.6503, -11.7044],
        [  8.4588,  -9.9920],
        [  8.1871,  -9.7566],
        [  8.1523,  -9.5639],
        [  8.8256, -10.4143],
        [  8.0810, -10.0096],
        [  8.1003,  -9.4960],
        [  8.5814, -10.1164],
        [  8.1998,  -9.9026],
        [  7.1190, -10.2141],
        [  8.5273, -10.4514],
        [  8.5052, -10.7505],
        [  8.1092,  -9.9163],
        [  8.4165,  -9.8386],
        [  8.1912,  -9.5776],
        [  8.4937,  -9.9173],
        [  8.5059,  -9.9500],
        [  7.3964, -10.5556],
        [  9.0377, -10.7287],
        [  7.7109, -10.3390],
        [  7.0697, -11.6849],
        [  8.2108,  -9.8815],
        [  8.1908, -10.3816],
        [  8.0899,  -9.6508],
        [  8.7527, -10.5531],
        [  8.1262,  -9.6024],
        [  7.1479, -10.2203],
        [  8.4896, -10.8553],
        [  8.0640,  -9.6421],
        [  8.3797, -10.5297],
        [  8.3577,  -9.8748],
        [  7.3395,  -9.8583],
        [  8.1503,  -9.7740],
        [  9.1416, -10.5510],
        [  7.9835,  -9.7909],
        [  7.8590, -10.9188],
        [  8.5638, -10.5784],
        [  8.5389, -10.1435],
        [  7.6842,  -9.8379],
        [  9.0199, -10.4220],
        [  7.2470, -10.6893],
        [  7.8611, -10.8008],
        [  9.2963, -11.0359],
        [  7.5916, -10.0413],
        [  8.3947,  -9.8992],
        [  8.2305,  -9.9912]], dtype=torch.float64, requires_grad=True)
pi: tensor([[4.9041e-08, 1.0000e+00],
        [9.9467e-01, 5.3274e-03]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 2.2778e-08], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1606, 0.1542],
         [0.2640, 0.1637]],

        [[0.1907, 0.1590],
         [0.0796, 0.3256]],

        [[0.4797, 0.0554],
         [0.3660, 0.2807]],

        [[0.4700, 0.2194],
         [0.0210, 0.3411]],

        [[0.3023, 0.3492],
         [0.9222, 0.7568]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -0.0019140907714378748
Average Adjusted Rand Index: 0.00033344555058593435
Iteration 0: Loss = -21900.94251546782
Iteration 10: Loss = -10977.261120954772
Iteration 20: Loss = -10977.135290738355
Iteration 30: Loss = -10975.45670172249
Iteration 40: Loss = -10975.455308117904
Iteration 50: Loss = -10975.455307971748
Iteration 60: Loss = -10975.455311168937
1
Iteration 70: Loss = -10975.45530229786
Iteration 80: Loss = -10975.455307050666
1
Iteration 90: Loss = -10975.455307768865
2
Iteration 100: Loss = -10975.455305480395
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.0212, 0.9788],
        [0.0167, 0.9833]], dtype=torch.float64)
alpha: tensor([0.0170, 0.9830])
beta: tensor([[[0.1141, 0.1383],
         [0.0845, 0.1607]],

        [[0.9689, 0.1511],
         [0.7350, 0.4312]],

        [[0.9570, 0.0691],
         [0.0482, 0.7881]],

        [[0.6915, 0.2055],
         [0.3067, 0.1600]],

        [[0.9524, 0.3392],
         [0.8290, 0.1439]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -0.00028920796095122244
Average Adjusted Rand Index: 0.0004951898005778471
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21900.90648384112
Iteration 100: Loss = -11059.066173667503
Iteration 200: Loss = -11024.956928429381
Iteration 300: Loss = -11003.256226647638
Iteration 400: Loss = -10982.281519892047
Iteration 500: Loss = -10980.030119641644
Iteration 600: Loss = -10979.259246233263
Iteration 700: Loss = -10978.873423205643
Iteration 800: Loss = -10978.632576049402
Iteration 900: Loss = -10978.451024842083
Iteration 1000: Loss = -10978.25419843424
Iteration 1100: Loss = -10977.85046643455
Iteration 1200: Loss = -10977.450042991264
Iteration 1300: Loss = -10977.270253164756
Iteration 1400: Loss = -10977.161803636098
Iteration 1500: Loss = -10977.087505787502
Iteration 1600: Loss = -10977.032785110772
Iteration 1700: Loss = -10976.990281112807
Iteration 1800: Loss = -10976.95572579257
Iteration 1900: Loss = -10976.926530014745
Iteration 2000: Loss = -10976.900355707097
Iteration 2100: Loss = -10976.872250279557
Iteration 2200: Loss = -10976.794792264636
Iteration 2300: Loss = -10976.69867028598
Iteration 2400: Loss = -10976.570060149792
Iteration 2500: Loss = -10976.437231227956
Iteration 2600: Loss = -10976.371939193927
Iteration 2700: Loss = -10976.302728084858
Iteration 2800: Loss = -10976.230800416326
Iteration 2900: Loss = -10976.14271052354
Iteration 3000: Loss = -10976.07206224202
Iteration 3100: Loss = -10975.782586369827
Iteration 3200: Loss = -10975.747665535771
Iteration 3300: Loss = -10975.720912818104
Iteration 3400: Loss = -10975.69142265026
Iteration 3500: Loss = -10975.649082584034
Iteration 3600: Loss = -10975.614187046942
Iteration 3700: Loss = -10975.597167588825
Iteration 3800: Loss = -10975.56324674242
Iteration 3900: Loss = -10975.532111145481
Iteration 4000: Loss = -10975.50229724051
Iteration 4100: Loss = -10975.486798493977
Iteration 4200: Loss = -10975.474825800542
Iteration 4300: Loss = -10975.462950339552
Iteration 4400: Loss = -10975.451774007157
Iteration 4500: Loss = -10975.44077631823
Iteration 4600: Loss = -10975.432489305269
Iteration 4700: Loss = -10975.423909383326
Iteration 4800: Loss = -10975.407838030556
Iteration 4900: Loss = -10975.408427887734
1
Iteration 5000: Loss = -10975.38254825951
Iteration 5100: Loss = -10975.372354399053
Iteration 5200: Loss = -10975.364198301408
Iteration 5300: Loss = -10975.357249087614
Iteration 5400: Loss = -10975.351092203562
Iteration 5500: Loss = -10975.345598636539
Iteration 5600: Loss = -10975.340391584505
Iteration 5700: Loss = -10975.335164883114
Iteration 5800: Loss = -10975.32753128294
Iteration 5900: Loss = -10975.315869087199
Iteration 6000: Loss = -10975.319320050718
1
Iteration 6100: Loss = -10975.294165029323
Iteration 6200: Loss = -10975.2779417029
Iteration 6300: Loss = -10975.248805629073
Iteration 6400: Loss = -10975.169986613058
Iteration 6500: Loss = -10975.054489516926
Iteration 6600: Loss = -10974.942084223427
Iteration 6700: Loss = -10974.871457873081
Iteration 6800: Loss = -10974.868417390388
Iteration 6900: Loss = -10974.860279880582
Iteration 7000: Loss = -10974.861900492486
1
Iteration 7100: Loss = -10974.8623523066
2
Iteration 7200: Loss = -10974.858171889913
Iteration 7300: Loss = -10974.849087025164
Iteration 7400: Loss = -10974.731299935007
Iteration 7500: Loss = -10974.730705656671
Iteration 7600: Loss = -10974.730099995371
Iteration 7700: Loss = -10974.72890001832
Iteration 7800: Loss = -10974.729891561758
1
Iteration 7900: Loss = -10974.672893372257
Iteration 8000: Loss = -10974.622838685891
Iteration 8100: Loss = -10974.608276104569
Iteration 8200: Loss = -10974.597018656506
Iteration 8300: Loss = -10974.465615205907
Iteration 8400: Loss = -10974.445087796597
Iteration 8500: Loss = -10974.4427948109
Iteration 8600: Loss = -10974.44263106887
Iteration 8700: Loss = -10974.441136548528
Iteration 8800: Loss = -10974.44494949929
1
Iteration 8900: Loss = -10974.440677789149
Iteration 9000: Loss = -10974.422746953462
Iteration 9100: Loss = -10974.299882187986
Iteration 9200: Loss = -10974.286697480999
Iteration 9300: Loss = -10974.19625858885
Iteration 9400: Loss = -10974.1896771016
Iteration 9500: Loss = -10974.127852211226
Iteration 9600: Loss = -10974.201402686776
1
Iteration 9700: Loss = -10974.144542920209
2
Iteration 9800: Loss = -10974.123069472194
Iteration 9900: Loss = -10974.122743843569
Iteration 10000: Loss = -10974.158284159505
1
Iteration 10100: Loss = -10974.120419772476
Iteration 10200: Loss = -10974.121918475543
1
Iteration 10300: Loss = -10974.119748427476
Iteration 10400: Loss = -10974.166012642017
1
Iteration 10500: Loss = -10974.119240425112
Iteration 10600: Loss = -10974.120738623531
1
Iteration 10700: Loss = -10974.11859796974
Iteration 10800: Loss = -10974.11711717954
Iteration 10900: Loss = -10974.117064487275
Iteration 11000: Loss = -10974.116825759054
Iteration 11100: Loss = -10974.118589401642
1
Iteration 11200: Loss = -10974.116630183886
Iteration 11300: Loss = -10974.116821671942
1
Iteration 11400: Loss = -10974.116480606537
Iteration 11500: Loss = -10974.116190411192
Iteration 11600: Loss = -10974.116022151846
Iteration 11700: Loss = -10974.116117075057
1
Iteration 11800: Loss = -10974.115939458114
Iteration 11900: Loss = -10974.115851554283
Iteration 12000: Loss = -10974.229715148407
1
Iteration 12100: Loss = -10974.115480589033
Iteration 12200: Loss = -10974.122104841865
1
Iteration 12300: Loss = -10974.11539930221
Iteration 12400: Loss = -10974.139540043163
1
Iteration 12500: Loss = -10974.115203802656
Iteration 12600: Loss = -10974.11514796272
Iteration 12700: Loss = -10974.11568068237
1
Iteration 12800: Loss = -10974.115100345894
Iteration 12900: Loss = -10974.277663341203
1
Iteration 13000: Loss = -10974.115025383418
Iteration 13100: Loss = -10974.114982483059
Iteration 13200: Loss = -10974.115132891708
1
Iteration 13300: Loss = -10974.11490998787
Iteration 13400: Loss = -10974.11973058781
1
Iteration 13500: Loss = -10974.114879965826
Iteration 13600: Loss = -10974.114858769874
Iteration 13700: Loss = -10974.11539123412
1
Iteration 13800: Loss = -10974.114870454614
2
Iteration 13900: Loss = -10974.114791915692
Iteration 14000: Loss = -10974.140701207465
1
Iteration 14100: Loss = -10974.128202961916
2
Iteration 14200: Loss = -10974.276470114466
3
Iteration 14300: Loss = -10974.114754734286
Iteration 14400: Loss = -10974.115286640119
1
Iteration 14500: Loss = -10974.115141567556
2
Iteration 14600: Loss = -10974.114729211846
Iteration 14700: Loss = -10974.114713097673
Iteration 14800: Loss = -10974.121209058112
1
Iteration 14900: Loss = -10974.114528443099
Iteration 15000: Loss = -10974.11451749144
Iteration 15100: Loss = -10974.117076311375
1
Iteration 15200: Loss = -10974.114510113073
Iteration 15300: Loss = -10974.114490245362
Iteration 15400: Loss = -10974.114724566347
1
Iteration 15500: Loss = -10974.114474730859
Iteration 15600: Loss = -10974.592845812644
1
Iteration 15700: Loss = -10974.11446645936
Iteration 15800: Loss = -10974.114463033045
Iteration 15900: Loss = -10974.16164064601
1
Iteration 16000: Loss = -10974.114459168512
Iteration 16100: Loss = -10974.114453960961
Iteration 16200: Loss = -10974.140872387936
1
Iteration 16300: Loss = -10974.114447074093
Iteration 16400: Loss = -10974.115613877446
1
Iteration 16500: Loss = -10974.114997145229
2
Iteration 16600: Loss = -10974.116053150703
3
Iteration 16700: Loss = -10974.115637848505
4
Iteration 16800: Loss = -10974.114393759104
Iteration 16900: Loss = -10974.114422342323
1
Iteration 17000: Loss = -10974.305259109644
2
Iteration 17100: Loss = -10974.113929246614
Iteration 17200: Loss = -10974.115837707672
1
Iteration 17300: Loss = -10974.113968354299
2
Iteration 17400: Loss = -10974.115402228912
3
Iteration 17500: Loss = -10974.113987970977
4
Iteration 17600: Loss = -10974.129821931696
5
Iteration 17700: Loss = -10974.113959959757
6
Iteration 17800: Loss = -10974.131029927252
7
Iteration 17900: Loss = -10974.113974829335
8
Iteration 18000: Loss = -10974.115796428436
9
Iteration 18100: Loss = -10974.113922868411
Iteration 18200: Loss = -10974.1540135361
1
Iteration 18300: Loss = -10974.11387554034
Iteration 18400: Loss = -10974.113896114828
1
Iteration 18500: Loss = -10974.115369606887
2
Iteration 18600: Loss = -10974.113900726456
3
Iteration 18700: Loss = -10974.136217439744
4
Iteration 18800: Loss = -10974.113905336588
5
Iteration 18900: Loss = -10974.114091104975
6
Iteration 19000: Loss = -10974.115295209644
7
Iteration 19100: Loss = -10974.113963198351
8
Iteration 19200: Loss = -10974.11393917843
9
Iteration 19300: Loss = -10974.113938260343
10
Stopping early at iteration 19300 due to no improvement.
tensor([[ 3.5427, -5.7475],
        [ 3.9354, -5.3949],
        [ 3.7596, -5.5359],
        [ 3.9020, -5.3104],
        [ 3.8715, -5.3253],
        [ 3.3084, -6.0764],
        [ 3.9420, -5.4095],
        [ 3.9409, -5.3272],
        [ 3.9764, -5.3637],
        [ 3.8694, -5.6659],
        [ 3.6179, -5.8677],
        [ 3.9399, -5.3797],
        [ 3.1114, -6.2782],
        [ 3.8646, -5.3508],
        [ 3.7957, -5.4404],
        [ 2.9464, -6.1980],
        [ 3.6420, -5.7568],
        [ 3.9242, -5.4267],
        [ 3.4755, -5.7573],
        [ 3.9986, -5.3849],
        [ 3.2220, -5.9157],
        [ 3.9349, -5.4196],
        [ 3.4627, -5.5725],
        [ 3.8996, -5.4402],
        [ 3.8686, -5.4625],
        [ 3.0253, -6.2253],
        [ 3.8468, -5.3409],
        [ 2.7777, -6.7472],
        [ 3.2793, -6.1021],
        [ 3.8757, -5.3812],
        [ 2.7592, -6.5324],
        [ 3.8461, -5.4567],
        [ 3.6844, -5.5067],
        [ 3.5546, -5.6816],
        [ 3.8548, -5.4234],
        [ 3.9775, -5.3648],
        [ 3.8152, -5.5015],
        [ 3.9659, -5.3654],
        [ 4.0248, -5.4194],
        [ 3.4078, -6.0039],
        [ 3.6011, -5.6579],
        [ 3.9655, -5.4518],
        [ 4.4351, -5.8644],
        [ 3.6355, -5.5087],
        [ 3.9372, -5.3238],
        [ 3.9679, -5.4054],
        [ 3.9469, -5.4267],
        [ 3.4261, -5.9051],
        [ 3.6572, -5.5961],
        [ 3.9203, -5.3368],
        [ 3.7660, -5.5072],
        [ 3.6549, -5.6622],
        [ 3.4832, -5.9115],
        [ 3.9251, -5.3198],
        [ 3.5042, -5.7624],
        [ 3.8213, -5.3877],
        [ 3.8281, -5.5595],
        [ 2.9052, -6.3813],
        [ 3.9071, -5.2972],
        [ 2.2808, -6.8960],
        [ 3.9957, -5.3990],
        [ 3.2626, -5.9840],
        [ 3.9129, -5.4297],
        [ 3.8957, -6.0222],
        [ 3.9947, -5.3954],
        [ 3.9079, -5.3519],
        [ 3.8937, -5.3987],
        [ 3.8757, -5.4927],
        [ 4.0120, -5.4079],
        [ 3.9134, -5.3671],
        [ 3.3420, -5.8846],
        [ 3.9748, -5.3731],
        [ 2.9975, -6.3463],
        [ 3.7840, -5.3332],
        [ 3.9675, -5.3570],
        [ 2.8693, -6.3489],
        [ 3.7076, -5.8374],
        [ 3.9336, -5.3816],
        [ 2.8292, -6.5168],
        [ 2.3805, -6.9957],
        [ 3.9885, -5.3784],
        [ 3.8997, -5.3403],
        [ 2.8753, -6.3664],
        [ 2.3690, -6.9842],
        [ 3.8803, -5.4197],
        [ 3.9310, -5.4375],
        [ 3.0822, -6.1814],
        [ 3.9629, -5.3522],
        [ 3.8190, -5.5631],
        [ 3.4388, -5.8587],
        [ 3.3315, -6.2391],
        [ 3.7649, -5.3886],
        [ 3.9627, -5.3748],
        [ 3.4090, -5.7791],
        [ 3.8292, -5.4611],
        [ 3.8005, -5.4375],
        [ 3.1313, -6.2259],
        [ 3.8976, -5.3961],
        [ 2.7856, -6.3862],
        [ 3.8847, -5.2718]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.1720, 0.8280],
        [0.0095, 0.9905]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9991e-01, 9.0486e-05], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1566, 0.1596],
         [0.0845, 0.1647]],

        [[0.9689, 0.1575],
         [0.7350, 0.4312]],

        [[0.9570, 0.0812],
         [0.0482, 0.7881]],

        [[0.6915, 0.2146],
         [0.3067, 0.1600]],

        [[0.9524, 0.3450],
         [0.8290, 0.1439]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: -0.011374456256342739
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -0.0017672282072931302
Average Adjusted Rand Index: -0.0017797014506907005
Iteration 0: Loss = -30102.489804744993
Iteration 10: Loss = -10977.521212998774
Iteration 20: Loss = -10977.138035838643
Iteration 30: Loss = -10975.457087724053
Iteration 40: Loss = -10975.455330651173
Iteration 50: Loss = -10975.455312463544
Iteration 60: Loss = -10975.45529476367
Iteration 70: Loss = -10975.455326631654
1
Iteration 80: Loss = -10975.45529563955
2
Iteration 90: Loss = -10975.455295472255
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.9833, 0.0167],
        [0.9788, 0.0212]], dtype=torch.float64)
alpha: tensor([0.9830, 0.0170])
beta: tensor([[[0.1607, 0.1383],
         [0.9962, 0.1141]],

        [[0.2005, 0.1511],
         [0.1834, 0.3124]],

        [[0.2880, 0.0691],
         [0.6640, 0.5014]],

        [[0.0621, 0.2055],
         [0.2601, 0.9399]],

        [[0.3733, 0.3392],
         [0.7092, 0.5358]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -0.00028920796095122244
Average Adjusted Rand Index: 0.0004951898005778471
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30099.519536332507
Iteration 100: Loss = -11057.845666533332
Iteration 200: Loss = -11022.592896001335
Iteration 300: Loss = -11009.204261817218
Iteration 400: Loss = -10996.863510363684
Iteration 500: Loss = -10988.983453965364
Iteration 600: Loss = -10985.507064422278
Iteration 700: Loss = -10984.735566674559
Iteration 800: Loss = -10984.253721960682
Iteration 900: Loss = -10983.917363754043
Iteration 1000: Loss = -10983.668663429698
Iteration 1100: Loss = -10983.477510330558
Iteration 1200: Loss = -10983.326450318933
Iteration 1300: Loss = -10983.204556378387
Iteration 1400: Loss = -10983.103781356773
Iteration 1500: Loss = -10983.01805009737
Iteration 1600: Loss = -10982.941410368625
Iteration 1700: Loss = -10982.860182941311
Iteration 1800: Loss = -10982.457668775283
Iteration 1900: Loss = -10977.703908430583
Iteration 2000: Loss = -10977.536039377035
Iteration 2100: Loss = -10976.920431325798
Iteration 2200: Loss = -10976.364611443885
Iteration 2300: Loss = -10976.134800290527
Iteration 2400: Loss = -10976.016316681831
Iteration 2500: Loss = -10975.930928417214
Iteration 2600: Loss = -10975.829077338836
Iteration 2700: Loss = -10975.74289506602
Iteration 2800: Loss = -10975.703419549762
Iteration 2900: Loss = -10975.670318073528
Iteration 3000: Loss = -10975.641581354383
Iteration 3100: Loss = -10975.616407315745
Iteration 3200: Loss = -10975.594087817477
Iteration 3300: Loss = -10975.574200204264
Iteration 3400: Loss = -10975.556237096827
Iteration 3500: Loss = -10975.5393336716
Iteration 3600: Loss = -10975.521387228539
Iteration 3700: Loss = -10975.48569768621
Iteration 3800: Loss = -10975.427464022987
Iteration 3900: Loss = -10975.389591766647
Iteration 4000: Loss = -10975.37100347657
Iteration 4100: Loss = -10975.35383559116
Iteration 4200: Loss = -10975.345462441477
Iteration 4300: Loss = -10975.336116161996
Iteration 4400: Loss = -10975.326593932728
Iteration 4500: Loss = -10975.315823833307
Iteration 4600: Loss = -10975.306029772533
Iteration 4700: Loss = -10975.296221878616
Iteration 4800: Loss = -10975.28477539581
Iteration 4900: Loss = -10975.270438251071
Iteration 5000: Loss = -10975.251723032732
Iteration 5100: Loss = -10975.227545687712
Iteration 5200: Loss = -10975.199145266364
Iteration 5300: Loss = -10975.171932810741
Iteration 5400: Loss = -10975.15258535332
Iteration 5500: Loss = -10975.142359669117
Iteration 5600: Loss = -10975.13767606058
Iteration 5700: Loss = -10975.13518568449
Iteration 5800: Loss = -10975.133335853121
Iteration 5900: Loss = -10975.1315918581
Iteration 6000: Loss = -10975.109002742478
Iteration 6100: Loss = -10974.786705558532
Iteration 6200: Loss = -10974.759684406756
Iteration 6300: Loss = -10974.684717565278
Iteration 6400: Loss = -10974.670037725351
Iteration 6500: Loss = -10974.66260035957
Iteration 6600: Loss = -10974.655466223103
Iteration 6700: Loss = -10974.659981786688
1
Iteration 6800: Loss = -10974.648031018922
Iteration 6900: Loss = -10974.64549440943
Iteration 7000: Loss = -10974.643680404139
Iteration 7100: Loss = -10974.641680597722
Iteration 7200: Loss = -10974.6402004229
Iteration 7300: Loss = -10974.640222174496
1
Iteration 7400: Loss = -10974.637821606466
Iteration 7500: Loss = -10974.636832616921
Iteration 7600: Loss = -10974.639176634175
1
Iteration 7700: Loss = -10974.635150190314
Iteration 7800: Loss = -10974.634416884555
Iteration 7900: Loss = -10974.633762614858
Iteration 8000: Loss = -10974.633458832688
Iteration 8100: Loss = -10974.632624219308
Iteration 8200: Loss = -10974.632120393095
Iteration 8300: Loss = -10974.658379162222
1
Iteration 8400: Loss = -10974.631245915214
Iteration 8500: Loss = -10974.63080979987
Iteration 8600: Loss = -10974.633129658163
1
Iteration 8700: Loss = -10974.630119916827
Iteration 8800: Loss = -10974.633818065267
1
Iteration 8900: Loss = -10974.629416018013
Iteration 9000: Loss = -10974.629143892327
Iteration 9100: Loss = -10974.628856429568
Iteration 9200: Loss = -10974.628555933115
Iteration 9300: Loss = -10974.629182102293
1
Iteration 9400: Loss = -10974.626668217366
Iteration 9500: Loss = -10974.626275483744
Iteration 9600: Loss = -10974.626006285416
Iteration 9700: Loss = -10974.625816529724
Iteration 9800: Loss = -10974.625564159434
Iteration 9900: Loss = -10974.625359014792
Iteration 10000: Loss = -10974.625244182314
Iteration 10100: Loss = -10974.62493170062
Iteration 10200: Loss = -10974.624650938425
Iteration 10300: Loss = -10974.626047571837
1
Iteration 10400: Loss = -10974.62402328353
Iteration 10500: Loss = -10974.654096737315
1
Iteration 10600: Loss = -10974.622973313832
Iteration 10700: Loss = -10974.622576556094
Iteration 10800: Loss = -10974.62019679478
Iteration 10900: Loss = -10974.61720664957
Iteration 11000: Loss = -10974.60967213257
Iteration 11100: Loss = -10974.27457190913
Iteration 11200: Loss = -10974.264816409092
Iteration 11300: Loss = -10974.258705669296
Iteration 11400: Loss = -10974.253379162781
Iteration 11500: Loss = -10974.282829224143
1
Iteration 11600: Loss = -10974.251530083977
Iteration 11700: Loss = -10974.446563725716
1
Iteration 11800: Loss = -10974.117483802735
Iteration 11900: Loss = -10974.116950549422
Iteration 12000: Loss = -10974.116680080306
Iteration 12100: Loss = -10974.1162910258
Iteration 12200: Loss = -10974.11622187926
Iteration 12300: Loss = -10974.115945476566
Iteration 12400: Loss = -10974.115726603623
Iteration 12500: Loss = -10974.246303894222
1
Iteration 12600: Loss = -10974.11507046501
Iteration 12700: Loss = -10974.114968605816
Iteration 12800: Loss = -10974.119488884826
1
Iteration 12900: Loss = -10974.115173591086
2
Iteration 13000: Loss = -10974.114741926562
Iteration 13100: Loss = -10974.146778397859
1
Iteration 13200: Loss = -10974.114583421355
Iteration 13300: Loss = -10974.114965851271
1
Iteration 13400: Loss = -10974.115035042876
2
Iteration 13500: Loss = -10974.14895437928
3
Iteration 13600: Loss = -10974.11439248655
Iteration 13700: Loss = -10974.118080314534
1
Iteration 13800: Loss = -10974.114363999695
Iteration 13900: Loss = -10974.156191906497
1
Iteration 14000: Loss = -10974.11429623497
Iteration 14100: Loss = -10974.11425124794
Iteration 14200: Loss = -10974.122796709324
1
Iteration 14300: Loss = -10974.114269013155
2
Iteration 14400: Loss = -10974.114189749751
Iteration 14500: Loss = -10974.114213209385
1
Iteration 14600: Loss = -10974.128052913848
2
Iteration 14700: Loss = -10974.114156920854
Iteration 14800: Loss = -10974.130189690917
1
Iteration 14900: Loss = -10974.114095679917
Iteration 15000: Loss = -10974.115550460949
1
Iteration 15100: Loss = -10974.114050989909
Iteration 15200: Loss = -10974.115071344591
1
Iteration 15300: Loss = -10974.114172663974
2
Iteration 15400: Loss = -10974.11402993765
Iteration 15500: Loss = -10974.116815850715
1
Iteration 15600: Loss = -10974.114029547198
Iteration 15700: Loss = -10974.114177029604
1
Iteration 15800: Loss = -10974.114044073782
2
Iteration 15900: Loss = -10974.11400367253
Iteration 16000: Loss = -10974.14696504362
1
Iteration 16100: Loss = -10974.113940628667
Iteration 16200: Loss = -10974.113982367213
1
Iteration 16300: Loss = -10974.114893824053
2
Iteration 16400: Loss = -10974.11397604021
3
Iteration 16500: Loss = -10974.113964271528
4
Iteration 16600: Loss = -10974.114176659781
5
Iteration 16700: Loss = -10974.113951133659
6
Iteration 16800: Loss = -10974.115424109215
7
Iteration 16900: Loss = -10974.113981929382
8
Iteration 17000: Loss = -10974.113932961172
Iteration 17100: Loss = -10974.117315690712
1
Iteration 17200: Loss = -10974.1139235909
Iteration 17300: Loss = -10974.114183039457
1
Iteration 17400: Loss = -10974.116485534025
2
Iteration 17500: Loss = -10974.11401537198
3
Iteration 17600: Loss = -10974.114397159574
4
Iteration 17700: Loss = -10974.115399756238
5
Iteration 17800: Loss = -10974.170409582126
6
Iteration 17900: Loss = -10974.11394958628
7
Iteration 18000: Loss = -10974.120822664772
8
Iteration 18100: Loss = -10974.140793053639
9
Iteration 18200: Loss = -10974.113936997619
10
Stopping early at iteration 18200 due to no improvement.
tensor([[-5.1697,  3.5868],
        [-5.2984,  3.4874],
        [-5.0735,  3.6870],
        [-5.4662,  3.2126],
        [-5.0292,  3.6333],
        [-6.7341,  2.1189],
        [-5.4863,  3.3315],
        [-5.2208,  3.5146],
        [-5.0966,  3.7091],
        [-5.2818,  3.7200],
        [-5.2666,  3.6854],
        [-5.9213,  2.8650],
        [-5.2742,  3.5830],
        [-5.0382,  3.6426],
        [-5.1603,  3.5414],
        [-4.9982,  3.6113],
        [-5.3531,  3.5130],
        [-5.8272,  2.9907],
        [-5.1574,  3.5420],
        [-5.5949,  3.2559],
        [-5.0107,  3.5926],
        [-5.3527,  3.4681],
        [-4.9598,  3.5402],
        [-5.0983,  3.7078],
        [-6.6717,  2.1266],
        [-5.0596,  3.6557],
        [-5.0236,  3.6372],
        [-6.0304,  2.9622],
        [-5.1966,  3.6519],
        [-5.1662,  3.5566],
        [-6.0710,  2.6870],
        [-5.0827,  3.6861],
        [-5.8540,  2.8036],
        [-5.0519,  3.6499],
        [-5.2198,  3.5244],
        [-5.0970,  3.7095],
        [-5.2477,  3.5357],
        [-5.1245,  3.6723],
        [-5.8271,  3.0845],
        [-5.6914,  3.1870],
        [-5.0608,  3.6634],
        [-5.1349,  3.7486],
        [-5.5798,  4.1859],
        [-5.5733,  3.0371],
        [-5.5265,  3.2013],
        [-5.2886,  3.5505],
        [-5.5310,  3.3089],
        [-6.7065,  2.0913],
        [-6.2204,  2.4997],
        [-6.6694,  2.0542],
        [-5.1982,  3.5416],
        [-6.2039,  2.5805],
        [-5.5764,  3.2861],
        [-5.5591,  3.1528],
        [-5.5809,  3.1519],
        [-5.6642,  3.0102],
        [-5.2031,  3.6503],
        [-5.4145,  3.3391],
        [-5.2556,  3.4144],
        [-5.1158,  3.5264],
        [-5.2090,  3.6516],
        [-6.2586,  2.4554],
        [-5.5377,  3.2714],
        [-5.4839,  3.9008],
        [-5.1696,  3.6864],
        [-5.3620,  3.3641],
        [-6.4316,  2.3282],
        [-5.2125,  3.6165],
        [-5.3870,  3.4991],
        [-5.8856,  2.8620],
        [-5.0852,  3.6081],
        [-5.5739,  3.2408],
        [-5.1254,  3.6826],
        [-5.0679,  3.5155],
        [-5.1222,  3.6686],
        [-5.1311,  3.5525],
        [-5.7425,  3.2688],
        [-5.7275,  3.0546],
        [-6.0926,  2.7199],
        [-5.1159,  3.7256],
        [-5.2987,  3.5348],
        [-5.0462,  3.6590],
        [-5.0477,  3.6588],
        [-5.3405,  3.4788],
        [-5.4156,  3.3517],
        [-6.7256,  2.1103],
        [-5.2824,  3.4476],
        [-5.1258,  3.6548],
        [-5.5031,  3.3441],
        [-5.1042,  3.6581],
        [-5.2256,  3.8122],
        [-5.1010,  3.5187],
        [-5.1110,  3.6924],
        [-5.4766,  3.1770],
        [-5.0930,  3.6624],
        [-5.2428,  3.4613],
        [-6.7193,  2.1041],
        [-6.6876,  2.0724],
        [-5.0508,  3.5868],
        [-5.0743,  3.5481]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9905, 0.0095],
        [0.8278, 0.1722]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.5429e-04, 9.9985e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1647, 0.1596],
         [0.9962, 0.1566]],

        [[0.2005, 0.1575],
         [0.1834, 0.3124]],

        [[0.2880, 0.0813],
         [0.6640, 0.5014]],

        [[0.0621, 0.2145],
         [0.2601, 0.9399]],

        [[0.3733, 0.3450],
         [0.7092, 0.5358]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: -0.011374456256342739
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -0.0017672282072931302
Average Adjusted Rand Index: -0.0017797014506907005
10873.378638192185
new:  [-0.0016965008955278312, -0.0019140907714378748, -0.0017672282072931302, -0.0017672282072931302] [0.0004951898005778471, 0.00033344555058593435, -0.0017797014506907005, -0.0017797014506907005] [10974.625282709554, 10977.745823045601, 10974.113938260343, 10974.113936997619]
prior:  [-0.00028920796095122244, 0.05577673195100744, -0.00028920796095122244, -0.00028920796095122244] [0.0004951898005778471, 0.0580309280836905, 0.0004951898005778471, 0.0004951898005778471] [10975.455333429078, 10981.055183555287, 10975.455305480395, 10975.455295472255]
-----------------------------------------------------------------------------------------
This iteration is 13
True Objective function: Loss = -10856.57399173985
Iteration 0: Loss = -22816.869703033088
Iteration 10: Loss = -10972.542934864066
Iteration 20: Loss = -10972.384875750327
Iteration 30: Loss = -10972.415033608224
1
Iteration 40: Loss = -10972.4636427672
2
Iteration 50: Loss = -10972.513061477548
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.6081, 0.3919],
        [0.6107, 0.3893]], dtype=torch.float64)
alpha: tensor([0.6048, 0.3952])
beta: tensor([[[1.4330e-01, 1.7097e-01],
         [3.3919e-01, 1.8850e-01]],

        [[3.0150e-01, 1.6421e-01],
         [2.0005e-01, 1.0543e-01]],

        [[8.4612e-01, 1.5789e-01],
         [9.1275e-01, 5.1204e-01]],

        [[3.1835e-02, 1.6230e-01],
         [1.6412e-01, 4.0138e-01]],

        [[2.0816e-01, 1.6851e-01],
         [7.5556e-01, 8.1095e-04]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.00632145364104389
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.01575757575757576
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.01420863309352518
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.012196794415221883
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 62
Adjusted Rand Index: 0.04982801546516484
Global Adjusted Rand Index: 0.018024339744845644
Average Adjusted Rand Index: 0.014783776708417556
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22775.669134816435
Iteration 100: Loss = -10981.81030696763
Iteration 200: Loss = -10977.241633919402
Iteration 300: Loss = -10975.853879419708
Iteration 400: Loss = -10975.168425634643
Iteration 500: Loss = -10974.741485149592
Iteration 600: Loss = -10974.51694408598
Iteration 700: Loss = -10974.422188431225
Iteration 800: Loss = -10974.367311053804
Iteration 900: Loss = -10974.326079716771
Iteration 1000: Loss = -10974.292517246502
Iteration 1100: Loss = -10974.262917442302
Iteration 1200: Loss = -10974.234681546171
Iteration 1300: Loss = -10974.206396621319
Iteration 1400: Loss = -10974.176823474261
Iteration 1500: Loss = -10974.144390419964
Iteration 1600: Loss = -10974.105832183284
Iteration 1700: Loss = -10974.05610188069
Iteration 1800: Loss = -10973.989634627695
Iteration 1900: Loss = -10973.906475510314
Iteration 2000: Loss = -10973.812428969533
Iteration 2100: Loss = -10973.700669943968
Iteration 2200: Loss = -10973.540278517738
Iteration 2300: Loss = -10973.055172585007
Iteration 2400: Loss = -10972.15111949772
Iteration 2500: Loss = -10971.927845414686
Iteration 2600: Loss = -10971.850741916674
Iteration 2700: Loss = -10971.815268602488
Iteration 2800: Loss = -10971.796096123242
Iteration 2900: Loss = -10971.783844318004
Iteration 3000: Loss = -10971.775105761284
Iteration 3100: Loss = -10971.768332788548
Iteration 3200: Loss = -10971.76275358603
Iteration 3300: Loss = -10971.757964552167
Iteration 3400: Loss = -10971.753876594184
Iteration 3500: Loss = -10971.750257703789
Iteration 3600: Loss = -10971.747073173583
Iteration 3700: Loss = -10971.744219054162
Iteration 3800: Loss = -10971.741713806994
Iteration 3900: Loss = -10971.739528376698
Iteration 4000: Loss = -10971.737577915592
Iteration 4100: Loss = -10971.735766474363
Iteration 4200: Loss = -10971.734205446983
Iteration 4300: Loss = -10971.732708054997
Iteration 4400: Loss = -10971.731434259342
Iteration 4500: Loss = -10971.730268832958
Iteration 4600: Loss = -10971.729099931454
Iteration 4700: Loss = -10971.728134731993
Iteration 4800: Loss = -10971.727219245577
Iteration 4900: Loss = -10971.72637409388
Iteration 5000: Loss = -10971.725612759097
Iteration 5100: Loss = -10971.72487315673
Iteration 5200: Loss = -10971.724261131252
Iteration 5300: Loss = -10971.723603230568
Iteration 5400: Loss = -10971.723017179756
Iteration 5500: Loss = -10971.722451295964
Iteration 5600: Loss = -10971.721980303353
Iteration 5700: Loss = -10971.72148856275
Iteration 5800: Loss = -10971.721233197743
Iteration 5900: Loss = -10971.72379252473
1
Iteration 6000: Loss = -10971.726949286965
2
Iteration 6100: Loss = -10971.719932324035
Iteration 6200: Loss = -10971.719562100128
Iteration 6300: Loss = -10971.719239917815
Iteration 6400: Loss = -10971.719135993944
Iteration 6500: Loss = -10971.718636953643
Iteration 6600: Loss = -10971.719364359182
1
Iteration 6700: Loss = -10971.718195518906
Iteration 6800: Loss = -10971.717940024013
Iteration 6900: Loss = -10971.71774392743
Iteration 7000: Loss = -10971.717532888342
Iteration 7100: Loss = -10971.717357992437
Iteration 7200: Loss = -10971.718690900643
1
Iteration 7300: Loss = -10971.717001058778
Iteration 7400: Loss = -10971.716844667762
Iteration 7500: Loss = -10971.845889013399
1
Iteration 7600: Loss = -10971.71654167007
Iteration 7700: Loss = -10971.716434220354
Iteration 7800: Loss = -10971.716370420285
Iteration 7900: Loss = -10971.716212728481
Iteration 8000: Loss = -10971.716084175103
Iteration 8100: Loss = -10971.715944052004
Iteration 8200: Loss = -10971.744387364151
1
Iteration 8300: Loss = -10971.715768909376
Iteration 8400: Loss = -10971.715674003512
Iteration 8500: Loss = -10971.949180022371
1
Iteration 8600: Loss = -10971.715499928188
Iteration 8700: Loss = -10971.715421844407
Iteration 8800: Loss = -10971.715459824443
1
Iteration 8900: Loss = -10971.715295207334
Iteration 9000: Loss = -10971.715208260208
Iteration 9100: Loss = -10971.715166908027
Iteration 9200: Loss = -10971.716553399867
1
Iteration 9300: Loss = -10971.715017701808
Iteration 9400: Loss = -10971.714983132493
Iteration 9500: Loss = -10971.753836774104
1
Iteration 9600: Loss = -10971.71487957636
Iteration 9700: Loss = -10971.714818918934
Iteration 9800: Loss = -10971.787494123837
1
Iteration 9900: Loss = -10971.71473606398
Iteration 10000: Loss = -10971.714890906247
1
Iteration 10100: Loss = -10971.714628015487
Iteration 10200: Loss = -10971.77658453374
1
Iteration 10300: Loss = -10971.714561273144
Iteration 10400: Loss = -10971.734215019524
1
Iteration 10500: Loss = -10971.714497429715
Iteration 10600: Loss = -10971.715643253749
1
Iteration 10700: Loss = -10971.714637295208
2
Iteration 10800: Loss = -10971.716174753123
3
Iteration 10900: Loss = -10971.714412909547
Iteration 11000: Loss = -10971.797945176108
1
Iteration 11100: Loss = -10971.714355972925
Iteration 11200: Loss = -10971.714322726952
Iteration 11300: Loss = -10971.718690636417
1
Iteration 11400: Loss = -10971.71429706219
Iteration 11500: Loss = -10971.714265797284
Iteration 11600: Loss = -10971.716853925851
1
Iteration 11700: Loss = -10971.714224121924
Iteration 11800: Loss = -10971.714315255707
1
Iteration 11900: Loss = -10971.71424378004
2
Iteration 12000: Loss = -10971.71421627352
Iteration 12100: Loss = -10971.714150219264
Iteration 12200: Loss = -10971.733765020434
1
Iteration 12300: Loss = -10971.714176125055
2
Iteration 12400: Loss = -10971.714181238502
3
Iteration 12500: Loss = -10971.714316084814
4
Iteration 12600: Loss = -10971.874379998464
5
Iteration 12700: Loss = -10971.714088627381
Iteration 12800: Loss = -10971.714528109938
1
Iteration 12900: Loss = -10971.714088842335
2
Iteration 13000: Loss = -10971.714094079312
3
Iteration 13100: Loss = -10971.714076653707
Iteration 13200: Loss = -10971.714072888184
Iteration 13300: Loss = -10971.714044274133
Iteration 13400: Loss = -10971.714203664751
1
Iteration 13500: Loss = -10971.724121632018
2
Iteration 13600: Loss = -10971.714032800304
Iteration 13700: Loss = -10971.714261122814
1
Iteration 13800: Loss = -10971.71398511292
Iteration 13900: Loss = -10971.714155644055
1
Iteration 14000: Loss = -10971.713984046963
Iteration 14100: Loss = -10971.714577377734
1
Iteration 14200: Loss = -10971.73083804568
2
Iteration 14300: Loss = -10971.713947723054
Iteration 14400: Loss = -10971.814885899808
1
Iteration 14500: Loss = -10971.713989368278
2
Iteration 14600: Loss = -10971.745865344887
3
Iteration 14700: Loss = -10971.713959372744
4
Iteration 14800: Loss = -10971.716253091936
5
Iteration 14900: Loss = -10971.713977448075
6
Iteration 15000: Loss = -10971.714003688972
7
Iteration 15100: Loss = -10971.715845022803
8
Iteration 15200: Loss = -10971.713951928328
9
Iteration 15300: Loss = -10971.804610050616
10
Stopping early at iteration 15300 due to no improvement.
tensor([[-3.6466, -0.9686],
        [-3.9390, -0.6762],
        [-3.3336, -1.2816],
        [-3.5990, -1.0162],
        [-4.0004, -0.6148],
        [-4.0264, -0.5888],
        [-3.7124, -0.9028],
        [-0.9242, -3.6911],
        [-3.3007, -1.3145],
        [-3.7429, -0.8723],
        [-3.9167, -0.6985],
        [-4.2113, -0.4039],
        [-3.6583, -0.9569],
        [-3.4888, -1.1264],
        [-4.0544, -0.5609],
        [-3.5545, -1.0607],
        [-3.9326, -0.6826],
        [-3.7263, -0.8889],
        [-3.3291, -1.2861],
        [-3.5827, -1.0325],
        [-3.6092, -1.0060],
        [-3.6328, -0.9824],
        [-4.0062, -0.6091],
        [-3.1442, -1.4711],
        [-3.6270, -0.9882],
        [-3.2164, -1.3988],
        [-3.3519, -1.2633],
        [-3.6999, -0.9153],
        [-3.9658, -0.6494],
        [-3.5797, -1.0356],
        [-4.0665, -0.5487],
        [-3.5182, -1.0970],
        [-3.8691, -0.7461],
        [-4.1826, -0.4326],
        [-3.5560, -1.0593],
        [-3.2243, -1.3910],
        [-3.7055, -0.9097],
        [-3.7345, -0.8807],
        [-3.9498, -0.6655],
        [-4.2795, -0.3357],
        [-4.2930, -0.3222],
        [-3.4454, -1.1698],
        [-3.1830, -1.4322],
        [-3.7367, -0.8785],
        [-2.8172, -1.7980],
        [-2.8348, -1.7804],
        [-4.0227, -0.5926],
        [-3.4756, -1.1396],
        [-3.6471, -0.9681],
        [-3.8237, -0.7915],
        [-3.9778, -0.6375],
        [-3.6561, -0.9591],
        [-4.0914, -0.5238],
        [-4.5094, -0.1058],
        [-4.5408, -0.0744],
        [-4.0547, -0.5606],
        [-4.0405, -0.5747],
        [-3.8212, -0.7940],
        [-4.0644, -0.5508],
        [-3.9093, -0.7059],
        [-4.0734, -0.5418],
        [-4.1019, -0.5133],
        [-3.1000, -1.5153],
        [-4.2882, -0.3270],
        [-3.9369, -0.6783],
        [-3.8102, -0.8051],
        [-3.5207, -1.0945],
        [-3.4707, -1.1445],
        [-4.1055, -0.5098],
        [-3.4556, -1.1596],
        [-3.9154, -0.6998],
        [-3.7582, -0.8570],
        [-3.4615, -1.1537],
        [-3.5234, -1.0918],
        [-3.7431, -0.8722],
        [-3.9936, -0.6216],
        [-3.3813, -1.2339],
        [-3.7937, -0.8215],
        [-4.0669, -0.5483],
        [-3.7411, -0.8741],
        [-3.0583, -1.5569],
        [-3.8488, -0.7664],
        [-3.5390, -1.0763],
        [-3.9897, -0.6255],
        [-3.4916, -1.1236],
        [-3.5867, -1.0285],
        [-2.9376, -1.6776],
        [-4.3550, -0.2602],
        [-4.1876, -0.4276],
        [-4.2543, -0.3610],
        [-3.9479, -0.6673],
        [-3.8267, -0.7885],
        [-3.1766, -1.4386],
        [-3.6456, -0.9697],
        [-2.9975, -1.6177],
        [-3.6572, -0.9580],
        [-3.8495, -0.7657],
        [-3.6591, -0.9561],
        [-3.5594, -1.0558],
        [-2.9307, -1.6846]], dtype=torch.float64, requires_grad=True)
pi: tensor([[2.9844e-01, 7.0156e-01],
        [2.5537e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0751, 0.9249], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.9681e-01, 1.8817e-01],
         [3.3919e-01, 1.6357e-01]],

        [[3.0150e-01, 7.5730e-02],
         [2.0005e-01, 1.0543e-01]],

        [[8.4612e-01, 1.0147e-01],
         [9.1275e-01, 5.1204e-01]],

        [[3.1835e-02, 1.6806e-01],
         [1.6412e-01, 4.0138e-01]],

        [[2.0816e-01, 6.2731e-01],
         [7.5556e-01, 8.1095e-04]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0014318072775748106
Average Adjusted Rand Index: -0.001892058443367131
Iteration 0: Loss = -21785.85091147242
Iteration 10: Loss = -10972.694422564304
Iteration 20: Loss = -10972.689815644588
Iteration 30: Loss = -10972.732695884875
1
Iteration 40: Loss = -10972.777402086342
2
Iteration 50: Loss = -10972.82167959702
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7447, 0.2553],
        [0.7609, 0.2391]], dtype=torch.float64)
alpha: tensor([0.7452, 0.2548])
beta: tensor([[[0.1493, 0.1771],
         [0.6926, 0.1950]],

        [[0.5176, 0.1707],
         [0.9577, 0.1746]],

        [[0.6134, 0.1630],
         [0.9530, 0.8901]],

        [[0.3593, 0.1677],
         [0.2539, 0.1651]],

        [[0.0026, 0.1774],
         [0.5834, 0.4825]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0035265748433768707
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.005453571602473584
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.009009494263563287
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.004682108332720131
Global Adjusted Rand Index: 0.00032718194871431865
Average Adjusted Rand Index: -0.0030801938947100684
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21785.803031379055
Iteration 100: Loss = -11043.102714428385
Iteration 200: Loss = -10992.502479736058
Iteration 300: Loss = -10976.910365575864
Iteration 400: Loss = -10975.459349925635
Iteration 500: Loss = -10974.668387852449
Iteration 600: Loss = -10974.244858444876
Iteration 700: Loss = -10973.945461977992
Iteration 800: Loss = -10973.33139947075
Iteration 900: Loss = -10972.43584527362
Iteration 1000: Loss = -10972.196000860182
Iteration 1100: Loss = -10972.065111140508
Iteration 1200: Loss = -10971.973060796092
Iteration 1300: Loss = -10971.899054345313
Iteration 1400: Loss = -10971.837111558089
Iteration 1500: Loss = -10971.78658399904
Iteration 1600: Loss = -10971.748793869838
Iteration 1700: Loss = -10971.722827652136
Iteration 1800: Loss = -10971.70478790409
Iteration 1900: Loss = -10971.690206577772
Iteration 2000: Loss = -10971.675366943042
Iteration 2100: Loss = -10971.659608959408
Iteration 2200: Loss = -10971.641837947644
Iteration 2300: Loss = -10971.623041193234
Iteration 2400: Loss = -10971.605714091365
Iteration 2500: Loss = -10971.587867375221
Iteration 2600: Loss = -10971.566924848263
Iteration 2700: Loss = -10971.542607706522
Iteration 2800: Loss = -10971.512942775198
Iteration 2900: Loss = -10971.475443379502
Iteration 3000: Loss = -10971.429525206117
Iteration 3100: Loss = -10971.38510216365
Iteration 3200: Loss = -10971.349780475954
Iteration 3300: Loss = -10971.317779087816
Iteration 3400: Loss = -10971.287346913774
Iteration 3500: Loss = -10971.25920293533
Iteration 3600: Loss = -10971.200651332467
Iteration 3700: Loss = -10971.045920900895
Iteration 3800: Loss = -10970.90421368854
Iteration 3900: Loss = -10970.742651782808
Iteration 4000: Loss = -10970.678473543578
Iteration 4100: Loss = -10970.65168100693
Iteration 4200: Loss = -10970.632639889483
Iteration 4300: Loss = -10970.61667883121
Iteration 4400: Loss = -10970.60203471659
Iteration 4500: Loss = -10970.580235927064
Iteration 4600: Loss = -10970.569444051842
Iteration 4700: Loss = -10970.562266837103
Iteration 4800: Loss = -10970.556611009157
Iteration 4900: Loss = -10970.55217040205
Iteration 5000: Loss = -10970.548844215078
Iteration 5100: Loss = -10970.583798569705
1
Iteration 5200: Loss = -10970.544447320915
Iteration 5300: Loss = -10970.542781004307
Iteration 5400: Loss = -10970.549415577672
1
Iteration 5500: Loss = -10970.540413706605
Iteration 5600: Loss = -10970.5395948305
Iteration 5700: Loss = -10970.538907271262
Iteration 5800: Loss = -10970.541619186453
1
Iteration 5900: Loss = -10970.537720713475
Iteration 6000: Loss = -10970.537208474634
Iteration 6100: Loss = -10970.564318023387
1
Iteration 6200: Loss = -10970.536386984382
Iteration 6300: Loss = -10970.536006780298
Iteration 6400: Loss = -10970.535658999062
Iteration 6500: Loss = -10970.535232919796
Iteration 6600: Loss = -10970.534676629568
Iteration 6700: Loss = -10970.534354347663
Iteration 6800: Loss = -10970.535511357713
1
Iteration 6900: Loss = -10970.53405381474
Iteration 7000: Loss = -10970.533940095966
Iteration 7100: Loss = -10970.577070630485
1
Iteration 7200: Loss = -10970.53362755801
Iteration 7300: Loss = -10970.533114322885
Iteration 7400: Loss = -10970.528317623945
Iteration 7500: Loss = -10970.528206594781
Iteration 7600: Loss = -10970.530742199946
1
Iteration 7700: Loss = -10970.527981890786
Iteration 7800: Loss = -10970.533099428867
1
Iteration 7900: Loss = -10970.52761622727
Iteration 8000: Loss = -10970.527199619997
Iteration 8100: Loss = -10970.564403728944
1
Iteration 8200: Loss = -10970.526859646356
Iteration 8300: Loss = -10970.526802864126
Iteration 8400: Loss = -10970.784850122858
1
Iteration 8500: Loss = -10970.526744537694
Iteration 8600: Loss = -10970.526699401113
Iteration 8700: Loss = -10970.529212281963
1
Iteration 8800: Loss = -10970.52663048627
Iteration 8900: Loss = -10970.52661652392
Iteration 9000: Loss = -10970.527860022106
1
Iteration 9100: Loss = -10970.526598444245
Iteration 9200: Loss = -10970.526640600981
1
Iteration 9300: Loss = -10970.612422569653
2
Iteration 9400: Loss = -10970.526637512958
3
Iteration 9500: Loss = -10970.53139758735
4
Iteration 9600: Loss = -10970.572306955899
5
Iteration 9700: Loss = -10970.621827724593
6
Iteration 9800: Loss = -10970.502276403879
Iteration 9900: Loss = -10970.505047054996
1
Iteration 10000: Loss = -10970.502239779396
Iteration 10100: Loss = -10970.51925096934
1
Iteration 10200: Loss = -10970.50216044394
Iteration 10300: Loss = -10970.502102205148
Iteration 10400: Loss = -10970.502060579138
Iteration 10500: Loss = -10970.501960175137
Iteration 10600: Loss = -10970.502215824621
1
Iteration 10700: Loss = -10970.501881175833
Iteration 10800: Loss = -10970.532563700584
1
Iteration 10900: Loss = -10970.502136099843
2
Iteration 11000: Loss = -10970.502707421652
3
Iteration 11100: Loss = -10970.501910028937
4
Iteration 11200: Loss = -10970.50189516918
5
Iteration 11300: Loss = -10970.503905912348
6
Iteration 11400: Loss = -10970.548494537954
7
Iteration 11500: Loss = -10970.53131570705
8
Iteration 11600: Loss = -10970.515329333557
9
Iteration 11700: Loss = -10970.644787654168
10
Stopping early at iteration 11700 due to no improvement.
tensor([[-3.4099,  2.0100],
        [-4.3316,  1.1003],
        [-3.3940,  2.0042],
        [-3.9293,  1.4900],
        [-4.1698,  1.2689],
        [-3.4763,  1.9644],
        [-3.7462,  1.6747],
        [-3.2813,  1.8905],
        [-4.1927,  1.2076],
        [-3.6263,  1.7978],
        [-3.4217,  2.0127],
        [-3.4240,  2.0276],
        [-3.5193,  1.9010],
        [-3.4256,  1.9834],
        [-3.4167,  2.0217],
        [-3.5199,  1.8951],
        [-3.6113,  1.8240],
        [-3.4122,  2.0127],
        [-3.5005,  1.9060],
        [-3.4158,  1.9983],
        [-3.4088,  2.0134],
        [-3.9109,  1.5089],
        [-3.6019,  1.8356],
        [-3.9084,  1.4808],
        [-4.1809,  1.2389],
        [-3.4287,  1.9655],
        [-3.9045,  1.4992],
        [-3.4030,  2.0164],
        [-3.4227,  2.0121],
        [-3.4411,  1.9732],
        [-3.4209,  2.0191],
        [-3.4766,  1.9335],
        [-3.6747,  1.7676],
        [-3.6535,  1.7971],
        [-3.4171,  1.9977],
        [-3.4809,  1.9116],
        [-4.0376,  1.3829],
        [-3.4084,  2.0167],
        [-3.4722,  1.9608],
        [-3.6276,  1.8275],
        [-4.4383,  1.0151],
        [-3.9840,  1.4208],
        [-3.3939,  2.0026],
        [-3.4382,  1.9863],
        [-3.4551,  1.9179],
        [-3.4116,  1.9524],
        [-3.4184,  2.0219],
        [-3.4001,  2.0100],
        [-3.4856,  1.9327],
        [-3.5665,  1.8635],
        [-5.0243,  0.4091],
        [-3.4075,  2.0129],
        [-3.4172,  2.0286],
        [-3.4317,  2.0414],
        [-3.6904,  1.7809],
        [-3.5239,  1.9159],
        [-3.6222,  1.8172],
        [-3.4198,  2.0103],
        [-3.6013,  1.8418],
        [-3.4314,  2.0039],
        [-3.5744,  1.8647],
        [-3.5650,  1.8807],
        [-3.4174,  1.9954],
        [-4.7343,  0.7190],
        [-3.6244,  1.8088],
        [-3.6319,  1.7982],
        [-3.4092,  2.0079],
        [-3.5740,  1.8361],
        [-3.4252,  2.0205],
        [-5.0135,  0.3983],
        [-3.4232,  2.0085],
        [-3.4139,  2.0110],
        [-3.4222,  1.9861],
        [-3.4402,  1.9819],
        [-3.4471,  1.9779],
        [-3.4103,  2.0240],
        [-3.5219,  1.8814],
        [-3.5081,  1.9171],
        [-3.5894,  1.8532],
        [-3.4799,  1.9448],
        [-3.4401,  1.9625],
        [-3.5232,  1.9064],
        [-3.4933,  1.9183],
        [-3.5883,  1.8463],
        [-3.3993,  2.0091],
        [-5.0150,  0.3998],
        [-3.3802,  1.8982],
        [-3.4290,  2.0341],
        [-3.5825,  1.8696],
        [-3.7133,  1.7433],
        [-3.4453,  1.9942],
        [-3.5962,  1.8340],
        [-4.7704,  0.6188],
        [-3.9444,  1.4805],
        [-3.4965,  1.8906],
        [-3.8075,  1.6120],
        [-3.4688,  1.9610],
        [-3.5030,  1.9172],
        [-3.5268,  1.8882],
        [-4.0242,  1.3572]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9752, 0.0248],
        [0.9811, 0.0189]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0044, 0.9956], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1607, 0.1700],
         [0.6926, 0.1687]],

        [[0.5176, 0.0651],
         [0.9577, 0.1746]],

        [[0.6134, 0.1159],
         [0.9530, 0.8901]],

        [[0.3593, 0.0887],
         [0.2539, 0.1651]],

        [[0.0026, 0.2283],
         [0.5834, 0.4825]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: -0.0009582365841354273
Average Adjusted Rand Index: -0.0027279473405407524
Iteration 0: Loss = -20305.46539237511
Iteration 10: Loss = -10973.101278936469
Iteration 20: Loss = -10972.49894269051
Iteration 30: Loss = -10972.171498206719
Iteration 40: Loss = -10972.173682241215
1
Iteration 50: Loss = -10972.22464559296
2
Iteration 60: Loss = -10972.278484782135
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.4978, 0.5022],
        [0.4899, 0.5101]], dtype=torch.float64)
alpha: tensor([0.4983, 0.5017])
beta: tensor([[[0.1847, 0.1668],
         [0.1439, 0.1380]],

        [[0.7469, 0.1593],
         [0.1247, 0.5796]],

        [[0.2955, 0.1534],
         [0.5858, 0.9390]],

        [[0.6572, 0.1580],
         [0.6319, 0.7364]],

        [[0.0914, 0.1629],
         [0.2843, 0.8054]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 1
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 3
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.004538048947463526
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 34
Adjusted Rand Index: 0.09333333333333334
Global Adjusted Rand Index: 0.0064780008922557175
Average Adjusted Rand Index: 0.018927811809694724
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20305.329566480996
Iteration 100: Loss = -10984.023231305211
Iteration 200: Loss = -10976.996474749858
Iteration 300: Loss = -10975.736272230153
Iteration 400: Loss = -10975.084256028294
Iteration 500: Loss = -10974.647359573044
Iteration 600: Loss = -10974.310726074948
Iteration 700: Loss = -10974.04106990011
Iteration 800: Loss = -10973.866976190171
Iteration 900: Loss = -10973.750522065879
Iteration 1000: Loss = -10973.617874924219
Iteration 1100: Loss = -10973.242113373497
Iteration 1200: Loss = -10972.387712405789
Iteration 1300: Loss = -10971.998651033542
Iteration 1400: Loss = -10971.804322275675
Iteration 1500: Loss = -10971.641546010143
Iteration 1600: Loss = -10971.510003072586
Iteration 1700: Loss = -10971.370738033656
Iteration 1800: Loss = -10971.260050367735
Iteration 1900: Loss = -10971.19087245338
Iteration 2000: Loss = -10971.132803094672
Iteration 2100: Loss = -10971.075785751122
Iteration 2200: Loss = -10971.02949031805
Iteration 2300: Loss = -10971.000065363442
Iteration 2400: Loss = -10970.982497457091
Iteration 2500: Loss = -10970.972014687233
Iteration 2600: Loss = -10970.964554051217
Iteration 2700: Loss = -10970.959684623003
Iteration 2800: Loss = -10971.174073173106
1
Iteration 2900: Loss = -10970.953881421612
Iteration 3000: Loss = -10970.952229470568
Iteration 3100: Loss = -10970.951041089073
Iteration 3200: Loss = -10970.95084571436
Iteration 3300: Loss = -10970.949306323957
Iteration 3400: Loss = -10970.948671258891
Iteration 3500: Loss = -10970.948131132656
Iteration 3600: Loss = -10970.947526924212
Iteration 3700: Loss = -10970.947042548662
Iteration 3800: Loss = -10971.059580915326
1
Iteration 3900: Loss = -10970.946129993046
Iteration 4000: Loss = -10970.945629765314
Iteration 4100: Loss = -10970.945192110186
Iteration 4200: Loss = -10970.944726428124
Iteration 4300: Loss = -10970.944093540138
Iteration 4400: Loss = -10970.943562586835
Iteration 4500: Loss = -10970.949187567498
1
Iteration 4600: Loss = -10970.942763956084
Iteration 4700: Loss = -10970.94237530138
Iteration 4800: Loss = -10970.941952962097
Iteration 4900: Loss = -10970.94147477623
Iteration 5000: Loss = -10970.941033766669
Iteration 5100: Loss = -10970.941103328261
1
Iteration 5200: Loss = -10970.940034427203
Iteration 5300: Loss = -10970.939500940356
Iteration 5400: Loss = -10970.93888800896
Iteration 5500: Loss = -10970.940045839116
1
Iteration 5600: Loss = -10970.93761780553
Iteration 5700: Loss = -10970.936826943247
Iteration 5800: Loss = -10971.03377235919
1
Iteration 5900: Loss = -10970.934410305066
Iteration 6000: Loss = -10970.933281987293
Iteration 6100: Loss = -10970.932556286796
Iteration 6200: Loss = -10970.932821293974
1
Iteration 6300: Loss = -10970.931177551396
Iteration 6400: Loss = -10970.93050978093
Iteration 6500: Loss = -10970.931468943862
1
Iteration 6600: Loss = -10970.929362067041
Iteration 6700: Loss = -10970.92880201269
Iteration 6800: Loss = -10970.928377712999
Iteration 6900: Loss = -10970.928008280438
Iteration 7000: Loss = -10970.927659095241
Iteration 7100: Loss = -10970.927452475844
Iteration 7200: Loss = -10970.92763048379
1
Iteration 7300: Loss = -10970.927108226251
Iteration 7400: Loss = -10970.927030522354
Iteration 7500: Loss = -10970.953989822816
1
Iteration 7600: Loss = -10970.926953165375
Iteration 7700: Loss = -10970.926879363147
Iteration 7800: Loss = -10971.076180155927
1
Iteration 7900: Loss = -10970.926905386874
2
Iteration 8000: Loss = -10970.92688070213
3
Iteration 8100: Loss = -10970.933172929872
4
Iteration 8200: Loss = -10970.926885029054
5
Iteration 8300: Loss = -10970.926852231185
Iteration 8400: Loss = -10970.92686564348
1
Iteration 8500: Loss = -10970.928028305416
2
Iteration 8600: Loss = -10970.92685488229
3
Iteration 8700: Loss = -10970.926859531522
4
Iteration 8800: Loss = -10971.004559077692
5
Iteration 8900: Loss = -10970.926856497923
6
Iteration 9000: Loss = -10970.926858904693
7
Iteration 9100: Loss = -10971.055466835007
8
Iteration 9200: Loss = -10970.92684215949
Iteration 9300: Loss = -10970.926845662858
1
Iteration 9400: Loss = -10970.927161435418
2
Iteration 9500: Loss = -10970.926907702577
3
Iteration 9600: Loss = -10970.9268227081
Iteration 9700: Loss = -10970.926841178953
1
Iteration 9800: Loss = -10970.927322337604
2
Iteration 9900: Loss = -10970.926850661119
3
Iteration 10000: Loss = -10970.926883268638
4
Iteration 10100: Loss = -10970.92765515087
5
Iteration 10200: Loss = -10970.926819495717
Iteration 10300: Loss = -10970.926841549233
1
Iteration 10400: Loss = -10970.945278110867
2
Iteration 10500: Loss = -10970.926841988208
3
Iteration 10600: Loss = -10970.926856449474
4
Iteration 10700: Loss = -10970.927051004825
5
Iteration 10800: Loss = -10970.926846123946
6
Iteration 10900: Loss = -10970.926835032322
7
Iteration 11000: Loss = -10971.000137657224
8
Iteration 11100: Loss = -10970.926844654423
9
Iteration 11200: Loss = -10970.926857109549
10
Stopping early at iteration 11200 due to no improvement.
tensor([[ 1.6680, -3.1517],
        [ 1.9291, -3.4186],
        [ 1.3421, -2.7883],
        [ 1.5307, -3.2694],
        [ 0.9140, -4.5928],
        [ 2.0421, -3.4848],
        [ 1.7298, -3.1394],
        [ 0.8939, -2.2970],
        [ 0.3329, -3.7494],
        [ 1.5831, -3.4351],
        [ 1.9678, -3.3544],
        [ 2.2325, -3.6601],
        [ 0.1057, -4.7171],
        [ 1.5137, -2.9535],
        [ 1.8171, -3.7359],
        [ 0.7026, -3.9114],
        [ 1.6816, -3.6710],
        [ 1.7692, -3.2221],
        [ 1.4923, -3.0829],
        [ 1.2912, -3.3848],
        [ 1.7633, -3.1838],
        [ 1.6244, -3.1453],
        [ 0.5018, -5.0012],
        [ 1.1698, -2.5765],
        [ 1.6115, -3.1912],
        [ 1.2198, -2.7042],
        [ 1.3467, -2.9069],
        [ 1.6428, -3.1989],
        [ 1.9793, -3.3792],
        [ 1.3575, -3.3193],
        [ 1.6620, -3.8819],
        [ 1.5415, -2.9357],
        [ 2.0816, -3.5569],
        [ 2.2307, -3.6348],
        [ 1.3482, -3.2677],
        [ 1.2828, -2.6702],
        [ 1.2254, -3.6147],
        [ 0.1817, -4.7969],
        [ 1.9692, -3.3986],
        [ 1.6513, -4.3788],
        [ 2.1065, -3.9598],
        [ 1.4671, -2.8689],
        [ 1.3226, -2.7728],
        [ 0.6467, -4.3214],
        [ 0.6742, -3.0389],
        [ 1.3607, -2.7744],
        [ 2.0547, -3.4571],
        [ 0.8543, -3.5915],
        [ 1.6758, -3.1323],
        [ 1.8169, -3.3362],
        [ 0.8416, -4.5313],
        [ 1.6496, -3.1628],
        [ 1.9931, -3.7026],
        [ 2.2169, -4.3561],
        [ 2.5983, -3.9959],
        [ 1.7162, -3.8075],
        [ 1.9131, -3.6522],
        [ 1.2541, -3.8951],
        [ 2.1373, -3.5494],
        [ 1.4858, -3.8358],
        [ 2.0601, -3.4944],
        [ 1.9607, -3.7247],
        [ 1.5522, -3.0941],
        [ 1.9597, -4.0918],
        [ 1.6940, -3.6368],
        [ 1.6574, -3.4763],
        [ 1.5478, -3.3330],
        [ 1.5287, -2.9246],
        [ 2.0703, -3.6105],
        [ 0.4194, -3.9983],
        [ 1.4858, -3.8561],
        [ 1.4980, -3.5228],
        [ 1.4862, -2.9315],
        [ 0.1566, -4.7718],
        [ 0.4469, -4.5666],
        [ 1.9916, -3.4215],
        [ 0.6427, -3.6481],
        [ 0.3253, -4.7061],
        [ 2.1386, -3.5270],
        [ 1.7603, -3.2646],
        [ 0.5867, -3.6065],
        [ 1.8697, -3.3494],
        [ 1.3397, -3.1703],
        [ 0.8891, -4.4897],
        [ 1.0566, -3.4317],
        [ 1.1851, -3.4899],
        [ 1.7830, -3.5529],
        [ 2.3853, -3.8281],
        [ 2.2428, -3.6311],
        [ 2.3073, -3.6966],
        [ 1.6471, -3.8210],
        [ 1.7570, -3.3982],
        [ 1.1696, -2.6023],
        [ 1.6052, -3.6008],
        [ 1.1948, -2.9641],
        [ 1.5571, -3.2777],
        [ 1.3701, -3.8037],
        [ 1.4514, -3.3658],
        [ 1.5754, -3.0436],
        [ 1.0255, -2.5311]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9734, 0.0266],
        [0.9193, 0.0807]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9917, 0.0083], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1640, 0.1911],
         [0.1439, 0.0832]],

        [[0.7469, 0.0733],
         [0.1247, 0.5796]],

        [[0.2955, 0.1134],
         [0.5858, 0.9390]],

        [[0.6572, 0.0924],
         [0.6319, 0.7364]],

        [[0.0914, 0.2333],
         [0.2843, 0.8054]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: -0.0009448268386825988
Average Adjusted Rand Index: -0.0027279473405407524
Iteration 0: Loss = -29674.35680374637
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.2462,    nan]],

        [[0.4208,    nan],
         [0.9850, 0.4021]],

        [[0.3324,    nan],
         [0.4712, 0.9591]],

        [[0.7861,    nan],
         [0.1521, 0.8140]],

        [[0.1750,    nan],
         [0.5568, 0.8414]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29673.71011847401
Iteration 100: Loss = -10984.9766471406
Iteration 200: Loss = -10979.232050918064
Iteration 300: Loss = -10977.398784338862
Iteration 400: Loss = -10976.545597272001
Iteration 500: Loss = -10975.999493760035
Iteration 600: Loss = -10975.623959615426
Iteration 700: Loss = -10975.354343436991
Iteration 800: Loss = -10975.153696235984
Iteration 900: Loss = -10974.99979485023
Iteration 1000: Loss = -10974.879156092009
Iteration 1100: Loss = -10974.782794002302
Iteration 1200: Loss = -10974.704391087871
Iteration 1300: Loss = -10974.639653242717
Iteration 1400: Loss = -10974.585554190528
Iteration 1500: Loss = -10974.539758846953
Iteration 1600: Loss = -10974.500614674793
Iteration 1700: Loss = -10974.466826512817
Iteration 1800: Loss = -10974.43740657348
Iteration 1900: Loss = -10974.41163893497
Iteration 2000: Loss = -10974.388840663429
Iteration 2100: Loss = -10974.368534390054
Iteration 2200: Loss = -10974.350395922998
Iteration 2300: Loss = -10974.333947872337
Iteration 2400: Loss = -10974.318965172355
Iteration 2500: Loss = -10974.305240859916
Iteration 2600: Loss = -10974.292516251015
Iteration 2700: Loss = -10974.280596875153
Iteration 2800: Loss = -10974.26926566561
Iteration 2900: Loss = -10974.258458650012
Iteration 3000: Loss = -10974.247873250672
Iteration 3100: Loss = -10974.237352559127
Iteration 3200: Loss = -10974.226715630253
Iteration 3300: Loss = -10974.2157743204
Iteration 3400: Loss = -10974.204526801681
Iteration 3500: Loss = -10974.192914142282
Iteration 3600: Loss = -10974.181048676677
Iteration 3700: Loss = -10974.169251418398
Iteration 3800: Loss = -10974.15776901325
Iteration 3900: Loss = -10974.14664079544
Iteration 4000: Loss = -10974.135855363087
Iteration 4100: Loss = -10974.125395137293
Iteration 4200: Loss = -10974.114958778906
Iteration 4300: Loss = -10974.104301858759
Iteration 4400: Loss = -10974.093069413244
Iteration 4500: Loss = -10974.080289455094
Iteration 4600: Loss = -10974.064429025544
Iteration 4700: Loss = -10974.041147155036
Iteration 4800: Loss = -10973.998628713529
Iteration 4900: Loss = -10973.913527967494
Iteration 5000: Loss = -10973.690732449311
Iteration 5100: Loss = -10972.670183428061
Iteration 5200: Loss = -10972.249267897721
Iteration 5300: Loss = -10971.930175530244
Iteration 5400: Loss = -10971.640927909548
Iteration 5500: Loss = -10971.406264561632
Iteration 5600: Loss = -10971.328223075743
Iteration 5700: Loss = -10971.119219114531
Iteration 5800: Loss = -10971.04874279981
Iteration 5900: Loss = -10971.007902921112
Iteration 6000: Loss = -10970.985770279902
Iteration 6100: Loss = -10970.970782342547
Iteration 6200: Loss = -10970.962094785034
Iteration 6300: Loss = -10970.956114211294
Iteration 6400: Loss = -10970.953445419296
Iteration 6500: Loss = -10970.948880475955
Iteration 6600: Loss = -10970.946938737061
Iteration 6700: Loss = -10971.038258641778
1
Iteration 6800: Loss = -10970.944521079922
Iteration 6900: Loss = -10970.943702604422
Iteration 7000: Loss = -10970.943064661687
Iteration 7100: Loss = -10970.942583156895
Iteration 7200: Loss = -10970.941944239828
Iteration 7300: Loss = -10970.941537979192
Iteration 7400: Loss = -10970.941184153324
Iteration 7500: Loss = -10970.940710739262
Iteration 7600: Loss = -10970.940369132582
Iteration 7700: Loss = -10970.942003738353
1
Iteration 7800: Loss = -10970.939786584391
Iteration 7900: Loss = -10970.93949325336
Iteration 8000: Loss = -10970.939238573763
Iteration 8100: Loss = -10970.93918236132
Iteration 8200: Loss = -10970.938730159478
Iteration 8300: Loss = -10970.938498338066
Iteration 8400: Loss = -10970.99512166804
1
Iteration 8500: Loss = -10970.937959053801
Iteration 8600: Loss = -10970.937571785113
Iteration 8700: Loss = -10970.937021416443
Iteration 8800: Loss = -10970.936333106041
Iteration 8900: Loss = -10970.935748151702
Iteration 9000: Loss = -10970.935453508162
Iteration 9100: Loss = -10970.93631927096
1
Iteration 9200: Loss = -10970.935107908843
Iteration 9300: Loss = -10970.935011033484
Iteration 9400: Loss = -10970.939518372646
1
Iteration 9500: Loss = -10970.934859427387
Iteration 9600: Loss = -10970.934788513032
Iteration 9700: Loss = -10970.94129108485
1
Iteration 9800: Loss = -10970.934680170809
Iteration 9900: Loss = -10970.93466185901
Iteration 10000: Loss = -10971.121124784808
1
Iteration 10100: Loss = -10970.934576279731
Iteration 10200: Loss = -10970.93450523374
Iteration 10300: Loss = -10970.94789144284
1
Iteration 10400: Loss = -10970.934446909327
Iteration 10500: Loss = -10970.934416826663
Iteration 10600: Loss = -10970.934431917063
1
Iteration 10700: Loss = -10970.934507205195
2
Iteration 10800: Loss = -10970.93436173797
Iteration 10900: Loss = -10970.934341511014
Iteration 11000: Loss = -10970.934570939799
1
Iteration 11100: Loss = -10970.934310221757
Iteration 11200: Loss = -10970.934322396673
1
Iteration 11300: Loss = -10970.935141527196
2
Iteration 11400: Loss = -10970.934264337087
Iteration 11500: Loss = -10970.93427097998
1
Iteration 11600: Loss = -10970.998013888666
2
Iteration 11700: Loss = -10970.934255656968
Iteration 11800: Loss = -10970.934220904943
Iteration 11900: Loss = -10970.935394938337
1
Iteration 12000: Loss = -10970.934231971787
2
Iteration 12100: Loss = -10970.934204129244
Iteration 12200: Loss = -10970.934194520969
Iteration 12300: Loss = -10970.934714603965
1
Iteration 12400: Loss = -10970.934163054506
Iteration 12500: Loss = -10970.934142101758
Iteration 12600: Loss = -10970.935590257837
1
Iteration 12700: Loss = -10970.934144694746
2
Iteration 12800: Loss = -10970.934136210411
Iteration 12900: Loss = -10970.934282916225
1
Iteration 13000: Loss = -10970.93421355925
2
Iteration 13100: Loss = -10970.934137690978
3
Iteration 13200: Loss = -10970.934112589019
Iteration 13300: Loss = -10970.934149172099
1
Iteration 13400: Loss = -10970.934662523294
2
Iteration 13500: Loss = -10970.93409783437
Iteration 13600: Loss = -10970.937360730131
1
Iteration 13700: Loss = -10970.934111453864
2
Iteration 13800: Loss = -10970.93412342792
3
Iteration 13900: Loss = -10970.93413942567
4
Iteration 14000: Loss = -10970.934134688863
5
Iteration 14100: Loss = -10970.934106732371
6
Iteration 14200: Loss = -10970.934252425968
7
Iteration 14300: Loss = -10970.934093221465
Iteration 14400: Loss = -10970.93425279057
1
Iteration 14500: Loss = -10970.93426961888
2
Iteration 14600: Loss = -10970.93410343056
3
Iteration 14700: Loss = -10970.934596248888
4
Iteration 14800: Loss = -10970.93409819146
5
Iteration 14900: Loss = -10970.934588642704
6
Iteration 15000: Loss = -10970.934132194208
7
Iteration 15100: Loss = -10970.934271970817
8
Iteration 15200: Loss = -10970.934078190101
Iteration 15300: Loss = -10970.943922799748
1
Iteration 15400: Loss = -10970.934109045616
2
Iteration 15500: Loss = -10970.934098941856
3
Iteration 15600: Loss = -10970.938803966239
4
Iteration 15700: Loss = -10970.934119998905
5
Iteration 15800: Loss = -10970.935422225815
6
Iteration 15900: Loss = -10970.934057291915
Iteration 16000: Loss = -10970.940568627087
1
Iteration 16100: Loss = -10970.934064533032
2
Iteration 16200: Loss = -10970.942784107016
3
Iteration 16300: Loss = -10970.934138258828
4
Iteration 16400: Loss = -10970.934078529357
5
Iteration 16500: Loss = -10970.943581950653
6
Iteration 16600: Loss = -10970.934095559178
7
Iteration 16700: Loss = -10970.952688879659
8
Iteration 16800: Loss = -10970.934072259792
9
Iteration 16900: Loss = -10970.934879838127
10
Stopping early at iteration 16900 due to no improvement.
tensor([[-3.2230,  1.8031],
        [-4.1786,  1.3993],
        [-2.8645,  1.4409],
        [-4.6496,  0.3584],
        [-4.2016,  1.5455],
        [-3.6815,  2.0834],
        [-3.2775,  1.7916],
        [-2.9775,  0.4321],
        [-2.8614,  1.4015],
        [-3.5847,  1.6451],
        [-3.4772,  2.0770],
        [-3.7821,  2.3626],
        [-3.3665,  1.6636],
        [-3.2463,  1.4129],
        [-3.7675,  2.0185],
        [-3.1620,  1.6556],
        [-3.5279,  2.0547],
        [-3.4644,  1.7348],
        [-3.1711,  1.5983],
        [-3.2100,  1.6628],
        [-3.2837,  1.8801],
        [-3.2660,  1.7163],
        [-4.1382,  1.5970],
        [-4.0431, -0.1329],
        [-3.4384,  1.5732],
        [-2.7827,  1.3128],
        [-3.4388,  0.9952],
        [-4.2179,  0.8193],
        [-3.8961,  1.6893],
        [-3.4572,  1.4085],
        [-3.9971,  1.7805],
        [-3.0675,  1.5912],
        [-3.8756,  2.0112],
        [-3.7606,  2.3595],
        [-3.3320,  1.4786],
        [-2.8536,  1.2662],
        [-3.4513,  1.5920],
        [-3.4900,  1.7061],
        [-3.5410,  2.0521],
        [-4.1904,  2.1023],
        [-5.0472,  1.2778],
        [-2.9617,  1.5509],
        [-2.8285,  1.4375],
        [-3.3290,  1.8571],
        [-2.7653,  1.1208],
        [-3.1982,  1.1561],
        [-3.6311,  2.1121],
        [-3.0320,  1.6086],
        [-3.2277,  1.7853],
        [-3.8840,  1.4939],
        [-3.5141,  2.0856],
        [-3.7272,  1.2931],
        [-3.6653,  2.2775],
        [-4.2564,  2.6024],
        [-4.2489,  2.6292],
        [-5.1875,  0.5723],
        [-3.5921,  2.2056],
        [-3.4477,  1.9263],
        [-3.7217,  2.2124],
        [-3.4852,  2.0693],
        [-3.6023,  2.1844],
        [-4.1852,  1.7468],
        [-3.1481,  1.7083],
        [-4.4240,  1.8883],
        [-3.4714,  2.0833],
        [-3.3741,  1.9867],
        [-3.3048,  1.7948],
        [-3.5232,  1.1153],
        [-4.2057,  1.7225],
        [-3.4711,  1.1365],
        [-4.3059,  1.2648],
        [-3.3709,  1.8625],
        [-4.3441,  0.2716],
        [-4.6658,  0.4724],
        [-3.3280,  1.8916],
        [-3.5546,  2.0795],
        [-3.0007,  1.4670],
        [-4.0937,  1.1470],
        [-3.7733,  2.1412],
        [-3.3116,  1.9248],
        [-2.8892,  1.4940],
        [-3.7898,  1.6472],
        [-3.4196,  1.2773],
        [-3.4960,  2.1073],
        [-3.7121,  0.9589],
        [-3.6315,  1.2393],
        [-3.5469,  2.0784],
        [-4.0628,  2.4218],
        [-3.7573,  2.3707],
        [-3.8503,  2.4204],
        [-3.5484,  2.1619],
        [-3.3914,  1.9882],
        [-2.6861,  1.2430],
        [-3.6837,  1.7505],
        [-3.1024,  1.2626],
        [-4.0619,  0.9785],
        [-3.7477,  1.6482],
        [-9.3430,  5.6153],
        [-3.1156,  1.7055],
        [-2.6690,  1.0446]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.0786, 0.9214],
        [0.0267, 0.9733]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0067, 0.9933], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0811, 0.1923],
         [0.2462, 0.1639]],

        [[0.4208, 0.0734],
         [0.9850, 0.4021]],

        [[0.3324, 0.1136],
         [0.4712, 0.9591]],

        [[0.7861, 0.0923],
         [0.1521, 0.8140]],

        [[0.1750, 0.2337],
         [0.5568, 0.8414]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: -0.0009448268386825988
Average Adjusted Rand Index: -0.0027279473405407524
Iteration 0: Loss = -37003.190334833795
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.3840,    nan]],

        [[0.2711,    nan],
         [0.5520, 0.3389]],

        [[0.3006,    nan],
         [0.0744, 0.4779]],

        [[0.0346,    nan],
         [0.8206, 0.2048]],

        [[0.1380,    nan],
         [0.3837, 0.5439]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37002.792162870006
Iteration 100: Loss = -11019.13974089571
Iteration 200: Loss = -10989.666425850477
Iteration 300: Loss = -10983.739809256045
Iteration 400: Loss = -10978.46043317953
Iteration 500: Loss = -10977.313544673369
Iteration 600: Loss = -10976.57239482871
Iteration 700: Loss = -10976.060388851682
Iteration 800: Loss = -10975.69334061977
Iteration 900: Loss = -10975.41019776839
Iteration 1000: Loss = -10975.192447979645
Iteration 1100: Loss = -10975.01404809017
Iteration 1200: Loss = -10974.848649008813
Iteration 1300: Loss = -10974.753375433173
Iteration 1400: Loss = -10974.680039137715
Iteration 1500: Loss = -10974.681616219807
1
Iteration 1600: Loss = -10974.565599542142
Iteration 1700: Loss = -10974.519442956225
Iteration 1800: Loss = -10974.481896378316
Iteration 1900: Loss = -10974.450603794285
Iteration 2000: Loss = -10974.423195817955
Iteration 2100: Loss = -10974.400783416982
Iteration 2200: Loss = -10974.567475102109
1
Iteration 2300: Loss = -10974.364218143764
Iteration 2400: Loss = -10974.34896946673
Iteration 2500: Loss = -10974.335324213955
Iteration 2600: Loss = -10974.343332272076
1
Iteration 2700: Loss = -10974.31224348288
Iteration 2800: Loss = -10974.302487811385
Iteration 2900: Loss = -10974.293667301443
Iteration 3000: Loss = -10974.28567740977
Iteration 3100: Loss = -10974.278020867858
Iteration 3200: Loss = -10974.27095972922
Iteration 3300: Loss = -10974.371422307078
1
Iteration 3400: Loss = -10974.257747851103
Iteration 3500: Loss = -10974.251379896044
Iteration 3600: Loss = -10974.24505419233
Iteration 3700: Loss = -10974.248121572346
1
Iteration 3800: Loss = -10974.23183355764
Iteration 3900: Loss = -10974.224781665742
Iteration 4000: Loss = -10974.21811005244
Iteration 4100: Loss = -10974.209680640819
Iteration 4200: Loss = -10974.201788419592
Iteration 4300: Loss = -10974.193936937212
Iteration 4400: Loss = -10974.188743755467
Iteration 4500: Loss = -10974.178804554756
Iteration 4600: Loss = -10974.17162084702
Iteration 4700: Loss = -10974.176985115475
1
Iteration 4800: Loss = -10974.158159980641
Iteration 4900: Loss = -10974.15175402886
Iteration 5000: Loss = -10974.145486573143
Iteration 5100: Loss = -10974.158009356597
1
Iteration 5200: Loss = -10974.13292995962
Iteration 5300: Loss = -10974.126435746344
Iteration 5400: Loss = -10974.11976876892
Iteration 5500: Loss = -10974.112780356645
Iteration 5600: Loss = -10974.105327898835
Iteration 5700: Loss = -10974.097301934471
Iteration 5800: Loss = -10974.091781106488
Iteration 5900: Loss = -10974.076403768227
Iteration 6000: Loss = -10974.060614994829
Iteration 6100: Loss = -10974.037485456798
Iteration 6200: Loss = -10973.97909949928
Iteration 6300: Loss = -10973.876435361848
Iteration 6400: Loss = -10973.76876150414
Iteration 6500: Loss = -10973.659152054635
Iteration 6600: Loss = -10973.533420320267
Iteration 6700: Loss = -10973.389603417389
Iteration 6800: Loss = -10973.245639666859
Iteration 6900: Loss = -10973.128732419103
Iteration 7000: Loss = -10973.044858597512
Iteration 7100: Loss = -10972.980918389145
Iteration 7200: Loss = -10972.927062516506
Iteration 7300: Loss = -10972.869989116743
Iteration 7400: Loss = -10972.814927965503
Iteration 7500: Loss = -10972.74347190841
Iteration 7600: Loss = -10972.587726552478
Iteration 7700: Loss = -10972.20026823978
Iteration 7800: Loss = -10972.018205480255
Iteration 7900: Loss = -10971.963109182008
Iteration 8000: Loss = -10971.935921186208
Iteration 8100: Loss = -10971.924873572048
Iteration 8200: Loss = -10971.918186764682
Iteration 8300: Loss = -10971.9171651434
Iteration 8400: Loss = -10971.910488389804
Iteration 8500: Loss = -10971.908226300566
Iteration 8600: Loss = -10972.01829092604
1
Iteration 8700: Loss = -10971.905175774737
Iteration 8800: Loss = -10971.904154015056
Iteration 8900: Loss = -10971.90334161171
Iteration 9000: Loss = -10971.902675124984
Iteration 9100: Loss = -10971.902083984565
Iteration 9200: Loss = -10971.901650223099
Iteration 9300: Loss = -10971.911251259402
1
Iteration 9400: Loss = -10971.900874956522
Iteration 9500: Loss = -10971.900550144726
Iteration 9600: Loss = -10972.355000231168
1
Iteration 9700: Loss = -10971.90006162651
Iteration 9800: Loss = -10971.899905390455
Iteration 9900: Loss = -10971.89973494093
Iteration 10000: Loss = -10971.900992026141
1
Iteration 10100: Loss = -10971.899438992108
Iteration 10200: Loss = -10971.899302988342
Iteration 10300: Loss = -10971.918872753326
1
Iteration 10400: Loss = -10971.899042302135
Iteration 10500: Loss = -10971.898986880835
Iteration 10600: Loss = -10971.97599573535
1
Iteration 10700: Loss = -10971.898945248256
Iteration 10800: Loss = -10971.898681088582
Iteration 10900: Loss = -10971.900675822495
1
Iteration 11000: Loss = -10971.89858303834
Iteration 11100: Loss = -10971.898661276431
1
Iteration 11200: Loss = -10971.898450687471
Iteration 11300: Loss = -10971.898440717065
Iteration 11400: Loss = -10971.898335202837
Iteration 11500: Loss = -10971.900178816322
1
Iteration 11600: Loss = -10971.898253835576
Iteration 11700: Loss = -10971.916838031564
1
Iteration 11800: Loss = -10971.898164583228
Iteration 11900: Loss = -10971.898113854691
Iteration 12000: Loss = -10971.898984550036
1
Iteration 12100: Loss = -10971.898027465386
Iteration 12200: Loss = -10971.89801692368
Iteration 12300: Loss = -10971.899234721419
1
Iteration 12400: Loss = -10971.897931633035
Iteration 12500: Loss = -10971.897880991224
Iteration 12600: Loss = -10971.899625586077
1
Iteration 12700: Loss = -10971.897848030274
Iteration 12800: Loss = -10971.897857709266
1
Iteration 12900: Loss = -10971.89781498433
Iteration 13000: Loss = -10971.915414438545
1
Iteration 13100: Loss = -10971.90578358677
2
Iteration 13200: Loss = -10971.897734517397
Iteration 13300: Loss = -10971.898065662186
1
Iteration 13400: Loss = -10972.000696574081
2
Iteration 13500: Loss = -10971.897725228646
Iteration 13600: Loss = -10972.144864791737
1
Iteration 13700: Loss = -10971.89770106211
Iteration 13800: Loss = -10971.897679521251
Iteration 13900: Loss = -10971.898089649045
1
Iteration 14000: Loss = -10971.897649998593
Iteration 14100: Loss = -10971.912966057074
1
Iteration 14200: Loss = -10971.897653869539
2
Iteration 14300: Loss = -10971.897659254644
3
Iteration 14400: Loss = -10971.897772324295
4
Iteration 14500: Loss = -10972.016983716076
5
Iteration 14600: Loss = -10971.897936500198
6
Iteration 14700: Loss = -10971.90417134863
7
Iteration 14800: Loss = -10971.898560782485
8
Iteration 14900: Loss = -10971.907400652024
9
Iteration 15000: Loss = -10971.901363633207
10
Stopping early at iteration 15000 due to no improvement.
tensor([[-2.7263,  1.2535],
        [-2.6834,  1.2528],
        [-2.7314,  1.2913],
        [-2.8208,  1.1626],
        [-3.0544,  0.8718],
        [-3.4127,  0.5221],
        [-2.8877,  1.0730],
        [-2.8343,  1.1881],
        [-2.7085,  1.3082],
        [-2.9357,  1.0337],
        [-2.6603,  1.2648],
        [-2.6717,  1.2371],
        [-2.6903,  1.2789],
        [-2.7656,  1.2377],
        [-2.7707,  1.1650],
        [-2.7087,  1.2754],
        [-3.6288,  0.3034],
        [-2.7401,  1.2297],
        [-2.7932,  1.2032],
        [-3.1782,  0.7904],
        [-2.7537,  1.2186],
        [-3.5920,  0.3780],
        [-2.9204,  0.9873],
        [-2.7187,  1.3138],
        [-2.7128,  1.2651],
        [-3.5676,  0.4636],
        [-3.0172,  0.9973],
        [-3.7302,  0.2159],
        [-2.6839,  1.2545],
        [-2.9447,  1.0416],
        [-4.2711, -0.3441],
        [-2.6943,  1.3017],
        [-3.0122,  0.9137],
        [-2.8234,  1.0520],
        [-3.3738,  0.6047],
        [-2.7225,  1.3178],
        [-2.9342,  1.0378],
        [-2.9052,  1.0496],
        [-2.6862,  1.2610],
        [-2.8142,  1.0823],
        [-3.1808,  0.7046],
        [-2.8898,  1.1242],
        [-2.8813,  1.1464],
        [-2.9745,  0.9802],
        [-3.6908,  0.3513],
        [-2.9376,  1.0599],
        [-3.1518,  0.7393],
        [-3.9441,  0.0394],
        [-3.0052,  0.9773],
        [-2.8245,  1.1182],
        [-4.2056, -0.2684],
        [-3.3377,  0.6309],
        [-2.6659,  1.2550],
        [-3.1594,  0.7031],
        [-4.0762, -0.2186],
        [-3.5326,  0.4017],
        [-2.6621,  1.2704],
        [-3.8721,  0.0877],
        [-2.8266,  1.1012],
        [-3.8535,  0.0910],
        [-2.6766,  1.2319],
        [-2.6106,  1.2215],
        [-2.7433,  1.2424],
        [-3.0115,  0.8739],
        [-2.9245,  1.0171],
        [-2.6682,  1.2743],
        [-2.9476,  1.0265],
        [-2.8719,  1.1312],
        [-3.0802,  0.8420],
        [-2.8364,  1.1662],
        [-2.6735,  1.2711],
        [-3.2615,  0.6623],
        [-2.6854,  1.2977],
        [-2.7484,  1.2251],
        [-2.9726,  0.9955],
        [-2.7091,  1.2216],
        [-2.7307,  1.2843],
        [-3.2891,  0.6787],
        [-2.7107,  1.2120],
        [-3.1323,  0.8352],
        [-2.8297,  1.1890],
        [-2.6726,  1.2831],
        [-2.8454,  1.1354],
        [-2.8663,  1.0432],
        [-2.7360,  1.2219],
        [-2.6939,  1.2911],
        [-2.8297,  1.0650],
        [-2.8857,  0.9581],
        [-2.7624,  1.1436],
        [-2.7134,  1.1872],
        [-2.8497,  1.0886],
        [-2.7483,  1.2050],
        [-3.5218,  0.5266],
        [-2.7322,  1.2173],
        [-2.7317,  1.2762],
        [-2.8751,  1.1050],
        [-2.7974,  1.1536],
        [-2.6612,  1.2734],
        [-2.9277,  1.0363],
        [-3.0956,  0.9666]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9744, 0.0256],
        [0.9912, 0.0088]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0189, 0.9811], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1607, 0.1673],
         [0.3840, 0.1699]],

        [[0.2711, 0.1919],
         [0.5520, 0.3389]],

        [[0.3006, 0.1189],
         [0.0744, 0.4779]],

        [[0.0346, 0.0892],
         [0.8206, 0.2048]],

        [[0.1380, 0.2302],
         [0.3837, 0.5439]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: -0.00021282257348139618
Average Adjusted Rand Index: -0.0014350180476114596
10856.57399173985
new:  [-0.0009582365841354273, -0.0009448268386825988, -0.0009448268386825988, -0.00021282257348139618] [-0.0027279473405407524, -0.0027279473405407524, -0.0027279473405407524, -0.0014350180476114596] [10970.644787654168, 10970.926857109549, 10970.934879838127, 10971.901363633207]
prior:  [0.00032718194871431865, 0.0064780008922557175, 0.0, 0.0] [-0.0030801938947100684, 0.018927811809694724, 0.0, 0.0] [10972.82167959702, 10972.278484782135, nan, nan]
-----------------------------------------------------------------------------------------
This iteration is 14
True Objective function: Loss = -10800.800205334708
Iteration 0: Loss = -29513.83522181957
Iteration 10: Loss = -10833.945726607755
Iteration 20: Loss = -10833.941162885812
Iteration 30: Loss = -10833.941544970932
1
Iteration 40: Loss = -10833.942129856923
2
Iteration 50: Loss = -10833.942791703943
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[6.7899e-01, 3.2101e-01],
        [9.9995e-01, 4.6430e-05]], dtype=torch.float64)
alpha: tensor([0.7564, 0.2436])
beta: tensor([[[0.1558, 0.1711],
         [0.7068, 0.1619]],

        [[0.0564, 0.1571],
         [0.5535, 0.0233]],

        [[0.1899, 0.1510],
         [0.2075, 0.7547]],

        [[0.0064, 0.1511],
         [0.3667, 0.8835]],

        [[0.1043, 0.1636],
         [0.4196, 0.3176]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29251.36052750028
Iteration 100: Loss = -10836.794205195358
Iteration 200: Loss = -10833.996440281262
Iteration 300: Loss = -10833.704687196401
Iteration 400: Loss = -10833.527377808296
Iteration 500: Loss = -10833.373764305074
Iteration 600: Loss = -10833.231478565041
Iteration 700: Loss = -10833.094553884914
Iteration 800: Loss = -10832.953229059565
Iteration 900: Loss = -10832.792038315529
Iteration 1000: Loss = -10832.599293391017
Iteration 1100: Loss = -10832.40252738399
Iteration 1200: Loss = -10832.244316228034
Iteration 1300: Loss = -10832.107658355775
Iteration 1400: Loss = -10831.944505034378
Iteration 1500: Loss = -10831.747825190578
Iteration 1600: Loss = -10831.652420699238
Iteration 1700: Loss = -10831.627633295382
Iteration 1800: Loss = -10831.61830036864
Iteration 1900: Loss = -10831.613271369899
Iteration 2000: Loss = -10831.610101301485
Iteration 2100: Loss = -10831.607886170861
Iteration 2200: Loss = -10831.60631187268
Iteration 2300: Loss = -10831.60511929154
Iteration 2400: Loss = -10831.604209751491
Iteration 2500: Loss = -10831.603448243217
Iteration 2600: Loss = -10831.602860274283
Iteration 2700: Loss = -10831.60239685406
Iteration 2800: Loss = -10831.601939662814
Iteration 2900: Loss = -10831.601638774222
Iteration 3000: Loss = -10831.60136027427
Iteration 3100: Loss = -10831.601123320403
Iteration 3200: Loss = -10831.600857715348
Iteration 3300: Loss = -10831.600700646019
Iteration 3400: Loss = -10831.600551199215
Iteration 3500: Loss = -10831.600393957353
Iteration 3600: Loss = -10831.600275595642
Iteration 3700: Loss = -10831.600192187887
Iteration 3800: Loss = -10831.600035318701
Iteration 3900: Loss = -10831.59996907588
Iteration 4000: Loss = -10831.599883183306
Iteration 4100: Loss = -10831.599796185728
Iteration 4200: Loss = -10831.599728680047
Iteration 4300: Loss = -10831.599659531388
Iteration 4400: Loss = -10831.599604168254
Iteration 4500: Loss = -10831.599579397496
Iteration 4600: Loss = -10831.59952455228
Iteration 4700: Loss = -10831.599429202197
Iteration 4800: Loss = -10831.610991585128
1
Iteration 4900: Loss = -10831.602400432417
2
Iteration 5000: Loss = -10831.599339448952
Iteration 5100: Loss = -10831.600649719028
1
Iteration 5200: Loss = -10831.59930764083
Iteration 5300: Loss = -10831.600257124297
1
Iteration 5400: Loss = -10831.599193284726
Iteration 5500: Loss = -10831.655665858985
1
Iteration 5600: Loss = -10831.59918149883
Iteration 5700: Loss = -10831.599119784449
Iteration 5800: Loss = -10831.796713593994
1
Iteration 5900: Loss = -10831.599071793671
Iteration 6000: Loss = -10831.599018677725
Iteration 6100: Loss = -10831.608527412323
1
Iteration 6200: Loss = -10831.599013359957
Iteration 6300: Loss = -10831.598953487186
Iteration 6400: Loss = -10831.689090454018
1
Iteration 6500: Loss = -10831.598891198073
Iteration 6600: Loss = -10831.598879514324
Iteration 6700: Loss = -10831.640744177597
1
Iteration 6800: Loss = -10831.598857705194
Iteration 6900: Loss = -10831.598733209437
Iteration 7000: Loss = -10831.598724632588
Iteration 7100: Loss = -10831.598661721599
Iteration 7200: Loss = -10831.598617564028
Iteration 7300: Loss = -10831.598549528913
Iteration 7400: Loss = -10831.598873989678
1
Iteration 7500: Loss = -10831.598404582086
Iteration 7600: Loss = -10831.598314452018
Iteration 7700: Loss = -10831.598261697749
Iteration 7800: Loss = -10831.59808312034
Iteration 7900: Loss = -10831.598084389032
1
Iteration 8000: Loss = -10831.597794645953
Iteration 8100: Loss = -10831.613879652037
1
Iteration 8200: Loss = -10831.597336337032
Iteration 8300: Loss = -10831.597086452895
Iteration 8400: Loss = -10831.59703227318
Iteration 8500: Loss = -10831.598968066663
1
Iteration 8600: Loss = -10831.604652940172
2
Iteration 8700: Loss = -10831.597440977595
3
Iteration 8800: Loss = -10831.78034215007
4
Iteration 8900: Loss = -10831.596650384201
Iteration 9000: Loss = -10831.59672020833
1
Iteration 9100: Loss = -10831.597568127248
2
Iteration 9200: Loss = -10831.609639953189
3
Iteration 9300: Loss = -10831.634260473422
4
Iteration 9400: Loss = -10831.59668281605
5
Iteration 9500: Loss = -10831.597072705214
6
Iteration 9600: Loss = -10831.596750926348
7
Iteration 9700: Loss = -10831.596744416776
8
Iteration 9800: Loss = -10831.611245859134
9
Iteration 9900: Loss = -10831.596652343382
10
Stopping early at iteration 9900 due to no improvement.
tensor([[-4.2778, -0.3375],
        [-4.3467, -0.2685],
        [-4.2387, -0.3765],
        [-4.2649, -0.3503],
        [-4.3764, -0.2388],
        [-4.3254, -0.2898],
        [-4.2839, -0.3313],
        [-4.2702, -0.3450],
        [-4.2224, -0.3928],
        [-4.2441, -0.3711],
        [-4.2493, -0.3660],
        [-4.2874, -0.3279],
        [-4.3671, -0.2482],
        [-4.1997, -0.4156],
        [-4.3226, -0.2926],
        [-4.2166, -0.3986],
        [-4.2081, -0.4071],
        [-4.2465, -0.3687],
        [-4.1992, -0.4160],
        [-4.2991, -0.3162],
        [-4.2913, -0.3239],
        [-4.3267, -0.2886],
        [-4.2690, -0.3462],
        [-4.2269, -0.3884],
        [-4.3220, -0.2932],
        [-4.2854, -0.3298],
        [-4.2679, -0.3473],
        [-4.1755, -0.4397],
        [-4.2136, -0.4016],
        [-4.2894, -0.3258],
        [-4.2010, -0.4142],
        [-4.3003, -0.3149],
        [-5.5478,  0.9326],
        [-4.2318, -0.3834],
        [-4.2302, -0.3850],
        [-4.3064, -0.3088],
        [-4.3023, -0.3130],
        [-4.3052, -0.3100],
        [-4.2856, -0.3296],
        [-4.1478, -0.4674],
        [-4.2854, -0.3298],
        [-4.3069, -0.3083],
        [-4.3260, -0.2893],
        [-4.2187, -0.3965],
        [-4.2638, -0.3514],
        [-4.2671, -0.3481],
        [-4.3401, -0.2751],
        [-4.2791, -0.3362],
        [-4.2745, -0.3407],
        [-4.2662, -0.3491],
        [-4.2882, -0.3271],
        [-4.2504, -0.3648],
        [-4.3495, -0.2657],
        [-4.2657, -0.3495],
        [-4.2287, -0.3865],
        [-4.3237, -0.2915],
        [-4.2970, -0.3182],
        [-4.3787, -0.2365],
        [-4.2612, -0.3540],
        [-4.2827, -0.3325],
        [-4.2018, -0.4134],
        [-4.3355, -0.2797],
        [-4.2625, -0.3527],
        [-4.1977, -0.4175],
        [-4.2257, -0.3895],
        [-4.3108, -0.3044],
        [-4.2651, -0.3501],
        [-4.2133, -0.4019],
        [-4.2103, -0.4049],
        [-4.2749, -0.3403],
        [-4.2537, -0.3616],
        [-4.2734, -0.3418],
        [-4.2741, -0.3412],
        [-4.2762, -0.3390],
        [-4.2259, -0.3893],
        [-4.2764, -0.3389],
        [-4.2420, -0.3732],
        [-4.3356, -0.2796],
        [-4.2200, -0.3953],
        [-4.1873, -0.4279],
        [-4.2850, -0.3302],
        [-4.2836, -0.3316],
        [-4.2328, -0.3824],
        [-4.2699, -0.3453],
        [-4.1391, -0.4761],
        [-4.3521, -0.2631],
        [-4.2139, -0.4014],
        [-4.2802, -0.3350],
        [-4.2232, -0.3920],
        [-4.2361, -0.3791],
        [-4.2344, -0.3808],
        [-4.2515, -0.3637],
        [-4.2000, -0.4152],
        [-4.1780, -0.4372],
        [-4.2866, -0.3286],
        [-4.3162, -0.2991],
        [-4.2425, -0.3727],
        [-4.2326, -0.3826],
        [-4.2560, -0.3592],
        [-4.2668, -0.3484]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 1.2270e-06],
        [6.8967e-01, 3.1033e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0195, 0.9805], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1550, 0.1716],
         [0.7068, 0.1680]],

        [[0.0564, 0.1596],
         [0.5535, 0.0233]],

        [[0.1899, 0.1500],
         [0.2075, 0.7547]],

        [[0.0064, 0.1721],
         [0.3667, 0.8835]],

        [[0.1043, 0.2920],
         [0.4196, 0.3176]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: -0.005513710431743218
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: -0.006096049957872825
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: -0.00036009454354445005
Average Adjusted Rand Index: -0.002264129112568802
Iteration 0: Loss = -21980.25877946411
Iteration 10: Loss = -10834.358262142645
Iteration 20: Loss = -10834.04960362211
Iteration 30: Loss = -10833.971073503399
Iteration 40: Loss = -10833.96098683279
Iteration 50: Loss = -10833.963157597076
1
Iteration 60: Loss = -10833.967930293153
2
Iteration 70: Loss = -10833.974104020637
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.0264, 0.9736],
        [0.1093, 0.8907]], dtype=torch.float64)
alpha: tensor([0.1026, 0.8974])
beta: tensor([[[0.1771, 0.1827],
         [0.1219, 0.1551]],

        [[0.7485, 0.1616],
         [0.2590, 0.3516]],

        [[0.5891, 0.1500],
         [0.1439, 0.7433]],

        [[0.3074, 0.1504],
         [0.3775, 0.6452]],

        [[0.4684, 0.1810],
         [0.4298, 0.8818]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21979.38418643543
Iteration 100: Loss = -10884.69699464954
Iteration 200: Loss = -10859.917564834572
Iteration 300: Loss = -10847.35423558264
Iteration 400: Loss = -10840.406179907883
Iteration 500: Loss = -10835.372839498628
Iteration 600: Loss = -10833.308231432238
Iteration 700: Loss = -10832.82121893906
Iteration 800: Loss = -10832.59329842423
Iteration 900: Loss = -10832.444077429636
Iteration 1000: Loss = -10832.333792244348
Iteration 1100: Loss = -10832.248531011313
Iteration 1200: Loss = -10832.181047174012
Iteration 1300: Loss = -10832.126597312612
Iteration 1400: Loss = -10832.082122401422
Iteration 1500: Loss = -10832.045394631845
Iteration 1600: Loss = -10832.014603255097
Iteration 1700: Loss = -10831.98865478116
Iteration 1800: Loss = -10831.966553417838
Iteration 1900: Loss = -10831.947551511443
Iteration 2000: Loss = -10831.93113919629
Iteration 2100: Loss = -10831.916779608917
Iteration 2200: Loss = -10831.90418070752
Iteration 2300: Loss = -10831.893066572342
Iteration 2400: Loss = -10831.88324817322
Iteration 2500: Loss = -10831.87448707988
Iteration 2600: Loss = -10831.86667300849
Iteration 2700: Loss = -10831.859612369859
Iteration 2800: Loss = -10831.85328956013
Iteration 2900: Loss = -10831.847562484218
Iteration 3000: Loss = -10831.842384132464
Iteration 3100: Loss = -10831.837669202223
Iteration 3200: Loss = -10831.833434357062
Iteration 3300: Loss = -10831.829486935661
Iteration 3400: Loss = -10831.825894459236
Iteration 3500: Loss = -10831.822588407927
Iteration 3600: Loss = -10831.819584375993
Iteration 3700: Loss = -10831.81683932027
Iteration 3800: Loss = -10831.814219021524
Iteration 3900: Loss = -10831.811862558052
Iteration 4000: Loss = -10831.809693017241
Iteration 4100: Loss = -10831.807599885877
Iteration 4200: Loss = -10831.80572983127
Iteration 4300: Loss = -10831.803998638215
Iteration 4400: Loss = -10831.802337220404
Iteration 4500: Loss = -10831.800861831574
Iteration 4600: Loss = -10831.799456991494
Iteration 4700: Loss = -10831.79814368922
Iteration 4800: Loss = -10831.79691919744
Iteration 4900: Loss = -10831.795829665873
Iteration 5000: Loss = -10831.794754459133
Iteration 5100: Loss = -10831.793798460387
Iteration 5200: Loss = -10831.792929174671
Iteration 5300: Loss = -10831.792142839766
Iteration 5400: Loss = -10831.79134227901
Iteration 5500: Loss = -10831.790634709901
Iteration 5600: Loss = -10831.790009565831
Iteration 5700: Loss = -10831.789438728427
Iteration 5800: Loss = -10831.788895552112
Iteration 5900: Loss = -10831.788380186044
Iteration 6000: Loss = -10831.789380229799
1
Iteration 6100: Loss = -10831.787468293205
Iteration 6200: Loss = -10831.787082119263
Iteration 6300: Loss = -10831.786720010326
Iteration 6400: Loss = -10831.786463449762
Iteration 6500: Loss = -10831.786087425404
Iteration 6600: Loss = -10831.785793536486
Iteration 6700: Loss = -10831.79350601681
1
Iteration 6800: Loss = -10831.785266465364
Iteration 6900: Loss = -10831.78501174426
Iteration 7000: Loss = -10831.78478985929
Iteration 7100: Loss = -10831.784705876065
Iteration 7200: Loss = -10831.784370783449
Iteration 7300: Loss = -10831.78440024587
1
Iteration 7400: Loss = -10831.783878607572
Iteration 7500: Loss = -10831.787833082295
1
Iteration 7600: Loss = -10831.78372436329
Iteration 7700: Loss = -10831.783442275331
Iteration 7800: Loss = -10831.796107997665
1
Iteration 7900: Loss = -10831.783227305013
Iteration 8000: Loss = -10831.783133033461
Iteration 8100: Loss = -10831.783011046973
Iteration 8200: Loss = -10831.784567720744
1
Iteration 8300: Loss = -10831.783186152845
2
Iteration 8400: Loss = -10831.782751196628
Iteration 8500: Loss = -10831.783672946154
1
Iteration 8600: Loss = -10831.782566668653
Iteration 8700: Loss = -10831.800254845539
1
Iteration 8800: Loss = -10831.782446272799
Iteration 8900: Loss = -10831.782351102067
Iteration 9000: Loss = -10831.784195562373
1
Iteration 9100: Loss = -10831.782248320562
Iteration 9200: Loss = -10831.92497074429
1
Iteration 9300: Loss = -10831.782088003281
Iteration 9400: Loss = -10831.782041206765
Iteration 9500: Loss = -10831.782372310074
1
Iteration 9600: Loss = -10831.781933524868
Iteration 9700: Loss = -10831.822647960296
1
Iteration 9800: Loss = -10831.781845675036
Iteration 9900: Loss = -10831.781775294166
Iteration 10000: Loss = -10831.80393244866
1
Iteration 10100: Loss = -10831.781686217631
Iteration 10200: Loss = -10831.783426120004
1
Iteration 10300: Loss = -10831.782302820235
2
Iteration 10400: Loss = -10831.82105862301
3
Iteration 10500: Loss = -10831.781594047616
Iteration 10600: Loss = -10831.811829058362
1
Iteration 10700: Loss = -10831.781527419595
Iteration 10800: Loss = -10831.79277777483
1
Iteration 10900: Loss = -10831.781509871322
Iteration 11000: Loss = -10831.78165213766
1
Iteration 11100: Loss = -10831.78257709491
2
Iteration 11200: Loss = -10831.781443426022
Iteration 11300: Loss = -10831.781617380475
1
Iteration 11400: Loss = -10831.78318103525
2
Iteration 11500: Loss = -10831.78136449097
Iteration 11600: Loss = -10831.781474962543
1
Iteration 11700: Loss = -10831.78125665715
Iteration 11800: Loss = -10831.781261558635
1
Iteration 11900: Loss = -10831.781280891442
2
Iteration 12000: Loss = -10831.78124687677
Iteration 12100: Loss = -10831.781257546838
1
Iteration 12200: Loss = -10831.78204534956
2
Iteration 12300: Loss = -10831.78265698086
3
Iteration 12400: Loss = -10831.787612998643
4
Iteration 12500: Loss = -10831.781355787138
5
Iteration 12600: Loss = -10831.899447332864
6
Iteration 12700: Loss = -10831.781212231974
Iteration 12800: Loss = -10831.781198196877
Iteration 12900: Loss = -10831.78157201715
1
Iteration 13000: Loss = -10831.781069735
Iteration 13100: Loss = -10831.783648913262
1
Iteration 13200: Loss = -10831.781022548972
Iteration 13300: Loss = -10831.990928342386
1
Iteration 13400: Loss = -10831.781004125633
Iteration 13500: Loss = -10831.78099062507
Iteration 13600: Loss = -10831.884917898915
1
Iteration 13700: Loss = -10831.780966963412
Iteration 13800: Loss = -10831.783358527164
1
Iteration 13900: Loss = -10831.798515680084
2
Iteration 14000: Loss = -10831.784665155477
3
Iteration 14100: Loss = -10831.789463082161
4
Iteration 14200: Loss = -10831.783109870741
5
Iteration 14300: Loss = -10831.801558961888
6
Iteration 14400: Loss = -10831.781373354335
7
Iteration 14500: Loss = -10831.780655734205
Iteration 14600: Loss = -10831.804741672688
1
Iteration 14700: Loss = -10831.777882118145
Iteration 14800: Loss = -10831.770018241461
Iteration 14900: Loss = -10831.778477455404
1
Iteration 15000: Loss = -10831.752821863767
Iteration 15100: Loss = -10831.72855248956
Iteration 15200: Loss = -10831.675729271732
Iteration 15300: Loss = -10831.604213520639
Iteration 15400: Loss = -10831.602019845492
Iteration 15500: Loss = -10831.601288655154
Iteration 15600: Loss = -10831.601823043165
1
Iteration 15700: Loss = -10831.600214564953
Iteration 15800: Loss = -10831.60427283832
1
Iteration 15900: Loss = -10831.600024140454
Iteration 16000: Loss = -10831.601047185892
1
Iteration 16100: Loss = -10831.599794437747
Iteration 16200: Loss = -10831.706280857949
1
Iteration 16300: Loss = -10831.599970966745
2
Iteration 16400: Loss = -10831.600417687445
3
Iteration 16500: Loss = -10831.636912067128
4
Iteration 16600: Loss = -10831.599645125058
Iteration 16700: Loss = -10831.599902441814
1
Iteration 16800: Loss = -10831.602349063434
2
Iteration 16900: Loss = -10831.601524099568
3
Iteration 17000: Loss = -10831.599532099324
Iteration 17100: Loss = -10831.600194089297
1
Iteration 17200: Loss = -10831.601768334654
2
Iteration 17300: Loss = -10831.600116299513
3
Iteration 17400: Loss = -10831.59970625906
4
Iteration 17500: Loss = -10831.599965150966
5
Iteration 17600: Loss = -10831.637156071758
6
Iteration 17700: Loss = -10831.599670164493
7
Iteration 17800: Loss = -10831.60370790146
8
Iteration 17900: Loss = -10831.642347345945
9
Iteration 18000: Loss = -10831.711244330207
10
Stopping early at iteration 18000 due to no improvement.
tensor([[ 2.6734, -5.5220],
        [ 3.4514, -5.0286],
        [ 3.2546, -4.7608],
        [ 3.3802, -4.8574],
        [ 3.5350, -4.9548],
        [ 3.4559, -4.9538],
        [ 3.3260, -4.9550],
        [ 3.0187, -5.2296],
        [ 3.3467, -4.7349],
        [ 2.9840, -5.0581],
        [ 3.2885, -4.8697],
        [ 3.3674, -4.8930],
        [ 3.2121, -5.3351],
        [ 2.6143, -5.3533],
        [ 3.1619, -5.2468],
        [ 3.0579, -4.9057],
        [ 2.9754, -4.9459],
        [ 3.1904, -4.8879],
        [ 3.2003, -4.7013],
        [ 3.2748, -4.8880],
        [ 3.2845, -4.9361],
        [ 3.4992, -4.9671],
        [ 2.9228, -5.2113],
        [ 3.1007, -4.8837],
        [ 3.3866, -4.9687],
        [ 3.3735, -4.8622],
        [ 3.2203, -4.9508],
        [ 2.9890, -4.8703],
        [ 3.2590, -4.6496],
        [ 3.4383, -4.8283],
        [ 2.4265, -5.5327],
        [ 3.1523, -5.1305],
        [ 4.7065, -6.1655],
        [ 3.2064, -4.8627],
        [ 1.7784, -6.2647],
        [ 3.3474, -4.9304],
        [ 2.9308, -5.4338],
        [ 3.2920, -5.0519],
        [ 2.9409, -5.2402],
        [ 3.0621, -4.6955],
        [ 3.3699, -4.8915],
        [ 3.3781, -4.8702],
        [ 3.3217, -5.1664],
        [ 3.3012, -4.6921],
        [ 3.1347, -5.0781],
        [ 3.2978, -4.8467],
        [ 2.7841, -5.6105],
        [ 3.2436, -4.9958],
        [ 3.1966, -5.0162],
        [ 2.9534, -5.1225],
        [ 3.2291, -5.0053],
        [ 2.6038, -5.5289],
        [ 3.3849, -5.1303],
        [ 2.7412, -5.5026],
        [ 2.8727, -5.0676],
        [ 3.3247, -5.0832],
        [ 3.0842, -5.2706],
        [ 2.2234, -6.3179],
        [ 3.3736, -4.7846],
        [ 3.4002, -4.9337],
        [ 3.0019, -4.9396],
        [ 2.9101, -5.4742],
        [ 2.9937, -5.1919],
        [ 2.8501, -5.1501],
        [ 2.5207, -5.4909],
        [ 3.4352, -4.8705],
        [ 3.3837, -4.7792],
        [ 3.1848, -4.8278],
        [ 2.9679, -4.9631],
        [ 1.7964, -6.4116],
        [ 3.3496, -4.7973],
        [ 3.3923, -4.8653],
        [ 3.3519, -4.8768],
        [ 3.3639, -4.8256],
        [ 3.2709, -4.7355],
        [ 3.2914, -4.9807],
        [ 3.0761, -4.9684],
        [ 2.3037, -6.1549],
        [ 3.2089, -4.8106],
        [ 3.2610, -4.6479],
        [ 3.3951, -4.8115],
        [ 3.2935, -4.9336],
        [ 3.3393, -4.7406],
        [ 3.3647, -4.7837],
        [ 3.1224, -4.6144],
        [ 3.4886, -5.0590],
        [ 3.2519, -4.7347],
        [ 3.2618, -5.0354],
        [ 3.0547, -5.0057],
        [ 3.3080, -4.7981],
        [ 2.4235, -5.6565],
        [ 3.3470, -4.7396],
        [ 1.8149, -6.1207],
        [ 3.1762, -4.6335],
        [ 3.3172, -4.9186],
        [ 3.1987, -5.1684],
        [ 3.2088, -4.9068],
        [ 3.3679, -4.7543],
        [ 3.2383, -4.9081],
        [ 2.4232, -5.8218]], dtype=torch.float64, requires_grad=True)
pi: tensor([[3.0861e-01, 6.9139e-01],
        [5.7727e-06, 9.9999e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9971e-01, 2.9125e-04], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1670, 0.1752],
         [0.1219, 0.1560]],

        [[0.7485, 0.1585],
         [0.2590, 0.3516]],

        [[0.5891, 0.1491],
         [0.1439, 0.7433]],

        [[0.3074, 0.1712],
         [0.3775, 0.6452]],

        [[0.4684, 0.2924],
         [0.4298, 0.8818]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: -0.005513710431743218
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: -0.006096049957872825
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: -0.00036009454354445005
Average Adjusted Rand Index: -0.002264129112568802
Iteration 0: Loss = -19883.057090993756
Iteration 10: Loss = -10833.901249924851
Iteration 20: Loss = -10833.899498381055
Iteration 30: Loss = -10833.900409437676
1
Iteration 40: Loss = -10833.901947587674
2
Iteration 50: Loss = -10833.903804203792
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7945, 0.2055],
        [0.8032, 0.1968]], dtype=torch.float64)
alpha: tensor([0.7947, 0.2053])
beta: tensor([[[0.1547, 0.1739],
         [0.2764, 0.1676]],

        [[0.9854, 0.1590],
         [0.1831, 0.0398]],

        [[0.4362, 0.1518],
         [0.4308, 0.8337]],

        [[0.9328, 0.1517],
         [0.7835, 0.2323]],

        [[0.4633, 0.1677],
         [0.7744, 0.7478]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19882.77284795974
Iteration 100: Loss = -10913.441935802817
Iteration 200: Loss = -10859.29154381818
Iteration 300: Loss = -10841.491685206485
Iteration 400: Loss = -10835.920437246557
Iteration 500: Loss = -10833.710590167715
Iteration 600: Loss = -10833.280416268504
Iteration 700: Loss = -10833.053117692389
Iteration 800: Loss = -10832.91083509266
Iteration 900: Loss = -10832.81396671906
Iteration 1000: Loss = -10832.744218341864
Iteration 1100: Loss = -10832.69177604201
Iteration 1200: Loss = -10832.650971234578
Iteration 1300: Loss = -10832.61805698515
Iteration 1400: Loss = -10832.589909709324
Iteration 1500: Loss = -10832.563466631045
Iteration 1600: Loss = -10832.524992365543
Iteration 1700: Loss = -10832.484732594461
Iteration 1800: Loss = -10832.433890055088
Iteration 1900: Loss = -10832.372377514954
Iteration 2000: Loss = -10832.320679098271
Iteration 2100: Loss = -10832.292333661251
Iteration 2200: Loss = -10832.314605233893
1
Iteration 2300: Loss = -10832.271066449435
Iteration 2400: Loss = -10832.259009328725
Iteration 2500: Loss = -10832.230556817323
Iteration 2600: Loss = -10832.254590269984
1
Iteration 2700: Loss = -10832.189942332678
Iteration 2800: Loss = -10832.173238086581
Iteration 2900: Loss = -10832.163071029538
Iteration 3000: Loss = -10832.152679803818
Iteration 3100: Loss = -10832.144365008644
Iteration 3200: Loss = -10832.134034545144
Iteration 3300: Loss = -10832.124226107484
Iteration 3400: Loss = -10832.112464288193
Iteration 3500: Loss = -10832.103759599782
Iteration 3600: Loss = -10832.342075842673
1
Iteration 3700: Loss = -10832.084749960824
Iteration 3800: Loss = -10832.0739285857
Iteration 3900: Loss = -10832.063470021681
Iteration 4000: Loss = -10832.055137075464
Iteration 4100: Loss = -10832.046087539618
Iteration 4200: Loss = -10832.037208381422
Iteration 4300: Loss = -10832.02795601437
Iteration 4400: Loss = -10832.01925916698
Iteration 4500: Loss = -10832.010776938241
Iteration 4600: Loss = -10832.070577160755
1
Iteration 4700: Loss = -10831.996234814187
Iteration 4800: Loss = -10831.98416470763
Iteration 4900: Loss = -10831.971453070297
Iteration 5000: Loss = -10831.957298616755
Iteration 5100: Loss = -10831.94246474559
Iteration 5200: Loss = -10831.929324264938
Iteration 5300: Loss = -10831.905485442701
Iteration 5400: Loss = -10831.882908439551
Iteration 5500: Loss = -10832.089065746713
1
Iteration 5600: Loss = -10831.82995408533
Iteration 5700: Loss = -10831.800861261285
Iteration 5800: Loss = -10831.791849288406
Iteration 5900: Loss = -10831.80585340095
1
Iteration 6000: Loss = -10831.77510832375
Iteration 6100: Loss = -10831.773141502765
Iteration 6200: Loss = -10831.777521547736
1
Iteration 6300: Loss = -10831.772535616898
Iteration 6400: Loss = -10831.773286574005
1
Iteration 6500: Loss = -10831.772223316155
Iteration 6600: Loss = -10831.772019848475
Iteration 6700: Loss = -10831.772093080634
1
Iteration 6800: Loss = -10831.771677125891
Iteration 6900: Loss = -10831.771447037427
Iteration 7000: Loss = -10831.771280166759
Iteration 7100: Loss = -10831.771042581238
Iteration 7200: Loss = -10831.781709131386
1
Iteration 7300: Loss = -10831.77064629619
Iteration 7400: Loss = -10831.7712766858
1
Iteration 7500: Loss = -10831.770237155348
Iteration 7600: Loss = -10831.790235876702
1
Iteration 7700: Loss = -10831.773064060819
2
Iteration 7800: Loss = -10831.769800645876
Iteration 7900: Loss = -10831.832723135403
1
Iteration 8000: Loss = -10831.768950257298
Iteration 8100: Loss = -10831.80314793332
1
Iteration 8200: Loss = -10831.768354614911
Iteration 8300: Loss = -10831.768171655029
Iteration 8400: Loss = -10831.767491133975
Iteration 8500: Loss = -10831.767068188548
Iteration 8600: Loss = -10831.766548588928
Iteration 8700: Loss = -10831.766375485871
Iteration 8800: Loss = -10831.765669924951
Iteration 8900: Loss = -10831.779260759093
1
Iteration 9000: Loss = -10831.76423048051
Iteration 9100: Loss = -10831.764875556342
1
Iteration 9200: Loss = -10831.76261358422
Iteration 9300: Loss = -10831.989038356629
1
Iteration 9400: Loss = -10831.760107788563
Iteration 9500: Loss = -10831.758271265322
Iteration 9600: Loss = -10831.755365580644
Iteration 9700: Loss = -10831.774288423594
1
Iteration 9800: Loss = -10831.766783690635
2
Iteration 9900: Loss = -10831.801876395237
3
Iteration 10000: Loss = -10831.714639698574
Iteration 10100: Loss = -10831.662300163947
Iteration 10200: Loss = -10831.634434713049
Iteration 10300: Loss = -10831.610918019125
Iteration 10400: Loss = -10831.605489400965
Iteration 10500: Loss = -10831.622958555394
1
Iteration 10600: Loss = -10831.603171974704
Iteration 10700: Loss = -10831.603344554165
1
Iteration 10800: Loss = -10831.736132331947
2
Iteration 10900: Loss = -10831.601956327617
Iteration 11000: Loss = -10831.602604926306
1
Iteration 11100: Loss = -10831.627144326387
2
Iteration 11200: Loss = -10831.60466721534
3
Iteration 11300: Loss = -10831.624228475364
4
Iteration 11400: Loss = -10831.62178717248
5
Iteration 11500: Loss = -10831.600489722448
Iteration 11600: Loss = -10831.603526535717
1
Iteration 11700: Loss = -10831.600341962125
Iteration 11800: Loss = -10831.605116744624
1
Iteration 11900: Loss = -10831.600253195069
Iteration 12000: Loss = -10831.686506201
1
Iteration 12100: Loss = -10831.600098313367
Iteration 12200: Loss = -10831.617640364762
1
Iteration 12300: Loss = -10831.60004122155
Iteration 12400: Loss = -10831.601724164038
1
Iteration 12500: Loss = -10831.599924806274
Iteration 12600: Loss = -10831.599887578926
Iteration 12700: Loss = -10831.600215010689
1
Iteration 12800: Loss = -10831.601170592943
2
Iteration 12900: Loss = -10831.643458141587
3
Iteration 13000: Loss = -10831.600690246976
4
Iteration 13100: Loss = -10831.599825936442
Iteration 13200: Loss = -10831.603263020537
1
Iteration 13300: Loss = -10831.62656450482
2
Iteration 13400: Loss = -10831.599698513355
Iteration 13500: Loss = -10831.600206987738
1
Iteration 13600: Loss = -10831.601189943665
2
Iteration 13700: Loss = -10831.600183690074
3
Iteration 13800: Loss = -10831.60495053611
4
Iteration 13900: Loss = -10831.600815308324
5
Iteration 14000: Loss = -10831.599666638811
Iteration 14100: Loss = -10831.600696453854
1
Iteration 14200: Loss = -10831.599801086495
2
Iteration 14300: Loss = -10831.613463817668
3
Iteration 14400: Loss = -10831.600214258058
4
Iteration 14500: Loss = -10831.600554587267
5
Iteration 14600: Loss = -10831.607113119866
6
Iteration 14700: Loss = -10831.606430625656
7
Iteration 14800: Loss = -10831.60065738709
8
Iteration 14900: Loss = -10831.599656644945
Iteration 15000: Loss = -10831.599831152285
1
Iteration 15100: Loss = -10831.59982907428
2
Iteration 15200: Loss = -10831.61155130881
3
Iteration 15300: Loss = -10831.657756182092
4
Iteration 15400: Loss = -10831.694078082573
5
Iteration 15500: Loss = -10831.59953224022
Iteration 15600: Loss = -10831.60025052227
1
Iteration 15700: Loss = -10831.623707706674
2
Iteration 15800: Loss = -10831.599483139582
Iteration 15900: Loss = -10831.60969647643
1
Iteration 16000: Loss = -10831.599822964226
2
Iteration 16100: Loss = -10831.601051379705
3
Iteration 16200: Loss = -10831.625436826822
4
Iteration 16300: Loss = -10831.600804541846
5
Iteration 16400: Loss = -10831.599502523804
6
Iteration 16500: Loss = -10831.599525104864
7
Iteration 16600: Loss = -10831.599565265587
8
Iteration 16700: Loss = -10831.601384246518
9
Iteration 16800: Loss = -10831.600977004815
10
Stopping early at iteration 16800 due to no improvement.
tensor([[ -8.5858,   6.8753],
        [-10.6555,   8.9405],
        [-10.4479,   8.3335],
        [ -9.0932,   6.3505],
        [-10.8384,   8.3984],
        [-10.5775,   8.7674],
        [ -8.8851,   6.7334],
        [ -9.0657,   7.5554],
        [ -8.9894,   6.5816],
        [ -9.6542,   5.8954],
        [ -8.5263,   6.9541],
        [ -8.6308,   6.9831],
        [-10.2873,   8.3657],
        [ -8.5125,   6.9042],
        [-10.1409,   8.6935],
        [ -9.8704,   5.6142],
        [-10.2559,   8.2946],
        [ -9.0158,   6.5557],
        [ -8.6649,   6.6474],
        [ -8.8259,   6.8222],
        [ -8.3121,   6.8180],
        [ -8.7897,   6.9831],
        [-10.8806,   7.5402],
        [ -9.4228,   6.5636],
        [ -9.9807,   5.8273],
        [-10.8524,   8.0348],
        [ -8.6273,   6.9097],
        [ -9.5375,   5.8667],
        [ -8.4802,   6.5558],
        [ -8.6073,   6.8644],
        [ -9.2385,   5.9479],
        [ -9.5681,   8.0238],
        [ -9.5188,   6.8510],
        [ -9.1391,   6.3255],
        [ -8.4347,   6.8095],
        [ -9.5296,   8.1432],
        [ -8.7997,   6.8022],
        [ -8.4544,   7.0412],
        [ -9.7605,   8.3680],
        [ -8.6126,   6.6253],
        [ -8.2857,   6.6165],
        [ -8.4516,   6.8632],
        [ -8.5137,   7.1031],
        [ -8.3884,   6.9467],
        [ -9.9049,   8.1800],
        [ -8.6793,   6.7554],
        [ -8.8556,   6.8605],
        [ -8.3367,   6.8486],
        [ -9.0087,   6.6466],
        [ -8.4915,   6.9479],
        [ -8.4599,   7.0057],
        [ -8.6442,   6.6981],
        [-10.1630,   8.7180],
        [ -8.4743,   7.0876],
        [ -8.4952,   6.8004],
        [ -8.4347,   6.8620],
        [ -8.4293,   7.0414],
        [ -8.3612,   6.8935],
        [ -8.3823,   6.9839],
        [-10.0069,   8.5702],
        [ -9.7980,   8.2791],
        [ -8.5711,   7.1413],
        [ -8.9500,   6.4746],
        [ -8.1205,   6.6624],
        [ -8.5373,   6.8084],
        [ -8.3933,   6.9973],
        [ -8.3835,   6.9800],
        [ -8.4645,   6.9338],
        [ -8.9565,   6.1508],
        [ -8.4335,   6.8561],
        [ -8.9619,   6.6142],
        [ -9.0733,   6.3918],
        [-10.0489,   8.6566],
        [ -9.6216,   6.0012],
        [-11.4934,   7.6360],
        [ -8.4350,   7.0402],
        [ -8.4286,   6.9884],
        [-10.3803,   7.7560],
        [ -8.5817,   6.5837],
        [ -8.2743,   6.8269],
        [ -8.4694,   6.9507],
        [ -9.7862,   5.8084],
        [ -8.4409,   6.6461],
        [ -9.7537,   8.0444],
        [-11.5402,   7.9957],
        [-10.1283,   8.2949],
        [ -8.6795,   6.6899],
        [ -9.1141,   6.3825],
        [-10.5568,   8.6454],
        [ -8.3861,   6.9330],
        [ -8.4085,   7.0082],
        [ -8.4328,   7.0181],
        [ -8.3425,   6.8647],
        [-10.1782,   8.7893],
        [-10.2356,   8.6073],
        [ -8.7594,   6.9935],
        [-10.3784,   8.7708],
        [ -9.2344,   6.3531],
        [ -8.2663,   6.8756],
        [ -8.5807,   6.9832]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 2.7530e-06],
        [6.9007e-01, 3.0993e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.5372e-07, 1.0000e+00], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1549, 0.1736],
         [0.2764, 0.1681]],

        [[0.9854, 0.1596],
         [0.1831, 0.0398]],

        [[0.4362, 0.1502],
         [0.4308, 0.8337]],

        [[0.9328, 0.1719],
         [0.7835, 0.2323]],

        [[0.4633, 0.2911],
         [0.7744, 0.7478]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: -0.005513710431743218
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: -0.006096049957872825
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: -0.00036009454354445005
Average Adjusted Rand Index: -0.002264129112568802
Iteration 0: Loss = -16199.667626412847
Iteration 10: Loss = -10833.919311671067
Iteration 20: Loss = -10833.902238890569
Iteration 30: Loss = -10833.899276041286
Iteration 40: Loss = -10833.89881580827
Iteration 50: Loss = -10833.898744547458
Iteration 60: Loss = -10833.89874570956
1
Iteration 70: Loss = -10833.898735236755
Iteration 80: Loss = -10833.898771248334
1
Iteration 90: Loss = -10833.898769298994
2
Iteration 100: Loss = -10833.898777093786
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.5805, 0.4195],
        [0.3756, 0.6244]], dtype=torch.float64)
alpha: tensor([0.4725, 0.5275])
beta: tensor([[[0.1577, 0.1668],
         [0.8491, 0.1569]],

        [[0.3203, 0.1562],
         [0.4239, 0.0291]],

        [[0.1433, 0.1514],
         [0.5174, 0.2032]],

        [[0.0351, 0.1514],
         [0.2947, 0.2142]],

        [[0.8378, 0.1606],
         [0.6264, 0.2702]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16199.334116729897
Iteration 100: Loss = -10871.722553173637
Iteration 200: Loss = -10836.032582218846
Iteration 300: Loss = -10834.820498314324
Iteration 400: Loss = -10834.394557423382
Iteration 500: Loss = -10833.876790667207
Iteration 600: Loss = -10833.216567953674
Iteration 700: Loss = -10832.789224711714
Iteration 800: Loss = -10832.484011136146
Iteration 900: Loss = -10832.30928342302
Iteration 1000: Loss = -10832.208385451591
Iteration 1100: Loss = -10832.14358901719
Iteration 1200: Loss = -10832.102389018968
Iteration 1300: Loss = -10832.074010778893
Iteration 1400: Loss = -10832.053302695062
Iteration 1500: Loss = -10832.037224336638
Iteration 1600: Loss = -10832.02299525305
Iteration 1700: Loss = -10832.006926991151
Iteration 1800: Loss = -10831.99032134462
Iteration 1900: Loss = -10831.976881227187
Iteration 2000: Loss = -10831.970744363869
Iteration 2100: Loss = -10831.966366092212
Iteration 2200: Loss = -10831.962780584336
Iteration 2300: Loss = -10831.95973005781
Iteration 2400: Loss = -10831.956954768179
Iteration 2500: Loss = -10831.954297121141
Iteration 2600: Loss = -10831.951263677009
Iteration 2700: Loss = -10831.948307267781
Iteration 2800: Loss = -10831.945776242415
Iteration 2900: Loss = -10831.933448485996
Iteration 3000: Loss = -10831.924468930132
Iteration 3100: Loss = -10831.916766111202
Iteration 3200: Loss = -10831.907970347964
Iteration 3300: Loss = -10831.904521170192
Iteration 3400: Loss = -10831.896109633635
Iteration 3500: Loss = -10831.891855825146
Iteration 3600: Loss = -10831.888460290651
Iteration 3700: Loss = -10831.877059557102
Iteration 3800: Loss = -10831.870983559336
Iteration 3900: Loss = -10831.866403147113
Iteration 4000: Loss = -10831.857299295136
Iteration 4100: Loss = -10831.837140852685
Iteration 4200: Loss = -10831.831125100995
Iteration 4300: Loss = -10831.82744148712
Iteration 4400: Loss = -10831.801556479957
Iteration 4500: Loss = -10831.808647515081
1
Iteration 4600: Loss = -10831.790787577078
Iteration 4700: Loss = -10831.790382458477
Iteration 4800: Loss = -10831.784024397626
Iteration 4900: Loss = -10831.771718739545
Iteration 5000: Loss = -10831.758029417762
Iteration 5100: Loss = -10831.73465761985
Iteration 5200: Loss = -10831.723962507938
Iteration 5300: Loss = -10831.740585926093
1
Iteration 5400: Loss = -10831.64305594808
Iteration 5500: Loss = -10831.627717176452
Iteration 5600: Loss = -10831.619019294969
Iteration 5700: Loss = -10831.614219744815
Iteration 5800: Loss = -10831.610597837449
Iteration 5900: Loss = -10831.609542937967
Iteration 6000: Loss = -10831.60824935774
Iteration 6100: Loss = -10831.606551354367
Iteration 6200: Loss = -10831.604181560753
Iteration 6300: Loss = -10831.603367315258
Iteration 6400: Loss = -10831.603038069125
Iteration 6500: Loss = -10831.602151690577
Iteration 6600: Loss = -10831.6014098234
Iteration 6700: Loss = -10831.60179720821
1
Iteration 6800: Loss = -10831.600608798524
Iteration 6900: Loss = -10831.60337800751
1
Iteration 7000: Loss = -10831.602982548056
2
Iteration 7100: Loss = -10831.599701740874
Iteration 7200: Loss = -10831.604746979787
1
Iteration 7300: Loss = -10831.59925872562
Iteration 7400: Loss = -10831.599410202085
1
Iteration 7500: Loss = -10831.599866522238
2
Iteration 7600: Loss = -10831.6170666171
3
Iteration 7700: Loss = -10831.602750182548
4
Iteration 7800: Loss = -10831.637737968631
5
Iteration 7900: Loss = -10831.599207708428
Iteration 8000: Loss = -10831.60364152178
1
Iteration 8100: Loss = -10831.598144742606
Iteration 8200: Loss = -10831.604101716495
1
Iteration 8300: Loss = -10831.598328213122
2
Iteration 8400: Loss = -10831.597841812283
Iteration 8500: Loss = -10831.615993796644
1
Iteration 8600: Loss = -10831.597707279016
Iteration 8700: Loss = -10831.597935001906
1
Iteration 8800: Loss = -10831.597675564451
Iteration 8900: Loss = -10831.59825639411
1
Iteration 9000: Loss = -10831.597768056205
2
Iteration 9100: Loss = -10831.700661354895
3
Iteration 9200: Loss = -10831.597665209358
Iteration 9300: Loss = -10831.597349656127
Iteration 9400: Loss = -10831.603050094267
1
Iteration 9500: Loss = -10831.597265628505
Iteration 9600: Loss = -10831.730578051933
1
Iteration 9700: Loss = -10831.597194120051
Iteration 9800: Loss = -10831.606761647286
1
Iteration 9900: Loss = -10831.597194036649
Iteration 10000: Loss = -10831.597406829782
1
Iteration 10100: Loss = -10831.597754874005
2
Iteration 10200: Loss = -10831.602050356974
3
Iteration 10300: Loss = -10831.597033890303
Iteration 10400: Loss = -10831.616578393143
1
Iteration 10500: Loss = -10831.59730331594
2
Iteration 10600: Loss = -10831.616413562497
3
Iteration 10700: Loss = -10831.597513252622
4
Iteration 10800: Loss = -10831.596926272792
Iteration 10900: Loss = -10831.597006461137
1
Iteration 11000: Loss = -10831.597069060652
2
Iteration 11100: Loss = -10831.597008576276
3
Iteration 11200: Loss = -10831.597407735973
4
Iteration 11300: Loss = -10831.614512977849
5
Iteration 11400: Loss = -10831.601867253565
6
Iteration 11500: Loss = -10831.63687397195
7
Iteration 11600: Loss = -10831.610423834707
8
Iteration 11700: Loss = -10831.60829455126
9
Iteration 11800: Loss = -10831.604137730294
10
Stopping early at iteration 11800 due to no improvement.
tensor([[-3.1059,  0.8588],
        [-2.8924,  1.2099],
        [-4.2505, -0.3647],
        [-2.7361,  1.2021],
        [-2.9488,  1.2123],
        [-2.8588,  1.2013],
        [-2.6860,  1.2908],
        [-3.0535,  0.8978],
        [-2.8137,  1.0449],
        [-2.7173,  1.1803],
        [-2.6634,  1.2437],
        [-3.5091,  0.4779],
        [-3.8510,  0.2925],
        [-3.2753,  0.5372],
        [-2.9801,  1.0751],
        [-2.8182,  1.0233],
        [-2.6412,  1.1826],
        [-2.7855,  1.1152],
        [-2.6007,  1.2049],
        [-2.7251,  1.2799],
        [-2.8495,  1.1421],
        [-3.6082,  0.4543],
        [-2.7717,  1.1755],
        [-2.6260,  1.2356],
        [-3.9846,  0.0729],
        [-3.0418,  0.9434],
        [-2.8454,  1.0985],
        [-2.6213,  1.1380],
        [-2.6186,  1.2140],
        [-4.3016, -0.3136],
        [-2.7465,  1.0667],
        [-2.8618,  1.1490],
        [-4.1560,  2.3267],
        [-2.8182,  1.0591],
        [-2.6857,  1.1864],
        [-2.7133,  1.3077],
        [-3.0983,  0.9154],
        [-4.3183, -0.2969],
        [-2.7230,  1.2576],
        [-2.5441,  1.1578],
        [-2.6985,  1.2807],
        [-2.7760,  1.2478],
        [-3.4439,  0.6169],
        [-2.6379,  1.2106],
        [-2.6741,  1.2634],
        [-2.9112,  1.0341],
        [-2.7377,  1.3509],
        [-2.9274,  1.0399],
        [-2.6834,  1.2800],
        [-2.8684,  1.0706],
        [-2.8289,  1.1614],
        [-2.6655,  1.2436],
        [-2.8760,  1.2321],
        [-2.6644,  1.2757],
        [-3.0544,  0.8092],
        [-2.9042,  1.1521],
        [-3.1168,  0.8863],
        [-3.5236,  0.6448],
        [-2.9424,  0.9876],
        [-2.7662,  1.2085],
        [-2.6193,  1.1912],
        [-3.1668,  0.9170],
        [-3.2386,  0.6982],
        [-2.7938,  1.0164],
        [-2.6500,  1.2098],
        [-2.8157,  1.2142],
        [-2.7052,  1.2394],
        [-2.6944,  1.1444],
        [-2.9222,  0.9056],
        [-2.7307,  1.2285],
        [-3.3096,  0.6104],
        [-2.8931,  1.0645],
        [-3.0988,  0.8574],
        [-2.9532,  1.0098],
        [-2.7888,  1.0718],
        [-2.6990,  1.2636],
        [-3.3742,  0.5182],
        [-2.7396,  1.3406],
        [-2.7333,  1.1164],
        [-2.5991,  1.1863],
        [-2.6948,  1.2873],
        [-3.0164,  0.9648],
        [-2.6464,  1.2308],
        [-2.9417,  1.0055],
        [-2.5611,  1.1235],
        [-2.7565,  1.3570],
        [-3.4479,  0.3900],
        [-3.3434,  0.6274],
        [-2.8282,  1.0317],
        [-2.6993,  1.1861],
        [-2.9765,  0.9059],
        [-2.6595,  1.2517],
        [-3.2622,  0.5483],
        [-2.7908,  0.9721],
        [-2.7453,  1.2395],
        [-3.0450,  0.9964],
        [-2.7951,  1.0999],
        [-2.8960,  0.9769],
        [-2.9839,  0.9417],
        [-2.7877,  1.1560]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9999e-01, 9.8860e-06],
        [6.8600e-01, 3.1400e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0193, 0.9807], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1550, 0.1717],
         [0.8491, 0.1679]],

        [[0.3203, 0.1595],
         [0.4239, 0.0291]],

        [[0.1433, 0.1501],
         [0.5174, 0.2032]],

        [[0.0351, 0.1720],
         [0.2947, 0.2142]],

        [[0.8378, 0.2920],
         [0.6264, 0.2702]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: -0.005513710431743218
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: -0.006096049957872825
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: -0.00036009454354445005
Average Adjusted Rand Index: -0.002264129112568802
Iteration 0: Loss = -21025.47537398927
Iteration 10: Loss = -10834.050765895745
Iteration 20: Loss = -10833.99320484221
Iteration 30: Loss = -10833.985185528993
Iteration 40: Loss = -10833.98998393346
1
Iteration 50: Loss = -10834.001224287953
2
Iteration 60: Loss = -10834.01783971872
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.0807, 0.9193],
        [0.0571, 0.9429]], dtype=torch.float64)
alpha: tensor([0.0604, 0.9396])
beta: tensor([[[0.1877, 0.1887],
         [0.8362, 0.1555]],

        [[0.8839, 0.1638],
         [0.3932, 0.6303]],

        [[0.4949, 0.1474],
         [0.3913, 0.7779]],

        [[0.7038, 0.1485],
         [0.3349, 0.2955]],

        [[0.7847, 0.1996],
         [0.4478, 0.6371]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: 0.0006956275301036035
Average Adjusted Rand Index: -0.0004529465619428516
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21025.207499117612
Iteration 100: Loss = -10844.569294515755
Iteration 200: Loss = -10839.635378559991
Iteration 300: Loss = -10837.602427082338
Iteration 400: Loss = -10836.569287627019
Iteration 500: Loss = -10835.96531175257
Iteration 600: Loss = -10835.572380640591
Iteration 700: Loss = -10835.293861251961
Iteration 800: Loss = -10835.08134683207
Iteration 900: Loss = -10834.908269654507
Iteration 1000: Loss = -10834.759462257602
Iteration 1100: Loss = -10834.629262862369
Iteration 1200: Loss = -10834.521361930918
Iteration 1300: Loss = -10834.4404377124
Iteration 1400: Loss = -10834.382475148228
Iteration 1500: Loss = -10834.339588615796
Iteration 1600: Loss = -10834.306067224146
Iteration 1700: Loss = -10834.278556346864
Iteration 1800: Loss = -10834.255430605444
Iteration 1900: Loss = -10834.235544935727
Iteration 2000: Loss = -10834.218277584987
Iteration 2100: Loss = -10834.20312153384
Iteration 2200: Loss = -10834.189789359543
Iteration 2300: Loss = -10834.177923095964
Iteration 2400: Loss = -10834.167376815676
Iteration 2500: Loss = -10834.157976238084
Iteration 2600: Loss = -10834.149517214923
Iteration 2700: Loss = -10834.141969194798
Iteration 2800: Loss = -10834.135144776472
Iteration 2900: Loss = -10834.129008130245
Iteration 3000: Loss = -10834.123434037314
Iteration 3100: Loss = -10834.11844814694
Iteration 3200: Loss = -10834.113849178037
Iteration 3300: Loss = -10834.109692314707
Iteration 3400: Loss = -10834.105894747474
Iteration 3500: Loss = -10834.102406773727
Iteration 3600: Loss = -10834.099205353046
Iteration 3700: Loss = -10834.096293540688
Iteration 3800: Loss = -10834.093629917634
Iteration 3900: Loss = -10834.091158023488
Iteration 4000: Loss = -10834.088893923754
Iteration 4100: Loss = -10834.086867162989
Iteration 4200: Loss = -10834.08501612803
Iteration 4300: Loss = -10834.083373999216
Iteration 4400: Loss = -10834.081847211874
Iteration 4500: Loss = -10834.101604601385
1
Iteration 4600: Loss = -10834.079279738331
Iteration 4700: Loss = -10834.078244211272
Iteration 4800: Loss = -10834.077294276181
Iteration 4900: Loss = -10834.077710299784
1
Iteration 5000: Loss = -10834.075755627133
Iteration 5100: Loss = -10834.075101120985
Iteration 5200: Loss = -10834.080512439032
1
Iteration 5300: Loss = -10834.07412825037
Iteration 5400: Loss = -10834.073623339935
Iteration 5500: Loss = -10834.073281718276
Iteration 5600: Loss = -10834.091865642804
1
Iteration 5700: Loss = -10834.072627674035
Iteration 5800: Loss = -10834.072364170652
Iteration 5900: Loss = -10834.072103459644
Iteration 6000: Loss = -10834.073165626443
1
Iteration 6100: Loss = -10834.071602184917
Iteration 6200: Loss = -10834.071370458738
Iteration 6300: Loss = -10834.186296907774
1
Iteration 6400: Loss = -10834.070932404436
Iteration 6500: Loss = -10834.070684707272
Iteration 6600: Loss = -10834.07043501796
Iteration 6700: Loss = -10834.13881672176
1
Iteration 6800: Loss = -10834.069990140171
Iteration 6900: Loss = -10834.069772579454
Iteration 7000: Loss = -10834.069537718515
Iteration 7100: Loss = -10834.069821344161
1
Iteration 7200: Loss = -10834.069099961713
Iteration 7300: Loss = -10834.068861768406
Iteration 7400: Loss = -10834.11494866939
1
Iteration 7500: Loss = -10834.06828749829
Iteration 7600: Loss = -10834.067998854818
Iteration 7700: Loss = -10834.067589661718
Iteration 7800: Loss = -10834.067223831951
Iteration 7900: Loss = -10834.06673677292
Iteration 8000: Loss = -10834.066158504833
Iteration 8100: Loss = -10834.128637685228
1
Iteration 8200: Loss = -10834.064741423153
Iteration 8300: Loss = -10834.063697418602
Iteration 8400: Loss = -10834.062349731654
Iteration 8500: Loss = -10834.069598784023
1
Iteration 8600: Loss = -10834.057855330435
Iteration 8700: Loss = -10834.053648572071
Iteration 8800: Loss = -10834.045926620298
Iteration 8900: Loss = -10834.028763465098
Iteration 9000: Loss = -10833.971131473469
Iteration 9100: Loss = -10833.646964996808
Iteration 9200: Loss = -10833.28434403764
Iteration 9300: Loss = -10832.919893188282
Iteration 9400: Loss = -10832.818084603312
Iteration 9500: Loss = -10832.265441042438
Iteration 9600: Loss = -10831.83868632919
Iteration 9700: Loss = -10831.80466477705
Iteration 9800: Loss = -10831.795543583969
Iteration 9900: Loss = -10831.78538649379
Iteration 10000: Loss = -10831.779774884168
Iteration 10100: Loss = -10831.767871645892
Iteration 10200: Loss = -10831.75118773774
Iteration 10300: Loss = -10831.702514181228
Iteration 10400: Loss = -10831.619540771366
Iteration 10500: Loss = -10831.605048309868
Iteration 10600: Loss = -10831.601724308897
Iteration 10700: Loss = -10831.600435248185
Iteration 10800: Loss = -10831.59922837324
Iteration 10900: Loss = -10831.598666890884
Iteration 11000: Loss = -10831.598236060576
Iteration 11100: Loss = -10831.597915940589
Iteration 11200: Loss = -10831.606081977487
1
Iteration 11300: Loss = -10831.637961860735
2
Iteration 11400: Loss = -10831.597353094017
Iteration 11500: Loss = -10831.657229051874
1
Iteration 11600: Loss = -10831.597204541691
Iteration 11700: Loss = -10831.602407486738
1
Iteration 11800: Loss = -10831.597349548327
2
Iteration 11900: Loss = -10831.597215293546
3
Iteration 12000: Loss = -10831.597147161547
Iteration 12100: Loss = -10831.609861569239
1
Iteration 12200: Loss = -10831.597330349015
2
Iteration 12300: Loss = -10831.596870273626
Iteration 12400: Loss = -10831.598950202953
1
Iteration 12500: Loss = -10831.596859663854
Iteration 12600: Loss = -10831.604052057415
1
Iteration 12700: Loss = -10831.59682685723
Iteration 12800: Loss = -10831.596870232068
1
Iteration 12900: Loss = -10831.598574575499
2
Iteration 13000: Loss = -10831.59840611282
3
Iteration 13100: Loss = -10831.59835200785
4
Iteration 13200: Loss = -10831.606418786994
5
Iteration 13300: Loss = -10831.596798565608
Iteration 13400: Loss = -10831.601357778134
1
Iteration 13500: Loss = -10831.596772052482
Iteration 13600: Loss = -10831.599822013017
1
Iteration 13700: Loss = -10831.596729356326
Iteration 13800: Loss = -10831.59747975708
1
Iteration 13900: Loss = -10831.597326141129
2
Iteration 14000: Loss = -10831.604057628778
3
Iteration 14100: Loss = -10831.59790192018
4
Iteration 14200: Loss = -10831.597825653229
5
Iteration 14300: Loss = -10831.596835505377
6
Iteration 14400: Loss = -10831.602965579115
7
Iteration 14500: Loss = -10831.633244903787
8
Iteration 14600: Loss = -10831.596705956286
Iteration 14700: Loss = -10831.596745776418
1
Iteration 14800: Loss = -10831.602321977984
2
Iteration 14900: Loss = -10831.596687908146
Iteration 15000: Loss = -10831.59862253315
1
Iteration 15100: Loss = -10831.624524205812
2
Iteration 15200: Loss = -10831.597725162264
3
Iteration 15300: Loss = -10831.596919689791
4
Iteration 15400: Loss = -10831.59774104735
5
Iteration 15500: Loss = -10831.604842066681
6
Iteration 15600: Loss = -10831.596822709542
7
Iteration 15700: Loss = -10831.604823976304
8
Iteration 15800: Loss = -10831.611004182716
9
Iteration 15900: Loss = -10831.596800565025
10
Stopping early at iteration 15900 due to no improvement.
tensor([[ 1.2804, -2.6695],
        [ 1.0635, -3.0261],
        [ 1.0541, -2.8165],
        [ 1.2579, -2.6671],
        [ 1.3513, -2.7969],
        [ 1.2479, -2.7985],
        [ 0.4082, -3.5542],
        [ 1.2730, -2.6622],
        [ 1.2051, -2.6322],
        [ 1.0319, -2.8493],
        [ 0.7603, -3.1303],
        [ 0.8503, -3.1194],
        [ 1.2710, -2.8593],
        [ 1.2032, -2.5896],
        [ 0.9992, -3.0419],
        [ 1.1772, -2.6491],
        [ 0.8958, -2.9131],
        [ 0.4802, -3.4068],
        [ 0.8930, -2.8984],
        [ 1.2450, -2.7465],
        [ 1.2457, -2.7314],
        [ 1.2080, -2.8416],
        [ 1.2338, -2.6972],
        [ 1.1639, -2.6832],
        [-0.1998, -4.2390],
        [ 0.3866, -3.5756],
        [ 1.2586, -2.6709],
        [ 1.1590, -2.5845],
        [ 1.1447, -2.6752],
        [ 1.1605, -2.8131],
        [ 1.0826, -2.7117],
        [ 0.5459, -3.4477],
        [ 2.2965, -4.1935],
        [ 1.0910, -2.7662],
        [ 1.0597, -2.7930],
        [ 1.3056, -2.7019],
        [ 0.8441, -3.1560],
        [ 1.2258, -2.7796],
        [ 0.9757, -2.9897],
        [ 0.7932, -2.8948],
        [ 1.2840, -2.6816],
        [ 1.0134, -2.9946],
        [ 1.2820, -2.7665],
        [ 1.1308, -2.7002],
        [ 1.2678, -2.6544],
        [-0.1725, -4.1007],
        [ 0.6297, -3.4440],
        [ 0.9932, -2.9592],
        [ 1.0712, -2.8692],
        [ 1.2567, -2.6690],
        [ 0.8368, -3.1344],
        [ 1.2444, -2.6504],
        [ 1.0216, -3.0739],
        [ 1.0923, -2.8338],
        [ 1.1116, -2.7384],
        [ 1.2585, -2.7845],
        [-0.3128, -4.3024],
        [ 1.0624, -3.0911],
        [ 0.9615, -2.9527],
        [ 1.0683, -2.8930],
        [ 0.8304, -2.9665],
        [ 1.3397, -2.7261],
        [ 1.0542, -2.8646],
        [ 1.1759, -2.6135],
        [ 1.2066, -2.6382],
        [ 1.1998, -2.8141],
        [ 1.1195, -2.8033],
        [ 1.1988, -2.6207],
        [ 1.1812, -2.6322],
        [ 1.1797, -2.7648],
        [ 1.0846, -2.8160],
        [ 0.5719, -3.3702],
        [ 1.0488, -2.8941],
        [ 1.2059, -2.7405],
        [ 1.2210, -2.6242],
        [ 1.2158, -2.7315],
        [ 0.5226, -3.3550],
        [ 0.4666, -3.6008],
        [ 1.1587, -2.6734],
        [ 1.1778, -2.5897],
        [ 1.2724, -2.6913],
        [ 0.5864, -3.3757],
        [ 1.1594, -2.6991],
        [ 1.1842, -2.7494],
        [ 1.0807, -2.5898],
        [ 1.3569, -2.7439],
        [ 1.0618, -2.7591],
        [ 1.1492, -2.8067],
        [ 0.5118, -3.3260],
        [ 0.4242, -3.4394],
        [ 1.0677, -2.7945],
        [ 1.1847, -2.7117],
        [ 1.0780, -2.7149],
        [ 1.0425, -2.7062],
        [ 1.2863, -2.6799],
        [ 1.3179, -2.7095],
        [ 0.4536, -3.4257],
        [ 0.8256, -3.0342],
        [ 1.0486, -2.8579],
        [ 1.2412, -2.6877]], dtype=torch.float64, requires_grad=True)
pi: tensor([[3.1036e-01, 6.8964e-01],
        [1.9000e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9808, 0.0192], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1680, 0.1716],
         [0.8362, 0.1549]],

        [[0.8839, 0.1596],
         [0.3932, 0.6303]],

        [[0.4949, 0.1501],
         [0.3913, 0.7779]],

        [[0.7038, 0.1721],
         [0.3349, 0.2955]],

        [[0.7847, 0.2920],
         [0.4478, 0.6371]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: -0.005513710431743218
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: -0.006096049957872825
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: -0.00036009454354445005
Average Adjusted Rand Index: -0.002264129112568802
10800.800205334708
new:  [-0.00036009454354445005, -0.00036009454354445005, -0.00036009454354445005, -0.00036009454354445005] [-0.002264129112568802, -0.002264129112568802, -0.002264129112568802, -0.002264129112568802] [10831.711244330207, 10831.600977004815, 10831.604137730294, 10831.596800565025]
prior:  [0.0, 0.0, 0.0, 0.0006956275301036035] [0.0, 0.0, 0.0, -0.0004529465619428516] [10833.974104020637, 10833.903804203792, 10833.898777093786, 10834.01783971872]
-----------------------------------------------------------------------------------------
This iteration is 15
True Objective function: Loss = -10832.598344230853
Iteration 0: Loss = -18873.218501079908
Iteration 10: Loss = -10894.456091991684
Iteration 20: Loss = -10876.686660318948
Iteration 30: Loss = -10870.339376278513
Iteration 40: Loss = -10850.610728740194
Iteration 50: Loss = -10806.09612303301
Iteration 60: Loss = -10788.529866671446
Iteration 70: Loss = -10788.216200975941
Iteration 80: Loss = -10788.366111726022
1
Iteration 90: Loss = -10788.41699418937
2
Iteration 100: Loss = -10788.41910913496
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.6694, 0.3306],
        [0.3050, 0.6950]], dtype=torch.float64)
alpha: tensor([0.4935, 0.5065])
beta: tensor([[[0.1965, 0.1029],
         [0.9729, 0.2400]],

        [[0.9682, 0.1026],
         [0.7469, 0.8407]],

        [[0.2610, 0.0927],
         [0.5942, 0.6055]],

        [[0.3609, 0.0979],
         [0.2746, 0.1125]],

        [[0.2748, 0.0959],
         [0.3337, 0.7739]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 85
Adjusted Rand Index: 0.4848303190856317
time is 1
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080682750429576
time is 2
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 94
Adjusted Rand Index: 0.7721141809334062
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 92
Adjusted Rand Index: 0.7026048523603536
time is 4
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7721116882917585
Global Adjusted Rand Index: 0.7050088732518717
Average Adjusted Rand Index: 0.7079458631428215
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18866.567078713775
Iteration 100: Loss = -10908.396085927507
Iteration 200: Loss = -10900.286635665509
Iteration 300: Loss = -10897.448844133769
Iteration 400: Loss = -10896.732417690731
Iteration 500: Loss = -10896.436982243647
Iteration 600: Loss = -10896.27049124223
Iteration 700: Loss = -10896.165139025434
Iteration 800: Loss = -10896.0936693912
Iteration 900: Loss = -10896.042608693811
Iteration 1000: Loss = -10896.0066559623
Iteration 1100: Loss = -10895.980455704841
Iteration 1200: Loss = -10895.960898764917
Iteration 1300: Loss = -10895.945874487285
Iteration 1400: Loss = -10895.934016116107
Iteration 1500: Loss = -10895.924449904101
Iteration 1600: Loss = -10895.916563275689
Iteration 1700: Loss = -10895.909993893423
Iteration 1800: Loss = -10895.904395705014
Iteration 1900: Loss = -10895.899561340586
Iteration 2000: Loss = -10895.89541560375
Iteration 2100: Loss = -10895.891783611321
Iteration 2200: Loss = -10895.888565988198
Iteration 2300: Loss = -10895.885782325457
Iteration 2400: Loss = -10895.883204016689
Iteration 2500: Loss = -10895.880947199961
Iteration 2600: Loss = -10895.878569176255
Iteration 2700: Loss = -10895.875044419538
Iteration 2800: Loss = -10895.87360572932
Iteration 2900: Loss = -10895.872269502588
Iteration 3000: Loss = -10895.871076232532
Iteration 3100: Loss = -10895.869935071785
Iteration 3200: Loss = -10895.868978867784
Iteration 3300: Loss = -10895.867995767614
Iteration 3400: Loss = -10895.867115431365
Iteration 3500: Loss = -10895.866341845764
Iteration 3600: Loss = -10895.86559587754
Iteration 3700: Loss = -10895.8649001622
Iteration 3800: Loss = -10895.864267685383
Iteration 3900: Loss = -10895.863682720983
Iteration 4000: Loss = -10895.863108510493
Iteration 4100: Loss = -10895.862595515197
Iteration 4200: Loss = -10895.862135812209
Iteration 4300: Loss = -10895.861674577174
Iteration 4400: Loss = -10895.861248692663
Iteration 4500: Loss = -10895.860826385022
Iteration 4600: Loss = -10895.860486200856
Iteration 4700: Loss = -10895.860221160752
Iteration 4800: Loss = -10895.859840312469
Iteration 4900: Loss = -10895.859496388766
Iteration 5000: Loss = -10895.859191300959
Iteration 5100: Loss = -10895.858931885625
Iteration 5200: Loss = -10895.858651277547
Iteration 5300: Loss = -10895.858414634089
Iteration 5400: Loss = -10895.858156649736
Iteration 5500: Loss = -10895.857938294657
Iteration 5600: Loss = -10895.857657956421
Iteration 5700: Loss = -10895.857483618422
Iteration 5800: Loss = -10895.861086935392
1
Iteration 5900: Loss = -10895.861370340901
2
Iteration 6000: Loss = -10895.862143419266
3
Iteration 6100: Loss = -10895.860741986218
4
Iteration 6200: Loss = -10895.866529528603
5
Iteration 6300: Loss = -10895.856855192424
Iteration 6400: Loss = -10895.856244919696
Iteration 6500: Loss = -10895.886588738987
1
Iteration 6600: Loss = -10895.856047871406
Iteration 6700: Loss = -10895.862022725529
1
Iteration 6800: Loss = -10895.855827346168
Iteration 6900: Loss = -10895.855763268602
Iteration 7000: Loss = -10896.350972775363
1
Iteration 7100: Loss = -10895.855595882662
Iteration 7200: Loss = -10895.855486970751
Iteration 7300: Loss = -10895.855378710161
Iteration 7400: Loss = -10895.855451850905
1
Iteration 7500: Loss = -10895.85524718454
Iteration 7600: Loss = -10895.85519917286
Iteration 7700: Loss = -10895.855986236598
1
Iteration 7800: Loss = -10895.855056524997
Iteration 7900: Loss = -10895.854990157111
Iteration 8000: Loss = -10896.084480924012
1
Iteration 8100: Loss = -10895.854899630094
Iteration 8200: Loss = -10895.854862258575
Iteration 8300: Loss = -10895.854799590008
Iteration 8400: Loss = -10895.854819422271
1
Iteration 8500: Loss = -10895.854721382104
Iteration 8600: Loss = -10895.854680429456
Iteration 8700: Loss = -10895.896943728345
1
Iteration 8800: Loss = -10895.854609015718
Iteration 8900: Loss = -10895.854576870779
Iteration 9000: Loss = -10895.854537875559
Iteration 9100: Loss = -10895.85467474215
1
Iteration 9200: Loss = -10895.85444655993
Iteration 9300: Loss = -10895.854424374511
Iteration 9400: Loss = -10895.854412829172
Iteration 9500: Loss = -10895.855410524326
1
Iteration 9600: Loss = -10895.854365519912
Iteration 9700: Loss = -10895.854333991696
Iteration 9800: Loss = -10895.895815657259
1
Iteration 9900: Loss = -10895.854245449704
Iteration 10000: Loss = -10895.854210226515
Iteration 10100: Loss = -10895.984370240587
1
Iteration 10200: Loss = -10895.854203917686
Iteration 10300: Loss = -10895.854213237071
1
Iteration 10400: Loss = -10895.855632269497
2
Iteration 10500: Loss = -10895.854131812528
Iteration 10600: Loss = -10895.854133922905
1
Iteration 10700: Loss = -10895.854131101712
Iteration 10800: Loss = -10895.854348890296
1
Iteration 10900: Loss = -10895.854112128562
Iteration 11000: Loss = -10895.858317256416
1
Iteration 11100: Loss = -10895.854106228086
Iteration 11200: Loss = -10895.85404686931
Iteration 11300: Loss = -10895.859832548847
1
Iteration 11400: Loss = -10896.012206612495
2
Iteration 11500: Loss = -10895.854418015877
3
Iteration 11600: Loss = -10895.870862974532
4
Iteration 11700: Loss = -10895.854067890692
5
Iteration 11800: Loss = -10896.17717158387
6
Iteration 11900: Loss = -10895.85426727645
7
Iteration 12000: Loss = -10895.853998357536
Iteration 12100: Loss = -10895.854912004977
1
Iteration 12200: Loss = -10895.854002178594
2
Iteration 12300: Loss = -10895.856884547211
3
Iteration 12400: Loss = -10895.85394276746
Iteration 12500: Loss = -10895.854066307862
1
Iteration 12600: Loss = -10895.853946082003
2
Iteration 12700: Loss = -10895.86937200958
3
Iteration 12800: Loss = -10895.85394935605
4
Iteration 12900: Loss = -10895.854003814766
5
Iteration 13000: Loss = -10895.854035459155
6
Iteration 13100: Loss = -10895.853911014317
Iteration 13200: Loss = -10895.855847396459
1
Iteration 13300: Loss = -10895.853944871858
2
Iteration 13400: Loss = -10895.856016784073
3
Iteration 13500: Loss = -10895.853914974461
4
Iteration 13600: Loss = -10895.854360932653
5
Iteration 13700: Loss = -10895.853902261188
Iteration 13800: Loss = -10895.867489489734
1
Iteration 13900: Loss = -10895.853914985348
2
Iteration 14000: Loss = -10895.854125090396
3
Iteration 14100: Loss = -10895.854037708536
4
Iteration 14200: Loss = -10895.853938851275
5
Iteration 14300: Loss = -10895.853930996693
6
Iteration 14400: Loss = -10895.854473239544
7
Iteration 14500: Loss = -10895.853917018943
8
Iteration 14600: Loss = -10895.858153532674
9
Iteration 14700: Loss = -10895.856278709149
10
Stopping early at iteration 14700 due to no improvement.
tensor([[-8.8888,  4.2736],
        [-0.3503, -4.2649],
        [-7.6235,  3.0083],
        [-7.9003,  3.2851],
        [-6.5973,  1.9821],
        [-4.5673, -0.0479],
        [-6.3472,  1.7320],
        [-7.6628,  3.0475],
        [-4.5562, -0.0590],
        [-6.4903,  1.8751],
        [-8.6596,  4.0444],
        [-5.0715,  0.4562],
        [ 0.0606, -4.6758],
        [-7.8235,  3.2083],
        [-3.9924, -0.6228],
        [-7.3817,  2.7665],
        [-8.2615,  3.6463],
        [-6.5912,  1.9760],
        [-5.2783,  0.6631],
        [-7.1390,  2.5237],
        [-5.5876,  0.9724],
        [ 0.2976, -4.9128],
        [-6.7619,  2.1466],
        [-1.9607, -2.6546],
        [-5.4374,  0.8221],
        [-6.4787,  1.8634],
        [-5.4259,  0.8107],
        [-7.2106,  2.5953],
        [-7.0950,  2.4798],
        [-5.6484,  1.0332],
        [-5.0453,  0.4300],
        [-4.2807, -0.3345],
        [-6.6934,  2.0782],
        [-8.5705,  3.9552],
        [-5.4080,  0.7928],
        [-6.7762,  2.1610],
        [-3.5703, -1.0449],
        [-8.5289,  3.9137],
        [-7.0420,  2.4268],
        [-4.2640, -0.3512],
        [-7.0254,  2.4101],
        [-7.9625,  3.3473],
        [-4.7809,  0.1657],
        [-7.1461,  2.5309],
        [-7.8636,  3.2484],
        [-7.8914,  3.2762],
        [-5.6413,  1.0260],
        [-5.3293,  0.7141],
        [-6.3763,  1.7611],
        [-8.0457,  3.4305],
        [-6.7799,  2.1646],
        [-6.7253,  2.1100],
        [-5.4511,  0.8359],
        [-7.1202,  2.5050],
        [-7.9473,  3.3320],
        [-8.4528,  3.8375],
        [-8.2685,  3.6533],
        [-6.2644,  1.6492],
        [-4.8435,  0.2283],
        [-1.3014, -3.3138],
        [-5.3536,  0.7384],
        [-5.9057,  1.2905],
        [-6.2857,  1.6705],
        [-6.7395,  2.1243],
        [-5.7818,  1.1666],
        [-4.2128, -0.4024],
        [-4.5943, -0.0209],
        [-7.0536,  2.4384],
        [-7.9881,  3.3729],
        [-8.3070,  3.6918],
        [-6.9201,  2.3049],
        [-5.8567,  1.2414],
        [-6.2844,  1.6692],
        [-5.7537,  1.1385],
        [-8.1128,  3.4975],
        [-8.0829,  3.4677],
        [-7.6911,  3.0759],
        [-5.9582,  1.3430],
        [-5.4993,  0.8841],
        [-2.8319, -1.7834],
        [-8.5703,  3.9551],
        [-5.7898,  1.1746],
        [-6.6275,  2.0123],
        [-6.2118,  1.5966],
        [-4.7111,  0.0959],
        [-7.4682,  2.8529],
        [-5.2680,  0.6528],
        [-7.4992,  2.8840],
        [-4.7522,  0.1370],
        [-5.9798,  1.3646],
        [-7.0539,  2.4387],
        [-6.7877,  2.1725],
        [-5.4632,  0.8480],
        [-8.9537,  4.3384],
        [-5.4976,  0.8824],
        [-6.4249,  1.8097],
        [-7.9664,  3.3512],
        [-3.6945, -0.9207],
        [-4.7794,  0.1642],
        [-4.7826,  0.1674]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9987e-01, 1.2936e-04],
        [1.4947e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0505, 0.9495], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0769, 0.1489],
         [0.9729, 0.1617]],

        [[0.9682, 0.2064],
         [0.7469, 0.8407]],

        [[0.2610, 0.2023],
         [0.5942, 0.6055]],

        [[0.3609, 0.0871],
         [0.2746, 0.1125]],

        [[0.2748, 0.1127],
         [0.3337, 0.7739]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.0006438680677883739
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0015221131464426677
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.006809752538456861
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.01644068827141728
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.004851708247904022
Global Adjusted Rand Index: 0.0032204647060002805
Average Adjusted Rand Index: 0.0032465502695478154
Iteration 0: Loss = -51177.7538843206
Iteration 10: Loss = -10903.599027992943
Iteration 20: Loss = -10903.599062059962
1
Iteration 30: Loss = -10903.599014190322
Iteration 40: Loss = -10903.598457332337
Iteration 50: Loss = -10903.588375843456
Iteration 60: Loss = -10903.43836532876
Iteration 70: Loss = -10902.617091872522
Iteration 80: Loss = -10902.200530581596
Iteration 90: Loss = -10902.23647383396
1
Iteration 100: Loss = -10902.32996289427
2
Iteration 110: Loss = -10902.338547034617
3
Stopping early at iteration 109 due to no improvement.
pi: tensor([[0.1386, 0.8614],
        [0.0429, 0.9571]], dtype=torch.float64)
alpha: tensor([0.0462, 0.9538])
beta: tensor([[[0.2334, 0.1923],
         [0.6333, 0.1557]],

        [[0.2015, 0.2105],
         [0.4354, 0.1851]],

        [[0.7850, 0.2058],
         [0.0137, 0.0853]],

        [[0.3932, 0.1854],
         [0.4478, 0.8984]],

        [[0.3202, 0.1504],
         [0.4658, 0.5919]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 3.206515228510384e-05
Average Adjusted Rand Index: -0.0003077958928485548
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -51176.9383941247
Iteration 100: Loss = -11033.700095432407
Iteration 200: Loss = -10970.562753057342
Iteration 300: Loss = -10937.246102195943
Iteration 400: Loss = -10927.549777531234
Iteration 500: Loss = -10918.716602930166
Iteration 600: Loss = -10915.535747541036
Iteration 700: Loss = -10909.230534873202
Iteration 800: Loss = -10906.574779838698
Iteration 900: Loss = -10905.87514206201
Iteration 1000: Loss = -10905.419563029915
Iteration 1100: Loss = -10905.090286642533
Iteration 1200: Loss = -10904.841250562842
Iteration 1300: Loss = -10904.646231031655
Iteration 1400: Loss = -10904.48925797636
Iteration 1500: Loss = -10904.360231778128
Iteration 1600: Loss = -10904.252401911835
Iteration 1700: Loss = -10904.161055771652
Iteration 1800: Loss = -10904.082860481796
Iteration 1900: Loss = -10904.015253492958
Iteration 2000: Loss = -10903.956360168415
Iteration 2100: Loss = -10903.904673701849
Iteration 2200: Loss = -10903.858987328462
Iteration 2300: Loss = -10903.818413247665
Iteration 2400: Loss = -10903.782180717135
Iteration 2500: Loss = -10903.7495911436
Iteration 2600: Loss = -10903.720193180345
Iteration 2700: Loss = -10903.69336676697
Iteration 2800: Loss = -10903.66878549469
Iteration 2900: Loss = -10903.64631294412
Iteration 3000: Loss = -10903.625740883144
Iteration 3100: Loss = -10903.606702050398
Iteration 3200: Loss = -10903.58884621462
Iteration 3300: Loss = -10903.571617239439
Iteration 3400: Loss = -10903.554201826319
Iteration 3500: Loss = -10903.53471513059
Iteration 3600: Loss = -10903.506396875378
Iteration 3700: Loss = -10903.452535978087
Iteration 3800: Loss = -10903.393551441386
Iteration 3900: Loss = -10903.349099037576
Iteration 4000: Loss = -10903.313865239512
Iteration 4100: Loss = -10903.28600191376
Iteration 4200: Loss = -10903.26536017643
Iteration 4300: Loss = -10903.250097968526
Iteration 4400: Loss = -10903.237813854903
Iteration 4500: Loss = -10903.227180289963
Iteration 4600: Loss = -10903.217508548976
Iteration 4700: Loss = -10903.20858436615
Iteration 4800: Loss = -10903.200130750465
Iteration 4900: Loss = -10903.192132463788
Iteration 5000: Loss = -10903.184409979474
Iteration 5100: Loss = -10903.176839227624
Iteration 5200: Loss = -10903.169407084995
Iteration 5300: Loss = -10903.162213097477
Iteration 5400: Loss = -10903.155211837704
Iteration 5500: Loss = -10903.148390409578
Iteration 5600: Loss = -10903.141817743324
Iteration 5700: Loss = -10903.13548117513
Iteration 5800: Loss = -10903.129344667397
Iteration 5900: Loss = -10903.123419709536
Iteration 6000: Loss = -10903.117802065613
Iteration 6100: Loss = -10903.112333114643
Iteration 6200: Loss = -10903.107182213422
Iteration 6300: Loss = -10903.10223010657
Iteration 6400: Loss = -10903.097652705917
Iteration 6500: Loss = -10903.093328506528
Iteration 6600: Loss = -10903.089304247587
Iteration 6700: Loss = -10903.085537680321
Iteration 6800: Loss = -10903.082149841537
Iteration 6900: Loss = -10903.079064861678
Iteration 7000: Loss = -10903.076237093332
Iteration 7100: Loss = -10903.073644655336
Iteration 7200: Loss = -10903.071340268796
Iteration 7300: Loss = -10903.069245878947
Iteration 7400: Loss = -10903.067381814784
Iteration 7500: Loss = -10903.065732753732
Iteration 7600: Loss = -10903.064223657117
Iteration 7700: Loss = -10903.062866070428
Iteration 7800: Loss = -10903.061640534088
Iteration 7900: Loss = -10903.060487383258
Iteration 8000: Loss = -10903.059542285577
Iteration 8100: Loss = -10903.058571331381
Iteration 8200: Loss = -10903.057690901036
Iteration 8300: Loss = -10903.056878368434
Iteration 8400: Loss = -10903.056161110982
Iteration 8500: Loss = -10903.055434825588
Iteration 8600: Loss = -10903.05479311985
Iteration 8700: Loss = -10903.05419651005
Iteration 8800: Loss = -10903.053573026422
Iteration 8900: Loss = -10903.05303144509
Iteration 9000: Loss = -10903.05252089328
Iteration 9100: Loss = -10903.052058115843
Iteration 9200: Loss = -10903.0518233956
Iteration 9300: Loss = -10903.05114069184
Iteration 9400: Loss = -10903.053589355684
1
Iteration 9500: Loss = -10903.050393672727
Iteration 9600: Loss = -10903.049858740118
Iteration 9700: Loss = -10903.055365689064
1
Iteration 9800: Loss = -10903.049183739166
Iteration 9900: Loss = -10903.048856679305
Iteration 10000: Loss = -10903.048640891282
Iteration 10100: Loss = -10903.048307327694
Iteration 10200: Loss = -10903.049530903312
1
Iteration 10300: Loss = -10903.04774068927
Iteration 10400: Loss = -10903.04752221475
Iteration 10500: Loss = -10903.047309455913
Iteration 10600: Loss = -10903.047125016225
Iteration 10700: Loss = -10903.046837775408
Iteration 10800: Loss = -10903.046670157104
Iteration 10900: Loss = -10903.05050642588
1
Iteration 11000: Loss = -10903.046274841337
Iteration 11100: Loss = -10903.046071689667
Iteration 11200: Loss = -10903.3721930627
1
Iteration 11300: Loss = -10903.045796861981
Iteration 11400: Loss = -10903.045626028817
Iteration 11500: Loss = -10903.04551844105
Iteration 11600: Loss = -10903.04541819864
Iteration 11700: Loss = -10903.045283162015
Iteration 11800: Loss = -10903.04521144949
Iteration 11900: Loss = -10903.04594725753
1
Iteration 12000: Loss = -10903.04497939607
Iteration 12100: Loss = -10903.044891525356
Iteration 12200: Loss = -10903.045072136461
1
Iteration 12300: Loss = -10903.044713176298
Iteration 12400: Loss = -10903.04465488097
Iteration 12500: Loss = -10903.044569872258
Iteration 12600: Loss = -10903.047603509045
1
Iteration 12700: Loss = -10903.04442466929
Iteration 12800: Loss = -10903.044355050899
Iteration 12900: Loss = -10903.5873008171
1
Iteration 13000: Loss = -10903.044257912552
Iteration 13100: Loss = -10903.04421751093
Iteration 13200: Loss = -10903.044126239165
Iteration 13300: Loss = -10903.047554888282
1
Iteration 13400: Loss = -10903.044045923953
Iteration 13500: Loss = -10903.044012400405
Iteration 13600: Loss = -10903.057658303718
1
Iteration 13700: Loss = -10903.043961854597
Iteration 13800: Loss = -10903.054148317719
1
Iteration 13900: Loss = -10903.043891360681
Iteration 14000: Loss = -10903.043871492679
Iteration 14100: Loss = -10903.04427421622
1
Iteration 14200: Loss = -10903.04378642794
Iteration 14300: Loss = -10903.044224024286
1
Iteration 14400: Loss = -10903.043706858572
Iteration 14500: Loss = -10903.043695245899
Iteration 14600: Loss = -10903.048020084483
1
Iteration 14700: Loss = -10903.043636915865
Iteration 14800: Loss = -10903.052510615282
1
Iteration 14900: Loss = -10903.043594838187
Iteration 15000: Loss = -10903.0435885906
Iteration 15100: Loss = -10903.04360161626
1
Iteration 15200: Loss = -10903.04352133325
Iteration 15300: Loss = -10903.043524129633
1
Iteration 15400: Loss = -10903.043510739406
Iteration 15500: Loss = -10903.04348996415
Iteration 15600: Loss = -10903.043483840864
Iteration 15700: Loss = -10903.04348334839
Iteration 15800: Loss = -10903.043414462878
Iteration 15900: Loss = -10903.044654150563
1
Iteration 16000: Loss = -10903.044893888933
2
Iteration 16100: Loss = -10903.043441325557
3
Iteration 16200: Loss = -10903.043446046293
4
Iteration 16300: Loss = -10903.045855060574
5
Iteration 16400: Loss = -10903.04334271264
Iteration 16500: Loss = -10903.04235091594
Iteration 16600: Loss = -10902.228071216605
Iteration 16700: Loss = -10902.228093437168
1
Iteration 16800: Loss = -10902.227745035441
Iteration 16900: Loss = -10902.355604886485
1
Iteration 17000: Loss = -10902.032386002318
Iteration 17100: Loss = -10902.032337414508
Iteration 17200: Loss = -10902.066894943875
1
Iteration 17300: Loss = -10902.032335532347
Iteration 17400: Loss = -10902.032302325968
Iteration 17500: Loss = -10902.058126356927
1
Iteration 17600: Loss = -10901.934230001181
Iteration 17700: Loss = -10901.925891788986
Iteration 17800: Loss = -10901.809281202019
Iteration 17900: Loss = -10901.750068777268
Iteration 18000: Loss = -10901.674136867927
Iteration 18100: Loss = -10901.635673583385
Iteration 18200: Loss = -10901.52832625409
Iteration 18300: Loss = -10901.46456880248
Iteration 18400: Loss = -10901.501560831004
1
Iteration 18500: Loss = -10901.375125086479
Iteration 18600: Loss = -10901.359788096952
Iteration 18700: Loss = -10901.34514859123
Iteration 18800: Loss = -10901.313778380776
Iteration 18900: Loss = -10901.324986080394
1
Iteration 19000: Loss = -10901.249873511395
Iteration 19100: Loss = -10901.173513908516
Iteration 19200: Loss = -10901.150505543736
Iteration 19300: Loss = -10901.131924831585
Iteration 19400: Loss = -10901.09214860262
Iteration 19500: Loss = -10901.054516810764
Iteration 19600: Loss = -10901.045214802705
Iteration 19700: Loss = -10901.016952230048
Iteration 19800: Loss = -10901.052314594275
1
Iteration 19900: Loss = -10900.95375555321
tensor([[ -7.3657,   5.5235],
        [ -6.9477,   5.5592],
        [ -6.9770,   5.5888],
        [ -7.0708,   5.4383],
        [ -8.3779,   4.1076],
        [ -7.0058,   5.4806],
        [ -7.5676,   5.0204],
        [ -7.0727,   5.6788],
        [ -7.2669,   5.3420],
        [ -7.0320,   5.6086],
        [ -6.9350,   5.5405],
        [ -7.1052,   5.4068],
        [ -6.9822,   5.5661],
        [ -7.3378,   5.0536],
        [ -6.9856,   5.4870],
        [ -7.0751,   5.6528],
        [ -7.5821,   5.0490],
        [ -6.8999,   5.4516],
        [ -6.9483,   5.5566],
        [ -7.8950,   4.6078],
        [ -7.0469,   5.4955],
        [ -6.9840,   5.5974],
        [ -7.4381,   5.0550],
        [ -7.1933,   5.2891],
        [ -7.0996,   5.4171],
        [-11.6101,   6.9948],
        [ -7.0020,   5.5596],
        [ -7.0435,   5.4623],
        [ -7.0847,   5.5294],
        [ -7.0453,   5.4461],
        [ -6.9246,   5.3831],
        [ -7.3699,   5.1511],
        [ -7.9930,   4.4475],
        [-10.1087,   8.0373],
        [ -7.3569,   5.1464],
        [ -7.0194,   5.4955],
        [ -6.9686,   5.5822],
        [ -7.1586,   5.3626],
        [ -6.9811,   5.5902],
        [ -7.1028,   5.4964],
        [ -6.9216,   5.5132],
        [ -7.1541,   5.3204],
        [ -7.0137,   5.6079],
        [ -7.3449,   5.1029],
        [ -8.0281,   4.3382],
        [ -7.0556,   5.5236],
        [ -6.9296,   5.4874],
        [ -7.0611,   5.4298],
        [ -7.3081,   5.1517],
        [ -7.0240,   5.6266],
        [ -7.0819,   5.3128],
        [ -7.0209,   5.5454],
        [ -6.8854,   5.4980],
        [ -7.0265,   5.5493],
        [ -6.9555,   5.3667],
        [ -8.1344,   4.2207],
        [ -7.3419,   5.4681],
        [ -7.3168,   5.6270],
        [ -6.9957,   5.5839],
        [ -7.3583,   5.0349],
        [ -7.1848,   5.5704],
        [ -7.5330,   5.0760],
        [ -7.0868,   5.3931],
        [ -7.1258,   5.4141],
        [ -7.1100,   5.5442],
        [ -7.8833,   4.5053],
        [ -6.9136,   5.5266],
        [ -7.0978,   5.4944],
        [ -7.8680,   4.7152],
        [ -7.5794,   5.0890],
        [ -6.9930,   5.4137],
        [ -6.9326,   5.5309],
        [ -6.8318,   5.4434],
        [ -7.8138,   4.4926],
        [ -7.3140,   5.2266],
        [ -7.0840,   5.5968],
        [ -7.6253,   4.8815],
        [ -7.0937,   5.3027],
        [ -6.9620,   5.5060],
        [ -6.9841,   5.4644],
        [ -7.0335,   5.4307],
        [ -7.1704,   5.4107],
        [ -7.3640,   5.3837],
        [ -7.0473,   5.3518],
        [ -7.0745,   5.3952],
        [ -7.5776,   4.8224],
        [ -7.8666,   4.6070],
        [ -7.1201,   5.4784],
        [ -8.0774,   4.3500],
        [ -6.9488,   5.5617],
        [ -7.3349,   5.0330],
        [ -7.4553,   4.9330],
        [ -6.9483,   5.3440],
        [ -7.3973,   5.2884],
        [ -7.6999,   5.5060],
        [ -7.1227,   5.4773],
        [ -7.0639,   5.4389],
        [ -7.9870,   4.7800],
        [ -7.3010,   5.2030],
        [ -7.1935,   5.4998]], dtype=torch.float64, requires_grad=True)
pi: tensor([[4.6804e-06, 1.0000e+00],
        [5.2178e-02, 9.4782e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([3.5784e-06, 1.0000e+00], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1563, 0.1558],
         [0.6333, 0.1615]],

        [[0.2015, 0.1992],
         [0.4354, 0.1851]],

        [[0.7850, 0.1685],
         [0.0137, 0.0853]],

        [[0.3932, 0.0919],
         [0.4478, 0.8984]],

        [[0.3202, 0.1346],
         [0.4658, 0.5919]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 3.206515228510384e-05
Average Adjusted Rand Index: 0.0008288350629887435
Iteration 0: Loss = -25754.824355842877
Iteration 10: Loss = -10903.234360264047
Iteration 20: Loss = -10902.410396644676
Iteration 30: Loss = -10902.208765869796
Iteration 40: Loss = -10902.305770635647
1
Iteration 50: Loss = -10902.35111410692
2
Iteration 60: Loss = -10902.273333982022
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.1174, 0.8826],
        [0.0498, 0.9502]], dtype=torch.float64)
alpha: tensor([0.0527, 0.9473])
beta: tensor([[[0.2229, 0.1779],
         [0.3143, 0.1558]],

        [[0.3361, 0.2074],
         [0.2161, 0.6727]],

        [[0.1616, 0.2019],
         [0.9712, 0.9187]],

        [[0.3115, 0.1828],
         [0.1479, 0.0598]],

        [[0.1727, 0.1511],
         [0.4636, 0.0499]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 3.206515228510384e-05
Average Adjusted Rand Index: -0.0003077958928485548
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25754.175891033756
Iteration 100: Loss = -10978.120489720564
Iteration 200: Loss = -10942.091402832177
Iteration 300: Loss = -10928.109637480286
Iteration 400: Loss = -10922.759951956616
Iteration 500: Loss = -10910.446013692279
Iteration 600: Loss = -10906.540169879205
Iteration 700: Loss = -10905.515853638612
Iteration 800: Loss = -10904.984546751997
Iteration 900: Loss = -10904.639605479488
Iteration 1000: Loss = -10904.399347338109
Iteration 1100: Loss = -10904.223952445911
Iteration 1200: Loss = -10904.091269550969
Iteration 1300: Loss = -10903.986666750856
Iteration 1400: Loss = -10903.902257912132
Iteration 1500: Loss = -10903.832905032843
Iteration 1600: Loss = -10903.775029778899
Iteration 1700: Loss = -10903.726192567694
Iteration 1800: Loss = -10903.684684388985
Iteration 1900: Loss = -10903.649196955206
Iteration 2000: Loss = -10903.61821640142
Iteration 2100: Loss = -10903.591444485293
Iteration 2200: Loss = -10903.567997569828
Iteration 2300: Loss = -10903.5475569044
Iteration 2400: Loss = -10903.528981506133
Iteration 2500: Loss = -10903.51259836129
Iteration 2600: Loss = -10903.526395443057
1
Iteration 2700: Loss = -10903.48417077029
Iteration 2800: Loss = -10903.471593642113
Iteration 2900: Loss = -10903.459774222823
Iteration 3000: Loss = -10903.449687549557
Iteration 3100: Loss = -10903.43690906688
Iteration 3200: Loss = -10903.424491971333
Iteration 3300: Loss = -10903.641036217225
1
Iteration 3400: Loss = -10903.382334383661
Iteration 3500: Loss = -10903.179623752158
Iteration 3600: Loss = -10902.390348932378
Iteration 3700: Loss = -10902.157287593102
Iteration 3800: Loss = -10901.948935612458
Iteration 3900: Loss = -10901.87287382816
Iteration 4000: Loss = -10901.818926920932
Iteration 4100: Loss = -10901.79073778945
Iteration 4200: Loss = -10901.77104883046
Iteration 4300: Loss = -10901.756061889842
Iteration 4400: Loss = -10901.744213581065
Iteration 4500: Loss = -10901.734483153927
Iteration 4600: Loss = -10901.726342782764
Iteration 4700: Loss = -10901.719429759833
Iteration 4800: Loss = -10901.713412608808
Iteration 4900: Loss = -10901.708173031531
Iteration 5000: Loss = -10901.703516124599
Iteration 5100: Loss = -10901.712265514398
1
Iteration 5200: Loss = -10901.695422756411
Iteration 5300: Loss = -10901.691406378653
Iteration 5400: Loss = -10901.686203172669
Iteration 5500: Loss = -10901.676241869232
Iteration 5600: Loss = -10901.55635378843
Iteration 5700: Loss = -10900.600946563047
Iteration 5800: Loss = -10900.568306981513
Iteration 5900: Loss = -10900.551905764829
Iteration 6000: Loss = -10900.540164148593
Iteration 6100: Loss = -10900.531745869132
Iteration 6200: Loss = -10900.567673627733
1
Iteration 6300: Loss = -10900.519873806123
Iteration 6400: Loss = -10900.515418755303
Iteration 6500: Loss = -10900.511648271338
Iteration 6600: Loss = -10900.50966054602
Iteration 6700: Loss = -10900.505576061652
Iteration 6800: Loss = -10900.503064657527
Iteration 6900: Loss = -10900.588363237353
1
Iteration 7000: Loss = -10900.498851081413
Iteration 7100: Loss = -10900.497026949277
Iteration 7200: Loss = -10900.495333701632
Iteration 7300: Loss = -10900.493926518393
Iteration 7400: Loss = -10900.492493132046
Iteration 7500: Loss = -10900.49121933874
Iteration 7600: Loss = -10900.496832939338
1
Iteration 7700: Loss = -10900.48904034201
Iteration 7800: Loss = -10900.487983844178
Iteration 7900: Loss = -10900.487056276763
Iteration 8000: Loss = -10900.518166365671
1
Iteration 8100: Loss = -10900.48537779974
Iteration 8200: Loss = -10900.48459570461
Iteration 8300: Loss = -10900.4838650687
Iteration 8400: Loss = -10900.492058601743
1
Iteration 8500: Loss = -10900.482578370382
Iteration 8600: Loss = -10900.481990304095
Iteration 8700: Loss = -10900.481458396172
Iteration 8800: Loss = -10900.481329775106
Iteration 8900: Loss = -10900.48037371757
Iteration 9000: Loss = -10900.479921673374
Iteration 9100: Loss = -10900.486982732004
1
Iteration 9200: Loss = -10900.479059478068
Iteration 9300: Loss = -10900.478634149626
Iteration 9400: Loss = -10900.478263081093
Iteration 9500: Loss = -10900.593003833865
1
Iteration 9600: Loss = -10900.477564746005
Iteration 9700: Loss = -10900.47725384371
Iteration 9800: Loss = -10900.47696005395
Iteration 9900: Loss = -10900.495621175476
1
Iteration 10000: Loss = -10900.476382754296
Iteration 10100: Loss = -10900.476125450112
Iteration 10200: Loss = -10900.475855732595
Iteration 10300: Loss = -10900.476305642822
1
Iteration 10400: Loss = -10900.475424831788
Iteration 10500: Loss = -10900.475211942567
Iteration 10600: Loss = -10900.47500089913
Iteration 10700: Loss = -10900.478864036368
1
Iteration 10800: Loss = -10900.474670186091
Iteration 10900: Loss = -10900.474486283361
Iteration 11000: Loss = -10900.474287315854
Iteration 11100: Loss = -10900.474252796961
Iteration 11200: Loss = -10900.474037154298
Iteration 11300: Loss = -10900.473850804945
Iteration 11400: Loss = -10900.475329006511
1
Iteration 11500: Loss = -10900.47361900959
Iteration 11600: Loss = -10900.47349644077
Iteration 11700: Loss = -10900.530586498857
1
Iteration 11800: Loss = -10900.473308525912
Iteration 11900: Loss = -10900.473170384963
Iteration 12000: Loss = -10900.473100536712
Iteration 12100: Loss = -10900.556486554053
1
Iteration 12200: Loss = -10900.4729160662
Iteration 12300: Loss = -10900.472827165695
Iteration 12400: Loss = -10900.472764834692
Iteration 12500: Loss = -10900.569197007337
1
Iteration 12600: Loss = -10900.472626725365
Iteration 12700: Loss = -10900.472552897687
Iteration 12800: Loss = -10900.47251822
Iteration 12900: Loss = -10900.473036935658
1
Iteration 13000: Loss = -10900.472428017449
Iteration 13100: Loss = -10900.475259917144
1
Iteration 13200: Loss = -10900.472644262423
2
Iteration 13300: Loss = -10900.472259277853
Iteration 13400: Loss = -10900.472199490107
Iteration 13500: Loss = -10900.472168861608
Iteration 13600: Loss = -10900.625994764654
1
Iteration 13700: Loss = -10900.472089902909
Iteration 13800: Loss = -10900.47204341926
Iteration 13900: Loss = -10900.472027290818
Iteration 14000: Loss = -10900.5497617758
1
Iteration 14100: Loss = -10900.471975284036
Iteration 14200: Loss = -10900.471958605081
Iteration 14300: Loss = -10900.471906400146
Iteration 14400: Loss = -10900.471928731637
1
Iteration 14500: Loss = -10900.471883962739
Iteration 14600: Loss = -10900.471816798621
Iteration 14700: Loss = -10900.471823193298
1
Iteration 14800: Loss = -10900.535655825024
2
Iteration 14900: Loss = -10900.471800366406
Iteration 15000: Loss = -10900.471747763648
Iteration 15100: Loss = -10900.471722261274
Iteration 15200: Loss = -10900.477622948374
1
Iteration 15300: Loss = -10900.471712233255
Iteration 15400: Loss = -10900.471699757794
Iteration 15500: Loss = -10900.473166731117
1
Iteration 15600: Loss = -10900.471719871555
2
Iteration 15700: Loss = -10900.471668990014
Iteration 15800: Loss = -10900.472712841914
1
Iteration 15900: Loss = -10900.471704105954
2
Iteration 16000: Loss = -10900.47197008561
3
Iteration 16100: Loss = -10900.47164709909
Iteration 16200: Loss = -10900.471620875956
Iteration 16300: Loss = -10900.502079971906
1
Iteration 16400: Loss = -10900.472179869217
2
Iteration 16500: Loss = -10900.471603094895
Iteration 16600: Loss = -10900.471616118044
1
Iteration 16700: Loss = -10900.474204979437
2
Iteration 16800: Loss = -10900.471608221436
3
Iteration 16900: Loss = -10900.471574776158
Iteration 17000: Loss = -10900.472181273019
1
Iteration 17100: Loss = -10900.472215252728
2
Iteration 17200: Loss = -10900.471585374438
3
Iteration 17300: Loss = -10900.471554919004
Iteration 17400: Loss = -10900.61831319714
1
Iteration 17500: Loss = -10900.226819133959
Iteration 17600: Loss = -10900.22677128826
Iteration 17700: Loss = -10900.226772401697
1
Iteration 17800: Loss = -10900.235148751502
2
Iteration 17900: Loss = -10900.226766732292
Iteration 18000: Loss = -10900.22675742364
Iteration 18100: Loss = -10900.22674798133
Iteration 18200: Loss = -10900.226762145445
1
Iteration 18300: Loss = -10900.226760745787
2
Iteration 18400: Loss = -10900.226719973536
Iteration 18500: Loss = -10900.226736236757
1
Iteration 18600: Loss = -10900.22680765655
2
Iteration 18700: Loss = -10900.226797659312
3
Iteration 18800: Loss = -10900.226699344734
Iteration 18900: Loss = -10900.226708016184
1
Iteration 19000: Loss = -10900.558857066379
2
Iteration 19100: Loss = -10900.22670756916
3
Iteration 19200: Loss = -10900.226717919193
4
Iteration 19300: Loss = -10900.226707149212
5
Iteration 19400: Loss = -10900.299549557798
6
Iteration 19500: Loss = -10900.226715548324
7
Iteration 19600: Loss = -10900.22671693179
8
Iteration 19700: Loss = -10900.2267188418
9
Iteration 19800: Loss = -10900.227508543623
10
Stopping early at iteration 19800 due to no improvement.
tensor([[ -7.2210,   5.7740],
        [ -9.5466,   7.3741],
        [ -9.2899,   7.8646],
        [-11.1479,   7.4550],
        [ -9.1003,   7.6993],
        [-11.0831,   7.9894],
        [-11.4911,   7.0137],
        [ -8.8740,   6.2446],
        [ -9.0316,   7.6256],
        [ -9.3203,   7.1574],
        [ -9.2458,   7.7898],
        [ -9.7179,   7.7015],
        [ -9.5821,   8.0815],
        [ -9.9024,   8.4659],
        [ -9.4636,   8.0744],
        [ -9.5977,   6.7141],
        [ -9.2143,   7.2114],
        [ -9.7128,   8.2837],
        [ -9.3767,   7.9271],
        [ -9.9773,   7.8724],
        [ -9.3141,   7.8688],
        [-11.0134,   6.3982],
        [ -9.4599,   8.0729],
        [ -9.2919,   7.8574],
        [ -9.4117,   7.9203],
        [-10.3643,   8.1333],
        [-11.2190,   6.6038],
        [ -9.6739,   7.3157],
        [-10.4771,   6.8755],
        [ -9.5853,   8.0489],
        [ -9.6317,   8.1752],
        [ -9.4174,   8.0204],
        [ -9.5381,   8.0103],
        [ -9.1720,   7.3241],
        [ -9.2443,   7.8557],
        [ -9.1565,   7.7462],
        [ -9.1558,   7.7596],
        [-10.2646,   7.0712],
        [ -9.3350,   7.9470],
        [ -9.9013,   7.0789],
        [-10.7754,   8.0057],
        [ -9.4020,   8.0051],
        [ -9.2867,   7.3940],
        [-10.7792,   8.1337],
        [-11.0751,   8.2386],
        [-10.4562,   7.6987],
        [ -9.3519,   7.8463],
        [ -9.4070,   7.8649],
        [ -9.8339,   7.5876],
        [ -9.2169,   7.0594],
        [ -9.5107,   8.0515],
        [ -9.0407,   7.6451],
        [-10.0707,   8.1624],
        [ -9.1462,   7.5516],
        [ -9.5255,   7.9910],
        [-10.8336,   8.6021],
        [ -0.2811,  -1.2095],
        [ -7.9288,   6.4181],
        [-10.8096,   6.1943],
        [ -9.4671,   7.9210],
        [ -9.0330,   7.2257],
        [ -9.1246,   7.3752],
        [ -9.2133,   7.4960],
        [-10.2334,   7.8260],
        [ -9.1563,   7.6706],
        [ -9.4697,   8.0051],
        [ -9.4840,   7.7556],
        [ -9.8064,   7.5314],
        [ -9.2698,   7.8078],
        [ -9.3196,   7.4196],
        [ -9.5622,   8.1529],
        [ -9.4218,   7.8429],
        [-11.7765,   8.2557],
        [ -9.4369,   8.0505],
        [-10.1699,   7.3394],
        [ -8.8973,   7.0287],
        [-10.0153,   7.7140],
        [ -9.4570,   7.9015],
        [ -9.7430,   7.8552],
        [-10.7927,   6.7981],
        [-11.2812,   6.6659],
        [ -9.0415,   7.5340],
        [ -8.4126,   6.7566],
        [ -9.5422,   8.1443],
        [ -9.5668,   8.1459],
        [ -9.4682,   7.8929],
        [-10.2569,   7.8987],
        [ -9.4207,   7.3395],
        [ -9.3768,   7.7703],
        [ -9.2663,   7.7186],
        [-10.1368,   8.3923],
        [-10.4537,   8.1448],
        [ -9.7613,   8.2692],
        [ -8.9563,   7.5699],
        [  0.6149,  -2.2462],
        [ -9.9870,   7.1219],
        [ -9.4128,   7.6281],
        [ -8.5930,   7.0108],
        [ -9.3982,   8.0107],
        [ -9.0139,   7.6276]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 2.5811e-06],
        [3.7494e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0167, 0.9833], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2000, 0.2545],
         [0.3143, 0.1602]],

        [[0.3361, 0.2235],
         [0.2161, 0.6727]],

        [[0.1616, 0.0943],
         [0.9712, 0.9187]],

        [[0.3115, 0.1680],
         [0.1479, 0.0598]],

        [[0.1727, 0.1262],
         [0.4636, 0.0499]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
Global Adjusted Rand Index: -9.312886032595102e-05
Average Adjusted Rand Index: 0.0022089942882114956
Iteration 0: Loss = -27461.002976499192
Iteration 10: Loss = -10903.599025430034
Iteration 20: Loss = -10903.599050684339
1
Iteration 30: Loss = -10903.599081136006
2
Iteration 40: Loss = -10903.598725328577
Iteration 50: Loss = -10903.593439959028
Iteration 60: Loss = -10903.510655143486
Iteration 70: Loss = -10902.840991093142
Iteration 80: Loss = -10902.229286082224
Iteration 90: Loss = -10902.216278448563
Iteration 100: Loss = -10902.31142543656
1
Iteration 110: Loss = -10902.349759211917
2
Iteration 120: Loss = -10902.26777643662
3
Stopping early at iteration 119 due to no improvement.
pi: tensor([[0.1160, 0.8840],
        [0.0504, 0.9496]], dtype=torch.float64)
alpha: tensor([0.0532, 0.9468])
beta: tensor([[[0.2220, 0.1770],
         [0.8861, 0.1558]],

        [[0.7130, 0.2071],
         [0.8575, 0.9430]],

        [[0.6714, 0.2015],
         [0.7159, 0.0411]],

        [[0.1622, 0.1826],
         [0.2399, 0.8584]],

        [[0.1908, 0.1512],
         [0.7112, 0.8155]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 3.206515228510384e-05
Average Adjusted Rand Index: -0.0003077958928485548
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27461.643293921763
Iteration 100: Loss = -10928.123389809607
Iteration 200: Loss = -10909.291991982607
Iteration 300: Loss = -10906.810593454442
Iteration 400: Loss = -10905.678099927343
Iteration 500: Loss = -10905.038802596637
Iteration 600: Loss = -10904.632918411598
Iteration 700: Loss = -10904.355220938702
Iteration 800: Loss = -10904.154550354448
Iteration 900: Loss = -10904.002907955388
Iteration 1000: Loss = -10903.88185217013
Iteration 1100: Loss = -10903.774225305999
Iteration 1200: Loss = -10903.69106003912
Iteration 1300: Loss = -10903.617663406967
Iteration 1400: Loss = -10903.55442140987
Iteration 1500: Loss = -10903.485919873261
Iteration 1600: Loss = -10903.430973868763
Iteration 1700: Loss = -10903.386793597392
Iteration 1800: Loss = -10903.347607836055
Iteration 1900: Loss = -10903.311127813467
Iteration 2000: Loss = -10903.275770088025
Iteration 2100: Loss = -10903.237289091447
Iteration 2200: Loss = -10903.186883904335
Iteration 2300: Loss = -10903.10622908693
Iteration 2400: Loss = -10902.97566594291
Iteration 2500: Loss = -10902.87410171471
Iteration 2600: Loss = -10902.8151059282
Iteration 2700: Loss = -10902.768955104217
Iteration 2800: Loss = -10902.725225520746
Iteration 2900: Loss = -10902.684221611255
Iteration 3000: Loss = -10902.647375266728
Iteration 3100: Loss = -10902.614111811605
Iteration 3200: Loss = -10902.58242018266
Iteration 3300: Loss = -10902.554076554563
Iteration 3400: Loss = -10902.527118929718
Iteration 3500: Loss = -10902.500574357804
Iteration 3600: Loss = -10902.47385737185
Iteration 3700: Loss = -10902.447153936411
Iteration 3800: Loss = -10902.420370796073
Iteration 3900: Loss = -10902.392982287409
Iteration 4000: Loss = -10902.337318084865
Iteration 4100: Loss = -10902.28836818055
Iteration 4200: Loss = -10902.252795281645
Iteration 4300: Loss = -10902.215147938838
Iteration 4400: Loss = -10902.177172263297
Iteration 4500: Loss = -10902.139713319595
Iteration 4600: Loss = -10902.103906452934
Iteration 4700: Loss = -10902.069702744297
Iteration 4800: Loss = -10902.036124238159
Iteration 4900: Loss = -10902.002770736532
Iteration 5000: Loss = -10901.968665080227
Iteration 5100: Loss = -10901.93383884619
Iteration 5200: Loss = -10901.897029371481
Iteration 5300: Loss = -10901.851076109515
Iteration 5400: Loss = -10901.808314868345
Iteration 5500: Loss = -10901.774019150997
Iteration 5600: Loss = -10901.745349954985
Iteration 5700: Loss = -10901.727822805005
Iteration 5800: Loss = -10901.717922933014
Iteration 5900: Loss = -10901.711736443795
Iteration 6000: Loss = -10901.708200859808
Iteration 6100: Loss = -10901.706317113292
Iteration 6200: Loss = -10901.704866172413
Iteration 6300: Loss = -10901.703913223966
Iteration 6400: Loss = -10901.7031157206
Iteration 6500: Loss = -10901.702349304058
Iteration 6600: Loss = -10901.699918369719
Iteration 6700: Loss = -10901.612827865925
Iteration 6800: Loss = -10901.586002117132
Iteration 6900: Loss = -10901.579979527294
Iteration 7000: Loss = -10901.573700208586
Iteration 7100: Loss = -10901.571564896532
Iteration 7200: Loss = -10901.57259423536
1
Iteration 7300: Loss = -10901.568582734442
Iteration 7400: Loss = -10901.567098083518
Iteration 7500: Loss = -10901.56639754373
Iteration 7600: Loss = -10901.562180998028
Iteration 7700: Loss = -10901.557583258971
Iteration 7800: Loss = -10901.548809384145
Iteration 7900: Loss = -10901.52362785798
Iteration 8000: Loss = -10901.498216791408
Iteration 8100: Loss = -10901.387372635332
Iteration 8200: Loss = -10901.381967086412
Iteration 8300: Loss = -10901.380069239727
Iteration 8400: Loss = -10901.392591700958
1
Iteration 8500: Loss = -10901.379184095757
Iteration 8600: Loss = -10901.378886965505
Iteration 8700: Loss = -10901.38457122812
1
Iteration 8800: Loss = -10901.378353513075
Iteration 8900: Loss = -10901.378104707652
Iteration 9000: Loss = -10901.377887104096
Iteration 9100: Loss = -10901.377697681603
Iteration 9200: Loss = -10901.395695904264
1
Iteration 9300: Loss = -10901.377409227785
Iteration 9400: Loss = -10901.38046786156
1
Iteration 9500: Loss = -10901.377206726525
Iteration 9600: Loss = -10901.394386705882
1
Iteration 9700: Loss = -10901.377072072919
Iteration 9800: Loss = -10901.377140550243
1
Iteration 9900: Loss = -10901.376990706574
Iteration 10000: Loss = -10901.383683353617
1
Iteration 10100: Loss = -10901.376949477082
Iteration 10200: Loss = -10901.376987865033
1
Iteration 10300: Loss = -10901.400009041237
2
Iteration 10400: Loss = -10901.376864928665
Iteration 10500: Loss = -10901.376835920832
Iteration 10600: Loss = -10901.386478128528
1
Iteration 10700: Loss = -10901.376864841406
2
Iteration 10800: Loss = -10901.377695978576
3
Iteration 10900: Loss = -10901.402291229047
4
Iteration 11000: Loss = -10901.380366907924
5
Iteration 11100: Loss = -10901.376835884132
Iteration 11200: Loss = -10901.37696359516
1
Iteration 11300: Loss = -10901.376856719751
2
Iteration 11400: Loss = -10901.39111325928
3
Iteration 11500: Loss = -10901.376810479927
Iteration 11600: Loss = -10901.39849549348
1
Iteration 11700: Loss = -10901.376826002
2
Iteration 11800: Loss = -10901.380203384711
3
Iteration 11900: Loss = -10901.396651648027
4
Iteration 12000: Loss = -10901.378984474908
5
Iteration 12100: Loss = -10901.383862665021
6
Iteration 12200: Loss = -10901.376866647235
7
Iteration 12300: Loss = -10901.377554937944
8
Iteration 12400: Loss = -10901.381562995073
9
Iteration 12500: Loss = -10901.381121589944
10
Stopping early at iteration 12500 due to no improvement.
tensor([[-1.6504, -0.7183],
        [-4.8339,  3.3199],
        [-5.0630,  0.4794],
        [-4.2109,  2.8212],
        [-4.7911,  3.2070],
        [-4.8025,  3.3700],
        [-3.6516,  1.9358],
        [-2.9715,  0.9216],
        [-3.8319,  1.6707],
        [-3.3667,  1.8150],
        [-4.3987,  2.9914],
        [-4.8209,  3.3950],
        [-5.0072,  2.5672],
        [-5.4283,  3.4866],
        [-4.4681,  2.9090],
        [-3.2036,  0.7471],
        [-3.1987,  1.7304],
        [-5.8467,  4.4306],
        [-4.8850,  2.8885],
        [-4.2949,  2.8923],
        [-4.8209,  2.4754],
        [-4.2290,  2.7716],
        [-4.8691,  3.1080],
        [-5.8134,  2.7397],
        [-5.0508,  1.8404],
        [-5.6689,  4.1962],
        [-3.7887,  2.0478],
        [-4.0813,  2.6592],
        [-4.1014,  2.2827],
        [-4.5719,  2.9469],
        [-6.2126,  3.7118],
        [-4.4873,  2.3134],
        [-5.0597,  3.5623],
        [-3.3446,  1.9438],
        [-4.6282,  2.4555],
        [-4.4668,  2.5886],
        [-3.6430,  2.0794],
        [-4.1178,  2.6437],
        [-5.0152,  1.6304],
        [-3.8997,  2.1572],
        [-5.2487,  3.7991],
        [-4.0992,  2.6981],
        [-3.2636,  1.6611],
        [-5.3850,  3.5302],
        [-6.7363,  3.3205],
        [-4.4492,  2.2221],
        [-5.4769,  2.6337],
        [-4.8384,  2.4901],
        [-4.5431,  2.9783],
        [-2.7779,  1.3904],
        [-5.2623,  3.8015],
        [-3.9979,  2.6097],
        [-5.1909,  3.8046],
        [-3.5413,  2.0965],
        [-6.3075,  3.6500],
        [-5.9255,  3.3887],
        [-2.7959,  0.1761],
        [-3.4895,  2.0735],
        [-3.9923,  1.9839],
        [-5.1916,  3.7732],
        [-2.4154,  0.7323],
        [-4.9040,  3.0984],
        [-5.1068,  3.4533],
        [-5.0473,  1.1287],
        [-5.0591,  1.2753],
        [-6.3839,  3.1143],
        [-4.6702,  2.7894],
        [-3.8206,  2.4008],
        [-4.1075,  2.0261],
        [-3.0603,  1.2867],
        [-5.4026,  3.6904],
        [-4.8803,  2.4879],
        [-6.1259,  4.5001],
        [-5.8160,  4.1385],
        [-4.1698,  2.7344],
        [-3.2000,  0.9100],
        [-4.4603,  2.9622],
        [-5.0029,  3.6078],
        [-6.0777,  1.5742],
        [-4.9897,  2.5076],
        [-5.1003,  2.3933],
        [-3.7281,  2.3178],
        [-2.5558,  0.9197],
        [-5.5633,  4.1760],
        [-4.7642,  3.3319],
        [-5.8102,  4.3959],
        [-4.4453,  2.9851],
        [-3.7886,  2.4017],
        [-5.2762,  3.2297],
        [-5.1363,  2.3239],
        [-6.0902,  3.7935],
        [-5.1908,  3.5154],
        [-6.0074,  4.5821],
        [-2.9064,  1.4451],
        [-0.8388, -2.0905],
        [-4.4904,  3.0875],
        [-4.1114,  2.6994],
        [-2.4995,  0.7976],
        [-4.7047,  3.2618],
        [-3.5087,  1.7050]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.2756, 0.7244],
        [0.0892, 0.9108]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0142, 0.9858], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2155, 0.2575],
         [0.8861, 0.1554]],

        [[0.7130, 0.2004],
         [0.8575, 0.9430]],

        [[0.6714, 0.1856],
         [0.7159, 0.0411]],

        [[0.1622, 0.1783],
         [0.2399, 0.8584]],

        [[0.1908, 0.1613],
         [0.7112, 0.8155]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00019239700891914054
Average Adjusted Rand Index: 1.727640633392342e-05
Iteration 0: Loss = -24526.531468762892
Iteration 10: Loss = -10903.599021859942
Iteration 20: Loss = -10903.59902186081
1
Iteration 30: Loss = -10903.59902187915
2
Iteration 40: Loss = -10903.599022451695
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[3.1756e-01, 6.8244e-01],
        [1.4532e-11, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([2.1693e-11, 1.0000e+00])
beta: tensor([[[0.2961, 0.2671],
         [0.9445, 0.1589]],

        [[0.5412, 0.2497],
         [0.3285, 0.5317]],

        [[0.8258, 0.2583],
         [0.0976, 0.7075]],

        [[0.8516, 0.2117],
         [0.5385, 0.0137]],

        [[0.5777, 0.1290],
         [0.5881, 0.8718]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24525.851905262498
Iteration 100: Loss = -10944.86677693501
Iteration 200: Loss = -10922.286374381041
Iteration 300: Loss = -10916.031364258673
Iteration 400: Loss = -10906.875592734703
Iteration 500: Loss = -10905.647102700033
Iteration 600: Loss = -10905.042116896675
Iteration 700: Loss = -10904.655259188537
Iteration 800: Loss = -10904.388008828568
Iteration 900: Loss = -10904.18870864651
Iteration 1000: Loss = -10904.028959790136
Iteration 1100: Loss = -10903.862871188509
Iteration 1200: Loss = -10903.507171875002
Iteration 1300: Loss = -10903.517906494728
1
Iteration 1400: Loss = -10903.289354325829
Iteration 1500: Loss = -10903.221420970494
Iteration 1600: Loss = -10903.117194005747
Iteration 1700: Loss = -10902.960210757705
Iteration 1800: Loss = -10902.773092046578
Iteration 1900: Loss = -10902.533240830966
Iteration 2000: Loss = -10902.368283163605
Iteration 2100: Loss = -10902.226577469337
Iteration 2200: Loss = -10902.138920180621
Iteration 2300: Loss = -10902.04917615611
Iteration 2400: Loss = -10901.953211594766
Iteration 2500: Loss = -10901.856832034253
Iteration 2600: Loss = -10901.769331085723
Iteration 2700: Loss = -10901.699112120179
Iteration 2800: Loss = -10901.638449892493
Iteration 2900: Loss = -10901.569622682635
Iteration 3000: Loss = -10901.503341210131
Iteration 3100: Loss = -10901.442991611118
Iteration 3200: Loss = -10901.391655016294
Iteration 3300: Loss = -10901.345906490793
Iteration 3400: Loss = -10901.428462011116
1
Iteration 3500: Loss = -10901.224159402209
Iteration 3600: Loss = -10900.40271249404
Iteration 3700: Loss = -10899.363902117595
Iteration 3800: Loss = -10899.006445107545
Iteration 3900: Loss = -10898.823122644666
Iteration 4000: Loss = -10898.714820970823
Iteration 4100: Loss = -10898.71367482004
Iteration 4200: Loss = -10898.54418382441
Iteration 4300: Loss = -10898.515518472916
Iteration 4400: Loss = -10898.473734761288
Iteration 4500: Loss = -10898.449397117354
Iteration 4600: Loss = -10898.444373358936
Iteration 4700: Loss = -10898.414479962834
Iteration 4800: Loss = -10898.408404837268
Iteration 4900: Loss = -10898.39074816442
Iteration 5000: Loss = -10898.389254895346
Iteration 5100: Loss = -10898.367658654764
Iteration 5200: Loss = -10898.427215565565
1
Iteration 5300: Loss = -10898.319593001892
Iteration 5400: Loss = -10898.307470495816
Iteration 5500: Loss = -10898.34516372491
1
Iteration 5600: Loss = -10898.29293225956
Iteration 5700: Loss = -10898.32704807314
1
Iteration 5800: Loss = -10898.278718204096
Iteration 5900: Loss = -10898.27131242699
Iteration 6000: Loss = -10898.26466375003
Iteration 6100: Loss = -10898.256646428243
Iteration 6200: Loss = -10898.252123543252
Iteration 6300: Loss = -10898.247716965996
Iteration 6400: Loss = -10898.288473009981
1
Iteration 6500: Loss = -10898.233601582657
Iteration 6600: Loss = -10898.228320809758
Iteration 6700: Loss = -10898.21982342937
Iteration 6800: Loss = -10898.215250928599
Iteration 6900: Loss = -10898.211538638807
Iteration 7000: Loss = -10898.213814985991
1
Iteration 7100: Loss = -10898.203244740744
Iteration 7200: Loss = -10898.19907018268
Iteration 7300: Loss = -10898.200363225209
1
Iteration 7400: Loss = -10898.194717005174
Iteration 7500: Loss = -10898.164056411242
Iteration 7600: Loss = -10898.14285259023
Iteration 7700: Loss = -10898.137147431893
Iteration 7800: Loss = -10898.135477193478
Iteration 7900: Loss = -10898.13462432591
Iteration 8000: Loss = -10898.132651370808
Iteration 8100: Loss = -10898.138625339589
1
Iteration 8200: Loss = -10898.129313707144
Iteration 8300: Loss = -10898.128086281435
Iteration 8400: Loss = -10898.126872100867
Iteration 8500: Loss = -10898.125624900944
Iteration 8600: Loss = -10898.123964191735
Iteration 8700: Loss = -10898.145184463909
1
Iteration 8800: Loss = -10898.12150794918
Iteration 8900: Loss = -10898.120345925847
Iteration 9000: Loss = -10898.127589688327
1
Iteration 9100: Loss = -10898.116693912621
Iteration 9200: Loss = -10898.11592532007
Iteration 9300: Loss = -10898.115375565041
Iteration 9400: Loss = -10898.115878389888
1
Iteration 9500: Loss = -10898.114317161275
Iteration 9600: Loss = -10898.113493224688
Iteration 9700: Loss = -10898.641615035322
1
Iteration 9800: Loss = -10898.108606812892
Iteration 9900: Loss = -10898.107918717154
Iteration 10000: Loss = -10898.106561350749
Iteration 10100: Loss = -10898.106042670566
Iteration 10200: Loss = -10898.10551605939
Iteration 10300: Loss = -10898.104989499367
Iteration 10400: Loss = -10898.104420916892
Iteration 10500: Loss = -10898.103781623995
Iteration 10600: Loss = -10898.103011754878
Iteration 10700: Loss = -10898.129049852627
1
Iteration 10800: Loss = -10898.101961373499
Iteration 10900: Loss = -10898.101659148748
Iteration 11000: Loss = -10898.101336733298
Iteration 11100: Loss = -10898.101000147277
Iteration 11200: Loss = -10898.100343360069
Iteration 11300: Loss = -10898.100021742272
Iteration 11400: Loss = -10898.099841726256
Iteration 11500: Loss = -10898.09949133009
Iteration 11600: Loss = -10898.099113044376
Iteration 11700: Loss = -10898.0988743171
Iteration 11800: Loss = -10898.098112429383
Iteration 11900: Loss = -10898.097929238693
Iteration 12000: Loss = -10898.097845940283
Iteration 12100: Loss = -10898.097592107166
Iteration 12200: Loss = -10898.097452946387
Iteration 12300: Loss = -10898.097426449138
Iteration 12400: Loss = -10898.097231645042
Iteration 12500: Loss = -10898.105712762746
1
Iteration 12600: Loss = -10898.097106821459
Iteration 12700: Loss = -10898.096986663217
Iteration 12800: Loss = -10898.159748779843
1
Iteration 12900: Loss = -10898.096869438681
Iteration 13000: Loss = -10898.096839489814
Iteration 13100: Loss = -10898.096809729865
Iteration 13200: Loss = -10898.096746508212
Iteration 13300: Loss = -10898.096682515843
Iteration 13400: Loss = -10898.096502041613
Iteration 13500: Loss = -10898.112208042392
1
Iteration 13600: Loss = -10898.095997397862
Iteration 13700: Loss = -10898.095928824716
Iteration 13800: Loss = -10898.09783126887
1
Iteration 13900: Loss = -10898.09585997795
Iteration 14000: Loss = -10898.095871632695
1
Iteration 14100: Loss = -10898.095788776325
Iteration 14200: Loss = -10898.095764858897
Iteration 14300: Loss = -10898.188641935489
1
Iteration 14400: Loss = -10898.095709664323
Iteration 14500: Loss = -10898.137432791898
1
Iteration 14600: Loss = -10898.09562588482
Iteration 14700: Loss = -10898.130022107363
1
Iteration 14800: Loss = -10898.095470362863
Iteration 14900: Loss = -10898.095380265135
Iteration 15000: Loss = -10898.095393606967
1
Iteration 15100: Loss = -10898.206154172969
2
Iteration 15200: Loss = -10898.09502179469
Iteration 15300: Loss = -10898.09506754578
1
Iteration 15400: Loss = -10898.112724030185
2
Iteration 15500: Loss = -10898.094273788476
Iteration 15600: Loss = -10898.107429719756
1
Iteration 15700: Loss = -10898.097601342632
2
Iteration 15800: Loss = -10898.09472704211
3
Iteration 15900: Loss = -10898.149135265574
4
Iteration 16000: Loss = -10898.094100417062
Iteration 16100: Loss = -10898.09455008673
1
Iteration 16200: Loss = -10898.094008999366
Iteration 16300: Loss = -10898.11120575575
1
Iteration 16400: Loss = -10898.093738768393
Iteration 16500: Loss = -10898.244883179741
1
Iteration 16600: Loss = -10898.093753699493
2
Iteration 16700: Loss = -10898.093749744803
3
Iteration 16800: Loss = -10898.093747031819
4
Iteration 16900: Loss = -10898.093719116105
Iteration 17000: Loss = -10898.098286416795
1
Iteration 17100: Loss = -10898.093665867244
Iteration 17200: Loss = -10898.093789022485
1
Iteration 17300: Loss = -10898.094102731018
2
Iteration 17400: Loss = -10898.399820964622
3
Iteration 17500: Loss = -10898.093463548985
Iteration 17600: Loss = -10898.093651485102
1
Iteration 17700: Loss = -10898.093425005907
Iteration 17800: Loss = -10898.093445210436
1
Iteration 17900: Loss = -10898.093559241297
2
Iteration 18000: Loss = -10898.093395964685
Iteration 18100: Loss = -10898.094056619684
1
Iteration 18200: Loss = -10898.093400725233
2
Iteration 18300: Loss = -10898.093559474444
3
Iteration 18400: Loss = -10898.095605838678
4
Iteration 18500: Loss = -10898.214055803519
5
Iteration 18600: Loss = -10898.095251531413
6
Iteration 18700: Loss = -10898.093385375114
Iteration 18800: Loss = -10898.094349260107
1
Iteration 18900: Loss = -10898.093379281148
Iteration 19000: Loss = -10898.093541576285
1
Iteration 19100: Loss = -10898.093355971798
Iteration 19200: Loss = -10898.093468150188
1
Iteration 19300: Loss = -10898.093406941105
2
Iteration 19400: Loss = -10898.134563189491
3
Iteration 19500: Loss = -10898.09358522293
4
Iteration 19600: Loss = -10898.093954463398
5
Iteration 19700: Loss = -10898.187299703262
6
Iteration 19800: Loss = -10898.09330743233
Iteration 19900: Loss = -10898.095800771964
1
tensor([[-10.2855,   7.5755],
        [ -7.7365,   5.2683],
        [-10.2395,   7.9447],
        [-10.0234,   8.3030],
        [ -9.9219,   8.3302],
        [-10.2349,   8.8462],
        [ -9.7964,   8.3232],
        [-10.7828,   8.1764],
        [-10.3424,   8.7699],
        [-10.1112,   8.4179],
        [-10.7543,   8.7108],
        [-10.2812,   8.2725],
        [ -9.9567,   8.2283],
        [-10.1034,   8.4518],
        [ -9.8245,   8.3426],
        [ -9.6478,   8.1650],
        [-10.3619,   8.7563],
        [-12.5536,   8.0388],
        [ -9.2209,   7.4599],
        [-12.2223,   7.6071],
        [ -9.7934,   8.3596],
        [-10.1795,   8.7396],
        [-10.1509,   8.7636],
        [ -8.3334,   6.2827],
        [-10.4981,   9.0753],
        [-10.0700,   8.5080],
        [-11.0068,   8.4552],
        [-10.2657,   8.5461],
        [-10.2504,   8.7557],
        [-10.0174,   8.4481],
        [-10.7062,   8.3883],
        [ -9.9558,   8.4253],
        [ -9.8000,   8.3994],
        [-10.8933,   8.4670],
        [-10.4495,   8.8848],
        [-10.4173,   8.7874],
        [-10.0902,   8.3127],
        [-10.9699,   8.7807],
        [-10.2787,   8.4604],
        [-10.0705,   8.3378],
        [-10.3078,   7.3146],
        [-10.5947,   8.8985],
        [-11.4203,   9.0299],
        [ -9.8318,   8.3796],
        [ -9.5068,   8.1160],
        [ -9.9203,   8.1752],
        [-10.5588,   8.6578],
        [ -9.8037,   8.2662],
        [ -9.6326,   8.2457],
        [-10.1915,   8.6143],
        [ -9.6038,   8.2174],
        [ -9.8966,   8.3957],
        [-10.0096,   8.1625],
        [-10.6865,   8.5079],
        [ -9.8317,   8.3870],
        [-10.0254,   8.3204],
        [-10.5983,   8.5826],
        [ -9.4005,   7.9052],
        [ -9.9912,   8.6018],
        [ -9.9980,   8.5057],
        [-10.8477,   8.9270],
        [-10.5849,   8.7027],
        [ -9.7691,   8.3824],
        [-10.1742,   8.6370],
        [ -9.4564,   7.8587],
        [-10.1203,   8.2804],
        [-10.6967,   8.7242],
        [-10.8457,   7.7292],
        [ -9.4716,   8.0141],
        [-10.2683,   8.6539],
        [-10.1400,   8.1716],
        [-10.3013,   8.5503],
        [-10.0444,   8.5142],
        [-10.4500,   8.4020],
        [-11.1620,   7.6631],
        [-10.5830,   8.2923],
        [-10.2335,   8.6957],
        [-10.5353,   8.4635],
        [-10.8838,   8.4836],
        [ -9.5721,   8.0794],
        [-10.0874,   8.4085],
        [-10.9941,   8.2892],
        [-12.1618,   7.7065],
        [-10.4312,   7.7613],
        [ -9.7208,   8.3183],
        [-10.6427,   7.9985],
        [-10.9342,   8.5431],
        [-10.1859,   8.2189],
        [ -9.5782,   8.1309],
        [ -9.8541,   8.4483],
        [-10.3416,   8.4023],
        [-10.6997,   8.0666],
        [ -9.8872,   8.3652],
        [ -9.8285,   8.3659],
        [-10.4471,   9.0608],
        [-10.3942,   8.8626],
        [-10.3597,   8.5575],
        [ -9.8242,   8.3534],
        [ -9.8513,   8.4607],
        [ -9.8536,   7.7482]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9999e-01, 1.0725e-05],
        [1.8311e-02, 9.8169e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([3.9601e-08, 1.0000e+00], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0995, 0.1384],
         [0.9445, 0.1619]],

        [[0.5412, 0.2201],
         [0.3285, 0.5317]],

        [[0.8258, 0.2057],
         [0.0976, 0.7075]],

        [[0.8516, 0.0865],
         [0.5385, 0.0137]],

        [[0.5777, 0.1264],
         [0.5881, 0.8718]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0007766707522784365
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0010149900615556472
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.011530202595462608
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.004212316740111065
Global Adjusted Rand Index: 0.00035791375535684
Average Adjusted Rand Index: 0.0015112410329257506
10832.598344230853
new:  [3.206515228510384e-05, -9.312886032595102e-05, 0.00019239700891914054, 0.00035791375535684] [0.0008288350629887435, 0.0022089942882114956, 1.727640633392342e-05, 0.0015112410329257506] [10900.921616719877, 10900.227508543623, 10901.381121589944, 10898.093138543893]
prior:  [3.206515228510384e-05, 3.206515228510384e-05, 3.206515228510384e-05, 0.0] [-0.0003077958928485548, -0.0003077958928485548, -0.0003077958928485548, 0.0] [10902.338547034617, 10902.273333982022, 10902.26777643662, 10903.599022451695]
-----------------------------------------------------------------------------------------
This iteration is 16
True Objective function: Loss = -11135.596719487165
Iteration 0: Loss = -23250.696531225665
Iteration 10: Loss = -11303.313443250163
Iteration 20: Loss = -11301.991241702826
Iteration 30: Loss = -11301.614999422136
Iteration 40: Loss = -11300.999391016956
Iteration 50: Loss = -11299.62655574501
Iteration 60: Loss = -11297.013848478606
Iteration 70: Loss = -11291.273516479157
Iteration 80: Loss = -11220.768012638548
Iteration 90: Loss = -11102.29231632672
Iteration 100: Loss = -11101.47100870114
Iteration 110: Loss = -11101.47012191221
Iteration 120: Loss = -11101.470107049317
Iteration 130: Loss = -11101.470098822821
Iteration 140: Loss = -11101.470114139323
1
Iteration 150: Loss = -11101.47012836069
2
Iteration 160: Loss = -11101.470102230614
3
Stopping early at iteration 159 due to no improvement.
pi: tensor([[0.7764, 0.2236],
        [0.3008, 0.6992]], dtype=torch.float64)
alpha: tensor([0.5922, 0.4078])
beta: tensor([[[0.2527, 0.0942],
         [0.6414, 0.1933]],

        [[0.8179, 0.0977],
         [0.0654, 0.3366]],

        [[0.9713, 0.1092],
         [0.3295, 0.4037]],

        [[0.8785, 0.1009],
         [0.8933, 0.3638]],

        [[0.6046, 0.0948],
         [0.8765, 0.9629]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599252625159036
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8077222191419615
time is 2
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8077222191419615
time is 3
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.8832424897202349
Average Adjusted Rand Index: 0.8835575660521238
tensor([[-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23172.72699256709
Iteration 100: Loss = -11309.088310316396
Iteration 200: Loss = -11307.100269468872
Iteration 300: Loss = -11293.32578184533
Iteration 400: Loss = -11178.318514163962
Iteration 500: Loss = -11124.251579758615
Iteration 600: Loss = -11099.651904946957
Iteration 700: Loss = -11096.96113493663
Iteration 800: Loss = -11096.62191418128
Iteration 900: Loss = -11096.41775780769
Iteration 1000: Loss = -11096.37382942127
Iteration 1100: Loss = -11096.34459948579
Iteration 1200: Loss = -11096.324946573413
Iteration 1300: Loss = -11096.310988049841
Iteration 1400: Loss = -11096.300311631396
Iteration 1500: Loss = -11096.291866214058
Iteration 1600: Loss = -11096.284700934264
Iteration 1700: Loss = -11096.278733944213
Iteration 1800: Loss = -11096.274152530565
Iteration 1900: Loss = -11096.270143575282
Iteration 2000: Loss = -11096.25288842437
Iteration 2100: Loss = -11096.249651178896
Iteration 2200: Loss = -11096.246995016307
Iteration 2300: Loss = -11096.24471276682
Iteration 2400: Loss = -11096.24259140485
Iteration 2500: Loss = -11096.240739985331
Iteration 2600: Loss = -11096.239094347018
Iteration 2700: Loss = -11096.237204417412
Iteration 2800: Loss = -11096.198824357853
Iteration 2900: Loss = -11096.194475553184
Iteration 3000: Loss = -11096.192841820355
Iteration 3100: Loss = -11096.191494848454
Iteration 3200: Loss = -11096.190600332711
Iteration 3300: Loss = -11096.189946263332
Iteration 3400: Loss = -11096.189267876833
Iteration 3500: Loss = -11096.1906829227
1
Iteration 3600: Loss = -11096.187965834239
Iteration 3700: Loss = -11096.189068260655
1
Iteration 3800: Loss = -11096.187065777845
Iteration 3900: Loss = -11096.18679552147
Iteration 4000: Loss = -11096.186454472534
Iteration 4100: Loss = -11096.18617967599
Iteration 4200: Loss = -11096.18592140255
Iteration 4300: Loss = -11096.185784102103
Iteration 4400: Loss = -11096.18602511692
1
Iteration 4500: Loss = -11096.185540222095
Iteration 4600: Loss = -11096.18515609911
Iteration 4700: Loss = -11096.185343575875
1
Iteration 4800: Loss = -11096.18488196624
Iteration 4900: Loss = -11096.19494962547
1
Iteration 5000: Loss = -11096.184660621304
Iteration 5100: Loss = -11096.185002960254
1
Iteration 5200: Loss = -11096.18441782938
Iteration 5300: Loss = -11096.186349090603
1
Iteration 5400: Loss = -11096.184211787104
Iteration 5500: Loss = -11096.184177472063
Iteration 5600: Loss = -11096.184100839817
Iteration 5700: Loss = -11096.183990186877
Iteration 5800: Loss = -11096.18652482703
1
Iteration 5900: Loss = -11096.184455418488
2
Iteration 6000: Loss = -11096.183974011063
Iteration 6100: Loss = -11096.188780825563
1
Iteration 6200: Loss = -11096.184621835859
2
Iteration 6300: Loss = -11096.183900383905
Iteration 6400: Loss = -11096.183758635127
Iteration 6500: Loss = -11096.18377530003
1
Iteration 6600: Loss = -11096.184234668835
2
Iteration 6700: Loss = -11096.183494169201
Iteration 6800: Loss = -11096.183494105182
Iteration 6900: Loss = -11096.18341313576
Iteration 7000: Loss = -11096.217090551176
1
Iteration 7100: Loss = -11096.187493080679
2
Iteration 7200: Loss = -11096.183519903416
3
Iteration 7300: Loss = -11096.183795737643
4
Iteration 7400: Loss = -11096.184619462905
5
Iteration 7500: Loss = -11096.183871836642
6
Iteration 7600: Loss = -11096.194119546135
7
Iteration 7700: Loss = -11096.184070747717
8
Iteration 7800: Loss = -11096.183103042522
Iteration 7900: Loss = -11096.183165889615
1
Iteration 8000: Loss = -11096.183016668334
Iteration 8100: Loss = -11096.185988121495
1
Iteration 8200: Loss = -11096.183000146892
Iteration 8300: Loss = -11096.250142548333
1
Iteration 8400: Loss = -11096.182949771422
Iteration 8500: Loss = -11096.182935251249
Iteration 8600: Loss = -11096.183195606933
1
Iteration 8700: Loss = -11096.182916260257
Iteration 8800: Loss = -11096.18291861221
1
Iteration 8900: Loss = -11096.182907167018
Iteration 9000: Loss = -11096.182907421853
1
Iteration 9100: Loss = -11096.18290905253
2
Iteration 9200: Loss = -11096.182895312508
Iteration 9300: Loss = -11096.182852725666
Iteration 9400: Loss = -11096.194505881362
1
Iteration 9500: Loss = -11096.182892604242
2
Iteration 9600: Loss = -11096.182856140507
3
Iteration 9700: Loss = -11096.186147013968
4
Iteration 9800: Loss = -11096.182836353913
Iteration 9900: Loss = -11096.182856857014
1
Iteration 10000: Loss = -11096.182862588712
2
Iteration 10100: Loss = -11096.182817080444
Iteration 10200: Loss = -11096.183235299897
1
Iteration 10300: Loss = -11096.182819523146
2
Iteration 10400: Loss = -11096.1828088201
Iteration 10500: Loss = -11096.527475470071
1
Iteration 10600: Loss = -11096.182809704342
2
Iteration 10700: Loss = -11096.182789682369
Iteration 10800: Loss = -11096.187482639454
1
Iteration 10900: Loss = -11096.182807922323
2
Iteration 11000: Loss = -11096.182794554366
3
Iteration 11100: Loss = -11096.183964231332
4
Iteration 11200: Loss = -11096.182812092815
5
Iteration 11300: Loss = -11096.182822398363
6
Iteration 11400: Loss = -11096.202048721743
7
Iteration 11500: Loss = -11096.182800182456
8
Iteration 11600: Loss = -11096.182796294343
9
Iteration 11700: Loss = -11096.192567074197
10
Stopping early at iteration 11700 due to no improvement.
tensor([[  3.7373,  -8.3526],
        [ -0.7324,  -3.8828],
        [ -8.2425,   3.6273],
        [ -4.6842,   0.0690],
        [ -7.3261,   2.7109],
        [ -7.7122,   3.0970],
        [ -7.7653,   3.1501],
        [  1.4134,  -6.0286],
        [ -4.3612,  -0.2540],
        [ -4.8209,   0.2056],
        [ -5.3614,   0.7462],
        [  0.5748,  -5.1901],
        [ -6.9996,   2.3844],
        [ -9.6696,   5.0544],
        [-10.4954,   5.8801],
        [ -9.4281,   4.8129],
        [ -8.0433,   3.4281],
        [ -8.7117,   4.0965],
        [-10.4644,   5.8492],
        [  2.1564,  -6.7717],
        [  5.6494, -10.2646],
        [ -8.3216,   3.7064],
        [  1.3818,  -5.9970],
        [  0.0742,  -4.6895],
        [ -5.9109,   1.2957],
        [ -6.9448,   2.3296],
        [ -5.5198,   0.9046],
        [ -4.2960,  -0.3192],
        [  5.3622,  -9.9774],
        [  4.2328,  -8.8480],
        [ -1.0574,  -3.5578],
        [  0.7270,  -5.3422],
        [ -9.0348,   4.4196],
        [ -6.6337,   2.0185],
        [ -4.2443,  -0.3709],
        [ -7.5951,   2.9799],
        [ -6.7793,   2.1641],
        [ -8.6238,   4.0086],
        [ -9.0703,   4.4551],
        [ -0.2600,  -4.3552],
        [  1.5624,  -6.1776],
        [ -8.0576,   3.4424],
        [ -8.9743,   4.3591],
        [ -8.4940,   3.8787],
        [ -6.7359,   2.1207],
        [ -5.3572,   0.7420],
        [ -3.6184,  -0.9968],
        [ -9.1207,   4.5055],
        [ -9.3042,   4.6890],
        [ -6.0156,   1.4003],
        [ -6.4070,   1.7918],
        [  2.4781,  -7.0933],
        [ -0.6158,  -3.9995],
        [ -0.2807,  -4.3345],
        [  1.0429,  -5.6581],
        [ -8.0755,   3.4603],
        [  3.3961,  -8.0113],
        [ -9.8299,   5.2146],
        [  4.5814,  -9.1966],
        [ -0.7003,  -3.9149],
        [ -8.8337,   4.2185],
        [  0.9305,  -5.5457],
        [ -9.1403,   4.5251],
        [ -7.7934,   3.1781],
        [  0.8100,  -5.4252],
        [ -6.4732,   1.8579],
        [  0.0154,  -4.6306],
        [ -3.0165,  -1.5987],
        [ -1.5983,  -3.0169],
        [  2.3256,  -6.9408],
        [ -3.9565,  -0.6587],
        [ -6.8743,   2.2591],
        [ -4.0877,  -0.5275],
        [ -0.3601,  -4.2551],
        [  1.9584,  -6.5736],
        [  0.5103,  -5.1255],
        [ -5.0973,   0.4821],
        [ -1.2739,  -3.3413],
        [ -6.6603,   2.0451],
        [ -8.3951,   3.7799],
        [ -9.4036,   4.7884],
        [-10.1830,   5.5678],
        [ -6.8936,   2.2784],
        [  3.0804,  -7.6956],
        [ -8.9097,   4.2945],
        [ -6.4088,   1.7936],
        [ -7.4631,   2.8479],
        [  2.5241,  -7.1393],
        [ -6.8596,   2.2444],
        [ -5.4538,   0.8385],
        [ -5.2768,   0.6616],
        [  1.4908,  -6.1060],
        [  1.7156,  -6.3308],
        [ -7.1918,   2.5766],
        [ -3.1011,  -1.5141],
        [  0.7784,  -5.3937],
        [ -4.8587,   0.2435],
        [ -0.9538,  -3.6614],
        [  1.9553,  -6.5705],
        [  2.6139,  -7.2291]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7293, 0.2707],
        [0.2042, 0.7958]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3807, 0.6193], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1957, 0.0942],
         [0.6414, 0.2583]],

        [[0.8179, 0.0986],
         [0.0654, 0.3366]],

        [[0.9713, 0.1101],
         [0.3295, 0.4037]],

        [[0.8785, 0.1012],
         [0.8933, 0.3638]],

        [[0.6046, 0.0946],
         [0.8765, 0.9629]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599252625159036
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8822858823962049
time is 2
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8446221001520428
time is 3
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.8984111563755577
Average Adjusted Rand Index: 0.898011624216665
Iteration 0: Loss = -26434.548710737763
Iteration 10: Loss = -11299.583231449406
Iteration 20: Loss = -11296.83448826948
Iteration 30: Loss = -11290.711352389882
Iteration 40: Loss = -11211.880205898977
Iteration 50: Loss = -11102.031987265316
Iteration 60: Loss = -11101.470507348817
Iteration 70: Loss = -11101.470097116355
Iteration 80: Loss = -11101.470099468424
1
Iteration 90: Loss = -11101.470105562363
2
Iteration 100: Loss = -11101.4701109017
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.6992, 0.3008],
        [0.2236, 0.7764]], dtype=torch.float64)
alpha: tensor([0.4078, 0.5922])
beta: tensor([[[0.1933, 0.0942],
         [0.8289, 0.2527]],

        [[0.9474, 0.0977],
         [0.6728, 0.6756]],

        [[0.3121, 0.1092],
         [0.2461, 0.3436]],

        [[0.3407, 0.1009],
         [0.9354, 0.7695]],

        [[0.4001, 0.0948],
         [0.6531, 0.9078]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599252625159036
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8077222191419615
time is 2
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8077222191419615
time is 3
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.8832424897202349
Average Adjusted Rand Index: 0.8835575660521238
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26434.28287785657
Iteration 100: Loss = -11335.219932207003
Iteration 200: Loss = -11319.4815631232
Iteration 300: Loss = -11310.15108728997
Iteration 400: Loss = -11308.178269191598
Iteration 500: Loss = -11307.586311918736
Iteration 600: Loss = -11306.451432834854
Iteration 700: Loss = -11305.058522540287
Iteration 800: Loss = -11304.350540457124
Iteration 900: Loss = -11303.837440813679
Iteration 1000: Loss = -11303.132702806224
Iteration 1100: Loss = -11302.312137682744
Iteration 1200: Loss = -11302.161167567658
Iteration 1300: Loss = -11302.11148727355
Iteration 1400: Loss = -11301.954453097633
Iteration 1500: Loss = -11301.71684905573
Iteration 1600: Loss = -11301.686951898437
Iteration 1700: Loss = -11301.588270798744
Iteration 1800: Loss = -11301.234344725011
Iteration 1900: Loss = -11301.054357835435
Iteration 2000: Loss = -11300.857625103907
Iteration 2100: Loss = -11300.687087356146
Iteration 2200: Loss = -11300.572780024513
Iteration 2300: Loss = -11300.495472432769
Iteration 2400: Loss = -11300.439688497989
Iteration 2500: Loss = -11300.398587242822
Iteration 2600: Loss = -11300.366651043601
Iteration 2700: Loss = -11300.341175055015
Iteration 2800: Loss = -11300.320275520691
Iteration 2900: Loss = -11300.303000890744
Iteration 3000: Loss = -11300.288447844898
Iteration 3100: Loss = -11300.276166936925
Iteration 3200: Loss = -11300.265605599678
Iteration 3300: Loss = -11300.2563552896
Iteration 3400: Loss = -11300.248164542547
Iteration 3500: Loss = -11300.240793585204
Iteration 3600: Loss = -11300.234163289417
Iteration 3700: Loss = -11300.22812290767
Iteration 3800: Loss = -11300.222612743246
Iteration 3900: Loss = -11300.217554326899
Iteration 4000: Loss = -11300.212921861195
Iteration 4100: Loss = -11300.208621427268
Iteration 4200: Loss = -11300.204649465462
Iteration 4300: Loss = -11300.200989754509
Iteration 4400: Loss = -11300.197552963456
Iteration 4500: Loss = -11300.19432655486
Iteration 4600: Loss = -11300.191356938607
Iteration 4700: Loss = -11300.188611282965
Iteration 4800: Loss = -11300.18599929653
Iteration 4900: Loss = -11300.183550846781
Iteration 5000: Loss = -11300.18125649561
Iteration 5100: Loss = -11300.17911152067
Iteration 5200: Loss = -11300.177124849213
Iteration 5300: Loss = -11300.17522752349
Iteration 5400: Loss = -11300.173408958563
Iteration 5500: Loss = -11300.171750110045
Iteration 5600: Loss = -11300.17016063937
Iteration 5700: Loss = -11300.168661527321
Iteration 5800: Loss = -11300.167261533097
Iteration 5900: Loss = -11300.165933041751
Iteration 6000: Loss = -11300.164691899598
Iteration 6100: Loss = -11300.163528844574
Iteration 6200: Loss = -11300.162399184484
Iteration 6300: Loss = -11300.161375751703
Iteration 6400: Loss = -11300.16034881551
Iteration 6500: Loss = -11300.159417294706
Iteration 6600: Loss = -11300.158569536212
Iteration 6700: Loss = -11300.157703101691
Iteration 6800: Loss = -11300.156932316555
Iteration 6900: Loss = -11300.158400164346
1
Iteration 7000: Loss = -11300.155442350535
Iteration 7100: Loss = -11300.154798187426
Iteration 7200: Loss = -11300.154232066658
Iteration 7300: Loss = -11300.153627064075
Iteration 7400: Loss = -11300.153013885138
Iteration 7500: Loss = -11300.152444306526
Iteration 7600: Loss = -11300.152223486746
Iteration 7700: Loss = -11300.151498964451
Iteration 7800: Loss = -11300.151019658633
Iteration 7900: Loss = -11300.167324719549
1
Iteration 8000: Loss = -11300.150218616549
Iteration 8100: Loss = -11300.149815982086
Iteration 8200: Loss = -11300.149683449818
Iteration 8300: Loss = -11300.149082368449
Iteration 8400: Loss = -11300.148794471548
Iteration 8500: Loss = -11300.152335442936
1
Iteration 8600: Loss = -11300.148159091365
Iteration 8700: Loss = -11300.147905405722
Iteration 8800: Loss = -11300.19002176105
1
Iteration 8900: Loss = -11300.147384822294
Iteration 9000: Loss = -11300.147192586142
Iteration 9100: Loss = -11300.21211299125
1
Iteration 9200: Loss = -11300.147135913308
Iteration 9300: Loss = -11300.146525797649
Iteration 9400: Loss = -11300.147276993299
1
Iteration 9500: Loss = -11300.146164591375
Iteration 9600: Loss = -11300.145976633878
Iteration 9700: Loss = -11300.361573985016
1
Iteration 9800: Loss = -11300.145709978611
Iteration 9900: Loss = -11300.145485465377
Iteration 10000: Loss = -11300.146807738638
1
Iteration 10100: Loss = -11300.145228096795
Iteration 10200: Loss = -11300.145379472904
1
Iteration 10300: Loss = -11300.14532122588
2
Iteration 10400: Loss = -11300.144902603652
Iteration 10500: Loss = -11300.166527703912
1
Iteration 10600: Loss = -11300.144680850464
Iteration 10700: Loss = -11300.144594299481
Iteration 10800: Loss = -11300.14968743838
1
Iteration 10900: Loss = -11300.144583036781
Iteration 11000: Loss = -11300.14430000567
Iteration 11100: Loss = -11300.18002503448
1
Iteration 11200: Loss = -11300.1441597339
Iteration 11300: Loss = -11300.144133590307
Iteration 11400: Loss = -11300.203643025212
1
Iteration 11500: Loss = -11300.143976126854
Iteration 11600: Loss = -11300.144135687515
1
Iteration 11700: Loss = -11300.143846119816
Iteration 11800: Loss = -11300.14386935874
1
Iteration 11900: Loss = -11300.143791850209
Iteration 12000: Loss = -11300.143752832255
Iteration 12100: Loss = -11300.143780576103
1
Iteration 12200: Loss = -11300.143832386642
2
Iteration 12300: Loss = -11300.143549941908
Iteration 12400: Loss = -11300.14354211734
Iteration 12500: Loss = -11300.191385578437
1
Iteration 12600: Loss = -11300.143476028059
Iteration 12700: Loss = -11300.1434028312
Iteration 12800: Loss = -11300.143489009326
1
Iteration 12900: Loss = -11300.143459492056
2
Iteration 13000: Loss = -11300.143463614442
3
Iteration 13100: Loss = -11300.143443264302
4
Iteration 13200: Loss = -11300.14349998385
5
Iteration 13300: Loss = -11300.143476892226
6
Iteration 13400: Loss = -11300.143219879827
Iteration 13500: Loss = -11300.145449601303
1
Iteration 13600: Loss = -11300.143181891177
Iteration 13700: Loss = -11300.169602240176
1
Iteration 13800: Loss = -11300.143142279909
Iteration 13900: Loss = -11300.157141544878
1
Iteration 14000: Loss = -11300.143119057724
Iteration 14100: Loss = -11300.2945316816
1
Iteration 14200: Loss = -11300.143107402771
Iteration 14300: Loss = -11300.14308914568
Iteration 14400: Loss = -11300.14358189515
1
Iteration 14500: Loss = -11300.167593408874
2
Iteration 14600: Loss = -11300.144244465546
3
Iteration 14700: Loss = -11300.143143399451
4
Iteration 14800: Loss = -11300.143069062937
Iteration 14900: Loss = -11300.14299935006
Iteration 15000: Loss = -11300.157921524868
1
Iteration 15100: Loss = -11300.1430407431
2
Iteration 15200: Loss = -11300.143380688834
3
Iteration 15300: Loss = -11300.143000224129
4
Iteration 15400: Loss = -11300.143095050495
5
Iteration 15500: Loss = -11300.143366221948
6
Iteration 15600: Loss = -11300.143639371088
7
Iteration 15700: Loss = -11300.14347248394
8
Iteration 15800: Loss = -11298.952783251621
Iteration 15900: Loss = -11298.948680105843
Iteration 16000: Loss = -11298.948761093447
1
Iteration 16100: Loss = -11298.948464935364
Iteration 16200: Loss = -11298.94839739597
Iteration 16300: Loss = -11298.948636335801
1
Iteration 16400: Loss = -11298.948354329119
Iteration 16500: Loss = -11298.949404124762
1
Iteration 16600: Loss = -11298.962795857879
2
Iteration 16700: Loss = -11298.948245194892
Iteration 16800: Loss = -11298.94826884961
1
Iteration 16900: Loss = -11298.948697862266
2
Iteration 17000: Loss = -11298.949108675168
3
Iteration 17100: Loss = -11298.948263336722
4
Iteration 17200: Loss = -11298.948278183714
5
Iteration 17300: Loss = -11298.948169968162
Iteration 17400: Loss = -11298.948248043065
1
Iteration 17500: Loss = -11298.948158442292
Iteration 17600: Loss = -11298.94965801518
1
Iteration 17700: Loss = -11299.103821274759
2
Iteration 17800: Loss = -11298.948166105218
3
Iteration 17900: Loss = -11298.948157194727
Iteration 18000: Loss = -11298.948126914776
Iteration 18100: Loss = -11299.122736796728
1
Iteration 18200: Loss = -11298.948493983798
2
Iteration 18300: Loss = -11298.949615921734
3
Iteration 18400: Loss = -11298.948121152116
Iteration 18500: Loss = -11298.952950170844
1
Iteration 18600: Loss = -11298.948129067963
2
Iteration 18700: Loss = -11298.948164243928
3
Iteration 18800: Loss = -11298.948103943396
Iteration 18900: Loss = -11299.186147793811
1
Iteration 19000: Loss = -11298.94814239984
2
Iteration 19100: Loss = -11298.948101236818
Iteration 19200: Loss = -11298.948114994724
1
Iteration 19300: Loss = -11298.948507397132
2
Iteration 19400: Loss = -11298.948285319899
3
Iteration 19500: Loss = -11298.948107272894
4
Iteration 19600: Loss = -11298.949242050474
5
Iteration 19700: Loss = -11298.948841286314
6
Iteration 19800: Loss = -11298.949616980339
7
Iteration 19900: Loss = -11298.948120592202
8
tensor([[  8.0338,  -9.4697],
        [  7.1969,  -9.9747],
        [  8.2012,  -9.6051],
        [  8.2077,  -9.6784],
        [  7.9233, -10.2341],
        [  7.7170,  -9.3943],
        [  6.9498,  -9.9896],
        [  8.3263, -10.2080],
        [  8.2225,  -9.6138],
        [  7.7789,  -9.1796],
        [  7.9293, -10.1842],
        [  7.7933,  -9.6690],
        [  7.8652,  -9.4842],
        [  7.5208,  -9.0255],
        [  7.1884,  -9.9962],
        [  7.6457,  -9.2717],
        [  7.8346,  -9.2783],
        [  7.6832,  -9.2342],
        [  7.2590,  -9.4893],
        [  7.3053,  -9.0525],
        [  8.0068,  -9.4740],
        [  7.6091,  -9.3956],
        [  7.9286,  -9.3833],
        [  6.4367, -10.2670],
        [  7.6979,  -9.4780],
        [  7.8130,  -9.2502],
        [  7.8368,  -9.4815],
        [ -5.6141,   4.1649],
        [  7.0582, -10.1129],
        [  8.4923, -10.6294],
        [  7.1140,  -9.8714],
        [  8.1358,  -9.5368],
        [  6.9633,  -9.4560],
        [  7.3295,  -9.0687],
        [  8.0220,  -9.4231],
        [  7.6050,  -8.9920],
        [  7.5484, -11.1076],
        [  7.6927,  -9.2019],
        [  7.2673,  -8.8888],
        [  7.9944,  -9.7720],
        [  7.8193,  -9.2078],
        [  7.3113,  -9.5286],
        [  7.9330,  -9.9452],
        [  7.5890,  -9.2765],
        [  7.6589,  -9.3325],
        [  7.9078,  -9.8474],
        [  8.2447,  -9.6355],
        [  6.8624, -10.8321],
        [  7.6419,  -9.2027],
        [  7.4520, -10.1396],
        [  7.5930,  -9.0129],
        [  7.5787,  -9.3000],
        [  7.4967, -10.3657],
        [  8.0276,  -9.4271],
        [  8.1494,  -9.6116],
        [  7.2710,  -9.2293],
        [  7.8724,  -9.2613],
        [  7.5870,  -9.1115],
        [  7.5762,  -9.5345],
        [  7.8044,  -9.4089],
        [  7.8773, -10.0597],
        [  7.6437,  -9.3656],
        [  7.3889,  -9.3661],
        [  7.9863,  -9.3749],
        [  8.0050,  -9.9670],
        [  7.7483,  -9.2873],
        [  8.0030,  -9.5382],
        [  8.2535,  -9.7445],
        [  8.2578,  -9.9119],
        [  7.2013, -10.3182],
        [  6.9719,  -9.7453],
        [  7.5780,  -9.0716],
        [  6.8410, -10.0152],
        [  7.7268,  -9.6066],
        [  7.8337,  -9.4351],
        [  8.3801,  -9.7860],
        [  7.9753,  -9.3991],
        [  7.5110,  -9.5015],
        [  7.7250,  -9.3559],
        [  7.5421,  -9.7115],
        [  7.4930,  -9.7281],
        [  7.8128,  -9.3525],
        [  7.5780,  -9.0974],
        [  8.1862,  -9.5744],
        [  7.7418,  -9.1908],
        [  7.7930,  -9.3267],
        [  6.8980, -11.5132],
        [  8.0398,  -9.5787],
        [  8.1965,  -9.7898],
        [  8.1095,  -9.7585],
        [  7.8576, -10.1476],
        [  7.9104,  -9.8065],
        [  7.4702,  -9.4194],
        [  7.9917,  -9.4741],
        [  7.8917,  -9.4508],
        [  7.0142,  -8.4248],
        [  7.6857, -10.4964],
        [  7.8651,  -9.3293],
        [  8.1095,  -9.6535],
        [  8.1984,  -9.6560]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 4.7435e-08],
        [1.2465e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9900, 0.0100], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1697, 0.2020],
         [0.8289, 0.2910]],

        [[0.9474, 0.2387],
         [0.6728, 0.6756]],

        [[0.3121, 0.3232],
         [0.2461, 0.3436]],

        [[0.3407, 0.0707],
         [0.9354, 0.7695]],

        [[0.4001, 0.2338],
         [0.6531, 0.9078]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: -0.007201490658206174
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 40
Adjusted Rand Index: -0.007201490658206174
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 39
Adjusted Rand Index: -0.007726325420627255
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
Global Adjusted Rand Index: -0.0030653678042551883
Average Adjusted Rand Index: -0.004712179825394256
Iteration 0: Loss = -15459.460025088365
Iteration 10: Loss = -11105.323176888687
Iteration 20: Loss = -11101.472901677278
Iteration 30: Loss = -11101.47004189212
Iteration 40: Loss = -11101.470098587635
1
Iteration 50: Loss = -11101.470095208815
2
Iteration 60: Loss = -11101.470105106648
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.6992, 0.3008],
        [0.2236, 0.7764]], dtype=torch.float64)
alpha: tensor([0.4078, 0.5922])
beta: tensor([[[0.1933, 0.0942],
         [0.4474, 0.2527]],

        [[0.7001, 0.0977],
         [0.2093, 0.6846]],

        [[0.8332, 0.1092],
         [0.2326, 0.5719]],

        [[0.5326, 0.1009],
         [0.8823, 0.8781]],

        [[0.7340, 0.0948],
         [0.4110, 0.7323]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599252625159036
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8077222191419615
time is 2
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8077222191419615
time is 3
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.8832424897202349
Average Adjusted Rand Index: 0.8835575660521238
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -15459.00196195981
Iteration 100: Loss = -11305.749689916875
Iteration 200: Loss = -11275.751566999847
Iteration 300: Loss = -11145.02316907351
Iteration 400: Loss = -11101.231775708053
Iteration 500: Loss = -11099.395787769927
Iteration 600: Loss = -11099.211222428912
Iteration 700: Loss = -11099.030605697208
Iteration 800: Loss = -11096.56579816163
Iteration 900: Loss = -11096.535984696144
Iteration 1000: Loss = -11096.515779457142
Iteration 1100: Loss = -11096.495263639988
Iteration 1200: Loss = -11096.453531603458
Iteration 1300: Loss = -11096.428668564695
Iteration 1400: Loss = -11096.414393010355
Iteration 1500: Loss = -11096.403862563717
Iteration 1600: Loss = -11096.396152962956
Iteration 1700: Loss = -11096.389758029796
Iteration 1800: Loss = -11096.385618302102
Iteration 1900: Loss = -11096.383059497684
Iteration 2000: Loss = -11096.38097929984
Iteration 2100: Loss = -11096.379199501953
Iteration 2200: Loss = -11096.377663939329
Iteration 2300: Loss = -11096.37618961419
Iteration 2400: Loss = -11096.37471677103
Iteration 2500: Loss = -11096.373054833677
Iteration 2600: Loss = -11096.371590840039
Iteration 2700: Loss = -11096.370624123518
Iteration 2800: Loss = -11096.369763438883
Iteration 2900: Loss = -11096.369032767716
Iteration 3000: Loss = -11096.368344506174
Iteration 3100: Loss = -11096.367643028358
Iteration 3200: Loss = -11096.367000344533
Iteration 3300: Loss = -11096.366224690984
Iteration 3400: Loss = -11096.3651852936
Iteration 3500: Loss = -11096.362192807394
Iteration 3600: Loss = -11096.358592007817
Iteration 3700: Loss = -11096.35784597493
Iteration 3800: Loss = -11096.357369285084
Iteration 3900: Loss = -11096.356760263563
Iteration 4000: Loss = -11096.35616763832
Iteration 4100: Loss = -11096.355430884485
Iteration 4200: Loss = -11096.356204684353
1
Iteration 4300: Loss = -11096.402986488942
2
Iteration 4400: Loss = -11096.350734394675
Iteration 4500: Loss = -11096.349280101716
Iteration 4600: Loss = -11096.348242606287
Iteration 4700: Loss = -11096.352435223187
1
Iteration 4800: Loss = -11096.347815282448
Iteration 4900: Loss = -11096.347658521718
Iteration 5000: Loss = -11096.347532706777
Iteration 5100: Loss = -11096.347303759185
Iteration 5200: Loss = -11096.654106334681
1
Iteration 5300: Loss = -11096.346959080434
Iteration 5400: Loss = -11096.346807617765
Iteration 5500: Loss = -11096.451766877535
1
Iteration 5600: Loss = -11096.346552149545
Iteration 5700: Loss = -11096.346436464763
Iteration 5800: Loss = -11096.346350930215
Iteration 5900: Loss = -11096.346352910605
1
Iteration 6000: Loss = -11096.346223597518
Iteration 6100: Loss = -11096.346172956019
Iteration 6200: Loss = -11096.346141320106
Iteration 6300: Loss = -11096.346000237496
Iteration 6400: Loss = -11096.345990638147
Iteration 6500: Loss = -11096.345990171087
Iteration 6600: Loss = -11096.345855877269
Iteration 6700: Loss = -11096.34630595776
1
Iteration 6800: Loss = -11096.345695160051
Iteration 6900: Loss = -11096.344068040864
Iteration 7000: Loss = -11096.355752858382
1
Iteration 7100: Loss = -11096.34315598834
Iteration 7200: Loss = -11096.343071305737
Iteration 7300: Loss = -11096.377082842046
1
Iteration 7400: Loss = -11096.343056873193
Iteration 7500: Loss = -11096.343001099134
Iteration 7600: Loss = -11096.418480760138
1
Iteration 7700: Loss = -11096.34293919714
Iteration 7800: Loss = -11096.342942693922
1
Iteration 7900: Loss = -11096.342837504899
Iteration 8000: Loss = -11096.343154787943
1
Iteration 8100: Loss = -11096.342267108133
Iteration 8200: Loss = -11096.34221601123
Iteration 8300: Loss = -11096.342901502567
1
Iteration 8400: Loss = -11096.342018502653
Iteration 8500: Loss = -11096.3393880105
Iteration 8600: Loss = -11096.333554088018
Iteration 8700: Loss = -11096.333462827133
Iteration 8800: Loss = -11096.33342878501
Iteration 8900: Loss = -11096.3337479555
1
Iteration 9000: Loss = -11096.33340150611
Iteration 9100: Loss = -11096.333473619889
1
Iteration 9200: Loss = -11096.333388961513
Iteration 9300: Loss = -11096.33337480563
Iteration 9400: Loss = -11096.412448467963
1
Iteration 9500: Loss = -11096.333340485253
Iteration 9600: Loss = -11096.333334024403
Iteration 9700: Loss = -11096.787221088962
1
Iteration 9800: Loss = -11096.333079762171
Iteration 9900: Loss = -11096.333054285713
Iteration 10000: Loss = -11096.860802264146
1
Iteration 10100: Loss = -11096.333056937621
2
Iteration 10200: Loss = -11096.333053112732
Iteration 10300: Loss = -11096.359089516473
1
Iteration 10400: Loss = -11096.33300172916
Iteration 10500: Loss = -11096.333033232531
1
Iteration 10600: Loss = -11096.33514348044
2
Iteration 10700: Loss = -11096.33292706004
Iteration 10800: Loss = -11096.332884459342
Iteration 10900: Loss = -11096.332667539515
Iteration 11000: Loss = -11096.33244021253
Iteration 11100: Loss = -11096.332485925212
1
Iteration 11200: Loss = -11096.332469278379
2
Iteration 11300: Loss = -11096.332420367837
Iteration 11400: Loss = -11096.332407199186
Iteration 11500: Loss = -11096.332851364617
1
Iteration 11600: Loss = -11096.332393254828
Iteration 11700: Loss = -11096.332911070915
1
Iteration 11800: Loss = -11096.332459150612
2
Iteration 11900: Loss = -11096.332368940857
Iteration 12000: Loss = -11096.334040081205
1
Iteration 12100: Loss = -11096.332452093891
2
Iteration 12200: Loss = -11096.332369827205
3
Iteration 12300: Loss = -11096.657105650982
4
Iteration 12400: Loss = -11096.33238571604
5
Iteration 12500: Loss = -11096.203055483918
Iteration 12600: Loss = -11096.203736253881
1
Iteration 12700: Loss = -11096.241568899479
2
Iteration 12800: Loss = -11096.201634672572
Iteration 12900: Loss = -11096.20422121692
1
Iteration 13000: Loss = -11096.201618363766
Iteration 13100: Loss = -11096.200385198024
Iteration 13200: Loss = -11096.18038618253
Iteration 13300: Loss = -11096.472870019381
1
Iteration 13400: Loss = -11096.180395457892
2
Iteration 13500: Loss = -11096.180413812943
3
Iteration 13600: Loss = -11096.183187280236
4
Iteration 13700: Loss = -11096.180406316511
5
Iteration 13800: Loss = -11096.180448974344
6
Iteration 13900: Loss = -11096.180459120602
7
Iteration 14000: Loss = -11096.180403091708
8
Iteration 14100: Loss = -11096.198923661064
9
Iteration 14200: Loss = -11096.180393902534
10
Stopping early at iteration 14200 due to no improvement.
tensor([[ 5.0707, -7.1302],
        [ 0.7805, -2.3730],
        [-6.6486,  5.2606],
        [-3.1803,  1.5678],
        [-5.7380,  4.2989],
        [-6.0741,  4.6841],
        [-6.1510,  4.7012],
        [ 3.0017, -4.4400],
        [-4.3585, -0.2567],
        [-3.2759,  1.7508],
        [-3.7857,  2.3209],
        [ 2.1240, -3.6410],
        [-5.7342,  3.6349],
        [-8.5558,  6.4439],
        [-9.2247,  7.8174],
        [-9.0393,  7.3547],
        [-6.4575,  5.0231],
        [-7.5593,  5.3778],
        [-9.3120,  7.8764],
        [ 3.4103, -5.5177],
        [ 4.7649, -6.1969],
        [-6.8565,  5.1739],
        [ 2.8448, -4.5340],
        [ 1.6017, -3.1644],
        [-4.7119,  2.4937],
        [-6.7190,  2.5555],
        [-5.5197,  0.9045],
        [-3.2229,  0.7505],
        [ 3.3447, -5.8766],
        [ 6.0443, -7.7963],
        [-0.1395, -2.6443],
        [ 2.3204, -3.7487],
        [-9.6659,  8.2446],
        [-5.0213,  3.6285],
        [-2.6893,  1.1805],
        [-6.1761,  4.4000],
        [-5.1820,  3.7612],
        [-7.3882,  5.2562],
        [-8.1362,  6.3440],
        [ 1.2822, -2.8152],
        [ 3.1182, -4.6214],
        [-7.8112,  3.7165],
        [-7.3961,  5.9422],
        [-7.3596,  5.0131],
        [-5.7531,  3.1034],
        [-3.7433,  2.3564],
        [-2.1265,  0.4899],
        [-8.0681,  6.6736],
        [-8.3346,  6.8077],
        [-4.4012,  3.0144],
        [-4.8082,  3.3866],
        [ 4.0920, -5.4791],
        [ 0.2723, -3.1151],
        [ 0.9320, -3.1241],
        [ 1.0432, -5.6584],
        [-8.0754,  3.4602],
        [ 4.9431, -6.4702],
        [-8.5509,  6.9222],
        [ 5.8624, -8.0344],
        [ 0.7909, -2.4291],
        [-7.2377,  5.8169],
        [ 2.5450, -3.9313],
        [-7.5368,  6.1482],
        [-7.0950,  5.0020],
        [ 2.2565, -3.9788],
        [-5.2703,  3.0606],
        [ 1.4700, -3.1752],
        [-1.4323, -0.0201],
        [-0.9990, -2.4238],
        [ 3.9359, -5.3303],
        [-2.3629,  0.9311],
        [-5.6529,  3.4805],
        [-4.0856, -0.5296],
        [ 1.1742, -2.7282],
        [ 3.3394, -5.1919],
        [ 1.0201, -4.6163],
        [-3.4991,  2.0799],
        [ 0.2437, -1.8290],
        [-5.3226,  3.3826],
        [-7.1386,  5.0362],
        [-8.5149,  7.1281],
        [-8.8833,  7.4968],
        [-5.8806,  3.2826],
        [ 4.2031, -6.5733],
        [-8.0186,  5.5274],
        [-5.3660,  2.8363],
        [-5.9522,  4.3289],
        [ 4.0734, -5.5899],
        [-5.3203,  3.7798],
        [-4.0155,  2.2765],
        [-3.6894,  2.2483],
        [ 2.5664, -5.0300],
        [ 3.3108, -4.7363],
        [-6.1588,  3.6096],
        [-1.4856,  0.0973],
        [ 2.2866, -3.8856],
        [-3.3192,  1.7817],
        [ 0.6361, -2.0746],
        [ 3.2805, -5.2456],
        [ 4.1976, -5.6451]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7284, 0.2716],
        [0.2047, 0.7953]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3795, 0.6205], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1961, 0.0941],
         [0.4474, 0.2579]],

        [[0.7001, 0.0985],
         [0.2093, 0.6846]],

        [[0.8332, 0.1099],
         [0.2326, 0.5719]],

        [[0.5326, 0.1013],
         [0.8823, 0.8781]],

        [[0.7340, 0.0947],
         [0.4110, 0.7323]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599252625159036
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8822858823962049
time is 2
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8446221001520428
time is 3
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.8984111563755577
Average Adjusted Rand Index: 0.898011624216665
Iteration 0: Loss = -18836.653192890757
Iteration 10: Loss = -11307.34180239753
Iteration 20: Loss = -11307.330484054779
Iteration 30: Loss = -11307.33006106542
Iteration 40: Loss = -11307.330024999175
Iteration 50: Loss = -11307.330001349323
Iteration 60: Loss = -11307.329945935155
Iteration 70: Loss = -11307.330022735345
1
Iteration 80: Loss = -11307.32995929178
2
Iteration 90: Loss = -11307.329994465907
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.2607, 0.7393],
        [0.9596, 0.0404]], dtype=torch.float64)
alpha: tensor([0.5648, 0.4352])
beta: tensor([[[0.1688, 0.1698],
         [0.6873, 0.1691]],

        [[0.7060, 0.1682],
         [0.7669, 0.4929]],

        [[0.5352, 0.1759],
         [0.0468, 0.9439]],

        [[0.5568, 0.1649],
         [0.5293, 0.5196]],

        [[0.6895, 0.1658],
         [0.8733, 0.7439]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 39
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 38
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18803.295321794718
Iteration 100: Loss = -11312.635866277715
Iteration 200: Loss = -11299.76185356797
Iteration 300: Loss = -11298.613832481315
Iteration 400: Loss = -11298.261014998609
Iteration 500: Loss = -11298.096783106177
Iteration 600: Loss = -11298.00736695607
Iteration 700: Loss = -11297.94992211776
Iteration 800: Loss = -11297.91200752501
Iteration 900: Loss = -11297.888943070076
Iteration 1000: Loss = -11297.873863173403
Iteration 1100: Loss = -11297.862683093035
Iteration 1200: Loss = -11297.853899089505
Iteration 1300: Loss = -11297.846739049659
Iteration 1400: Loss = -11297.84071083237
Iteration 1500: Loss = -11297.835837906443
Iteration 1600: Loss = -11297.831875094796
Iteration 1700: Loss = -11297.828921962666
Iteration 1800: Loss = -11297.826486393053
Iteration 1900: Loss = -11297.824571791434
Iteration 2000: Loss = -11297.823062956559
Iteration 2100: Loss = -11297.821912236412
Iteration 2200: Loss = -11297.820868610093
Iteration 2300: Loss = -11297.819947381236
Iteration 2400: Loss = -11297.819074927462
Iteration 2500: Loss = -11297.81825169239
Iteration 2600: Loss = -11297.817508095652
Iteration 2700: Loss = -11297.816730040107
Iteration 2800: Loss = -11297.815991755348
Iteration 2900: Loss = -11297.815229087695
Iteration 3000: Loss = -11297.814424707618
Iteration 3100: Loss = -11297.81361884811
Iteration 3200: Loss = -11297.812657256152
Iteration 3300: Loss = -11297.81151331373
Iteration 3400: Loss = -11297.810245873483
Iteration 3500: Loss = -11297.809123854388
Iteration 3600: Loss = -11297.808454732953
Iteration 3700: Loss = -11297.807886950124
Iteration 3800: Loss = -11297.807430853793
Iteration 3900: Loss = -11297.806981361187
Iteration 4000: Loss = -11297.806481270774
Iteration 4100: Loss = -11297.80583468684
Iteration 4200: Loss = -11297.804973655502
Iteration 4300: Loss = -11297.803121702025
Iteration 4400: Loss = -11297.80153369334
Iteration 4500: Loss = -11297.80121271864
Iteration 4600: Loss = -11297.80095549136
Iteration 4700: Loss = -11297.800754385362
Iteration 4800: Loss = -11297.80054798737
Iteration 4900: Loss = -11297.800341282162
Iteration 5000: Loss = -11297.800172770934
Iteration 5100: Loss = -11297.800000375917
Iteration 5200: Loss = -11297.799817959103
Iteration 5300: Loss = -11297.799657458876
Iteration 5400: Loss = -11297.802914441054
1
Iteration 5500: Loss = -11297.799911911085
2
Iteration 5600: Loss = -11297.79971400822
3
Iteration 5700: Loss = -11297.828072837592
4
Iteration 5800: Loss = -11297.79976369495
5
Iteration 5900: Loss = -11297.79899930667
Iteration 6000: Loss = -11297.803706761726
1
Iteration 6100: Loss = -11297.798744363252
Iteration 6200: Loss = -11297.800510075254
1
Iteration 6300: Loss = -11297.798597310803
Iteration 6400: Loss = -11297.801063545301
1
Iteration 6500: Loss = -11297.798427819673
Iteration 6600: Loss = -11297.79836897478
Iteration 6700: Loss = -11298.061853906851
1
Iteration 6800: Loss = -11297.798223034615
Iteration 6900: Loss = -11297.798125509416
Iteration 7000: Loss = -11297.79994920002
1
Iteration 7100: Loss = -11297.798098293393
Iteration 7200: Loss = -11297.797971837783
Iteration 7300: Loss = -11297.797917863098
Iteration 7400: Loss = -11297.797909503019
Iteration 7500: Loss = -11297.797840751064
Iteration 7600: Loss = -11297.797778335356
Iteration 7700: Loss = -11297.799149064525
1
Iteration 7800: Loss = -11297.797669151736
Iteration 7900: Loss = -11297.797682822416
1
Iteration 8000: Loss = -11297.810073193885
2
Iteration 8100: Loss = -11297.797590361308
Iteration 8200: Loss = -11297.797572045722
Iteration 8300: Loss = -11297.810202022369
1
Iteration 8400: Loss = -11297.79749416931
Iteration 8500: Loss = -11297.797485336334
Iteration 8600: Loss = -11297.823808305075
1
Iteration 8700: Loss = -11297.797423604537
Iteration 8800: Loss = -11297.797373954436
Iteration 8900: Loss = -11297.797672530614
1
Iteration 9000: Loss = -11297.797375592309
2
Iteration 9100: Loss = -11297.797316070884
Iteration 9200: Loss = -11297.864466458854
1
Iteration 9300: Loss = -11297.797286004103
Iteration 9400: Loss = -11297.797272360376
Iteration 9500: Loss = -11297.798520245957
1
Iteration 9600: Loss = -11297.797248429099
Iteration 9700: Loss = -11297.797234866304
Iteration 9800: Loss = -11297.797240153539
1
Iteration 9900: Loss = -11297.797323956116
2
Iteration 10000: Loss = -11297.797160574144
Iteration 10100: Loss = -11297.79753104702
1
Iteration 10200: Loss = -11297.797143024967
Iteration 10300: Loss = -11297.797155958662
1
Iteration 10400: Loss = -11297.832640919936
2
Iteration 10500: Loss = -11297.797150559512
3
Iteration 10600: Loss = -11297.797112353996
Iteration 10700: Loss = -11297.798762602348
1
Iteration 10800: Loss = -11297.797477732205
2
Iteration 10900: Loss = -11297.7970979449
Iteration 11000: Loss = -11297.798629984201
1
Iteration 11100: Loss = -11297.79711393541
2
Iteration 11200: Loss = -11297.797765740112
3
Iteration 11300: Loss = -11297.797115258012
4
Iteration 11400: Loss = -11297.798147419997
5
Iteration 11500: Loss = -11297.797430248516
6
Iteration 11600: Loss = -11297.797139238972
7
Iteration 11700: Loss = -11297.7970553818
Iteration 11800: Loss = -11297.797018561885
Iteration 11900: Loss = -11297.797044514879
1
Iteration 12000: Loss = -11297.797005748056
Iteration 12100: Loss = -11297.797019572266
1
Iteration 12200: Loss = -11297.79724898649
2
Iteration 12300: Loss = -11297.796995613224
Iteration 12400: Loss = -11297.89692295938
1
Iteration 12500: Loss = -11297.796989926543
Iteration 12600: Loss = -11297.79700333495
1
Iteration 12700: Loss = -11297.797885711558
2
Iteration 12800: Loss = -11297.80645076511
3
Iteration 12900: Loss = -11297.801130815536
4
Iteration 13000: Loss = -11297.796954341175
Iteration 13100: Loss = -11298.032032131741
1
Iteration 13200: Loss = -11297.79699061806
2
Iteration 13300: Loss = -11297.829664686751
3
Iteration 13400: Loss = -11297.796956311951
4
Iteration 13500: Loss = -11297.797137748486
5
Iteration 13600: Loss = -11297.797098619229
6
Iteration 13700: Loss = -11297.809853620316
7
Iteration 13800: Loss = -11297.797055372048
8
Iteration 13900: Loss = -11297.79702464283
9
Iteration 14000: Loss = -11297.80196117082
10
Stopping early at iteration 14000 due to no improvement.
tensor([[ 4.1530, -6.1898],
        [ 3.3632, -7.0951],
        [ 3.2207, -6.7763],
        [ 3.8582, -5.2520],
        [ 4.0724, -5.4681],
        [ 4.2102, -5.5979],
        [ 3.1318, -4.8505],
        [ 4.4602, -5.8650],
        [ 3.7479, -5.7320],
        [ 3.5632, -5.3150],
        [ 4.0879, -5.4747],
        [ 4.5189, -5.9067],
        [ 3.7186, -5.1087],
        [ 3.2740, -5.3971],
        [ 3.4034, -4.7899],
        [ 3.2434, -4.7941],
        [ 3.8436, -5.5001],
        [ 3.3578, -4.7897],
        [ 2.8791, -4.7546],
        [ 4.0977, -5.4843],
        [ 4.8014, -6.1988],
        [ 3.3881, -5.0357],
        [ 3.3515, -5.9092],
        [ 3.7954, -5.7816],
        [ 4.1487, -5.6246],
        [ 3.3181, -4.7045],
        [ 3.8194, -5.2841],
        [ 3.7528, -5.1405],
        [ 4.7525, -6.5799],
        [ 4.9915, -6.7458],
        [ 4.4387, -5.9522],
        [ 4.3411, -6.2531],
        [ 3.2563, -4.9107],
        [ 3.2536, -4.7181],
        [ 3.7246, -5.1726],
        [ 2.5693, -5.4155],
        [ 4.0106, -5.7828],
        [ 3.2844, -4.6933],
        [ 3.5584, -5.0461],
        [ 4.1144, -6.1367],
        [ 4.5072, -5.9172],
        [ 3.5750, -5.3255],
        [ 3.9593, -5.3479],
        [ 3.3553, -4.9691],
        [ 3.4371, -5.1044],
        [ 3.4096, -5.6321],
        [ 4.1260, -6.0347],
        [ 3.8701, -5.2575],
        [ 3.5152, -4.9015],
        [ 3.3005, -5.8753],
        [ 3.2963, -4.6844],
        [ 4.0591, -5.6247],
        [ 3.8861, -6.9347],
        [ 4.2034, -6.5700],
        [ 5.0573, -6.4805],
        [ 2.1313, -6.0524],
        [ 5.0317, -6.5274],
        [ 3.0902, -4.6661],
        [ 4.4112, -5.8290],
        [ 4.0985, -5.6699],
        [ 3.2796, -5.8404],
        [ 4.8395, -6.2291],
        [ 1.8109, -5.9373],
        [ 3.3809, -4.9285],
        [ 4.0870, -5.4808],
        [ 4.0709, -5.4860],
        [ 3.8314, -5.4277],
        [ 3.9167, -5.7963],
        [ 4.4791, -5.9200],
        [ 3.2004, -4.6513],
        [ 4.1658, -5.6381],
        [ 3.7144, -5.4035],
        [ 4.1671, -5.6421],
        [ 4.0175, -5.6304],
        [ 4.2511, -5.6878],
        [ 5.1660, -6.7118],
        [ 3.4292, -4.8771],
        [ 4.2573, -5.7823],
        [ 3.4298, -5.2304],
        [ 3.4189, -4.9846],
        [ 3.2684, -5.1624],
        [ 2.7672, -5.4219],
        [ 3.7039, -6.3392],
        [ 4.3010, -5.7198],
        [ 3.9790, -5.7564],
        [ 3.8691, -5.2581],
        [ 3.4755, -6.0754],
        [ 4.1373, -5.7668],
        [ 4.5132, -5.9587],
        [ 3.6158, -5.9580],
        [ 4.3049, -5.7137],
        [ 4.8723, -6.4529],
        [ 4.6148, -6.0500],
        [ 3.1143, -6.2144],
        [ 3.6220, -5.2674],
        [ 4.4253, -5.8347],
        [ 3.9058, -5.2996],
        [ 3.5422, -5.3467],
        [ 4.0209, -7.1262],
        [ 4.5293, -6.0114]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9567, 0.0433],
        [0.8203, 0.1797]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9988e-01, 1.2404e-04], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1736, 0.2093],
         [0.6873, 0.1403]],

        [[0.7060, 0.1180],
         [0.7669, 0.4929]],

        [[0.5352, 0.2674],
         [0.0468, 0.9439]],

        [[0.5568, 0.1001],
         [0.5293, 0.5196]],

        [[0.6895, 0.0827],
         [0.8733, 0.7439]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 39
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 39
Adjusted Rand Index: -0.007726325420627255
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.012864505300896736
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 39
Adjusted Rand Index: 0.030848131024562662
Global Adjusted Rand Index: 0.011484370056219013
Average Adjusted Rand Index: 0.007197262180966428
Iteration 0: Loss = -21069.25660698001
Iteration 10: Loss = -11301.496102517116
Iteration 20: Loss = -11300.738789599922
Iteration 30: Loss = -11299.088354876361
Iteration 40: Loss = -11296.017084267784
Iteration 50: Loss = -11287.565970023505
Iteration 60: Loss = -11172.87463297453
Iteration 70: Loss = -11101.57368004463
Iteration 80: Loss = -11101.470162095427
Iteration 90: Loss = -11101.470078388842
Iteration 100: Loss = -11101.470095152861
1
Iteration 110: Loss = -11101.470111023631
2
Iteration 120: Loss = -11101.470111922117
3
Stopping early at iteration 119 due to no improvement.
pi: tensor([[0.6992, 0.3008],
        [0.2236, 0.7764]], dtype=torch.float64)
alpha: tensor([0.4078, 0.5922])
beta: tensor([[[0.1933, 0.0942],
         [0.1976, 0.2527]],

        [[0.8238, 0.0977],
         [0.5422, 0.4267]],

        [[0.6776, 0.1092],
         [0.4145, 0.9266]],

        [[0.2080, 0.1009],
         [0.0913, 0.9191]],

        [[0.4698, 0.0948],
         [0.5229, 0.7286]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599252625159036
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8077222191419615
time is 2
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8077222191419615
time is 3
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.8832424897202349
Average Adjusted Rand Index: 0.8835575660521238
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21069.272480962576
Iteration 100: Loss = -11311.492331586689
Iteration 200: Loss = -11293.510549093988
Iteration 300: Loss = -11291.174774006864
Iteration 400: Loss = -11290.187601909825
Iteration 500: Loss = -11289.707126739617
Iteration 600: Loss = -11289.405163675206
Iteration 700: Loss = -11289.213176581161
Iteration 800: Loss = -11289.097344783217
Iteration 900: Loss = -11289.022885059023
Iteration 1000: Loss = -11288.97180190136
Iteration 1100: Loss = -11288.935157093298
Iteration 1200: Loss = -11288.90778857881
Iteration 1300: Loss = -11288.886840594583
Iteration 1400: Loss = -11288.870114632628
Iteration 1500: Loss = -11288.854725893183
Iteration 1600: Loss = -11288.839627297868
Iteration 1700: Loss = -11288.830967042228
Iteration 1800: Loss = -11288.824368302652
Iteration 1900: Loss = -11288.81899918803
Iteration 2000: Loss = -11288.814572958456
Iteration 2100: Loss = -11288.810800370009
Iteration 2200: Loss = -11288.807623625722
Iteration 2300: Loss = -11288.804895727966
Iteration 2400: Loss = -11288.80249423186
Iteration 2500: Loss = -11288.800403619403
Iteration 2600: Loss = -11288.798563062383
Iteration 2700: Loss = -11288.796886961838
Iteration 2800: Loss = -11288.79535157946
Iteration 2900: Loss = -11288.793980623059
Iteration 3000: Loss = -11288.79265633662
Iteration 3100: Loss = -11288.791340837546
Iteration 3200: Loss = -11288.789939215787
Iteration 3300: Loss = -11288.788189859268
Iteration 3400: Loss = -11288.78500220463
Iteration 3500: Loss = -11288.77411417352
Iteration 3600: Loss = -11288.636090741138
Iteration 3700: Loss = -11284.681148205178
Iteration 3800: Loss = -11284.560546448634
Iteration 3900: Loss = -11284.535156882828
Iteration 4000: Loss = -11284.522669945007
Iteration 4100: Loss = -11284.514678068157
Iteration 4200: Loss = -11284.509407162657
Iteration 4300: Loss = -11284.505673201736
Iteration 4400: Loss = -11284.502947585795
Iteration 4500: Loss = -11284.500728067265
Iteration 4600: Loss = -11284.499021909458
Iteration 4700: Loss = -11284.497601595729
Iteration 4800: Loss = -11284.496425616864
Iteration 4900: Loss = -11284.495453242134
Iteration 5000: Loss = -11284.494590968445
Iteration 5100: Loss = -11284.493847987109
Iteration 5200: Loss = -11284.493148304267
Iteration 5300: Loss = -11284.492556624751
Iteration 5400: Loss = -11284.492034237028
Iteration 5500: Loss = -11284.49154943904
Iteration 5600: Loss = -11284.491116210656
Iteration 5700: Loss = -11284.490719781299
Iteration 5800: Loss = -11284.490349414213
Iteration 5900: Loss = -11284.490058709996
Iteration 6000: Loss = -11284.489741430023
Iteration 6100: Loss = -11284.48943550672
Iteration 6200: Loss = -11284.490261542609
1
Iteration 6300: Loss = -11284.488982084895
Iteration 6400: Loss = -11284.488747201716
Iteration 6500: Loss = -11284.48920579151
1
Iteration 6600: Loss = -11284.48836979343
Iteration 6700: Loss = -11284.496077205322
1
Iteration 6800: Loss = -11284.488010350198
Iteration 6900: Loss = -11284.487869953455
Iteration 7000: Loss = -11284.487736708379
Iteration 7100: Loss = -11284.48761267162
Iteration 7200: Loss = -11284.490863257182
1
Iteration 7300: Loss = -11284.487381698285
Iteration 7400: Loss = -11284.487412302497
1
Iteration 7500: Loss = -11284.487153802873
Iteration 7600: Loss = -11284.487033443633
Iteration 7700: Loss = -11284.486974240413
Iteration 7800: Loss = -11284.489783390864
1
Iteration 7900: Loss = -11284.486783735836
Iteration 8000: Loss = -11284.486700495852
Iteration 8100: Loss = -11284.489613656573
1
Iteration 8200: Loss = -11284.486570396122
Iteration 8300: Loss = -11284.486500370589
Iteration 8400: Loss = -11284.489887796151
1
Iteration 8500: Loss = -11284.486361969162
Iteration 8600: Loss = -11284.486284632883
Iteration 8700: Loss = -11284.491048774085
1
Iteration 8800: Loss = -11284.486235090597
Iteration 8900: Loss = -11284.48615275503
Iteration 9000: Loss = -11284.490190895187
1
Iteration 9100: Loss = -11284.486074591474
Iteration 9200: Loss = -11284.486055443482
Iteration 9300: Loss = -11284.504677133426
1
Iteration 9400: Loss = -11284.48597046829
Iteration 9500: Loss = -11284.485920488883
Iteration 9600: Loss = -11284.485925012983
1
Iteration 9700: Loss = -11284.48587487485
Iteration 9800: Loss = -11284.485858762419
Iteration 9900: Loss = -11284.485839785006
Iteration 10000: Loss = -11284.491300934358
1
Iteration 10100: Loss = -11284.485778524466
Iteration 10200: Loss = -11284.485749872627
Iteration 10300: Loss = -11284.48575326403
1
Iteration 10400: Loss = -11284.486243911331
2
Iteration 10500: Loss = -11284.485579801276
Iteration 10600: Loss = -11284.48536655619
Iteration 10700: Loss = -11284.53827453516
1
Iteration 10800: Loss = -11284.485241856866
Iteration 10900: Loss = -11284.485249589969
1
Iteration 11000: Loss = -11284.485207606982
Iteration 11100: Loss = -11284.486654978218
1
Iteration 11200: Loss = -11284.485199864885
Iteration 11300: Loss = -11284.485163748843
Iteration 11400: Loss = -11284.510281574656
1
Iteration 11500: Loss = -11284.485124026298
Iteration 11600: Loss = -11284.485150248576
1
Iteration 11700: Loss = -11284.507279340007
2
Iteration 11800: Loss = -11284.485125348048
3
Iteration 11900: Loss = -11284.485144540844
4
Iteration 12000: Loss = -11284.486850947473
5
Iteration 12100: Loss = -11284.485119911671
Iteration 12200: Loss = -11284.485069807417
Iteration 12300: Loss = -11284.760817764307
1
Iteration 12400: Loss = -11284.485068604134
Iteration 12500: Loss = -11284.485041496178
Iteration 12600: Loss = -11284.485022587682
Iteration 12700: Loss = -11284.485189779394
1
Iteration 12800: Loss = -11284.485095383017
2
Iteration 12900: Loss = -11284.485149301538
3
Iteration 13000: Loss = -11284.485070958415
4
Iteration 13100: Loss = -11284.485032233068
5
Iteration 13200: Loss = -11284.485002225121
Iteration 13300: Loss = -11284.485192148908
1
Iteration 13400: Loss = -11284.485020624108
2
Iteration 13500: Loss = -11284.485010948112
3
Iteration 13600: Loss = -11284.48515723922
4
Iteration 13700: Loss = -11284.484996197983
Iteration 13800: Loss = -11284.484995479557
Iteration 13900: Loss = -11284.513031320706
1
Iteration 14000: Loss = -11284.484989440918
Iteration 14100: Loss = -11284.485013913705
1
Iteration 14200: Loss = -11284.485465238418
2
Iteration 14300: Loss = -11284.484997190826
3
Iteration 14400: Loss = -11284.485012363419
4
Iteration 14500: Loss = -11284.485032950171
5
Iteration 14600: Loss = -11284.484982213022
Iteration 14700: Loss = -11284.499009836323
1
Iteration 14800: Loss = -11284.484961879283
Iteration 14900: Loss = -11284.484981368852
1
Iteration 15000: Loss = -11284.491618967893
2
Iteration 15100: Loss = -11284.485010033495
3
Iteration 15200: Loss = -11284.48494951603
Iteration 15300: Loss = -11284.484945869417
Iteration 15400: Loss = -11284.579770682902
1
Iteration 15500: Loss = -11284.484951417928
2
Iteration 15600: Loss = -11284.484954617536
3
Iteration 15700: Loss = -11284.491902858677
4
Iteration 15800: Loss = -11284.48495120341
5
Iteration 15900: Loss = -11284.502986650637
6
Iteration 16000: Loss = -11284.484946853536
7
Iteration 16100: Loss = -11284.489992548826
8
Iteration 16200: Loss = -11284.485007236259
9
Iteration 16300: Loss = -11284.484955769469
10
Stopping early at iteration 16300 due to no improvement.
tensor([[ -1.1974,  -0.9900],
        [  0.0999,  -1.5191],
        [  3.3036,  -6.5530],
        [  3.4760,  -4.8718],
        [  4.9695,  -6.7521],
        [  3.9707,  -6.8434],
        [  7.5900,  -9.1323],
        [  1.6522,  -3.0877],
        [  4.6944,  -6.1018],
        [  6.4935,  -7.9201],
        [  4.0716,  -5.5054],
        [  1.7153,  -3.1466],
        [  3.7467,  -5.8160],
        [  5.6826,  -7.2789],
        [  7.3927,  -9.5508],
        [  7.1652,  -8.7999],
        [  4.0077,  -6.2697],
        [  6.8974,  -8.5060],
        [  7.8902,  -9.7766],
        [  3.1722,  -5.1067],
        [ -2.9395,   1.5458],
        [  6.7029,  -9.1032],
        [  5.1396,  -7.2843],
        [  0.4225,  -3.0540],
        [  3.2393,  -4.9556],
        [  6.3190,  -8.5514],
        [  2.4102,  -3.9877],
        [  6.5330,  -7.9385],
        [ -0.8368,  -0.6436],
        [ -5.9526,   4.5529],
        [  1.3667,  -3.6050],
        [  1.4156,  -2.8019],
        [  7.7241,  -9.1213],
        [  7.0420,  -8.4307],
        [  3.4022,  -4.7936],
        [  5.8727,  -7.2763],
        [  3.3769,  -4.8708],
        [  7.0437,  -8.5121],
        [  6.9920,  -8.4421],
        [  2.1011,  -3.9575],
        [ -0.1558,  -1.3144],
        [  5.2020,  -8.8187],
        [  3.4110,  -6.6775],
        [  7.1084,  -8.4972],
        [  5.7207,  -7.1071],
        [  4.5956,  -6.3945],
        [  2.5759,  -4.8593],
        [  7.1250,  -8.5157],
        [  7.2913,  -8.8363],
        [  2.1368,  -4.9338],
        [  6.7378,  -8.7619],
        [ -4.2495,   2.8624],
        [  1.4149,  -2.9588],
        [  1.9786,  -3.8652],
        [ -1.4998,   0.0416],
        [  4.9120,  -6.3694],
        [ -3.7538,   2.0579],
        [  7.5932, -10.5669],
        [  0.2061,  -2.0675],
        [  1.7881,  -3.4220],
        [  5.9127,  -7.7133],
        [ -1.5470,  -0.7346],
        [  6.2322,  -7.8258],
        [  5.7240,  -7.1955],
        [  1.3360,  -3.0372],
        [  3.6361,  -5.0537],
        [  1.6064,  -3.4857],
        [  2.8959,  -4.8958],
        [  0.9106,  -2.3189],
        [  3.6030,  -5.0037],
        [  2.8511,  -5.2018],
        [  4.7050,  -6.1614],
        [  2.8999,  -5.2905],
        [  1.9576,  -5.9933],
        [ -2.4290,   1.0425],
        [ -3.3156,   0.8005],
        [  3.7612,  -5.5825],
        [  2.6826,  -4.0698],
        [  5.7227,  -7.5527],
        [  6.1414,  -9.6204],
        [  7.3235,  -8.8193],
        [  7.2999,  -9.1328],
        [  3.7902,  -6.0752],
        [ -4.6101,   3.0327],
        [  4.1948,  -6.6028],
        [  3.7081,  -5.3419],
        [  5.1028,  -6.6251],
        [  3.4223,  -5.1271],
        [  3.2925,  -4.6905],
        [  3.3945,  -4.7849],
        [  2.9798,  -4.3664],
        [ -3.9388,   2.5366],
        [  2.2645,  -4.6775],
        [  4.6119,  -6.7028],
        [  5.6039,  -7.0082],
        [  1.9278,  -3.4483],
        [  5.4295,  -7.2293],
        [  2.6102,  -3.9976],
        [ -2.8721,   0.6022],
        [ -0.4494,  -0.9369]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.6847e-01, 3.1532e-02],
        [1.0000e+00, 3.2500e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8744, 0.1256], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1733, 0.0756],
         [0.1976, 0.2996]],

        [[0.8238, 0.2777],
         [0.5422, 0.4267]],

        [[0.6776, 0.2678],
         [0.4145, 0.9266]],

        [[0.2080, 0.2463],
         [0.0913, 0.9191]],

        [[0.4698, 0.0750],
         [0.5229, 0.7286]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 26
Adjusted Rand Index: 0.205900142430687
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: -0.013574768645372375
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 39
Adjusted Rand Index: -0.007726325420627255
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 39
Adjusted Rand Index: 0.030848131024562662
Global Adjusted Rand Index: 0.027070884172210426
Average Adjusted Rand Index: 0.04249675738982436
11135.596719487165
new:  [-0.0030653678042551883, 0.8984111563755577, 0.011484370056219013, 0.027070884172210426] [-0.004712179825394256, 0.898011624216665, 0.007197262180966428, 0.04249675738982436] [11298.950996516562, 11096.180393902534, 11297.80196117082, 11284.484955769469]
prior:  [0.8832424897202349, 0.8832424897202349, 0.0, 0.8832424897202349] [0.8835575660521238, 0.8835575660521238, 0.0, 0.8835575660521238] [11101.4701109017, 11101.470105106648, 11307.329994465907, 11101.470111922117]
-----------------------------------------------------------------------------------------
This iteration is 17
True Objective function: Loss = -10969.634682281672
Iteration 0: Loss = -18342.25765354658
Iteration 10: Loss = -11007.534188919331
Iteration 20: Loss = -11007.51333953145
Iteration 30: Loss = -11007.512533002604
Iteration 40: Loss = -11007.516033931875
1
Iteration 50: Loss = -11007.522217080003
2
Iteration 60: Loss = -11007.53015714747
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.8655, 0.1345],
        [0.8547, 0.1453]], dtype=torch.float64)
alpha: tensor([0.8656, 0.1344])
beta: tensor([[[0.1530, 0.1675],
         [0.1365, 0.2134]],

        [[0.7012, 0.1909],
         [0.6157, 0.3993]],

        [[0.4606, 0.1910],
         [0.2934, 0.0935]],

        [[0.4889, 0.1855],
         [0.2338, 0.6326]],

        [[0.1986, 0.1874],
         [0.2604, 0.2958]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.00035341041046094813
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: -0.0003845451714286067
Average Adjusted Rand Index: -0.0007316234276684654
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18314.430645983608
Iteration 100: Loss = -11011.098759932558
Iteration 200: Loss = -11009.509028229244
Iteration 300: Loss = -11009.020395105872
Iteration 400: Loss = -11008.82950958098
Iteration 500: Loss = -11008.749432966233
Iteration 600: Loss = -11008.707937248048
Iteration 700: Loss = -11008.68259612992
Iteration 800: Loss = -11008.665761616978
Iteration 900: Loss = -11008.653836111092
Iteration 1000: Loss = -11008.645033108092
Iteration 1100: Loss = -11008.638321912309
Iteration 1200: Loss = -11008.633027067479
Iteration 1300: Loss = -11008.628758025949
Iteration 1400: Loss = -11008.625116768691
Iteration 1500: Loss = -11008.621932050695
Iteration 1600: Loss = -11008.619097449813
Iteration 1700: Loss = -11008.616409991197
Iteration 1800: Loss = -11008.613839015383
Iteration 1900: Loss = -11008.611269232699
Iteration 2000: Loss = -11008.608676616373
Iteration 2100: Loss = -11008.60590002433
Iteration 2200: Loss = -11008.602871030378
Iteration 2300: Loss = -11008.599475745996
Iteration 2400: Loss = -11008.595593126034
Iteration 2500: Loss = -11008.59103792584
Iteration 2600: Loss = -11008.585454666829
Iteration 2700: Loss = -11008.578724615349
Iteration 2800: Loss = -11008.570222431463
Iteration 2900: Loss = -11008.559410777214
Iteration 3000: Loss = -11008.54535889969
Iteration 3100: Loss = -11008.52657785961
Iteration 3200: Loss = -11008.500601505653
Iteration 3300: Loss = -11008.464104790239
Iteration 3400: Loss = -11008.414274128689
Iteration 3500: Loss = -11008.34370081691
Iteration 3600: Loss = -11008.247807718431
Iteration 3700: Loss = -11008.205360686949
Iteration 3800: Loss = -11008.176457888614
Iteration 3900: Loss = -11008.15543089303
Iteration 4000: Loss = -11008.138966905399
Iteration 4100: Loss = -11008.126558011243
Iteration 4200: Loss = -11008.117505487266
Iteration 4300: Loss = -11008.11073563666
Iteration 4400: Loss = -11008.105657064918
Iteration 4500: Loss = -11008.101779244304
Iteration 4600: Loss = -11008.09877638531
Iteration 4700: Loss = -11008.096430216367
Iteration 4800: Loss = -11008.094591814262
Iteration 4900: Loss = -11008.093445221695
Iteration 5000: Loss = -11008.098886631098
1
Iteration 5100: Loss = -11008.09442618914
2
Iteration 5200: Loss = -11008.090093525856
Iteration 5300: Loss = -11008.08934060739
Iteration 5400: Loss = -11008.088742631673
Iteration 5500: Loss = -11008.088241566078
Iteration 5600: Loss = -11008.08776937071
Iteration 5700: Loss = -11008.087408257117
Iteration 5800: Loss = -11008.087116502289
Iteration 5900: Loss = -11008.086768110215
Iteration 6000: Loss = -11008.086533453557
Iteration 6100: Loss = -11008.086299369283
Iteration 6200: Loss = -11008.086078087323
Iteration 6300: Loss = -11008.085856577787
Iteration 6400: Loss = -11008.085760637734
Iteration 6500: Loss = -11008.08562209392
Iteration 6600: Loss = -11008.08545649028
Iteration 6700: Loss = -11008.135062305992
1
Iteration 6800: Loss = -11008.085254224647
Iteration 6900: Loss = -11008.085151605157
Iteration 7000: Loss = -11008.085056047994
Iteration 7100: Loss = -11008.096363767869
1
Iteration 7200: Loss = -11008.084947441337
Iteration 7300: Loss = -11008.084845984129
Iteration 7400: Loss = -11008.084789528575
Iteration 7500: Loss = -11008.084771166514
Iteration 7600: Loss = -11008.084684625817
Iteration 7700: Loss = -11008.096991713186
1
Iteration 7800: Loss = -11008.087719723095
2
Iteration 7900: Loss = -11008.13917846373
3
Iteration 8000: Loss = -11008.08479121006
4
Iteration 8100: Loss = -11008.094722181657
5
Iteration 8200: Loss = -11008.095393643978
6
Iteration 8300: Loss = -11008.085795061506
7
Iteration 8400: Loss = -11008.084445047409
Iteration 8500: Loss = -11008.086063918043
1
Iteration 8600: Loss = -11008.087827101388
2
Iteration 8700: Loss = -11008.084789412831
3
Iteration 8800: Loss = -11008.08432493394
Iteration 8900: Loss = -11008.085517830013
1
Iteration 9000: Loss = -11008.142785899885
2
Iteration 9100: Loss = -11008.08423016543
Iteration 9200: Loss = -11008.085281393074
1
Iteration 9300: Loss = -11008.084185389363
Iteration 9400: Loss = -11008.094926059199
1
Iteration 9500: Loss = -11008.084305750675
2
Iteration 9600: Loss = -11008.084164522937
Iteration 9700: Loss = -11008.084134195697
Iteration 9800: Loss = -11008.084337046019
1
Iteration 9900: Loss = -11008.084128125323
Iteration 10000: Loss = -11008.097515587868
1
Iteration 10100: Loss = -11008.084124841302
Iteration 10200: Loss = -11008.12542579419
1
Iteration 10300: Loss = -11008.084100070533
Iteration 10400: Loss = -11008.172737240571
1
Iteration 10500: Loss = -11008.08406316443
Iteration 10600: Loss = -11008.086197372473
1
Iteration 10700: Loss = -11008.120987065833
2
Iteration 10800: Loss = -11008.087815912952
3
Iteration 10900: Loss = -11008.084135839801
4
Iteration 11000: Loss = -11008.086791867368
5
Iteration 11100: Loss = -11008.084147557687
6
Iteration 11200: Loss = -11008.09147490867
7
Iteration 11300: Loss = -11008.095741818343
8
Iteration 11400: Loss = -11008.084544585887
9
Iteration 11500: Loss = -11008.084014535796
Iteration 11600: Loss = -11008.10373383969
1
Iteration 11700: Loss = -11008.204492643335
2
Iteration 11800: Loss = -11008.084035605121
3
Iteration 11900: Loss = -11008.084082452451
4
Iteration 12000: Loss = -11008.11985087964
5
Iteration 12100: Loss = -11008.083974347894
Iteration 12200: Loss = -11008.094301829053
1
Iteration 12300: Loss = -11008.090261723342
2
Iteration 12400: Loss = -11008.084594253387
3
Iteration 12500: Loss = -11008.09207633127
4
Iteration 12600: Loss = -11008.084080842546
5
Iteration 12700: Loss = -11008.084012678944
6
Iteration 12800: Loss = -11008.086184544809
7
Iteration 12900: Loss = -11008.084793556269
8
Iteration 13000: Loss = -11008.083976387188
9
Iteration 13100: Loss = -11008.084326620088
10
Stopping early at iteration 13100 due to no improvement.
tensor([[-4.7854,  0.1702],
        [-4.7813,  0.1661],
        [-4.7832,  0.1679],
        [-4.7706,  0.1554],
        [-4.7896,  0.1744],
        [-4.7654,  0.1502],
        [-4.7780,  0.1627],
        [-4.7821,  0.1669],
        [-4.7770,  0.1618],
        [-4.7867,  0.1715],
        [-4.7770,  0.1618],
        [-4.7768,  0.1616],
        [-4.7833,  0.1681],
        [-4.7652,  0.1499],
        [-4.7784,  0.1632],
        [-4.7720,  0.1567],
        [-4.7877,  0.1724],
        [-4.7750,  0.1597],
        [-4.7886,  0.1734],
        [-4.7867,  0.1715],
        [-4.7897,  0.1745],
        [-4.7961,  0.1809],
        [-4.7755,  0.1603],
        [-4.8109,  0.1956],
        [-4.7793,  0.1641],
        [-4.7816,  0.1664],
        [-4.7859,  0.1706],
        [-4.7693,  0.1541],
        [-4.7882,  0.1730],
        [-4.8439,  0.2287],
        [-4.8696,  0.2544],
        [-4.7831,  0.1679],
        [-4.7719,  0.1567],
        [-4.7771,  0.1619],
        [-4.7853,  0.1700],
        [-4.7839,  0.1687],
        [-4.8013,  0.1861],
        [-4.7627,  0.1475],
        [-4.7820,  0.1668],
        [-4.7735,  0.1583],
        [-4.7749,  0.1597],
        [-4.7714,  0.1562],
        [-4.7802,  0.1650],
        [-4.7832,  0.1680],
        [-4.7682,  0.1530],
        [-4.7918,  0.1766],
        [-4.7820,  0.1668],
        [-4.7704,  0.1552],
        [-4.8012,  0.1860],
        [-4.8199,  0.2047],
        [-4.7910,  0.1758],
        [-4.7795,  0.1643],
        [-4.7821,  0.1669],
        [-4.7965,  0.1813],
        [-4.8125,  0.1973],
        [-4.7709,  0.1557],
        [-4.7712,  0.1559],
        [-4.7716,  0.1564],
        [-4.8364,  0.2212],
        [-4.7815,  0.1663],
        [-4.7885,  0.1733],
        [-4.7782,  0.1630],
        [-4.7822,  0.1670],
        [-4.7733,  0.1581],
        [-4.7853,  0.1701],
        [-4.7761,  0.1608],
        [-4.7805,  0.1653],
        [-4.7795,  0.1643],
        [-4.7964,  0.1812],
        [-4.7702,  0.1549],
        [-4.7707,  0.1555],
        [-4.7653,  0.1501],
        [-4.7947,  0.1795],
        [-4.7715,  0.1563],
        [-4.7727,  0.1575],
        [-4.7835,  0.1682],
        [-4.7757,  0.1605],
        [-4.7729,  0.1577],
        [-4.7785,  0.1633],
        [-4.7796,  0.1644],
        [-4.8152,  0.2000],
        [-4.7742,  0.1589],
        [-4.7900,  0.1748],
        [-4.7784,  0.1632],
        [-4.7658,  0.1506],
        [-4.7739,  0.1587],
        [-4.7749,  0.1597],
        [-4.7772,  0.1620],
        [-4.8197,  0.2044],
        [-4.7807,  0.1655],
        [-4.7686,  0.1533],
        [-4.7628,  0.1476],
        [-4.7619,  0.1467],
        [-4.7879,  0.1727],
        [-4.7797,  0.1645],
        [-4.7973,  0.1821],
        [-4.7801,  0.1648],
        [-4.7930,  0.1778],
        [-4.7737,  0.1584],
        [-4.7881,  0.1729]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 1.1310e-06],
        [9.7915e-01, 2.0850e-02]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0070, 0.9930], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1637, 0.1584],
         [0.1365, 0.1587]],

        [[0.7012, 0.2097],
         [0.6157, 0.3993]],

        [[0.4606, 0.1227],
         [0.2934, 0.0935]],

        [[0.4889, 0.1991],
         [0.2338, 0.6326]],

        [[0.1986, 0.2485],
         [0.2604, 0.2958]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0002379744974308241
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -20504.466863882113
Iteration 10: Loss = -11007.746436819038
Iteration 20: Loss = -11007.588235709563
Iteration 30: Loss = -11007.544710085116
Iteration 40: Loss = -11007.523611740233
Iteration 50: Loss = -11007.514432952763
Iteration 60: Loss = -11007.51248027001
Iteration 70: Loss = -11007.515086068477
1
Iteration 80: Loss = -11007.520688335884
2
Iteration 90: Loss = -11007.528313746976
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.1438, 0.8562],
        [0.1327, 0.8673]], dtype=torch.float64)
alpha: tensor([0.1326, 0.8674])
beta: tensor([[[0.2137, 0.1676],
         [0.9547, 0.1531]],

        [[0.6799, 0.1912],
         [0.2995, 0.2091]],

        [[0.4553, 0.1913],
         [0.7008, 0.6593]],

        [[0.1843, 0.1857],
         [0.7290, 0.2116]],

        [[0.9969, 0.1877],
         [0.9731, 0.2921]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.00035341041046094813
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: -0.0003845451714286067
Average Adjusted Rand Index: -0.0007316234276684654
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20504.289082289077
Iteration 100: Loss = -11008.977196660264
Iteration 200: Loss = -11007.805320504518
Iteration 300: Loss = -11007.216275840463
Iteration 400: Loss = -11007.050400293161
Iteration 500: Loss = -11007.001165314452
Iteration 600: Loss = -11006.982453935705
Iteration 700: Loss = -11006.972334491164
Iteration 800: Loss = -11006.967182662951
Iteration 900: Loss = -11006.962972033061
Iteration 1000: Loss = -11006.971888205317
1
Iteration 1100: Loss = -11006.9583750624
Iteration 1200: Loss = -11006.956816292663
Iteration 1300: Loss = -11006.955706354138
Iteration 1400: Loss = -11006.954683869852
Iteration 1500: Loss = -11006.954196679397
Iteration 1600: Loss = -11006.953912206442
Iteration 1700: Loss = -11006.953695554648
Iteration 1800: Loss = -11006.953563099096
Iteration 1900: Loss = -11006.953478982123
Iteration 2000: Loss = -11006.953423818024
Iteration 2100: Loss = -11006.95349099984
1
Iteration 2200: Loss = -11006.953348271945
Iteration 2300: Loss = -11006.95333261582
Iteration 2400: Loss = -11006.953333284257
1
Iteration 2500: Loss = -11006.953354489899
2
Iteration 2600: Loss = -11006.95430730475
3
Iteration 2700: Loss = -11006.953274878531
Iteration 2800: Loss = -11006.953288112842
1
Iteration 2900: Loss = -11006.954799109897
2
Iteration 3000: Loss = -11006.953301840103
3
Iteration 3100: Loss = -11006.953311729561
4
Iteration 3200: Loss = -11006.953322729209
5
Iteration 3300: Loss = -11006.953323901093
6
Iteration 3400: Loss = -11006.95365801168
7
Iteration 3500: Loss = -11006.953267053641
Iteration 3600: Loss = -11006.954983750778
1
Iteration 3700: Loss = -11006.953269426765
2
Iteration 3800: Loss = -11006.953274048457
3
Iteration 3900: Loss = -11006.953296749116
4
Iteration 4000: Loss = -11006.953263348207
Iteration 4100: Loss = -11006.953244977023
Iteration 4200: Loss = -11006.953265143824
1
Iteration 4300: Loss = -11006.953248510548
2
Iteration 4400: Loss = -11006.953250839892
3
Iteration 4500: Loss = -11006.953257533907
4
Iteration 4600: Loss = -11006.953290953774
5
Iteration 4700: Loss = -11006.953988942758
6
Iteration 4800: Loss = -11006.95325900968
7
Iteration 4900: Loss = -11006.95327066287
8
Iteration 5000: Loss = -11006.953270372653
9
Iteration 5100: Loss = -11006.953257928584
10
Stopping early at iteration 5100 due to no improvement.
tensor([[-3.5055,  1.0995],
        [-3.1163,  1.6048],
        [-3.3286,  0.9441],
        [-3.2149,  1.6334],
        [-2.6786,  1.0565],
        [-3.3795,  1.6594],
        [-3.2926,  1.1713],
        [-2.8829,  1.4771],
        [-3.0601,  1.5940],
        [-3.2177,  0.7088],
        [-2.9840,  1.5803],
        [-3.0712,  1.3967],
        [-3.3261,  0.8585],
        [-3.4430,  1.6840],
        [-2.9810,  1.5878],
        [-3.1144,  1.7281],
        [-3.4654,  0.7134],
        [-3.0337,  1.6249],
        [-2.9234,  1.2441],
        [-2.7917,  1.3827],
        [-3.2272,  1.6501],
        [-3.7121,  0.6962],
        [-3.5284,  1.0441],
        [-3.2721,  1.2837],
        [-3.3094,  1.1589],
        [-2.9642,  1.2390],
        [-2.7494,  1.3457],
        [-3.4664,  1.5588],
        [-3.7373,  0.7627],
        [-3.2339,  0.6200],
        [-2.8234,  1.2909],
        [-2.9763,  1.3893],
        [-3.4317,  1.4007],
        [-3.0273,  1.4549],
        [-3.1415,  1.3107],
        [-2.7864,  1.3937],
        [-3.2122,  1.4510],
        [-3.9603,  1.2634],
        [-3.8246,  0.7248],
        [-2.9950,  1.5808],
        [-3.1774,  1.7389],
        [-3.0738,  1.6842],
        [-3.4217,  1.0404],
        [-3.6648,  0.9527],
        [-3.3269,  1.7842],
        [-3.0192,  1.4736],
        [-2.8881,  1.3042],
        [-4.5017,  0.2608],
        [-3.0246,  1.4600],
        [-3.2353,  1.2975],
        [-2.8466,  1.3948],
        [-2.8437,  1.4473],
        [-3.0656,  1.3060],
        [-3.1645,  1.5910],
        [-3.2864,  1.2696],
        [-3.3669,  1.5675],
        [-3.5059,  1.2548],
        [-3.1238,  1.7227],
        [-2.9887,  1.6020],
        [-3.0154,  1.3583],
        [-2.9500,  1.5622],
        [-4.1309,  0.3390],
        [-3.2403,  1.2197],
        [-3.0708,  1.6747],
        [-2.7759,  1.3217],
        [-3.1344,  1.5114],
        [-3.0553,  1.6690],
        [-3.1573,  1.3926],
        [-3.2732,  1.3943],
        [-3.3262,  1.6059],
        [-3.5531,  1.2981],
        [-3.5234,  1.6022],
        [-2.8132,  1.4221],
        [-3.2083,  1.5474],
        [-3.1276,  1.6312],
        [-2.7562,  1.3491],
        [-3.1835,  1.4762],
        [-3.1104,  1.5608],
        [-3.4350,  1.2084],
        [-3.0049,  1.4499],
        [-3.6337,  0.9025],
        [-3.3574,  1.6479],
        [-2.8808,  1.4661],
        [-3.3056,  1.2521],
        [-3.4599,  1.6605],
        [-3.1916,  1.5530],
        [-3.0378,  1.6212],
        [-3.1625,  1.7405],
        [-3.3168,  0.8371],
        [-2.8427,  1.4466],
        [-3.1783,  1.6843],
        [-3.3764,  1.8487],
        [-4.4887,  0.8227],
        [-4.0020,  0.1834],
        [-3.0709,  1.4792],
        [-2.9205,  1.3788],
        [-3.6040,  0.9413],
        [-4.1223,  0.0335],
        [-3.6624,  1.0833],
        [-3.1962,  1.5876]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.1383, 0.8617],
        [0.1031, 0.8969]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0108, 0.9892], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2325, 0.1688],
         [0.9547, 0.1566]],

        [[0.6799, 0.1981],
         [0.2995, 0.2091]],

        [[0.4553, 0.1963],
         [0.7008, 0.6593]],

        [[0.1843, 0.1906],
         [0.7290, 0.2116]],

        [[0.9969, 0.1938],
         [0.9731, 0.2921]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 4.2092736862769894e-05
Average Adjusted Rand Index: -0.0006936617740552836
Iteration 0: Loss = -29551.216057166097
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.2413,    nan]],

        [[0.1397,    nan],
         [0.5256, 0.4232]],

        [[0.2579,    nan],
         [0.3665, 0.9280]],

        [[0.5010,    nan],
         [0.9845, 0.4270]],

        [[0.8031,    nan],
         [0.5778, 0.8079]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29548.94440847632
Iteration 100: Loss = -11022.630012910495
Iteration 200: Loss = -11011.88269164653
Iteration 300: Loss = -11010.38205731855
Iteration 400: Loss = -11009.794378082113
Iteration 500: Loss = -11009.488160676527
Iteration 600: Loss = -11009.294890694588
Iteration 700: Loss = -11009.161346950226
Iteration 800: Loss = -11009.062893119288
Iteration 900: Loss = -11008.987235470735
Iteration 1000: Loss = -11008.926650731573
Iteration 1100: Loss = -11008.876783977177
Iteration 1200: Loss = -11008.835260661004
Iteration 1300: Loss = -11008.79998945782
Iteration 1400: Loss = -11008.769099864623
Iteration 1500: Loss = -11008.741685208115
Iteration 1600: Loss = -11008.71691953434
Iteration 1700: Loss = -11008.69444400452
Iteration 1800: Loss = -11008.673832633143
Iteration 1900: Loss = -11008.654651105691
Iteration 2000: Loss = -11008.636178124141
Iteration 2100: Loss = -11008.617601927583
Iteration 2200: Loss = -11008.597990954868
Iteration 2300: Loss = -11008.576549821157
Iteration 2400: Loss = -11008.552480480284
Iteration 2500: Loss = -11008.524526278487
Iteration 2600: Loss = -11008.491377409917
Iteration 2700: Loss = -11008.452573623554
Iteration 2800: Loss = -11008.40904970887
Iteration 2900: Loss = -11008.36295171569
Iteration 3000: Loss = -11008.31454871983
Iteration 3100: Loss = -11008.261433248072
Iteration 3200: Loss = -11008.200649192135
Iteration 3300: Loss = -11008.129694312569
Iteration 3400: Loss = -11008.046583660767
Iteration 3500: Loss = -11007.950959877313
Iteration 3600: Loss = -11007.845843013656
Iteration 3700: Loss = -11007.737857188937
Iteration 3800: Loss = -11007.633563828329
Iteration 3900: Loss = -11007.535213321482
Iteration 4000: Loss = -11007.440693298713
Iteration 4100: Loss = -11007.351170766955
Iteration 4200: Loss = -11007.271751943566
Iteration 4300: Loss = -11007.20399520349
Iteration 4400: Loss = -11007.146284674016
Iteration 4500: Loss = -11007.100313859379
Iteration 4600: Loss = -11007.066799029928
Iteration 4700: Loss = -11007.042860225522
Iteration 4800: Loss = -11007.024625959108
Iteration 4900: Loss = -11007.010336052736
Iteration 5000: Loss = -11006.99896334283
Iteration 5100: Loss = -11006.989688682848
Iteration 5200: Loss = -11006.982440897204
Iteration 5300: Loss = -11006.977218991651
Iteration 5400: Loss = -11006.97333668123
Iteration 5500: Loss = -11006.970348381172
Iteration 5600: Loss = -11006.967926419286
Iteration 5700: Loss = -11006.9659071731
Iteration 5800: Loss = -11006.964215565855
Iteration 5900: Loss = -11006.962811433465
Iteration 6000: Loss = -11006.961682409506
Iteration 6100: Loss = -11006.960845814609
Iteration 6200: Loss = -11006.960138660294
Iteration 6300: Loss = -11006.959623498447
Iteration 6400: Loss = -11006.959537839728
Iteration 6500: Loss = -11006.958806506815
Iteration 6600: Loss = -11006.958525562673
Iteration 6700: Loss = -11006.95821276328
Iteration 6800: Loss = -11006.957943592512
Iteration 6900: Loss = -11006.957704532499
Iteration 7000: Loss = -11006.959802980547
1
Iteration 7100: Loss = -11006.957657101373
Iteration 7200: Loss = -11006.956939645364
Iteration 7300: Loss = -11006.957068487567
1
Iteration 7400: Loss = -11006.970437650354
2
Iteration 7500: Loss = -11006.956602622024
Iteration 7600: Loss = -11006.956452636745
Iteration 7700: Loss = -11006.95658713842
1
Iteration 7800: Loss = -11007.007388272166
2
Iteration 7900: Loss = -11006.956235364756
Iteration 8000: Loss = -11006.969181937357
1
Iteration 8100: Loss = -11006.956155588092
Iteration 8200: Loss = -11006.995820751623
1
Iteration 8300: Loss = -11006.956059686232
Iteration 8400: Loss = -11006.956039588435
Iteration 8500: Loss = -11006.955922689885
Iteration 8600: Loss = -11006.955840598976
Iteration 8700: Loss = -11006.956996010966
1
Iteration 8800: Loss = -11006.955708427042
Iteration 8900: Loss = -11006.955627198937
Iteration 9000: Loss = -11006.962361748278
1
Iteration 9100: Loss = -11006.955415380551
Iteration 9200: Loss = -11006.955321542602
Iteration 9300: Loss = -11007.000198817173
1
Iteration 9400: Loss = -11006.955052588164
Iteration 9500: Loss = -11006.95488316826
Iteration 9600: Loss = -11006.959948082214
1
Iteration 9700: Loss = -11006.954555918092
Iteration 9800: Loss = -11006.954361447863
Iteration 9900: Loss = -11006.95502471145
1
Iteration 10000: Loss = -11006.953974501736
Iteration 10100: Loss = -11006.953819033168
Iteration 10200: Loss = -11006.953706130975
Iteration 10300: Loss = -11006.953481366842
Iteration 10400: Loss = -11006.954662960936
1
Iteration 10500: Loss = -11006.953351130649
Iteration 10600: Loss = -11007.095391114848
1
Iteration 10700: Loss = -11006.953282115746
Iteration 10800: Loss = -11006.964295069138
1
Iteration 10900: Loss = -11006.953266775161
Iteration 11000: Loss = -11007.323927176833
1
Iteration 11100: Loss = -11006.95328038084
2
Iteration 11200: Loss = -11006.95326573059
Iteration 11300: Loss = -11006.953321449464
1
Iteration 11400: Loss = -11006.963523828435
2
Iteration 11500: Loss = -11006.953282904176
3
Iteration 11600: Loss = -11006.96137232422
4
Iteration 11700: Loss = -11006.953288873989
5
Iteration 11800: Loss = -11006.953445673875
6
Iteration 11900: Loss = -11006.953317271486
7
Iteration 12000: Loss = -11007.062444086321
8
Iteration 12100: Loss = -11006.953241312716
Iteration 12200: Loss = -11007.02831787411
1
Iteration 12300: Loss = -11006.953267891175
2
Iteration 12400: Loss = -11006.954170411302
3
Iteration 12500: Loss = -11006.953268489331
4
Iteration 12600: Loss = -11006.953871022293
5
Iteration 12700: Loss = -11006.961703698296
6
Iteration 12800: Loss = -11006.97562207798
7
Iteration 12900: Loss = -11006.953249242442
8
Iteration 13000: Loss = -11006.954702724697
9
Iteration 13100: Loss = -11006.953627178857
10
Stopping early at iteration 13100 due to no improvement.
tensor([[-2.9947,  1.6030],
        [-3.3669,  1.3471],
        [-4.4405, -0.1747],
        [-3.1208,  1.7204],
        [-2.5593,  1.1687],
        [-3.3497,  1.6822],
        [-3.5545,  0.9024],
        [-2.8732,  1.4793],
        [-3.0525,  1.5944],
        [-2.7174,  1.2019],
        [-3.0159,  1.5411],
        [-2.9305,  1.5300],
        [-3.4605,  0.7174],
        [-3.6306,  1.4894],
        [-3.4102,  1.1517],
        [-3.3336,  1.5015],
        [-2.7984,  1.3731],
        [-3.1558,  1.4956],
        [-2.7873,  1.3729],
        [-3.2681,  0.8988],
        [-3.1286,  1.7416],
        [-2.8986,  1.5023],
        [-3.5511,  1.0142],
        [-3.9447,  0.6038],
        [-2.9637,  1.4974],
        [-3.4891,  0.7071],
        [-2.7393,  1.3482],
        [-3.3180,  1.6998],
        [-2.9628,  1.5300],
        [-2.9266,  0.9201],
        [-2.7904,  1.3165],
        [-2.9824,  1.3759],
        [-3.3907,  1.4346],
        [-3.6009,  0.8743],
        [-3.0252,  1.4196],
        [-3.0381,  1.1347],
        [-3.2526,  1.4035],
        [-3.4442,  1.7725],
        [-3.0128,  1.5293],
        [-3.0306,  1.5379],
        [-3.2633,  1.6459],
        [-3.0738,  1.6770],
        [-3.2828,  1.1721],
        [-3.0211,  1.5892],
        [-3.6463,  1.4578],
        [-3.1599,  1.3254],
        [-2.8636,  1.3213],
        [-3.5826,  1.1728],
        [-3.6981,  0.7796],
        [-2.9842,  1.5412],
        [-2.9273,  1.3069],
        [-2.8390,  1.4450],
        [-4.4900, -0.1253],
        [-3.0676,  1.6808],
        [-3.0532,  1.4954],
        [-3.2686,  1.6589],
        [-3.7108,  1.0424],
        [-3.1959,  1.6436],
        [-3.0092,  1.5742],
        [-2.8842,  1.4822],
        [-2.9489,  1.5561],
        [-2.9622,  1.5005],
        [-2.9585,  1.4942],
        [-4.3804,  0.3570],
        [-3.2264,  0.8638],
        [-3.0250,  1.6136],
        [-3.1245,  1.5927],
        [-2.9844,  1.5582],
        [-3.4623,  1.1980],
        [-3.4040,  1.5210],
        [-3.1359,  1.7083],
        [-3.4246,  1.6941],
        [-3.8591,  0.3695],
        [-3.5087,  1.2398],
        [-3.2129,  1.5388],
        [-2.7463,  1.3516],
        [-3.1488,  1.5037],
        [-3.3576,  1.3064],
        [-3.7284,  0.9078],
        [-3.0088,  1.4395],
        [-2.9969,  1.5320],
        [-3.2027,  1.7953],
        [-3.0201,  1.3196],
        [-3.1198,  1.4307],
        [-3.4066,  1.7069],
        [-3.2485,  1.4889],
        [-3.0235,  1.6283],
        [-4.2292,  0.6665],
        [-3.7016,  0.4456],
        [-3.9465,  0.3355],
        [-3.4207,  1.4348],
        [-3.4281,  1.7900],
        [-3.3676,  1.9370],
        [-2.7826,  1.3954],
        [-3.1360,  1.4070],
        [-2.8989,  1.3931],
        [-3.5328,  1.0053],
        [-2.9942,  1.1542],
        [-3.1189,  1.6196],
        [-3.0868,  1.6900]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.1386, 0.8614],
        [0.1033, 0.8967]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0109, 0.9891], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2324, 0.1689],
         [0.2413, 0.1565]],

        [[0.1397, 0.1981],
         [0.5256, 0.4232]],

        [[0.2579, 0.1962],
         [0.3665, 0.9280]],

        [[0.5010, 0.1906],
         [0.9845, 0.4270]],

        [[0.8031, 0.1938],
         [0.5778, 0.8079]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 4.2092736862769894e-05
Average Adjusted Rand Index: -0.0006936617740552836
Iteration 0: Loss = -26115.61269763991
Iteration 10: Loss = -11008.839359219866
Iteration 20: Loss = -11008.77023244522
Iteration 30: Loss = -11008.600396215534
Iteration 40: Loss = -11008.321980628732
Iteration 50: Loss = -11008.048585076767
Iteration 60: Loss = -11007.874300682497
Iteration 70: Loss = -11007.793525358022
Iteration 80: Loss = -11007.754931684238
Iteration 90: Loss = -11007.683615836831
Iteration 100: Loss = -11007.568639011974
Iteration 110: Loss = -11007.522362741085
Iteration 120: Loss = -11007.513191316191
Iteration 130: Loss = -11007.512702125485
Iteration 140: Loss = -11007.516292701517
1
Iteration 150: Loss = -11007.522458463958
2
Iteration 160: Loss = -11007.530501337616
3
Stopping early at iteration 159 due to no improvement.
pi: tensor([[0.1455, 0.8545],
        [0.1348, 0.8652]], dtype=torch.float64)
alpha: tensor([0.1347, 0.8653])
beta: tensor([[[0.2134, 0.1675],
         [0.6248, 0.1530]],

        [[0.2162, 0.1909],
         [0.5956, 0.6986]],

        [[0.4626, 0.1910],
         [0.5857, 0.5833]],

        [[0.3076, 0.1855],
         [0.6880, 0.4862]],

        [[0.4682, 0.1873],
         [0.2979, 0.4469]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.00035341041046094813
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: -0.0003845451714286067
Average Adjusted Rand Index: -0.0007316234276684654
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26114.983067105048
Iteration 100: Loss = -11019.852877993395
Iteration 200: Loss = -11013.854893163993
Iteration 300: Loss = -11011.640741792868
Iteration 400: Loss = -11010.512212015377
Iteration 500: Loss = -11009.828638857814
Iteration 600: Loss = -11009.318222703914
Iteration 700: Loss = -11008.750615316452
Iteration 800: Loss = -11008.19041778949
Iteration 900: Loss = -11007.873497391038
Iteration 1000: Loss = -11007.677535033594
Iteration 1100: Loss = -11007.530319365771
Iteration 1200: Loss = -11007.412205473798
Iteration 1300: Loss = -11007.31689289091
Iteration 1400: Loss = -11007.2413987934
Iteration 1500: Loss = -11007.181803443187
Iteration 1600: Loss = -11007.133585138125
Iteration 1700: Loss = -11007.09474145242
Iteration 1800: Loss = -11007.066617242766
Iteration 1900: Loss = -11007.046843681052
Iteration 2000: Loss = -11007.031503271526
Iteration 2100: Loss = -11007.019248352246
Iteration 2200: Loss = -11007.009204959568
Iteration 2300: Loss = -11007.00082507972
Iteration 2400: Loss = -11006.993824474626
Iteration 2500: Loss = -11006.988055154941
Iteration 2600: Loss = -11006.983399142957
Iteration 2700: Loss = -11006.979598018414
Iteration 2800: Loss = -11006.976489526463
Iteration 2900: Loss = -11006.973863097515
Iteration 3000: Loss = -11006.971697847513
Iteration 3100: Loss = -11006.969856155096
Iteration 3200: Loss = -11006.968231285586
Iteration 3300: Loss = -11006.966864448417
Iteration 3400: Loss = -11006.965773653865
Iteration 3500: Loss = -11006.964756378866
Iteration 3600: Loss = -11006.963930916409
Iteration 3700: Loss = -11006.963213985122
Iteration 3800: Loss = -11006.96262173104
Iteration 3900: Loss = -11006.962043979027
Iteration 4000: Loss = -11006.961667081627
Iteration 4100: Loss = -11006.961248070578
Iteration 4200: Loss = -11006.960924329356
Iteration 4300: Loss = -11006.960618262226
Iteration 4400: Loss = -11006.960332758277
Iteration 4500: Loss = -11006.96011996353
Iteration 4600: Loss = -11006.95992588532
Iteration 4700: Loss = -11006.95978397594
Iteration 4800: Loss = -11006.9595735233
Iteration 4900: Loss = -11006.959470218306
Iteration 5000: Loss = -11006.95927255638
Iteration 5100: Loss = -11006.959150727174
Iteration 5200: Loss = -11006.95899964214
Iteration 5300: Loss = -11006.958913248327
Iteration 5400: Loss = -11006.958818580546
Iteration 5500: Loss = -11006.958718566602
Iteration 5600: Loss = -11006.958602503522
Iteration 5700: Loss = -11006.958582877862
Iteration 5800: Loss = -11006.958452617424
Iteration 5900: Loss = -11006.958412789962
Iteration 6000: Loss = -11006.958370011966
Iteration 6100: Loss = -11006.958372465017
1
Iteration 6200: Loss = -11006.958310415928
Iteration 6300: Loss = -11006.958208920312
Iteration 6400: Loss = -11006.959326954277
1
Iteration 6500: Loss = -11006.95812262103
Iteration 6600: Loss = -11006.958110462361
Iteration 6700: Loss = -11006.959934765762
1
Iteration 6800: Loss = -11006.958043385557
Iteration 6900: Loss = -11006.958134637332
1
Iteration 7000: Loss = -11006.978876466723
2
Iteration 7100: Loss = -11006.957972654238
Iteration 7200: Loss = -11006.958003269487
1
Iteration 7300: Loss = -11006.960187941499
2
Iteration 7400: Loss = -11006.957934122645
Iteration 7500: Loss = -11006.989956942503
1
Iteration 7600: Loss = -11006.957921019979
Iteration 7700: Loss = -11006.958267418922
1
Iteration 7800: Loss = -11007.064962721075
2
Iteration 7900: Loss = -11006.957897492024
Iteration 8000: Loss = -11006.958072905325
1
Iteration 8100: Loss = -11006.983585958822
2
Iteration 8200: Loss = -11006.957841401201
Iteration 8300: Loss = -11006.959861061443
1
Iteration 8400: Loss = -11006.957872455268
2
Iteration 8500: Loss = -11006.95808979348
3
Iteration 8600: Loss = -11006.957859173845
4
Iteration 8700: Loss = -11006.958034866815
5
Iteration 8800: Loss = -11006.957850420182
6
Iteration 8900: Loss = -11006.958332308941
7
Iteration 9000: Loss = -11006.957832694008
Iteration 9100: Loss = -11006.957828711033
Iteration 9200: Loss = -11006.958175976999
1
Iteration 9300: Loss = -11006.958046474063
2
Iteration 9400: Loss = -11006.958808649686
3
Iteration 9500: Loss = -11007.019749196783
4
Iteration 9600: Loss = -11006.987504915656
5
Iteration 9700: Loss = -11007.032604248017
6
Iteration 9800: Loss = -11007.027048856327
7
Iteration 9900: Loss = -11006.990562486579
8
Iteration 10000: Loss = -11006.95996211924
9
Iteration 10100: Loss = -11006.95979287381
10
Stopping early at iteration 10100 due to no improvement.
tensor([[-4.9624,  3.3160],
        [-4.9549,  3.4110],
        [-4.9149,  3.0398],
        [-5.4057,  3.0702],
        [-4.4229,  3.0347],
        [-5.0284,  3.6421],
        [-5.3926,  2.7391],
        [-4.9138,  3.1229],
        [-5.4896,  2.8254],
        [-5.2039,  2.4237],
        [-5.3883,  2.8307],
        [-5.2925,  2.8554],
        [-4.6444,  3.2326],
        [-5.0837,  3.6656],
        [-4.9525,  3.2738],
        [-4.9949,  3.4941],
        [-5.0090,  2.8492],
        [-5.0378,  3.2813],
        [-4.7370,  3.1216],
        [-4.6390,  3.2275],
        [-5.3350,  3.1726],
        [-4.7433,  3.3374],
        [-4.8921,  3.3317],
        [-5.4628,  2.7537],
        [-4.7632,  3.3701],
        [-4.6372,  3.2496],
        [-4.5882,  3.2019],
        [-5.0232,  3.6331],
        [-4.9251,  3.2526],
        [-4.4934,  3.0901],
        [-4.6027,  3.2157],
        [-4.7161,  3.3243],
        [-5.1659,  3.3269],
        [-4.8901,  3.2528],
        [-5.5603,  2.5634],
        [-4.7108,  3.1653],
        [-4.9818,  3.3519],
        [-5.1296,  3.7249],
        [-4.8092,  3.4020],
        [-5.0114,  3.2304],
        [-4.9755,  3.5800],
        [-5.8449,  2.5724],
        [-4.8603,  3.2747],
        [-4.9970,  3.2791],
        [-5.3005,  3.4628],
        [-5.0749,  3.0919],
        [-4.9505,  2.9379],
        [-5.4373,  2.9836],
        [-4.8593,  3.2999],
        [-5.1946,  3.0072],
        [-4.7874,  3.1432],
        [-5.9509,  2.0204],
        [-4.7204,  3.3242],
        [-5.2168,  3.2070],
        [-4.8612,  3.3583],
        [-5.0081,  3.5650],
        [-4.9247,  3.4832],
        [-4.9354,  3.5464],
        [-5.3784,  2.8770],
        [-4.7642,  3.2821],
        [-4.8639,  3.3231],
        [-4.9277,  3.2123],
        [-4.9838,  3.1384],
        [-4.9472,  3.4437],
        [-4.6062,  3.1846],
        [-5.3072,  3.0055],
        [-6.0498,  2.3333],
        [-4.8329,  3.3778],
        [-4.8801,  3.4486],
        [-5.3897,  3.1969],
        [-5.0217,  3.4722],
        [-5.0742,  3.6814],
        [-4.7565,  3.1702],
        [-4.8997,  3.5053],
        [-4.8945,  3.5048],
        [-4.6486,  3.1505],
        [-5.4326,  2.8694],
        [-4.9637,  3.3631],
        [-4.8574,  3.4402],
        [-4.8322,  3.3004],
        [-4.8782,  3.3337],
        [-5.2049,  3.4257],
        [-4.9622,  3.0552],
        [-4.8929,  3.3198],
        [-5.3203,  3.4515],
        [-5.0849,  3.3183],
        [-5.2205,  3.0819],
        [-5.4383,  3.1240],
        [-4.7140,  3.1439],
        [-4.7043,  3.2663],
        [-4.9803,  3.5227],
        [-5.3553,  3.5135],
        [-6.7876,  2.1724],
        [-4.7218,  3.1433],
        [-5.0899,  3.1275],
        [-5.3235,  2.6742],
        [-4.7985,  3.4122],
        [-4.6294,  3.2175],
        [-5.1410,  3.2481],
        [-5.0581,  3.3830]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.1305, 0.8695],
        [0.1014, 0.8986]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([2.7747e-04, 9.9972e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2332, 0.1684],
         [0.6248, 0.1567]],

        [[0.2162, 0.1981],
         [0.5956, 0.6986]],

        [[0.4626, 0.1964],
         [0.5857, 0.5833]],

        [[0.3076, 0.1907],
         [0.6880, 0.4862]],

        [[0.4682, 0.1941],
         [0.2979, 0.4469]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 4.2092736862769894e-05
Average Adjusted Rand Index: -0.0006936617740552836
Iteration 0: Loss = -31481.71341333526
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.6743,    nan]],

        [[0.4680,    nan],
         [0.8466, 0.4872]],

        [[0.8929,    nan],
         [0.3618, 0.5299]],

        [[0.9767,    nan],
         [0.4711, 0.7938]],

        [[0.3775,    nan],
         [0.8204, 0.2586]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31481.313075838352
Iteration 100: Loss = -11032.783458269021
Iteration 200: Loss = -11020.30732937544
Iteration 300: Loss = -11015.585405525766
Iteration 400: Loss = -11013.24617657186
Iteration 500: Loss = -11011.91129617556
Iteration 600: Loss = -11011.075969765725
Iteration 700: Loss = -11010.515843344288
Iteration 800: Loss = -11010.120935731116
Iteration 900: Loss = -11009.831519652189
Iteration 1000: Loss = -11009.612778632109
Iteration 1100: Loss = -11009.44339526546
Iteration 1200: Loss = -11009.309460914807
Iteration 1300: Loss = -11009.201724860126
Iteration 1400: Loss = -11009.113761641385
Iteration 1500: Loss = -11009.040942606092
Iteration 1600: Loss = -11008.979963892305
Iteration 1700: Loss = -11008.92820392442
Iteration 1800: Loss = -11008.883878904344
Iteration 1900: Loss = -11008.845454547692
Iteration 2000: Loss = -11008.811742487844
Iteration 2100: Loss = -11008.781763595185
Iteration 2200: Loss = -11008.75470907042
Iteration 2300: Loss = -11008.729793691899
Iteration 2400: Loss = -11008.706407907835
Iteration 2500: Loss = -11008.683723786371
Iteration 2600: Loss = -11008.66087517746
Iteration 2700: Loss = -11008.63623154545
Iteration 2800: Loss = -11008.605882210666
Iteration 2900: Loss = -11008.553237888986
Iteration 3000: Loss = -11008.377185475265
Iteration 3100: Loss = -11008.116390913652
Iteration 3200: Loss = -11007.966673580288
Iteration 3300: Loss = -11007.869350858731
Iteration 3400: Loss = -11007.797214330298
Iteration 3500: Loss = -11007.73892998598
Iteration 3600: Loss = -11007.689063606804
Iteration 3700: Loss = -11007.644633058051
Iteration 3800: Loss = -11007.604008714692
Iteration 3900: Loss = -11007.566366700774
Iteration 4000: Loss = -11007.531479210606
Iteration 4100: Loss = -11007.499378085658
Iteration 4200: Loss = -11007.47025647143
Iteration 4300: Loss = -11007.444235849369
Iteration 4400: Loss = -11007.421348447066
Iteration 4500: Loss = -11007.401281079252
Iteration 4600: Loss = -11007.383483863552
Iteration 4700: Loss = -11007.369742735162
Iteration 4800: Loss = -11007.359517988734
Iteration 4900: Loss = -11007.35149481268
Iteration 5000: Loss = -11007.345175286184
Iteration 5100: Loss = -11007.34022562823
Iteration 5200: Loss = -11007.336214000019
Iteration 5300: Loss = -11007.332936541636
Iteration 5400: Loss = -11007.330183121087
Iteration 5500: Loss = -11007.327880292798
Iteration 5600: Loss = -11007.326331148675
Iteration 5700: Loss = -11007.324239342077
Iteration 5800: Loss = -11007.322799015843
Iteration 5900: Loss = -11007.329073981684
1
Iteration 6000: Loss = -11007.320359946703
Iteration 6100: Loss = -11007.319316582203
Iteration 6200: Loss = -11007.854230652452
1
Iteration 6300: Loss = -11007.317446995745
Iteration 6400: Loss = -11007.31661209596
Iteration 6500: Loss = -11007.315829207251
Iteration 6600: Loss = -11007.315182677274
Iteration 6700: Loss = -11007.31440982536
Iteration 6800: Loss = -11007.313735167269
Iteration 6900: Loss = -11007.328523719189
1
Iteration 7000: Loss = -11007.31252077948
Iteration 7100: Loss = -11007.311867133454
Iteration 7200: Loss = -11007.311225814377
Iteration 7300: Loss = -11007.31183786282
1
Iteration 7400: Loss = -11007.310114615424
Iteration 7500: Loss = -11007.309676411429
Iteration 7600: Loss = -11007.313301603805
1
Iteration 7700: Loss = -11007.308939139126
Iteration 7800: Loss = -11007.308634288349
Iteration 7900: Loss = -11007.343299324497
1
Iteration 8000: Loss = -11007.307964628915
Iteration 8100: Loss = -11007.307647181498
Iteration 8200: Loss = -11007.307369887432
Iteration 8300: Loss = -11007.367762485575
1
Iteration 8400: Loss = -11007.306833347848
Iteration 8500: Loss = -11007.30659103125
Iteration 8600: Loss = -11007.306320695408
Iteration 8700: Loss = -11007.307684433741
1
Iteration 8800: Loss = -11007.305863888532
Iteration 8900: Loss = -11007.305636179102
Iteration 9000: Loss = -11007.305457691902
Iteration 9100: Loss = -11007.305244667072
Iteration 9200: Loss = -11007.305246500637
1
Iteration 9300: Loss = -11007.304862362518
Iteration 9400: Loss = -11007.304677506643
Iteration 9500: Loss = -11007.304517236287
Iteration 9600: Loss = -11007.30444085327
Iteration 9700: Loss = -11007.304185742045
Iteration 9800: Loss = -11007.304022738348
Iteration 9900: Loss = -11007.421426957733
1
Iteration 10000: Loss = -11007.303742159165
Iteration 10100: Loss = -11007.303635205415
Iteration 10200: Loss = -11007.303451900709
Iteration 10300: Loss = -11007.307414529292
1
Iteration 10400: Loss = -11007.303232962859
Iteration 10500: Loss = -11007.303112139718
Iteration 10600: Loss = -11007.323332225606
1
Iteration 10700: Loss = -11007.302909311848
Iteration 10800: Loss = -11007.302803516528
Iteration 10900: Loss = -11007.321202672776
1
Iteration 11000: Loss = -11007.302606289513
Iteration 11100: Loss = -11007.30252169847
Iteration 11200: Loss = -11007.302394021544
Iteration 11300: Loss = -11007.309656283403
1
Iteration 11400: Loss = -11007.302257811452
Iteration 11500: Loss = -11007.302181198189
Iteration 11600: Loss = -11007.302098404354
Iteration 11700: Loss = -11007.302490198814
1
Iteration 11800: Loss = -11007.301930163549
Iteration 11900: Loss = -11007.301871732185
Iteration 12000: Loss = -11007.321801753282
1
Iteration 12100: Loss = -11007.301711343902
Iteration 12200: Loss = -11007.30167910308
Iteration 12300: Loss = -11007.301618278369
Iteration 12400: Loss = -11007.301584304387
Iteration 12500: Loss = -11007.301438138882
Iteration 12600: Loss = -11007.301374316683
Iteration 12700: Loss = -11007.30383017381
1
Iteration 12800: Loss = -11007.301225998172
Iteration 12900: Loss = -11007.301148912322
Iteration 13000: Loss = -11007.301076594797
Iteration 13100: Loss = -11007.301493719726
1
Iteration 13200: Loss = -11007.300867316973
Iteration 13300: Loss = -11007.300758683248
Iteration 13400: Loss = -11007.324394404002
1
Iteration 13500: Loss = -11007.300399101336
Iteration 13600: Loss = -11007.300222706543
Iteration 13700: Loss = -11007.299904249994
Iteration 13800: Loss = -11007.299603750273
Iteration 13900: Loss = -11007.298953958389
Iteration 14000: Loss = -11007.298080396866
Iteration 14100: Loss = -11007.296786146379
Iteration 14200: Loss = -11007.294261840478
Iteration 14300: Loss = -11007.293885085326
Iteration 14400: Loss = -11007.288345297735
Iteration 14500: Loss = -11007.288411857899
1
Iteration 14600: Loss = -11007.287902723765
Iteration 14700: Loss = -11007.287800171403
Iteration 14800: Loss = -11007.322204442546
1
Iteration 14900: Loss = -11007.287749979832
Iteration 15000: Loss = -11007.287746298089
Iteration 15100: Loss = -11007.519561419418
1
Iteration 15200: Loss = -11007.28769679504
Iteration 15300: Loss = -11007.287706873632
1
Iteration 15400: Loss = -11007.311159478617
2
Iteration 15500: Loss = -11007.287686296668
Iteration 15600: Loss = -11007.287694922887
1
Iteration 15700: Loss = -11007.287813969377
2
Iteration 15800: Loss = -11007.287687436048
3
Iteration 15900: Loss = -11007.287619632223
Iteration 16000: Loss = -11007.287832402022
1
Iteration 16100: Loss = -11007.287608112316
Iteration 16200: Loss = -11007.287776355493
1
Iteration 16300: Loss = -11007.287745448935
2
Iteration 16400: Loss = -11007.287627171534
3
Iteration 16500: Loss = -11007.292179396558
4
Iteration 16600: Loss = -11007.28762500426
5
Iteration 16700: Loss = -11007.287606672322
Iteration 16800: Loss = -11007.288207120659
1
Iteration 16900: Loss = -11007.287562909982
Iteration 17000: Loss = -11007.297463899866
1
Iteration 17100: Loss = -11007.287544516534
Iteration 17200: Loss = -11007.28755482017
1
Iteration 17300: Loss = -11007.287750016358
2
Iteration 17400: Loss = -11007.287571562438
3
Iteration 17500: Loss = -11007.311130944428
4
Iteration 17600: Loss = -11007.287544930676
5
Iteration 17700: Loss = -11007.28757518121
6
Iteration 17800: Loss = -11007.295418618145
7
Iteration 17900: Loss = -11007.287542707489
Iteration 18000: Loss = -11007.287517851459
Iteration 18100: Loss = -11007.291340124966
1
Iteration 18200: Loss = -11007.287529270043
2
Iteration 18300: Loss = -11007.287508912756
Iteration 18400: Loss = -11007.341817243561
1
Iteration 18500: Loss = -11007.287513948157
2
Iteration 18600: Loss = -11007.28756504571
3
Iteration 18700: Loss = -11007.290218964283
4
Iteration 18800: Loss = -11007.287515179074
5
Iteration 18900: Loss = -11007.28751435171
6
Iteration 19000: Loss = -11007.28895495727
7
Iteration 19100: Loss = -11007.28752268591
8
Iteration 19200: Loss = -11007.287495865325
Iteration 19300: Loss = -11007.415301125082
1
Iteration 19400: Loss = -11007.287515586233
2
Iteration 19500: Loss = -11007.287497853184
3
Iteration 19600: Loss = -11007.805336587031
4
Iteration 19700: Loss = -11007.287529804633
5
Iteration 19800: Loss = -11007.287512263192
6
Iteration 19900: Loss = -11007.287522510303
7
tensor([[-2.9206,  1.4826],
        [-3.0250,  1.2375],
        [-3.1531,  1.7170],
        [-3.2898,  0.8313],
        [-3.7975,  1.7841],
        [-3.1311,  0.7567],
        [-3.5343,  1.0964],
        [-3.0822,  1.6674],
        [-4.1849,  0.1778],
        [-3.7714,  1.5658],
        [-3.1245,  1.3721],
        [-4.6228,  0.0076],
        [-3.1992,  1.7994],
        [-3.2841,  0.4824],
        [-2.9475,  1.5244],
        [-2.7833,  1.3433],
        [-3.4065,  1.5587],
        [-3.8323,  0.5434],
        [-3.2367,  1.7557],
        [-3.1990,  1.7864],
        [-3.0136,  1.0289],
        [-3.0318,  1.6030],
        [-2.9324,  1.5451],
        [-2.9728,  1.4736],
        [-3.0007,  1.6140],
        [-3.8481,  1.1200],
        [-3.5312,  1.5712],
        [-2.7287,  1.1532],
        [-3.4413,  1.1008],
        [-3.7037,  1.6843],
        [-3.3046,  1.7731],
        [-3.8184,  0.9239],
        [-3.1617,  0.9862],
        [-3.0399,  1.5587],
        [-3.0652,  1.5434],
        [-3.2309,  1.7726],
        [-3.3284,  0.9858],
        [-2.6943,  0.9468],
        [-3.7776,  0.7142],
        [-4.5557, -0.0595],
        [-2.8975,  1.1184],
        [-3.0204,  1.2278],
        [-3.3530,  1.2749],
        [-3.0242,  1.3805],
        [-2.6801,  1.0928],
        [-3.0185,  1.5353],
        [-3.2042,  1.7824],
        [-4.1782,  0.0663],
        [-3.1142,  1.4297],
        [-2.9507,  1.5377],
        [-3.1899,  1.6945],
        [-3.2242,  1.6462],
        [-3.0683,  1.6805],
        [-3.1235,  1.0454],
        [-2.9230,  1.5225],
        [-2.7075,  1.2878],
        [-3.1239,  1.1272],
        [-2.9174,  1.2029],
        [-3.0647,  1.3321],
        [-3.1494,  1.5807],
        [-3.0469,  1.4862],
        [-3.0335,  1.5837],
        [-3.0542,  1.5628],
        [-3.2872,  0.9656],
        [-3.3743,  1.7274],
        [-3.0659,  1.3173],
        [-3.0091,  1.2477],
        [-3.6160,  0.8845],
        [-3.1284,  1.1885],
        [-2.7124,  1.3031],
        [-2.9906,  1.1212],
        [-2.6664,  1.0981],
        [-4.0018,  0.8841],
        [-2.9882,  1.2608],
        [-2.8997,  1.3278],
        [-3.5692,  1.5352],
        [-3.8953,  0.4647],
        [-4.2233,  0.1357],
        [-2.8877,  1.4912],
        [-3.6792,  0.9585],
        [-3.2226,  1.2569],
        [-3.2851,  0.6170],
        [-3.0666,  1.6745],
        [-3.0384,  1.4463],
        [-2.5832,  1.1794],
        [-3.0111,  1.2491],
        [-2.9008,  1.4628],
        [-3.1973,  0.8276],
        [-3.1875,  1.7801],
        [-3.3242,  1.5412],
        [-3.1516,  0.9585],
        [-2.6537,  0.9816],
        [-2.4672,  1.0594],
        [-3.3314,  1.6335],
        [-2.9942,  1.5016],
        [-3.4811,  1.3094],
        [-2.9567,  1.5524],
        [-3.3702,  1.6064],
        [-2.9995,  1.2549],
        [-2.8682,  1.2921]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.7876e-05, 9.9998e-01],
        [1.9400e-02, 9.8060e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0121, 0.9879], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2131, 0.1462],
         [0.6743, 0.1625]],

        [[0.4680, 0.2151],
         [0.8466, 0.4872]],

        [[0.8929, 0.2080],
         [0.3618, 0.5299]],

        [[0.9767, 0.2068],
         [0.4711, 0.7938]],

        [[0.3775, 0.0723],
         [0.8204, 0.2586]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: -0.0005093525538803316
Average Adjusted Rand Index: -0.0005926784880256398
10969.634682281672
new:  [4.2092736862769894e-05, 4.2092736862769894e-05, 4.2092736862769894e-05, -0.0005093525538803316] [-0.0006936617740552836, -0.0006936617740552836, -0.0006936617740552836, -0.0005926784880256398] [11006.953257928584, 11006.953627178857, 11006.95979287381, 11007.289673960066]
prior:  [-0.0003845451714286067, 0.0, -0.0003845451714286067, 0.0] [-0.0007316234276684654, 0.0, -0.0007316234276684654, 0.0] [11007.528313746976, nan, 11007.530501337616, nan]
-----------------------------------------------------------------------------------------
This iteration is 18
True Objective function: Loss = -10867.610104028518
Iteration 0: Loss = -32944.79061802903
Iteration 10: Loss = -10976.103095895858
Iteration 20: Loss = -10976.10309589969
1
Iteration 30: Loss = -10976.103095968223
2
Iteration 40: Loss = -10976.103097193656
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.3994e-21, 1.0000e+00],
        [5.1640e-11, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([4.8857e-11, 1.0000e+00])
beta: tensor([[[0.2920, 0.1402],
         [0.8346, 0.1607]],

        [[0.5463, 0.2613],
         [0.9056, 0.1407]],

        [[0.3112, 0.2229],
         [0.6061, 0.6178]],

        [[0.2188, 0.2005],
         [0.1855, 0.9585]],

        [[0.0986, 0.2339],
         [0.6880, 0.3648]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -32988.30980924613
Iteration 100: Loss = -10981.239342164765
Iteration 200: Loss = -10978.780284286187
Iteration 300: Loss = -10977.683828983116
Iteration 400: Loss = -10977.121905782024
Iteration 500: Loss = -10976.794674656612
Iteration 600: Loss = -10976.582463321614
Iteration 700: Loss = -10976.431866176754
Iteration 800: Loss = -10976.31636963297
Iteration 900: Loss = -10976.221608090278
Iteration 1000: Loss = -10976.139549153097
Iteration 1100: Loss = -10976.068024880999
Iteration 1200: Loss = -10976.010170971507
Iteration 1300: Loss = -10975.969073589878
Iteration 1400: Loss = -10975.944005978097
Iteration 1500: Loss = -10975.93016590755
Iteration 1600: Loss = -10975.922132877726
Iteration 1700: Loss = -10975.916534819396
Iteration 1800: Loss = -10975.912048923521
Iteration 1900: Loss = -10975.90818366027
Iteration 2000: Loss = -10975.90467178676
Iteration 2100: Loss = -10975.901508278255
Iteration 2200: Loss = -10975.898558052646
Iteration 2300: Loss = -10975.89593090692
Iteration 2400: Loss = -10975.893470728905
Iteration 2500: Loss = -10975.891187639394
Iteration 2600: Loss = -10975.88909279383
Iteration 2700: Loss = -10975.887142361398
Iteration 2800: Loss = -10975.885284421245
Iteration 2900: Loss = -10975.883602803438
Iteration 3000: Loss = -10975.88201426657
Iteration 3100: Loss = -10975.880474516018
Iteration 3200: Loss = -10975.879100186945
Iteration 3300: Loss = -10975.877784079508
Iteration 3400: Loss = -10975.876557237036
Iteration 3500: Loss = -10975.87539746482
Iteration 3600: Loss = -10975.874288758552
Iteration 3700: Loss = -10975.873231966298
Iteration 3800: Loss = -10975.872295293842
Iteration 3900: Loss = -10975.871362463507
Iteration 4000: Loss = -10975.870507716207
Iteration 4100: Loss = -10975.86972181431
Iteration 4200: Loss = -10975.868941370949
Iteration 4300: Loss = -10975.868215168122
Iteration 4400: Loss = -10975.867477515796
Iteration 4500: Loss = -10975.866846772706
Iteration 4600: Loss = -10975.866252012504
Iteration 4700: Loss = -10975.86568360541
Iteration 4800: Loss = -10975.865124331043
Iteration 4900: Loss = -10975.864577768898
Iteration 5000: Loss = -10975.864106369649
Iteration 5100: Loss = -10975.863639769173
Iteration 5200: Loss = -10975.863187025061
Iteration 5300: Loss = -10975.862757365105
Iteration 5400: Loss = -10975.862320098522
Iteration 5500: Loss = -10975.861968990386
Iteration 5600: Loss = -10975.861594320266
Iteration 5700: Loss = -10975.861199828754
Iteration 5800: Loss = -10975.860894629424
Iteration 5900: Loss = -10975.860493810369
Iteration 6000: Loss = -10975.860192452714
Iteration 6100: Loss = -10975.859899606694
Iteration 6200: Loss = -10975.859620332883
Iteration 6300: Loss = -10975.85933966137
Iteration 6400: Loss = -10975.859030231286
Iteration 6500: Loss = -10975.85874596735
Iteration 6600: Loss = -10975.878318367477
1
Iteration 6700: Loss = -10975.858169418614
Iteration 6800: Loss = -10975.857893758679
Iteration 6900: Loss = -10975.857575658658
Iteration 7000: Loss = -10975.85930083693
1
Iteration 7100: Loss = -10975.85695308691
Iteration 7200: Loss = -10975.856629037455
Iteration 7300: Loss = -10975.921358946815
1
Iteration 7400: Loss = -10975.855967923271
Iteration 7500: Loss = -10975.855594102544
Iteration 7600: Loss = -10975.855189298858
Iteration 7700: Loss = -10975.869324501466
1
Iteration 7800: Loss = -10975.854375281939
Iteration 7900: Loss = -10975.853952122665
Iteration 8000: Loss = -10975.853460443395
Iteration 8100: Loss = -10976.132055959395
1
Iteration 8200: Loss = -10975.85246191479
Iteration 8300: Loss = -10975.851874197282
Iteration 8400: Loss = -10975.851206508773
Iteration 8500: Loss = -10975.852396493743
1
Iteration 8600: Loss = -10975.849613445986
Iteration 8700: Loss = -10975.848573617408
Iteration 8800: Loss = -10975.847222847065
Iteration 8900: Loss = -10975.847568721865
1
Iteration 9000: Loss = -10975.843121709288
Iteration 9100: Loss = -10975.839705501316
Iteration 9200: Loss = -10975.834118209397
Iteration 9300: Loss = -10975.823191826796
Iteration 9400: Loss = -10975.79187660385
Iteration 9500: Loss = -10975.613339502665
Iteration 9600: Loss = -10975.316470831516
Iteration 9700: Loss = -10975.082030348809
Iteration 9800: Loss = -10975.075442894957
Iteration 9900: Loss = -10975.07161305335
Iteration 10000: Loss = -10975.070447697037
Iteration 10100: Loss = -10975.069752673922
Iteration 10200: Loss = -10975.125589660624
1
Iteration 10300: Loss = -10975.068228035681
Iteration 10400: Loss = -10975.070181330295
1
Iteration 10500: Loss = -10975.07275188631
2
Iteration 10600: Loss = -10975.067600253082
Iteration 10700: Loss = -10975.06754020303
Iteration 10800: Loss = -10975.089625200577
1
Iteration 10900: Loss = -10975.06716924665
Iteration 11000: Loss = -10975.09144255952
1
Iteration 11100: Loss = -10975.117254609437
2
Iteration 11200: Loss = -10975.067063661323
Iteration 11300: Loss = -10975.066915925127
Iteration 11400: Loss = -10975.071319658991
1
Iteration 11500: Loss = -10975.088482930265
2
Iteration 11600: Loss = -10975.088581602218
3
Iteration 11700: Loss = -10975.067416921529
4
Iteration 11800: Loss = -10975.089325699337
5
Iteration 11900: Loss = -10975.075388065055
6
Iteration 12000: Loss = -10975.066835202193
Iteration 12100: Loss = -10975.077731180483
1
Iteration 12200: Loss = -10975.066601236698
Iteration 12300: Loss = -10975.06654368171
Iteration 12400: Loss = -10975.06721070735
1
Iteration 12500: Loss = -10975.108103746981
2
Iteration 12600: Loss = -10975.073391048765
3
Iteration 12700: Loss = -10975.06647228916
Iteration 12800: Loss = -10975.07469145175
1
Iteration 12900: Loss = -10975.072573859949
2
Iteration 13000: Loss = -10975.066676120654
3
Iteration 13100: Loss = -10975.066460085312
Iteration 13200: Loss = -10975.137847363618
1
Iteration 13300: Loss = -10975.066314657017
Iteration 13400: Loss = -10975.072649639436
1
Iteration 13500: Loss = -10975.085717950411
2
Iteration 13600: Loss = -10975.242932119367
3
Iteration 13700: Loss = -10975.066237637011
Iteration 13800: Loss = -10975.066437798325
1
Iteration 13900: Loss = -10975.067979694137
2
Iteration 14000: Loss = -10975.0661977149
Iteration 14100: Loss = -10975.084233393098
1
Iteration 14200: Loss = -10975.066999113009
2
Iteration 14300: Loss = -10975.070770505878
3
Iteration 14400: Loss = -10975.068303782591
4
Iteration 14500: Loss = -10975.06630670067
5
Iteration 14600: Loss = -10975.06652060024
6
Iteration 14700: Loss = -10975.096086971847
7
Iteration 14800: Loss = -10975.06623976464
8
Iteration 14900: Loss = -10975.073764675753
9
Iteration 15000: Loss = -10975.067184404354
10
Stopping early at iteration 15000 due to no improvement.
tensor([[-2.4987, -2.1165],
        [-2.3611, -2.2542],
        [-2.3902, -2.2251],
        [-2.3780, -2.2372],
        [-2.5065, -2.1087],
        [-2.3866, -2.2286],
        [-2.4124, -2.2029],
        [-2.3518, -2.2635],
        [-2.3165, -2.2987],
        [-2.5879, -2.0273],
        [-2.4283, -2.1869],
        [-2.2830, -2.3323],
        [-2.2638, -2.3514],
        [-2.4638, -2.1514],
        [-2.4409, -2.1744],
        [-2.5499, -2.0654],
        [-2.2789, -2.3363],
        [-2.5598, -2.0554],
        [-2.4951, -2.1201],
        [-2.5323, -2.0829],
        [-2.3707, -2.2445],
        [-2.5613, -2.0539],
        [-2.5794, -2.0358],
        [-2.3815, -2.2337],
        [-2.4584, -2.1568],
        [-2.4683, -2.1469],
        [-2.4200, -2.1952],
        [-2.3670, -2.2482],
        [-2.4870, -2.1282],
        [-2.2995, -2.3157],
        [-2.5543, -2.0610],
        [-2.5133, -2.1019],
        [-2.5558, -2.0594],
        [-2.5523, -2.0629],
        [-2.4742, -2.1410],
        [-2.2388, -2.3764],
        [-2.5403, -2.0749],
        [-2.5055, -2.1097],
        [-2.6839, -1.9314],
        [-2.5975, -2.0177],
        [-2.4495, -2.1657],
        [-2.5528, -2.0624],
        [-2.4019, -2.2133],
        [-2.5039, -2.1113],
        [-2.3259, -2.2893],
        [-2.3719, -2.2433],
        [-2.5019, -2.1133],
        [-2.4817, -2.1335],
        [-2.4176, -2.1976],
        [-2.4925, -2.1227],
        [-2.3520, -2.2632],
        [-2.2812, -2.3340],
        [-2.3882, -2.2270],
        [-2.4903, -2.1249],
        [-2.4615, -2.1537],
        [-2.4679, -2.1473],
        [-2.4650, -2.1502],
        [-2.5543, -2.0609],
        [-2.4119, -2.2033],
        [-2.4373, -2.1780],
        [-2.3690, -2.2462],
        [-2.3918, -2.2234],
        [-2.3734, -2.2419],
        [-2.4315, -2.1837],
        [-2.4701, -2.1452],
        [-2.5619, -2.0533],
        [-2.4542, -2.1610],
        [-2.5293, -2.0860],
        [-2.5934, -2.0219],
        [-2.4171, -2.1981],
        [-2.4307, -2.1845],
        [-2.5561, -2.0591],
        [-2.3691, -2.2461],
        [-2.4260, -2.1892],
        [-1.9765, -2.6388],
        [-2.4387, -2.1765],
        [-2.5123, -2.1030],
        [-2.3956, -2.2197],
        [-2.5937, -2.0215],
        [-2.4642, -2.1510],
        [-2.3201, -2.2951],
        [-2.6124, -2.0028],
        [-2.2392, -2.3760],
        [-2.5220, -2.0932],
        [-2.5202, -2.0950],
        [-2.5073, -2.1079],
        [-2.4869, -2.1283],
        [-2.3892, -2.2260],
        [-2.3553, -2.2599],
        [-2.6438, -1.9714],
        [-2.2730, -2.3422],
        [-2.4036, -2.2117],
        [-2.4894, -2.1258],
        [-2.5744, -2.0409],
        [-2.1492, -2.4660],
        [-2.4880, -2.1272],
        [-2.3409, -2.2744],
        [-2.4270, -2.1882],
        [-2.4865, -2.1288],
        [-2.4032, -2.2120]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.1725e-01, 8.8275e-01],
        [8.3383e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4336, 0.5664], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1520, 0.1578],
         [0.8346, 0.1638]],

        [[0.5463, 0.1257],
         [0.9056, 0.1407]],

        [[0.3112, 0.2409],
         [0.6061, 0.6178]],

        [[0.2188, 0.7337],
         [0.1855, 0.9585]],

        [[0.0986, 0.1364],
         [0.6880, 0.3648]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.009696969696969697
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -2.7862102935448927e-05
Average Adjusted Rand Index: -0.0019393939393939393
Iteration 0: Loss = -27512.37421934144
Iteration 10: Loss = -10974.468957871257
Iteration 20: Loss = -10973.989776627395
Iteration 30: Loss = -10973.849713775673
Iteration 40: Loss = -10973.752317878594
Iteration 50: Loss = -10973.725512933113
Iteration 60: Loss = -10973.72423192745
Iteration 70: Loss = -10973.735257454753
1
Iteration 80: Loss = -10973.754221673425
2
Iteration 90: Loss = -10973.778607555421
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.8685, 0.1315],
        [0.8483, 0.1517]], dtype=torch.float64)
alpha: tensor([0.8686, 0.1314])
beta: tensor([[[0.1513, 0.1696],
         [0.3672, 0.2335]],

        [[0.4135, 0.1941],
         [0.1647, 0.9859]],

        [[0.2812, 0.1861],
         [0.4545, 0.7062]],

        [[0.0521, 0.1856],
         [0.4964, 0.1364]],

        [[0.0683, 0.1937],
         [0.0619, 0.2145]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.0013070675007002147
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.010091437982433116
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.014778186472389411
Global Adjusted Rand Index: 0.0013829266002793894
Average Adjusted Rand Index: -0.0016528341894872366
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27512.078380815088
Iteration 100: Loss = -11002.827925482581
Iteration 200: Loss = -10978.974201674253
Iteration 300: Loss = -10977.479421288745
Iteration 400: Loss = -10976.874267974956
Iteration 500: Loss = -10976.539810951183
Iteration 600: Loss = -10976.323132938041
Iteration 700: Loss = -10976.165875298744
Iteration 800: Loss = -10976.03284441714
Iteration 900: Loss = -10975.90146961787
Iteration 1000: Loss = -10975.777284607608
Iteration 1100: Loss = -10975.693433762277
Iteration 1200: Loss = -10975.627470022517
Iteration 1300: Loss = -10975.566442109908
Iteration 1400: Loss = -10975.492378737994
Iteration 1500: Loss = -10975.300581939384
Iteration 1600: Loss = -10974.96402353222
Iteration 1700: Loss = -10974.88039085726
Iteration 1800: Loss = -10974.813332220201
Iteration 1900: Loss = -10974.725154242506
Iteration 2000: Loss = -10974.599879211317
Iteration 2100: Loss = -10974.430369591988
Iteration 2200: Loss = -10974.26105389443
Iteration 2300: Loss = -10974.156539085496
Iteration 2400: Loss = -10974.080593274133
Iteration 2500: Loss = -10973.680886179473
Iteration 2600: Loss = -10894.683131047505
Iteration 2700: Loss = -10885.97118622839
Iteration 2800: Loss = -10884.6779429488
Iteration 2900: Loss = -10881.934853224944
Iteration 3000: Loss = -10881.655735643695
Iteration 3100: Loss = -10872.885851911544
Iteration 3200: Loss = -10872.840430972361
Iteration 3300: Loss = -10872.821352773772
Iteration 3400: Loss = -10872.812273678486
Iteration 3500: Loss = -10872.806061653568
Iteration 3600: Loss = -10872.797064628418
Iteration 3700: Loss = -10872.793543796794
Iteration 3800: Loss = -10872.790579250755
Iteration 3900: Loss = -10872.78778951039
Iteration 4000: Loss = -10872.785105833673
Iteration 4100: Loss = -10872.782631071046
Iteration 4200: Loss = -10872.78048662988
Iteration 4300: Loss = -10872.77853560765
Iteration 4400: Loss = -10872.776846448434
Iteration 4500: Loss = -10872.775272610297
Iteration 4600: Loss = -10872.773658852593
Iteration 4700: Loss = -10872.771992431597
Iteration 4800: Loss = -10872.770155969843
Iteration 4900: Loss = -10872.768061087743
Iteration 5000: Loss = -10872.765568820792
Iteration 5100: Loss = -10872.76220376988
Iteration 5200: Loss = -10872.76012409482
Iteration 5300: Loss = -10872.750318355756
Iteration 5400: Loss = -10872.737321832392
Iteration 5500: Loss = -10872.70223321548
Iteration 5600: Loss = -10872.298283098828
Iteration 5700: Loss = -10857.684785140711
Iteration 5800: Loss = -10845.370125729121
Iteration 5900: Loss = -10839.057504145982
Iteration 6000: Loss = -10838.965434044205
Iteration 6100: Loss = -10838.899471039173
Iteration 6200: Loss = -10838.895626022635
Iteration 6300: Loss = -10830.68268753669
Iteration 6400: Loss = -10830.269188074304
Iteration 6500: Loss = -10830.266169364759
Iteration 6600: Loss = -10830.254428488142
Iteration 6700: Loss = -10830.2422580489
Iteration 6800: Loss = -10830.246742026733
1
Iteration 6900: Loss = -10830.24085608081
Iteration 7000: Loss = -10830.239823703438
Iteration 7100: Loss = -10826.786328430038
Iteration 7200: Loss = -10826.676727640905
Iteration 7300: Loss = -10826.676837526778
1
Iteration 7400: Loss = -10826.67733501339
2
Iteration 7500: Loss = -10826.67706845484
3
Iteration 7600: Loss = -10826.676141431373
Iteration 7700: Loss = -10826.676303689077
1
Iteration 7800: Loss = -10826.675322437606
Iteration 7900: Loss = -10826.674718646278
Iteration 8000: Loss = -10826.677312836882
1
Iteration 8100: Loss = -10826.675157650094
2
Iteration 8200: Loss = -10826.67821798038
3
Iteration 8300: Loss = -10826.67426309278
Iteration 8400: Loss = -10826.676913041856
1
Iteration 8500: Loss = -10826.675172435358
2
Iteration 8600: Loss = -10826.694092981374
3
Iteration 8700: Loss = -10826.675083691309
4
Iteration 8800: Loss = -10826.703323105385
5
Iteration 8900: Loss = -10826.679458567802
6
Iteration 9000: Loss = -10826.69021586089
7
Iteration 9100: Loss = -10826.67315406146
Iteration 9200: Loss = -10826.680978707953
1
Iteration 9300: Loss = -10826.677239177207
2
Iteration 9400: Loss = -10826.673341684334
3
Iteration 9500: Loss = -10826.673792863196
4
Iteration 9600: Loss = -10826.675996723694
5
Iteration 9700: Loss = -10826.697274556613
6
Iteration 9800: Loss = -10826.69203316521
7
Iteration 9900: Loss = -10826.677149257774
8
Iteration 10000: Loss = -10826.671763051794
Iteration 10100: Loss = -10826.676169846016
1
Iteration 10200: Loss = -10826.672861521141
2
Iteration 10300: Loss = -10826.671092324717
Iteration 10400: Loss = -10826.670812326096
Iteration 10500: Loss = -10826.673315093556
1
Iteration 10600: Loss = -10826.66416955752
Iteration 10700: Loss = -10826.662595414042
Iteration 10800: Loss = -10826.662593926781
Iteration 10900: Loss = -10826.666202351373
1
Iteration 11000: Loss = -10826.65590553634
Iteration 11100: Loss = -10826.655998075561
1
Iteration 11200: Loss = -10826.878271859254
2
Iteration 11300: Loss = -10826.655648760141
Iteration 11400: Loss = -10826.659281009086
1
Iteration 11500: Loss = -10826.747339561864
2
Iteration 11600: Loss = -10826.689701985435
3
Iteration 11700: Loss = -10826.657803345095
4
Iteration 11800: Loss = -10826.656024831287
5
Iteration 11900: Loss = -10826.656290735718
6
Iteration 12000: Loss = -10826.65761343494
7
Iteration 12100: Loss = -10826.655773041
8
Iteration 12200: Loss = -10826.673460846945
9
Iteration 12300: Loss = -10826.656024787506
10
Stopping early at iteration 12300 due to no improvement.
tensor([[ 4.3505, -6.7717],
        [ 1.1681, -2.6064],
        [-2.2101,  0.6388],
        [ 2.9055, -4.9083],
        [ 4.3145, -5.7052],
        [-4.8438,  2.2619],
        [-0.7220, -0.6656],
        [-3.4769,  2.0168],
        [ 2.5468, -4.0158],
        [-6.1679,  4.4046],
        [-4.4128,  2.8017],
        [-0.2221, -2.1660],
        [ 1.0399, -2.7688],
        [ 3.9358, -5.8670],
        [ 3.7814, -5.6998],
        [-4.0170,  2.6263],
        [ 1.9587, -3.3489],
        [-6.5148,  4.5319],
        [ 0.6387, -2.2800],
        [ 2.7389, -5.1427],
        [-4.4236,  2.1664],
        [ 5.7338, -7.5324],
        [-7.6960,  6.3096],
        [-4.6709,  3.2815],
        [ 4.7947, -6.8020],
        [-4.3795,  2.8244],
        [-6.6186,  5.2022],
        [ 1.0795, -2.7682],
        [ 3.3830, -4.9001],
        [-3.1659,  1.7777],
        [ 4.9668, -7.6105],
        [ 3.0045, -4.6422],
        [-2.8090,  1.3313],
        [-1.0653, -0.4977],
        [ 3.4056, -6.1918],
        [-3.8503,  2.4493],
        [-1.2631, -0.2357],
        [ 3.3565, -5.0772],
        [ 4.9623, -6.4186],
        [ 3.4747, -4.8659],
        [ 0.4146, -2.2757],
        [-4.5582,  3.1718],
        [-1.3875, -0.0871],
        [-3.1073,  0.4792],
        [ 3.3482, -4.7362],
        [-3.9903,  2.5313],
        [ 3.7275, -6.6330],
        [-1.8397,  0.4190],
        [-1.4172,  0.0260],
        [ 2.4439, -6.7427],
        [ 2.9636, -4.3606],
        [ 2.4410, -3.8950],
        [-5.4302,  3.9936],
        [-0.7909, -0.7448],
        [-3.9859,  2.3024],
        [ 0.6269, -2.0204],
        [-4.3309,  2.5211],
        [ 3.7344, -5.1215],
        [-2.9646, -0.6122],
        [ 1.7030, -3.5826],
        [-4.1819,  2.4637],
        [-3.9267,  2.5194],
        [ 2.6951, -4.1139],
        [-2.9086,  1.0830],
        [-6.6439,  5.1419],
        [ 4.0619, -6.2598],
        [-2.2780,  0.5409],
        [-5.4104,  3.4776],
        [ 4.1799, -5.9622],
        [-2.2561,  0.5492],
        [ 2.6461, -4.2508],
        [ 4.2497, -5.6463],
        [ 1.7403, -3.3333],
        [ 0.2218, -1.7387],
        [ 2.1092, -3.5331],
        [-1.5166,  0.0636],
        [ 4.5627, -5.9631],
        [ 2.7122, -4.4315],
        [-5.9914,  3.6167],
        [ 3.7092, -5.5824],
        [-2.7235,  1.0094],
        [-5.2018,  3.7560],
        [ 3.6026, -5.2234],
        [ 1.2775, -3.4872],
        [ 0.0268, -2.0813],
        [ 3.8170, -5.2496],
        [ 2.0254, -4.1872],
        [-5.8973,  2.2091],
        [ 0.5266, -2.1207],
        [-3.7425,  1.0045],
        [-1.4211, -2.0806],
        [ 3.0491, -4.4410],
        [-5.4502,  3.4832],
        [-6.9570,  5.5043],
        [ 3.0179, -5.1564],
        [ 1.1102, -2.9395],
        [ 1.8335, -5.0101],
        [ 0.3121, -1.7816],
        [-5.6051,  3.3618],
        [ 1.3789, -5.5705]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7606, 0.2394],
        [0.2016, 0.7984]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5645, 0.4355], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2034, 0.0993],
         [0.3672, 0.2500]],

        [[0.4135, 0.1001],
         [0.1647, 0.9859]],

        [[0.2812, 0.1063],
         [0.4545, 0.7062]],

        [[0.0521, 0.0913],
         [0.4964, 0.1364]],

        [[0.0683, 0.0979],
         [0.0619, 0.2145]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 1
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7721141809334062
time is 2
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448420005390695
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
Global Adjusted Rand Index: 0.8534825503276663
Average Adjusted Rand Index: 0.853329971961179
Iteration 0: Loss = -31697.091260675832
Iteration 10: Loss = -10976.103095895629
Iteration 20: Loss = -10976.103095895629
1
Iteration 30: Loss = -10976.103095895656
2
Iteration 40: Loss = -10976.103095896138
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.0000e+00, 2.0299e-14],
        [1.0000e+00, 2.9673e-28]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 1.9205e-14])
beta: tensor([[[0.1607, 0.1402],
         [0.0530, 0.2920]],

        [[0.3059, 0.2613],
         [0.0965, 0.5270]],

        [[0.5401, 0.2229],
         [0.4132, 0.2350]],

        [[0.0857, 0.2005],
         [0.3003, 0.5224]],

        [[0.4062, 0.2339],
         [0.5324, 0.3234]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31698.015807472057
Iteration 100: Loss = -11014.445354194137
Iteration 200: Loss = -10990.892637343515
Iteration 300: Loss = -10982.849436396053
Iteration 400: Loss = -10978.780302490546
Iteration 500: Loss = -10977.922088901532
Iteration 600: Loss = -10977.406655494056
Iteration 700: Loss = -10977.060765305527
Iteration 800: Loss = -10976.810618046966
Iteration 900: Loss = -10976.612124689442
Iteration 1000: Loss = -10976.454793866897
Iteration 1100: Loss = -10976.31734898809
Iteration 1200: Loss = -10976.191482802775
Iteration 1300: Loss = -10976.106001535882
Iteration 1400: Loss = -10976.040594104563
Iteration 1500: Loss = -10975.983949391162
Iteration 1600: Loss = -10975.94129210891
Iteration 1700: Loss = -10975.907285344723
Iteration 1800: Loss = -10975.853418613846
Iteration 1900: Loss = -10975.726314306114
Iteration 2000: Loss = -10975.589275106426
Iteration 2100: Loss = -10975.473961108102
Iteration 2200: Loss = -10975.351588294892
Iteration 2300: Loss = -10975.210011675134
Iteration 2400: Loss = -10975.066514372016
Iteration 2500: Loss = -10974.91730729682
Iteration 2600: Loss = -10974.745943797723
Iteration 2700: Loss = -10974.56126396269
Iteration 2800: Loss = -10974.385563121075
Iteration 2900: Loss = -10974.231912909732
Iteration 3000: Loss = -10974.0954815986
Iteration 3100: Loss = -10973.966837315267
Iteration 3200: Loss = -10973.84269930241
Iteration 3300: Loss = -10973.725414786326
Iteration 3400: Loss = -10973.623498031475
Iteration 3500: Loss = -10973.53565556713
Iteration 3600: Loss = -10973.45641723209
Iteration 3700: Loss = -10973.38147758672
Iteration 3800: Loss = -10973.307798684684
Iteration 3900: Loss = -10973.244236009823
Iteration 4000: Loss = -10973.195750833625
Iteration 4100: Loss = -10973.15380775004
Iteration 4200: Loss = -10973.115892249554
Iteration 4300: Loss = -10973.083928053202
Iteration 4400: Loss = -10973.056699468778
Iteration 4500: Loss = -10973.033366603153
Iteration 4600: Loss = -10973.010747332677
Iteration 4700: Loss = -10972.99162178053
Iteration 4800: Loss = -10972.972886012098
Iteration 4900: Loss = -10972.95512838391
Iteration 5000: Loss = -10972.940815221278
Iteration 5100: Loss = -10972.928063822397
Iteration 5200: Loss = -10972.915178479212
Iteration 5300: Loss = -10972.903333814762
Iteration 5400: Loss = -10972.892044094828
Iteration 5500: Loss = -10972.88123360764
Iteration 5600: Loss = -10972.869064525747
Iteration 5700: Loss = -10972.85960143518
Iteration 5800: Loss = -10972.857657239258
Iteration 5900: Loss = -10972.846362454842
Iteration 6000: Loss = -10972.841970741612
Iteration 6100: Loss = -10972.83859547771
Iteration 6200: Loss = -10972.835705510868
Iteration 6300: Loss = -10972.833225960232
Iteration 6400: Loss = -10972.833079911012
Iteration 6500: Loss = -10972.827326328399
Iteration 6600: Loss = -10972.828350997326
1
Iteration 6700: Loss = -10972.821509975265
Iteration 6800: Loss = -10972.82211804058
1
Iteration 6900: Loss = -10972.820085944426
Iteration 7000: Loss = -10972.821510237074
1
Iteration 7100: Loss = -10972.822466236275
2
Iteration 7200: Loss = -10972.817808846748
Iteration 7300: Loss = -10972.81728821012
Iteration 7400: Loss = -10972.815224060065
Iteration 7500: Loss = -10972.818446622545
1
Iteration 7600: Loss = -10972.822925692284
2
Iteration 7700: Loss = -10972.812568200929
Iteration 7800: Loss = -10972.810529680259
Iteration 7900: Loss = -10972.8103207115
Iteration 8000: Loss = -10972.81781384184
1
Iteration 8100: Loss = -10972.80930283998
Iteration 8200: Loss = -10972.809274259807
Iteration 8300: Loss = -10972.809536588145
1
Iteration 8400: Loss = -10972.81031652476
2
Iteration 8500: Loss = -10972.812840517168
3
Iteration 8600: Loss = -10972.810307569363
4
Iteration 8700: Loss = -10972.815726362347
5
Iteration 8800: Loss = -10972.804153424822
Iteration 8900: Loss = -10972.804267658355
1
Iteration 9000: Loss = -10972.805086128436
2
Iteration 9100: Loss = -10972.803605144738
Iteration 9200: Loss = -10972.804384696952
1
Iteration 9300: Loss = -10972.85726437166
2
Iteration 9400: Loss = -10972.818494653755
3
Iteration 9500: Loss = -10972.810912186418
4
Iteration 9600: Loss = -10972.94794242931
5
Iteration 9700: Loss = -10972.79682014639
Iteration 9800: Loss = -10972.802560547325
1
Iteration 9900: Loss = -10972.891795579952
2
Iteration 10000: Loss = -10972.798685124635
3
Iteration 10100: Loss = -10972.777957209284
Iteration 10200: Loss = -10972.843075159348
1
Iteration 10300: Loss = -10972.774169835728
Iteration 10400: Loss = -10972.773221614343
Iteration 10500: Loss = -10972.77230857621
Iteration 10600: Loss = -10972.772201834303
Iteration 10700: Loss = -10972.772710237803
1
Iteration 10800: Loss = -10972.752823116078
Iteration 10900: Loss = -10837.861342389011
Iteration 11000: Loss = -10827.626865487224
Iteration 11100: Loss = -10827.312282979758
Iteration 11200: Loss = -10827.305619668105
Iteration 11300: Loss = -10827.144287921508
Iteration 11400: Loss = -10827.116243847671
Iteration 11500: Loss = -10826.746920414438
Iteration 11600: Loss = -10826.742541038791
Iteration 11700: Loss = -10826.738138283301
Iteration 11800: Loss = -10826.73323522544
Iteration 11900: Loss = -10826.731590718473
Iteration 12000: Loss = -10826.714539843777
Iteration 12100: Loss = -10826.727615607999
1
Iteration 12200: Loss = -10826.759195616063
2
Iteration 12300: Loss = -10826.716904804736
3
Iteration 12400: Loss = -10826.729932646525
4
Iteration 12500: Loss = -10826.705331042005
Iteration 12600: Loss = -10826.682609461444
Iteration 12700: Loss = -10826.676618540025
Iteration 12800: Loss = -10826.673828304827
Iteration 12900: Loss = -10826.667442442269
Iteration 13000: Loss = -10826.884624667546
1
Iteration 13100: Loss = -10826.66623871791
Iteration 13200: Loss = -10826.670890960153
1
Iteration 13300: Loss = -10826.671545040586
2
Iteration 13400: Loss = -10826.662388082443
Iteration 13500: Loss = -10826.65914791116
Iteration 13600: Loss = -10826.659003590737
Iteration 13700: Loss = -10826.659710749436
1
Iteration 13800: Loss = -10826.689578929212
2
Iteration 13900: Loss = -10826.65686096444
Iteration 14000: Loss = -10826.67960275618
1
Iteration 14100: Loss = -10826.656323796307
Iteration 14200: Loss = -10826.657466090206
1
Iteration 14300: Loss = -10826.655596565302
Iteration 14400: Loss = -10826.654830707725
Iteration 14500: Loss = -10826.667990708862
1
Iteration 14600: Loss = -10826.652274158694
Iteration 14700: Loss = -10826.652367448707
1
Iteration 14800: Loss = -10826.680703723856
2
Iteration 14900: Loss = -10826.650938430454
Iteration 15000: Loss = -10826.652093264076
1
Iteration 15100: Loss = -10826.664524219254
2
Iteration 15200: Loss = -10826.648927204591
Iteration 15300: Loss = -10826.652690282608
1
Iteration 15400: Loss = -10826.650068347859
2
Iteration 15500: Loss = -10826.64657314464
Iteration 15600: Loss = -10826.646116137821
Iteration 15700: Loss = -10826.645782201052
Iteration 15800: Loss = -10826.646348411332
1
Iteration 15900: Loss = -10826.657982662502
2
Iteration 16000: Loss = -10826.645833917159
3
Iteration 16100: Loss = -10826.649574481224
4
Iteration 16200: Loss = -10826.653110584764
5
Iteration 16300: Loss = -10826.652075011445
6
Iteration 16400: Loss = -10826.650071265303
7
Iteration 16500: Loss = -10826.646486283456
8
Iteration 16600: Loss = -10826.696988618365
9
Iteration 16700: Loss = -10826.664950750837
10
Stopping early at iteration 16700 due to no improvement.
tensor([[ 4.0132, -7.1420],
        [ 1.0277, -2.7476],
        [-2.6803,  0.1687],
        [ 3.2040, -4.6102],
        [ 3.5719, -6.4477],
        [-4.3701,  2.7355],
        [-0.8351, -0.7748],
        [-3.7025,  1.7976],
        [ 2.3908, -4.1726],
        [-6.1110,  4.4625],
        [-4.3140,  2.9008],
        [-0.0100, -1.9559],
        [ 1.0976, -2.7092],
        [ 4.1822, -5.6236],
        [ 4.0317, -5.4505],
        [-4.4114,  2.2553],
        [ 1.3356, -3.9723],
        [-9.0766,  5.2876],
        [ 0.7015, -2.2185],
        [ 3.1746, -4.7081],
        [-4.0782,  2.5120],
        [ 5.8818, -7.6448],
        [-5.4314,  4.0385],
        [-5.1337,  2.8189],
        [ 4.9414, -6.7370],
        [-4.3476,  2.8601],
        [-7.8475,  4.4885],
        [ 1.1006, -2.7497],
        [ 2.4566, -5.8259],
        [-3.2196,  1.7281],
        [ 5.9948, -7.3817],
        [ 2.3538, -5.2926],
        [-4.0419,  0.1009],
        [-1.0015, -0.4414],
        [ 4.0363, -5.5603],
        [-3.8474,  2.4520],
        [-1.2121, -0.1855],
        [ 5.0103, -6.5447],
        [ 4.8946, -6.4869],
        [ 3.4602, -4.8801],
        [-0.1572, -2.8496],
        [-4.5668,  3.1637],
        [-1.3426, -0.0439],
        [-2.6584,  0.9321],
        [ 3.1076, -4.9770],
        [-4.3618,  2.1631],
        [ 4.2961, -6.0668],
        [-1.8506,  0.4057],
        [-1.4163,  0.0254],
        [ 3.8211, -5.3663],
        [ 2.3783, -4.9461],
        [ 2.4751, -3.8614],
        [-7.6161,  6.2251],
        [-0.8604, -0.8159],
        [-4.1880,  2.1026],
        [ 0.6136, -2.0298],
        [-7.0544,  4.5827],
        [ 3.6514, -5.2030],
        [-1.9143,  0.4425],
        [ 1.6240, -3.6591],
        [-4.1510,  2.4946],
        [-4.2828,  2.1661],
        [ 2.6434, -4.1656],
        [-2.7675,  1.2266],
        [-6.6352,  5.2132],
        [ 3.4334, -6.8790],
        [-2.2959,  0.5218],
        [-5.3193,  3.5908],
        [ 4.3672, -5.7765],
        [-2.1307,  0.6811],
        [ 2.3723, -4.5233],
        [ 4.0081, -5.8890],
        [ 1.4295, -3.6444],
        [-0.2447, -2.2073],
        [ 1.9931, -3.6485],
        [-1.5332,  0.0509],
        [ 4.3778, -6.1457],
        [ 2.0797, -5.0638],
        [-6.9940,  5.1631],
        [ 3.8037, -5.4881],
        [-4.1033, -0.3710],
        [-5.2877,  3.6736],
        [ 3.6113, -5.2134],
        [ 1.2604, -3.5023],
        [ 0.0702, -2.0413],
        [ 3.0144, -6.0532],
        [ 2.4053, -3.8059],
        [-5.8266,  2.2799],
        [ 0.5424, -2.1095],
        [-3.1713,  1.5800],
        [-0.4830, -1.1401],
        [ 3.0497, -4.4395],
        [-7.2524,  5.8004],
        [-7.1304,  5.6539],
        [ 3.3542, -4.8210],
        [ 0.5049, -3.5471],
        [ 2.7076, -4.1369],
        [ 0.1753, -1.9213],
        [-5.3247,  3.6421],
        [ 2.7242, -4.2238]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7635, 0.2365],
        [0.2000, 0.8000]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5641, 0.4359], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2032, 0.0996],
         [0.0530, 0.2499]],

        [[0.3059, 0.1001],
         [0.0965, 0.5270]],

        [[0.5401, 0.1072],
         [0.4132, 0.2350]],

        [[0.0857, 0.0914],
         [0.3003, 0.5224]],

        [[0.4062, 0.0980],
         [0.5324, 0.3234]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 1
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7721141809334062
time is 2
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448420005390695
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
Global Adjusted Rand Index: 0.8534825503276663
Average Adjusted Rand Index: 0.853329971961179
Iteration 0: Loss = -32485.739291623584
Iteration 10: Loss = -10975.666581999223
Iteration 20: Loss = -10975.666467795829
Iteration 30: Loss = -10975.6664466881
Iteration 40: Loss = -10975.666455921659
1
Iteration 50: Loss = -10975.666496711121
2
Iteration 60: Loss = -10975.666458885797
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.1782, 0.8218],
        [0.5643, 0.4357]], dtype=torch.float64)
alpha: tensor([0.4071, 0.5929])
beta: tensor([[[0.1616, 0.1568],
         [0.7186, 0.1600]],

        [[0.1367, 0.1580],
         [0.5241, 0.8638]],

        [[0.6355, 0.1624],
         [0.4015, 0.3452]],

        [[0.6462, 0.1611],
         [0.2693, 0.5551]],

        [[0.5844, 0.1657],
         [0.4070, 0.2223]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -32485.366510880547
Iteration 100: Loss = -11026.370752420335
Iteration 200: Loss = -10942.132011490921
Iteration 300: Loss = -10897.666695768665
Iteration 400: Loss = -10896.536469714918
Iteration 500: Loss = -10896.313551078401
Iteration 600: Loss = -10896.151515818707
Iteration 700: Loss = -10889.257702557472
Iteration 800: Loss = -10889.190467018108
Iteration 900: Loss = -10889.164856758172
Iteration 1000: Loss = -10889.148187558503
Iteration 1100: Loss = -10889.135620195724
Iteration 1200: Loss = -10889.1255999656
Iteration 1300: Loss = -10889.116956956994
Iteration 1400: Loss = -10889.108321736654
Iteration 1500: Loss = -10889.0940520129
Iteration 1600: Loss = -10889.06075248797
Iteration 1700: Loss = -10889.03258630228
Iteration 1800: Loss = -10889.001334327559
Iteration 1900: Loss = -10888.993543438595
Iteration 2000: Loss = -10888.985259390876
Iteration 2100: Loss = -10888.952059353673
Iteration 2200: Loss = -10888.93981704103
Iteration 2300: Loss = -10888.938267597961
Iteration 2400: Loss = -10888.936997135232
Iteration 2500: Loss = -10888.935831491499
Iteration 2600: Loss = -10888.934798257527
Iteration 2700: Loss = -10888.933758972515
Iteration 2800: Loss = -10888.93253222697
Iteration 2900: Loss = -10888.929817679473
Iteration 3000: Loss = -10888.923544564394
Iteration 3100: Loss = -10888.922836975367
Iteration 3200: Loss = -10888.926381362466
1
Iteration 3300: Loss = -10888.922177762157
Iteration 3400: Loss = -10888.922650501312
1
Iteration 3500: Loss = -10888.921492658119
Iteration 3600: Loss = -10888.921239506444
Iteration 3700: Loss = -10888.920996981342
Iteration 3800: Loss = -10888.921327541479
1
Iteration 3900: Loss = -10888.920464043636
Iteration 4000: Loss = -10888.920098496048
Iteration 4100: Loss = -10888.919618916343
Iteration 4200: Loss = -10888.916839781883
Iteration 4300: Loss = -10888.91018463504
Iteration 4400: Loss = -10888.91234835375
1
Iteration 4500: Loss = -10888.909477269262
Iteration 4600: Loss = -10888.90911152697
Iteration 4700: Loss = -10888.924801511732
1
Iteration 4800: Loss = -10888.908575892869
Iteration 4900: Loss = -10888.908359229843
Iteration 5000: Loss = -10888.90874657582
1
Iteration 5100: Loss = -10888.907941836487
Iteration 5200: Loss = -10888.907758248512
Iteration 5300: Loss = -10888.907779618758
1
Iteration 5400: Loss = -10888.907531323217
Iteration 5500: Loss = -10888.907272415094
Iteration 5600: Loss = -10888.905581182184
Iteration 5700: Loss = -10888.899956520638
Iteration 5800: Loss = -10888.899834663383
Iteration 5900: Loss = -10888.901246620317
1
Iteration 6000: Loss = -10888.899417736246
Iteration 6100: Loss = -10888.898603636018
Iteration 6200: Loss = -10888.897783546448
Iteration 6300: Loss = -10888.897610747763
Iteration 6400: Loss = -10888.908909612877
1
Iteration 6500: Loss = -10888.905620480491
2
Iteration 6600: Loss = -10888.901747597592
3
Iteration 6700: Loss = -10888.896602954013
Iteration 6800: Loss = -10888.896589242278
Iteration 6900: Loss = -10888.903152645902
1
Iteration 7000: Loss = -10888.900665885938
2
Iteration 7100: Loss = -10888.91146735496
3
Iteration 7200: Loss = -10888.903046683623
4
Iteration 7300: Loss = -10888.89732342683
5
Iteration 7400: Loss = -10888.902435178603
6
Iteration 7500: Loss = -10888.897483598756
7
Iteration 7600: Loss = -10888.896670512406
8
Iteration 7700: Loss = -10888.897590181767
9
Iteration 7800: Loss = -10888.897969705431
10
Stopping early at iteration 7800 due to no improvement.
tensor([[-4.8736,  3.3986],
        [-2.5602, -1.0873],
        [ 0.5165, -2.1308],
        [-4.1480,  2.4208],
        [-4.9929,  3.5795],
        [ 4.6290, -6.1122],
        [-0.2629, -2.3887],
        [ 0.9863, -3.9378],
        [-2.7689,  1.3801],
        [ 4.5256, -6.2445],
        [ 2.2737, -3.9087],
        [-1.9877,  0.5236],
        [-1.8954,  0.4761],
        [-4.7459,  3.2072],
        [-4.5737,  2.2431],
        [ 2.1048, -3.6392],
        [-2.3573,  0.8195],
        [ 2.8237, -7.4389],
        [-1.2389, -0.1754],
        [-3.8032,  2.0503],
        [ 2.5490, -3.9366],
        [-6.3058,  4.8131],
        [ 3.4257, -4.9650],
        [ 3.5418, -5.1331],
        [-5.7727,  4.2466],
        [ 2.2922, -4.7226],
        [ 4.9090, -6.3854],
        [-3.0402,  1.6340],
        [-4.0116,  2.5534],
        [ 2.7120, -4.2367],
        [-6.5645,  5.1093],
        [-4.3344,  2.6070],
        [ 1.4982, -3.0805],
        [-0.6218, -1.2127],
        [-5.4021,  3.9782],
        [ 1.6841, -3.4625],
        [-1.2011, -0.4317],
        [-4.3625,  2.6785],
        [-7.3030,  5.2567],
        [-5.1807,  1.8979],
        [-2.2441,  0.8179],
        [ 2.7246, -4.4936],
        [ 0.4253, -1.9683],
        [ 1.2418, -2.9959],
        [-4.2701,  1.9013],
        [ 3.6324, -5.0941],
        [-5.3197,  3.8216],
        [-0.2760, -1.9210],
        [-0.2880, -1.1011],
        [-3.9201,  2.5056],
        [-3.5427,  1.6808],
        [-3.9389, -0.3919],
        [ 2.1848, -5.4752],
        [-1.3101, -0.5914],
        [ 3.9481, -5.3835],
        [-0.9986, -0.3918],
        [ 2.2112, -3.9985],
        [-4.3403,  2.9318],
        [ 1.2306, -2.6358],
        [-3.5897,  1.0653],
        [ 1.6492, -5.1329],
        [ 0.7004, -4.9408],
        [-3.1532,  1.7669],
        [ 0.8879, -2.2747],
        [ 5.7448, -7.2016],
        [-4.8493,  2.6808],
        [ 0.4753, -2.0446],
        [ 3.4052, -4.7923],
        [-4.9468,  3.3645],
        [ 0.9007, -2.9227],
        [-4.8002,  3.3530],
        [-4.9433,  3.5397],
        [-2.3187,  0.7294],
        [-2.9189,  0.7480],
        [-4.1756,  1.0193],
        [ 0.6941, -3.3029],
        [-6.5587,  2.8515],
        [-3.4151,  1.6653],
        [ 2.9774, -7.5926],
        [-4.3800,  2.9839],
        [ 0.7866, -2.3772],
        [ 2.6838, -4.1007],
        [-4.2528,  2.7185],
        [-2.2889,  0.9017],
        [-2.5902,  0.8987],
        [-3.8572,  1.6439],
        [-3.4198,  1.9685],
        [ 2.5942, -5.2548],
        [-2.6134,  0.8425],
        [ 0.6389, -2.0270],
        [-0.0952, -1.9858],
        [-4.2670,  1.3701],
        [ 5.0070, -6.3933],
        [ 4.6272, -6.5122],
        [-3.9326,  2.4997],
        [-3.6144,  2.0023],
        [-3.9552,  1.3930],
        [-3.1861,  0.7751],
        [ 2.7649, -5.1566],
        [-3.3321,  1.5495]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4458, 0.5542],
        [0.4830, 0.5170]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4449, 0.5551], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2358, 0.0987],
         [0.7186, 0.2179]],

        [[0.1367, 0.0992],
         [0.5241, 0.8638]],

        [[0.6355, 0.0983],
         [0.4015, 0.3452]],

        [[0.6462, 0.0914],
         [0.2693, 0.5551]],

        [[0.5844, 0.0966],
         [0.4070, 0.2223]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 8
Adjusted Rand Index: 0.7026007738649165
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 81
Adjusted Rand Index: 0.37855677651733366
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 94
Adjusted Rand Index: 0.7721141809334062
time is 4
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824191749678101
Global Adjusted Rand Index: 0.061635478920690295
Average Adjusted Rand Index: 0.7391381812566933
Iteration 0: Loss = -22411.853860202027
Iteration 10: Loss = -10974.671353347474
Iteration 20: Loss = -10973.948466844184
Iteration 30: Loss = -10973.920634422784
Iteration 40: Loss = -10973.950192102058
1
Iteration 50: Loss = -10973.982370640375
2
Iteration 60: Loss = -10974.014349342373
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.2589, 0.7411],
        [0.2332, 0.7668]], dtype=torch.float64)
alpha: tensor([0.2362, 0.7638])
beta: tensor([[[0.2101, 0.1661],
         [0.2822, 0.1473]],

        [[0.6455, 0.1757],
         [0.8438, 0.0817]],

        [[0.3376, 0.1751],
         [0.2265, 0.1268]],

        [[0.1825, 0.1749],
         [0.4250, 0.0957]],

        [[0.7203, 0.1812],
         [0.2040, 0.5476]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.003422492374587702
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.015076275178066338
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.006214218236452756
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.016623577451856865
Global Adjusted Rand Index: 0.0047637962924011405
Average Adjusted Rand Index: -0.0004783880096578215
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22411.65397954866
Iteration 100: Loss = -11048.012759788338
Iteration 200: Loss = -11007.203387407535
Iteration 300: Loss = -10994.287547268506
Iteration 400: Loss = -10978.79004406897
Iteration 500: Loss = -10977.58914268991
Iteration 600: Loss = -10977.065175347248
Iteration 700: Loss = -10976.759143228366
Iteration 800: Loss = -10976.556670513672
Iteration 900: Loss = -10976.417955926589
Iteration 1000: Loss = -10976.315660246075
Iteration 1100: Loss = -10976.236956199014
Iteration 1200: Loss = -10976.17366174537
Iteration 1300: Loss = -10976.118392994716
Iteration 1400: Loss = -10976.070084551806
Iteration 1500: Loss = -10976.02855429488
Iteration 1600: Loss = -10975.98903980452
Iteration 1700: Loss = -10975.950381326622
Iteration 1800: Loss = -10975.915533607635
Iteration 1900: Loss = -10975.886579557588
Iteration 2000: Loss = -10975.838756379639
Iteration 2100: Loss = -10975.814555560024
Iteration 2200: Loss = -10975.766691495071
Iteration 2300: Loss = -10975.712493264951
Iteration 2400: Loss = -10975.685726295402
Iteration 2500: Loss = -10975.58428804558
Iteration 2600: Loss = -10975.384913345886
Iteration 2700: Loss = -10975.36013456239
Iteration 2800: Loss = -10975.340499490967
Iteration 2900: Loss = -10975.325381265773
Iteration 3000: Loss = -10975.29492248001
Iteration 3100: Loss = -10975.153730939248
Iteration 3200: Loss = -10975.145630700095
Iteration 3300: Loss = -10975.139556877213
Iteration 3400: Loss = -10975.134104020886
Iteration 3500: Loss = -10975.128376395196
Iteration 3600: Loss = -10975.120579416553
Iteration 3700: Loss = -10975.109209893291
Iteration 3800: Loss = -10975.095702748433
Iteration 3900: Loss = -10975.076489704692
Iteration 4000: Loss = -10975.046176917858
Iteration 4100: Loss = -10975.026548489428
Iteration 4200: Loss = -10975.011993214066
Iteration 4300: Loss = -10975.004218497677
Iteration 4400: Loss = -10974.995095516744
Iteration 4500: Loss = -10974.99016804111
Iteration 4600: Loss = -10974.962272191264
Iteration 4700: Loss = -10974.946880392707
Iteration 4800: Loss = -10974.876129122462
Iteration 4900: Loss = -10974.83548247494
Iteration 5000: Loss = -10974.819606084913
Iteration 5100: Loss = -10974.811426938993
Iteration 5200: Loss = -10974.78909615204
Iteration 5300: Loss = -10974.404681877259
Iteration 5400: Loss = -10974.371352887763
Iteration 5500: Loss = -10974.360078520784
Iteration 5600: Loss = -10974.354664882138
Iteration 5700: Loss = -10974.350718666938
Iteration 5800: Loss = -10974.330483746488
Iteration 5900: Loss = -10974.326894055745
Iteration 6000: Loss = -10974.32281700308
Iteration 6100: Loss = -10974.321991198794
Iteration 6200: Loss = -10974.321268234706
Iteration 6300: Loss = -10974.320550057288
Iteration 6400: Loss = -10974.32003058482
Iteration 6500: Loss = -10974.319463432797
Iteration 6600: Loss = -10974.319022904756
Iteration 6700: Loss = -10974.318666092686
Iteration 6800: Loss = -10974.318560133246
Iteration 6900: Loss = -10974.317884601463
Iteration 7000: Loss = -10974.316859995912
Iteration 7100: Loss = -10974.321454815496
1
Iteration 7200: Loss = -10974.310764674157
Iteration 7300: Loss = -10974.310575848094
Iteration 7400: Loss = -10974.333805676384
1
Iteration 7500: Loss = -10974.310190251164
Iteration 7600: Loss = -10974.30992127086
Iteration 7700: Loss = -10974.309737531556
Iteration 7800: Loss = -10974.309550040265
Iteration 7900: Loss = -10974.308466975059
Iteration 8000: Loss = -10974.306349917973
Iteration 8100: Loss = -10974.563329263361
1
Iteration 8200: Loss = -10974.305891862026
Iteration 8300: Loss = -10974.305658629006
Iteration 8400: Loss = -10974.305503890946
Iteration 8500: Loss = -10974.307101205037
1
Iteration 8600: Loss = -10974.305113030518
Iteration 8700: Loss = -10974.30497828911
Iteration 8800: Loss = -10974.304837772594
Iteration 8900: Loss = -10974.304614401235
Iteration 9000: Loss = -10974.303345117172
Iteration 9100: Loss = -10974.302233518885
Iteration 9200: Loss = -10974.307961432523
1
Iteration 9300: Loss = -10974.298652323117
Iteration 9400: Loss = -10974.297208720029
Iteration 9500: Loss = -10974.30098401258
1
Iteration 9600: Loss = -10974.296810129865
Iteration 9700: Loss = -10974.375558203614
1
Iteration 9800: Loss = -10974.29641649745
Iteration 9900: Loss = -10974.298161743418
1
Iteration 10000: Loss = -10974.29480253548
Iteration 10100: Loss = -10974.509552079731
1
Iteration 10200: Loss = -10974.257256185127
Iteration 10300: Loss = -10974.259272392941
1
Iteration 10400: Loss = -10974.256355402005
Iteration 10500: Loss = -10974.255977575198
Iteration 10600: Loss = -10974.255807545358
Iteration 10700: Loss = -10974.255649205881
Iteration 10800: Loss = -10974.456022635497
1
Iteration 10900: Loss = -10974.25538976964
Iteration 11000: Loss = -10974.267654004781
1
Iteration 11100: Loss = -10974.25520233283
Iteration 11200: Loss = -10974.25524425473
1
Iteration 11300: Loss = -10974.255007042097
Iteration 11400: Loss = -10974.260974697176
1
Iteration 11500: Loss = -10974.254887208985
Iteration 11600: Loss = -10974.275449139353
1
Iteration 11700: Loss = -10974.254747252382
Iteration 11800: Loss = -10974.254746149572
Iteration 11900: Loss = -10974.271027065752
1
Iteration 12000: Loss = -10974.254598737554
Iteration 12100: Loss = -10974.254651738345
1
Iteration 12200: Loss = -10974.25454864314
Iteration 12300: Loss = -10974.25460485211
1
Iteration 12400: Loss = -10974.257037926074
2
Iteration 12500: Loss = -10974.26007001858
3
Iteration 12600: Loss = -10974.254390111204
Iteration 12700: Loss = -10974.274390655268
1
Iteration 12800: Loss = -10974.254332951306
Iteration 12900: Loss = -10974.273333110257
1
Iteration 13000: Loss = -10974.254291683994
Iteration 13100: Loss = -10974.269118884013
1
Iteration 13200: Loss = -10974.254245179467
Iteration 13300: Loss = -10974.254247417024
1
Iteration 13400: Loss = -10974.25595488604
2
Iteration 13500: Loss = -10974.254179197527
Iteration 13600: Loss = -10974.254238290892
1
Iteration 13700: Loss = -10974.254194030756
2
Iteration 13800: Loss = -10974.254148499651
Iteration 13900: Loss = -10974.297195093584
1
Iteration 14000: Loss = -10974.25411279435
Iteration 14100: Loss = -10974.254100548083
Iteration 14200: Loss = -10974.265897071327
1
Iteration 14300: Loss = -10974.254096520885
Iteration 14400: Loss = -10974.254087124651
Iteration 14500: Loss = -10974.254046437452
Iteration 14600: Loss = -10974.254059453215
1
Iteration 14700: Loss = -10974.272211111786
2
Iteration 14800: Loss = -10974.254013364101
Iteration 14900: Loss = -10974.254722130972
1
Iteration 15000: Loss = -10974.254021947803
2
Iteration 15100: Loss = -10974.25715133681
3
Iteration 15200: Loss = -10974.254152592212
4
Iteration 15300: Loss = -10974.254196513337
5
Iteration 15400: Loss = -10974.254068371205
6
Iteration 15500: Loss = -10974.253956148617
Iteration 15600: Loss = -10974.254004467748
1
Iteration 15700: Loss = -10974.25406069376
2
Iteration 15800: Loss = -10974.2539819641
3
Iteration 15900: Loss = -10974.254560960082
4
Iteration 16000: Loss = -10974.254741131828
5
Iteration 16100: Loss = -10974.263139645082
6
Iteration 16200: Loss = -10974.255609045304
7
Iteration 16300: Loss = -10974.283340161146
8
Iteration 16400: Loss = -10974.253927847842
Iteration 16500: Loss = -10974.273633581368
1
Iteration 16600: Loss = -10974.285275765436
2
Iteration 16700: Loss = -10974.265713850804
3
Iteration 16800: Loss = -10974.253936905841
4
Iteration 16900: Loss = -10974.254018687836
5
Iteration 17000: Loss = -10974.253937335427
6
Iteration 17100: Loss = -10974.254067107098
7
Iteration 17200: Loss = -10974.255655278112
8
Iteration 17300: Loss = -10974.253937815247
9
Iteration 17400: Loss = -10974.255837124643
10
Stopping early at iteration 17400 due to no improvement.
tensor([[ -8.7143,   6.0496],
        [ -2.4732,   0.5382],
        [ -8.6409,   5.7270],
        [ -8.0822,   6.6286],
        [ -8.3832,   6.2743],
        [ -3.0243,   1.2889],
        [ -7.9555,   6.0016],
        [ -9.0410,   5.5040],
        [ -7.6525,   5.9121],
        [ -8.2414,   5.9806],
        [ -8.1204,   6.7168],
        [ -8.3123,   6.4450],
        [ -4.8670,   2.7068],
        [ -8.3472,   5.9040],
        [ -3.8467,   2.3530],
        [ -7.6792,   6.2875],
        [ -9.2267,   4.8039],
        [ -7.9077,   6.1011],
        [ -8.0532,   6.6669],
        [ -7.7505,   6.2885],
        [ -8.5159,   5.9272],
        [ -8.0850,   6.6284],
        [ -8.0208,   6.6338],
        [ -8.1957,   6.7941],
        [ -7.9552,   6.1304],
        [ -9.4381,   4.8229],
        [ -8.0885,   6.6966],
        [ -8.0219,   6.6297],
        [ -8.0844,   6.6981],
        [ -2.0017,   0.4974],
        [ -8.2004,   5.8306],
        [ -3.5271,   2.0837],
        [ -8.7218,   6.0519],
        [ -8.4572,   6.0831],
        [ -3.6538,  -0.9614],
        [ -0.4104,  -1.5665],
        [ -8.3461,   5.7767],
        [ -8.2987,   6.0260],
        [ -8.3732,   5.9675],
        [ -8.0727,   6.6200],
        [ -8.2358,   6.4283],
        [ -3.2193,   1.7862],
        [ -8.2638,   6.5682],
        [ -7.7275,   5.9040],
        [ -8.6641,   6.0338],
        [ -7.8415,   6.1499],
        [ -7.7594,   6.3269],
        [ -8.4075,   6.3869],
        [ -8.2976,   6.6835],
        [ -8.6080,   6.1932],
        [ -9.0644,   5.5927],
        [ -8.2133,   6.5773],
        [ -8.3835,   5.5956],
        [ -4.5622,   2.6952],
        [ -8.1823,   6.4844],
        [ -8.0377,   6.6216],
        [ -7.9534,   6.4686],
        [ -8.5308,   6.0685],
        [ -8.6677,   5.6663],
        [ -7.8779,   6.3727],
        [ -7.8266,   6.1328],
        [ -9.5319,   5.2183],
        [ -8.1200,   6.6676],
        [ -8.2786,   6.4606],
        [ -8.0519,   6.6418],
        [ -8.0771,   6.6277],
        [ -8.1425,   6.7487],
        [ -7.9732,   6.2666],
        [ -8.5215,   5.5352],
        [ -8.2856,   6.5677],
        [ -7.2095,   5.7551],
        [-10.2325,   8.8084],
        [ -8.4444,   6.2932],
        [ -7.8936,   6.4427],
        [  0.4101,  -2.2229],
        [ -8.2004,   6.7680],
        [ -8.6136,   5.8442],
        [ -3.3027,   1.7882],
        [ -8.0614,   6.6743],
        [ -8.5542,   6.2800],
        [ -8.8861,   4.9473],
        [ -8.9508,   4.5805],
        [ -3.2077,   1.8190],
        [ -7.9605,   6.5738],
        [ -8.2946,   6.4739],
        [ -8.3380,   5.9781],
        [ -7.8970,   6.1966],
        [ -9.4456,   4.8304],
        [ -2.7030,   0.7019],
        [ -7.8214,   6.3311],
        [ -7.9166,   6.1868],
        [ -8.3859,   5.9306],
        [ -8.0572,   6.2168],
        [ -8.0980,   6.6218],
        [ -7.0550,   5.6433],
        [ -7.4398,   5.8064],
        [ -2.3140,  -0.5178],
        [ -7.8761,   6.4575],
        [ -8.0729,   6.1180],
        [ -8.2390,   6.7392]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9998e-01, 2.1926e-05],
        [1.6061e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0209, 0.9791], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2842, 0.1665],
         [0.2822, 0.1620]],

        [[0.6455, 0.0916],
         [0.8438, 0.0817]],

        [[0.3376, 0.2444],
         [0.2265, 0.1268]],

        [[0.1825, 0.1682],
         [0.4250, 0.0957]],

        [[0.7203, 0.1816],
         [0.2040, 0.5476]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: -0.009009494263563287
Global Adjusted Rand Index: 0.0006122479046515223
Average Adjusted Rand Index: -0.0012058195686791605
10867.610104028518
new:  [0.8534825503276663, 0.8534825503276663, 0.061635478920690295, 0.0006122479046515223] [0.853329971961179, 0.853329971961179, 0.7391381812566933, -0.0012058195686791605] [10826.656024787506, 10826.664950750837, 10888.897969705431, 10974.255837124643]
prior:  [0.0013829266002793894, 0.0, 0.0, 0.0047637962924011405] [-0.0016528341894872366, 0.0, 0.0, -0.0004783880096578215] [10973.778607555421, 10976.103095896138, 10975.666458885797, 10974.014349342373]
-----------------------------------------------------------------------------------------
This iteration is 19
True Objective function: Loss = -10895.850256308191
Iteration 0: Loss = -15297.21589732225
Iteration 10: Loss = -11047.065661077757
Iteration 20: Loss = -11045.144527053273
Iteration 30: Loss = -11044.269996169245
Iteration 40: Loss = -11039.912291924467
Iteration 50: Loss = -10861.476586153593
Iteration 60: Loss = -10856.33599108815
Iteration 70: Loss = -10856.333445852857
Iteration 80: Loss = -10856.334077524667
1
Iteration 90: Loss = -10856.334178406441
2
Iteration 100: Loss = -10856.334241335531
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.7442, 0.2558],
        [0.3164, 0.6836]], dtype=torch.float64)
alpha: tensor([0.5533, 0.4467])
beta: tensor([[[0.2460, 0.0906],
         [0.1147, 0.2024]],

        [[0.1103, 0.0924],
         [0.6716, 0.7520]],

        [[0.6945, 0.0991],
         [0.3592, 0.1546]],

        [[0.1259, 0.0947],
         [0.8248, 0.4856]],

        [[0.0167, 0.0922],
         [0.2683, 0.2642]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448436976567872
time is 1
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8077222191419615
time is 3
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721426378272603
time is 4
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448326530612245
Global Adjusted Rand Index: 0.8460832335055567
Average Adjusted Rand Index: 0.8459078967414421
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -15736.412577946143
Iteration 100: Loss = -11054.767183261829
Iteration 200: Loss = -11052.544840533608
Iteration 300: Loss = -11051.802601807049
Iteration 400: Loss = -11051.454411274715
Iteration 500: Loss = -11051.272662672314
Iteration 600: Loss = -11051.181405345438
Iteration 700: Loss = -11051.13760214433
Iteration 800: Loss = -11051.114724436342
Iteration 900: Loss = -11051.100344156359
Iteration 1000: Loss = -11051.089297048302
Iteration 1100: Loss = -11051.079471933446
Iteration 1200: Loss = -11051.069647220116
Iteration 1300: Loss = -11051.058986616377
Iteration 1400: Loss = -11051.046487878733
Iteration 1500: Loss = -11051.02990991936
Iteration 1600: Loss = -11050.999108121707
Iteration 1700: Loss = -11050.765434972654
Iteration 1800: Loss = -11046.396562576276
Iteration 1900: Loss = -11046.173070837112
Iteration 2000: Loss = -11046.07701091206
Iteration 2100: Loss = -11045.848215080869
Iteration 2200: Loss = -11045.77910330473
Iteration 2300: Loss = -11045.76455595306
Iteration 2400: Loss = -11045.756568341247
Iteration 2500: Loss = -11045.750486313065
Iteration 2600: Loss = -11045.744887359619
Iteration 2700: Loss = -11045.738883610622
Iteration 2800: Loss = -11045.73425552722
Iteration 2900: Loss = -11045.731084092347
Iteration 3000: Loss = -11045.728008009972
Iteration 3100: Loss = -11045.72550342995
Iteration 3200: Loss = -11045.722875624879
Iteration 3300: Loss = -11045.719773606204
Iteration 3400: Loss = -11045.716137739091
Iteration 3500: Loss = -11045.712219178027
Iteration 3600: Loss = -11045.707371872171
Iteration 3700: Loss = -11045.701449043825
Iteration 3800: Loss = -11045.693628399174
Iteration 3900: Loss = -11045.682534330057
Iteration 4000: Loss = -11045.666039176986
Iteration 4100: Loss = -11045.641932403401
Iteration 4200: Loss = -11045.613445396226
Iteration 4300: Loss = -11045.588506854707
Iteration 4400: Loss = -11045.576184996633
Iteration 4500: Loss = -11045.57004618868
Iteration 4600: Loss = -11045.566245585058
Iteration 4700: Loss = -11045.564404832025
Iteration 4800: Loss = -11045.563242179229
Iteration 4900: Loss = -11045.562221679806
Iteration 5000: Loss = -11045.561000854212
Iteration 5100: Loss = -11045.560575715921
Iteration 5200: Loss = -11045.560346841086
Iteration 5300: Loss = -11045.560141603526
Iteration 5400: Loss = -11045.56004418599
Iteration 5500: Loss = -11045.559899055734
Iteration 5600: Loss = -11045.559804583663
Iteration 5700: Loss = -11045.559739852017
Iteration 5800: Loss = -11045.55968785131
Iteration 5900: Loss = -11045.559626163904
Iteration 6000: Loss = -11045.55958908848
Iteration 6100: Loss = -11045.559712506452
1
Iteration 6200: Loss = -11045.56799025947
2
Iteration 6300: Loss = -11045.559697450593
3
Iteration 6400: Loss = -11045.55950831581
Iteration 6500: Loss = -11045.632847839523
1
Iteration 6600: Loss = -11045.5594223411
Iteration 6700: Loss = -11045.652850587214
1
Iteration 6800: Loss = -11045.55938101754
Iteration 6900: Loss = -11045.55937748825
Iteration 7000: Loss = -11045.559874871109
1
Iteration 7100: Loss = -11045.559310552397
Iteration 7200: Loss = -11045.559294708528
Iteration 7300: Loss = -11045.583026081415
1
Iteration 7400: Loss = -11045.559222776981
Iteration 7500: Loss = -11045.562746503478
1
Iteration 7600: Loss = -11045.559208247527
Iteration 7700: Loss = -11045.559161615889
Iteration 7800: Loss = -11045.561775434899
1
Iteration 7900: Loss = -11045.559179564434
2
Iteration 8000: Loss = -11045.559155671071
Iteration 8100: Loss = -11045.559146305059
Iteration 8200: Loss = -11045.559159704175
1
Iteration 8300: Loss = -11045.693115840813
2
Iteration 8400: Loss = -11045.559090702014
Iteration 8500: Loss = -11045.559101206552
1
Iteration 8600: Loss = -11045.559152176202
2
Iteration 8700: Loss = -11045.560632159124
3
Iteration 8800: Loss = -11045.748357344533
4
Iteration 8900: Loss = -11045.559071434007
Iteration 9000: Loss = -11045.559104039643
1
Iteration 9100: Loss = -11045.578274445623
2
Iteration 9200: Loss = -11045.55907568986
3
Iteration 9300: Loss = -11045.55911335194
4
Iteration 9400: Loss = -11045.559150759686
5
Iteration 9500: Loss = -11045.559071185708
Iteration 9600: Loss = -11045.58887015412
1
Iteration 9700: Loss = -11045.55908378912
2
Iteration 9800: Loss = -11045.57590206198
3
Iteration 9900: Loss = -11045.55908223985
4
Iteration 10000: Loss = -11045.629380661849
5
Iteration 10100: Loss = -11045.55907271887
6
Iteration 10200: Loss = -11045.55911604011
7
Iteration 10300: Loss = -11045.559093424006
8
Iteration 10400: Loss = -11045.559063577222
Iteration 10500: Loss = -11045.571495420989
1
Iteration 10600: Loss = -11045.559073085624
2
Iteration 10700: Loss = -11045.559205998748
3
Iteration 10800: Loss = -11045.709232635518
4
Iteration 10900: Loss = -11045.559060446862
Iteration 11000: Loss = -11045.559048848605
Iteration 11100: Loss = -11045.559669815502
1
Iteration 11200: Loss = -11045.559081594847
2
Iteration 11300: Loss = -11045.560376418374
3
Iteration 11400: Loss = -11045.559052164097
4
Iteration 11500: Loss = -11045.559038870753
Iteration 11600: Loss = -11045.559123003095
1
Iteration 11700: Loss = -11045.559189122541
2
Iteration 11800: Loss = -11045.559061575506
3
Iteration 11900: Loss = -11045.559167860794
4
Iteration 12000: Loss = -11045.559109566853
5
Iteration 12100: Loss = -11045.67736336981
6
Iteration 12200: Loss = -11045.559032365589
Iteration 12300: Loss = -11045.718054229352
1
Iteration 12400: Loss = -11045.5590708764
2
Iteration 12500: Loss = -11045.559023287451
Iteration 12600: Loss = -11045.559409103533
1
Iteration 12700: Loss = -11045.559063511548
2
Iteration 12800: Loss = -11045.836058484696
3
Iteration 12900: Loss = -11045.559022038417
Iteration 13000: Loss = -11045.559052900835
1
Iteration 13100: Loss = -11045.575728378548
2
Iteration 13200: Loss = -11045.559055990216
3
Iteration 13300: Loss = -11045.562157292872
4
Iteration 13400: Loss = -11045.559389404687
5
Iteration 13500: Loss = -11045.561446309977
6
Iteration 13600: Loss = -11045.566301677
7
Iteration 13700: Loss = -11045.559057742701
8
Iteration 13800: Loss = -11045.560483472273
9
Iteration 13900: Loss = -11045.559139623045
10
Stopping early at iteration 13900 due to no improvement.
tensor([[-6.1733,  1.5581],
        [-7.7147,  3.0994],
        [-6.9669,  2.3517],
        [-6.8991,  2.2838],
        [-8.8381,  4.2229],
        [-7.7373,  3.1221],
        [-0.3010, -4.3142],
        [-7.1856,  2.5704],
        [-6.9009,  2.2857],
        [-5.8038,  1.1885],
        [-4.1077, -0.5076],
        [-7.4274,  2.8122],
        [-8.4792,  3.8640],
        [-6.1681,  1.5529],
        [-8.5251,  3.9099],
        [-5.0400,  0.4248],
        [-6.9409,  2.3257],
        [-6.9506,  2.3354],
        [-7.3743,  2.7591],
        [-5.9227,  1.3075],
        [-5.4957,  0.8805],
        [-9.6045,  4.9893],
        [-6.7036,  2.0884],
        [-4.4903, -0.1250],
        [-4.4038, -0.2114],
        [-8.7249,  4.1097],
        [-6.1742,  1.5590],
        [-9.9425,  5.3273],
        [-5.7453,  1.1301],
        [-4.5596, -0.0556],
        [-6.9823,  2.3671],
        [-8.1144,  3.4992],
        [-7.2551,  2.6399],
        [-7.3427,  2.7275],
        [-5.5370,  0.9218],
        [-7.0033,  2.3881],
        [-4.4242, -0.1910],
        [-5.8324,  1.2172],
        [-4.0816, -0.5336],
        [-9.5817,  4.9664],
        [-6.0064,  1.3912],
        [-4.6879,  0.0727],
        [-7.2133,  2.5981],
        [-4.7762,  0.1609],
        [-6.2704,  1.6551],
        [-6.9750,  2.3598],
        [-5.4897,  0.8745],
        [-6.5165,  1.9013],
        [-5.7539,  1.1387],
        [-2.9403, -1.6749],
        [-2.7941, -1.8211],
        [-8.7213,  4.1061],
        [-8.1029,  3.4877],
        [-4.4770, -0.1383],
        [-7.3739,  2.7587],
        [-7.2786,  2.6634],
        [-8.8517,  4.2365],
        [-3.8499, -0.7653],
        [-0.1975, -4.4177],
        [-5.8501,  1.2349],
        [-5.4356,  0.8204],
        [-4.0203, -0.5949],
        [-3.4422, -1.1730],
        [-8.3674,  3.7522],
        [-6.9523,  2.3371],
        [-6.2904,  1.6752],
        [-5.7403,  1.1251],
        [-7.6192,  3.0040],
        [-8.0751,  3.4599],
        [-7.7327,  3.1175],
        [-6.9879,  2.3727],
        [-0.1473, -4.4679],
        [-5.5052,  0.8900],
        [-7.1083,  2.4931],
        [-6.6630,  2.0477],
        [-7.7390,  3.1238],
        [-5.1803,  0.5651],
        [-9.6386,  5.0234],
        [-7.5028,  2.8876],
        [-3.5502, -1.0650],
        [-7.0463,  2.4311],
        [-6.8609,  2.2457],
        [-7.7104,  3.0952],
        [-6.1807,  1.5654],
        [-5.8375,  1.2223],
        [-4.2590, -0.3562],
        [-9.9102,  5.2949],
        [-4.5941, -0.0212],
        [-4.0386, -0.5766],
        [-7.3074,  2.6921],
        [ 0.6941, -5.3093],
        [-7.3652,  2.7499],
        [-4.1561, -0.4591],
        [-4.9463,  0.3311],
        [-7.4721,  2.8569],
        [-6.8185,  2.2033],
        [-6.3085,  1.6933],
        [-5.9904,  1.3752],
        [-5.2787,  0.6635],
        [-4.2145, -0.4007]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4986, 0.5014],
        [0.0051, 0.9949]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0497, 0.9503], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4226, 0.0851],
         [0.1147, 0.1661]],

        [[0.1103, 0.0885],
         [0.6716, 0.7520]],

        [[0.6945, 0.2491],
         [0.3592, 0.1546]],

        [[0.1259, 0.1307],
         [0.8248, 0.4856]],

        [[0.0167, 0.1538],
         [0.2683, 0.2642]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.011530202595462608
time is 1
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.018778022358394132
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 61
Adjusted Rand Index: -0.007726325420627255
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.004960463163867598
Average Adjusted Rand Index: 0.004516379906645896
Iteration 0: Loss = -17975.689942883346
Iteration 10: Loss = -11046.685196842729
Iteration 20: Loss = -11046.226917308963
Iteration 30: Loss = -11045.830388916518
Iteration 40: Loss = -11045.415547631554
Iteration 50: Loss = -11044.901052326939
Iteration 60: Loss = -11043.968691147935
Iteration 70: Loss = -11031.546092350985
Iteration 80: Loss = -10857.132291727312
Iteration 90: Loss = -10856.332516779423
Iteration 100: Loss = -10856.333864640812
1
Iteration 110: Loss = -10856.334188695871
2
Iteration 120: Loss = -10856.334235102522
3
Stopping early at iteration 119 due to no improvement.
pi: tensor([[0.7442, 0.2558],
        [0.3164, 0.6836]], dtype=torch.float64)
alpha: tensor([0.5533, 0.4467])
beta: tensor([[[0.2460, 0.0906],
         [0.6486, 0.2024]],

        [[0.0297, 0.0924],
         [0.8945, 0.7569]],

        [[0.5217, 0.0991],
         [0.1098, 0.4078]],

        [[0.7002, 0.0947],
         [0.4153, 0.4847]],

        [[0.1696, 0.0922],
         [0.0183, 0.5552]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448436976567872
time is 1
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8077222191419615
time is 3
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721426378272603
time is 4
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448326530612245
Global Adjusted Rand Index: 0.8460832335055567
Average Adjusted Rand Index: 0.8459078967414421
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17975.471056128765
Iteration 100: Loss = -11055.644442709017
Iteration 200: Loss = -11052.39291347692
Iteration 300: Loss = -11051.14591406231
Iteration 400: Loss = -11050.519670999967
Iteration 500: Loss = -11050.143576924305
Iteration 600: Loss = -11049.867530983538
Iteration 700: Loss = -11049.665103782105
Iteration 800: Loss = -11049.546225653421
Iteration 900: Loss = -11049.468187821349
Iteration 1000: Loss = -11049.413052986125
Iteration 1100: Loss = -11049.372991065591
Iteration 1200: Loss = -11049.343972774157
Iteration 1300: Loss = -11049.32328541987
Iteration 1400: Loss = -11049.3086653089
Iteration 1500: Loss = -11049.297994276549
Iteration 1600: Loss = -11049.290129240708
Iteration 1700: Loss = -11049.284198625368
Iteration 1800: Loss = -11049.304048669168
1
Iteration 1900: Loss = -11049.276380953286
Iteration 2000: Loss = -11049.273792435206
Iteration 2100: Loss = -11049.732305353427
1
Iteration 2200: Loss = -11049.270183000483
Iteration 2300: Loss = -11049.268931862465
Iteration 2400: Loss = -11049.267889450988
Iteration 2500: Loss = -11049.26760583057
Iteration 2600: Loss = -11049.266366088488
Iteration 2700: Loss = -11049.26575537905
Iteration 2800: Loss = -11049.265290097035
Iteration 2900: Loss = -11049.26497022467
Iteration 3000: Loss = -11049.264514096292
Iteration 3100: Loss = -11049.264206579495
Iteration 3200: Loss = -11049.269798203506
1
Iteration 3300: Loss = -11049.263726530415
Iteration 3400: Loss = -11049.263534488666
Iteration 3500: Loss = -11049.286171134927
1
Iteration 3600: Loss = -11049.263268988876
Iteration 3700: Loss = -11049.263142331012
Iteration 3800: Loss = -11049.26310731341
Iteration 3900: Loss = -11049.262954395264
Iteration 4000: Loss = -11049.262887631185
Iteration 4100: Loss = -11049.26277695885
Iteration 4200: Loss = -11049.262737976527
Iteration 4300: Loss = -11049.262705741483
Iteration 4400: Loss = -11049.262627832602
Iteration 4500: Loss = -11049.262610151482
Iteration 4600: Loss = -11049.262572929532
Iteration 4700: Loss = -11049.26250697551
Iteration 4800: Loss = -11049.26411133213
1
Iteration 4900: Loss = -11049.262434165148
Iteration 5000: Loss = -11049.262434989783
1
Iteration 5100: Loss = -11049.777526138998
2
Iteration 5200: Loss = -11049.262408133945
Iteration 5300: Loss = -11049.262427985534
1
Iteration 5400: Loss = -11049.262356033545
Iteration 5500: Loss = -11049.265223675638
1
Iteration 5600: Loss = -11049.262335121606
Iteration 5700: Loss = -11049.262325171452
Iteration 5800: Loss = -11049.267531579946
1
Iteration 5900: Loss = -11049.262340420119
2
Iteration 6000: Loss = -11049.262295001254
Iteration 6100: Loss = -11049.262814316666
1
Iteration 6200: Loss = -11049.262281565949
Iteration 6300: Loss = -11049.262280999537
Iteration 6400: Loss = -11049.268823145096
1
Iteration 6500: Loss = -11049.262246982087
Iteration 6600: Loss = -11049.262283781596
1
Iteration 6700: Loss = -11049.27389708082
2
Iteration 6800: Loss = -11049.262276668363
3
Iteration 6900: Loss = -11049.262245602897
Iteration 7000: Loss = -11049.291191114087
1
Iteration 7100: Loss = -11049.262263833021
2
Iteration 7200: Loss = -11049.264680511944
3
Iteration 7300: Loss = -11049.262606199787
4
Iteration 7400: Loss = -11049.265253587018
5
Iteration 7500: Loss = -11049.26224164907
Iteration 7600: Loss = -11049.262669767222
1
Iteration 7700: Loss = -11049.26225092602
2
Iteration 7800: Loss = -11049.262257579057
3
Iteration 7900: Loss = -11049.262239619056
Iteration 8000: Loss = -11049.262471862125
1
Iteration 8100: Loss = -11049.262183643194
Iteration 8200: Loss = -11049.270736748824
1
Iteration 8300: Loss = -11049.262197911541
2
Iteration 8400: Loss = -11049.26229129674
3
Iteration 8500: Loss = -11049.26230605235
4
Iteration 8600: Loss = -11049.262886126688
5
Iteration 8700: Loss = -11049.26217710369
Iteration 8800: Loss = -11049.267190958002
1
Iteration 8900: Loss = -11049.262158527092
Iteration 9000: Loss = -11049.279704723425
1
Iteration 9100: Loss = -11049.262114893583
Iteration 9200: Loss = -11049.27280569808
1
Iteration 9300: Loss = -11049.262734135558
2
Iteration 9400: Loss = -11049.263312684056
3
Iteration 9500: Loss = -11049.265732208238
4
Iteration 9600: Loss = -11049.262156772675
5
Iteration 9700: Loss = -11049.263366366453
6
Iteration 9800: Loss = -11049.294914501364
7
Iteration 9900: Loss = -11049.26205381647
Iteration 10000: Loss = -11049.263335900418
1
Iteration 10100: Loss = -11049.262005815997
Iteration 10200: Loss = -11049.262146721509
1
Iteration 10300: Loss = -11049.262004600463
Iteration 10400: Loss = -11049.26207264217
1
Iteration 10500: Loss = -11049.276303196155
2
Iteration 10600: Loss = -11049.272435597168
3
Iteration 10700: Loss = -11049.26308872691
4
Iteration 10800: Loss = -11049.26191750018
Iteration 10900: Loss = -11049.26253294095
1
Iteration 11000: Loss = -11049.292200264845
2
Iteration 11100: Loss = -11049.261802907084
Iteration 11200: Loss = -11049.262467143666
1
Iteration 11300: Loss = -11049.264547195988
2
Iteration 11400: Loss = -11049.267669488425
3
Iteration 11500: Loss = -11049.261650682696
Iteration 11600: Loss = -11049.337148703935
1
Iteration 11700: Loss = -11049.26153468525
Iteration 11800: Loss = -11049.314331000234
1
Iteration 11900: Loss = -11049.261357196281
Iteration 12000: Loss = -11049.267535062903
1
Iteration 12100: Loss = -11049.26704646611
2
Iteration 12200: Loss = -11049.29653107578
3
Iteration 12300: Loss = -11049.279810629052
4
Iteration 12400: Loss = -11049.261108129731
Iteration 12500: Loss = -11049.261270859632
1
Iteration 12600: Loss = -11049.269870129805
2
Iteration 12700: Loss = -11049.260994282957
Iteration 12800: Loss = -11049.262203019223
1
Iteration 12900: Loss = -11049.437569528196
2
Iteration 13000: Loss = -11049.261018301833
3
Iteration 13100: Loss = -11049.26124389708
4
Iteration 13200: Loss = -11049.271123784081
5
Iteration 13300: Loss = -11049.2611519332
6
Iteration 13400: Loss = -11049.261967643384
7
Iteration 13500: Loss = -11049.339633401387
8
Iteration 13600: Loss = -11049.260967917287
Iteration 13700: Loss = -11049.26096563758
Iteration 13800: Loss = -11049.280454114803
1
Iteration 13900: Loss = -11049.262088637317
2
Iteration 14000: Loss = -11049.264169789174
3
Iteration 14100: Loss = -11049.26117523964
4
Iteration 14200: Loss = -11049.261019507778
5
Iteration 14300: Loss = -11049.273517237756
6
Iteration 14400: Loss = -11049.26114656376
7
Iteration 14500: Loss = -11049.261059147948
8
Iteration 14600: Loss = -11049.289090383254
9
Iteration 14700: Loss = -11049.261176533657
10
Stopping early at iteration 14700 due to no improvement.
tensor([[ 1.9199, -3.4861],
        [ 1.3604, -3.4738],
        [ 0.6995, -4.0175],
        [ 1.4748, -3.0960],
        [-0.0903, -4.5249],
        [ 1.4669, -3.3850],
        [ 2.0329, -3.6575],
        [ 1.3919, -3.5962],
        [ 1.8225, -3.2431],
        [ 2.0501, -3.4779],
        [ 1.9675, -3.8600],
        [ 1.6384, -3.2155],
        [ 1.4217, -2.8447],
        [ 1.8581, -3.5288],
        [ 1.4045, -3.1712],
        [ 1.7577, -4.0678],
        [ 1.6054, -3.4577],
        [ 0.4908, -4.6437],
        [ 1.0083, -3.9799],
        [ 1.8387, -3.5717],
        [ 1.7933, -3.1907],
        [ 1.3393, -2.7723],
        [ 1.0071, -3.8549],
        [ 1.4422, -3.9506],
        [ 2.0600, -3.4539],
        [ 1.3221, -3.1023],
        [ 1.7267, -3.6888],
        [-0.3460, -4.2692],
        [ 1.9360, -3.4753],
        [ 2.1945, -3.7827],
        [ 1.4900, -3.6454],
        [ 1.6099, -3.1104],
        [ 1.6775, -3.2884],
        [ 1.5455, -3.3798],
        [ 1.7498, -3.2412],
        [ 1.7510, -3.2353],
        [ 1.7407, -3.3944],
        [ 1.9921, -3.5616],
        [ 2.1284, -3.5276],
        [ 1.3215, -2.8325],
        [ 1.9511, -3.4376],
        [ 2.2786, -3.6834],
        [ 1.6872, -3.2935],
        [ 1.8362, -3.5758],
        [ 1.3701, -3.3178],
        [ 1.8721, -3.2587],
        [ 1.5172, -3.0283],
        [ 1.5887, -3.1030],
        [ 1.7133, -3.5746],
        [ 2.0774, -3.4710],
        [ 2.2101, -3.9001],
        [ 1.4585, -2.9539],
        [ 1.2853, -3.4308],
        [ 2.2185, -3.7440],
        [ 1.1524, -3.1427],
        [ 1.6381, -3.2104],
        [ 1.2219, -2.6111],
        [ 2.3362, -3.7304],
        [ 1.2065, -4.4850],
        [ 1.8366, -3.7175],
        [ 1.6737, -4.0086],
        [ 2.0399, -3.6282],
        [ 0.7386, -5.0918],
        [ 0.0402, -4.5193],
        [ 1.7982, -3.3194],
        [ 1.8746, -3.2613],
        [ 1.6949, -3.1545],
        [ 1.7121, -3.1068],
        [ 1.5190, -3.1030],
        [ 1.6663, -3.1988],
        [ 1.8636, -3.2543],
        [ 2.1421, -4.2493],
        [ 1.7460, -3.3865],
        [ 1.1206, -3.3102],
        [ 1.6502, -3.0596],
        [ 1.5248, -3.3128],
        [ 1.8779, -3.3970],
        [ 1.2959, -2.8414],
        [ 1.6828, -3.1497],
        [ 1.9023, -3.6556],
        [ 1.5909, -2.9775],
        [ 1.5484, -3.5895],
        [ 1.5843, -3.2275],
        [ 1.8840, -3.5324],
        [ 1.8358, -3.7168],
        [ 1.9285, -3.8979],
        [ 1.1865, -2.8136],
        [ 2.2582, -3.7101],
        [ 0.8131, -4.2968],
        [ 1.2573, -3.3257],
        [ 2.3632, -3.8929],
        [ 1.7491, -3.2464],
        [ 2.0363, -3.5122],
        [ 1.8010, -4.0338],
        [ 0.6883, -3.7257],
        [ 1.7069, -3.2900],
        [ 1.4727, -3.0948],
        [ 1.4196, -3.4325],
        [ 2.0044, -3.6846],
        [ 2.1785, -3.6539]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.0263, 0.9737],
        [0.0464, 0.9536]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9930, 0.0070], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1628, 0.1832],
         [0.6486, 0.1620]],

        [[0.0297, 0.2220],
         [0.8945, 0.7569]],

        [[0.5217, 0.2303],
         [0.1098, 0.4078]],

        [[0.7002, 0.1751],
         [0.4153, 0.4847]],

        [[0.1696, 0.1571],
         [0.0183, 0.5552]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 61
Adjusted Rand Index: -0.007726325420627255
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.000307727139266984
Average Adjusted Rand Index: -0.001545265084125451
Iteration 0: Loss = -18418.41132387198
Iteration 10: Loss = -11049.539618820329
Iteration 20: Loss = -11048.234918796354
Iteration 30: Loss = -11047.60735105978
Iteration 40: Loss = -11046.99508388881
Iteration 50: Loss = -11046.511769501803
Iteration 60: Loss = -11046.097987499485
Iteration 70: Loss = -11045.70069492166
Iteration 80: Loss = -11045.265802869746
Iteration 90: Loss = -11044.679880321233
Iteration 100: Loss = -11043.27021812158
Iteration 110: Loss = -10979.945510554897
Iteration 120: Loss = -10856.446392526681
Iteration 130: Loss = -10856.332871732158
Iteration 140: Loss = -10856.334101571072
1
Iteration 150: Loss = -10856.334235311964
2
Iteration 160: Loss = -10856.334250635018
3
Stopping early at iteration 159 due to no improvement.
pi: tensor([[0.6836, 0.3164],
        [0.2558, 0.7442]], dtype=torch.float64)
alpha: tensor([0.4467, 0.5533])
beta: tensor([[[0.2024, 0.0906],
         [0.1828, 0.2460]],

        [[0.2433, 0.0924],
         [0.4935, 0.5970]],

        [[0.4221, 0.0991],
         [0.1672, 0.8446]],

        [[0.9647, 0.0947],
         [0.5786, 0.2898]],

        [[0.4860, 0.0922],
         [0.5187, 0.5925]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448436976567872
time is 1
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8077222191419615
time is 3
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721426378272603
time is 4
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448326530612245
Global Adjusted Rand Index: 0.8460832335055567
Average Adjusted Rand Index: 0.8459078967414421
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18418.71054460175
Iteration 100: Loss = -11052.156227909843
Iteration 200: Loss = -11049.862977433142
Iteration 300: Loss = -11048.706388858645
Iteration 400: Loss = -11048.319936354063
Iteration 500: Loss = -11048.023985680507
Iteration 600: Loss = -11047.73740069652
Iteration 700: Loss = -11047.49945821915
Iteration 800: Loss = -11047.27161901337
Iteration 900: Loss = -11046.975464667992
Iteration 1000: Loss = -11045.96877253866
Iteration 1100: Loss = -11034.846120690121
Iteration 1200: Loss = -10858.3516446925
Iteration 1300: Loss = -10854.968215719622
Iteration 1400: Loss = -10853.902899179571
Iteration 1500: Loss = -10852.146792633092
Iteration 1600: Loss = -10852.094791036234
Iteration 1700: Loss = -10852.056263279239
Iteration 1800: Loss = -10852.028057010257
Iteration 1900: Loss = -10852.012121561722
Iteration 2000: Loss = -10852.00176616738
Iteration 2100: Loss = -10851.991919344791
Iteration 2200: Loss = -10851.980193541865
Iteration 2300: Loss = -10851.975087283334
Iteration 2400: Loss = -10851.970812777367
Iteration 2500: Loss = -10851.967230790046
Iteration 2600: Loss = -10851.963947773273
Iteration 2700: Loss = -10851.957652403524
Iteration 2800: Loss = -10851.871687099841
Iteration 2900: Loss = -10851.869719886066
Iteration 3000: Loss = -10851.868024588963
Iteration 3100: Loss = -10851.866560095003
Iteration 3200: Loss = -10851.865256043904
Iteration 3300: Loss = -10851.864123689205
Iteration 3400: Loss = -10851.863052069966
Iteration 3500: Loss = -10851.862040000393
Iteration 3600: Loss = -10851.861098637848
Iteration 3700: Loss = -10851.860115087478
Iteration 3800: Loss = -10851.859119054288
Iteration 3900: Loss = -10851.858913161816
Iteration 4000: Loss = -10851.85726785549
Iteration 4100: Loss = -10851.857613036442
1
Iteration 4200: Loss = -10851.85588321998
Iteration 4300: Loss = -10851.855400081726
Iteration 4400: Loss = -10851.851090541428
Iteration 4500: Loss = -10851.85268774714
1
Iteration 4600: Loss = -10851.850696634407
Iteration 4700: Loss = -10851.849234987518
Iteration 4800: Loss = -10851.848986071269
Iteration 4900: Loss = -10851.848663953118
Iteration 5000: Loss = -10851.850395493339
1
Iteration 5100: Loss = -10851.848143125218
Iteration 5200: Loss = -10851.85092632756
1
Iteration 5300: Loss = -10851.847659201816
Iteration 5400: Loss = -10851.848849619775
1
Iteration 5500: Loss = -10851.850622836419
2
Iteration 5600: Loss = -10851.84755448383
Iteration 5700: Loss = -10851.84627419895
Iteration 5800: Loss = -10851.851471155856
1
Iteration 5900: Loss = -10851.844691626131
Iteration 6000: Loss = -10851.846339657115
1
Iteration 6100: Loss = -10851.844483370569
Iteration 6200: Loss = -10851.843387677658
Iteration 6300: Loss = -10851.84331247761
Iteration 6400: Loss = -10851.820682551763
Iteration 6500: Loss = -10851.815573309055
Iteration 6600: Loss = -10851.814971793756
Iteration 6700: Loss = -10851.814809187597
Iteration 6800: Loss = -10851.81895607111
1
Iteration 6900: Loss = -10851.829063272908
2
Iteration 7000: Loss = -10851.814672351093
Iteration 7100: Loss = -10851.814387892799
Iteration 7200: Loss = -10851.814592589617
1
Iteration 7300: Loss = -10851.81415000043
Iteration 7400: Loss = -10851.81391366337
Iteration 7500: Loss = -10851.815032228873
1
Iteration 7600: Loss = -10851.811031399207
Iteration 7700: Loss = -10851.821031595258
1
Iteration 7800: Loss = -10851.809618870257
Iteration 7900: Loss = -10851.81518641648
1
Iteration 8000: Loss = -10851.809463902639
Iteration 8100: Loss = -10851.81006437296
1
Iteration 8200: Loss = -10851.80940896619
Iteration 8300: Loss = -10851.809321427922
Iteration 8400: Loss = -10852.00610651432
1
Iteration 8500: Loss = -10851.809272951512
Iteration 8600: Loss = -10851.809217138116
Iteration 8700: Loss = -10851.809224250639
1
Iteration 8800: Loss = -10851.809203270244
Iteration 8900: Loss = -10851.81219567696
1
Iteration 9000: Loss = -10851.818028340373
2
Iteration 9100: Loss = -10851.811842435183
3
Iteration 9200: Loss = -10851.80807472228
Iteration 9300: Loss = -10851.807747746367
Iteration 9400: Loss = -10851.852748368034
1
Iteration 9500: Loss = -10851.793255464474
Iteration 9600: Loss = -10851.79353361991
1
Iteration 9700: Loss = -10851.793722431576
2
Iteration 9800: Loss = -10851.793025100995
Iteration 9900: Loss = -10851.792419060988
Iteration 10000: Loss = -10851.793296238866
1
Iteration 10100: Loss = -10851.792314127924
Iteration 10200: Loss = -10851.845267589473
1
Iteration 10300: Loss = -10851.792279424099
Iteration 10400: Loss = -10851.792236810532
Iteration 10500: Loss = -10851.792798370128
1
Iteration 10600: Loss = -10851.792189787042
Iteration 10700: Loss = -10851.792198016057
1
Iteration 10800: Loss = -10851.792344747302
2
Iteration 10900: Loss = -10851.79264992146
3
Iteration 11000: Loss = -10851.804988051013
4
Iteration 11100: Loss = -10851.792140795602
Iteration 11200: Loss = -10851.815641075773
1
Iteration 11300: Loss = -10851.791829911981
Iteration 11400: Loss = -10851.797225828897
1
Iteration 11500: Loss = -10851.790116986407
Iteration 11600: Loss = -10851.792174230888
1
Iteration 11700: Loss = -10851.850059655866
2
Iteration 11800: Loss = -10851.790175210903
3
Iteration 11900: Loss = -10851.790152673115
4
Iteration 12000: Loss = -10851.969726933416
5
Iteration 12100: Loss = -10851.79011462584
Iteration 12200: Loss = -10851.79010126642
Iteration 12300: Loss = -10851.790878460604
1
Iteration 12400: Loss = -10851.790102771105
2
Iteration 12500: Loss = -10851.813255538907
3
Iteration 12600: Loss = -10851.790083099908
Iteration 12700: Loss = -10851.790105461925
1
Iteration 12800: Loss = -10851.790133077027
2
Iteration 12900: Loss = -10851.790094504759
3
Iteration 13000: Loss = -10851.791857246148
4
Iteration 13100: Loss = -10851.790023528825
Iteration 13200: Loss = -10852.055455014715
1
Iteration 13300: Loss = -10851.790058156166
2
Iteration 13400: Loss = -10851.790049214313
3
Iteration 13500: Loss = -10851.790295630622
4
Iteration 13600: Loss = -10851.790027822368
5
Iteration 13700: Loss = -10851.811397740099
6
Iteration 13800: Loss = -10851.790019266044
Iteration 13900: Loss = -10851.790024466194
1
Iteration 14000: Loss = -10851.790919830666
2
Iteration 14100: Loss = -10851.790020787452
3
Iteration 14200: Loss = -10851.891839661372
4
Iteration 14300: Loss = -10851.790005145702
Iteration 14400: Loss = -10851.789974186298
Iteration 14500: Loss = -10851.79429653709
1
Iteration 14600: Loss = -10851.790007205314
2
Iteration 14700: Loss = -10851.790007291569
3
Iteration 14800: Loss = -10851.790136960877
4
Iteration 14900: Loss = -10851.790012149153
5
Iteration 15000: Loss = -10851.803153218036
6
Iteration 15100: Loss = -10851.790000518438
7
Iteration 15200: Loss = -10851.790101417593
8
Iteration 15300: Loss = -10851.790022874515
9
Iteration 15400: Loss = -10851.789941520532
Iteration 15500: Loss = -10851.793540765128
1
Iteration 15600: Loss = -10851.789714101778
Iteration 15700: Loss = -10851.789698965493
Iteration 15800: Loss = -10851.789633865385
Iteration 15900: Loss = -10851.789632370186
Iteration 16000: Loss = -10851.790782337746
1
Iteration 16100: Loss = -10851.789570017816
Iteration 16200: Loss = -10851.794310691952
1
Iteration 16300: Loss = -10851.789567423477
Iteration 16400: Loss = -10851.789546940257
Iteration 16500: Loss = -10851.794172337726
1
Iteration 16600: Loss = -10851.789567422642
2
Iteration 16700: Loss = -10851.825300044353
3
Iteration 16800: Loss = -10851.789572064075
4
Iteration 16900: Loss = -10851.916320290693
5
Iteration 17000: Loss = -10851.789563301372
6
Iteration 17100: Loss = -10851.789578217289
7
Iteration 17200: Loss = -10851.789613834568
8
Iteration 17300: Loss = -10851.820774329886
9
Iteration 17400: Loss = -10851.789511693982
Iteration 17500: Loss = -10851.795996402418
1
Iteration 17600: Loss = -10851.789565746098
2
Iteration 17700: Loss = -10851.790497183107
3
Iteration 17800: Loss = -10851.789565632456
4
Iteration 17900: Loss = -10851.789771285738
5
Iteration 18000: Loss = -10851.789543837607
6
Iteration 18100: Loss = -10851.789575694333
7
Iteration 18200: Loss = -10851.789552349786
8
Iteration 18300: Loss = -10851.789660658473
9
Iteration 18400: Loss = -10851.789548122719
10
Stopping early at iteration 18400 due to no improvement.
tensor([[  6.2207,  -7.6400],
        [ -5.8572,   4.4670],
        [ -3.9366,   2.0482],
        [  1.9842,  -5.5563],
        [ -9.5849,   7.7904],
        [ -9.0410,   7.6359],
        [  5.1319,  -7.2768],
        [  3.2566,  -5.2559],
        [ -4.6278,   3.1639],
        [  4.9823,  -6.4376],
        [  3.8531,  -5.3653],
        [ -2.6550,   1.1378],
        [  2.0951,  -3.5043],
        [ -6.7169,   2.4101],
        [ -6.8953,   5.5034],
        [  3.7527,  -5.5378],
        [  2.0245,  -3.4157],
        [ -6.7113,   5.2959],
        [ -6.3180,   2.9017],
        [  1.8058,  -3.8672],
        [ -5.2100,   1.7078],
        [-10.6853,   8.4106],
        [ -4.1360,   2.2070],
        [  0.9395,  -2.8238],
        [  1.7303,  -3.1319],
        [ -9.0077,   7.0121],
        [ -4.1716,   2.7827],
        [-10.1221,   8.2196],
        [  4.2358,  -5.6569],
        [  3.1125,  -4.7521],
        [ -2.6712,   0.9218],
        [ -6.8644,   5.4476],
        [  1.2545,  -3.5020],
        [ -6.6017,   5.1083],
        [  4.9755,  -6.9463],
        [  1.0071,  -2.7221],
        [  3.2353,  -6.6203],
        [ -2.8582,   1.4241],
        [ -3.8468,  -0.7685],
        [ -5.3083,   3.8983],
        [  2.6189,  -4.0786],
        [ -0.2166,  -2.0274],
        [ -5.1352,   3.5086],
        [  3.5410,  -5.1975],
        [ -5.4152,   3.9574],
        [ -6.7695,   5.0595],
        [  3.2337,  -5.8811],
        [ -2.5476,   0.3504],
        [ -6.0272,   4.5568],
        [  3.7729,  -5.1612],
        [  0.7601,  -2.8174],
        [ -6.5902,   5.2017],
        [ -1.7579,  -0.5221],
        [  4.1927,  -6.2416],
        [ -9.7292,   8.1793],
        [  1.9897,  -3.4343],
        [ -6.8316,   5.2489],
        [  1.4106,  -3.7533],
        [  4.2366,  -5.8886],
        [ -1.8291,   0.2104],
        [ -4.3686,   2.8045],
        [  4.3817,  -5.7780],
        [  5.9559,  -7.6000],
        [ -8.3382,   6.8570],
        [ -2.9276,  -0.0186],
        [ -1.6070,  -0.3167],
        [  6.9315,  -8.4860],
        [ -6.4345,   1.8193],
        [ -6.8964,   4.0542],
        [ -6.5248,   5.1054],
        [ -6.9744,   5.2995],
        [  2.6499,  -4.6431],
        [  3.1509,  -4.5389],
        [  3.0077,  -5.0884],
        [  1.1599,  -2.5715],
        [ -6.0536,   4.1795],
        [ -6.2450,   4.8551],
        [ -6.5548,   4.9603],
        [  3.4667,  -5.9996],
        [ -3.6805,   2.2929],
        [  1.8146,  -3.2047],
        [ -4.1939,   2.4181],
        [ -7.3641,   2.7489],
        [ -6.1316,   4.5138],
        [ -3.9682,   2.1273],
        [  3.7882,  -5.2386],
        [ -0.7156,  -1.8338],
        [  2.1412,  -4.0201],
        [  3.4433,  -5.8209],
        [ -4.4037,   1.6599],
        [  2.5496,  -3.9379],
        [ -6.2350,   3.4145],
        [  5.2290,  -8.1200],
        [ -1.6266,   0.1863],
        [ -4.5450,   3.1503],
        [ -4.0010,   1.1442],
        [  5.5911,  -7.1208],
        [ -4.3661,   1.4283],
        [  5.5775,  -7.2438],
        [  2.9044,  -4.2939]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7125, 0.2875],
        [0.2429, 0.7571]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4640, 0.5360], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2059, 0.0902],
         [0.1828, 0.2520]],

        [[0.2433, 0.0920],
         [0.4935, 0.5970]],

        [[0.4221, 0.0996],
         [0.1672, 0.8446]],

        [[0.9647, 0.0958],
         [0.5786, 0.2898]],

        [[0.4860, 0.0920],
         [0.5187, 0.5925]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448436976567872
time is 1
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8822045595164437
time is 3
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448326530612245
Global Adjusted Rand Index: 0.890913405634346
Average Adjusted Rand Index: 0.8905364604818743
Iteration 0: Loss = -16032.793852940953
Iteration 10: Loss = -11029.298855475748
Iteration 20: Loss = -10857.95578030697
Iteration 30: Loss = -10856.337826039055
Iteration 40: Loss = -10856.334702186901
Iteration 50: Loss = -10856.334360985447
Iteration 60: Loss = -10856.334298741636
Iteration 70: Loss = -10856.334284713497
Iteration 80: Loss = -10856.334267118862
Iteration 90: Loss = -10856.334276844973
1
Iteration 100: Loss = -10856.334275582676
2
Iteration 110: Loss = -10856.334273959352
3
Stopping early at iteration 109 due to no improvement.
pi: tensor([[0.6836, 0.3164],
        [0.2558, 0.7442]], dtype=torch.float64)
alpha: tensor([0.4467, 0.5533])
beta: tensor([[[0.2024, 0.0906],
         [0.1876, 0.2460]],

        [[0.0267, 0.0924],
         [0.6659, 0.1353]],

        [[0.0581, 0.0991],
         [0.5575, 0.0206]],

        [[0.0894, 0.0947],
         [0.3358, 0.8158]],

        [[0.5409, 0.0922],
         [0.4258, 0.7930]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448436976567872
time is 1
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8077222191419615
time is 3
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721426378272603
time is 4
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448326530612245
Global Adjusted Rand Index: 0.8460832335055567
Average Adjusted Rand Index: 0.8459078967414421
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16032.714242221233
Iteration 100: Loss = -11067.50167439886
Iteration 200: Loss = -11050.07784221351
Iteration 300: Loss = -11048.166782678827
Iteration 400: Loss = -11047.783795466405
Iteration 500: Loss = -11048.155393487345
1
Iteration 600: Loss = -11047.432690875567
Iteration 700: Loss = -11047.321722703742
Iteration 800: Loss = -11047.236075818975
Iteration 900: Loss = -11047.17618417753
Iteration 1000: Loss = -11047.137236335802
Iteration 1100: Loss = -11047.112631276019
Iteration 1200: Loss = -11047.096793555613
Iteration 1300: Loss = -11047.085918057253
Iteration 1400: Loss = -11047.078086103145
Iteration 1500: Loss = -11047.09071472123
1
Iteration 1600: Loss = -11047.067314823958
Iteration 1700: Loss = -11047.063283514526
Iteration 1800: Loss = -11047.059836304408
Iteration 1900: Loss = -11047.057052278376
Iteration 2000: Loss = -11047.053986298966
Iteration 2100: Loss = -11047.051478748925
Iteration 2200: Loss = -11047.071557005085
1
Iteration 2300: Loss = -11047.046758771727
Iteration 2400: Loss = -11047.044533882525
Iteration 2500: Loss = -11047.083194384595
1
Iteration 2600: Loss = -11047.03986701501
Iteration 2700: Loss = -11047.037212930098
Iteration 2800: Loss = -11047.034202321987
Iteration 2900: Loss = -11047.03036876521
Iteration 3000: Loss = -11047.02470612653
Iteration 3100: Loss = -11047.015092035537
Iteration 3200: Loss = -11047.551210888223
1
Iteration 3300: Loss = -11046.877528747966
Iteration 3400: Loss = -11043.249944479685
Iteration 3500: Loss = -11040.268762446018
Iteration 3600: Loss = -11032.522049982084
Iteration 3700: Loss = -11029.65198735187
Iteration 3800: Loss = -11029.603145179295
Iteration 3900: Loss = -11029.599403283459
Iteration 4000: Loss = -11029.53006867201
Iteration 4100: Loss = -11029.27089613003
Iteration 4200: Loss = -10990.22154485346
Iteration 4300: Loss = -10981.9599729702
Iteration 4400: Loss = -10975.272938306402
Iteration 4500: Loss = -10971.743829819594
Iteration 4600: Loss = -10960.155421118247
Iteration 4700: Loss = -10952.597287952633
Iteration 4800: Loss = -10943.049002951524
Iteration 4900: Loss = -10933.31840106452
Iteration 5000: Loss = -10927.911349800484
Iteration 5100: Loss = -10927.760465548672
Iteration 5200: Loss = -10915.832071373648
Iteration 5300: Loss = -10908.024946828773
Iteration 5400: Loss = -10899.50534797846
Iteration 5500: Loss = -10889.522870138031
Iteration 5600: Loss = -10879.623103064125
Iteration 5700: Loss = -10879.620513278562
Iteration 5800: Loss = -10879.615074437143
Iteration 5900: Loss = -10879.737342118115
1
Iteration 6000: Loss = -10879.605627429559
Iteration 6100: Loss = -10879.656571587953
1
Iteration 6200: Loss = -10879.602695536845
Iteration 6300: Loss = -10879.654395954683
1
Iteration 6400: Loss = -10877.013958493406
Iteration 6500: Loss = -10874.493358601887
Iteration 6600: Loss = -10874.388195502128
Iteration 6700: Loss = -10874.391382669022
1
Iteration 6800: Loss = -10874.385026256447
Iteration 6900: Loss = -10874.390680263648
1
Iteration 7000: Loss = -10874.38133892242
Iteration 7100: Loss = -10874.455295257274
1
Iteration 7200: Loss = -10874.315644460163
Iteration 7300: Loss = -10874.31431419676
Iteration 7400: Loss = -10874.310836028155
Iteration 7500: Loss = -10874.308621596556
Iteration 7600: Loss = -10874.308428094908
Iteration 7700: Loss = -10874.308174125092
Iteration 7800: Loss = -10874.307114474315
Iteration 7900: Loss = -10874.267876384629
Iteration 8000: Loss = -10874.223953803357
Iteration 8100: Loss = -10874.22725753101
1
Iteration 8200: Loss = -10874.201705007565
Iteration 8300: Loss = -10874.201045406184
Iteration 8400: Loss = -10874.19682096098
Iteration 8500: Loss = -10874.19777589936
1
Iteration 8600: Loss = -10874.195869971649
Iteration 8700: Loss = -10874.19453008745
Iteration 8800: Loss = -10874.195535762532
1
Iteration 8900: Loss = -10874.206164635001
2
Iteration 9000: Loss = -10874.193956358364
Iteration 9100: Loss = -10868.366090540703
Iteration 9200: Loss = -10867.966256695834
Iteration 9300: Loss = -10867.926600535413
Iteration 9400: Loss = -10865.394325517253
Iteration 9500: Loss = -10864.11007066442
Iteration 9600: Loss = -10864.08020915418
Iteration 9700: Loss = -10864.07844318987
Iteration 9800: Loss = -10864.075601065313
Iteration 9900: Loss = -10864.068578580262
Iteration 10000: Loss = -10864.069697601417
1
Iteration 10100: Loss = -10864.068074176377
Iteration 10200: Loss = -10864.068974825192
1
Iteration 10300: Loss = -10864.06782395488
Iteration 10400: Loss = -10864.06774786864
Iteration 10500: Loss = -10864.067285688827
Iteration 10600: Loss = -10864.067340988213
1
Iteration 10700: Loss = -10864.06706175739
Iteration 10800: Loss = -10864.076196979402
1
Iteration 10900: Loss = -10864.066208106644
Iteration 11000: Loss = -10864.041003658293
Iteration 11100: Loss = -10864.040955645114
Iteration 11200: Loss = -10864.067551859329
1
Iteration 11300: Loss = -10864.040657456664
Iteration 11400: Loss = -10864.039794470018
Iteration 11500: Loss = -10864.03502190745
Iteration 11600: Loss = -10864.034204407402
Iteration 11700: Loss = -10864.032976334547
Iteration 11800: Loss = -10858.68743135442
Iteration 11900: Loss = -10855.291221426385
Iteration 12000: Loss = -10855.326056159936
1
Iteration 12100: Loss = -10855.311624871421
2
Iteration 12200: Loss = -10855.288604044032
Iteration 12300: Loss = -10855.288968254748
1
Iteration 12400: Loss = -10855.332625114575
2
Iteration 12500: Loss = -10855.241530146239
Iteration 12600: Loss = -10855.27620195026
1
Iteration 12700: Loss = -10855.241462161153
Iteration 12800: Loss = -10855.262423408876
1
Iteration 12900: Loss = -10855.241184278699
Iteration 13000: Loss = -10855.241211523567
1
Iteration 13100: Loss = -10855.230873247034
Iteration 13200: Loss = -10855.230250233948
Iteration 13300: Loss = -10851.940410642335
Iteration 13400: Loss = -10851.938552457857
Iteration 13500: Loss = -10852.047737006611
1
Iteration 13600: Loss = -10851.938293953212
Iteration 13700: Loss = -10852.011854637783
1
Iteration 13800: Loss = -10851.938171887989
Iteration 13900: Loss = -10851.95569837909
1
Iteration 14000: Loss = -10851.937459213043
Iteration 14100: Loss = -10851.928068011
Iteration 14200: Loss = -10851.929157177368
1
Iteration 14300: Loss = -10851.928065362788
Iteration 14400: Loss = -10852.030613018394
1
Iteration 14500: Loss = -10851.928073330932
2
Iteration 14600: Loss = -10851.928035175342
Iteration 14700: Loss = -10851.92817789902
1
Iteration 14800: Loss = -10851.927984910182
Iteration 14900: Loss = -10851.927430760614
Iteration 15000: Loss = -10851.926683749549
Iteration 15100: Loss = -10851.926707469958
1
Iteration 15200: Loss = -10851.955088564733
2
Iteration 15300: Loss = -10851.926611291465
Iteration 15400: Loss = -10851.948271050107
1
Iteration 15500: Loss = -10851.92645854543
Iteration 15600: Loss = -10851.926428450373
Iteration 15700: Loss = -10851.92808197718
1
Iteration 15800: Loss = -10851.925871308973
Iteration 15900: Loss = -10851.903923047326
Iteration 16000: Loss = -10851.961379336026
1
Iteration 16100: Loss = -10851.901965566094
Iteration 16200: Loss = -10851.910531460699
1
Iteration 16300: Loss = -10851.897410170146
Iteration 16400: Loss = -10851.892329996967
Iteration 16500: Loss = -10852.013329424786
1
Iteration 16600: Loss = -10851.886207638703
Iteration 16700: Loss = -10851.886413354321
1
Iteration 16800: Loss = -10851.955754634095
2
Iteration 16900: Loss = -10851.885932320174
Iteration 17000: Loss = -10851.888374164684
1
Iteration 17100: Loss = -10851.885893541079
Iteration 17200: Loss = -10851.913689573717
1
Iteration 17300: Loss = -10851.885801064307
Iteration 17400: Loss = -10851.88576108865
Iteration 17500: Loss = -10851.879156914705
Iteration 17600: Loss = -10851.864794925488
Iteration 17700: Loss = -10851.866440442524
1
Iteration 17800: Loss = -10851.864758046126
Iteration 17900: Loss = -10851.897701957927
1
Iteration 18000: Loss = -10851.864659262796
Iteration 18100: Loss = -10851.864680349876
1
Iteration 18200: Loss = -10851.86490606988
2
Iteration 18300: Loss = -10851.864644017887
Iteration 18400: Loss = -10851.870772679707
1
Iteration 18500: Loss = -10851.864507185328
Iteration 18600: Loss = -10851.854525645664
Iteration 18700: Loss = -10851.849100207735
Iteration 18800: Loss = -10851.838823540571
Iteration 18900: Loss = -10851.83899296746
1
Iteration 19000: Loss = -10851.870817378722
2
Iteration 19100: Loss = -10851.90312560045
3
Iteration 19200: Loss = -10851.838521254249
Iteration 19300: Loss = -10851.838757508138
1
Iteration 19400: Loss = -10852.151560570977
2
Iteration 19500: Loss = -10851.83847261358
Iteration 19600: Loss = -10852.043544076225
1
Iteration 19700: Loss = -10851.836082130276
Iteration 19800: Loss = -10851.836092639953
1
Iteration 19900: Loss = -10851.83609779279
2
tensor([[  6.0936,  -7.7725],
        [ -5.8807,   4.4492],
        [ -9.1398,   7.0826],
        [  2.7453,  -4.7914],
        [ -9.6854,   8.2373],
        [-11.3673,   6.7521],
        [  5.0958,  -7.3261],
        [  3.5681,  -4.9553],
        [ -4.6714,   3.1736],
        [  7.4186,  -8.8676],
        [  3.6880,  -5.5407],
        [-10.0010,   8.5154],
        [  1.7996,  -3.8068],
        [ -5.9080,   3.2304],
        [ -6.9188,   5.4888],
        [  3.9518,  -5.3498],
        [  1.1808,  -4.2697],
        [ -6.8070,   5.2065],
        [ -5.3212,   3.9039],
        [  0.5991,  -5.0825],
        [ -4.2122,   2.7551],
        [-10.4982,   8.6818],
        [ -9.3977,   7.5146],
        [  1.0235,  -2.6983],
        [  1.6754,  -3.1962],
        [ -8.3042,   5.4664],
        [ -4.2056,   2.7418],
        [-10.8323,   8.6606],
        [  3.5751,  -6.3312],
        [  3.0321,  -4.8444],
        [ -2.4871,   1.0976],
        [ -9.8585,   8.1933],
        [  1.6352,  -3.0756],
        [ -7.8886,   3.8327],
        [  5.2264,  -6.7117],
        [  1.0676,  -2.6695],
        [  4.1754,  -5.6572],
        [ -3.1980,   1.0818],
        [ -2.3299,   0.7459],
        [ -8.9890,   7.4005],
        [  2.5836,  -4.1338],
        [  0.2050,  -1.6071],
        [ -5.1778,   3.4681],
        [  3.6577,  -5.0523],
        [ -5.5743,   3.7972],
        [ -6.9824,   4.8979],
        [  3.5381,  -5.5875],
        [ -2.9957,  -0.1113],
        [ -6.8648,   3.7332],
        [  5.1784,  -7.1539],
        [  0.6177,  -2.9712],
        [ -7.5976,   4.2431],
        [ -1.3147,  -0.0910],
        [  3.6848,  -6.7526],
        [-10.8185,   6.5057],
        [  1.5447,  -3.8546],
        [ -7.1469,   4.9959],
        [  1.4465,  -3.7261],
        [  4.1444,  -5.9930],
        [ -1.7544,   0.3356],
        [ -4.4359,   2.7434],
        [  8.8311, -10.2438],
        [  6.0049,  -7.5655],
        [ -8.3178,   6.9109],
        [ -3.0206,  -0.1088],
        [ -1.7917,  -0.5123],
        [  6.5184,  -8.8658],
        [ -4.8996,   3.3535],
        [ -6.8313,   4.1664],
        [ -7.7302,   3.8994],
        [ -6.9108,   5.4140],
        [  2.7032,  -4.5948],
        [  2.1197,  -5.5692],
        [  2.8226,  -5.2855],
        [  1.0880,  -2.6499],
        [ -6.5335,   3.7060],
        [ -7.8589,   3.2437],
        [ -6.4546,   5.0598],
        [  8.1645, -10.0393],
        [ -3.7024,   2.3158],
        [  1.7802,  -3.2573],
        [ -4.0404,   2.5656],
        [ -5.7669,   4.3506],
        [ -6.2301,   4.4078],
        [ -3.7454,   2.3547],
        [  3.6235,  -5.4174],
        [ -0.2474,  -1.3774],
        [  2.3237,  -3.8485],
        [  3.3030,  -5.9736],
        [ -3.8541,   2.2450],
        [  2.3665,  -4.1298],
        [ -6.1852,   3.4606],
        [  5.9879,  -7.3785],
        [ -1.7942,   0.0194],
        [ -4.5656,   3.1262],
        [ -3.2635,   1.8761],
        [  5.3400,  -7.3866],
        [ -5.2038,   0.5886],
        [  6.5305,  -8.1511],
        [  2.9041,  -4.3056]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7124, 0.2876],
        [0.2430, 0.7570]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4637, 0.5363], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2060, 0.0902],
         [0.1876, 0.2520]],

        [[0.0267, 0.0920],
         [0.6659, 0.1353]],

        [[0.0581, 0.0995],
         [0.5575, 0.0206]],

        [[0.0894, 0.0958],
         [0.3358, 0.8158]],

        [[0.5409, 0.0920],
         [0.4258, 0.7930]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448436976567872
time is 1
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8822045595164437
time is 3
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448326530612245
Global Adjusted Rand Index: 0.890913405634346
Average Adjusted Rand Index: 0.8905364604818743
Iteration 0: Loss = -25924.59629601255
Iteration 10: Loss = -11051.287744513602
Iteration 20: Loss = -11051.287744564916
1
Iteration 30: Loss = -11051.287747762597
2
Iteration 40: Loss = -11051.287843131162
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.0000e+00, 7.8271e-09],
        [6.2876e-01, 3.7124e-01]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 1.1360e-08])
beta: tensor([[[0.1625, 0.1220],
         [0.6171, 0.2110]],

        [[0.4719, 0.2403],
         [0.5408, 0.9211]],

        [[0.6931, 0.2618],
         [0.3586, 0.4639]],

        [[0.2389, 0.1313],
         [0.5261, 0.1865]],

        [[0.1589, 0.1396],
         [0.0148, 0.5938]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25924.72024549015
Iteration 100: Loss = -11054.236865904666
Iteration 200: Loss = -11052.477639090608
Iteration 300: Loss = -11051.771644505941
Iteration 400: Loss = -11051.32844663424
Iteration 500: Loss = -11050.659574671767
Iteration 600: Loss = -11049.796877867631
Iteration 700: Loss = -11049.21953136193
Iteration 800: Loss = -11048.757363586836
Iteration 900: Loss = -11048.438433654394
Iteration 1000: Loss = -11048.201985568132
Iteration 1100: Loss = -11048.017590782445
Iteration 1200: Loss = -11047.850715691307
Iteration 1300: Loss = -11047.679213907048
Iteration 1400: Loss = -11047.486271051275
Iteration 1500: Loss = -11047.235024583608
Iteration 1600: Loss = -11046.897781855876
Iteration 1700: Loss = -11046.522302529285
Iteration 1800: Loss = -11046.183398712621
Iteration 1900: Loss = -11045.727616221498
Iteration 2000: Loss = -11034.711910818653
Iteration 2100: Loss = -10907.052160890824
Iteration 2200: Loss = -10873.622955340845
Iteration 2300: Loss = -10858.406635304782
Iteration 2400: Loss = -10856.463838377049
Iteration 2500: Loss = -10853.809445096464
Iteration 2600: Loss = -10853.741080045686
Iteration 2700: Loss = -10853.694869275398
Iteration 2800: Loss = -10853.646996756062
Iteration 2900: Loss = -10852.657789976329
Iteration 3000: Loss = -10852.534480585797
Iteration 3100: Loss = -10852.518009219873
Iteration 3200: Loss = -10852.499266778363
Iteration 3300: Loss = -10852.485379592714
Iteration 3400: Loss = -10852.476584496166
Iteration 3500: Loss = -10852.469511679916
Iteration 3600: Loss = -10852.463360505766
Iteration 3700: Loss = -10852.45774625324
Iteration 3800: Loss = -10852.452353737153
Iteration 3900: Loss = -10852.447092192946
Iteration 4000: Loss = -10852.442444392496
Iteration 4100: Loss = -10852.438902575223
Iteration 4200: Loss = -10852.435844523838
Iteration 4300: Loss = -10852.432249228801
Iteration 4400: Loss = -10852.427685361692
Iteration 4500: Loss = -10852.42382181759
Iteration 4600: Loss = -10852.421853304464
Iteration 4700: Loss = -10852.42042415749
Iteration 4800: Loss = -10852.419198174854
Iteration 4900: Loss = -10852.418009135426
Iteration 5000: Loss = -10852.41689041814
Iteration 5100: Loss = -10852.415837106573
Iteration 5200: Loss = -10852.414781877364
Iteration 5300: Loss = -10852.413735536185
Iteration 5400: Loss = -10852.412813645025
Iteration 5500: Loss = -10852.411799537882
Iteration 5600: Loss = -10852.410369702151
Iteration 5700: Loss = -10852.404508032716
Iteration 5800: Loss = -10852.394675380212
Iteration 5900: Loss = -10852.393778474372
Iteration 6000: Loss = -10852.39792871156
1
Iteration 6100: Loss = -10852.392614198814
Iteration 6200: Loss = -10852.392366347642
Iteration 6300: Loss = -10852.400520919327
1
Iteration 6400: Loss = -10852.38943783303
Iteration 6500: Loss = -10852.361974644688
Iteration 6600: Loss = -10852.361172543864
Iteration 6700: Loss = -10852.36051366204
Iteration 6800: Loss = -10852.360167240262
Iteration 6900: Loss = -10852.361234072452
1
Iteration 7000: Loss = -10852.360182794588
2
Iteration 7100: Loss = -10852.359679547973
Iteration 7200: Loss = -10852.361898722547
1
Iteration 7300: Loss = -10852.358387514583
Iteration 7400: Loss = -10852.356990400789
Iteration 7500: Loss = -10852.355225334892
Iteration 7600: Loss = -10852.355141293758
Iteration 7700: Loss = -10852.353152405316
Iteration 7800: Loss = -10852.352586980138
Iteration 7900: Loss = -10852.351862173835
Iteration 8000: Loss = -10852.35075711297
Iteration 8100: Loss = -10852.338227136823
Iteration 8200: Loss = -10852.033415833375
Iteration 8300: Loss = -10852.032200215113
Iteration 8400: Loss = -10852.01822638544
Iteration 8500: Loss = -10852.102597410016
1
Iteration 8600: Loss = -10852.001130929739
Iteration 8700: Loss = -10851.987448093783
Iteration 8800: Loss = -10851.988750693665
1
Iteration 8900: Loss = -10851.986673418733
Iteration 9000: Loss = -10851.993377288856
1
Iteration 9100: Loss = -10851.986355251083
Iteration 9200: Loss = -10851.992637934609
1
Iteration 9300: Loss = -10851.988242483487
2
Iteration 9400: Loss = -10851.995555520076
3
Iteration 9500: Loss = -10851.985475370015
Iteration 9600: Loss = -10851.985382653711
Iteration 9700: Loss = -10851.985322988852
Iteration 9800: Loss = -10851.984283832031
Iteration 9900: Loss = -10851.983795980603
Iteration 10000: Loss = -10851.983549436289
Iteration 10100: Loss = -10851.983465508032
Iteration 10200: Loss = -10851.982017821041
Iteration 10300: Loss = -10851.983539524084
1
Iteration 10400: Loss = -10851.981755638444
Iteration 10500: Loss = -10851.98367871051
1
Iteration 10600: Loss = -10851.981710571954
Iteration 10700: Loss = -10851.98364383483
1
Iteration 10800: Loss = -10851.981735217807
2
Iteration 10900: Loss = -10851.987726131942
3
Iteration 11000: Loss = -10851.981565399688
Iteration 11100: Loss = -10851.985728421014
1
Iteration 11200: Loss = -10851.98146891301
Iteration 11300: Loss = -10852.001811918464
1
Iteration 11400: Loss = -10851.981033656126
Iteration 11500: Loss = -10851.980466775123
Iteration 11600: Loss = -10851.981184634535
1
Iteration 11700: Loss = -10851.980440911158
Iteration 11800: Loss = -10851.980380401823
Iteration 11900: Loss = -10851.980673906817
1
Iteration 12000: Loss = -10851.980332042196
Iteration 12100: Loss = -10851.980496258222
1
Iteration 12200: Loss = -10851.98029354739
Iteration 12300: Loss = -10851.980258208974
Iteration 12400: Loss = -10851.990123829835
1
Iteration 12500: Loss = -10851.98022891165
Iteration 12600: Loss = -10851.99903110258
1
Iteration 12700: Loss = -10851.980144371228
Iteration 12800: Loss = -10852.038463083258
1
Iteration 12900: Loss = -10851.977914424486
Iteration 13000: Loss = -10851.976117856464
Iteration 13100: Loss = -10851.831406667305
Iteration 13200: Loss = -10851.832432444591
1
Iteration 13300: Loss = -10851.831360091403
Iteration 13400: Loss = -10851.83116212471
Iteration 13500: Loss = -10851.83105009317
Iteration 13600: Loss = -10851.8311655239
1
Iteration 13700: Loss = -10851.830969200457
Iteration 13800: Loss = -10851.907388687543
1
Iteration 13900: Loss = -10851.830871132604
Iteration 14000: Loss = -10851.830833874017
Iteration 14100: Loss = -10851.831322123406
1
Iteration 14200: Loss = -10851.847992861047
2
Iteration 14300: Loss = -10851.83082511745
Iteration 14400: Loss = -10851.835296712547
1
Iteration 14500: Loss = -10851.83078148803
Iteration 14600: Loss = -10851.897588197784
1
Iteration 14700: Loss = -10851.829940539865
Iteration 14800: Loss = -10851.83043542825
1
Iteration 14900: Loss = -10851.829952236223
2
Iteration 15000: Loss = -10851.829880172101
Iteration 15100: Loss = -10851.829997641162
1
Iteration 15200: Loss = -10851.830618634973
2
Iteration 15300: Loss = -10851.829871068701
Iteration 15400: Loss = -10851.84612697479
1
Iteration 15500: Loss = -10851.828452960744
Iteration 15600: Loss = -10851.828437295657
Iteration 15700: Loss = -10851.823043001345
Iteration 15800: Loss = -10851.824515807024
1
Iteration 15900: Loss = -10851.82300237111
Iteration 16000: Loss = -10851.84878268607
1
Iteration 16100: Loss = -10851.823022531538
2
Iteration 16200: Loss = -10851.823005793112
3
Iteration 16300: Loss = -10851.823425027644
4
Iteration 16400: Loss = -10851.822974792369
Iteration 16500: Loss = -10851.849596006507
1
Iteration 16600: Loss = -10851.822769621755
Iteration 16700: Loss = -10851.822777305875
1
Iteration 16800: Loss = -10851.824381753864
2
Iteration 16900: Loss = -10851.822746078466
Iteration 17000: Loss = -10852.019052019361
1
Iteration 17100: Loss = -10851.819738063305
Iteration 17200: Loss = -10851.819740501007
1
Iteration 17300: Loss = -10851.822013108886
2
Iteration 17400: Loss = -10851.819763821635
3
Iteration 17500: Loss = -10851.915122423816
4
Iteration 17600: Loss = -10851.818286298023
Iteration 17700: Loss = -10851.818235017658
Iteration 17800: Loss = -10851.82379900421
1
Iteration 17900: Loss = -10851.818068476947
Iteration 18000: Loss = -10851.818043694704
Iteration 18100: Loss = -10851.818154363826
1
Iteration 18200: Loss = -10851.81794975854
Iteration 18300: Loss = -10851.860874609325
1
Iteration 18400: Loss = -10851.817916356045
Iteration 18500: Loss = -10852.015727798596
1
Iteration 18600: Loss = -10851.817812497393
Iteration 18700: Loss = -10852.131467887119
1
Iteration 18800: Loss = -10851.817629919318
Iteration 18900: Loss = -10851.817604114765
Iteration 19000: Loss = -10851.817673754513
1
Iteration 19100: Loss = -10851.817556354308
Iteration 19200: Loss = -10851.824235652864
1
Iteration 19300: Loss = -10851.817531025958
Iteration 19400: Loss = -10851.86244765574
1
Iteration 19500: Loss = -10851.817555489022
2
Iteration 19600: Loss = -10851.817529443633
Iteration 19700: Loss = -10851.81759489699
1
Iteration 19800: Loss = -10851.817774374229
2
Iteration 19900: Loss = -10851.959070707098
3
tensor([[  6.1736,  -7.6834],
        [ -6.1501,   4.1845],
        [ -4.2204,   1.7719],
        [  2.6150,  -4.9135],
        [-11.1812,   8.3448],
        [ -9.1725,   7.7624],
        [  5.1211,  -7.2807],
        [  3.1672,  -5.3368],
        [ -5.4627,   2.3374],
        [  5.0126,  -6.4004],
        [  3.0887,  -6.1233],
        [ -2.6464,   1.1609],
        [  1.6547,  -3.9423],
        [ -5.2658,   3.8759],
        [ -6.9727,   5.4424],
        [  2.7950,  -6.4883],
        [  1.9208,  -3.5131],
        [ -6.7099,   5.3102],
        [ -5.3125,   3.9196],
        [  1.5329,  -4.1316],
        [ -4.1660,   2.7048],
        [-12.0196,   8.3422],
        [ -4.1720,   2.1779],
        [  0.9859,  -2.7656],
        [  1.5359,  -3.3210],
        [ -9.7538,   7.5540],
        [ -5.7900,   1.1747],
        [-12.1610,   8.3427],
        [  4.2453,  -5.7009],
        [  3.2378,  -4.6273],
        [ -2.5796,   1.0152],
        [ -6.8743,   5.4460],
        [  1.2857,  -3.4607],
        [ -7.2852,   4.4310],
        [  5.2604,  -6.6612],
        [  1.1130,  -2.6033],
        [  4.1751,  -5.7013],
        [ -2.8412,   1.4507],
        [ -2.3803,   0.7074],
        [ -5.3668,   3.8304],
        [  2.6640,  -4.0506],
        [ -1.4063,  -3.2089],
        [ -5.0519,   3.6062],
        [  3.4648,  -5.2884],
        [ -5.7104,   3.6148],
        [ -6.7171,   5.1273],
        [  3.7988,  -5.3082],
        [ -2.1492,   0.7261],
        [ -7.1475,   3.4504],
        [  3.7688,  -5.1586],
        [  6.1465,  -9.4380],
        [ -6.7755,   5.0322],
        [ -1.5853,  -0.3466],
        [  4.3535,  -6.0765],
        [ -9.2908,   7.8708],
        [  1.6862,  -3.8172],
        [ -7.1142,   4.9914],
        [  1.8884,  -3.3307],
        [  4.1515,  -6.0252],
        [ -2.1172,  -0.0674],
        [ -4.3551,   2.8274],
        [  3.6023,  -6.5496],
        [  5.9277,  -7.6227],
        [ -8.3924,   6.7915],
        [ -2.1489,   0.7492],
        [ -1.3437,  -0.1199],
        [  6.7076,  -8.7284],
        [ -5.1344,   3.1314],
        [ -6.2799,   4.6823],
        [-13.0633,   8.4481],
        [ -7.0390,   5.2491],
        [  2.7151,  -4.5712],
        [  2.9662,  -4.7129],
        [  2.8170,  -5.2701],
        [ -0.4473,  -4.1679],
        [ -6.6029,   3.6191],
        [ -6.2578,   4.8530],
        [ -6.4922,   5.0381],
        [  3.4475,  -6.0090],
        [ -3.6740,   2.2842],
        [  1.3303,  -3.6860],
        [ -5.2424,   1.3755],
        [ -5.7571,   4.3694],
        [ -6.1982,   4.4337],
        [ -3.7502,   2.3568],
        [  3.7895,  -5.2354],
        [ -1.6725,  -2.8088],
        [  2.3808,  -3.7731],
        [  3.8357,  -5.4298],
        [ -3.7291,   2.3418],
        [  1.3058,  -5.1771],
        [ -5.6785,   3.9753],
        [  5.6064,  -7.7373],
        [ -1.7003,   0.1219],
        [ -4.6095,   3.0977],
        [ -3.2440,   1.8482],
        [  5.6788,  -7.0903],
        [ -3.6715,   2.1373],
        [  5.4384,  -7.4021],
        [  6.4306, -10.9337]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7124, 0.2876],
        [0.2430, 0.7570]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4643, 0.5357], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2059, 0.0902],
         [0.6171, 0.2521]],

        [[0.4719, 0.0920],
         [0.5408, 0.9211]],

        [[0.6931, 0.0996],
         [0.3586, 0.4639]],

        [[0.2389, 0.0958],
         [0.5261, 0.1865]],

        [[0.1589, 0.0920],
         [0.0148, 0.5938]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448436976567872
time is 1
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8822045595164437
time is 3
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448326530612245
Global Adjusted Rand Index: 0.890913405634346
Average Adjusted Rand Index: 0.8905364604818743
10895.850256308191
new:  [0.000307727139266984, 0.890913405634346, 0.890913405634346, 0.890913405634346] [-0.001545265084125451, 0.8905364604818743, 0.8905364604818743, 0.8905364604818743] [11049.261176533657, 10851.789548122719, 10851.836069292376, 10851.817545196076]
prior:  [0.8460832335055567, 0.8460832335055567, 0.8460832335055567, 0.0] [0.8459078967414421, 0.8459078967414421, 0.8459078967414421, 0.0] [10856.334235102522, 10856.334250635018, 10856.334273959352, 11051.287843131162]
-----------------------------------------------------------------------------------------
This iteration is 20
True Objective function: Loss = -10760.22633548985
Iteration 0: Loss = -19734.457521434477
Iteration 10: Loss = -10805.465639827362
Iteration 20: Loss = -10805.465231487657
Iteration 30: Loss = -10805.463700168671
Iteration 40: Loss = -10805.456804566176
Iteration 50: Loss = -10805.428171045158
Iteration 60: Loss = -10805.329793182053
Iteration 70: Loss = -10805.128254645424
Iteration 80: Loss = -10804.918118102258
Iteration 90: Loss = -10804.775924116404
Iteration 100: Loss = -10804.693743598285
Iteration 110: Loss = -10804.64851155421
Iteration 120: Loss = -10804.62384717326
Iteration 130: Loss = -10804.610376860486
Iteration 140: Loss = -10804.602972368157
Iteration 150: Loss = -10804.598993452519
Iteration 160: Loss = -10804.596927866161
Iteration 170: Loss = -10804.595954385908
Iteration 180: Loss = -10804.595661626854
Iteration 190: Loss = -10804.595772139472
1
Iteration 200: Loss = -10804.596049275218
2
Iteration 210: Loss = -10804.59649278332
3
Stopping early at iteration 209 due to no improvement.
pi: tensor([[9.6149e-01, 3.8513e-02],
        [9.9915e-01, 8.5417e-04]], dtype=torch.float64)
alpha: tensor([0.9631, 0.0369])
beta: tensor([[[0.1569, 0.1982],
         [0.8114, 0.1473]],

        [[0.4220, 0.1300],
         [0.8889, 0.5641]],

        [[0.0991, 0.1692],
         [0.1298, 0.2627]],

        [[0.7360, 0.1137],
         [0.5608, 0.9480]],

        [[0.9107, 0.1504],
         [0.9615, 0.0290]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19689.397485568064
Iteration 100: Loss = -10807.303950768368
Iteration 200: Loss = -10805.95076926506
Iteration 300: Loss = -10805.534821816416
Iteration 400: Loss = -10805.346630636155
Iteration 500: Loss = -10805.26447850687
Iteration 600: Loss = -10805.230791318207
Iteration 700: Loss = -10805.210004007818
Iteration 800: Loss = -10805.19013773428
Iteration 900: Loss = -10805.167092335283
Iteration 1000: Loss = -10805.135603184197
Iteration 1100: Loss = -10805.084314993843
Iteration 1200: Loss = -10804.98360446659
Iteration 1300: Loss = -10804.8228086174
Iteration 1400: Loss = -10804.763777863916
Iteration 1500: Loss = -10804.736554916859
Iteration 1600: Loss = -10804.705366909506
Iteration 1700: Loss = -10804.665713938908
Iteration 1800: Loss = -10804.613286211172
Iteration 1900: Loss = -10804.54809157821
Iteration 2000: Loss = -10804.478149730408
Iteration 2100: Loss = -10804.405141137117
Iteration 2200: Loss = -10804.332367897878
Iteration 2300: Loss = -10804.26609187047
Iteration 2400: Loss = -10804.207648641892
Iteration 2500: Loss = -10804.163979436387
Iteration 2600: Loss = -10804.132792913977
Iteration 2700: Loss = -10804.10941131683
Iteration 2800: Loss = -10804.091367175091
Iteration 2900: Loss = -10804.077038986576
Iteration 3000: Loss = -10804.065480715955
Iteration 3100: Loss = -10804.055999544093
Iteration 3200: Loss = -10804.047997094181
Iteration 3300: Loss = -10804.041616081531
Iteration 3400: Loss = -10804.035896009776
Iteration 3500: Loss = -10804.031327842775
Iteration 3600: Loss = -10804.027643777115
Iteration 3700: Loss = -10804.024305927474
Iteration 3800: Loss = -10804.021605664931
Iteration 3900: Loss = -10804.020269778966
Iteration 4000: Loss = -10804.017423930214
Iteration 4100: Loss = -10804.015735721714
Iteration 4200: Loss = -10804.014305288603
Iteration 4300: Loss = -10804.013084477174
Iteration 4400: Loss = -10804.012028619481
Iteration 4500: Loss = -10804.011029266176
Iteration 4600: Loss = -10804.01038794591
Iteration 4700: Loss = -10804.009687350013
Iteration 4800: Loss = -10804.009949326022
1
Iteration 4900: Loss = -10804.008185270155
Iteration 5000: Loss = -10804.007954998784
Iteration 5100: Loss = -10804.012848502836
1
Iteration 5200: Loss = -10804.00669173712
Iteration 5300: Loss = -10804.006577472808
Iteration 5400: Loss = -10804.005912968332
Iteration 5500: Loss = -10804.005589501812
Iteration 5600: Loss = -10804.00799596432
1
Iteration 5700: Loss = -10804.005035811293
Iteration 5800: Loss = -10804.004741110291
Iteration 5900: Loss = -10804.005132081158
1
Iteration 6000: Loss = -10804.004336021384
Iteration 6100: Loss = -10804.004091855208
Iteration 6200: Loss = -10804.00389861123
Iteration 6300: Loss = -10804.004264879213
1
Iteration 6400: Loss = -10804.003574388957
Iteration 6500: Loss = -10804.003456724791
Iteration 6600: Loss = -10804.004727752008
1
Iteration 6700: Loss = -10804.003149675791
Iteration 6800: Loss = -10804.003078576625
Iteration 6900: Loss = -10804.015044031032
1
Iteration 7000: Loss = -10804.002807320489
Iteration 7100: Loss = -10804.002718137082
Iteration 7200: Loss = -10804.005240718885
1
Iteration 7300: Loss = -10804.002487494647
Iteration 7400: Loss = -10804.00243579812
Iteration 7500: Loss = -10804.00373364575
1
Iteration 7600: Loss = -10804.00229388759
Iteration 7700: Loss = -10804.002165742122
Iteration 7800: Loss = -10804.002109576868
Iteration 7900: Loss = -10804.002212487587
1
Iteration 8000: Loss = -10804.001980831206
Iteration 8100: Loss = -10804.001908177785
Iteration 8200: Loss = -10804.001892341166
Iteration 8300: Loss = -10804.001741092223
Iteration 8400: Loss = -10804.001670422853
Iteration 8500: Loss = -10804.00246150157
1
Iteration 8600: Loss = -10804.001470134552
Iteration 8700: Loss = -10804.001377943465
Iteration 8800: Loss = -10804.001255583817
Iteration 8900: Loss = -10804.000972681095
Iteration 9000: Loss = -10804.000630510816
Iteration 9100: Loss = -10804.000180443913
Iteration 9200: Loss = -10803.998596172478
Iteration 9300: Loss = -10804.002021281649
1
Iteration 9400: Loss = -10803.994684182688
Iteration 9500: Loss = -10803.993872103283
Iteration 9600: Loss = -10803.993692067836
Iteration 9700: Loss = -10803.99344213137
Iteration 9800: Loss = -10804.0864481107
1
Iteration 9900: Loss = -10803.993425654651
Iteration 10000: Loss = -10803.993635920668
1
Iteration 10100: Loss = -10803.993448837893
2
Iteration 10200: Loss = -10804.004211269737
3
Iteration 10300: Loss = -10803.993403010865
Iteration 10400: Loss = -10803.995126324737
1
Iteration 10500: Loss = -10803.993448450925
2
Iteration 10600: Loss = -10803.993492532858
3
Iteration 10700: Loss = -10803.999697133497
4
Iteration 10800: Loss = -10803.993443954983
5
Iteration 10900: Loss = -10803.997985096737
6
Iteration 11000: Loss = -10803.994672285717
7
Iteration 11100: Loss = -10803.99341940031
8
Iteration 11200: Loss = -10803.99503425023
9
Iteration 11300: Loss = -10803.993427854575
10
Stopping early at iteration 11300 due to no improvement.
tensor([[-5.0602,  0.4449],
        [-5.2003,  0.5851],
        [-5.5710,  0.9558],
        [-5.7162,  1.1010],
        [-5.8808,  1.2656],
        [-5.7428,  1.1276],
        [-5.1941,  0.5789],
        [-5.9004,  1.2852],
        [-5.9115,  1.2963],
        [-6.0826,  1.4674],
        [-5.5468,  0.9316],
        [-4.8571,  0.2419],
        [-6.3735,  1.7583],
        [-5.9088,  1.2936],
        [-5.5653,  0.9501],
        [-5.0319,  0.4167],
        [-4.6734,  0.0582],
        [-5.4034,  0.7882],
        [-6.0737,  1.4585],
        [-5.4068,  0.7916],
        [-5.0528,  0.4376],
        [-5.8861,  1.2709],
        [-6.3788,  1.7636],
        [-4.5687, -0.0466],
        [-5.5295,  0.9143],
        [-5.2155,  0.6003],
        [-5.2010,  0.5857],
        [-5.1919,  0.5767],
        [-5.7207,  1.1054],
        [-4.5528, -0.0625],
        [-5.8795,  1.2643],
        [-5.8716,  1.2563],
        [-4.5443, -0.0710],
        [-4.6987,  0.0835],
        [-5.8913,  1.2760],
        [-4.6934,  0.0782],
        [-5.8757,  1.2605],
        [-4.7492,  0.1339],
        [-5.0501,  0.4349],
        [-4.5371, -0.0782],
        [-6.0510,  1.4358],
        [-5.7508,  1.1356],
        [-5.0811,  0.4659],
        [-6.0534,  1.4382],
        [-4.7139,  0.0987],
        [-5.0510,  0.4358],
        [-5.4401,  0.8249],
        [-6.5495,  1.9342],
        [-4.7409,  0.1256],
        [-5.5818,  0.9666],
        [-4.1994, -0.4158],
        [-4.7587,  0.1435],
        [-5.7278,  1.1126],
        [-4.8875,  0.2723],
        [-5.2251,  0.6099],
        [-6.2406,  1.6254],
        [-5.2391,  0.6239],
        [-4.0121, -0.6031],
        [-5.2081,  0.5929],
        [-4.6931,  0.0779],
        [-4.8897,  0.2745],
        [-4.9128,  0.2976],
        [-5.4047,  0.7895],
        [-5.9131,  1.2979],
        [-4.7326,  0.1174],
        [-5.5376,  0.9224],
        [-3.5250, -1.0902],
        [-4.1782, -0.4370],
        [-4.5374, -0.0778],
        [-5.7146,  1.0993],
        [-6.2043,  1.5891],
        [-5.2399,  0.6247],
        [-5.2439,  0.6287],
        [-5.3853,  0.7700],
        [-6.0785,  1.4632],
        [-5.3740,  0.7587],
        [-6.0597,  1.4445],
        [-4.8855,  0.2703],
        [-5.2109,  0.5956],
        [-6.2731,  1.6579],
        [-6.4153,  1.8001],
        [-5.9197,  1.3045],
        [-5.7125,  1.0973],
        [-5.0494,  0.4342],
        [-5.1997,  0.5844],
        [-4.5116, -0.1036],
        [-5.7090,  1.0938],
        [-5.0339,  0.4186],
        [-6.7266,  2.1114],
        [-5.8967,  1.2814],
        [-3.7175, -0.8978],
        [-5.0557,  0.4405],
        [-5.4060,  0.7908],
        [-5.0383,  0.4231],
        [-6.0571,  1.4418],
        [-4.8813,  0.2661],
        [-4.7338,  0.1186],
        [-5.5497,  0.9344],
        [-6.2414,  1.6262],
        [-5.2123,  0.5971]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9814, 0.0186],
        [0.9150, 0.0850]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0053, 0.9947], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1574, 0.2155],
         [0.8114, 0.1633]],

        [[0.4220, 0.1442],
         [0.8889, 0.5641]],

        [[0.0991, 0.1842],
         [0.1298, 0.2627]],

        [[0.7360, 0.1020],
         [0.5608, 0.9480]],

        [[0.9107, 0.1654],
         [0.9615, 0.0290]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0016114094845201586
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -39131.842527554785
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.0368,    nan]],

        [[0.5099,    nan],
         [0.5333, 0.5707]],

        [[0.3478,    nan],
         [0.0533, 0.7549]],

        [[0.0952,    nan],
         [0.8579, 0.0475]],

        [[0.3675,    nan],
         [0.5733, 0.6498]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -39131.98417144813
Iteration 100: Loss = -10839.405263562918
Iteration 200: Loss = -10816.309792486234
Iteration 300: Loss = -10811.185320615374
Iteration 400: Loss = -10808.802694335354
Iteration 500: Loss = -10807.538661179322
Iteration 600: Loss = -10806.818136257698
Iteration 700: Loss = -10806.391572631901
Iteration 800: Loss = -10806.129820744563
Iteration 900: Loss = -10805.958329589885
Iteration 1000: Loss = -10805.836865481439
Iteration 1100: Loss = -10805.747622980864
Iteration 1200: Loss = -10805.67962262191
Iteration 1300: Loss = -10805.625702470657
Iteration 1400: Loss = -10805.581645088716
Iteration 1500: Loss = -10805.544720231244
Iteration 1600: Loss = -10805.51304043602
Iteration 1700: Loss = -10805.485466736216
Iteration 1800: Loss = -10805.461169359769
Iteration 1900: Loss = -10805.43956660318
Iteration 2000: Loss = -10805.420199038777
Iteration 2100: Loss = -10805.402743468892
Iteration 2200: Loss = -10805.386947983658
Iteration 2300: Loss = -10805.372477833098
Iteration 2400: Loss = -10805.359160391148
Iteration 2500: Loss = -10805.34677585763
Iteration 2600: Loss = -10805.335167576499
Iteration 2700: Loss = -10805.32413879302
Iteration 2800: Loss = -10805.313625725275
Iteration 2900: Loss = -10805.30339889327
Iteration 3000: Loss = -10805.293167940641
Iteration 3100: Loss = -10805.282793837627
Iteration 3200: Loss = -10805.271722281415
Iteration 3300: Loss = -10805.259476992813
Iteration 3400: Loss = -10805.244953341433
Iteration 3500: Loss = -10805.226750054253
Iteration 3600: Loss = -10805.203864386445
Iteration 3700: Loss = -10805.179590091097
Iteration 3800: Loss = -10805.159806887177
Iteration 3900: Loss = -10805.143659316558
Iteration 4000: Loss = -10805.129636556469
Iteration 4100: Loss = -10805.117010491285
Iteration 4200: Loss = -10805.105341573664
Iteration 4300: Loss = -10805.094399346366
Iteration 4400: Loss = -10805.083934739296
Iteration 4500: Loss = -10805.073763948687
Iteration 4600: Loss = -10805.063790919203
Iteration 4700: Loss = -10805.06396773019
1
Iteration 4800: Loss = -10805.043817897451
Iteration 4900: Loss = -10805.033540031243
Iteration 5000: Loss = -10805.022881568086
Iteration 5100: Loss = -10805.033157639811
1
Iteration 5200: Loss = -10804.999831577672
Iteration 5300: Loss = -10804.987043452062
Iteration 5400: Loss = -10804.973204936927
Iteration 5500: Loss = -10804.983678197827
1
Iteration 5600: Loss = -10804.94123364818
Iteration 5700: Loss = -10804.922647689446
Iteration 5800: Loss = -10804.901927573674
Iteration 5900: Loss = -10804.878799820333
Iteration 6000: Loss = -10804.852895963473
Iteration 6100: Loss = -10804.82419936052
Iteration 6200: Loss = -10805.30092161194
1
Iteration 6300: Loss = -10804.757586242136
Iteration 6400: Loss = -10804.720660095263
Iteration 6500: Loss = -10804.682496473211
Iteration 6600: Loss = -10804.655318080995
Iteration 6700: Loss = -10804.606933417224
Iteration 6800: Loss = -10804.570104965347
Iteration 6900: Loss = -10804.531472172945
Iteration 7000: Loss = -10804.495136070555
Iteration 7100: Loss = -10804.458814472624
Iteration 7200: Loss = -10804.42117621528
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 21%|██        | 21/100 [15:41:26<56:25:37, 2571.35s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 22%|██▏       | 22/100 [16:32:32<58:55:46, 2719.83s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 23%|██▎       | 23/100 [17:17:22<57:59:01, 2710.93s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 24%|██▍       | 24/100 [18:08:49<59:36:48, 2823.80s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 25%|██▌       | 25/100 [18:56:56<59:13:20, 2842.67s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 26%|██▌       | 26/100 [19:51:52<61:13:34, 2978.57s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
