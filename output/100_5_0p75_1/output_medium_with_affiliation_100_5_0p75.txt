nohup: ignoring input
  0%|          | 0/100 [00:00<?, ?it/s]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/adj_generator.py:101: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)
  Y = torch.tensor(Y)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  1%|          | 1/100 [47:28<78:20:01, 2848.50s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  2%|▏         | 2/100 [1:33:19<75:58:28, 2790.90s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  3%|▎         | 3/100 [2:11:21<68:56:44, 2558.81s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  4%|▍         | 4/100 [3:02:26<73:33:47, 2758.62s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  5%|▌         | 5/100 [3:54:49<76:27:18, 2897.25s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  6%|▌         | 6/100 [4:45:55<77:08:47, 2954.55s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  7%|▋         | 7/100 [5:26:54<72:08:37, 2792.66s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  8%|▊         | 8/100 [6:01:13<65:23:36, 2558.87s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  9%|▉         | 9/100 [6:46:47<66:04:14, 2613.79s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 10%|█         | 10/100 [7:33:56<67:00:12, 2680.14s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 11%|█         | 11/100 [8:28:12<70:37:07, 2856.49s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 12%|█▏        | 12/100 [9:14:41<69:19:31, 2836.04s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 13%|█▎        | 13/100 [10:04:05<69:28:05, 2874.54s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 14%|█▍        | 14/100 [10:59:51<72:04:32, 3017.12s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 15%|█▌        | 15/100 [11:50:44<71:29:28, 3027.86s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 16%|█▌        | 16/100 [12:29:52<65:52:19, 2823.09s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 17%|█▋        | 17/100 [13:11:51<62:58:58, 2731.79s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 18%|█▊        | 18/100 [14:06:34<65:59:45, 2897.38s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 19%|█▉        | 19/100 [14:52:34<64:15:59, 2856.29s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 20%|██        | 20/100 [15:41:44<64:05:47, 2884.35s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 21%|██        | 21/100 [16:36:24<65:53:57, 3003.01s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 22%|██▏       | 22/100 [17:19:26<62:19:48, 2876.77s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 23%|██▎       | 23/100 [18:08:56<62:07:36, 2904.63s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 24%|██▍       | 24/100 [18:56:09<60:52:02, 2883.19s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 25%|██▌       | 25/100 [19:39:58<58:28:37, 2806.90s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 26%|██▌       | 26/100 [20:29:17<58:37:58, 2852.41s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 27%|██▋       | 27/100 [21:07:36<54:28:45, 2686.65s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 28%|██▊       | 28/100 [21:57:42<55:38:47, 2782.32s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 29%|██▉       | 29/100 [22:30:53<50:11:38, 2545.05s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 30%|███       | 30/100 [23:19:58<51:49:08, 2664.98s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 31%|███       | 31/100 [24:07:52<52:16:55, 2727.76s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 32%|███▏      | 32/100 [24:51:16<50:49:11, 2690.47s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 33%|███▎      | 33/100 [25:37:23<50:29:56, 2713.38s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 34%|███▍      | 34/100 [26:28:36<51:43:26, 2821.31s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 35%|███▌      | 35/100 [27:13:51<50:21:58, 2789.51s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
-------------------------------------
This iteration is 0
True Objective function: Loss = -11768.753638192185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26444.45703125
Iteration 100: Loss = -17642.43359375
Iteration 200: Loss = -13450.7958984375
Iteration 300: Loss = -12694.2685546875
Iteration 400: Loss = -12557.337890625
Iteration 500: Loss = -12479.1796875
Iteration 600: Loss = -12427.111328125
Iteration 700: Loss = -12407.228515625
Iteration 800: Loss = -12393.6337890625
Iteration 900: Loss = -12382.4443359375
Iteration 1000: Loss = -12371.2646484375
Iteration 1100: Loss = -12363.2177734375
Iteration 1200: Loss = -12358.046875
Iteration 1300: Loss = -12351.306640625
Iteration 1400: Loss = -12347.6796875
Iteration 1500: Loss = -12344.2294921875
Iteration 1600: Loss = -12337.44140625
Iteration 1700: Loss = -12334.8095703125
Iteration 1800: Loss = -12330.34765625
Iteration 1900: Loss = -12329.087890625
Iteration 2000: Loss = -12328.259765625
Iteration 2100: Loss = -12327.6103515625
Iteration 2200: Loss = -12327.072265625
Iteration 2300: Loss = -12326.619140625
Iteration 2400: Loss = -12326.2333984375
Iteration 2500: Loss = -12325.8984375
Iteration 2600: Loss = -12325.6044921875
Iteration 2700: Loss = -12325.34375
Iteration 2800: Loss = -12325.1103515625
Iteration 2900: Loss = -12324.90234375
Iteration 3000: Loss = -12324.7138671875
Iteration 3100: Loss = -12324.5400390625
Iteration 3200: Loss = -12324.384765625
Iteration 3300: Loss = -12324.2421875
Iteration 3400: Loss = -12324.1103515625
Iteration 3500: Loss = -12323.990234375
Iteration 3600: Loss = -12323.87890625
Iteration 3700: Loss = -12323.7763671875
Iteration 3800: Loss = -12323.681640625
Iteration 3900: Loss = -12323.5947265625
Iteration 4000: Loss = -12323.509765625
Iteration 4100: Loss = -12323.43359375
Iteration 4200: Loss = -12323.359375
Iteration 4300: Loss = -12323.2919921875
Iteration 4400: Loss = -12323.2265625
Iteration 4500: Loss = -12323.1640625
Iteration 4600: Loss = -12323.103515625
Iteration 4700: Loss = -12323.041015625
Iteration 4800: Loss = -12322.9775390625
Iteration 4900: Loss = -12322.904296875
Iteration 5000: Loss = -12322.798828125
Iteration 5100: Loss = -12322.560546875
Iteration 5200: Loss = -12321.9375
Iteration 5300: Loss = -12321.0302734375
Iteration 5400: Loss = -12320.740234375
Iteration 5500: Loss = -12320.591796875
Iteration 5600: Loss = -12320.486328125
Iteration 5700: Loss = -12320.40234375
Iteration 5800: Loss = -12320.3369140625
Iteration 5900: Loss = -12320.283203125
Iteration 6000: Loss = -12320.2412109375
Iteration 6100: Loss = -12320.2060546875
Iteration 6200: Loss = -12320.1728515625
Iteration 6300: Loss = -12320.142578125
Iteration 6400: Loss = -12320.1162109375
Iteration 6500: Loss = -12320.091796875
Iteration 6600: Loss = -12320.0732421875
Iteration 6700: Loss = -12320.05078125
Iteration 6800: Loss = -12320.0302734375
Iteration 6900: Loss = -12320.013671875
Iteration 7000: Loss = -12319.9970703125
Iteration 7100: Loss = -12319.9814453125
Iteration 7200: Loss = -12319.966796875
Iteration 7300: Loss = -12319.9521484375
Iteration 7400: Loss = -12319.939453125
Iteration 7500: Loss = -12319.9267578125
Iteration 7600: Loss = -12319.9140625
Iteration 7700: Loss = -12319.9033203125
Iteration 7800: Loss = -12319.892578125
Iteration 7900: Loss = -12319.8818359375
Iteration 8000: Loss = -12319.873046875
Iteration 8100: Loss = -12319.8642578125
Iteration 8200: Loss = -12319.8564453125
Iteration 8300: Loss = -12319.8486328125
Iteration 8400: Loss = -12319.83984375
Iteration 8500: Loss = -12319.8310546875
Iteration 8600: Loss = -12319.822265625
Iteration 8700: Loss = -12319.8154296875
Iteration 8800: Loss = -12319.8095703125
Iteration 8900: Loss = -12319.8017578125
Iteration 9000: Loss = -12319.796875
Iteration 9100: Loss = -12319.791015625
Iteration 9200: Loss = -12319.783203125
Iteration 9300: Loss = -12319.775390625
Iteration 9400: Loss = -12319.771484375
Iteration 9500: Loss = -12319.7646484375
Iteration 9600: Loss = -12319.7578125
Iteration 9700: Loss = -12319.751953125
Iteration 9800: Loss = -12319.7470703125
Iteration 9900: Loss = -12319.7421875
Iteration 10000: Loss = -12319.7392578125
Iteration 10100: Loss = -12319.734375
Iteration 10200: Loss = -12319.732421875
Iteration 10300: Loss = -12319.73046875
Iteration 10400: Loss = -12319.7265625
Iteration 10500: Loss = -12319.724609375
Iteration 10600: Loss = -12319.7236328125
Iteration 10700: Loss = -12319.7197265625
Iteration 10800: Loss = -12319.720703125
1
Iteration 10900: Loss = -12319.71875
Iteration 11000: Loss = -12319.7197265625
1
Iteration 11100: Loss = -12319.7177734375
Iteration 11200: Loss = -12319.7158203125
Iteration 11300: Loss = -12319.7158203125
Iteration 11400: Loss = -12319.7158203125
Iteration 11500: Loss = -12319.7158203125
Iteration 11600: Loss = -12319.712890625
Iteration 11700: Loss = -12319.7138671875
1
Iteration 11800: Loss = -12319.7138671875
2
Iteration 11900: Loss = -12319.7119140625
Iteration 12000: Loss = -12319.7119140625
Iteration 12100: Loss = -12319.7109375
Iteration 12200: Loss = -12319.7109375
Iteration 12300: Loss = -12319.7099609375
Iteration 12400: Loss = -12319.7099609375
Iteration 12500: Loss = -12319.708984375
Iteration 12600: Loss = -12319.7080078125
Iteration 12700: Loss = -12319.7099609375
1
Iteration 12800: Loss = -12319.7099609375
2
Iteration 12900: Loss = -12319.7080078125
Iteration 13000: Loss = -12319.7060546875
Iteration 13100: Loss = -12319.70703125
1
Iteration 13200: Loss = -12319.70703125
2
Iteration 13300: Loss = -12319.7060546875
Iteration 13400: Loss = -12319.70703125
1
Iteration 13500: Loss = -12319.7060546875
Iteration 13600: Loss = -12319.7060546875
Iteration 13700: Loss = -12319.7060546875
Iteration 13800: Loss = -12319.7060546875
Iteration 13900: Loss = -12319.7060546875
Iteration 14000: Loss = -12319.7041015625
Iteration 14100: Loss = -12319.703125
Iteration 14200: Loss = -12319.7041015625
1
Iteration 14300: Loss = -12319.703125
Iteration 14400: Loss = -12319.7021484375
Iteration 14500: Loss = -12319.7021484375
Iteration 14600: Loss = -12319.7001953125
Iteration 14700: Loss = -12319.701171875
1
Iteration 14800: Loss = -12319.7001953125
Iteration 14900: Loss = -12319.7001953125
Iteration 15000: Loss = -12319.6982421875
Iteration 15100: Loss = -12319.6982421875
Iteration 15200: Loss = -12319.6953125
Iteration 15300: Loss = -12319.6923828125
Iteration 15400: Loss = -12319.4951171875
Iteration 15500: Loss = -12319.4013671875
Iteration 15600: Loss = -12319.3466796875
Iteration 15700: Loss = -12319.2958984375
Iteration 15800: Loss = -12319.23046875
Iteration 15900: Loss = -12319.103515625
Iteration 16000: Loss = -12318.8447265625
Iteration 16100: Loss = -12318.7998046875
Iteration 16200: Loss = -12318.7763671875
Iteration 16300: Loss = -12318.7734375
Iteration 16400: Loss = -12318.767578125
Iteration 16500: Loss = -12318.7705078125
1
Iteration 16600: Loss = -12318.7626953125
Iteration 16700: Loss = -12318.7587890625
Iteration 16800: Loss = -12318.7587890625
Iteration 16900: Loss = -12318.759765625
1
Iteration 17000: Loss = -12318.7568359375
Iteration 17100: Loss = -12318.7568359375
Iteration 17200: Loss = -12318.744140625
Iteration 17300: Loss = -12318.7431640625
Iteration 17400: Loss = -12318.7431640625
Iteration 17500: Loss = -12318.744140625
1
Iteration 17600: Loss = -12318.744140625
2
Iteration 17700: Loss = -12318.7431640625
Iteration 17800: Loss = -12318.744140625
1
Iteration 17900: Loss = -12318.744140625
2
Iteration 18000: Loss = -12318.744140625
3
Iteration 18100: Loss = -12318.7451171875
4
Iteration 18200: Loss = -12318.7431640625
Iteration 18300: Loss = -12318.7431640625
Iteration 18400: Loss = -12318.7431640625
Iteration 18500: Loss = -12318.7431640625
Iteration 18600: Loss = -12318.744140625
1
Iteration 18700: Loss = -12318.7431640625
Iteration 18800: Loss = -12318.7431640625
Iteration 18900: Loss = -12318.744140625
1
Iteration 19000: Loss = -12318.7431640625
Iteration 19100: Loss = -12318.744140625
1
Iteration 19200: Loss = -12318.7421875
Iteration 19300: Loss = -12318.74609375
1
Iteration 19400: Loss = -12318.744140625
2
Iteration 19500: Loss = -12318.7421875
Iteration 19600: Loss = -12318.7431640625
1
Iteration 19700: Loss = -12318.7412109375
Iteration 19800: Loss = -12318.744140625
1
Iteration 19900: Loss = -12318.7431640625
2
Iteration 20000: Loss = -12318.7451171875
3
Iteration 20100: Loss = -12318.7421875
4
Iteration 20200: Loss = -12318.7421875
5
Iteration 20300: Loss = -12318.7431640625
6
Iteration 20400: Loss = -12318.7421875
7
Iteration 20500: Loss = -12318.7431640625
8
Iteration 20600: Loss = -12318.744140625
9
Iteration 20700: Loss = -12318.7431640625
10
Iteration 20800: Loss = -12318.7431640625
11
Iteration 20900: Loss = -12318.7431640625
12
Iteration 21000: Loss = -12318.7431640625
13
Iteration 21100: Loss = -12318.744140625
14
Iteration 21200: Loss = -12318.744140625
15
Stopping early at iteration 21200 due to no improvement.
pi: tensor([[9.4149e-01, 5.8512e-02],
        [9.9989e-01, 1.1176e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9703, 0.0297], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1955, 0.0941],
         [0.2720, 0.2940]],

        [[0.8607, 0.2646],
         [0.9796, 0.9342]],

        [[0.3303, 0.2177],
         [0.9175, 0.9864]],

        [[0.5033, 0.2401],
         [0.0760, 0.9923]],

        [[0.0273, 0.2292],
         [0.9321, 0.1168]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.018778022358394132
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.000561472629214886
Average Adjusted Rand Index: 0.002203483648593193
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30246.34765625
Iteration 100: Loss = -20918.48828125
Iteration 200: Loss = -14159.4501953125
Iteration 300: Loss = -12991.1396484375
Iteration 400: Loss = -12702.484375
Iteration 500: Loss = -12601.5810546875
Iteration 600: Loss = -12553.525390625
Iteration 700: Loss = -12522.1494140625
Iteration 800: Loss = -12497.7080078125
Iteration 900: Loss = -12478.2138671875
Iteration 1000: Loss = -12465.1669921875
Iteration 1100: Loss = -12452.880859375
Iteration 1200: Loss = -12443.7119140625
Iteration 1300: Loss = -12435.513671875
Iteration 1400: Loss = -12428.8828125
Iteration 1500: Loss = -12422.6201171875
Iteration 1600: Loss = -12414.08984375
Iteration 1700: Loss = -12407.91796875
Iteration 1800: Loss = -12404.330078125
Iteration 1900: Loss = -12399.8046875
Iteration 2000: Loss = -12397.0048828125
Iteration 2100: Loss = -12394.4150390625
Iteration 2200: Loss = -12391.0146484375
Iteration 2300: Loss = -12389.30859375
Iteration 2400: Loss = -12387.8046875
Iteration 2500: Loss = -12386.3798828125
Iteration 2600: Loss = -12384.3994140625
Iteration 2700: Loss = -12381.685546875
Iteration 2800: Loss = -12380.3740234375
Iteration 2900: Loss = -12378.37109375
Iteration 3000: Loss = -12376.7958984375
Iteration 3100: Loss = -12375.548828125
Iteration 3200: Loss = -12374.7724609375
Iteration 3300: Loss = -12374.0791015625
Iteration 3400: Loss = -12372.6259765625
Iteration 3500: Loss = -12371.0576171875
Iteration 3600: Loss = -12370.4326171875
Iteration 3700: Loss = -12369.7724609375
Iteration 3800: Loss = -12368.8525390625
Iteration 3900: Loss = -12367.2412109375
Iteration 4000: Loss = -12365.3896484375
Iteration 4100: Loss = -12363.1279296875
Iteration 4200: Loss = -12361.8720703125
Iteration 4300: Loss = -12360.4130859375
Iteration 4400: Loss = -12359.8818359375
Iteration 4500: Loss = -12357.8076171875
Iteration 4600: Loss = -12357.5009765625
Iteration 4700: Loss = -12356.9462890625
Iteration 4800: Loss = -12356.0263671875
Iteration 4900: Loss = -12355.177734375
Iteration 5000: Loss = -12353.2177734375
Iteration 5100: Loss = -12352.7431640625
Iteration 5200: Loss = -12351.8388671875
Iteration 5300: Loss = -12351.2890625
Iteration 5400: Loss = -12350.302734375
Iteration 5500: Loss = -12350.0703125
Iteration 5600: Loss = -12349.92578125
Iteration 5700: Loss = -12349.7998046875
Iteration 5800: Loss = -12349.671875
Iteration 5900: Loss = -12349.3583984375
Iteration 6000: Loss = -12347.6923828125
Iteration 6100: Loss = -12347.4599609375
Iteration 6200: Loss = -12345.8701171875
Iteration 6300: Loss = -12345.2705078125
Iteration 6400: Loss = -12345.1357421875
Iteration 6500: Loss = -12343.9921875
Iteration 6600: Loss = -12343.9033203125
Iteration 6700: Loss = -12343.8369140625
Iteration 6800: Loss = -12343.77734375
Iteration 6900: Loss = -12343.72265625
Iteration 7000: Loss = -12343.669921875
Iteration 7100: Loss = -12343.62109375
Iteration 7200: Loss = -12343.572265625
Iteration 7300: Loss = -12343.5224609375
Iteration 7400: Loss = -12343.474609375
Iteration 7500: Loss = -12343.42578125
Iteration 7600: Loss = -12343.375
Iteration 7700: Loss = -12343.2509765625
Iteration 7800: Loss = -12342.3583984375
Iteration 7900: Loss = -12342.2998046875
Iteration 8000: Loss = -12341.79296875
Iteration 8100: Loss = -12341.05859375
Iteration 8200: Loss = -12340.181640625
Iteration 8300: Loss = -12340.0546875
Iteration 8400: Loss = -12340.013671875
Iteration 8500: Loss = -12339.9794921875
Iteration 8600: Loss = -12338.560546875
Iteration 8700: Loss = -12338.5380859375
Iteration 8800: Loss = -12338.51953125
Iteration 8900: Loss = -12338.5048828125
Iteration 9000: Loss = -12338.490234375
Iteration 9100: Loss = -12338.474609375
Iteration 9200: Loss = -12338.4619140625
Iteration 9300: Loss = -12338.451171875
Iteration 9400: Loss = -12337.18359375
Iteration 9500: Loss = -12337.08984375
Iteration 9600: Loss = -12337.0732421875
Iteration 9700: Loss = -12337.0615234375
Iteration 9800: Loss = -12337.05078125
Iteration 9900: Loss = -12337.0419921875
Iteration 10000: Loss = -12337.033203125
Iteration 10100: Loss = -12337.02734375
Iteration 10200: Loss = -12337.0185546875
Iteration 10300: Loss = -12337.013671875
Iteration 10400: Loss = -12337.005859375
Iteration 10500: Loss = -12336.998046875
Iteration 10600: Loss = -12336.994140625
Iteration 10700: Loss = -12336.98828125
Iteration 10800: Loss = -12336.9833984375
Iteration 10900: Loss = -12336.9775390625
Iteration 11000: Loss = -12336.9736328125
Iteration 11100: Loss = -12335.8818359375
Iteration 11200: Loss = -12335.85546875
Iteration 11300: Loss = -12334.6533203125
Iteration 11400: Loss = -12334.630859375
Iteration 11500: Loss = -12334.6259765625
Iteration 11600: Loss = -12334.6220703125
Iteration 11700: Loss = -12334.619140625
Iteration 11800: Loss = -12334.6171875
Iteration 11900: Loss = -12334.61328125
Iteration 12000: Loss = -12334.611328125
Iteration 12100: Loss = -12334.6064453125
Iteration 12200: Loss = -12334.3369140625
Iteration 12300: Loss = -12333.1162109375
Iteration 12400: Loss = -12333.111328125
Iteration 12500: Loss = -12333.1064453125
Iteration 12600: Loss = -12333.1044921875
Iteration 12700: Loss = -12333.1005859375
Iteration 12800: Loss = -12333.0986328125
Iteration 12900: Loss = -12333.0966796875
Iteration 13000: Loss = -12333.095703125
Iteration 13100: Loss = -12333.0947265625
Iteration 13200: Loss = -12333.0927734375
Iteration 13300: Loss = -12333.091796875
Iteration 13400: Loss = -12333.0927734375
1
Iteration 13500: Loss = -12333.087890625
Iteration 13600: Loss = -12333.0849609375
Iteration 13700: Loss = -12333.0869140625
1
Iteration 13800: Loss = -12333.0849609375
Iteration 13900: Loss = -12333.083984375
Iteration 14000: Loss = -12333.08203125
Iteration 14100: Loss = -12331.712890625
Iteration 14200: Loss = -12331.7060546875
Iteration 14300: Loss = -12331.705078125
Iteration 14400: Loss = -12331.7060546875
1
Iteration 14500: Loss = -12330.4052734375
Iteration 14600: Loss = -12330.396484375
Iteration 14700: Loss = -12330.3935546875
Iteration 14800: Loss = -12330.3955078125
1
Iteration 14900: Loss = -12330.39453125
2
Iteration 15000: Loss = -12330.39453125
3
Iteration 15100: Loss = -12330.39453125
4
Iteration 15200: Loss = -12330.392578125
Iteration 15300: Loss = -12330.392578125
Iteration 15400: Loss = -12330.390625
Iteration 15500: Loss = -12329.4638671875
Iteration 15600: Loss = -12328.212890625
Iteration 15700: Loss = -12328.212890625
Iteration 15800: Loss = -12328.208984375
Iteration 15900: Loss = -12328.2099609375
1
Iteration 16000: Loss = -12328.2099609375
2
Iteration 16100: Loss = -12328.2099609375
3
Iteration 16200: Loss = -12328.2080078125
Iteration 16300: Loss = -12328.208984375
1
Iteration 16400: Loss = -12328.208984375
2
Iteration 16500: Loss = -12328.2060546875
Iteration 16600: Loss = -12326.7783203125
Iteration 16700: Loss = -12326.775390625
Iteration 16800: Loss = -12326.775390625
Iteration 16900: Loss = -12326.77734375
1
Iteration 17000: Loss = -12326.7744140625
Iteration 17100: Loss = -12326.775390625
1
Iteration 17200: Loss = -12326.7734375
Iteration 17300: Loss = -12326.7734375
Iteration 17400: Loss = -12326.697265625
Iteration 17500: Loss = -12326.6875
Iteration 17600: Loss = -12326.689453125
1
Iteration 17700: Loss = -12326.6884765625
2
Iteration 17800: Loss = -12326.6884765625
3
Iteration 17900: Loss = -12326.6875
Iteration 18000: Loss = -12325.14453125
Iteration 18100: Loss = -12325.1376953125
Iteration 18200: Loss = -12325.13671875
Iteration 18300: Loss = -12325.13671875
Iteration 18400: Loss = -12325.13671875
Iteration 18500: Loss = -12325.13671875
Iteration 18600: Loss = -12325.13671875
Iteration 18700: Loss = -12325.1376953125
1
Iteration 18800: Loss = -12325.13671875
Iteration 18900: Loss = -12325.13671875
Iteration 19000: Loss = -12325.13671875
Iteration 19100: Loss = -12325.13671875
Iteration 19200: Loss = -12325.13671875
Iteration 19300: Loss = -12325.138671875
1
Iteration 19400: Loss = -12325.13671875
Iteration 19500: Loss = -12325.1357421875
Iteration 19600: Loss = -12325.1357421875
Iteration 19700: Loss = -12325.13671875
1
Iteration 19800: Loss = -12323.4951171875
Iteration 19900: Loss = -12323.490234375
Iteration 20000: Loss = -12321.55859375
Iteration 20100: Loss = -12321.55859375
Iteration 20200: Loss = -12321.556640625
Iteration 20300: Loss = -12321.5576171875
1
Iteration 20400: Loss = -12321.5576171875
2
Iteration 20500: Loss = -12321.556640625
Iteration 20600: Loss = -12321.5556640625
Iteration 20700: Loss = -12321.5546875
Iteration 20800: Loss = -12321.556640625
1
Iteration 20900: Loss = -12321.5556640625
2
Iteration 21000: Loss = -12321.5546875
Iteration 21100: Loss = -12321.5556640625
1
Iteration 21200: Loss = -12321.5556640625
2
Iteration 21300: Loss = -12321.556640625
3
Iteration 21400: Loss = -12319.978515625
Iteration 21500: Loss = -12319.3486328125
Iteration 21600: Loss = -12318.9052734375
Iteration 21700: Loss = -12318.880859375
Iteration 21800: Loss = -12318.87890625
Iteration 21900: Loss = -12318.876953125
Iteration 22000: Loss = -12318.8759765625
Iteration 22100: Loss = -12318.875
Iteration 22200: Loss = -12318.8740234375
Iteration 22300: Loss = -12318.8740234375
Iteration 22400: Loss = -12318.8740234375
Iteration 22500: Loss = -12318.8740234375
Iteration 22600: Loss = -12318.8740234375
Iteration 22700: Loss = -12318.873046875
Iteration 22800: Loss = -12318.873046875
Iteration 22900: Loss = -12318.8740234375
1
Iteration 23000: Loss = -12318.8740234375
2
Iteration 23100: Loss = -12318.873046875
Iteration 23200: Loss = -12318.8740234375
1
Iteration 23300: Loss = -12318.873046875
Iteration 23400: Loss = -12318.8740234375
1
Iteration 23500: Loss = -12318.8720703125
Iteration 23600: Loss = -12318.873046875
1
Iteration 23700: Loss = -12318.873046875
2
Iteration 23800: Loss = -12318.873046875
3
Iteration 23900: Loss = -12318.873046875
4
Iteration 24000: Loss = -12318.873046875
5
Iteration 24100: Loss = -12318.8740234375
6
Iteration 24200: Loss = -12318.873046875
7
Iteration 24300: Loss = -12318.873046875
8
Iteration 24400: Loss = -12318.873046875
9
Iteration 24500: Loss = -12318.873046875
10
Iteration 24600: Loss = -12318.8720703125
Iteration 24700: Loss = -12318.8720703125
Iteration 24800: Loss = -12318.873046875
1
Iteration 24900: Loss = -12318.873046875
2
Iteration 25000: Loss = -12318.8720703125
Iteration 25100: Loss = -12318.8720703125
Iteration 25200: Loss = -12318.873046875
1
Iteration 25300: Loss = -12318.8740234375
2
Iteration 25400: Loss = -12318.8740234375
3
Iteration 25500: Loss = -12318.87109375
Iteration 25600: Loss = -12318.8720703125
1
Iteration 25700: Loss = -12318.87109375
Iteration 25800: Loss = -12318.873046875
1
Iteration 25900: Loss = -12318.873046875
2
Iteration 26000: Loss = -12318.8720703125
3
Iteration 26100: Loss = -12318.8740234375
4
Iteration 26200: Loss = -12318.87109375
Iteration 26300: Loss = -12318.8720703125
1
Iteration 26400: Loss = -12318.8720703125
2
Iteration 26500: Loss = -12318.8720703125
3
Iteration 26600: Loss = -12318.873046875
4
Iteration 26700: Loss = -12318.873046875
5
Iteration 26800: Loss = -12318.873046875
6
Iteration 26900: Loss = -12318.8740234375
7
Iteration 27000: Loss = -12318.8720703125
8
Iteration 27100: Loss = -12318.87109375
Iteration 27200: Loss = -12318.873046875
1
Iteration 27300: Loss = -12318.873046875
2
Iteration 27400: Loss = -12318.8720703125
3
Iteration 27500: Loss = -12318.873046875
4
Iteration 27600: Loss = -12318.873046875
5
Iteration 27700: Loss = -12318.8720703125
6
Iteration 27800: Loss = -12318.873046875
7
Iteration 27900: Loss = -12318.873046875
8
Iteration 28000: Loss = -12318.8720703125
9
Iteration 28100: Loss = -12318.873046875
10
Iteration 28200: Loss = -12318.8720703125
11
Iteration 28300: Loss = -12318.87109375
Iteration 28400: Loss = -12318.8720703125
1
Iteration 28500: Loss = -12318.873046875
2
Iteration 28600: Loss = -12318.87109375
Iteration 28700: Loss = -12318.8720703125
1
Iteration 28800: Loss = -12318.873046875
2
Iteration 28900: Loss = -12318.873046875
3
Iteration 29000: Loss = -12318.8720703125
4
Iteration 29100: Loss = -12318.8720703125
5
Iteration 29200: Loss = -12318.8720703125
6
Iteration 29300: Loss = -12318.873046875
7
Iteration 29400: Loss = -12318.8720703125
8
Iteration 29500: Loss = -12318.8720703125
9
Iteration 29600: Loss = -12318.87109375
Iteration 29700: Loss = -12318.8720703125
1
Iteration 29800: Loss = -12318.87109375
Iteration 29900: Loss = -12318.873046875
1
pi: tensor([[2.9765e-02, 9.7024e-01],
        [4.1826e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9843, 0.0157], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1909, 0.0872],
         [0.1835, 0.1995]],

        [[0.7302, 0.2880],
         [0.0550, 0.9660]],

        [[0.2361, 0.2540],
         [0.9186, 0.9721]],

        [[0.0479, 0.3114],
         [0.0281, 0.1190]],

        [[0.2551, 0.2023],
         [0.6053, 0.0844]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.005453571602473584
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.002564548911064873
Average Adjusted Rand Index: 0.00023372449724886785
[0.000561472629214886, -0.002564548911064873] [0.002203483648593193, 0.00023372449724886785] [12318.744140625, 12318.873046875]
-------------------------------------
This iteration is 1
True Objective function: Loss = -11956.544653817185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -43827.8984375
Iteration 100: Loss = -27161.94140625
Iteration 200: Loss = -16389.0078125
Iteration 300: Loss = -13286.4736328125
Iteration 400: Loss = -12868.8251953125
Iteration 500: Loss = -12728.2529296875
Iteration 600: Loss = -12641.396484375
Iteration 700: Loss = -12593.4873046875
Iteration 800: Loss = -12556.8779296875
Iteration 900: Loss = -12530.6083984375
Iteration 1000: Loss = -12514.9951171875
Iteration 1100: Loss = -12505.6025390625
Iteration 1200: Loss = -12498.10546875
Iteration 1300: Loss = -12491.3828125
Iteration 1400: Loss = -12485.830078125
Iteration 1500: Loss = -12481.7802734375
Iteration 1600: Loss = -12478.609375
Iteration 1700: Loss = -12475.9990234375
Iteration 1800: Loss = -12473.8017578125
Iteration 1900: Loss = -12471.921875
Iteration 2000: Loss = -12470.296875
Iteration 2100: Loss = -12468.8798828125
Iteration 2200: Loss = -12467.62890625
Iteration 2300: Loss = -12463.658203125
Iteration 2400: Loss = -12460.05859375
Iteration 2500: Loss = -12459.0966796875
Iteration 2600: Loss = -12458.29296875
Iteration 2700: Loss = -12457.583984375
Iteration 2800: Loss = -12456.94921875
Iteration 2900: Loss = -12456.3759765625
Iteration 3000: Loss = -12455.8525390625
Iteration 3100: Loss = -12455.3798828125
Iteration 3200: Loss = -12454.951171875
Iteration 3300: Loss = -12454.5546875
Iteration 3400: Loss = -12453.970703125
Iteration 3500: Loss = -12451.0224609375
Iteration 3600: Loss = -12450.517578125
Iteration 3700: Loss = -12450.169921875
Iteration 3800: Loss = -12449.876953125
Iteration 3900: Loss = -12449.6162109375
Iteration 4000: Loss = -12449.3798828125
Iteration 4100: Loss = -12449.1630859375
Iteration 4200: Loss = -12448.431640625
Iteration 4300: Loss = -12443.4912109375
Iteration 4400: Loss = -12443.1767578125
Iteration 4500: Loss = -12442.9560546875
Iteration 4600: Loss = -12442.767578125
Iteration 4700: Loss = -12442.6015625
Iteration 4800: Loss = -12442.4482421875
Iteration 4900: Loss = -12442.3095703125
Iteration 5000: Loss = -12442.1796875
Iteration 5100: Loss = -12442.05859375
Iteration 5200: Loss = -12441.94921875
Iteration 5300: Loss = -12441.8466796875
Iteration 5400: Loss = -12441.7509765625
Iteration 5500: Loss = -12441.6630859375
Iteration 5600: Loss = -12441.578125
Iteration 5700: Loss = -12441.4990234375
Iteration 5800: Loss = -12441.4248046875
Iteration 5900: Loss = -12441.3544921875
Iteration 6000: Loss = -12441.2880859375
Iteration 6100: Loss = -12441.2275390625
Iteration 6200: Loss = -12441.16796875
Iteration 6300: Loss = -12441.115234375
Iteration 6400: Loss = -12441.0654296875
Iteration 6500: Loss = -12441.0185546875
Iteration 6600: Loss = -12440.9736328125
Iteration 6700: Loss = -12440.9306640625
Iteration 6800: Loss = -12440.890625
Iteration 6900: Loss = -12440.853515625
Iteration 7000: Loss = -12440.8173828125
Iteration 7100: Loss = -12440.78515625
Iteration 7200: Loss = -12440.7529296875
Iteration 7300: Loss = -12440.72265625
Iteration 7400: Loss = -12440.693359375
Iteration 7500: Loss = -12440.6669921875
Iteration 7600: Loss = -12440.642578125
Iteration 7700: Loss = -12440.619140625
Iteration 7800: Loss = -12440.59375
Iteration 7900: Loss = -12440.5732421875
Iteration 8000: Loss = -12440.552734375
Iteration 8100: Loss = -12440.53125
Iteration 8200: Loss = -12440.513671875
Iteration 8300: Loss = -12440.4931640625
Iteration 8400: Loss = -12440.4775390625
Iteration 8500: Loss = -12440.4609375
Iteration 8600: Loss = -12440.4453125
Iteration 8700: Loss = -12440.427734375
Iteration 8800: Loss = -12440.41015625
Iteration 8900: Loss = -12440.3955078125
Iteration 9000: Loss = -12440.3818359375
Iteration 9100: Loss = -12440.3671875
Iteration 9200: Loss = -12440.3544921875
Iteration 9300: Loss = -12440.3447265625
Iteration 9400: Loss = -12440.3330078125
Iteration 9500: Loss = -12440.322265625
Iteration 9600: Loss = -12440.3154296875
Iteration 9700: Loss = -12440.3056640625
Iteration 9800: Loss = -12440.296875
Iteration 9900: Loss = -12440.283203125
Iteration 10000: Loss = -12440.2666015625
Iteration 10100: Loss = -12440.2490234375
Iteration 10200: Loss = -12440.234375
Iteration 10300: Loss = -12440.2275390625
Iteration 10400: Loss = -12440.2197265625
Iteration 10500: Loss = -12440.2119140625
Iteration 10600: Loss = -12440.203125
Iteration 10700: Loss = -12440.1962890625
Iteration 10800: Loss = -12440.1884765625
Iteration 10900: Loss = -12440.1796875
Iteration 11000: Loss = -12440.1708984375
Iteration 11100: Loss = -12440.1591796875
Iteration 11200: Loss = -12440.1474609375
Iteration 11300: Loss = -12440.1318359375
Iteration 11400: Loss = -12440.109375
Iteration 11500: Loss = -12440.0751953125
Iteration 11600: Loss = -12440.0595703125
Iteration 11700: Loss = -12440.0498046875
Iteration 11800: Loss = -12440.0400390625
Iteration 11900: Loss = -12440.0283203125
Iteration 12000: Loss = -12440.0166015625
Iteration 12100: Loss = -12440.0087890625
Iteration 12200: Loss = -12439.99609375
Iteration 12300: Loss = -12439.982421875
Iteration 12400: Loss = -12439.9716796875
Iteration 12500: Loss = -12439.9580078125
Iteration 12600: Loss = -12439.943359375
Iteration 12700: Loss = -12439.927734375
Iteration 12800: Loss = -12439.9091796875
Iteration 12900: Loss = -12439.8876953125
Iteration 13000: Loss = -12439.8603515625
Iteration 13100: Loss = -12439.826171875
Iteration 13200: Loss = -12439.78515625
Iteration 13300: Loss = -12439.732421875
Iteration 13400: Loss = -12439.6669921875
Iteration 13500: Loss = -12439.5966796875
Iteration 13600: Loss = -12439.5283203125
Iteration 13700: Loss = -12439.4677734375
Iteration 13800: Loss = -12439.427734375
Iteration 13900: Loss = -12439.39453125
Iteration 14000: Loss = -12439.3642578125
Iteration 14100: Loss = -12439.341796875
Iteration 14200: Loss = -12439.3212890625
Iteration 14300: Loss = -12439.2939453125
Iteration 14400: Loss = -12439.2685546875
Iteration 14500: Loss = -12439.248046875
Iteration 14600: Loss = -12439.2236328125
Iteration 14700: Loss = -12439.1953125
Iteration 14800: Loss = -12439.166015625
Iteration 14900: Loss = -12439.13671875
Iteration 15000: Loss = -12439.119140625
Iteration 15100: Loss = -12439.1103515625
Iteration 15200: Loss = -12439.1044921875
Iteration 15300: Loss = -12439.09765625
Iteration 15400: Loss = -12439.0888671875
Iteration 15500: Loss = -12439.0859375
Iteration 15600: Loss = -12439.0849609375
Iteration 15700: Loss = -12439.0830078125
Iteration 15800: Loss = -12439.0830078125
Iteration 15900: Loss = -12439.0830078125
Iteration 16000: Loss = -12439.0810546875
Iteration 16100: Loss = -12439.0810546875
Iteration 16200: Loss = -12439.080078125
Iteration 16300: Loss = -12439.08203125
1
Iteration 16400: Loss = -12439.08203125
2
Iteration 16500: Loss = -12439.08203125
3
Iteration 16600: Loss = -12439.0791015625
Iteration 16700: Loss = -12439.0810546875
1
Iteration 16800: Loss = -12439.0791015625
Iteration 16900: Loss = -12439.0791015625
Iteration 17000: Loss = -12439.0791015625
Iteration 17100: Loss = -12439.080078125
1
Iteration 17200: Loss = -12439.080078125
2
Iteration 17300: Loss = -12439.080078125
3
Iteration 17400: Loss = -12439.0791015625
Iteration 17500: Loss = -12439.0791015625
Iteration 17600: Loss = -12439.078125
Iteration 17700: Loss = -12439.080078125
1
Iteration 17800: Loss = -12439.0791015625
2
Iteration 17900: Loss = -12439.080078125
3
Iteration 18000: Loss = -12439.078125
Iteration 18100: Loss = -12439.0791015625
1
Iteration 18200: Loss = -12439.0791015625
2
Iteration 18300: Loss = -12439.080078125
3
Iteration 18400: Loss = -12439.080078125
4
Iteration 18500: Loss = -12439.0791015625
5
Iteration 18600: Loss = -12439.0791015625
6
Iteration 18700: Loss = -12439.0791015625
7
Iteration 18800: Loss = -12439.080078125
8
Iteration 18900: Loss = -12439.0791015625
9
Iteration 19000: Loss = -12439.0791015625
10
Iteration 19100: Loss = -12439.080078125
11
Iteration 19200: Loss = -12439.080078125
12
Iteration 19300: Loss = -12439.0810546875
13
Iteration 19400: Loss = -12439.0791015625
14
Iteration 19500: Loss = -12439.080078125
15
Stopping early at iteration 19500 due to no improvement.
pi: tensor([[9.9994e-01, 5.9200e-05],
        [9.8881e-01, 1.1191e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9130, 0.0870], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1998, 0.2472],
         [0.9795, 0.3549]],

        [[0.8804, 0.1938],
         [0.0199, 0.0320]],

        [[0.0105, 0.1871],
         [0.8924, 0.0215]],

        [[0.0341, 0.2742],
         [0.1682, 0.9929]],

        [[0.0070, 0.1746],
         [0.0473, 0.9695]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001783122334233028
Average Adjusted Rand Index: -0.0015521208230856337
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25022.3046875
Iteration 100: Loss = -17476.150390625
Iteration 200: Loss = -13694.71875
Iteration 300: Loss = -12877.44921875
Iteration 400: Loss = -12683.08984375
Iteration 500: Loss = -12627.955078125
Iteration 600: Loss = -12597.1259765625
Iteration 700: Loss = -12577.3232421875
Iteration 800: Loss = -12560.470703125
Iteration 900: Loss = -12547.296875
Iteration 1000: Loss = -12536.5537109375
Iteration 1100: Loss = -12522.2587890625
Iteration 1200: Loss = -12507.0732421875
Iteration 1300: Loss = -12484.9765625
Iteration 1400: Loss = -12443.9375
Iteration 1500: Loss = -12409.841796875
Iteration 1600: Loss = -12391.6455078125
Iteration 1700: Loss = -12382.6865234375
Iteration 1800: Loss = -12363.6875
Iteration 1900: Loss = -12351.111328125
Iteration 2000: Loss = -12334.203125
Iteration 2100: Loss = -12328.5458984375
Iteration 2200: Loss = -12315.734375
Iteration 2300: Loss = -12311.611328125
Iteration 2400: Loss = -12304.150390625
Iteration 2500: Loss = -12302.9912109375
Iteration 2600: Loss = -12302.30078125
Iteration 2700: Loss = -12301.796875
Iteration 2800: Loss = -12301.404296875
Iteration 2900: Loss = -12301.087890625
Iteration 3000: Loss = -12300.8310546875
Iteration 3100: Loss = -12300.6181640625
Iteration 3200: Loss = -12300.4375
Iteration 3300: Loss = -12300.2822265625
Iteration 3400: Loss = -12300.1474609375
Iteration 3500: Loss = -12300.0283203125
Iteration 3600: Loss = -12299.919921875
Iteration 3700: Loss = -12299.822265625
Iteration 3800: Loss = -12299.6748046875
Iteration 3900: Loss = -12296.583984375
Iteration 4000: Loss = -12296.4658203125
Iteration 4100: Loss = -12296.384765625
Iteration 4200: Loss = -12296.314453125
Iteration 4300: Loss = -12296.2578125
Iteration 4400: Loss = -12296.2041015625
Iteration 4500: Loss = -12296.15625
Iteration 4600: Loss = -12296.1142578125
Iteration 4700: Loss = -12296.07421875
Iteration 4800: Loss = -12296.0380859375
Iteration 4900: Loss = -12296.00390625
Iteration 5000: Loss = -12295.97265625
Iteration 5100: Loss = -12295.9384765625
Iteration 5200: Loss = -12292.3916015625
Iteration 5300: Loss = -12291.662109375
Iteration 5400: Loss = -12291.60546875
Iteration 5500: Loss = -12288.8515625
Iteration 5600: Loss = -12288.265625
Iteration 5700: Loss = -12288.2275390625
Iteration 5800: Loss = -12288.2001953125
Iteration 5900: Loss = -12288.1767578125
Iteration 6000: Loss = -12288.15625
Iteration 6100: Loss = -12288.138671875
Iteration 6200: Loss = -12288.1181640625
Iteration 6300: Loss = -12288.1015625
Iteration 6400: Loss = -12288.083984375
Iteration 6500: Loss = -12288.0654296875
Iteration 6600: Loss = -12288.048828125
Iteration 6700: Loss = -12288.037109375
Iteration 6800: Loss = -12288.0244140625
Iteration 6900: Loss = -12288.015625
Iteration 7000: Loss = -12288.005859375
Iteration 7100: Loss = -12287.9970703125
Iteration 7200: Loss = -12287.990234375
Iteration 7300: Loss = -12287.9814453125
Iteration 7400: Loss = -12287.9755859375
Iteration 7500: Loss = -12287.9697265625
Iteration 7600: Loss = -12287.9619140625
Iteration 7700: Loss = -12287.95703125
Iteration 7800: Loss = -12287.951171875
Iteration 7900: Loss = -12287.9462890625
Iteration 8000: Loss = -12287.9423828125
Iteration 8100: Loss = -12287.9375
Iteration 8200: Loss = -12287.9326171875
Iteration 8300: Loss = -12287.9287109375
Iteration 8400: Loss = -12287.9248046875
Iteration 8500: Loss = -12287.921875
Iteration 8600: Loss = -12287.91796875
Iteration 8700: Loss = -12287.9150390625
Iteration 8800: Loss = -12287.912109375
Iteration 8900: Loss = -12287.908203125
Iteration 9000: Loss = -12287.9052734375
Iteration 9100: Loss = -12287.904296875
Iteration 9200: Loss = -12287.900390625
Iteration 9300: Loss = -12287.8984375
Iteration 9400: Loss = -12287.8955078125
Iteration 9500: Loss = -12287.892578125
Iteration 9600: Loss = -12287.8896484375
Iteration 9700: Loss = -12287.8828125
Iteration 9800: Loss = -12287.822265625
Iteration 9900: Loss = -12287.486328125
Iteration 10000: Loss = -12287.4814453125
Iteration 10100: Loss = -12286.806640625
Iteration 10200: Loss = -12277.56640625
Iteration 10300: Loss = -12277.4970703125
Iteration 10400: Loss = -12277.4736328125
Iteration 10500: Loss = -12277.458984375
Iteration 10600: Loss = -12277.4501953125
Iteration 10700: Loss = -12277.4453125
Iteration 10800: Loss = -12277.4404296875
Iteration 10900: Loss = -12277.435546875
Iteration 11000: Loss = -12277.431640625
Iteration 11100: Loss = -12277.4287109375
Iteration 11200: Loss = -12277.4267578125
Iteration 11300: Loss = -12277.423828125
Iteration 11400: Loss = -12277.4228515625
Iteration 11500: Loss = -12277.4208984375
Iteration 11600: Loss = -12277.419921875
Iteration 11700: Loss = -12275.392578125
Iteration 11800: Loss = -12274.8193359375
Iteration 11900: Loss = -12274.814453125
Iteration 12000: Loss = -12273.966796875
Iteration 12100: Loss = -12268.8125
Iteration 12200: Loss = -12268.7587890625
Iteration 12300: Loss = -12268.7470703125
Iteration 12400: Loss = -12268.7412109375
Iteration 12500: Loss = -12268.689453125
Iteration 12600: Loss = -12258.490234375
Iteration 12700: Loss = -12249.4111328125
Iteration 12800: Loss = -12242.9521484375
Iteration 12900: Loss = -12242.73828125
Iteration 13000: Loss = -12242.697265625
Iteration 13100: Loss = -12242.6748046875
Iteration 13200: Loss = -12242.3564453125
Iteration 13300: Loss = -12236.0732421875
Iteration 13400: Loss = -12236.0263671875
Iteration 13500: Loss = -12236.0068359375
Iteration 13600: Loss = -12235.99609375
Iteration 13700: Loss = -12235.9873046875
Iteration 13800: Loss = -12235.9833984375
Iteration 13900: Loss = -12235.9775390625
Iteration 14000: Loss = -12225.0439453125
Iteration 14100: Loss = -12210.9453125
Iteration 14200: Loss = -12200.8076171875
Iteration 14300: Loss = -12200.650390625
Iteration 14400: Loss = -12200.5908203125
Iteration 14500: Loss = -12200.4658203125
Iteration 14600: Loss = -12190.9267578125
Iteration 14700: Loss = -12190.83203125
Iteration 14800: Loss = -12190.7939453125
Iteration 14900: Loss = -12190.7724609375
Iteration 15000: Loss = -12190.755859375
Iteration 15100: Loss = -12190.7451171875
Iteration 15200: Loss = -12190.7373046875
Iteration 15300: Loss = -12190.728515625
Iteration 15400: Loss = -12190.7236328125
Iteration 15500: Loss = -12190.720703125
Iteration 15600: Loss = -12190.7158203125
Iteration 15700: Loss = -12190.7119140625
Iteration 15800: Loss = -12190.708984375
Iteration 15900: Loss = -12190.70703125
Iteration 16000: Loss = -12190.703125
Iteration 16100: Loss = -12190.7021484375
Iteration 16200: Loss = -12190.7001953125
Iteration 16300: Loss = -12190.6982421875
Iteration 16400: Loss = -12190.697265625
Iteration 16500: Loss = -12190.6962890625
Iteration 16600: Loss = -12190.6943359375
Iteration 16700: Loss = -12190.6943359375
Iteration 16800: Loss = -12190.6923828125
Iteration 16900: Loss = -12190.689453125
Iteration 17000: Loss = -12190.6884765625
Iteration 17100: Loss = -12190.6884765625
Iteration 17200: Loss = -12190.6884765625
Iteration 17300: Loss = -12190.6865234375
Iteration 17400: Loss = -12190.6865234375
Iteration 17500: Loss = -12190.685546875
Iteration 17600: Loss = -12190.685546875
Iteration 17700: Loss = -12190.685546875
Iteration 17800: Loss = -12190.685546875
Iteration 17900: Loss = -12190.68359375
Iteration 18000: Loss = -12190.6845703125
1
Iteration 18100: Loss = -12190.68359375
Iteration 18200: Loss = -12190.6826171875
Iteration 18300: Loss = -12190.6826171875
Iteration 18400: Loss = -12190.6826171875
Iteration 18500: Loss = -12190.681640625
Iteration 18600: Loss = -12190.681640625
Iteration 18700: Loss = -12190.6806640625
Iteration 18800: Loss = -12190.681640625
1
Iteration 18900: Loss = -12190.6806640625
Iteration 19000: Loss = -12190.552734375
Iteration 19100: Loss = -12190.30859375
Iteration 19200: Loss = -12190.306640625
Iteration 19300: Loss = -12190.3076171875
1
Iteration 19400: Loss = -12190.3095703125
2
Iteration 19500: Loss = -12190.306640625
Iteration 19600: Loss = -12190.3076171875
1
Iteration 19700: Loss = -12190.3076171875
2
Iteration 19800: Loss = -12190.3076171875
3
Iteration 19900: Loss = -12190.3076171875
4
Iteration 20000: Loss = -12190.3076171875
5
Iteration 20100: Loss = -12190.306640625
Iteration 20200: Loss = -12190.3076171875
1
Iteration 20300: Loss = -12190.306640625
Iteration 20400: Loss = -12190.3056640625
Iteration 20500: Loss = -12190.3046875
Iteration 20600: Loss = -12190.3046875
Iteration 20700: Loss = -12190.3056640625
1
Iteration 20800: Loss = -12190.3046875
Iteration 20900: Loss = -12190.3056640625
1
Iteration 21000: Loss = -12190.306640625
2
Iteration 21100: Loss = -12190.3046875
Iteration 21200: Loss = -12190.3046875
Iteration 21300: Loss = -12190.3046875
Iteration 21400: Loss = -12190.3056640625
1
Iteration 21500: Loss = -12190.3046875
Iteration 21600: Loss = -12189.943359375
Iteration 21700: Loss = -12185.4912109375
Iteration 21800: Loss = -12175.9794921875
Iteration 21900: Loss = -12168.0224609375
Iteration 22000: Loss = -12164.9365234375
Iteration 22100: Loss = -12161.5458984375
Iteration 22200: Loss = -12159.3056640625
Iteration 22300: Loss = -12154.001953125
Iteration 22400: Loss = -12150.408203125
Iteration 22500: Loss = -12140.791015625
Iteration 22600: Loss = -12125.79296875
Iteration 22700: Loss = -12098.62890625
Iteration 22800: Loss = -12070.28125
Iteration 22900: Loss = -12060.0078125
Iteration 23000: Loss = -12051.2392578125
Iteration 23100: Loss = -12034.5087890625
Iteration 23200: Loss = -12018.8408203125
Iteration 23300: Loss = -12017.6435546875
Iteration 23400: Loss = -12014.4248046875
Iteration 23500: Loss = -12009.15234375
Iteration 23600: Loss = -12008.9736328125
Iteration 23700: Loss = -12008.6640625
Iteration 23800: Loss = -12007.44140625
Iteration 23900: Loss = -12007.408203125
Iteration 24000: Loss = -12007.3857421875
Iteration 24100: Loss = -12007.369140625
Iteration 24200: Loss = -12007.35546875
Iteration 24300: Loss = -12007.3408203125
Iteration 24400: Loss = -12007.333984375
Iteration 24500: Loss = -12007.326171875
Iteration 24600: Loss = -12007.318359375
Iteration 24700: Loss = -12001.2138671875
Iteration 24800: Loss = -11987.3505859375
Iteration 24900: Loss = -11987.2138671875
Iteration 25000: Loss = -11987.162109375
Iteration 25100: Loss = -11987.1328125
Iteration 25200: Loss = -11987.115234375
Iteration 25300: Loss = -11987.1005859375
Iteration 25400: Loss = -11987.091796875
Iteration 25500: Loss = -11987.083984375
Iteration 25600: Loss = -11987.076171875
Iteration 25700: Loss = -11987.0703125
Iteration 25800: Loss = -11987.06640625
Iteration 25900: Loss = -11987.0625
Iteration 26000: Loss = -11987.05859375
Iteration 26100: Loss = -11987.056640625
Iteration 26200: Loss = -11987.052734375
Iteration 26300: Loss = -11987.0498046875
Iteration 26400: Loss = -11987.046875
Iteration 26500: Loss = -11987.044921875
Iteration 26600: Loss = -11987.0439453125
Iteration 26700: Loss = -11987.0419921875
Iteration 26800: Loss = -11987.0400390625
Iteration 26900: Loss = -11987.0390625
Iteration 27000: Loss = -11987.037109375
Iteration 27100: Loss = -11987.0361328125
Iteration 27200: Loss = -11987.03515625
Iteration 27300: Loss = -11987.0341796875
Iteration 27400: Loss = -11987.0322265625
Iteration 27500: Loss = -11987.03125
Iteration 27600: Loss = -11987.03125
Iteration 27700: Loss = -11987.0302734375
Iteration 27800: Loss = -11987.0283203125
Iteration 27900: Loss = -11987.02734375
Iteration 28000: Loss = -11987.02734375
Iteration 28100: Loss = -11987.0283203125
1
Iteration 28200: Loss = -11987.0263671875
Iteration 28300: Loss = -11987.0263671875
Iteration 28400: Loss = -11987.0263671875
Iteration 28500: Loss = -11987.0244140625
Iteration 28600: Loss = -11987.0234375
Iteration 28700: Loss = -11987.0224609375
Iteration 28800: Loss = -11987.0224609375
Iteration 28900: Loss = -11987.021484375
Iteration 29000: Loss = -11987.021484375
Iteration 29100: Loss = -11987.021484375
Iteration 29200: Loss = -11987.0205078125
Iteration 29300: Loss = -11987.021484375
1
Iteration 29400: Loss = -11987.0205078125
Iteration 29500: Loss = -11987.01953125
Iteration 29600: Loss = -11987.01953125
Iteration 29700: Loss = -11987.01953125
Iteration 29800: Loss = -11987.01953125
Iteration 29900: Loss = -11987.0185546875
pi: tensor([[0.3768, 0.6232],
        [0.6815, 0.3185]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4331, 0.5669], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2900, 0.1044],
         [0.4682, 0.3082]],

        [[0.4725, 0.1113],
         [0.0077, 0.9755]],

        [[0.9852, 0.1031],
         [0.0588, 0.8328]],

        [[0.9932, 0.0961],
         [0.9928, 0.0202]],

        [[0.9460, 0.0976],
         [0.9786, 0.5868]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9207675179163246
time is 3
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.03495817360762021
Average Adjusted Rand Index: 0.9681529707203966
[-0.001783122334233028, 0.03495817360762021] [-0.0015521208230856337, 0.9681529707203966] [12439.080078125, 11987.01953125]
-------------------------------------
This iteration is 2
True Objective function: Loss = -11852.075903817185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -39556.828125
Iteration 100: Loss = -27509.1640625
Iteration 200: Loss = -16737.6171875
Iteration 300: Loss = -13546.33203125
Iteration 400: Loss = -12937.9814453125
Iteration 500: Loss = -12728.5849609375
Iteration 600: Loss = -12612.1923828125
Iteration 700: Loss = -12565.90625
Iteration 800: Loss = -12535.2001953125
Iteration 900: Loss = -12512.205078125
Iteration 1000: Loss = -12492.5810546875
Iteration 1100: Loss = -12475.767578125
Iteration 1200: Loss = -12460.6630859375
Iteration 1300: Loss = -12443.2021484375
Iteration 1400: Loss = -12426.9375
Iteration 1500: Loss = -12418.1240234375
Iteration 1600: Loss = -12404.3115234375
Iteration 1700: Loss = -12391.2607421875
Iteration 1800: Loss = -12381.939453125
Iteration 1900: Loss = -12376.333984375
Iteration 2000: Loss = -12372.142578125
Iteration 2100: Loss = -12368.2197265625
Iteration 2200: Loss = -12363.0224609375
Iteration 2300: Loss = -12357.447265625
Iteration 2400: Loss = -12351.2744140625
Iteration 2500: Loss = -12349.380859375
Iteration 2600: Loss = -12347.9296875
Iteration 2700: Loss = -12346.728515625
Iteration 2800: Loss = -12345.65625
Iteration 2900: Loss = -12344.10546875
Iteration 3000: Loss = -12343.306640625
Iteration 3100: Loss = -12342.720703125
Iteration 3200: Loss = -12342.2099609375
Iteration 3300: Loss = -12341.7333984375
Iteration 3400: Loss = -12341.302734375
Iteration 3500: Loss = -12340.9111328125
Iteration 3600: Loss = -12340.5478515625
Iteration 3700: Loss = -12340.2177734375
Iteration 3800: Loss = -12339.90625
Iteration 3900: Loss = -12339.6015625
Iteration 4000: Loss = -12339.23828125
Iteration 4100: Loss = -12338.5615234375
Iteration 4200: Loss = -12338.1259765625
Iteration 4300: Loss = -12337.826171875
Iteration 4400: Loss = -12334.666015625
Iteration 4500: Loss = -12334.150390625
Iteration 4600: Loss = -12333.732421875
Iteration 4700: Loss = -12333.4541015625
Iteration 4800: Loss = -12333.2392578125
Iteration 4900: Loss = -12333.029296875
Iteration 5000: Loss = -12322.42578125
Iteration 5100: Loss = -12320.978515625
Iteration 5200: Loss = -12320.611328125
Iteration 5300: Loss = -12320.3857421875
Iteration 5400: Loss = -12320.2138671875
Iteration 5500: Loss = -12320.0703125
Iteration 5600: Loss = -12319.9423828125
Iteration 5700: Loss = -12319.8291015625
Iteration 5800: Loss = -12319.7265625
Iteration 5900: Loss = -12319.630859375
Iteration 6000: Loss = -12319.541015625
Iteration 6100: Loss = -12319.4609375
Iteration 6200: Loss = -12319.384765625
Iteration 6300: Loss = -12319.314453125
Iteration 6400: Loss = -12319.248046875
Iteration 6500: Loss = -12319.1875
Iteration 6600: Loss = -12319.1298828125
Iteration 6700: Loss = -12319.0751953125
Iteration 6800: Loss = -12319.0244140625
Iteration 6900: Loss = -12318.9775390625
Iteration 7000: Loss = -12318.9326171875
Iteration 7100: Loss = -12318.888671875
Iteration 7200: Loss = -12318.8486328125
Iteration 7300: Loss = -12318.8115234375
Iteration 7400: Loss = -12318.775390625
Iteration 7500: Loss = -12318.7421875
Iteration 7600: Loss = -12318.7080078125
Iteration 7700: Loss = -12318.6787109375
Iteration 7800: Loss = -12318.650390625
Iteration 7900: Loss = -12318.6220703125
Iteration 8000: Loss = -12318.59765625
Iteration 8100: Loss = -12318.5712890625
Iteration 8200: Loss = -12318.55078125
Iteration 8300: Loss = -12318.52734375
Iteration 8400: Loss = -12318.5068359375
Iteration 8500: Loss = -12318.4892578125
Iteration 8600: Loss = -12318.46875
Iteration 8700: Loss = -12318.451171875
Iteration 8800: Loss = -12318.435546875
Iteration 8900: Loss = -12318.419921875
Iteration 9000: Loss = -12318.4033203125
Iteration 9100: Loss = -12318.3896484375
Iteration 9200: Loss = -12318.3759765625
Iteration 9300: Loss = -12318.36328125
Iteration 9400: Loss = -12318.353515625
Iteration 9500: Loss = -12318.33984375
Iteration 9600: Loss = -12318.328125
Iteration 9700: Loss = -12318.3173828125
Iteration 9800: Loss = -12318.30859375
Iteration 9900: Loss = -12318.2998046875
Iteration 10000: Loss = -12318.2890625
Iteration 10100: Loss = -12318.2802734375
Iteration 10200: Loss = -12318.2734375
Iteration 10300: Loss = -12318.2646484375
Iteration 10400: Loss = -12318.259765625
Iteration 10500: Loss = -12318.25
Iteration 10600: Loss = -12318.2451171875
Iteration 10700: Loss = -12318.23828125
Iteration 10800: Loss = -12318.232421875
Iteration 10900: Loss = -12318.2265625
Iteration 11000: Loss = -12318.22265625
Iteration 11100: Loss = -12318.216796875
Iteration 11200: Loss = -12318.2109375
Iteration 11300: Loss = -12318.20703125
Iteration 11400: Loss = -12318.2021484375
Iteration 11500: Loss = -12318.19921875
Iteration 11600: Loss = -12318.1943359375
Iteration 11700: Loss = -12318.19140625
Iteration 11800: Loss = -12318.189453125
Iteration 11900: Loss = -12318.185546875
Iteration 12000: Loss = -12318.1787109375
Iteration 12100: Loss = -12318.1767578125
Iteration 12200: Loss = -12318.173828125
Iteration 12300: Loss = -12318.171875
Iteration 12400: Loss = -12318.169921875
Iteration 12500: Loss = -12318.16796875
Iteration 12600: Loss = -12318.1650390625
Iteration 12700: Loss = -12318.1640625
Iteration 12800: Loss = -12318.1611328125
Iteration 12900: Loss = -12318.16015625
Iteration 13000: Loss = -12318.158203125
Iteration 13100: Loss = -12318.15625
Iteration 13200: Loss = -12318.1572265625
1
Iteration 13300: Loss = -12318.154296875
Iteration 13400: Loss = -12318.1513671875
Iteration 13500: Loss = -12318.1513671875
Iteration 13600: Loss = -12318.1513671875
Iteration 13700: Loss = -12318.1474609375
Iteration 13800: Loss = -12318.1484375
1
Iteration 13900: Loss = -12318.1455078125
Iteration 14000: Loss = -12318.1455078125
Iteration 14100: Loss = -12318.146484375
1
Iteration 14200: Loss = -12318.1435546875
Iteration 14300: Loss = -12318.1435546875
Iteration 14400: Loss = -12318.1435546875
Iteration 14500: Loss = -12318.140625
Iteration 14600: Loss = -12318.140625
Iteration 14700: Loss = -12318.1396484375
Iteration 14800: Loss = -12318.1396484375
Iteration 14900: Loss = -12318.1376953125
Iteration 15000: Loss = -12318.138671875
1
Iteration 15100: Loss = -12318.13671875
Iteration 15200: Loss = -12318.134765625
Iteration 15300: Loss = -12318.13671875
1
Iteration 15400: Loss = -12318.1357421875
2
Iteration 15500: Loss = -12318.1357421875
3
Iteration 15600: Loss = -12318.134765625
Iteration 15700: Loss = -12318.1337890625
Iteration 15800: Loss = -12318.1357421875
1
Iteration 15900: Loss = -12318.1337890625
Iteration 16000: Loss = -12318.1337890625
Iteration 16100: Loss = -12318.1328125
Iteration 16200: Loss = -12318.1318359375
Iteration 16300: Loss = -12318.1318359375
Iteration 16400: Loss = -12318.130859375
Iteration 16500: Loss = -12318.1328125
1
Iteration 16600: Loss = -12318.1328125
2
Iteration 16700: Loss = -12318.1318359375
3
Iteration 16800: Loss = -12318.1298828125
Iteration 16900: Loss = -12318.130859375
1
Iteration 17000: Loss = -12318.1298828125
Iteration 17100: Loss = -12318.1298828125
Iteration 17200: Loss = -12318.130859375
1
Iteration 17300: Loss = -12318.12890625
Iteration 17400: Loss = -12318.1298828125
1
Iteration 17500: Loss = -12318.1298828125
2
Iteration 17600: Loss = -12318.12890625
Iteration 17700: Loss = -12318.12890625
Iteration 17800: Loss = -12318.12890625
Iteration 17900: Loss = -12318.12890625
Iteration 18000: Loss = -12318.12890625
Iteration 18100: Loss = -12318.12890625
Iteration 18200: Loss = -12318.130859375
1
Iteration 18300: Loss = -12318.126953125
Iteration 18400: Loss = -12318.12890625
1
Iteration 18500: Loss = -12318.1279296875
2
Iteration 18600: Loss = -12318.12890625
3
Iteration 18700: Loss = -12318.126953125
Iteration 18800: Loss = -12318.1279296875
1
Iteration 18900: Loss = -12318.126953125
Iteration 19000: Loss = -12318.126953125
Iteration 19100: Loss = -12318.1259765625
Iteration 19200: Loss = -12318.126953125
1
Iteration 19300: Loss = -12318.125
Iteration 19400: Loss = -12318.126953125
1
Iteration 19500: Loss = -12318.126953125
2
Iteration 19600: Loss = -12318.126953125
3
Iteration 19700: Loss = -12318.126953125
4
Iteration 19800: Loss = -12318.125
Iteration 19900: Loss = -12318.126953125
1
Iteration 20000: Loss = -12318.1259765625
2
Iteration 20100: Loss = -12318.1259765625
3
Iteration 20200: Loss = -12318.1240234375
Iteration 20300: Loss = -12318.1259765625
1
Iteration 20400: Loss = -12318.1259765625
2
Iteration 20500: Loss = -12318.1259765625
3
Iteration 20600: Loss = -12318.1259765625
4
Iteration 20700: Loss = -12318.1259765625
5
Iteration 20800: Loss = -12318.125
6
Iteration 20900: Loss = -12318.1259765625
7
Iteration 21000: Loss = -12318.1259765625
8
Iteration 21100: Loss = -12318.1259765625
9
Iteration 21200: Loss = -12318.126953125
10
Iteration 21300: Loss = -12318.1259765625
11
Iteration 21400: Loss = -12318.125
12
Iteration 21500: Loss = -12318.125
13
Iteration 21600: Loss = -12318.125
14
Iteration 21700: Loss = -12318.1259765625
15
Stopping early at iteration 21700 due to no improvement.
pi: tensor([[1.7937e-02, 9.8206e-01],
        [6.6163e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.6299e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1409, 0.5123],
         [0.0182, 0.1981]],

        [[0.9811, 0.2238],
         [0.0418, 0.9718]],

        [[0.1224, 0.1892],
         [0.0247, 0.1534]],

        [[0.0363, 0.2289],
         [0.9768, 0.9669]],

        [[0.0137, 0.6844],
         [0.6907, 0.0802]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19142.61328125
Iteration 100: Loss = -14305.279296875
Iteration 200: Loss = -12755.44921875
Iteration 300: Loss = -12511.6318359375
Iteration 400: Loss = -12444.5908203125
Iteration 500: Loss = -12415.9609375
Iteration 600: Loss = -12396.7919921875
Iteration 700: Loss = -12381.8583984375
Iteration 800: Loss = -12370.8759765625
Iteration 900: Loss = -12359.7216796875
Iteration 1000: Loss = -12346.271484375
Iteration 1100: Loss = -12339.8779296875
Iteration 1200: Loss = -12334.92578125
Iteration 1300: Loss = -12331.359375
Iteration 1400: Loss = -12328.52734375
Iteration 1500: Loss = -12326.4267578125
Iteration 1600: Loss = -12325.048828125
Iteration 1700: Loss = -12324.0712890625
Iteration 1800: Loss = -12323.3212890625
Iteration 1900: Loss = -12322.720703125
Iteration 2000: Loss = -12322.2275390625
Iteration 2100: Loss = -12321.814453125
Iteration 2200: Loss = -12321.462890625
Iteration 2300: Loss = -12321.1591796875
Iteration 2400: Loss = -12320.892578125
Iteration 2500: Loss = -12320.6591796875
Iteration 2600: Loss = -12320.451171875
Iteration 2700: Loss = -12320.265625
Iteration 2800: Loss = -12320.099609375
Iteration 2900: Loss = -12319.9482421875
Iteration 3000: Loss = -12319.810546875
Iteration 3100: Loss = -12319.6845703125
Iteration 3200: Loss = -12319.5712890625
Iteration 3300: Loss = -12319.46484375
Iteration 3400: Loss = -12319.3701171875
Iteration 3500: Loss = -12319.279296875
Iteration 3600: Loss = -12319.1953125
Iteration 3700: Loss = -12319.1181640625
Iteration 3800: Loss = -12319.0458984375
Iteration 3900: Loss = -12318.9775390625
Iteration 4000: Loss = -12318.9140625
Iteration 4100: Loss = -12318.8564453125
Iteration 4200: Loss = -12318.80078125
Iteration 4300: Loss = -12318.7490234375
Iteration 4400: Loss = -12318.701171875
Iteration 4500: Loss = -12318.658203125
Iteration 4600: Loss = -12318.615234375
Iteration 4700: Loss = -12318.578125
Iteration 4800: Loss = -12318.5400390625
Iteration 4900: Loss = -12318.5078125
Iteration 5000: Loss = -12318.4755859375
Iteration 5100: Loss = -12318.4453125
Iteration 5200: Loss = -12318.41796875
Iteration 5300: Loss = -12318.392578125
Iteration 5400: Loss = -12318.369140625
Iteration 5500: Loss = -12318.3466796875
Iteration 5600: Loss = -12318.326171875
Iteration 5700: Loss = -12318.3056640625
Iteration 5800: Loss = -12318.2880859375
Iteration 5900: Loss = -12318.2705078125
Iteration 6000: Loss = -12318.2548828125
Iteration 6100: Loss = -12318.240234375
Iteration 6200: Loss = -12318.224609375
Iteration 6300: Loss = -12318.2119140625
Iteration 6400: Loss = -12318.1982421875
Iteration 6500: Loss = -12318.1865234375
Iteration 6600: Loss = -12318.17578125
Iteration 6700: Loss = -12318.1640625
Iteration 6800: Loss = -12318.154296875
Iteration 6900: Loss = -12318.1455078125
Iteration 7000: Loss = -12318.1357421875
Iteration 7100: Loss = -12318.1279296875
Iteration 7200: Loss = -12318.1201171875
Iteration 7300: Loss = -12318.1123046875
Iteration 7400: Loss = -12318.1064453125
Iteration 7500: Loss = -12318.0986328125
Iteration 7600: Loss = -12318.0927734375
Iteration 7700: Loss = -12318.087890625
Iteration 7800: Loss = -12318.0810546875
Iteration 7900: Loss = -12318.076171875
Iteration 8000: Loss = -12318.0712890625
Iteration 8100: Loss = -12318.06640625
Iteration 8200: Loss = -12318.0625
Iteration 8300: Loss = -12318.05859375
Iteration 8400: Loss = -12318.0546875
Iteration 8500: Loss = -12318.0498046875
Iteration 8600: Loss = -12318.044921875
Iteration 8700: Loss = -12318.04296875
Iteration 8800: Loss = -12318.041015625
Iteration 8900: Loss = -12318.0380859375
Iteration 9000: Loss = -12318.03515625
Iteration 9100: Loss = -12318.0322265625
Iteration 9200: Loss = -12318.029296875
Iteration 9300: Loss = -12318.0263671875
Iteration 9400: Loss = -12318.0244140625
Iteration 9500: Loss = -12318.021484375
Iteration 9600: Loss = -12318.0205078125
Iteration 9700: Loss = -12318.0185546875
Iteration 9800: Loss = -12318.015625
Iteration 9900: Loss = -12318.0146484375
Iteration 10000: Loss = -12318.01171875
Iteration 10100: Loss = -12318.0107421875
Iteration 10200: Loss = -12318.0087890625
Iteration 10300: Loss = -12318.0078125
Iteration 10400: Loss = -12318.0078125
Iteration 10500: Loss = -12318.0048828125
Iteration 10600: Loss = -12318.001953125
Iteration 10700: Loss = -12318.001953125
Iteration 10800: Loss = -12317.9990234375
Iteration 10900: Loss = -12317.998046875
Iteration 11000: Loss = -12317.99609375
Iteration 11100: Loss = -12317.99609375
Iteration 11200: Loss = -12317.994140625
Iteration 11300: Loss = -12317.9921875
Iteration 11400: Loss = -12317.98828125
Iteration 11500: Loss = -12317.98828125
Iteration 11600: Loss = -12317.9853515625
Iteration 11700: Loss = -12317.9833984375
Iteration 11800: Loss = -12317.982421875
Iteration 11900: Loss = -12317.98046875
Iteration 12000: Loss = -12317.98046875
Iteration 12100: Loss = -12317.978515625
Iteration 12200: Loss = -12317.978515625
Iteration 12300: Loss = -12317.9765625
Iteration 12400: Loss = -12317.9765625
Iteration 12500: Loss = -12317.9775390625
1
Iteration 12600: Loss = -12317.9765625
Iteration 12700: Loss = -12317.9775390625
1
Iteration 12800: Loss = -12317.9765625
Iteration 12900: Loss = -12317.9775390625
1
Iteration 13000: Loss = -12317.9765625
Iteration 13100: Loss = -12317.9755859375
Iteration 13200: Loss = -12317.9755859375
Iteration 13300: Loss = -12317.9736328125
Iteration 13400: Loss = -12317.9736328125
Iteration 13500: Loss = -12317.97265625
Iteration 13600: Loss = -12317.9716796875
Iteration 13700: Loss = -12317.9716796875
Iteration 13800: Loss = -12317.970703125
Iteration 13900: Loss = -12317.970703125
Iteration 14000: Loss = -12317.970703125
Iteration 14100: Loss = -12317.9697265625
Iteration 14200: Loss = -12317.96875
Iteration 14300: Loss = -12317.9697265625
1
Iteration 14400: Loss = -12317.9697265625
2
Iteration 14500: Loss = -12317.96875
Iteration 14600: Loss = -12317.9677734375
Iteration 14700: Loss = -12317.9677734375
Iteration 14800: Loss = -12317.96875
1
Iteration 14900: Loss = -12317.9677734375
Iteration 15000: Loss = -12317.966796875
Iteration 15100: Loss = -12317.966796875
Iteration 15200: Loss = -12317.9658203125
Iteration 15300: Loss = -12317.9658203125
Iteration 15400: Loss = -12317.96484375
Iteration 15500: Loss = -12317.96484375
Iteration 15600: Loss = -12317.9638671875
Iteration 15700: Loss = -12317.9638671875
Iteration 15800: Loss = -12317.962890625
Iteration 15900: Loss = -12317.9638671875
1
Iteration 16000: Loss = -12317.962890625
Iteration 16100: Loss = -12317.9619140625
Iteration 16200: Loss = -12317.9609375
Iteration 16300: Loss = -12317.9599609375
Iteration 16400: Loss = -12317.958984375
Iteration 16500: Loss = -12317.958984375
Iteration 16600: Loss = -12317.9599609375
1
Iteration 16700: Loss = -12317.9580078125
Iteration 16800: Loss = -12317.95703125
Iteration 16900: Loss = -12317.958984375
1
Iteration 17000: Loss = -12317.9580078125
2
Iteration 17100: Loss = -12317.95703125
Iteration 17200: Loss = -12317.95703125
Iteration 17300: Loss = -12317.95703125
Iteration 17400: Loss = -12317.95703125
Iteration 17500: Loss = -12317.9541015625
Iteration 17600: Loss = -12317.9541015625
Iteration 17700: Loss = -12317.9453125
Iteration 17800: Loss = -12317.8125
Iteration 17900: Loss = -12317.61328125
Iteration 18000: Loss = -12317.5869140625
Iteration 18100: Loss = -12317.5849609375
Iteration 18200: Loss = -12317.5849609375
Iteration 18300: Loss = -12317.583984375
Iteration 18400: Loss = -12317.5830078125
Iteration 18500: Loss = -12317.5849609375
1
Iteration 18600: Loss = -12317.583984375
2
Iteration 18700: Loss = -12317.5859375
3
Iteration 18800: Loss = -12317.583984375
4
Iteration 18900: Loss = -12317.583984375
5
Iteration 19000: Loss = -12317.5849609375
6
Iteration 19100: Loss = -12317.5849609375
7
Iteration 19200: Loss = -12317.583984375
8
Iteration 19300: Loss = -12317.5869140625
9
Iteration 19400: Loss = -12317.583984375
10
Iteration 19500: Loss = -12317.5849609375
11
Iteration 19600: Loss = -12317.5849609375
12
Iteration 19700: Loss = -12317.5859375
13
Iteration 19800: Loss = -12317.5849609375
14
Iteration 19900: Loss = -12317.583984375
15
Stopping early at iteration 19900 due to no improvement.
pi: tensor([[0.0341, 0.9659],
        [0.0258, 0.9742]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9998e-01, 2.3313e-05], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1959, 0.2004],
         [0.9857, 0.1973]],

        [[0.6581, 0.2272],
         [0.1907, 0.2268]],

        [[0.0771, 0.1891],
         [0.2077, 0.4509]],

        [[0.9924, 0.2721],
         [0.0079, 0.0068]],

        [[0.0212, 0.1942],
         [0.5922, 0.1321]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0014232914896144923
Average Adjusted Rand Index: 0.0
[0.0, -0.0014232914896144923] [0.0, 0.0] [12318.1259765625, 12317.583984375]
-------------------------------------
This iteration is 3
True Objective function: Loss = -11994.435155674195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31818.853515625
Iteration 100: Loss = -19638.1875
Iteration 200: Loss = -14251.1181640625
Iteration 300: Loss = -13044.8955078125
Iteration 400: Loss = -12784.1982421875
Iteration 500: Loss = -12700.6005859375
Iteration 600: Loss = -12656.384765625
Iteration 700: Loss = -12630.912109375
Iteration 800: Loss = -12614.8017578125
Iteration 900: Loss = -12603.859375
Iteration 1000: Loss = -12595.9912109375
Iteration 1100: Loss = -12590.095703125
Iteration 1200: Loss = -12585.529296875
Iteration 1300: Loss = -12581.923828125
Iteration 1400: Loss = -12579.017578125
Iteration 1500: Loss = -12576.638671875
Iteration 1600: Loss = -12574.6689453125
Iteration 1700: Loss = -12573.015625
Iteration 1800: Loss = -12571.6142578125
Iteration 1900: Loss = -12570.4189453125
Iteration 2000: Loss = -12569.3916015625
Iteration 2100: Loss = -12568.5009765625
Iteration 2200: Loss = -12567.720703125
Iteration 2300: Loss = -12567.0361328125
Iteration 2400: Loss = -12566.4345703125
Iteration 2500: Loss = -12565.8994140625
Iteration 2600: Loss = -12565.421875
Iteration 2700: Loss = -12564.9970703125
Iteration 2800: Loss = -12564.615234375
Iteration 2900: Loss = -12564.2685546875
Iteration 3000: Loss = -12563.95703125
Iteration 3100: Loss = -12563.6728515625
Iteration 3200: Loss = -12563.41796875
Iteration 3300: Loss = -12563.1826171875
Iteration 3400: Loss = -12562.9658203125
Iteration 3500: Loss = -12562.7705078125
Iteration 3600: Loss = -12562.5888671875
Iteration 3700: Loss = -12562.4248046875
Iteration 3800: Loss = -12562.2724609375
Iteration 3900: Loss = -12562.130859375
Iteration 4000: Loss = -12562.0009765625
Iteration 4100: Loss = -12561.880859375
Iteration 4200: Loss = -12561.767578125
Iteration 4300: Loss = -12561.6650390625
Iteration 4400: Loss = -12561.568359375
Iteration 4500: Loss = -12561.478515625
Iteration 4600: Loss = -12561.3955078125
Iteration 4700: Loss = -12561.3173828125
Iteration 4800: Loss = -12561.2412109375
Iteration 4900: Loss = -12561.171875
Iteration 5000: Loss = -12561.109375
Iteration 5100: Loss = -12561.0498046875
Iteration 5200: Loss = -12560.994140625
Iteration 5300: Loss = -12560.9423828125
Iteration 5400: Loss = -12560.892578125
Iteration 5500: Loss = -12560.8447265625
Iteration 5600: Loss = -12560.7978515625
Iteration 5700: Loss = -12560.751953125
Iteration 5800: Loss = -12560.70703125
Iteration 5900: Loss = -12560.6572265625
Iteration 6000: Loss = -12560.60546875
Iteration 6100: Loss = -12560.5419921875
Iteration 6200: Loss = -12560.4599609375
Iteration 6300: Loss = -12560.36328125
Iteration 6400: Loss = -12560.2802734375
Iteration 6500: Loss = -12560.2080078125
Iteration 6600: Loss = -12560.150390625
Iteration 6700: Loss = -12560.103515625
Iteration 6800: Loss = -12560.064453125
Iteration 6900: Loss = -12560.03125
Iteration 7000: Loss = -12560.0048828125
Iteration 7100: Loss = -12559.9775390625
Iteration 7200: Loss = -12559.94921875
Iteration 7300: Loss = -12559.9267578125
Iteration 7400: Loss = -12559.90625
Iteration 7500: Loss = -12559.88671875
Iteration 7600: Loss = -12559.8701171875
Iteration 7700: Loss = -12559.8544921875
Iteration 7800: Loss = -12559.837890625
Iteration 7900: Loss = -12559.822265625
Iteration 8000: Loss = -12559.8076171875
Iteration 8100: Loss = -12559.791015625
Iteration 8200: Loss = -12559.7763671875
Iteration 8300: Loss = -12559.7607421875
Iteration 8400: Loss = -12559.7451171875
Iteration 8500: Loss = -12559.7294921875
Iteration 8600: Loss = -12559.7119140625
Iteration 8700: Loss = -12559.693359375
Iteration 8800: Loss = -12559.6767578125
Iteration 8900: Loss = -12559.6591796875
Iteration 9000: Loss = -12559.638671875
Iteration 9100: Loss = -12559.619140625
Iteration 9200: Loss = -12559.59765625
Iteration 9300: Loss = -12559.5751953125
Iteration 9400: Loss = -12559.5517578125
Iteration 9500: Loss = -12559.5263671875
Iteration 9600: Loss = -12559.50390625
Iteration 9700: Loss = -12559.4755859375
Iteration 9800: Loss = -12559.4462890625
Iteration 9900: Loss = -12559.4140625
Iteration 10000: Loss = -12559.3818359375
Iteration 10100: Loss = -12559.345703125
Iteration 10200: Loss = -12559.3076171875
Iteration 10300: Loss = -12559.2685546875
Iteration 10400: Loss = -12559.22265625
Iteration 10500: Loss = -12559.1767578125
Iteration 10600: Loss = -12559.1240234375
Iteration 10700: Loss = -12559.0712890625
Iteration 10800: Loss = -12559.0087890625
Iteration 10900: Loss = -12558.9462890625
Iteration 11000: Loss = -12558.880859375
Iteration 11100: Loss = -12558.814453125
Iteration 11200: Loss = -12558.7548828125
Iteration 11300: Loss = -12558.705078125
Iteration 11400: Loss = -12558.6689453125
Iteration 11500: Loss = -12558.646484375
Iteration 11600: Loss = -12558.6298828125
Iteration 11700: Loss = -12558.6240234375
Iteration 11800: Loss = -12558.6142578125
Iteration 11900: Loss = -12558.6083984375
Iteration 12000: Loss = -12558.6044921875
Iteration 12100: Loss = -12558.6005859375
Iteration 12200: Loss = -12558.5966796875
Iteration 12300: Loss = -12558.5966796875
Iteration 12400: Loss = -12558.5947265625
Iteration 12500: Loss = -12558.5927734375
Iteration 12600: Loss = -12558.5947265625
1
Iteration 12700: Loss = -12558.591796875
Iteration 12800: Loss = -12558.5908203125
Iteration 12900: Loss = -12558.5908203125
Iteration 13000: Loss = -12558.58984375
Iteration 13100: Loss = -12558.58984375
Iteration 13200: Loss = -12558.5908203125
1
Iteration 13300: Loss = -12558.587890625
Iteration 13400: Loss = -12558.587890625
Iteration 13500: Loss = -12558.5869140625
Iteration 13600: Loss = -12558.5869140625
Iteration 13700: Loss = -12558.5859375
Iteration 13800: Loss = -12558.5849609375
Iteration 13900: Loss = -12558.583984375
Iteration 14000: Loss = -12558.5849609375
1
Iteration 14100: Loss = -12558.583984375
Iteration 14200: Loss = -12558.58203125
Iteration 14300: Loss = -12558.583984375
1
Iteration 14400: Loss = -12558.58203125
Iteration 14500: Loss = -12558.5810546875
Iteration 14600: Loss = -12558.5810546875
Iteration 14700: Loss = -12558.580078125
Iteration 14800: Loss = -12558.5791015625
Iteration 14900: Loss = -12558.5771484375
Iteration 15000: Loss = -12558.5771484375
Iteration 15100: Loss = -12558.576171875
Iteration 15200: Loss = -12558.5751953125
Iteration 15300: Loss = -12558.57421875
Iteration 15400: Loss = -12558.5703125
Iteration 15500: Loss = -12558.56640625
Iteration 15600: Loss = -12558.5634765625
Iteration 15700: Loss = -12558.5595703125
Iteration 15800: Loss = -12558.5537109375
Iteration 15900: Loss = -12558.544921875
Iteration 16000: Loss = -12558.5302734375
Iteration 16100: Loss = -12558.5029296875
Iteration 16200: Loss = -12558.4599609375
Iteration 16300: Loss = -12558.4423828125
Iteration 16400: Loss = -12558.4384765625
Iteration 16500: Loss = -12558.4404296875
1
Iteration 16600: Loss = -12558.4384765625
Iteration 16700: Loss = -12558.4384765625
Iteration 16800: Loss = -12558.4384765625
Iteration 16900: Loss = -12558.4384765625
Iteration 17000: Loss = -12558.4384765625
Iteration 17100: Loss = -12558.439453125
1
Iteration 17200: Loss = -12558.4384765625
Iteration 17300: Loss = -12558.439453125
1
Iteration 17400: Loss = -12558.4375
Iteration 17500: Loss = -12558.4404296875
1
Iteration 17600: Loss = -12558.4384765625
2
Iteration 17700: Loss = -12558.4375
Iteration 17800: Loss = -12558.4384765625
1
Iteration 17900: Loss = -12558.4375
Iteration 18000: Loss = -12558.4375
Iteration 18100: Loss = -12558.4375
Iteration 18200: Loss = -12558.4384765625
1
Iteration 18300: Loss = -12558.4375
Iteration 18400: Loss = -12558.4375
Iteration 18500: Loss = -12558.4375
Iteration 18600: Loss = -12558.4375
Iteration 18700: Loss = -12558.4384765625
1
Iteration 18800: Loss = -12558.4384765625
2
Iteration 18900: Loss = -12558.4375
Iteration 19000: Loss = -12558.4375
Iteration 19100: Loss = -12558.4375
Iteration 19200: Loss = -12558.4375
Iteration 19300: Loss = -12558.435546875
Iteration 19400: Loss = -12558.4365234375
1
Iteration 19500: Loss = -12558.4365234375
2
Iteration 19600: Loss = -12558.4365234375
3
Iteration 19700: Loss = -12558.4365234375
4
Iteration 19800: Loss = -12558.4365234375
5
Iteration 19900: Loss = -12558.4365234375
6
Iteration 20000: Loss = -12558.4365234375
7
Iteration 20100: Loss = -12558.4375
8
Iteration 20200: Loss = -12558.439453125
9
Iteration 20300: Loss = -12558.4375
10
Iteration 20400: Loss = -12558.4375
11
Iteration 20500: Loss = -12558.4384765625
12
Iteration 20600: Loss = -12558.4365234375
13
Iteration 20700: Loss = -12558.435546875
Iteration 20800: Loss = -12558.4365234375
1
Iteration 20900: Loss = -12558.4365234375
2
Iteration 21000: Loss = -12558.4384765625
3
Iteration 21100: Loss = -12558.4365234375
4
Iteration 21200: Loss = -12558.435546875
Iteration 21300: Loss = -12558.4365234375
1
Iteration 21400: Loss = -12558.435546875
Iteration 21500: Loss = -12558.435546875
Iteration 21600: Loss = -12558.4375
1
Iteration 21700: Loss = -12558.4365234375
2
Iteration 21800: Loss = -12558.4375
3
Iteration 21900: Loss = -12558.4365234375
4
Iteration 22000: Loss = -12558.4375
5
Iteration 22100: Loss = -12558.4365234375
6
Iteration 22200: Loss = -12558.435546875
Iteration 22300: Loss = -12558.4375
1
Iteration 22400: Loss = -12558.4375
2
Iteration 22500: Loss = -12558.4365234375
3
Iteration 22600: Loss = -12558.435546875
Iteration 22700: Loss = -12558.435546875
Iteration 22800: Loss = -12558.4375
1
Iteration 22900: Loss = -12558.4384765625
2
Iteration 23000: Loss = -12558.4365234375
3
Iteration 23100: Loss = -12558.4365234375
4
Iteration 23200: Loss = -12558.4365234375
5
Iteration 23300: Loss = -12558.4384765625
6
Iteration 23400: Loss = -12558.4375
7
Iteration 23500: Loss = -12558.435546875
Iteration 23600: Loss = -12558.435546875
Iteration 23700: Loss = -12558.4365234375
1
Iteration 23800: Loss = -12558.4375
2
Iteration 23900: Loss = -12558.4365234375
3
Iteration 24000: Loss = -12558.4375
4
Iteration 24100: Loss = -12558.4375
5
Iteration 24200: Loss = -12558.4345703125
Iteration 24300: Loss = -12558.435546875
1
Iteration 24400: Loss = -12558.435546875
2
Iteration 24500: Loss = -12558.435546875
3
Iteration 24600: Loss = -12558.435546875
4
Iteration 24700: Loss = -12558.4365234375
5
Iteration 24800: Loss = -12558.4384765625
6
Iteration 24900: Loss = -12558.4365234375
7
Iteration 25000: Loss = -12558.4365234375
8
Iteration 25100: Loss = -12558.4365234375
9
Iteration 25200: Loss = -12558.4365234375
10
Iteration 25300: Loss = -12558.4365234375
11
Iteration 25400: Loss = -12558.4375
12
Iteration 25500: Loss = -12558.4375
13
Iteration 25600: Loss = -12558.4365234375
14
Iteration 25700: Loss = -12558.4375
15
Stopping early at iteration 25700 due to no improvement.
pi: tensor([[9.4057e-01, 5.9431e-02],
        [9.9994e-01, 5.9536e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8704, 0.1296], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2030, 0.2173],
         [0.4677, 0.2383]],

        [[0.0505, 0.2023],
         [0.8678, 0.0079]],

        [[0.0572, 0.2266],
         [0.2859, 0.1887]],

        [[0.1290, 0.1754],
         [0.9777, 0.2587]],

        [[0.2760, 0.2657],
         [0.4036, 0.8535]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 62
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -39134.265625
Iteration 100: Loss = -25298.880859375
Iteration 200: Loss = -15561.8916015625
Iteration 300: Loss = -13551.67578125
Iteration 400: Loss = -13014.70703125
Iteration 500: Loss = -12851.7177734375
Iteration 600: Loss = -12771.734375
Iteration 700: Loss = -12726.8583984375
Iteration 800: Loss = -12699.169921875
Iteration 900: Loss = -12677.4482421875
Iteration 1000: Loss = -12663.57421875
Iteration 1100: Loss = -12653.595703125
Iteration 1200: Loss = -12645.0703125
Iteration 1300: Loss = -12637.24609375
Iteration 1400: Loss = -12631.0302734375
Iteration 1500: Loss = -12621.2119140625
Iteration 1600: Loss = -12614.5087890625
Iteration 1700: Loss = -12609.5693359375
Iteration 1800: Loss = -12604.9755859375
Iteration 1900: Loss = -12600.6591796875
Iteration 2000: Loss = -12596.8271484375
Iteration 2100: Loss = -12593.7373046875
Iteration 2200: Loss = -12588.8134765625
Iteration 2300: Loss = -12585.1865234375
Iteration 2400: Loss = -12583.1845703125
Iteration 2500: Loss = -12581.5615234375
Iteration 2600: Loss = -12580.1826171875
Iteration 2700: Loss = -12578.984375
Iteration 2800: Loss = -12577.9111328125
Iteration 2900: Loss = -12576.955078125
Iteration 3000: Loss = -12576.109375
Iteration 3100: Loss = -12575.3583984375
Iteration 3200: Loss = -12574.666015625
Iteration 3300: Loss = -12573.9521484375
Iteration 3400: Loss = -12573.1357421875
Iteration 3500: Loss = -12572.369140625
Iteration 3600: Loss = -12571.7421875
Iteration 3700: Loss = -12571.15625
Iteration 3800: Loss = -12570.6103515625
Iteration 3900: Loss = -12570.1416015625
Iteration 4000: Loss = -12569.734375
Iteration 4100: Loss = -12569.3935546875
Iteration 4200: Loss = -12569.10546875
Iteration 4300: Loss = -12568.8623046875
Iteration 4400: Loss = -12568.6484375
Iteration 4500: Loss = -12568.462890625
Iteration 4600: Loss = -12568.298828125
Iteration 4700: Loss = -12568.146484375
Iteration 4800: Loss = -12568.0107421875
Iteration 4900: Loss = -12567.8876953125
Iteration 5000: Loss = -12567.7744140625
Iteration 5100: Loss = -12567.6689453125
Iteration 5200: Loss = -12567.5732421875
Iteration 5300: Loss = -12567.484375
Iteration 5400: Loss = -12567.40234375
Iteration 5500: Loss = -12567.328125
Iteration 5600: Loss = -12567.255859375
Iteration 5700: Loss = -12567.19140625
Iteration 5800: Loss = -12567.12890625
Iteration 5900: Loss = -12567.0712890625
Iteration 6000: Loss = -12567.0166015625
Iteration 6100: Loss = -12566.966796875
Iteration 6200: Loss = -12566.9189453125
Iteration 6300: Loss = -12566.875
Iteration 6400: Loss = -12566.8349609375
Iteration 6500: Loss = -12566.7958984375
Iteration 6600: Loss = -12566.7568359375
Iteration 6700: Loss = -12566.7216796875
Iteration 6800: Loss = -12566.689453125
Iteration 6900: Loss = -12566.658203125
Iteration 7000: Loss = -12566.625
Iteration 7100: Loss = -12566.5966796875
Iteration 7200: Loss = -12566.572265625
Iteration 7300: Loss = -12566.4892578125
Iteration 7400: Loss = -12561.208984375
Iteration 7500: Loss = -12561.06640625
Iteration 7600: Loss = -12560.9853515625
Iteration 7700: Loss = -12560.923828125
Iteration 7800: Loss = -12560.8720703125
Iteration 7900: Loss = -12560.8330078125
Iteration 8000: Loss = -12560.796875
Iteration 8100: Loss = -12560.763671875
Iteration 8200: Loss = -12560.7353515625
Iteration 8300: Loss = -12560.7099609375
Iteration 8400: Loss = -12560.6845703125
Iteration 8500: Loss = -12560.6640625
Iteration 8600: Loss = -12560.64453125
Iteration 8700: Loss = -12560.6259765625
Iteration 8800: Loss = -12560.609375
Iteration 8900: Loss = -12560.59375
Iteration 9000: Loss = -12560.5791015625
Iteration 9100: Loss = -12560.56640625
Iteration 9200: Loss = -12560.552734375
Iteration 9300: Loss = -12560.541015625
Iteration 9400: Loss = -12560.5302734375
Iteration 9500: Loss = -12560.51953125
Iteration 9600: Loss = -12560.5107421875
Iteration 9700: Loss = -12560.5
Iteration 9800: Loss = -12560.4921875
Iteration 9900: Loss = -12560.484375
Iteration 10000: Loss = -12560.4765625
Iteration 10100: Loss = -12560.4697265625
Iteration 10200: Loss = -12560.46484375
Iteration 10300: Loss = -12560.4560546875
Iteration 10400: Loss = -12560.4521484375
Iteration 10500: Loss = -12560.447265625
Iteration 10600: Loss = -12560.4404296875
Iteration 10700: Loss = -12560.43359375
Iteration 10800: Loss = -12560.4296875
Iteration 10900: Loss = -12560.4248046875
Iteration 11000: Loss = -12560.4228515625
Iteration 11100: Loss = -12560.416015625
Iteration 11200: Loss = -12560.4130859375
Iteration 11300: Loss = -12560.41015625
Iteration 11400: Loss = -12560.408203125
Iteration 11500: Loss = -12560.4013671875
Iteration 11600: Loss = -12560.400390625
Iteration 11700: Loss = -12560.396484375
Iteration 11800: Loss = -12560.3935546875
Iteration 11900: Loss = -12560.390625
Iteration 12000: Loss = -12560.388671875
Iteration 12100: Loss = -12560.38671875
Iteration 12200: Loss = -12560.3837890625
Iteration 12300: Loss = -12560.3828125
Iteration 12400: Loss = -12560.37890625
Iteration 12500: Loss = -12560.376953125
Iteration 12600: Loss = -12560.375
Iteration 12700: Loss = -12560.37109375
Iteration 12800: Loss = -12560.365234375
Iteration 12900: Loss = -12560.359375
Iteration 13000: Loss = -12560.3583984375
Iteration 13100: Loss = -12560.3564453125
Iteration 13200: Loss = -12560.353515625
Iteration 13300: Loss = -12560.353515625
Iteration 13400: Loss = -12560.3515625
Iteration 13500: Loss = -12560.349609375
Iteration 13600: Loss = -12560.3466796875
Iteration 13700: Loss = -12560.3427734375
Iteration 13800: Loss = -12560.34375
1
Iteration 13900: Loss = -12560.3388671875
Iteration 14000: Loss = -12560.337890625
Iteration 14100: Loss = -12560.3349609375
Iteration 14200: Loss = -12560.333984375
Iteration 14300: Loss = -12560.3310546875
Iteration 14400: Loss = -12560.326171875
Iteration 14500: Loss = -12560.3251953125
Iteration 14600: Loss = -12560.3212890625
Iteration 14700: Loss = -12560.3173828125
Iteration 14800: Loss = -12560.314453125
Iteration 14900: Loss = -12560.310546875
Iteration 15000: Loss = -12560.3076171875
Iteration 15100: Loss = -12560.3046875
Iteration 15200: Loss = -12560.30078125
Iteration 15300: Loss = -12560.2998046875
Iteration 15400: Loss = -12560.2958984375
Iteration 15500: Loss = -12560.29296875
Iteration 15600: Loss = -12560.291015625
Iteration 15700: Loss = -12560.287109375
Iteration 15800: Loss = -12560.28515625
Iteration 15900: Loss = -12560.2861328125
1
Iteration 16000: Loss = -12560.2841796875
Iteration 16100: Loss = -12560.2802734375
Iteration 16200: Loss = -12560.2802734375
Iteration 16300: Loss = -12560.2802734375
Iteration 16400: Loss = -12560.2783203125
Iteration 16500: Loss = -12560.27734375
Iteration 16600: Loss = -12560.2763671875
Iteration 16700: Loss = -12560.2744140625
Iteration 16800: Loss = -12560.2744140625
Iteration 16900: Loss = -12560.2734375
Iteration 17000: Loss = -12560.2744140625
1
Iteration 17100: Loss = -12560.2734375
Iteration 17200: Loss = -12560.2734375
Iteration 17300: Loss = -12560.271484375
Iteration 17400: Loss = -12560.26953125
Iteration 17500: Loss = -12560.271484375
1
Iteration 17600: Loss = -12560.271484375
2
Iteration 17700: Loss = -12560.2705078125
3
Iteration 17800: Loss = -12560.2705078125
4
Iteration 17900: Loss = -12560.2685546875
Iteration 18000: Loss = -12560.2705078125
1
Iteration 18100: Loss = -12560.2685546875
Iteration 18200: Loss = -12560.26953125
1
Iteration 18300: Loss = -12560.267578125
Iteration 18400: Loss = -12560.2685546875
1
Iteration 18500: Loss = -12560.267578125
Iteration 18600: Loss = -12560.2685546875
1
Iteration 18700: Loss = -12560.2685546875
2
Iteration 18800: Loss = -12560.267578125
Iteration 18900: Loss = -12560.2666015625
Iteration 19000: Loss = -12560.267578125
1
Iteration 19100: Loss = -12560.2666015625
Iteration 19200: Loss = -12560.265625
Iteration 19300: Loss = -12560.265625
Iteration 19400: Loss = -12560.263671875
Iteration 19500: Loss = -12560.263671875
Iteration 19600: Loss = -12560.2587890625
Iteration 19700: Loss = -12560.251953125
Iteration 19800: Loss = -12560.248046875
Iteration 19900: Loss = -12560.244140625
Iteration 20000: Loss = -12560.2412109375
Iteration 20100: Loss = -12560.2412109375
Iteration 20200: Loss = -12560.240234375
Iteration 20300: Loss = -12560.2392578125
Iteration 20400: Loss = -12560.23828125
Iteration 20500: Loss = -12560.2392578125
1
Iteration 20600: Loss = -12560.23828125
Iteration 20700: Loss = -12560.2392578125
1
Iteration 20800: Loss = -12560.2373046875
Iteration 20900: Loss = -12560.236328125
Iteration 21000: Loss = -12560.2373046875
1
Iteration 21100: Loss = -12560.23828125
2
Iteration 21200: Loss = -12560.2373046875
3
Iteration 21300: Loss = -12560.2373046875
4
Iteration 21400: Loss = -12560.23828125
5
Iteration 21500: Loss = -12560.2373046875
6
Iteration 21600: Loss = -12560.236328125
Iteration 21700: Loss = -12560.2373046875
1
Iteration 21800: Loss = -12560.2373046875
2
Iteration 21900: Loss = -12560.236328125
Iteration 22000: Loss = -12560.236328125
Iteration 22100: Loss = -12560.236328125
Iteration 22200: Loss = -12560.2373046875
1
Iteration 22300: Loss = -12560.2353515625
Iteration 22400: Loss = -12560.2373046875
1
Iteration 22500: Loss = -12560.236328125
2
Iteration 22600: Loss = -12560.23828125
3
Iteration 22700: Loss = -12560.236328125
4
Iteration 22800: Loss = -12560.2412109375
5
Iteration 22900: Loss = -12560.2373046875
6
Iteration 23000: Loss = -12560.2373046875
7
Iteration 23100: Loss = -12560.236328125
8
Iteration 23200: Loss = -12560.236328125
9
Iteration 23300: Loss = -12560.236328125
10
Iteration 23400: Loss = -12560.236328125
11
Iteration 23500: Loss = -12560.236328125
12
Iteration 23600: Loss = -12560.236328125
13
Iteration 23700: Loss = -12560.2353515625
Iteration 23800: Loss = -12560.2275390625
Iteration 23900: Loss = -12560.2158203125
Iteration 24000: Loss = -12560.21484375
Iteration 24100: Loss = -12560.205078125
Iteration 24200: Loss = -12560.1923828125
Iteration 24300: Loss = -12560.18359375
Iteration 24400: Loss = -12560.1796875
Iteration 24500: Loss = -12560.1708984375
Iteration 24600: Loss = -12560.1650390625
Iteration 24700: Loss = -12560.1650390625
Iteration 24800: Loss = -12560.1640625
Iteration 24900: Loss = -12560.1640625
Iteration 25000: Loss = -12560.1650390625
1
Iteration 25100: Loss = -12560.1640625
Iteration 25200: Loss = -12560.158203125
Iteration 25300: Loss = -12560.16015625
1
Iteration 25400: Loss = -12560.1533203125
Iteration 25500: Loss = -12560.1513671875
Iteration 25600: Loss = -12560.12890625
Iteration 25700: Loss = -12560.12890625
Iteration 25800: Loss = -12560.1298828125
1
Iteration 25900: Loss = -12560.12890625
Iteration 26000: Loss = -12560.1298828125
1
Iteration 26100: Loss = -12560.1298828125
2
Iteration 26200: Loss = -12560.12890625
Iteration 26300: Loss = -12560.12890625
Iteration 26400: Loss = -12560.12890625
Iteration 26500: Loss = -12560.1298828125
1
Iteration 26600: Loss = -12560.12890625
Iteration 26700: Loss = -12560.1298828125
1
Iteration 26800: Loss = -12560.12890625
Iteration 26900: Loss = -12560.12890625
Iteration 27000: Loss = -12560.12890625
Iteration 27100: Loss = -12560.12890625
Iteration 27200: Loss = -12560.12890625
Iteration 27300: Loss = -12560.1279296875
Iteration 27400: Loss = -12560.130859375
1
Iteration 27500: Loss = -12560.12890625
2
Iteration 27600: Loss = -12560.1298828125
3
Iteration 27700: Loss = -12560.130859375
4
Iteration 27800: Loss = -12560.12890625
5
Iteration 27900: Loss = -12560.12890625
6
Iteration 28000: Loss = -12560.12890625
7
Iteration 28100: Loss = -12560.1279296875
Iteration 28200: Loss = -12560.12890625
1
Iteration 28300: Loss = -12560.1279296875
Iteration 28400: Loss = -12560.1298828125
1
Iteration 28500: Loss = -12560.12890625
2
Iteration 28600: Loss = -12560.12890625
3
Iteration 28700: Loss = -12560.130859375
4
Iteration 28800: Loss = -12560.12890625
5
Iteration 28900: Loss = -12560.130859375
6
Iteration 29000: Loss = -12560.12890625
7
Iteration 29100: Loss = -12560.130859375
8
Iteration 29200: Loss = -12560.1298828125
9
Iteration 29300: Loss = -12560.130859375
10
Iteration 29400: Loss = -12560.1298828125
11
Iteration 29500: Loss = -12560.1298828125
12
Iteration 29600: Loss = -12560.1298828125
13
Iteration 29700: Loss = -12560.12890625
14
Iteration 29800: Loss = -12560.130859375
15
Stopping early at iteration 29800 due to no improvement.
pi: tensor([[9.9477e-01, 5.2344e-03],
        [1.0000e+00, 5.9544e-08]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.7162e-04, 9.9983e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2042, 0.2073],
         [0.9819, 0.2073]],

        [[0.9918, 0.2208],
         [0.0375, 0.9706]],

        [[0.9921, 0.2300],
         [0.9630, 0.3910]],

        [[0.0286, 0.1577],
         [0.4479, 0.0552]],

        [[0.4993, 0.3021],
         [0.8401, 0.0331]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 62
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0040355891165133745
Average Adjusted Rand Index: 0.0
[0.0, 0.0040355891165133745] [0.0, 0.0] [12558.4375, 12560.130859375]
-------------------------------------
This iteration is 4
True Objective function: Loss = -11873.294005635997
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -44813.12890625
Iteration 100: Loss = -27840.173828125
Iteration 200: Loss = -16350.955078125
Iteration 300: Loss = -13737.021484375
Iteration 400: Loss = -13106.2529296875
Iteration 500: Loss = -12894.0751953125
Iteration 600: Loss = -12790.287109375
Iteration 700: Loss = -12705.3525390625
Iteration 800: Loss = -12669.19921875
Iteration 900: Loss = -12637.9052734375
Iteration 1000: Loss = -12615.990234375
Iteration 1100: Loss = -12594.28515625
Iteration 1200: Loss = -12580.787109375
Iteration 1300: Loss = -12570.4208984375
Iteration 1400: Loss = -12558.3837890625
Iteration 1500: Loss = -12547.2490234375
Iteration 1600: Loss = -12542.025390625
Iteration 1700: Loss = -12534.6884765625
Iteration 1800: Loss = -12529.818359375
Iteration 1900: Loss = -12525.044921875
Iteration 2000: Loss = -12521.0517578125
Iteration 2100: Loss = -12517.6513671875
Iteration 2200: Loss = -12515.0302734375
Iteration 2300: Loss = -12513.0498046875
Iteration 2400: Loss = -12511.390625
Iteration 2500: Loss = -12510.0322265625
Iteration 2600: Loss = -12508.8671875
Iteration 2700: Loss = -12507.8369140625
Iteration 2800: Loss = -12506.9248046875
Iteration 2900: Loss = -12506.1005859375
Iteration 3000: Loss = -12505.373046875
Iteration 3100: Loss = -12504.7197265625
Iteration 3200: Loss = -12504.1181640625
Iteration 3300: Loss = -12503.5625
Iteration 3400: Loss = -12503.05859375
Iteration 3500: Loss = -12502.609375
Iteration 3600: Loss = -12502.197265625
Iteration 3700: Loss = -12501.81640625
Iteration 3800: Loss = -12501.4599609375
Iteration 3900: Loss = -12501.12890625
Iteration 4000: Loss = -12500.8173828125
Iteration 4100: Loss = -12500.525390625
Iteration 4200: Loss = -12500.2509765625
Iteration 4300: Loss = -12499.9912109375
Iteration 4400: Loss = -12499.748046875
Iteration 4500: Loss = -12499.5185546875
Iteration 4600: Loss = -12499.287109375
Iteration 4700: Loss = -12496.6572265625
Iteration 4800: Loss = -12494.8251953125
Iteration 4900: Loss = -12494.490234375
Iteration 5000: Loss = -12494.2666015625
Iteration 5100: Loss = -12494.0791015625
Iteration 5200: Loss = -12493.91015625
Iteration 5300: Loss = -12493.755859375
Iteration 5400: Loss = -12493.6171875
Iteration 5500: Loss = -12493.4912109375
Iteration 5600: Loss = -12493.375
Iteration 5700: Loss = -12493.2666015625
Iteration 5800: Loss = -12493.162109375
Iteration 5900: Loss = -12488.9951171875
Iteration 6000: Loss = -12488.806640625
Iteration 6100: Loss = -12488.6923828125
Iteration 6200: Loss = -12488.5947265625
Iteration 6300: Loss = -12488.5
Iteration 6400: Loss = -12488.3681640625
Iteration 6500: Loss = -12487.7548828125
Iteration 6600: Loss = -12487.5537109375
Iteration 6700: Loss = -12487.474609375
Iteration 6800: Loss = -12487.4169921875
Iteration 6900: Loss = -12487.361328125
Iteration 7000: Loss = -12487.310546875
Iteration 7100: Loss = -12487.263671875
Iteration 7200: Loss = -12487.2216796875
Iteration 7300: Loss = -12487.177734375
Iteration 7400: Loss = -12487.1376953125
Iteration 7500: Loss = -12487.0966796875
Iteration 7600: Loss = -12487.0517578125
Iteration 7700: Loss = -12486.9853515625
Iteration 7800: Loss = -12486.8095703125
Iteration 7900: Loss = -12486.658203125
Iteration 8000: Loss = -12486.609375
Iteration 8100: Loss = -12486.5732421875
Iteration 8200: Loss = -12486.537109375
Iteration 8300: Loss = -12486.513671875
Iteration 8400: Loss = -12486.4921875
Iteration 8500: Loss = -12486.470703125
Iteration 8600: Loss = -12486.44921875
Iteration 8700: Loss = -12486.337890625
Iteration 8800: Loss = -12481.8076171875
Iteration 8900: Loss = -12481.6865234375
Iteration 9000: Loss = -12481.6259765625
Iteration 9100: Loss = -12481.5859375
Iteration 9200: Loss = -12481.5546875
Iteration 9300: Loss = -12481.5322265625
Iteration 9400: Loss = -12481.5107421875
Iteration 9500: Loss = -12481.494140625
Iteration 9600: Loss = -12481.478515625
Iteration 9700: Loss = -12481.462890625
Iteration 9800: Loss = -12481.451171875
Iteration 9900: Loss = -12481.4375
Iteration 10000: Loss = -12481.4287109375
Iteration 10100: Loss = -12481.41796875
Iteration 10200: Loss = -12481.408203125
Iteration 10300: Loss = -12481.3994140625
Iteration 10400: Loss = -12481.3935546875
Iteration 10500: Loss = -12481.3828125
Iteration 10600: Loss = -12481.3759765625
Iteration 10700: Loss = -12481.369140625
Iteration 10800: Loss = -12481.3623046875
Iteration 10900: Loss = -12481.35546875
Iteration 11000: Loss = -12481.349609375
Iteration 11100: Loss = -12481.3447265625
Iteration 11200: Loss = -12481.33984375
Iteration 11300: Loss = -12481.3330078125
Iteration 11400: Loss = -12481.3291015625
Iteration 11500: Loss = -12481.32421875
Iteration 11600: Loss = -12481.3203125
Iteration 11700: Loss = -12481.31640625
Iteration 11800: Loss = -12481.3125
Iteration 11900: Loss = -12481.310546875
Iteration 12000: Loss = -12481.306640625
Iteration 12100: Loss = -12481.3037109375
Iteration 12200: Loss = -12481.2998046875
Iteration 12300: Loss = -12481.296875
Iteration 12400: Loss = -12481.2919921875
Iteration 12500: Loss = -12481.2900390625
Iteration 12600: Loss = -12481.283203125
Iteration 12700: Loss = -12481.2783203125
Iteration 12800: Loss = -12481.2763671875
Iteration 12900: Loss = -12481.271484375
Iteration 13000: Loss = -12481.2705078125
Iteration 13100: Loss = -12481.2685546875
Iteration 13200: Loss = -12481.265625
Iteration 13300: Loss = -12481.263671875
Iteration 13400: Loss = -12481.259765625
Iteration 13500: Loss = -12481.2587890625
Iteration 13600: Loss = -12481.2568359375
Iteration 13700: Loss = -12481.2529296875
Iteration 13800: Loss = -12481.251953125
Iteration 13900: Loss = -12481.2509765625
Iteration 14000: Loss = -12481.2490234375
Iteration 14100: Loss = -12481.2470703125
Iteration 14200: Loss = -12481.2470703125
Iteration 14300: Loss = -12481.248046875
1
Iteration 14400: Loss = -12481.244140625
Iteration 14500: Loss = -12481.244140625
Iteration 14600: Loss = -12481.2421875
Iteration 14700: Loss = -12481.2412109375
Iteration 14800: Loss = -12481.240234375
Iteration 14900: Loss = -12481.2412109375
1
Iteration 15000: Loss = -12481.2392578125
Iteration 15100: Loss = -12481.2373046875
Iteration 15200: Loss = -12481.236328125
Iteration 15300: Loss = -12481.234375
Iteration 15400: Loss = -12481.234375
Iteration 15500: Loss = -12481.2333984375
Iteration 15600: Loss = -12481.240234375
1
Iteration 15700: Loss = -12481.232421875
Iteration 15800: Loss = -12481.23046875
Iteration 15900: Loss = -12481.228515625
Iteration 16000: Loss = -12481.228515625
Iteration 16100: Loss = -12481.228515625
Iteration 16200: Loss = -12481.2275390625
Iteration 16300: Loss = -12481.228515625
1
Iteration 16400: Loss = -12481.228515625
2
Iteration 16500: Loss = -12481.2265625
Iteration 16600: Loss = -12481.2275390625
1
Iteration 16700: Loss = -12481.2255859375
Iteration 16800: Loss = -12481.2275390625
1
Iteration 16900: Loss = -12481.2265625
2
Iteration 17000: Loss = -12481.2255859375
Iteration 17100: Loss = -12481.2255859375
Iteration 17200: Loss = -12481.224609375
Iteration 17300: Loss = -12481.2255859375
1
Iteration 17400: Loss = -12481.224609375
Iteration 17500: Loss = -12481.224609375
Iteration 17600: Loss = -12481.224609375
Iteration 17700: Loss = -12481.22265625
Iteration 17800: Loss = -12481.22265625
Iteration 17900: Loss = -12481.22265625
Iteration 18000: Loss = -12481.220703125
Iteration 18100: Loss = -12481.220703125
Iteration 18200: Loss = -12481.2197265625
Iteration 18300: Loss = -12481.2197265625
Iteration 18400: Loss = -12481.2197265625
Iteration 18500: Loss = -12481.220703125
1
Iteration 18600: Loss = -12481.2197265625
Iteration 18700: Loss = -12481.2197265625
Iteration 18800: Loss = -12481.220703125
1
Iteration 18900: Loss = -12481.2197265625
Iteration 19000: Loss = -12481.2197265625
Iteration 19100: Loss = -12481.2197265625
Iteration 19200: Loss = -12481.220703125
1
Iteration 19300: Loss = -12481.220703125
2
Iteration 19400: Loss = -12481.2197265625
Iteration 19500: Loss = -12481.2197265625
Iteration 19600: Loss = -12481.2177734375
Iteration 19700: Loss = -12481.21875
1
Iteration 19800: Loss = -12481.21875
2
Iteration 19900: Loss = -12481.220703125
3
Iteration 20000: Loss = -12481.2177734375
Iteration 20100: Loss = -12481.216796875
Iteration 20200: Loss = -12481.216796875
Iteration 20300: Loss = -12481.2177734375
1
Iteration 20400: Loss = -12481.2158203125
Iteration 20500: Loss = -12481.21875
1
Iteration 20600: Loss = -12481.21875
2
Iteration 20700: Loss = -12481.216796875
3
Iteration 20800: Loss = -12481.2177734375
4
Iteration 20900: Loss = -12481.216796875
5
Iteration 21000: Loss = -12481.2177734375
6
Iteration 21100: Loss = -12481.2177734375
7
Iteration 21200: Loss = -12481.2158203125
Iteration 21300: Loss = -12481.2158203125
Iteration 21400: Loss = -12481.216796875
1
Iteration 21500: Loss = -12481.2158203125
Iteration 21600: Loss = -12481.216796875
1
Iteration 21700: Loss = -12481.216796875
2
Iteration 21800: Loss = -12481.2158203125
Iteration 21900: Loss = -12481.2158203125
Iteration 22000: Loss = -12481.2177734375
1
Iteration 22100: Loss = -12481.2158203125
Iteration 22200: Loss = -12481.2177734375
1
Iteration 22300: Loss = -12481.2158203125
Iteration 22400: Loss = -12481.216796875
1
Iteration 22500: Loss = -12481.2158203125
Iteration 22600: Loss = -12481.2177734375
1
Iteration 22700: Loss = -12481.2158203125
Iteration 22800: Loss = -12481.216796875
1
Iteration 22900: Loss = -12481.21484375
Iteration 23000: Loss = -12481.216796875
1
Iteration 23100: Loss = -12481.21484375
Iteration 23200: Loss = -12481.2197265625
1
Iteration 23300: Loss = -12481.2158203125
2
Iteration 23400: Loss = -12481.216796875
3
Iteration 23500: Loss = -12481.2158203125
4
Iteration 23600: Loss = -12481.216796875
5
Iteration 23700: Loss = -12481.216796875
6
Iteration 23800: Loss = -12481.2177734375
7
Iteration 23900: Loss = -12481.21875
8
Iteration 24000: Loss = -12481.21484375
Iteration 24100: Loss = -12481.2177734375
1
Iteration 24200: Loss = -12481.2158203125
2
Iteration 24300: Loss = -12481.216796875
3
Iteration 24400: Loss = -12481.21484375
Iteration 24500: Loss = -12481.21484375
Iteration 24600: Loss = -12481.216796875
1
Iteration 24700: Loss = -12481.2158203125
2
Iteration 24800: Loss = -12481.216796875
3
Iteration 24900: Loss = -12481.2177734375
4
Iteration 25000: Loss = -12481.2177734375
5
Iteration 25100: Loss = -12481.1923828125
Iteration 25200: Loss = -12481.19140625
Iteration 25300: Loss = -12481.1796875
Iteration 25400: Loss = -12481.166015625
Iteration 25500: Loss = -12481.080078125
Iteration 25600: Loss = -12481.0673828125
Iteration 25700: Loss = -12481.0556640625
Iteration 25800: Loss = -12481.0400390625
Iteration 25900: Loss = -12481.03515625
Iteration 26000: Loss = -12481.001953125
Iteration 26100: Loss = -12480.96875
Iteration 26200: Loss = -12480.947265625
Iteration 26300: Loss = -12480.931640625
Iteration 26400: Loss = -12480.92578125
Iteration 26500: Loss = -12480.9013671875
Iteration 26600: Loss = -12480.9013671875
Iteration 26700: Loss = -12480.8837890625
Iteration 26800: Loss = -12480.8828125
Iteration 26900: Loss = -12480.865234375
Iteration 27000: Loss = -12480.86328125
Iteration 27100: Loss = -12480.87109375
1
Iteration 27200: Loss = -12480.8623046875
Iteration 27300: Loss = -12480.8623046875
Iteration 27400: Loss = -12480.861328125
Iteration 27500: Loss = -12480.861328125
Iteration 27600: Loss = -12480.84765625
Iteration 27700: Loss = -12480.8486328125
1
Iteration 27800: Loss = -12480.8486328125
2
Iteration 27900: Loss = -12480.8486328125
3
Iteration 28000: Loss = -12480.84765625
Iteration 28100: Loss = -12480.8486328125
1
Iteration 28200: Loss = -12480.8486328125
2
Iteration 28300: Loss = -12480.84765625
Iteration 28400: Loss = -12480.84765625
Iteration 28500: Loss = -12480.84765625
Iteration 28600: Loss = -12480.8486328125
1
Iteration 28700: Loss = -12480.8505859375
2
Iteration 28800: Loss = -12480.84765625
Iteration 28900: Loss = -12480.84765625
Iteration 29000: Loss = -12480.8486328125
1
Iteration 29100: Loss = -12480.8466796875
Iteration 29200: Loss = -12480.84765625
1
Iteration 29300: Loss = -12480.84765625
2
Iteration 29400: Loss = -12480.849609375
3
Iteration 29500: Loss = -12480.84765625
4
Iteration 29600: Loss = -12480.84765625
5
Iteration 29700: Loss = -12480.845703125
Iteration 29800: Loss = -12480.8466796875
1
Iteration 29900: Loss = -12480.8486328125
2
pi: tensor([[9.9026e-01, 9.7383e-03],
        [9.9999e-01, 7.3308e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 1.5003e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2020, 0.2080],
         [0.0137, 0.3072]],

        [[0.8408, 0.2263],
         [0.0471, 0.2475]],

        [[0.9811, 0.3202],
         [0.7330, 0.2995]],

        [[0.0616, 0.1808],
         [0.1929, 0.0795]],

        [[0.0890, 0.2576],
         [0.8500, 0.7768]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00036641132057026226
Average Adjusted Rand Index: -0.0004529465619428516
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35758.40625
Iteration 100: Loss = -24185.86328125
Iteration 200: Loss = -16860.09765625
Iteration 300: Loss = -13651.822265625
Iteration 400: Loss = -12801.142578125
Iteration 500: Loss = -12613.2626953125
Iteration 600: Loss = -12556.533203125
Iteration 700: Loss = -12531.8974609375
Iteration 800: Loss = -12517.0849609375
Iteration 900: Loss = -12511.8486328125
Iteration 1000: Loss = -12508.4638671875
Iteration 1100: Loss = -12505.8876953125
Iteration 1200: Loss = -12503.93359375
Iteration 1300: Loss = -12502.1416015625
Iteration 1400: Loss = -12500.330078125
Iteration 1500: Loss = -12498.458984375
Iteration 1600: Loss = -12496.5849609375
Iteration 1700: Loss = -12494.505859375
Iteration 1800: Loss = -12492.6962890625
Iteration 1900: Loss = -12491.0498046875
Iteration 2000: Loss = -12489.4072265625
Iteration 2100: Loss = -12487.70703125
Iteration 2200: Loss = -12485.81640625
Iteration 2300: Loss = -12483.48828125
Iteration 2400: Loss = -12481.1298828125
Iteration 2500: Loss = -12478.44140625
Iteration 2600: Loss = -12474.138671875
Iteration 2700: Loss = -12471.35546875
Iteration 2800: Loss = -12465.8115234375
Iteration 2900: Loss = -12452.9130859375
Iteration 3000: Loss = -12439.4833984375
Iteration 3100: Loss = -12418.044921875
Iteration 3200: Loss = -12381.05078125
Iteration 3300: Loss = -12330.1826171875
Iteration 3400: Loss = -12269.462890625
Iteration 3500: Loss = -12208.9677734375
Iteration 3600: Loss = -12170.873046875
Iteration 3700: Loss = -12138.0576171875
Iteration 3800: Loss = -12102.6435546875
Iteration 3900: Loss = -12049.6962890625
Iteration 4000: Loss = -12001.224609375
Iteration 4100: Loss = -11951.0712890625
Iteration 4200: Loss = -11940.1845703125
Iteration 4300: Loss = -11931.517578125
Iteration 4400: Loss = -11930.697265625
Iteration 4500: Loss = -11930.205078125
Iteration 4600: Loss = -11926.5166015625
Iteration 4700: Loss = -11926.2099609375
Iteration 4800: Loss = -11925.9521484375
Iteration 4900: Loss = -11923.732421875
Iteration 5000: Loss = -11922.4775390625
Iteration 5100: Loss = -11922.3515625
Iteration 5200: Loss = -11922.2509765625
Iteration 5300: Loss = -11922.166015625
Iteration 5400: Loss = -11922.091796875
Iteration 5500: Loss = -11922.025390625
Iteration 5600: Loss = -11921.94140625
Iteration 5700: Loss = -11909.0908203125
Iteration 5800: Loss = -11898.3037109375
Iteration 5900: Loss = -11898.0703125
Iteration 6000: Loss = -11897.9716796875
Iteration 6100: Loss = -11897.9052734375
Iteration 6200: Loss = -11897.8544921875
Iteration 6300: Loss = -11897.8154296875
Iteration 6400: Loss = -11897.779296875
Iteration 6500: Loss = -11897.75
Iteration 6600: Loss = -11897.72265625
Iteration 6700: Loss = -11897.69921875
Iteration 6800: Loss = -11897.677734375
Iteration 6900: Loss = -11897.658203125
Iteration 7000: Loss = -11897.6416015625
Iteration 7100: Loss = -11897.6240234375
Iteration 7200: Loss = -11897.609375
Iteration 7300: Loss = -11897.5966796875
Iteration 7400: Loss = -11897.583984375
Iteration 7500: Loss = -11897.5712890625
Iteration 7600: Loss = -11897.5625
Iteration 7700: Loss = -11897.55078125
Iteration 7800: Loss = -11897.54296875
Iteration 7900: Loss = -11897.5341796875
Iteration 8000: Loss = -11897.52734375
Iteration 8100: Loss = -11897.51953125
Iteration 8200: Loss = -11897.5126953125
Iteration 8300: Loss = -11897.5048828125
Iteration 8400: Loss = -11897.5
Iteration 8500: Loss = -11897.4931640625
Iteration 8600: Loss = -11897.48828125
Iteration 8700: Loss = -11897.4833984375
Iteration 8800: Loss = -11897.4150390625
Iteration 8900: Loss = -11891.0068359375
Iteration 9000: Loss = -11890.95703125
Iteration 9100: Loss = -11890.939453125
Iteration 9200: Loss = -11890.9296875
Iteration 9300: Loss = -11890.923828125
Iteration 9400: Loss = -11890.9169921875
Iteration 9500: Loss = -11890.912109375
Iteration 9600: Loss = -11890.908203125
Iteration 9700: Loss = -11890.90625
Iteration 9800: Loss = -11890.900390625
Iteration 9900: Loss = -11890.8974609375
Iteration 10000: Loss = -11890.8955078125
Iteration 10100: Loss = -11890.892578125
Iteration 10200: Loss = -11890.8896484375
Iteration 10300: Loss = -11890.8876953125
Iteration 10400: Loss = -11890.8857421875
Iteration 10500: Loss = -11890.8837890625
Iteration 10600: Loss = -11890.880859375
Iteration 10700: Loss = -11890.8798828125
Iteration 10800: Loss = -11890.8779296875
Iteration 10900: Loss = -11890.875
Iteration 11000: Loss = -11875.33984375
Iteration 11100: Loss = -11874.4814453125
Iteration 11200: Loss = -11861.1630859375
Iteration 11300: Loss = -11860.9833984375
Iteration 11400: Loss = -11860.9248046875
Iteration 11500: Loss = -11860.8955078125
Iteration 11600: Loss = -11860.8759765625
Iteration 11700: Loss = -11860.8623046875
Iteration 11800: Loss = -11860.853515625
Iteration 11900: Loss = -11860.8466796875
Iteration 12000: Loss = -11860.8408203125
Iteration 12100: Loss = -11860.8349609375
Iteration 12200: Loss = -11860.830078125
Iteration 12300: Loss = -11860.8271484375
Iteration 12400: Loss = -11860.82421875
Iteration 12500: Loss = -11860.8212890625
Iteration 12600: Loss = -11860.8203125
Iteration 12700: Loss = -11860.8173828125
Iteration 12800: Loss = -11860.8154296875
Iteration 12900: Loss = -11860.8134765625
Iteration 13000: Loss = -11860.8115234375
Iteration 13100: Loss = -11860.8115234375
Iteration 13200: Loss = -11860.8095703125
Iteration 13300: Loss = -11860.8095703125
Iteration 13400: Loss = -11860.80859375
Iteration 13500: Loss = -11860.8076171875
Iteration 13600: Loss = -11860.8056640625
Iteration 13700: Loss = -11860.8046875
Iteration 13800: Loss = -11860.8037109375
Iteration 13900: Loss = -11860.802734375
Iteration 14000: Loss = -11860.802734375
Iteration 14100: Loss = -11860.802734375
Iteration 14200: Loss = -11860.80078125
Iteration 14300: Loss = -11860.7998046875
Iteration 14400: Loss = -11860.7998046875
Iteration 14500: Loss = -11860.7978515625
Iteration 14600: Loss = -11860.7978515625
Iteration 14700: Loss = -11860.7978515625
Iteration 14800: Loss = -11860.796875
Iteration 14900: Loss = -11860.796875
Iteration 15000: Loss = -11860.796875
Iteration 15100: Loss = -11860.7958984375
Iteration 15200: Loss = -11860.794921875
Iteration 15300: Loss = -11860.7958984375
1
Iteration 15400: Loss = -11860.7958984375
2
Iteration 15500: Loss = -11860.7939453125
Iteration 15600: Loss = -11860.794921875
1
Iteration 15700: Loss = -11860.7939453125
Iteration 15800: Loss = -11860.794921875
1
Iteration 15900: Loss = -11860.7939453125
Iteration 16000: Loss = -11860.7939453125
Iteration 16100: Loss = -11860.79296875
Iteration 16200: Loss = -11860.79296875
Iteration 16300: Loss = -11860.79296875
Iteration 16400: Loss = -11860.79296875
Iteration 16500: Loss = -11860.7919921875
Iteration 16600: Loss = -11860.79296875
1
Iteration 16700: Loss = -11860.791015625
Iteration 16800: Loss = -11860.7890625
Iteration 16900: Loss = -11860.7890625
Iteration 17000: Loss = -11860.7890625
Iteration 17100: Loss = -11860.7880859375
Iteration 17200: Loss = -11860.7890625
1
Iteration 17300: Loss = -11860.7880859375
Iteration 17400: Loss = -11860.7890625
1
Iteration 17500: Loss = -11860.7880859375
Iteration 17600: Loss = -11860.787109375
Iteration 17700: Loss = -11860.787109375
Iteration 17800: Loss = -11860.7880859375
1
Iteration 17900: Loss = -11860.787109375
Iteration 18000: Loss = -11860.7880859375
1
Iteration 18100: Loss = -11860.7890625
2
Iteration 18200: Loss = -11860.787109375
Iteration 18300: Loss = -11860.787109375
Iteration 18400: Loss = -11860.787109375
Iteration 18500: Loss = -11860.787109375
Iteration 18600: Loss = -11860.7880859375
1
Iteration 18700: Loss = -11860.787109375
Iteration 18800: Loss = -11860.7880859375
1
Iteration 18900: Loss = -11860.7861328125
Iteration 19000: Loss = -11860.787109375
1
Iteration 19100: Loss = -11860.787109375
2
Iteration 19200: Loss = -11860.7861328125
Iteration 19300: Loss = -11860.787109375
1
Iteration 19400: Loss = -11860.787109375
2
Iteration 19500: Loss = -11860.787109375
3
Iteration 19600: Loss = -11860.787109375
4
Iteration 19700: Loss = -11860.787109375
5
Iteration 19800: Loss = -11860.7861328125
Iteration 19900: Loss = -11860.787109375
1
Iteration 20000: Loss = -11860.787109375
2
Iteration 20100: Loss = -11860.787109375
3
Iteration 20200: Loss = -11860.7861328125
Iteration 20300: Loss = -11860.7880859375
1
Iteration 20400: Loss = -11860.787109375
2
Iteration 20500: Loss = -11860.7861328125
Iteration 20600: Loss = -11860.7880859375
1
Iteration 20700: Loss = -11860.7880859375
2
Iteration 20800: Loss = -11860.787109375
3
Iteration 20900: Loss = -11860.765625
Iteration 21000: Loss = -11860.763671875
Iteration 21100: Loss = -11860.763671875
Iteration 21200: Loss = -11860.763671875
Iteration 21300: Loss = -11860.763671875
Iteration 21400: Loss = -11860.763671875
Iteration 21500: Loss = -11860.7626953125
Iteration 21600: Loss = -11860.7626953125
Iteration 21700: Loss = -11860.7626953125
Iteration 21800: Loss = -11860.7626953125
Iteration 21900: Loss = -11860.7626953125
Iteration 22000: Loss = -11860.76171875
Iteration 22100: Loss = -11860.7646484375
1
Iteration 22200: Loss = -11860.7626953125
2
Iteration 22300: Loss = -11860.7626953125
3
Iteration 22400: Loss = -11860.7626953125
4
Iteration 22500: Loss = -11860.76171875
Iteration 22600: Loss = -11860.763671875
1
Iteration 22700: Loss = -11860.7626953125
2
Iteration 22800: Loss = -11860.76171875
Iteration 22900: Loss = -11860.76171875
Iteration 23000: Loss = -11860.7626953125
1
Iteration 23100: Loss = -11860.763671875
2
Iteration 23200: Loss = -11860.7626953125
3
Iteration 23300: Loss = -11860.7626953125
4
Iteration 23400: Loss = -11860.7607421875
Iteration 23500: Loss = -11860.7626953125
1
Iteration 23600: Loss = -11860.7626953125
2
Iteration 23700: Loss = -11860.7626953125
3
Iteration 23800: Loss = -11860.759765625
Iteration 23900: Loss = -11860.76171875
1
Iteration 24000: Loss = -11860.7607421875
2
Iteration 24100: Loss = -11860.759765625
Iteration 24200: Loss = -11860.76171875
1
Iteration 24300: Loss = -11860.7607421875
2
Iteration 24400: Loss = -11860.7607421875
3
Iteration 24500: Loss = -11860.7607421875
4
Iteration 24600: Loss = -11860.759765625
Iteration 24700: Loss = -11860.7607421875
1
Iteration 24800: Loss = -11860.7607421875
2
Iteration 24900: Loss = -11860.7607421875
3
Iteration 25000: Loss = -11860.759765625
Iteration 25100: Loss = -11860.7587890625
Iteration 25200: Loss = -11860.7607421875
1
Iteration 25300: Loss = -11860.759765625
2
Iteration 25400: Loss = -11860.7607421875
3
Iteration 25500: Loss = -11860.76171875
4
Iteration 25600: Loss = -11860.759765625
5
Iteration 25700: Loss = -11860.759765625
6
Iteration 25800: Loss = -11860.759765625
7
Iteration 25900: Loss = -11860.759765625
8
Iteration 26000: Loss = -11860.759765625
9
Iteration 26100: Loss = -11860.7607421875
10
Iteration 26200: Loss = -11860.7607421875
11
Iteration 26300: Loss = -11860.7607421875
12
Iteration 26400: Loss = -11860.7607421875
13
Iteration 26500: Loss = -11860.7607421875
14
Iteration 26600: Loss = -11860.7607421875
15
Stopping early at iteration 26600 due to no improvement.
pi: tensor([[0.2247, 0.7753],
        [0.8055, 0.1945]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4601, 0.5399], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3060, 0.0938],
         [0.7495, 0.3112]],

        [[0.6667, 0.1070],
         [0.4359, 0.0091]],

        [[0.2013, 0.0908],
         [0.9599, 0.0327]],

        [[0.9730, 0.0952],
         [0.3651, 0.6173]],

        [[0.0869, 0.0987],
         [0.6795, 0.9837]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599725554098923
Global Adjusted Rand Index: 0.03648944848768906
Average Adjusted Rand Index: 0.9919945110819786
[-0.00036641132057026226, 0.03648944848768906] [-0.0004529465619428516, 0.9919945110819786] [12480.84765625, 11860.7607421875]
-------------------------------------
This iteration is 5
True Objective function: Loss = -11856.369931225014
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20624.958984375
Iteration 100: Loss = -14938.650390625
Iteration 200: Loss = -12891.9326171875
Iteration 300: Loss = -12600.255859375
Iteration 400: Loss = -12544.9677734375
Iteration 500: Loss = -12514.20703125
Iteration 600: Loss = -12490.0078125
Iteration 700: Loss = -12472.505859375
Iteration 800: Loss = -12443.5908203125
Iteration 900: Loss = -12417.7451171875
Iteration 1000: Loss = -12407.947265625
Iteration 1100: Loss = -12401.2490234375
Iteration 1200: Loss = -12397.5205078125
Iteration 1300: Loss = -12395.8701171875
Iteration 1400: Loss = -12394.7451171875
Iteration 1500: Loss = -12393.9248046875
Iteration 1600: Loss = -12393.298828125
Iteration 1700: Loss = -12392.8046875
Iteration 1800: Loss = -12392.404296875
Iteration 1900: Loss = -12392.0830078125
Iteration 2000: Loss = -12391.8203125
Iteration 2100: Loss = -12391.5986328125
Iteration 2200: Loss = -12391.4140625
Iteration 2300: Loss = -12391.2548828125
Iteration 2400: Loss = -12391.1171875
Iteration 2500: Loss = -12390.9970703125
Iteration 2600: Loss = -12390.892578125
Iteration 2700: Loss = -12390.7978515625
Iteration 2800: Loss = -12390.6689453125
Iteration 2900: Loss = -12385.9765625
Iteration 3000: Loss = -12385.66015625
Iteration 3100: Loss = -12385.4892578125
Iteration 3200: Loss = -12385.3583984375
Iteration 3300: Loss = -12385.251953125
Iteration 3400: Loss = -12385.1572265625
Iteration 3500: Loss = -12385.0654296875
Iteration 3600: Loss = -12384.9384765625
Iteration 3700: Loss = -12384.853515625
Iteration 3800: Loss = -12384.7978515625
Iteration 3900: Loss = -12384.748046875
Iteration 4000: Loss = -12384.7041015625
Iteration 4100: Loss = -12384.6640625
Iteration 4200: Loss = -12384.625
Iteration 4300: Loss = -12384.5908203125
Iteration 4400: Loss = -12384.560546875
Iteration 4500: Loss = -12384.5322265625
Iteration 4600: Loss = -12384.5048828125
Iteration 4700: Loss = -12384.48046875
Iteration 4800: Loss = -12384.45703125
Iteration 4900: Loss = -12384.4365234375
Iteration 5000: Loss = -12384.4169921875
Iteration 5100: Loss = -12384.3984375
Iteration 5200: Loss = -12384.380859375
Iteration 5300: Loss = -12384.365234375
Iteration 5400: Loss = -12384.3515625
Iteration 5500: Loss = -12384.3388671875
Iteration 5600: Loss = -12384.3232421875
Iteration 5700: Loss = -12384.3115234375
Iteration 5800: Loss = -12384.30078125
Iteration 5900: Loss = -12384.2880859375
Iteration 6000: Loss = -12384.279296875
Iteration 6100: Loss = -12384.2607421875
Iteration 6200: Loss = -12384.24609375
Iteration 6300: Loss = -12384.2373046875
Iteration 6400: Loss = -12384.2294921875
Iteration 6500: Loss = -12384.22265625
Iteration 6600: Loss = -12384.2138671875
Iteration 6700: Loss = -12384.2080078125
Iteration 6800: Loss = -12384.201171875
Iteration 6900: Loss = -12384.1943359375
Iteration 7000: Loss = -12384.1875
Iteration 7100: Loss = -12384.181640625
Iteration 7200: Loss = -12384.17578125
Iteration 7300: Loss = -12384.171875
Iteration 7400: Loss = -12384.1669921875
Iteration 7500: Loss = -12384.1630859375
Iteration 7600: Loss = -12384.158203125
Iteration 7700: Loss = -12384.1513671875
Iteration 7800: Loss = -12384.1494140625
Iteration 7900: Loss = -12384.14453125
Iteration 8000: Loss = -12384.140625
Iteration 8100: Loss = -12384.13671875
Iteration 8200: Loss = -12384.1337890625
Iteration 8300: Loss = -12384.12890625
Iteration 8400: Loss = -12384.125
Iteration 8500: Loss = -12384.1201171875
Iteration 8600: Loss = -12384.1162109375
Iteration 8700: Loss = -12384.111328125
Iteration 8800: Loss = -12384.1064453125
Iteration 8900: Loss = -12384.09765625
Iteration 9000: Loss = -12384.0908203125
Iteration 9100: Loss = -12384.0771484375
Iteration 9200: Loss = -12384.0458984375
Iteration 9300: Loss = -12350.1123046875
Iteration 9400: Loss = -12315.3984375
Iteration 9500: Loss = -12203.6376953125
Iteration 9600: Loss = -12071.9287109375
Iteration 9700: Loss = -11978.1357421875
Iteration 9800: Loss = -11940.619140625
Iteration 9900: Loss = -11898.779296875
Iteration 10000: Loss = -11890.0
Iteration 10100: Loss = -11869.4873046875
Iteration 10200: Loss = -11854.638671875
Iteration 10300: Loss = -11853.9248046875
Iteration 10400: Loss = -11853.658203125
Iteration 10500: Loss = -11853.0732421875
Iteration 10600: Loss = -11847.65234375
Iteration 10700: Loss = -11847.5322265625
Iteration 10800: Loss = -11847.4521484375
Iteration 10900: Loss = -11847.3935546875
Iteration 11000: Loss = -11847.34375
Iteration 11100: Loss = -11847.3017578125
Iteration 11200: Loss = -11847.267578125
Iteration 11300: Loss = -11847.23828125
Iteration 11400: Loss = -11847.2119140625
Iteration 11500: Loss = -11847.1884765625
Iteration 11600: Loss = -11847.16796875
Iteration 11700: Loss = -11847.1494140625
Iteration 11800: Loss = -11847.1318359375
Iteration 11900: Loss = -11847.1181640625
Iteration 12000: Loss = -11847.10546875
Iteration 12100: Loss = -11847.09375
Iteration 12200: Loss = -11847.0830078125
Iteration 12300: Loss = -11847.07421875
Iteration 12400: Loss = -11847.064453125
Iteration 12500: Loss = -11847.05859375
Iteration 12600: Loss = -11847.05078125
Iteration 12700: Loss = -11847.0439453125
Iteration 12800: Loss = -11847.0380859375
Iteration 12900: Loss = -11847.0302734375
Iteration 13000: Loss = -11847.025390625
Iteration 13100: Loss = -11847.0205078125
Iteration 13200: Loss = -11847.0166015625
Iteration 13300: Loss = -11847.01171875
Iteration 13400: Loss = -11847.0078125
Iteration 13500: Loss = -11847.00390625
Iteration 13600: Loss = -11847.0
Iteration 13700: Loss = -11846.998046875
Iteration 13800: Loss = -11846.994140625
Iteration 13900: Loss = -11846.9912109375
Iteration 14000: Loss = -11846.990234375
Iteration 14100: Loss = -11846.9873046875
Iteration 14200: Loss = -11846.9853515625
Iteration 14300: Loss = -11846.9814453125
Iteration 14400: Loss = -11846.98046875
Iteration 14500: Loss = -11846.978515625
Iteration 14600: Loss = -11846.9755859375
Iteration 14700: Loss = -11846.974609375
Iteration 14800: Loss = -11846.9736328125
Iteration 14900: Loss = -11846.9716796875
Iteration 15000: Loss = -11846.9697265625
Iteration 15100: Loss = -11846.96875
Iteration 15200: Loss = -11846.9677734375
Iteration 15300: Loss = -11846.966796875
Iteration 15400: Loss = -11846.96484375
Iteration 15500: Loss = -11846.96484375
Iteration 15600: Loss = -11846.962890625
Iteration 15700: Loss = -11846.962890625
Iteration 15800: Loss = -11846.9619140625
Iteration 15900: Loss = -11846.9609375
Iteration 16000: Loss = -11846.9599609375
Iteration 16100: Loss = -11846.958984375
Iteration 16200: Loss = -11846.9580078125
Iteration 16300: Loss = -11846.95703125
Iteration 16400: Loss = -11846.95703125
Iteration 16500: Loss = -11846.9580078125
1
Iteration 16600: Loss = -11846.95703125
Iteration 16700: Loss = -11846.955078125
Iteration 16800: Loss = -11846.955078125
Iteration 16900: Loss = -11846.9541015625
Iteration 17000: Loss = -11846.955078125
1
Iteration 17100: Loss = -11846.953125
Iteration 17200: Loss = -11846.953125
Iteration 17300: Loss = -11846.951171875
Iteration 17400: Loss = -11846.9521484375
1
Iteration 17500: Loss = -11846.951171875
Iteration 17600: Loss = -11846.951171875
Iteration 17700: Loss = -11846.9501953125
Iteration 17800: Loss = -11846.951171875
1
Iteration 17900: Loss = -11846.94921875
Iteration 18000: Loss = -11846.94921875
Iteration 18100: Loss = -11846.9501953125
1
Iteration 18200: Loss = -11846.9501953125
2
Iteration 18300: Loss = -11846.94921875
Iteration 18400: Loss = -11846.9501953125
1
Iteration 18500: Loss = -11846.9501953125
2
Iteration 18600: Loss = -11846.9482421875
Iteration 18700: Loss = -11846.9482421875
Iteration 18800: Loss = -11846.9482421875
Iteration 18900: Loss = -11846.9462890625
Iteration 19000: Loss = -11846.9453125
Iteration 19100: Loss = -11846.9443359375
Iteration 19200: Loss = -11846.93359375
Iteration 19300: Loss = -11846.935546875
1
Iteration 19400: Loss = -11846.935546875
2
Iteration 19500: Loss = -11846.935546875
3
Iteration 19600: Loss = -11846.9345703125
4
Iteration 19700: Loss = -11846.935546875
5
Iteration 19800: Loss = -11846.9345703125
6
Iteration 19900: Loss = -11846.935546875
7
Iteration 20000: Loss = -11846.9345703125
8
Iteration 20100: Loss = -11846.9345703125
9
Iteration 20200: Loss = -11846.93359375
Iteration 20300: Loss = -11846.9345703125
1
Iteration 20400: Loss = -11846.93359375
Iteration 20500: Loss = -11846.93359375
Iteration 20600: Loss = -11846.93359375
Iteration 20700: Loss = -11846.9326171875
Iteration 20800: Loss = -11846.93359375
1
Iteration 20900: Loss = -11846.931640625
Iteration 21000: Loss = -11846.9326171875
1
Iteration 21100: Loss = -11846.93359375
2
Iteration 21200: Loss = -11846.921875
Iteration 21300: Loss = -11846.923828125
1
Iteration 21400: Loss = -11846.9228515625
2
Iteration 21500: Loss = -11846.9228515625
3
Iteration 21600: Loss = -11846.921875
Iteration 21700: Loss = -11846.923828125
1
Iteration 21800: Loss = -11846.9228515625
2
Iteration 21900: Loss = -11846.921875
Iteration 22000: Loss = -11846.9228515625
1
Iteration 22100: Loss = -11846.9228515625
2
Iteration 22200: Loss = -11846.9228515625
3
Iteration 22300: Loss = -11846.9228515625
4
Iteration 22400: Loss = -11846.921875
Iteration 22500: Loss = -11846.921875
Iteration 22600: Loss = -11846.921875
Iteration 22700: Loss = -11846.9208984375
Iteration 22800: Loss = -11846.9208984375
Iteration 22900: Loss = -11846.9208984375
Iteration 23000: Loss = -11846.9208984375
Iteration 23100: Loss = -11846.921875
1
Iteration 23200: Loss = -11846.921875
2
Iteration 23300: Loss = -11846.921875
3
Iteration 23400: Loss = -11846.9208984375
Iteration 23500: Loss = -11846.921875
1
Iteration 23600: Loss = -11846.921875
2
Iteration 23700: Loss = -11846.9228515625
3
Iteration 23800: Loss = -11846.921875
4
Iteration 23900: Loss = -11846.9130859375
Iteration 24000: Loss = -11846.9130859375
Iteration 24100: Loss = -11846.912109375
Iteration 24200: Loss = -11846.9150390625
1
Iteration 24300: Loss = -11846.9130859375
2
Iteration 24400: Loss = -11846.9140625
3
Iteration 24500: Loss = -11846.912109375
Iteration 24600: Loss = -11846.9130859375
1
Iteration 24700: Loss = -11846.9140625
2
Iteration 24800: Loss = -11846.9130859375
3
Iteration 24900: Loss = -11846.9130859375
4
Iteration 25000: Loss = -11846.9140625
5
Iteration 25100: Loss = -11846.9130859375
6
Iteration 25200: Loss = -11846.9130859375
7
Iteration 25300: Loss = -11846.912109375
Iteration 25400: Loss = -11846.9130859375
1
Iteration 25500: Loss = -11846.9130859375
2
Iteration 25600: Loss = -11846.9111328125
Iteration 25700: Loss = -11846.912109375
1
Iteration 25800: Loss = -11846.9130859375
2
Iteration 25900: Loss = -11846.9130859375
3
Iteration 26000: Loss = -11846.912109375
4
Iteration 26100: Loss = -11846.9130859375
5
Iteration 26200: Loss = -11846.9130859375
6
Iteration 26300: Loss = -11846.912109375
7
Iteration 26400: Loss = -11846.912109375
8
Iteration 26500: Loss = -11846.912109375
9
Iteration 26600: Loss = -11846.9140625
10
Iteration 26700: Loss = -11846.9130859375
11
Iteration 26800: Loss = -11846.9130859375
12
Iteration 26900: Loss = -11846.912109375
13
Iteration 27000: Loss = -11846.912109375
14
Iteration 27100: Loss = -11846.9130859375
15
Stopping early at iteration 27100 due to no improvement.
pi: tensor([[0.7874, 0.2126],
        [0.2602, 0.7398]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4051, 0.5949], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3015, 0.1097],
         [0.6690, 0.3059]],

        [[0.0420, 0.0939],
         [0.5867, 0.8235]],

        [[0.0137, 0.0934],
         [0.0774, 0.2033]],

        [[0.8697, 0.1004],
         [0.9835, 0.2050]],

        [[0.8608, 0.0850],
         [0.5333, 0.3023]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9207907017983009
time is 1
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320634059188
Average Adjusted Rand Index: 0.9841581403596603
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -61940.0703125
Iteration 100: Loss = -38467.33984375
Iteration 200: Loss = -20593.15234375
Iteration 300: Loss = -14264.6630859375
Iteration 400: Loss = -13238.943359375
Iteration 500: Loss = -12952.2255859375
Iteration 600: Loss = -12763.939453125
Iteration 700: Loss = -12681.7646484375
Iteration 800: Loss = -12628.7646484375
Iteration 900: Loss = -12591.666015625
Iteration 1000: Loss = -12564.423828125
Iteration 1100: Loss = -12543.70703125
Iteration 1200: Loss = -12527.5185546875
Iteration 1300: Loss = -12514.5888671875
Iteration 1400: Loss = -12504.0732421875
Iteration 1500: Loss = -12495.390625
Iteration 1600: Loss = -12488.126953125
Iteration 1700: Loss = -12481.9814453125
Iteration 1800: Loss = -12476.7353515625
Iteration 1900: Loss = -12472.21484375
Iteration 2000: Loss = -12468.2900390625
Iteration 2100: Loss = -12464.8603515625
Iteration 2200: Loss = -12461.841796875
Iteration 2300: Loss = -12459.17578125
Iteration 2400: Loss = -12456.8056640625
Iteration 2500: Loss = -12454.6904296875
Iteration 2600: Loss = -12452.7939453125
Iteration 2700: Loss = -12451.08984375
Iteration 2800: Loss = -12449.5498046875
Iteration 2900: Loss = -12448.154296875
Iteration 3000: Loss = -12446.8876953125
Iteration 3100: Loss = -12445.734375
Iteration 3200: Loss = -12444.6796875
Iteration 3300: Loss = -12443.71484375
Iteration 3400: Loss = -12442.83203125
Iteration 3500: Loss = -12442.017578125
Iteration 3600: Loss = -12441.2685546875
Iteration 3700: Loss = -12440.578125
Iteration 3800: Loss = -12439.939453125
Iteration 3900: Loss = -12439.349609375
Iteration 4000: Loss = -12438.802734375
Iteration 4100: Loss = -12438.2939453125
Iteration 4200: Loss = -12437.822265625
Iteration 4300: Loss = -12437.3818359375
Iteration 4400: Loss = -12436.974609375
Iteration 4500: Loss = -12436.5927734375
Iteration 4600: Loss = -12436.2353515625
Iteration 4700: Loss = -12435.90234375
Iteration 4800: Loss = -12435.591796875
Iteration 4900: Loss = -12435.30078125
Iteration 5000: Loss = -12435.029296875
Iteration 5100: Loss = -12434.7724609375
Iteration 5200: Loss = -12434.533203125
Iteration 5300: Loss = -12434.3095703125
Iteration 5400: Loss = -12434.09765625
Iteration 5500: Loss = -12433.8984375
Iteration 5600: Loss = -12433.7138671875
Iteration 5700: Loss = -12433.5390625
Iteration 5800: Loss = -12433.3720703125
Iteration 5900: Loss = -12433.216796875
Iteration 6000: Loss = -12433.0712890625
Iteration 6100: Loss = -12432.9326171875
Iteration 6200: Loss = -12432.8037109375
Iteration 6300: Loss = -12432.6806640625
Iteration 6400: Loss = -12432.5654296875
Iteration 6500: Loss = -12432.4560546875
Iteration 6600: Loss = -12432.353515625
Iteration 6700: Loss = -12432.2548828125
Iteration 6800: Loss = -12432.1650390625
Iteration 6900: Loss = -12432.0771484375
Iteration 7000: Loss = -12431.9951171875
Iteration 7100: Loss = -12431.9169921875
Iteration 7200: Loss = -12431.8447265625
Iteration 7300: Loss = -12431.7763671875
Iteration 7400: Loss = -12431.7099609375
Iteration 7500: Loss = -12431.6484375
Iteration 7600: Loss = -12431.58984375
Iteration 7700: Loss = -12431.533203125
Iteration 7800: Loss = -12431.482421875
Iteration 7900: Loss = -12431.4326171875
Iteration 8000: Loss = -12431.3857421875
Iteration 8100: Loss = -12431.341796875
Iteration 8200: Loss = -12431.2998046875
Iteration 8300: Loss = -12431.2607421875
Iteration 8400: Loss = -12431.22265625
Iteration 8500: Loss = -12431.1865234375
Iteration 8600: Loss = -12431.154296875
Iteration 8700: Loss = -12431.123046875
Iteration 8800: Loss = -12431.0908203125
Iteration 8900: Loss = -12431.0634765625
Iteration 9000: Loss = -12431.037109375
Iteration 9100: Loss = -12431.0107421875
Iteration 9200: Loss = -12430.9853515625
Iteration 9300: Loss = -12430.9619140625
Iteration 9400: Loss = -12430.9404296875
Iteration 9500: Loss = -12430.919921875
Iteration 9600: Loss = -12430.900390625
Iteration 9700: Loss = -12430.8818359375
Iteration 9800: Loss = -12430.86328125
Iteration 9900: Loss = -12430.8486328125
Iteration 10000: Loss = -12430.8310546875
Iteration 10100: Loss = -12430.8173828125
Iteration 10200: Loss = -12430.802734375
Iteration 10300: Loss = -12430.7890625
Iteration 10400: Loss = -12430.77734375
Iteration 10500: Loss = -12430.765625
Iteration 10600: Loss = -12430.7529296875
Iteration 10700: Loss = -12430.7431640625
Iteration 10800: Loss = -12430.7333984375
Iteration 10900: Loss = -12430.7236328125
Iteration 11000: Loss = -12430.71484375
Iteration 11100: Loss = -12430.705078125
Iteration 11200: Loss = -12430.7001953125
Iteration 11300: Loss = -12430.6923828125
Iteration 11400: Loss = -12430.68359375
Iteration 11500: Loss = -12430.6767578125
Iteration 11600: Loss = -12430.6728515625
Iteration 11700: Loss = -12430.6650390625
Iteration 11800: Loss = -12430.6611328125
Iteration 11900: Loss = -12430.654296875
Iteration 12000: Loss = -12430.6484375
Iteration 12100: Loss = -12430.64453125
Iteration 12200: Loss = -12430.638671875
Iteration 12300: Loss = -12430.6357421875
Iteration 12400: Loss = -12430.6318359375
Iteration 12500: Loss = -12430.626953125
Iteration 12600: Loss = -12430.6240234375
Iteration 12700: Loss = -12430.6201171875
Iteration 12800: Loss = -12430.6162109375
Iteration 12900: Loss = -12430.6123046875
Iteration 13000: Loss = -12430.6103515625
Iteration 13100: Loss = -12430.6083984375
Iteration 13200: Loss = -12430.607421875
Iteration 13300: Loss = -12430.6044921875
Iteration 13400: Loss = -12430.6025390625
Iteration 13500: Loss = -12430.599609375
Iteration 13600: Loss = -12430.59765625
Iteration 13700: Loss = -12430.59765625
Iteration 13800: Loss = -12430.5947265625
Iteration 13900: Loss = -12430.59375
Iteration 14000: Loss = -12430.591796875
Iteration 14100: Loss = -12430.58984375
Iteration 14200: Loss = -12430.58984375
Iteration 14300: Loss = -12430.587890625
Iteration 14400: Loss = -12430.587890625
Iteration 14500: Loss = -12430.5869140625
Iteration 14600: Loss = -12430.5849609375
Iteration 14700: Loss = -12430.5869140625
1
Iteration 14800: Loss = -12430.5849609375
Iteration 14900: Loss = -12430.5830078125
Iteration 15000: Loss = -12430.5810546875
Iteration 15100: Loss = -12430.580078125
Iteration 15200: Loss = -12430.58203125
1
Iteration 15300: Loss = -12430.580078125
Iteration 15400: Loss = -12430.5791015625
Iteration 15500: Loss = -12430.578125
Iteration 15600: Loss = -12430.5771484375
Iteration 15700: Loss = -12430.5771484375
Iteration 15800: Loss = -12430.5810546875
1
Iteration 15900: Loss = -12430.576171875
Iteration 16000: Loss = -12430.5771484375
1
Iteration 16100: Loss = -12430.5751953125
Iteration 16200: Loss = -12430.57421875
Iteration 16300: Loss = -12430.57421875
Iteration 16400: Loss = -12430.57421875
Iteration 16500: Loss = -12430.57421875
Iteration 16600: Loss = -12430.57421875
Iteration 16700: Loss = -12430.5732421875
Iteration 16800: Loss = -12430.5732421875
Iteration 16900: Loss = -12430.5732421875
Iteration 17000: Loss = -12430.5732421875
Iteration 17100: Loss = -12430.5732421875
Iteration 17200: Loss = -12430.5732421875
Iteration 17300: Loss = -12430.57421875
1
Iteration 17400: Loss = -12430.57421875
2
Iteration 17500: Loss = -12430.5771484375
3
Iteration 17600: Loss = -12430.5712890625
Iteration 17700: Loss = -12430.572265625
1
Iteration 17800: Loss = -12430.5732421875
2
Iteration 17900: Loss = -12430.572265625
3
Iteration 18000: Loss = -12430.572265625
4
Iteration 18100: Loss = -12430.5732421875
5
Iteration 18200: Loss = -12430.5712890625
Iteration 18300: Loss = -12430.572265625
1
Iteration 18400: Loss = -12430.572265625
2
Iteration 18500: Loss = -12430.5712890625
Iteration 18600: Loss = -12430.5703125
Iteration 18700: Loss = -12430.57421875
1
Iteration 18800: Loss = -12430.5712890625
2
Iteration 18900: Loss = -12430.5712890625
3
Iteration 19000: Loss = -12430.5712890625
4
Iteration 19100: Loss = -12430.5712890625
5
Iteration 19200: Loss = -12430.5712890625
6
Iteration 19300: Loss = -12430.5712890625
7
Iteration 19400: Loss = -12430.572265625
8
Iteration 19500: Loss = -12430.572265625
9
Iteration 19600: Loss = -12430.572265625
10
Iteration 19700: Loss = -12430.572265625
11
Iteration 19800: Loss = -12430.5732421875
12
Iteration 19900: Loss = -12430.572265625
13
Iteration 20000: Loss = -12430.5703125
Iteration 20100: Loss = -12430.5712890625
1
Iteration 20200: Loss = -12430.5703125
Iteration 20300: Loss = -12430.572265625
1
Iteration 20400: Loss = -12430.5712890625
2
Iteration 20500: Loss = -12430.5703125
Iteration 20600: Loss = -12430.572265625
1
Iteration 20700: Loss = -12430.5703125
Iteration 20800: Loss = -12430.5712890625
1
Iteration 20900: Loss = -12430.5703125
Iteration 21000: Loss = -12430.5712890625
1
Iteration 21100: Loss = -12430.572265625
2
Iteration 21200: Loss = -12430.5712890625
3
Iteration 21300: Loss = -12430.5703125
Iteration 21400: Loss = -12430.572265625
1
Iteration 21500: Loss = -12430.5703125
Iteration 21600: Loss = -12430.5712890625
1
Iteration 21700: Loss = -12430.5703125
Iteration 21800: Loss = -12430.572265625
1
Iteration 21900: Loss = -12430.5703125
Iteration 22000: Loss = -12430.5703125
Iteration 22100: Loss = -12430.572265625
1
Iteration 22200: Loss = -12430.5703125
Iteration 22300: Loss = -12430.5703125
Iteration 22400: Loss = -12430.572265625
1
Iteration 22500: Loss = -12430.572265625
2
Iteration 22600: Loss = -12430.5712890625
3
Iteration 22700: Loss = -12430.5703125
Iteration 22800: Loss = -12430.572265625
1
Iteration 22900: Loss = -12430.5712890625
2
Iteration 23000: Loss = -12430.5703125
Iteration 23100: Loss = -12430.5703125
Iteration 23200: Loss = -12430.5712890625
1
Iteration 23300: Loss = -12430.5703125
Iteration 23400: Loss = -12430.5693359375
Iteration 23500: Loss = -12430.5712890625
1
Iteration 23600: Loss = -12430.5703125
2
Iteration 23700: Loss = -12430.5703125
3
Iteration 23800: Loss = -12430.5712890625
4
Iteration 23900: Loss = -12430.5693359375
Iteration 24000: Loss = -12430.5703125
1
Iteration 24100: Loss = -12430.5732421875
2
Iteration 24200: Loss = -12430.5693359375
Iteration 24300: Loss = -12430.5712890625
1
Iteration 24400: Loss = -12430.5703125
2
Iteration 24500: Loss = -12430.5712890625
3
Iteration 24600: Loss = -12430.5712890625
4
Iteration 24700: Loss = -12430.5693359375
Iteration 24800: Loss = -12430.5712890625
1
Iteration 24900: Loss = -12430.5712890625
2
Iteration 25000: Loss = -12430.5703125
3
Iteration 25100: Loss = -12430.5712890625
4
Iteration 25200: Loss = -12430.5712890625
5
Iteration 25300: Loss = -12430.5712890625
6
Iteration 25400: Loss = -12430.5712890625
7
Iteration 25500: Loss = -12430.5693359375
Iteration 25600: Loss = -12430.5712890625
1
Iteration 25700: Loss = -12430.5712890625
2
Iteration 25800: Loss = -12430.5712890625
3
Iteration 25900: Loss = -12430.5712890625
4
Iteration 26000: Loss = -12430.5712890625
5
Iteration 26100: Loss = -12430.5712890625
6
Iteration 26200: Loss = -12430.5703125
7
Iteration 26300: Loss = -12430.5712890625
8
Iteration 26400: Loss = -12430.5712890625
9
Iteration 26500: Loss = -12430.5712890625
10
Iteration 26600: Loss = -12430.572265625
11
Iteration 26700: Loss = -12430.5712890625
12
Iteration 26800: Loss = -12430.5712890625
13
Iteration 26900: Loss = -12430.5693359375
Iteration 27000: Loss = -12430.5712890625
1
Iteration 27100: Loss = -12430.5703125
2
Iteration 27200: Loss = -12430.5712890625
3
Iteration 27300: Loss = -12430.5693359375
Iteration 27400: Loss = -12430.5712890625
1
Iteration 27500: Loss = -12430.5712890625
2
Iteration 27600: Loss = -12430.5712890625
3
Iteration 27700: Loss = -12430.572265625
4
Iteration 27800: Loss = -12430.572265625
5
Iteration 27900: Loss = -12430.572265625
6
Iteration 28000: Loss = -12430.5712890625
7
Iteration 28100: Loss = -12430.5712890625
8
Iteration 28200: Loss = -12430.5712890625
9
Iteration 28300: Loss = -12430.572265625
10
Iteration 28400: Loss = -12430.5712890625
11
Iteration 28500: Loss = -12430.5703125
12
Iteration 28600: Loss = -12430.572265625
13
Iteration 28700: Loss = -12430.5712890625
14
Iteration 28800: Loss = -12430.5712890625
15
Stopping early at iteration 28800 due to no improvement.
pi: tensor([[2.9689e-02, 9.7031e-01],
        [5.1743e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([3.7062e-06, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.9561, 0.2157],
         [0.0131, 0.2013]],

        [[0.1004, 0.2034],
         [0.3223, 0.1366]],

        [[0.6060, 0.2204],
         [0.4026, 0.0078]],

        [[0.8088, 0.2238],
         [0.6241, 0.2171]],

        [[0.0390, 0.1984],
         [0.0128, 0.0747]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.9840320634059188, 0.0] [0.9841581403596603, 0.0] [11846.9130859375, 12430.5712890625]
-------------------------------------
This iteration is 6
True Objective function: Loss = -11869.35784304604
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -40885.046875
Iteration 100: Loss = -27085.169921875
Iteration 200: Loss = -16896.232421875
Iteration 300: Loss = -13493.439453125
Iteration 400: Loss = -12749.9658203125
Iteration 500: Loss = -12556.69921875
Iteration 600: Loss = -12478.830078125
Iteration 700: Loss = -12448.005859375
Iteration 800: Loss = -12429.8095703125
Iteration 900: Loss = -12417.0546875
Iteration 1000: Loss = -12407.130859375
Iteration 1100: Loss = -12398.8359375
Iteration 1200: Loss = -12392.037109375
Iteration 1300: Loss = -12385.830078125
Iteration 1400: Loss = -12379.7265625
Iteration 1500: Loss = -12372.806640625
Iteration 1600: Loss = -12362.958984375
Iteration 1700: Loss = -12352.396484375
Iteration 1800: Loss = -12334.205078125
Iteration 1900: Loss = -12317.3798828125
Iteration 2000: Loss = -12303.8232421875
Iteration 2100: Loss = -12277.8740234375
Iteration 2200: Loss = -12246.5615234375
Iteration 2300: Loss = -12197.361328125
Iteration 2400: Loss = -12163.5009765625
Iteration 2500: Loss = -12141.6865234375
Iteration 2600: Loss = -12124.8173828125
Iteration 2700: Loss = -12106.921875
Iteration 2800: Loss = -12099.5205078125
Iteration 2900: Loss = -12089.58203125
Iteration 3000: Loss = -12083.1748046875
Iteration 3100: Loss = -12079.427734375
Iteration 3200: Loss = -12075.2275390625
Iteration 3300: Loss = -12074.5791015625
Iteration 3400: Loss = -12074.1435546875
Iteration 3500: Loss = -12073.783203125
Iteration 3600: Loss = -12073.466796875
Iteration 3700: Loss = -12073.1640625
Iteration 3800: Loss = -12072.7451171875
Iteration 3900: Loss = -12070.6220703125
Iteration 4000: Loss = -12062.56640625
Iteration 4100: Loss = -12061.7900390625
Iteration 4200: Loss = -12061.5810546875
Iteration 4300: Loss = -12061.4130859375
Iteration 4400: Loss = -12061.2646484375
Iteration 4500: Loss = -12061.130859375
Iteration 4600: Loss = -12061.0087890625
Iteration 4700: Loss = -12060.896484375
Iteration 4800: Loss = -12060.79296875
Iteration 4900: Loss = -12060.697265625
Iteration 5000: Loss = -12060.6083984375
Iteration 5100: Loss = -12060.525390625
Iteration 5200: Loss = -12060.447265625
Iteration 5300: Loss = -12060.3740234375
Iteration 5400: Loss = -12060.306640625
Iteration 5500: Loss = -12060.2412109375
Iteration 5600: Loss = -12060.1806640625
Iteration 5700: Loss = -12060.1083984375
Iteration 5800: Loss = -12055.115234375
Iteration 5900: Loss = -12054.9990234375
Iteration 6000: Loss = -12054.9384765625
Iteration 6100: Loss = -12054.890625
Iteration 6200: Loss = -12054.845703125
Iteration 6300: Loss = -12054.806640625
Iteration 6400: Loss = -12054.767578125
Iteration 6500: Loss = -12054.7333984375
Iteration 6600: Loss = -12054.69921875
Iteration 6700: Loss = -12054.66796875
Iteration 6800: Loss = -12054.6376953125
Iteration 6900: Loss = -12054.607421875
Iteration 7000: Loss = -12054.56640625
Iteration 7100: Loss = -12054.2744140625
Iteration 7200: Loss = -12054.154296875
Iteration 7300: Loss = -12054.1298828125
Iteration 7400: Loss = -12054.109375
Iteration 7500: Loss = -12054.0888671875
Iteration 7600: Loss = -12054.0693359375
Iteration 7700: Loss = -12054.052734375
Iteration 7800: Loss = -12054.0361328125
Iteration 7900: Loss = -12054.021484375
Iteration 8000: Loss = -12054.0068359375
Iteration 8100: Loss = -12053.9931640625
Iteration 8200: Loss = -12053.9794921875
Iteration 8300: Loss = -12053.966796875
Iteration 8400: Loss = -12053.955078125
Iteration 8500: Loss = -12053.9443359375
Iteration 8600: Loss = -12053.93359375
Iteration 8700: Loss = -12053.923828125
Iteration 8800: Loss = -12053.9140625
Iteration 8900: Loss = -12053.90625
Iteration 9000: Loss = -12053.8974609375
Iteration 9100: Loss = -12053.888671875
Iteration 9200: Loss = -12053.880859375
Iteration 9300: Loss = -12053.875
Iteration 9400: Loss = -12053.8671875
Iteration 9500: Loss = -12053.8603515625
Iteration 9600: Loss = -12053.8427734375
Iteration 9700: Loss = -12053.8359375
Iteration 9800: Loss = -12053.8310546875
Iteration 9900: Loss = -12053.8251953125
Iteration 10000: Loss = -12053.8193359375
Iteration 10100: Loss = -12053.8154296875
Iteration 10200: Loss = -12053.810546875
Iteration 10300: Loss = -12053.806640625
Iteration 10400: Loss = -12053.80078125
Iteration 10500: Loss = -12053.7998046875
Iteration 10600: Loss = -12053.7939453125
Iteration 10700: Loss = -12053.7919921875
Iteration 10800: Loss = -12053.7880859375
Iteration 10900: Loss = -12053.787109375
Iteration 11000: Loss = -12053.7822265625
Iteration 11100: Loss = -12053.779296875
Iteration 11200: Loss = -12053.7783203125
Iteration 11300: Loss = -12053.7744140625
Iteration 11400: Loss = -12053.7724609375
Iteration 11500: Loss = -12053.76953125
Iteration 11600: Loss = -12053.765625
Iteration 11700: Loss = -12053.763671875
Iteration 11800: Loss = -12053.7626953125
Iteration 11900: Loss = -12053.7607421875
Iteration 12000: Loss = -12053.7587890625
Iteration 12100: Loss = -12053.7568359375
Iteration 12200: Loss = -12053.7548828125
Iteration 12300: Loss = -12053.75390625
Iteration 12400: Loss = -12053.751953125
Iteration 12500: Loss = -12053.75
Iteration 12600: Loss = -12053.7490234375
Iteration 12700: Loss = -12053.7490234375
Iteration 12800: Loss = -12053.74609375
Iteration 12900: Loss = -12053.7451171875
Iteration 13000: Loss = -12053.74609375
1
Iteration 13100: Loss = -12053.744140625
Iteration 13200: Loss = -12053.7421875
Iteration 13300: Loss = -12053.7431640625
1
Iteration 13400: Loss = -12053.7412109375
Iteration 13500: Loss = -12053.740234375
Iteration 13600: Loss = -12053.7392578125
Iteration 13700: Loss = -12053.73828125
Iteration 13800: Loss = -12053.7373046875
Iteration 13900: Loss = -12053.7373046875
Iteration 14000: Loss = -12053.7373046875
Iteration 14100: Loss = -12053.7353515625
Iteration 14200: Loss = -12053.7333984375
Iteration 14300: Loss = -12053.734375
1
Iteration 14400: Loss = -12053.734375
2
Iteration 14500: Loss = -12053.7333984375
Iteration 14600: Loss = -12053.732421875
Iteration 14700: Loss = -12053.732421875
Iteration 14800: Loss = -12053.7314453125
Iteration 14900: Loss = -12053.7314453125
Iteration 15000: Loss = -12047.4853515625
Iteration 15100: Loss = -12047.2978515625
Iteration 15200: Loss = -12047.2802734375
Iteration 15300: Loss = -12047.271484375
Iteration 15400: Loss = -12047.267578125
Iteration 15500: Loss = -12047.265625
Iteration 15600: Loss = -12047.263671875
Iteration 15700: Loss = -12047.2626953125
Iteration 15800: Loss = -12047.26171875
Iteration 15900: Loss = -12047.2607421875
Iteration 16000: Loss = -12047.259765625
Iteration 16100: Loss = -12047.2587890625
Iteration 16200: Loss = -12047.2587890625
Iteration 16300: Loss = -12047.2578125
Iteration 16400: Loss = -12047.2587890625
1
Iteration 16500: Loss = -12047.2587890625
2
Iteration 16600: Loss = -12047.2568359375
Iteration 16700: Loss = -12047.255859375
Iteration 16800: Loss = -12047.255859375
Iteration 16900: Loss = -12047.2568359375
1
Iteration 17000: Loss = -12047.2548828125
Iteration 17100: Loss = -12047.255859375
1
Iteration 17200: Loss = -12047.255859375
2
Iteration 17300: Loss = -12047.255859375
3
Iteration 17400: Loss = -12047.2548828125
Iteration 17500: Loss = -12047.25390625
Iteration 17600: Loss = -12047.2548828125
1
Iteration 17700: Loss = -12047.25390625
Iteration 17800: Loss = -12047.25390625
Iteration 17900: Loss = -12047.2548828125
1
Iteration 18000: Loss = -12047.2548828125
2
Iteration 18100: Loss = -12047.2529296875
Iteration 18200: Loss = -12047.25390625
1
Iteration 18300: Loss = -12047.25390625
2
Iteration 18400: Loss = -12047.25390625
3
Iteration 18500: Loss = -12047.25390625
4
Iteration 18600: Loss = -12047.2529296875
Iteration 18700: Loss = -12047.2529296875
Iteration 18800: Loss = -12047.25390625
1
Iteration 18900: Loss = -12047.25390625
2
Iteration 19000: Loss = -12047.2548828125
3
Iteration 19100: Loss = -12047.2529296875
Iteration 19200: Loss = -12047.2529296875
Iteration 19300: Loss = -12047.2529296875
Iteration 19400: Loss = -12047.2529296875
Iteration 19500: Loss = -12047.2529296875
Iteration 19600: Loss = -12047.251953125
Iteration 19700: Loss = -12047.25390625
1
Iteration 19800: Loss = -12047.2529296875
2
Iteration 19900: Loss = -12047.2529296875
3
Iteration 20000: Loss = -12047.2529296875
4
Iteration 20100: Loss = -12047.251953125
Iteration 20200: Loss = -12047.2529296875
1
Iteration 20300: Loss = -12047.2548828125
2
Iteration 20400: Loss = -12047.2529296875
3
Iteration 20500: Loss = -12047.25390625
4
Iteration 20600: Loss = -12047.251953125
Iteration 20700: Loss = -12047.25390625
1
Iteration 20800: Loss = -12047.251953125
Iteration 20900: Loss = -12047.25390625
1
Iteration 21000: Loss = -12047.251953125
Iteration 21100: Loss = -12047.25390625
1
Iteration 21200: Loss = -12047.2529296875
2
Iteration 21300: Loss = -12047.2529296875
3
Iteration 21400: Loss = -12047.25390625
4
Iteration 21500: Loss = -12047.25390625
5
Iteration 21600: Loss = -12047.25390625
6
Iteration 21700: Loss = -12047.251953125
Iteration 21800: Loss = -12047.2529296875
1
Iteration 21900: Loss = -12047.251953125
Iteration 22000: Loss = -12047.25390625
1
Iteration 22100: Loss = -12047.2529296875
2
Iteration 22200: Loss = -12047.2529296875
3
Iteration 22300: Loss = -12047.25390625
4
Iteration 22400: Loss = -12047.2548828125
5
Iteration 22500: Loss = -12047.2529296875
6
Iteration 22600: Loss = -12047.2529296875
7
Iteration 22700: Loss = -12047.2529296875
8
Iteration 22800: Loss = -12047.2529296875
9
Iteration 22900: Loss = -12047.2529296875
10
Iteration 23000: Loss = -12047.2529296875
11
Iteration 23100: Loss = -12047.251953125
Iteration 23200: Loss = -12047.2529296875
1
Iteration 23300: Loss = -12047.251953125
Iteration 23400: Loss = -12047.25390625
1
Iteration 23500: Loss = -12047.2529296875
2
Iteration 23600: Loss = -12047.2529296875
3
Iteration 23700: Loss = -12047.2529296875
4
Iteration 23800: Loss = -12047.2529296875
5
Iteration 23900: Loss = -12047.2529296875
6
Iteration 24000: Loss = -12047.251953125
Iteration 24100: Loss = -12047.25390625
1
Iteration 24200: Loss = -12047.2529296875
2
Iteration 24300: Loss = -12047.25390625
3
Iteration 24400: Loss = -12047.25390625
4
Iteration 24500: Loss = -12047.251953125
Iteration 24600: Loss = -12047.25390625
1
Iteration 24700: Loss = -12047.25390625
2
Iteration 24800: Loss = -12047.2529296875
3
Iteration 24900: Loss = -12047.25390625
4
Iteration 25000: Loss = -12047.25390625
5
Iteration 25100: Loss = -12047.2529296875
6
Iteration 25200: Loss = -12047.251953125
Iteration 25300: Loss = -12047.2509765625
Iteration 25400: Loss = -12047.251953125
1
Iteration 25500: Loss = -12047.251953125
2
Iteration 25600: Loss = -12047.251953125
3
Iteration 25700: Loss = -12047.255859375
4
Iteration 25800: Loss = -12047.251953125
5
Iteration 25900: Loss = -12047.25390625
6
Iteration 26000: Loss = -12047.251953125
7
Iteration 26100: Loss = -12047.251953125
8
Iteration 26200: Loss = -12047.251953125
9
Iteration 26300: Loss = -12047.2529296875
10
Iteration 26400: Loss = -12047.25390625
11
Iteration 26500: Loss = -12047.2509765625
Iteration 26600: Loss = -12047.25390625
1
Iteration 26700: Loss = -12047.2529296875
2
Iteration 26800: Loss = -12047.251953125
3
Iteration 26900: Loss = -12047.25390625
4
Iteration 27000: Loss = -12047.251953125
5
Iteration 27100: Loss = -12047.2529296875
6
Iteration 27200: Loss = -12047.251953125
7
Iteration 27300: Loss = -12047.2529296875
8
Iteration 27400: Loss = -12047.2548828125
9
Iteration 27500: Loss = -12047.2529296875
10
Iteration 27600: Loss = -12047.2529296875
11
Iteration 27700: Loss = -12047.2529296875
12
Iteration 27800: Loss = -12047.25390625
13
Iteration 27900: Loss = -12047.251953125
14
Iteration 28000: Loss = -12047.2529296875
15
Stopping early at iteration 28000 due to no improvement.
pi: tensor([[0.4293, 0.5707],
        [0.5100, 0.4900]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([2.1913e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2976, 0.1935],
         [0.0078, 0.2536]],

        [[0.1306, 0.1008],
         [0.0519, 0.8692]],

        [[0.8757, 0.0964],
         [0.0605, 0.0294]],

        [[0.0078, 0.1013],
         [0.9808, 0.9713]],

        [[0.0530, 0.0935],
         [0.9523, 0.0203]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: -0.0019389008758148925
Average Adjusted Rand Index: 0.784
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -45524.71484375
Iteration 100: Loss = -32018.544921875
Iteration 200: Loss = -21937.62109375
Iteration 300: Loss = -16289.5966796875
Iteration 400: Loss = -13735.896484375
Iteration 500: Loss = -12704.81640625
Iteration 600: Loss = -12341.525390625
Iteration 700: Loss = -12203.126953125
Iteration 800: Loss = -12143.0146484375
Iteration 900: Loss = -12116.7880859375
Iteration 1000: Loss = -12095.83203125
Iteration 1100: Loss = -12072.7021484375
Iteration 1200: Loss = -12031.98046875
Iteration 1300: Loss = -11970.2353515625
Iteration 1400: Loss = -11928.9208984375
Iteration 1500: Loss = -11897.8583984375
Iteration 1600: Loss = -11879.06640625
Iteration 1700: Loss = -11874.84375
Iteration 1800: Loss = -11872.76953125
Iteration 1900: Loss = -11871.57421875
Iteration 2000: Loss = -11870.75
Iteration 2100: Loss = -11870.03125
Iteration 2200: Loss = -11869.525390625
Iteration 2300: Loss = -11869.103515625
Iteration 2400: Loss = -11868.7578125
Iteration 2500: Loss = -11868.4697265625
Iteration 2600: Loss = -11868.220703125
Iteration 2700: Loss = -11868.0068359375
Iteration 2800: Loss = -11867.818359375
Iteration 2900: Loss = -11867.6513671875
Iteration 3000: Loss = -11867.5048828125
Iteration 3100: Loss = -11867.373046875
Iteration 3200: Loss = -11867.2568359375
Iteration 3300: Loss = -11867.15234375
Iteration 3400: Loss = -11867.05859375
Iteration 3500: Loss = -11866.9736328125
Iteration 3600: Loss = -11866.896484375
Iteration 3700: Loss = -11866.8251953125
Iteration 3800: Loss = -11866.7568359375
Iteration 3900: Loss = -11866.689453125
Iteration 4000: Loss = -11866.5126953125
Iteration 4100: Loss = -11857.951171875
Iteration 4200: Loss = -11857.791015625
Iteration 4300: Loss = -11857.7197265625
Iteration 4400: Loss = -11857.6689453125
Iteration 4500: Loss = -11857.623046875
Iteration 4600: Loss = -11857.5849609375
Iteration 4700: Loss = -11857.548828125
Iteration 4800: Loss = -11857.5166015625
Iteration 4900: Loss = -11857.4873046875
Iteration 5000: Loss = -11857.4599609375
Iteration 5100: Loss = -11857.435546875
Iteration 5200: Loss = -11857.4111328125
Iteration 5300: Loss = -11857.388671875
Iteration 5400: Loss = -11857.3681640625
Iteration 5500: Loss = -11857.3505859375
Iteration 5600: Loss = -11857.3310546875
Iteration 5700: Loss = -11857.3154296875
Iteration 5800: Loss = -11857.298828125
Iteration 5900: Loss = -11857.28515625
Iteration 6000: Loss = -11857.2724609375
Iteration 6100: Loss = -11857.2587890625
Iteration 6200: Loss = -11857.2470703125
Iteration 6300: Loss = -11857.236328125
Iteration 6400: Loss = -11857.2255859375
Iteration 6500: Loss = -11857.21484375
Iteration 6600: Loss = -11857.205078125
Iteration 6700: Loss = -11857.197265625
Iteration 6800: Loss = -11857.1884765625
Iteration 6900: Loss = -11857.1806640625
Iteration 7000: Loss = -11857.173828125
Iteration 7100: Loss = -11857.1669921875
Iteration 7200: Loss = -11857.1611328125
Iteration 7300: Loss = -11857.1552734375
Iteration 7400: Loss = -11857.1494140625
Iteration 7500: Loss = -11857.1435546875
Iteration 7600: Loss = -11857.1376953125
Iteration 7700: Loss = -11857.1328125
Iteration 7800: Loss = -11857.1279296875
Iteration 7900: Loss = -11857.1240234375
Iteration 8000: Loss = -11857.119140625
Iteration 8100: Loss = -11857.1171875
Iteration 8200: Loss = -11857.11328125
Iteration 8300: Loss = -11857.109375
Iteration 8400: Loss = -11857.1044921875
Iteration 8500: Loss = -11857.1025390625
Iteration 8600: Loss = -11857.1005859375
Iteration 8700: Loss = -11857.0966796875
Iteration 8800: Loss = -11857.0947265625
Iteration 8900: Loss = -11857.091796875
Iteration 9000: Loss = -11857.08984375
Iteration 9100: Loss = -11857.0869140625
Iteration 9200: Loss = -11857.083984375
Iteration 9300: Loss = -11857.0830078125
Iteration 9400: Loss = -11857.080078125
Iteration 9500: Loss = -11857.0791015625
Iteration 9600: Loss = -11857.078125
Iteration 9700: Loss = -11857.076171875
Iteration 9800: Loss = -11857.07421875
Iteration 9900: Loss = -11857.0732421875
Iteration 10000: Loss = -11857.0712890625
Iteration 10100: Loss = -11857.0703125
Iteration 10200: Loss = -11857.0693359375
Iteration 10300: Loss = -11857.068359375
Iteration 10400: Loss = -11857.0673828125
Iteration 10500: Loss = -11857.06640625
Iteration 10600: Loss = -11857.0654296875
Iteration 10700: Loss = -11857.064453125
Iteration 10800: Loss = -11857.0625
Iteration 10900: Loss = -11857.0625
Iteration 11000: Loss = -11857.0615234375
Iteration 11100: Loss = -11857.060546875
Iteration 11200: Loss = -11857.0595703125
Iteration 11300: Loss = -11857.0595703125
Iteration 11400: Loss = -11857.0595703125
Iteration 11500: Loss = -11857.0576171875
Iteration 11600: Loss = -11857.05859375
1
Iteration 11700: Loss = -11857.056640625
Iteration 11800: Loss = -11857.056640625
Iteration 11900: Loss = -11857.0556640625
Iteration 12000: Loss = -11857.0556640625
Iteration 12100: Loss = -11857.0546875
Iteration 12200: Loss = -11857.0537109375
Iteration 12300: Loss = -11857.0546875
1
Iteration 12400: Loss = -11857.0537109375
Iteration 12500: Loss = -11857.0537109375
Iteration 12600: Loss = -11857.0537109375
Iteration 12700: Loss = -11857.052734375
Iteration 12800: Loss = -11857.052734375
Iteration 12900: Loss = -11857.0517578125
Iteration 13000: Loss = -11857.052734375
1
Iteration 13100: Loss = -11857.052734375
2
Iteration 13200: Loss = -11857.0517578125
Iteration 13300: Loss = -11857.05078125
Iteration 13400: Loss = -11857.05078125
Iteration 13500: Loss = -11857.05078125
Iteration 13600: Loss = -11857.0517578125
1
Iteration 13700: Loss = -11857.05078125
Iteration 13800: Loss = -11857.0498046875
Iteration 13900: Loss = -11857.0498046875
Iteration 14000: Loss = -11857.0498046875
Iteration 14100: Loss = -11857.0498046875
Iteration 14200: Loss = -11857.048828125
Iteration 14300: Loss = -11857.0498046875
1
Iteration 14400: Loss = -11857.0498046875
2
Iteration 14500: Loss = -11857.0498046875
3
Iteration 14600: Loss = -11857.048828125
Iteration 14700: Loss = -11857.0498046875
1
Iteration 14800: Loss = -11857.0498046875
2
Iteration 14900: Loss = -11857.048828125
Iteration 15000: Loss = -11857.0498046875
1
Iteration 15100: Loss = -11857.0478515625
Iteration 15200: Loss = -11857.048828125
1
Iteration 15300: Loss = -11857.048828125
2
Iteration 15400: Loss = -11857.048828125
3
Iteration 15500: Loss = -11857.048828125
4
Iteration 15600: Loss = -11857.0478515625
Iteration 15700: Loss = -11857.0458984375
Iteration 15800: Loss = -11857.046875
1
Iteration 15900: Loss = -11857.0478515625
2
Iteration 16000: Loss = -11857.0478515625
3
Iteration 16100: Loss = -11857.048828125
4
Iteration 16200: Loss = -11857.0478515625
5
Iteration 16300: Loss = -11857.0478515625
6
Iteration 16400: Loss = -11857.0478515625
7
Iteration 16500: Loss = -11857.0478515625
8
Iteration 16600: Loss = -11857.048828125
9
Iteration 16700: Loss = -11857.048828125
10
Iteration 16800: Loss = -11857.048828125
11
Iteration 16900: Loss = -11857.0478515625
12
Iteration 17000: Loss = -11857.0478515625
13
Iteration 17100: Loss = -11857.0478515625
14
Iteration 17200: Loss = -11857.046875
15
Stopping early at iteration 17200 due to no improvement.
pi: tensor([[0.2536, 0.7464],
        [0.6782, 0.3218]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4777, 0.5223], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2941, 0.1040],
         [0.8446, 0.3051]],

        [[0.9690, 0.1010],
         [0.2300, 0.7766]],

        [[0.8704, 0.0968],
         [0.9666, 0.1190]],

        [[0.0505, 0.1010],
         [0.1327, 0.1126]],

        [[0.9207, 0.0935],
         [0.0757, 0.1192]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.036488313267081254
Average Adjusted Rand Index: 0.97599868345458
[-0.0019389008758148925, 0.036488313267081254] [0.784, 0.97599868345458] [12047.2529296875, 11857.046875]
-------------------------------------
This iteration is 7
True Objective function: Loss = -11844.248878522678
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21657.25390625
Iteration 100: Loss = -16290.5673828125
Iteration 200: Loss = -13174.3974609375
Iteration 300: Loss = -12622.1015625
Iteration 400: Loss = -12520.392578125
Iteration 500: Loss = -12476.2880859375
Iteration 600: Loss = -12454.1845703125
Iteration 700: Loss = -12439.4921875
Iteration 800: Loss = -12425.4033203125
Iteration 900: Loss = -12415.3828125
Iteration 1000: Loss = -12408.22265625
Iteration 1100: Loss = -12402.62109375
Iteration 1200: Loss = -12397.888671875
Iteration 1300: Loss = -12394.1123046875
Iteration 1400: Loss = -12390.744140625
Iteration 1500: Loss = -12385.6533203125
Iteration 1600: Loss = -12381.0068359375
Iteration 1700: Loss = -12377.2275390625
Iteration 1800: Loss = -12375.2412109375
Iteration 1900: Loss = -12373.9619140625
Iteration 2000: Loss = -12372.904296875
Iteration 2100: Loss = -12368.7421875
Iteration 2200: Loss = -12367.88671875
Iteration 2300: Loss = -12367.2900390625
Iteration 2400: Loss = -12366.810546875
Iteration 2500: Loss = -12366.412109375
Iteration 2600: Loss = -12366.072265625
Iteration 2700: Loss = -12365.7822265625
Iteration 2800: Loss = -12365.5244140625
Iteration 2900: Loss = -12365.2998046875
Iteration 3000: Loss = -12365.1005859375
Iteration 3100: Loss = -12364.921875
Iteration 3200: Loss = -12364.7607421875
Iteration 3300: Loss = -12364.6181640625
Iteration 3400: Loss = -12364.4853515625
Iteration 3500: Loss = -12364.3671875
Iteration 3600: Loss = -12364.2587890625
Iteration 3700: Loss = -12364.1572265625
Iteration 3800: Loss = -12364.064453125
Iteration 3900: Loss = -12363.9794921875
Iteration 4000: Loss = -12363.8974609375
Iteration 4100: Loss = -12363.826171875
Iteration 4200: Loss = -12363.7587890625
Iteration 4300: Loss = -12363.6953125
Iteration 4400: Loss = -12363.63671875
Iteration 4500: Loss = -12363.5830078125
Iteration 4600: Loss = -12363.53125
Iteration 4700: Loss = -12363.4853515625
Iteration 4800: Loss = -12363.4404296875
Iteration 4900: Loss = -12363.3984375
Iteration 5000: Loss = -12363.359375
Iteration 5100: Loss = -12363.322265625
Iteration 5200: Loss = -12363.287109375
Iteration 5300: Loss = -12363.25390625
Iteration 5400: Loss = -12363.22265625
Iteration 5500: Loss = -12363.193359375
Iteration 5600: Loss = -12363.1630859375
Iteration 5700: Loss = -12363.1357421875
Iteration 5800: Loss = -12363.109375
Iteration 5900: Loss = -12363.0849609375
Iteration 6000: Loss = -12363.05859375
Iteration 6100: Loss = -12363.03515625
Iteration 6200: Loss = -12363.0087890625
Iteration 6300: Loss = -12362.9833984375
Iteration 6400: Loss = -12362.955078125
Iteration 6500: Loss = -12362.92578125
Iteration 6600: Loss = -12362.892578125
Iteration 6700: Loss = -12362.8544921875
Iteration 6800: Loss = -12362.81640625
Iteration 6900: Loss = -12362.7841796875
Iteration 7000: Loss = -12362.7607421875
Iteration 7100: Loss = -12362.73828125
Iteration 7200: Loss = -12362.7197265625
Iteration 7300: Loss = -12362.701171875
Iteration 7400: Loss = -12362.6796875
Iteration 7500: Loss = -12362.6611328125
Iteration 7600: Loss = -12362.6396484375
Iteration 7700: Loss = -12362.619140625
Iteration 7800: Loss = -12362.59765625
Iteration 7900: Loss = -12362.5703125
Iteration 8000: Loss = -12362.5400390625
Iteration 8100: Loss = -12362.5048828125
Iteration 8200: Loss = -12362.462890625
Iteration 8300: Loss = -12362.408203125
Iteration 8400: Loss = -12362.337890625
Iteration 8500: Loss = -12362.2470703125
Iteration 8600: Loss = -12362.1357421875
Iteration 8700: Loss = -12362.0263671875
Iteration 8800: Loss = -12361.9609375
Iteration 8900: Loss = -12361.9248046875
Iteration 9000: Loss = -12361.908203125
Iteration 9100: Loss = -12361.890625
Iteration 9200: Loss = -12361.8740234375
Iteration 9300: Loss = -12361.8564453125
Iteration 9400: Loss = -12361.83984375
Iteration 9500: Loss = -12361.8212890625
Iteration 9600: Loss = -12361.7998046875
Iteration 9700: Loss = -12361.7763671875
Iteration 9800: Loss = -12361.755859375
Iteration 9900: Loss = -12361.732421875
Iteration 10000: Loss = -12361.70703125
Iteration 10100: Loss = -12361.677734375
Iteration 10200: Loss = -12361.642578125
Iteration 10300: Loss = -12361.599609375
Iteration 10400: Loss = -12361.5546875
Iteration 10500: Loss = -12361.5107421875
Iteration 10600: Loss = -12361.470703125
Iteration 10700: Loss = -12361.439453125
Iteration 10800: Loss = -12361.4169921875
Iteration 10900: Loss = -12361.3974609375
Iteration 11000: Loss = -12361.384765625
Iteration 11100: Loss = -12361.37109375
Iteration 11200: Loss = -12361.3623046875
Iteration 11300: Loss = -12361.35546875
Iteration 11400: Loss = -12361.34765625
Iteration 11500: Loss = -12361.3427734375
Iteration 11600: Loss = -12361.3388671875
Iteration 11700: Loss = -12361.3349609375
Iteration 11800: Loss = -12361.3310546875
Iteration 11900: Loss = -12361.328125
Iteration 12000: Loss = -12361.3232421875
Iteration 12100: Loss = -12361.3232421875
Iteration 12200: Loss = -12361.3193359375
Iteration 12300: Loss = -12361.3203125
1
Iteration 12400: Loss = -12361.3173828125
Iteration 12500: Loss = -12361.31640625
Iteration 12600: Loss = -12361.3134765625
Iteration 12700: Loss = -12361.3115234375
Iteration 12800: Loss = -12361.3115234375
Iteration 12900: Loss = -12361.310546875
Iteration 13000: Loss = -12361.310546875
Iteration 13100: Loss = -12361.3076171875
Iteration 13200: Loss = -12361.3076171875
Iteration 13300: Loss = -12361.306640625
Iteration 13400: Loss = -12361.3056640625
Iteration 13500: Loss = -12361.3046875
Iteration 13600: Loss = -12361.3046875
Iteration 13700: Loss = -12361.302734375
Iteration 13800: Loss = -12361.302734375
Iteration 13900: Loss = -12361.3037109375
1
Iteration 14000: Loss = -12361.3017578125
Iteration 14100: Loss = -12361.30078125
Iteration 14200: Loss = -12361.2998046875
Iteration 14300: Loss = -12361.30078125
1
Iteration 14400: Loss = -12361.298828125
Iteration 14500: Loss = -12361.2978515625
Iteration 14600: Loss = -12361.2998046875
1
Iteration 14700: Loss = -12361.2978515625
Iteration 14800: Loss = -12361.2939453125
Iteration 14900: Loss = -12361.2880859375
Iteration 15000: Loss = -12361.2607421875
Iteration 15100: Loss = -12361.25390625
Iteration 15200: Loss = -12361.25
Iteration 15300: Loss = -12361.2470703125
Iteration 15400: Loss = -12361.2353515625
Iteration 15500: Loss = -12361.2216796875
Iteration 15600: Loss = -12361.2080078125
Iteration 15700: Loss = -12361.201171875
Iteration 15800: Loss = -12361.1982421875
Iteration 15900: Loss = -12361.1953125
Iteration 16000: Loss = -12361.19140625
Iteration 16100: Loss = -12361.1865234375
Iteration 16200: Loss = -12361.1767578125
Iteration 16300: Loss = -12361.1689453125
Iteration 16400: Loss = -12361.162109375
Iteration 16500: Loss = -12361.1552734375
Iteration 16600: Loss = -12361.1455078125
Iteration 16700: Loss = -12361.1357421875
Iteration 16800: Loss = -12361.1337890625
Iteration 16900: Loss = -12361.1328125
Iteration 17000: Loss = -12361.12890625
Iteration 17100: Loss = -12361.126953125
Iteration 17200: Loss = -12361.126953125
Iteration 17300: Loss = -12361.123046875
Iteration 17400: Loss = -12361.12109375
Iteration 17500: Loss = -12361.1181640625
Iteration 17600: Loss = -12361.1181640625
Iteration 17700: Loss = -12361.1181640625
Iteration 17800: Loss = -12361.1181640625
Iteration 17900: Loss = -12361.1171875
Iteration 18000: Loss = -12361.1162109375
Iteration 18100: Loss = -12361.1181640625
1
Iteration 18200: Loss = -12361.1171875
2
Iteration 18300: Loss = -12361.1171875
3
Iteration 18400: Loss = -12361.1181640625
4
Iteration 18500: Loss = -12361.1171875
5
Iteration 18600: Loss = -12361.1181640625
6
Iteration 18700: Loss = -12361.1171875
7
Iteration 18800: Loss = -12361.1171875
8
Iteration 18900: Loss = -12361.1181640625
9
Iteration 19000: Loss = -12361.1181640625
10
Iteration 19100: Loss = -12361.1181640625
11
Iteration 19200: Loss = -12361.1181640625
12
Iteration 19300: Loss = -12361.1181640625
13
Iteration 19400: Loss = -12361.1181640625
14
Iteration 19500: Loss = -12361.1181640625
15
Stopping early at iteration 19500 due to no improvement.
pi: tensor([[9.9999e-01, 5.0929e-06],
        [9.4138e-01, 5.8620e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0045, 0.9955], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1987, 0.2107],
         [0.6997, 0.2069]],

        [[0.0085, 0.1539],
         [0.8714, 0.3551]],

        [[0.2763, 0.2097],
         [0.8533, 0.0181]],

        [[0.3540, 0.1791],
         [0.0104, 0.8694]],

        [[0.0245, 0.1924],
         [0.0168, 0.3891]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.003531417574863382
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17363.576171875
Iteration 100: Loss = -13576.0712890625
Iteration 200: Loss = -12608.2392578125
Iteration 300: Loss = -12437.34765625
Iteration 400: Loss = -12402.5087890625
Iteration 500: Loss = -12384.1220703125
Iteration 600: Loss = -12375.4951171875
Iteration 700: Loss = -12371.42578125
Iteration 800: Loss = -12368.87109375
Iteration 900: Loss = -12367.181640625
Iteration 1000: Loss = -12365.9970703125
Iteration 1100: Loss = -12365.162109375
Iteration 1200: Loss = -12364.5478515625
Iteration 1300: Loss = -12364.080078125
Iteration 1400: Loss = -12363.7138671875
Iteration 1500: Loss = -12363.4228515625
Iteration 1600: Loss = -12363.1845703125
Iteration 1700: Loss = -12362.984375
Iteration 1800: Loss = -12362.81640625
Iteration 1900: Loss = -12362.6728515625
Iteration 2000: Loss = -12362.544921875
Iteration 2100: Loss = -12362.4189453125
Iteration 2200: Loss = -12362.32421875
Iteration 2300: Loss = -12362.2451171875
Iteration 2400: Loss = -12362.173828125
Iteration 2500: Loss = -12362.111328125
Iteration 2600: Loss = -12362.0546875
Iteration 2700: Loss = -12362.0048828125
Iteration 2800: Loss = -12361.9599609375
Iteration 2900: Loss = -12361.919921875
Iteration 3000: Loss = -12361.8828125
Iteration 3100: Loss = -12361.8505859375
Iteration 3200: Loss = -12361.8203125
Iteration 3300: Loss = -12361.79296875
Iteration 3400: Loss = -12361.7666015625
Iteration 3500: Loss = -12361.7431640625
Iteration 3600: Loss = -12361.720703125
Iteration 3700: Loss = -12361.7021484375
Iteration 3800: Loss = -12361.6826171875
Iteration 3900: Loss = -12361.6669921875
Iteration 4000: Loss = -12361.6494140625
Iteration 4100: Loss = -12361.634765625
Iteration 4200: Loss = -12361.6181640625
Iteration 4300: Loss = -12361.603515625
Iteration 4400: Loss = -12361.5908203125
Iteration 4500: Loss = -12361.578125
Iteration 4600: Loss = -12361.56640625
Iteration 4700: Loss = -12361.5537109375
Iteration 4800: Loss = -12361.54296875
Iteration 4900: Loss = -12361.5302734375
Iteration 5000: Loss = -12361.5205078125
Iteration 5100: Loss = -12361.509765625
Iteration 5200: Loss = -12361.5
Iteration 5300: Loss = -12361.48828125
Iteration 5400: Loss = -12361.4765625
Iteration 5500: Loss = -12361.4658203125
Iteration 5600: Loss = -12361.4541015625
Iteration 5700: Loss = -12361.443359375
Iteration 5800: Loss = -12361.43359375
Iteration 5900: Loss = -12361.419921875
Iteration 6000: Loss = -12361.41015625
Iteration 6100: Loss = -12361.3974609375
Iteration 6200: Loss = -12361.3857421875
Iteration 6300: Loss = -12361.37109375
Iteration 6400: Loss = -12361.3603515625
Iteration 6500: Loss = -12361.345703125
Iteration 6600: Loss = -12361.3330078125
Iteration 6700: Loss = -12361.31640625
Iteration 6800: Loss = -12361.3037109375
Iteration 6900: Loss = -12361.2890625
Iteration 7000: Loss = -12361.2734375
Iteration 7100: Loss = -12361.2607421875
Iteration 7200: Loss = -12361.2451171875
Iteration 7300: Loss = -12361.2333984375
Iteration 7400: Loss = -12361.2197265625
Iteration 7500: Loss = -12361.208984375
Iteration 7600: Loss = -12361.19921875
Iteration 7700: Loss = -12361.19140625
Iteration 7800: Loss = -12361.181640625
Iteration 7900: Loss = -12361.1748046875
Iteration 8000: Loss = -12361.1669921875
Iteration 8100: Loss = -12361.162109375
Iteration 8200: Loss = -12361.15625
Iteration 8300: Loss = -12361.15234375
Iteration 8400: Loss = -12361.1484375
Iteration 8500: Loss = -12361.142578125
Iteration 8600: Loss = -12361.140625
Iteration 8700: Loss = -12361.1357421875
Iteration 8800: Loss = -12361.1337890625
Iteration 8900: Loss = -12361.1298828125
Iteration 9000: Loss = -12361.1279296875
Iteration 9100: Loss = -12361.125
Iteration 9200: Loss = -12361.1220703125
Iteration 9300: Loss = -12361.119140625
Iteration 9400: Loss = -12361.1181640625
Iteration 9500: Loss = -12361.1162109375
Iteration 9600: Loss = -12361.111328125
Iteration 9700: Loss = -12361.107421875
Iteration 9800: Loss = -12361.1044921875
Iteration 9900: Loss = -12361.1025390625
Iteration 10000: Loss = -12361.099609375
Iteration 10100: Loss = -12361.09765625
Iteration 10200: Loss = -12361.09765625
Iteration 10300: Loss = -12361.0947265625
Iteration 10400: Loss = -12361.0927734375
Iteration 10500: Loss = -12361.0908203125
Iteration 10600: Loss = -12361.08984375
Iteration 10700: Loss = -12361.0869140625
Iteration 10800: Loss = -12361.083984375
Iteration 10900: Loss = -12361.08203125
Iteration 11000: Loss = -12361.080078125
Iteration 11100: Loss = -12361.0771484375
Iteration 11200: Loss = -12361.0732421875
Iteration 11300: Loss = -12361.068359375
Iteration 11400: Loss = -12361.0634765625
Iteration 11500: Loss = -12361.0576171875
Iteration 11600: Loss = -12361.05078125
Iteration 11700: Loss = -12361.0419921875
Iteration 11800: Loss = -12361.0302734375
Iteration 11900: Loss = -12361.015625
Iteration 12000: Loss = -12360.9951171875
Iteration 12100: Loss = -12360.9765625
Iteration 12200: Loss = -12360.95703125
Iteration 12300: Loss = -12360.943359375
Iteration 12400: Loss = -12360.935546875
Iteration 12500: Loss = -12360.931640625
Iteration 12600: Loss = -12360.931640625
Iteration 12700: Loss = -12360.9287109375
Iteration 12800: Loss = -12360.9287109375
Iteration 12900: Loss = -12360.927734375
Iteration 13000: Loss = -12360.927734375
Iteration 13100: Loss = -12360.927734375
Iteration 13200: Loss = -12360.9267578125
Iteration 13300: Loss = -12360.9267578125
Iteration 13400: Loss = -12360.927734375
1
Iteration 13500: Loss = -12360.927734375
2
Iteration 13600: Loss = -12360.9267578125
Iteration 13700: Loss = -12360.927734375
1
Iteration 13800: Loss = -12360.9267578125
Iteration 13900: Loss = -12360.9267578125
Iteration 14000: Loss = -12360.9267578125
Iteration 14100: Loss = -12360.92578125
Iteration 14200: Loss = -12360.92578125
Iteration 14300: Loss = -12360.9248046875
Iteration 14400: Loss = -12360.9248046875
Iteration 14500: Loss = -12360.9267578125
1
Iteration 14600: Loss = -12360.92578125
2
Iteration 14700: Loss = -12360.9267578125
3
Iteration 14800: Loss = -12360.9267578125
4
Iteration 14900: Loss = -12360.92578125
5
Iteration 15000: Loss = -12360.92578125
6
Iteration 15100: Loss = -12360.9248046875
Iteration 15200: Loss = -12360.92578125
1
Iteration 15300: Loss = -12360.92578125
2
Iteration 15400: Loss = -12360.92578125
3
Iteration 15500: Loss = -12360.923828125
Iteration 15600: Loss = -12360.92578125
1
Iteration 15700: Loss = -12360.9248046875
2
Iteration 15800: Loss = -12360.9248046875
3
Iteration 15900: Loss = -12360.92578125
4
Iteration 16000: Loss = -12360.9248046875
5
Iteration 16100: Loss = -12360.9248046875
6
Iteration 16200: Loss = -12360.9248046875
7
Iteration 16300: Loss = -12360.9248046875
8
Iteration 16400: Loss = -12360.92578125
9
Iteration 16500: Loss = -12360.92578125
10
Iteration 16600: Loss = -12360.9267578125
11
Iteration 16700: Loss = -12360.9248046875
12
Iteration 16800: Loss = -12360.9248046875
13
Iteration 16900: Loss = -12360.9248046875
14
Iteration 17000: Loss = -12360.9248046875
15
Stopping early at iteration 17000 due to no improvement.
pi: tensor([[0.0298, 0.9702],
        [0.4250, 0.5750]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9975, 0.0025], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2069, 0.2113],
         [0.9551, 0.1948]],

        [[0.9428, 0.1577],
         [0.9166, 0.9675]],

        [[0.9923, 0.2002],
         [0.9904, 0.6732]],

        [[0.0313, 0.2040],
         [0.1791, 0.9687]],

        [[0.9382, 0.2007],
         [0.9187, 0.0314]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.002941453417617692
Average Adjusted Rand Index: -0.0007272727272727272
[0.003531417574863382, 0.002941453417617692] [0.0, -0.0007272727272727272] [12361.1181640625, 12360.9248046875]
-------------------------------------
This iteration is 8
True Objective function: Loss = -12040.153987769521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -57338.69921875
Iteration 100: Loss = -41712.1875
Iteration 200: Loss = -27460.677734375
Iteration 300: Loss = -18385.25
Iteration 400: Loss = -14582.4169921875
Iteration 500: Loss = -13321.6787109375
Iteration 600: Loss = -12889.275390625
Iteration 700: Loss = -12710.8857421875
Iteration 800: Loss = -12647.5068359375
Iteration 900: Loss = -12629.37109375
Iteration 1000: Loss = -12596.0830078125
Iteration 1100: Loss = -12588.2197265625
Iteration 1200: Loss = -12579.939453125
Iteration 1300: Loss = -12576.271484375
Iteration 1400: Loss = -12573.4228515625
Iteration 1500: Loss = -12571.2509765625
Iteration 1600: Loss = -12565.9130859375
Iteration 1700: Loss = -12564.2958984375
Iteration 1800: Loss = -12563.037109375
Iteration 1900: Loss = -12561.994140625
Iteration 2000: Loss = -12561.1162109375
Iteration 2100: Loss = -12560.37109375
Iteration 2200: Loss = -12559.7294921875
Iteration 2300: Loss = -12559.16015625
Iteration 2400: Loss = -12558.630859375
Iteration 2500: Loss = -12558.111328125
Iteration 2600: Loss = -12557.58203125
Iteration 2700: Loss = -12557.04296875
Iteration 2800: Loss = -12556.5458984375
Iteration 2900: Loss = -12556.1328125
Iteration 3000: Loss = -12555.7685546875
Iteration 3100: Loss = -12555.29296875
Iteration 3200: Loss = -12552.099609375
Iteration 3300: Loss = -12548.8759765625
Iteration 3400: Loss = -12547.6298828125
Iteration 3500: Loss = -12547.255859375
Iteration 3600: Loss = -12547.001953125
Iteration 3700: Loss = -12546.791015625
Iteration 3800: Loss = -12546.6171875
Iteration 3900: Loss = -12546.45703125
Iteration 4000: Loss = -12546.3154296875
Iteration 4100: Loss = -12546.177734375
Iteration 4200: Loss = -12546.05859375
Iteration 4300: Loss = -12545.9501953125
Iteration 4400: Loss = -12545.845703125
Iteration 4500: Loss = -12545.7431640625
Iteration 4600: Loss = -12545.6435546875
Iteration 4700: Loss = -12545.544921875
Iteration 4800: Loss = -12545.447265625
Iteration 4900: Loss = -12545.3525390625
Iteration 5000: Loss = -12545.2578125
Iteration 5100: Loss = -12545.162109375
Iteration 5200: Loss = -12545.060546875
Iteration 5300: Loss = -12544.9521484375
Iteration 5400: Loss = -12544.830078125
Iteration 5500: Loss = -12544.7080078125
Iteration 5600: Loss = -12544.58984375
Iteration 5700: Loss = -12544.466796875
Iteration 5800: Loss = -12544.3359375
Iteration 5900: Loss = -12544.2060546875
Iteration 6000: Loss = -12544.08984375
Iteration 6100: Loss = -12543.9912109375
Iteration 6200: Loss = -12543.908203125
Iteration 6300: Loss = -12543.841796875
Iteration 6400: Loss = -12543.7841796875
Iteration 6500: Loss = -12543.732421875
Iteration 6600: Loss = -12543.6865234375
Iteration 6700: Loss = -12543.642578125
Iteration 6800: Loss = -12543.6015625
Iteration 6900: Loss = -12543.5625
Iteration 7000: Loss = -12543.5263671875
Iteration 7100: Loss = -12543.494140625
Iteration 7200: Loss = -12543.4609375
Iteration 7300: Loss = -12543.431640625
Iteration 7400: Loss = -12543.40234375
Iteration 7500: Loss = -12543.375
Iteration 7600: Loss = -12543.34765625
Iteration 7700: Loss = -12543.322265625
Iteration 7800: Loss = -12543.294921875
Iteration 7900: Loss = -12543.271484375
Iteration 8000: Loss = -12543.2451171875
Iteration 8100: Loss = -12543.2216796875
Iteration 8200: Loss = -12543.1982421875
Iteration 8300: Loss = -12543.173828125
Iteration 8400: Loss = -12543.1494140625
Iteration 8500: Loss = -12543.1259765625
Iteration 8600: Loss = -12543.0986328125
Iteration 8700: Loss = -12543.072265625
Iteration 8800: Loss = -12543.041015625
Iteration 8900: Loss = -12543.0087890625
Iteration 9000: Loss = -12542.96875
Iteration 9100: Loss = -12542.9189453125
Iteration 9200: Loss = -12542.869140625
Iteration 9300: Loss = -12542.8251953125
Iteration 9400: Loss = -12542.7861328125
Iteration 9500: Loss = -12542.7529296875
Iteration 9600: Loss = -12542.7265625
Iteration 9700: Loss = -12542.703125
Iteration 9800: Loss = -12542.68359375
Iteration 9900: Loss = -12542.669921875
Iteration 10000: Loss = -12542.6572265625
Iteration 10100: Loss = -12542.646484375
Iteration 10200: Loss = -12542.6376953125
Iteration 10300: Loss = -12542.6318359375
Iteration 10400: Loss = -12542.6240234375
Iteration 10500: Loss = -12542.615234375
Iteration 10600: Loss = -12542.6083984375
Iteration 10700: Loss = -12542.599609375
Iteration 10800: Loss = -12542.5869140625
Iteration 10900: Loss = -12542.54296875
Iteration 11000: Loss = -12542.2578125
Iteration 11100: Loss = -12541.2060546875
Iteration 11200: Loss = -12540.8427734375
Iteration 11300: Loss = -12540.7646484375
Iteration 11400: Loss = -12540.73828125
Iteration 11500: Loss = -12540.7255859375
Iteration 11600: Loss = -12540.716796875
Iteration 11700: Loss = -12540.7099609375
Iteration 11800: Loss = -12540.7041015625
Iteration 11900: Loss = -12540.7001953125
Iteration 12000: Loss = -12540.6962890625
Iteration 12100: Loss = -12540.693359375
Iteration 12200: Loss = -12540.69140625
Iteration 12300: Loss = -12540.6884765625
Iteration 12400: Loss = -12540.6865234375
Iteration 12500: Loss = -12540.6826171875
Iteration 12600: Loss = -12540.681640625
Iteration 12700: Loss = -12540.6787109375
Iteration 12800: Loss = -12540.677734375
Iteration 12900: Loss = -12540.6767578125
Iteration 13000: Loss = -12540.67578125
Iteration 13100: Loss = -12540.673828125
Iteration 13200: Loss = -12540.673828125
Iteration 13300: Loss = -12540.6650390625
Iteration 13400: Loss = -12540.470703125
Iteration 13500: Loss = -12540.0654296875
Iteration 13600: Loss = -12540.03125
Iteration 13700: Loss = -12540.0146484375
Iteration 13800: Loss = -12540.01171875
Iteration 13900: Loss = -12540.0078125
Iteration 14000: Loss = -12540.0078125
Iteration 14100: Loss = -12540.005859375
Iteration 14200: Loss = -12540.005859375
Iteration 14300: Loss = -12540.00390625
Iteration 14400: Loss = -12540.0029296875
Iteration 14500: Loss = -12539.9814453125
Iteration 14600: Loss = -12539.9765625
Iteration 14700: Loss = -12539.97265625
Iteration 14800: Loss = -12539.9716796875
Iteration 14900: Loss = -12539.9638671875
Iteration 15000: Loss = -12539.953125
Iteration 15100: Loss = -12539.953125
Iteration 15200: Loss = -12539.9541015625
1
Iteration 15300: Loss = -12539.953125
Iteration 15400: Loss = -12539.951171875
Iteration 15500: Loss = -12539.951171875
Iteration 15600: Loss = -12539.9521484375
1
Iteration 15700: Loss = -12539.9501953125
Iteration 15800: Loss = -12539.9501953125
Iteration 15900: Loss = -12539.9482421875
Iteration 16000: Loss = -12539.9501953125
1
Iteration 16100: Loss = -12539.9501953125
2
Iteration 16200: Loss = -12539.94921875
3
Iteration 16300: Loss = -12539.951171875
4
Iteration 16400: Loss = -12539.9501953125
5
Iteration 16500: Loss = -12539.94921875
6
Iteration 16600: Loss = -12539.9462890625
Iteration 16700: Loss = -12539.943359375
Iteration 16800: Loss = -12539.9423828125
Iteration 16900: Loss = -12539.943359375
1
Iteration 17000: Loss = -12539.943359375
2
Iteration 17100: Loss = -12539.943359375
3
Iteration 17200: Loss = -12539.9423828125
Iteration 17300: Loss = -12539.9423828125
Iteration 17400: Loss = -12539.9423828125
Iteration 17500: Loss = -12539.94140625
Iteration 17600: Loss = -12539.94140625
Iteration 17700: Loss = -12539.943359375
1
Iteration 17800: Loss = -12539.94140625
Iteration 17900: Loss = -12539.94140625
Iteration 18000: Loss = -12539.94140625
Iteration 18100: Loss = -12539.94140625
Iteration 18200: Loss = -12539.9404296875
Iteration 18300: Loss = -12539.94140625
1
Iteration 18400: Loss = -12539.94140625
2
Iteration 18500: Loss = -12539.9404296875
Iteration 18600: Loss = -12539.94140625
1
Iteration 18700: Loss = -12539.94140625
2
Iteration 18800: Loss = -12539.94140625
3
Iteration 18900: Loss = -12539.94140625
4
Iteration 19000: Loss = -12539.94140625
5
Iteration 19100: Loss = -12539.94140625
6
Iteration 19200: Loss = -12539.9404296875
Iteration 19300: Loss = -12539.9404296875
Iteration 19400: Loss = -12539.9404296875
Iteration 19500: Loss = -12539.94140625
1
Iteration 19600: Loss = -12539.939453125
Iteration 19700: Loss = -12539.9404296875
1
Iteration 19800: Loss = -12539.94140625
2
Iteration 19900: Loss = -12539.94140625
3
Iteration 20000: Loss = -12539.9404296875
4
Iteration 20100: Loss = -12539.9423828125
5
Iteration 20200: Loss = -12539.94140625
6
Iteration 20300: Loss = -12539.939453125
Iteration 20400: Loss = -12539.943359375
1
Iteration 20500: Loss = -12539.94140625
2
Iteration 20600: Loss = -12539.9404296875
3
Iteration 20700: Loss = -12539.9404296875
4
Iteration 20800: Loss = -12539.9404296875
5
Iteration 20900: Loss = -12539.9404296875
6
Iteration 21000: Loss = -12539.94140625
7
Iteration 21100: Loss = -12539.9404296875
8
Iteration 21200: Loss = -12539.9404296875
9
Iteration 21300: Loss = -12539.94140625
10
Iteration 21400: Loss = -12539.939453125
Iteration 21500: Loss = -12539.9404296875
1
Iteration 21600: Loss = -12539.94140625
2
Iteration 21700: Loss = -12539.9404296875
3
Iteration 21800: Loss = -12539.94140625
4
Iteration 21900: Loss = -12539.939453125
Iteration 22000: Loss = -12539.939453125
Iteration 22100: Loss = -12539.94140625
1
Iteration 22200: Loss = -12539.939453125
Iteration 22300: Loss = -12539.9384765625
Iteration 22400: Loss = -12539.9404296875
1
Iteration 22500: Loss = -12539.939453125
2
Iteration 22600: Loss = -12539.939453125
3
Iteration 22700: Loss = -12539.9404296875
4
Iteration 22800: Loss = -12539.939453125
5
Iteration 22900: Loss = -12539.94140625
6
Iteration 23000: Loss = -12539.94140625
7
Iteration 23100: Loss = -12539.9423828125
8
Iteration 23200: Loss = -12539.9384765625
Iteration 23300: Loss = -12539.939453125
1
Iteration 23400: Loss = -12539.9404296875
2
Iteration 23500: Loss = -12539.939453125
3
Iteration 23600: Loss = -12539.939453125
4
Iteration 23700: Loss = -12539.9404296875
5
Iteration 23800: Loss = -12539.939453125
6
Iteration 23900: Loss = -12539.939453125
7
Iteration 24000: Loss = -12539.939453125
8
Iteration 24100: Loss = -12539.9404296875
9
Iteration 24200: Loss = -12539.94140625
10
Iteration 24300: Loss = -12539.9404296875
11
Iteration 24400: Loss = -12539.939453125
12
Iteration 24500: Loss = -12539.9404296875
13
Iteration 24600: Loss = -12539.9404296875
14
Iteration 24700: Loss = -12539.939453125
15
Stopping early at iteration 24700 due to no improvement.
pi: tensor([[9.4742e-01, 5.2579e-02],
        [1.0000e+00, 9.5194e-07]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9900, 0.0100], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2040, 0.0920],
         [0.1352, 0.7274]],

        [[0.8898, 0.2238],
         [0.2697, 0.9388]],

        [[0.1368, 0.8041],
         [0.0940, 0.3277]],

        [[0.1317, 0.2819],
         [0.0205, 0.1369]],

        [[0.9591, 0.1480],
         [0.1068, 0.7163]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 38
Adjusted Rand Index: 0.01126387000386094
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.026008250167835027
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 37
Adjusted Rand Index: 0.060467754488253374
Global Adjusted Rand Index: 0.002724354495573527
Average Adjusted Rand Index: 0.009144674864855857
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -43469.0703125
Iteration 100: Loss = -23835.646484375
Iteration 200: Loss = -14785.935546875
Iteration 300: Loss = -13140.072265625
Iteration 400: Loss = -12841.2099609375
Iteration 500: Loss = -12712.087890625
Iteration 600: Loss = -12659.646484375
Iteration 700: Loss = -12630.93359375
Iteration 800: Loss = -12607.1015625
Iteration 900: Loss = -12592.4599609375
Iteration 1000: Loss = -12581.966796875
Iteration 1100: Loss = -12573.974609375
Iteration 1200: Loss = -12568.6357421875
Iteration 1300: Loss = -12564.544921875
Iteration 1400: Loss = -12561.271484375
Iteration 1500: Loss = -12558.6005859375
Iteration 1600: Loss = -12556.384765625
Iteration 1700: Loss = -12554.5224609375
Iteration 1800: Loss = -12552.9375
Iteration 1900: Loss = -12551.5751953125
Iteration 2000: Loss = -12550.396484375
Iteration 2100: Loss = -12549.3701171875
Iteration 2200: Loss = -12548.46875
Iteration 2300: Loss = -12547.6748046875
Iteration 2400: Loss = -12546.970703125
Iteration 2500: Loss = -12546.3427734375
Iteration 2600: Loss = -12545.783203125
Iteration 2700: Loss = -12545.28125
Iteration 2800: Loss = -12544.8310546875
Iteration 2900: Loss = -12544.4248046875
Iteration 3000: Loss = -12544.0654296875
Iteration 3100: Loss = -12543.7470703125
Iteration 3200: Loss = -12543.458984375
Iteration 3300: Loss = -12543.2060546875
Iteration 3400: Loss = -12542.9833984375
Iteration 3500: Loss = -12542.78515625
Iteration 3600: Loss = -12542.6025390625
Iteration 3700: Loss = -12542.4384765625
Iteration 3800: Loss = -12542.2861328125
Iteration 3900: Loss = -12542.1494140625
Iteration 4000: Loss = -12542.0205078125
Iteration 4100: Loss = -12541.90234375
Iteration 4200: Loss = -12541.7919921875
Iteration 4300: Loss = -12541.689453125
Iteration 4400: Loss = -12541.591796875
Iteration 4500: Loss = -12541.5
Iteration 4600: Loss = -12541.4169921875
Iteration 4700: Loss = -12541.337890625
Iteration 4800: Loss = -12541.263671875
Iteration 4900: Loss = -12541.1953125
Iteration 5000: Loss = -12541.1318359375
Iteration 5100: Loss = -12541.0703125
Iteration 5200: Loss = -12541.013671875
Iteration 5300: Loss = -12540.9599609375
Iteration 5400: Loss = -12540.908203125
Iteration 5500: Loss = -12540.859375
Iteration 5600: Loss = -12540.81640625
Iteration 5700: Loss = -12540.7744140625
Iteration 5800: Loss = -12540.7333984375
Iteration 5900: Loss = -12540.6953125
Iteration 6000: Loss = -12540.658203125
Iteration 6100: Loss = -12540.625
Iteration 6200: Loss = -12540.59375
Iteration 6300: Loss = -12540.5634765625
Iteration 6400: Loss = -12540.533203125
Iteration 6500: Loss = -12540.5068359375
Iteration 6600: Loss = -12540.48046875
Iteration 6700: Loss = -12540.4541015625
Iteration 6800: Loss = -12540.4306640625
Iteration 6900: Loss = -12540.41015625
Iteration 7000: Loss = -12540.3876953125
Iteration 7100: Loss = -12540.3681640625
Iteration 7200: Loss = -12540.3505859375
Iteration 7300: Loss = -12540.33203125
Iteration 7400: Loss = -12540.3154296875
Iteration 7500: Loss = -12540.298828125
Iteration 7600: Loss = -12540.283203125
Iteration 7700: Loss = -12540.26953125
Iteration 7800: Loss = -12540.255859375
Iteration 7900: Loss = -12540.2431640625
Iteration 8000: Loss = -12540.2294921875
Iteration 8100: Loss = -12540.2177734375
Iteration 8200: Loss = -12540.205078125
Iteration 8300: Loss = -12540.1943359375
Iteration 8400: Loss = -12540.18359375
Iteration 8500: Loss = -12540.173828125
Iteration 8600: Loss = -12540.1630859375
Iteration 8700: Loss = -12540.15234375
Iteration 8800: Loss = -12540.1435546875
Iteration 8900: Loss = -12540.1318359375
Iteration 9000: Loss = -12540.1220703125
Iteration 9100: Loss = -12540.10546875
Iteration 9200: Loss = -12540.0810546875
Iteration 9300: Loss = -12540.015625
Iteration 9400: Loss = -12539.873046875
Iteration 9500: Loss = -12539.7978515625
Iteration 9600: Loss = -12539.7470703125
Iteration 9700: Loss = -12539.7099609375
Iteration 9800: Loss = -12539.6767578125
Iteration 9900: Loss = -12539.6513671875
Iteration 10000: Loss = -12539.6201171875
Iteration 10100: Loss = -12539.583984375
Iteration 10200: Loss = -12539.5361328125
Iteration 10300: Loss = -12539.4873046875
Iteration 10400: Loss = -12539.4443359375
Iteration 10500: Loss = -12539.3955078125
Iteration 10600: Loss = -12539.2978515625
Iteration 10700: Loss = -12539.083984375
Iteration 10800: Loss = -12538.9462890625
Iteration 10900: Loss = -12538.8623046875
Iteration 11000: Loss = -12538.8046875
Iteration 11100: Loss = -12538.7578125
Iteration 11200: Loss = -12538.71875
Iteration 11300: Loss = -12538.68359375
Iteration 11400: Loss = -12538.6494140625
Iteration 11500: Loss = -12538.6142578125
Iteration 11600: Loss = -12538.578125
Iteration 11700: Loss = -12538.5419921875
Iteration 11800: Loss = -12538.5
Iteration 11900: Loss = -12538.455078125
Iteration 12000: Loss = -12538.40234375
Iteration 12100: Loss = -12538.3427734375
Iteration 12200: Loss = -12538.267578125
Iteration 12300: Loss = -12538.1787109375
Iteration 12400: Loss = -12538.0732421875
Iteration 12500: Loss = -12537.947265625
Iteration 12600: Loss = -12537.810546875
Iteration 12700: Loss = -12537.673828125
Iteration 12800: Loss = -12537.552734375
Iteration 12900: Loss = -12537.4560546875
Iteration 13000: Loss = -12537.3837890625
Iteration 13100: Loss = -12537.3330078125
Iteration 13200: Loss = -12537.2919921875
Iteration 13300: Loss = -12537.2626953125
Iteration 13400: Loss = -12537.2412109375
Iteration 13500: Loss = -12537.2236328125
Iteration 13600: Loss = -12537.2099609375
Iteration 13700: Loss = -12537.1982421875
Iteration 13800: Loss = -12537.1884765625
Iteration 13900: Loss = -12537.18359375
Iteration 14000: Loss = -12537.177734375
Iteration 14100: Loss = -12537.171875
Iteration 14200: Loss = -12537.16796875
Iteration 14300: Loss = -12537.1640625
Iteration 14400: Loss = -12537.1611328125
Iteration 14500: Loss = -12537.158203125
Iteration 14600: Loss = -12537.1572265625
Iteration 14700: Loss = -12537.1552734375
Iteration 14800: Loss = -12537.1552734375
Iteration 14900: Loss = -12537.1533203125
Iteration 15000: Loss = -12537.15234375
Iteration 15100: Loss = -12537.150390625
Iteration 15200: Loss = -12537.15234375
1
Iteration 15300: Loss = -12537.150390625
Iteration 15400: Loss = -12537.1494140625
Iteration 15500: Loss = -12537.1484375
Iteration 15600: Loss = -12537.1474609375
Iteration 15700: Loss = -12537.1474609375
Iteration 15800: Loss = -12537.146484375
Iteration 15900: Loss = -12537.1455078125
Iteration 16000: Loss = -12537.1328125
Iteration 16100: Loss = -12535.826171875
Iteration 16200: Loss = -12535.7314453125
Iteration 16300: Loss = -12535.7236328125
Iteration 16400: Loss = -12535.72265625
Iteration 16500: Loss = -12535.7216796875
Iteration 16600: Loss = -12535.720703125
Iteration 16700: Loss = -12535.720703125
Iteration 16800: Loss = -12535.720703125
Iteration 16900: Loss = -12535.71875
Iteration 17000: Loss = -12535.7197265625
1
Iteration 17100: Loss = -12535.71875
Iteration 17200: Loss = -12535.71875
Iteration 17300: Loss = -12535.71875
Iteration 17400: Loss = -12535.71875
Iteration 17500: Loss = -12535.716796875
Iteration 17600: Loss = -12535.7177734375
1
Iteration 17700: Loss = -12535.7177734375
2
Iteration 17800: Loss = -12535.720703125
3
Iteration 17900: Loss = -12535.716796875
Iteration 18000: Loss = -12535.7177734375
1
Iteration 18100: Loss = -12535.716796875
Iteration 18200: Loss = -12535.716796875
Iteration 18300: Loss = -12535.7158203125
Iteration 18400: Loss = -12535.7177734375
1
Iteration 18500: Loss = -12535.7177734375
2
Iteration 18600: Loss = -12535.7158203125
Iteration 18700: Loss = -12535.7158203125
Iteration 18800: Loss = -12535.7158203125
Iteration 18900: Loss = -12535.716796875
1
Iteration 19000: Loss = -12535.71484375
Iteration 19100: Loss = -12535.716796875
1
Iteration 19200: Loss = -12535.716796875
2
Iteration 19300: Loss = -12535.716796875
3
Iteration 19400: Loss = -12535.716796875
4
Iteration 19500: Loss = -12535.7158203125
5
Iteration 19600: Loss = -12535.716796875
6
Iteration 19700: Loss = -12535.7158203125
7
Iteration 19800: Loss = -12535.7158203125
8
Iteration 19900: Loss = -12535.7158203125
9
Iteration 20000: Loss = -12535.7158203125
10
Iteration 20100: Loss = -12535.7158203125
11
Iteration 20200: Loss = -12535.716796875
12
Iteration 20300: Loss = -12535.7158203125
13
Iteration 20400: Loss = -12535.716796875
14
Iteration 20500: Loss = -12535.71484375
Iteration 20600: Loss = -12535.716796875
1
Iteration 20700: Loss = -12535.7158203125
2
Iteration 20800: Loss = -12535.716796875
3
Iteration 20900: Loss = -12535.716796875
4
Iteration 21000: Loss = -12535.71484375
Iteration 21100: Loss = -12535.7158203125
1
Iteration 21200: Loss = -12535.7158203125
2
Iteration 21300: Loss = -12535.71484375
Iteration 21400: Loss = -12535.7138671875
Iteration 21500: Loss = -12535.7158203125
1
Iteration 21600: Loss = -12535.7158203125
2
Iteration 21700: Loss = -12535.7158203125
3
Iteration 21800: Loss = -12535.71484375
4
Iteration 21900: Loss = -12535.71484375
5
Iteration 22000: Loss = -12535.716796875
6
Iteration 22100: Loss = -12535.7138671875
Iteration 22200: Loss = -12535.7158203125
1
Iteration 22300: Loss = -12535.7158203125
2
Iteration 22400: Loss = -12535.7158203125
3
Iteration 22500: Loss = -12535.7158203125
4
Iteration 22600: Loss = -12535.71484375
5
Iteration 22700: Loss = -12535.71484375
6
Iteration 22800: Loss = -12535.7158203125
7
Iteration 22900: Loss = -12535.7158203125
8
Iteration 23000: Loss = -12535.7158203125
9
Iteration 23100: Loss = -12535.71484375
10
Iteration 23200: Loss = -12535.7158203125
11
Iteration 23300: Loss = -12535.7158203125
12
Iteration 23400: Loss = -12535.7158203125
13
Iteration 23500: Loss = -12535.7138671875
Iteration 23600: Loss = -12535.7158203125
1
Iteration 23700: Loss = -12535.7158203125
2
Iteration 23800: Loss = -12535.71484375
3
Iteration 23900: Loss = -12535.7158203125
4
Iteration 24000: Loss = -12535.7158203125
5
Iteration 24100: Loss = -12535.71484375
6
Iteration 24200: Loss = -12535.7158203125
7
Iteration 24300: Loss = -12535.71484375
8
Iteration 24400: Loss = -12535.7177734375
9
Iteration 24500: Loss = -12535.7158203125
10
Iteration 24600: Loss = -12535.71484375
11
Iteration 24700: Loss = -12535.7158203125
12
Iteration 24800: Loss = -12535.7158203125
13
Iteration 24900: Loss = -12535.7158203125
14
Iteration 25000: Loss = -12535.7158203125
15
Stopping early at iteration 25000 due to no improvement.
pi: tensor([[2.7059e-05, 9.9997e-01],
        [1.9579e-02, 9.8042e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0363, 0.9637], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2247, 0.1101],
         [0.8876, 0.2049]],

        [[0.6038, 0.2305],
         [0.3222, 0.2142]],

        [[0.3301, 0.1071],
         [0.7713, 0.9231]],

        [[0.1043, 0.2813],
         [0.9098, 0.9638]],

        [[0.2016, 0.3225],
         [0.9931, 0.0228]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 63
Adjusted Rand Index: 0.023297732239618434
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: 0.0014933016447381412
Average Adjusted Rand Index: 0.003474189471872407
[0.002724354495573527, 0.0014933016447381412] [0.009144674864855857, 0.003474189471872407] [12539.939453125, 12535.7158203125]
-------------------------------------
This iteration is 9
True Objective function: Loss = -11945.917011463333
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35027.25390625
Iteration 100: Loss = -21351.6484375
Iteration 200: Loss = -14948.94921875
Iteration 300: Loss = -13373.162109375
Iteration 400: Loss = -12891.884765625
Iteration 500: Loss = -12710.771484375
Iteration 600: Loss = -12626.12890625
Iteration 700: Loss = -12573.6279296875
Iteration 800: Loss = -12540.2138671875
Iteration 900: Loss = -12521.9189453125
Iteration 1000: Loss = -12510.123046875
Iteration 1100: Loss = -12494.845703125
Iteration 1200: Loss = -12486.640625
Iteration 1300: Loss = -12482.2705078125
Iteration 1400: Loss = -12479.07421875
Iteration 1500: Loss = -12476.580078125
Iteration 1600: Loss = -12474.5634765625
Iteration 1700: Loss = -12472.8544921875
Iteration 1800: Loss = -12471.3232421875
Iteration 1900: Loss = -12469.8984375
Iteration 2000: Loss = -12467.8408203125
Iteration 2100: Loss = -12466.2763671875
Iteration 2200: Loss = -12465.36328125
Iteration 2300: Loss = -12464.6240234375
Iteration 2400: Loss = -12463.98828125
Iteration 2500: Loss = -12463.4384765625
Iteration 2600: Loss = -12462.9521484375
Iteration 2700: Loss = -12462.517578125
Iteration 2800: Loss = -12462.130859375
Iteration 2900: Loss = -12461.7841796875
Iteration 3000: Loss = -12461.4697265625
Iteration 3100: Loss = -12461.1865234375
Iteration 3200: Loss = -12460.9287109375
Iteration 3300: Loss = -12460.693359375
Iteration 3400: Loss = -12460.4794921875
Iteration 3500: Loss = -12460.2822265625
Iteration 3600: Loss = -12460.1025390625
Iteration 3700: Loss = -12459.9365234375
Iteration 3800: Loss = -12459.783203125
Iteration 3900: Loss = -12459.6416015625
Iteration 4000: Loss = -12459.5126953125
Iteration 4100: Loss = -12459.39453125
Iteration 4200: Loss = -12459.2822265625
Iteration 4300: Loss = -12459.1787109375
Iteration 4400: Loss = -12459.0830078125
Iteration 4500: Loss = -12458.994140625
Iteration 4600: Loss = -12458.912109375
Iteration 4700: Loss = -12458.8330078125
Iteration 4800: Loss = -12458.7626953125
Iteration 4900: Loss = -12458.6943359375
Iteration 5000: Loss = -12458.6318359375
Iteration 5100: Loss = -12458.57421875
Iteration 5200: Loss = -12458.51953125
Iteration 5300: Loss = -12458.46875
Iteration 5400: Loss = -12458.419921875
Iteration 5500: Loss = -12458.373046875
Iteration 5600: Loss = -12458.3330078125
Iteration 5700: Loss = -12458.2919921875
Iteration 5800: Loss = -12458.255859375
Iteration 5900: Loss = -12458.2216796875
Iteration 6000: Loss = -12458.1875
Iteration 6100: Loss = -12458.12890625
Iteration 6200: Loss = -12456.3193359375
Iteration 6300: Loss = -12456.1650390625
Iteration 6400: Loss = -12456.068359375
Iteration 6500: Loss = -12455.998046875
Iteration 6600: Loss = -12455.9404296875
Iteration 6700: Loss = -12455.890625
Iteration 6800: Loss = -12455.845703125
Iteration 6900: Loss = -12455.806640625
Iteration 7000: Loss = -12455.7705078125
Iteration 7100: Loss = -12455.73828125
Iteration 7200: Loss = -12455.7099609375
Iteration 7300: Loss = -12455.681640625
Iteration 7400: Loss = -12455.6533203125
Iteration 7500: Loss = -12455.62890625
Iteration 7600: Loss = -12455.6044921875
Iteration 7700: Loss = -12455.58203125
Iteration 7800: Loss = -12455.560546875
Iteration 7900: Loss = -12455.5390625
Iteration 8000: Loss = -12455.51953125
Iteration 8100: Loss = -12455.501953125
Iteration 8200: Loss = -12455.486328125
Iteration 8300: Loss = -12455.4716796875
Iteration 8400: Loss = -12455.4580078125
Iteration 8500: Loss = -12455.4462890625
Iteration 8600: Loss = -12455.4365234375
Iteration 8700: Loss = -12455.4267578125
Iteration 8800: Loss = -12455.4189453125
Iteration 8900: Loss = -12455.412109375
Iteration 9000: Loss = -12455.4052734375
Iteration 9100: Loss = -12455.3974609375
Iteration 9200: Loss = -12455.3916015625
Iteration 9300: Loss = -12455.3857421875
Iteration 9400: Loss = -12455.3798828125
Iteration 9500: Loss = -12455.375
Iteration 9600: Loss = -12455.3671875
Iteration 9700: Loss = -12455.3603515625
Iteration 9800: Loss = -12455.3544921875
Iteration 9900: Loss = -12455.34765625
Iteration 10000: Loss = -12455.3388671875
Iteration 10100: Loss = -12455.3271484375
Iteration 10200: Loss = -12455.3134765625
Iteration 10300: Loss = -12455.2998046875
Iteration 10400: Loss = -12455.28515625
Iteration 10500: Loss = -12455.2705078125
Iteration 10600: Loss = -12455.130859375
Iteration 10700: Loss = -12451.291015625
Iteration 10800: Loss = -12451.119140625
Iteration 10900: Loss = -12451.0234375
Iteration 11000: Loss = -12450.953125
Iteration 11100: Loss = -12450.6533203125
Iteration 11200: Loss = -12450.6103515625
Iteration 11300: Loss = -12450.58203125
Iteration 11400: Loss = -12450.5625
Iteration 11500: Loss = -12450.544921875
Iteration 11600: Loss = -12450.5302734375
Iteration 11700: Loss = -12450.5185546875
Iteration 11800: Loss = -12450.5078125
Iteration 11900: Loss = -12450.5009765625
Iteration 12000: Loss = -12450.4912109375
Iteration 12100: Loss = -12450.484375
Iteration 12200: Loss = -12450.4775390625
Iteration 12300: Loss = -12450.4716796875
Iteration 12400: Loss = -12450.4677734375
Iteration 12500: Loss = -12450.4619140625
Iteration 12600: Loss = -12450.4580078125
Iteration 12700: Loss = -12450.455078125
Iteration 12800: Loss = -12450.451171875
Iteration 12900: Loss = -12450.447265625
Iteration 13000: Loss = -12450.4443359375
Iteration 13100: Loss = -12450.44140625
Iteration 13200: Loss = -12450.439453125
Iteration 13300: Loss = -12450.435546875
Iteration 13400: Loss = -12450.435546875
Iteration 13500: Loss = -12450.4326171875
Iteration 13600: Loss = -12450.43359375
1
Iteration 13700: Loss = -12450.431640625
Iteration 13800: Loss = -12450.4287109375
Iteration 13900: Loss = -12450.427734375
Iteration 14000: Loss = -12450.423828125
Iteration 14100: Loss = -12450.4228515625
Iteration 14200: Loss = -12450.4228515625
Iteration 14300: Loss = -12450.4208984375
Iteration 14400: Loss = -12450.421875
1
Iteration 14500: Loss = -12450.419921875
Iteration 14600: Loss = -12450.4189453125
Iteration 14700: Loss = -12450.41796875
Iteration 14800: Loss = -12450.4169921875
Iteration 14900: Loss = -12450.41015625
Iteration 15000: Loss = -12450.40625
Iteration 15100: Loss = -12450.404296875
Iteration 15200: Loss = -12450.4033203125
Iteration 15300: Loss = -12450.40234375
Iteration 15400: Loss = -12450.40234375
Iteration 15500: Loss = -12450.40234375
Iteration 15600: Loss = -12450.4013671875
Iteration 15700: Loss = -12450.4013671875
Iteration 15800: Loss = -12450.4013671875
Iteration 15900: Loss = -12450.400390625
Iteration 16000: Loss = -12450.400390625
Iteration 16100: Loss = -12450.3994140625
Iteration 16200: Loss = -12450.3984375
Iteration 16300: Loss = -12450.3984375
Iteration 16400: Loss = -12450.3974609375
Iteration 16500: Loss = -12450.3974609375
Iteration 16600: Loss = -12450.3955078125
Iteration 16700: Loss = -12450.3974609375
1
Iteration 16800: Loss = -12450.396484375
2
Iteration 16900: Loss = -12450.3955078125
Iteration 17000: Loss = -12450.3955078125
Iteration 17100: Loss = -12450.396484375
1
Iteration 17200: Loss = -12450.3955078125
Iteration 17300: Loss = -12450.3955078125
Iteration 17400: Loss = -12450.39453125
Iteration 17500: Loss = -12450.3935546875
Iteration 17600: Loss = -12450.3935546875
Iteration 17700: Loss = -12450.392578125
Iteration 17800: Loss = -12450.39453125
1
Iteration 17900: Loss = -12450.392578125
Iteration 18000: Loss = -12450.3916015625
Iteration 18100: Loss = -12450.3916015625
Iteration 18200: Loss = -12450.392578125
1
Iteration 18300: Loss = -12450.3935546875
2
Iteration 18400: Loss = -12450.3935546875
3
Iteration 18500: Loss = -12450.392578125
4
Iteration 18600: Loss = -12450.392578125
5
Iteration 18700: Loss = -12450.3916015625
Iteration 18800: Loss = -12450.392578125
1
Iteration 18900: Loss = -12450.3935546875
2
Iteration 19000: Loss = -12450.390625
Iteration 19100: Loss = -12450.3916015625
1
Iteration 19200: Loss = -12450.3916015625
2
Iteration 19300: Loss = -12450.392578125
3
Iteration 19400: Loss = -12450.390625
Iteration 19500: Loss = -12450.3896484375
Iteration 19600: Loss = -12450.390625
1
Iteration 19700: Loss = -12450.390625
2
Iteration 19800: Loss = -12450.3896484375
Iteration 19900: Loss = -12450.392578125
1
Iteration 20000: Loss = -12450.3896484375
Iteration 20100: Loss = -12450.390625
1
Iteration 20200: Loss = -12450.390625
2
Iteration 20300: Loss = -12450.390625
3
Iteration 20400: Loss = -12450.3896484375
Iteration 20500: Loss = -12450.390625
1
Iteration 20600: Loss = -12450.3896484375
Iteration 20700: Loss = -12450.3896484375
Iteration 20800: Loss = -12450.388671875
Iteration 20900: Loss = -12450.388671875
Iteration 21000: Loss = -12450.3896484375
1
Iteration 21100: Loss = -12450.3896484375
2
Iteration 21200: Loss = -12450.392578125
3
Iteration 21300: Loss = -12450.388671875
Iteration 21400: Loss = -12450.390625
1
Iteration 21500: Loss = -12450.388671875
Iteration 21600: Loss = -12450.390625
1
Iteration 21700: Loss = -12450.3896484375
2
Iteration 21800: Loss = -12450.390625
3
Iteration 21900: Loss = -12450.390625
4
Iteration 22000: Loss = -12450.3896484375
5
Iteration 22100: Loss = -12450.3896484375
6
Iteration 22200: Loss = -12450.390625
7
Iteration 22300: Loss = -12450.3896484375
8
Iteration 22400: Loss = -12450.3916015625
9
Iteration 22500: Loss = -12450.3896484375
10
Iteration 22600: Loss = -12450.390625
11
Iteration 22700: Loss = -12450.390625
12
Iteration 22800: Loss = -12450.3896484375
13
Iteration 22900: Loss = -12450.3896484375
14
Iteration 23000: Loss = -12450.3896484375
15
Stopping early at iteration 23000 due to no improvement.
pi: tensor([[1.0000e+00, 2.5179e-06],
        [1.8191e-01, 8.1809e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9753, 0.0247], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2002, 0.2841],
         [0.3348, 0.7224]],

        [[0.9023, 0.1727],
         [0.9682, 0.4221]],

        [[0.4424, 0.3098],
         [0.6174, 0.4935]],

        [[0.0440, 0.2313],
         [0.4511, 0.0739]],

        [[0.4580, 0.2230],
         [0.9243, 0.2169]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: -0.010213452814961178
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: -0.00018519451173730755
Average Adjusted Rand Index: -0.0032617771531430724
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23531.482421875
Iteration 100: Loss = -16613.3046875
Iteration 200: Loss = -13148.3115234375
Iteration 300: Loss = -12741.1220703125
Iteration 400: Loss = -12648.529296875
Iteration 500: Loss = -12590.2275390625
Iteration 600: Loss = -12562.4921875
Iteration 700: Loss = -12547.1923828125
Iteration 800: Loss = -12536.728515625
Iteration 900: Loss = -12524.025390625
Iteration 1000: Loss = -12517.2177734375
Iteration 1100: Loss = -12511.220703125
Iteration 1200: Loss = -12503.9892578125
Iteration 1300: Loss = -12498.28515625
Iteration 1400: Loss = -12492.091796875
Iteration 1500: Loss = -12487.29296875
Iteration 1600: Loss = -12481.00390625
Iteration 1700: Loss = -12477.466796875
Iteration 1800: Loss = -12474.4736328125
Iteration 1900: Loss = -12471.03125
Iteration 2000: Loss = -12467.330078125
Iteration 2100: Loss = -12465.8466796875
Iteration 2200: Loss = -12463.6650390625
Iteration 2300: Loss = -12461.3046875
Iteration 2400: Loss = -12460.544921875
Iteration 2500: Loss = -12459.9765625
Iteration 2600: Loss = -12459.509765625
Iteration 2700: Loss = -12459.1123046875
Iteration 2800: Loss = -12458.771484375
Iteration 2900: Loss = -12458.478515625
Iteration 3000: Loss = -12458.2236328125
Iteration 3100: Loss = -12457.99609375
Iteration 3200: Loss = -12457.794921875
Iteration 3300: Loss = -12457.6123046875
Iteration 3400: Loss = -12457.4482421875
Iteration 3500: Loss = -12457.2978515625
Iteration 3600: Loss = -12457.1630859375
Iteration 3700: Loss = -12457.03515625
Iteration 3800: Loss = -12456.9208984375
Iteration 3900: Loss = -12456.81640625
Iteration 4000: Loss = -12456.7197265625
Iteration 4100: Loss = -12456.62890625
Iteration 4200: Loss = -12456.5458984375
Iteration 4300: Loss = -12456.4658203125
Iteration 4400: Loss = -12456.3955078125
Iteration 4500: Loss = -12456.330078125
Iteration 4600: Loss = -12456.267578125
Iteration 4700: Loss = -12456.2080078125
Iteration 4800: Loss = -12456.154296875
Iteration 4900: Loss = -12456.103515625
Iteration 5000: Loss = -12456.0556640625
Iteration 5100: Loss = -12456.0107421875
Iteration 5200: Loss = -12455.970703125
Iteration 5300: Loss = -12455.9326171875
Iteration 5400: Loss = -12455.896484375
Iteration 5500: Loss = -12455.8603515625
Iteration 5600: Loss = -12455.8271484375
Iteration 5700: Loss = -12455.794921875
Iteration 5800: Loss = -12455.767578125
Iteration 5900: Loss = -12455.73828125
Iteration 6000: Loss = -12455.7099609375
Iteration 6100: Loss = -12455.6865234375
Iteration 6200: Loss = -12455.662109375
Iteration 6300: Loss = -12455.638671875
Iteration 6400: Loss = -12455.6181640625
Iteration 6500: Loss = -12455.59765625
Iteration 6600: Loss = -12455.580078125
Iteration 6700: Loss = -12455.5615234375
Iteration 6800: Loss = -12455.544921875
Iteration 6900: Loss = -12455.529296875
Iteration 7000: Loss = -12455.5146484375
Iteration 7100: Loss = -12455.4990234375
Iteration 7200: Loss = -12455.486328125
Iteration 7300: Loss = -12455.4736328125
Iteration 7400: Loss = -12455.4619140625
Iteration 7500: Loss = -12455.44921875
Iteration 7600: Loss = -12455.439453125
Iteration 7700: Loss = -12455.427734375
Iteration 7800: Loss = -12455.4169921875
Iteration 7900: Loss = -12455.408203125
Iteration 8000: Loss = -12455.400390625
Iteration 8100: Loss = -12455.390625
Iteration 8200: Loss = -12455.384765625
Iteration 8300: Loss = -12455.375
Iteration 8400: Loss = -12455.3662109375
Iteration 8500: Loss = -12455.359375
Iteration 8600: Loss = -12455.3515625
Iteration 8700: Loss = -12455.34375
Iteration 8800: Loss = -12455.3369140625
Iteration 8900: Loss = -12455.3291015625
Iteration 9000: Loss = -12455.322265625
Iteration 9100: Loss = -12455.3154296875
Iteration 9200: Loss = -12455.306640625
Iteration 9300: Loss = -12455.2978515625
Iteration 9400: Loss = -12455.2890625
Iteration 9500: Loss = -12455.2783203125
Iteration 9600: Loss = -12455.265625
Iteration 9700: Loss = -12455.251953125
Iteration 9800: Loss = -12455.2333984375
Iteration 9900: Loss = -12455.1943359375
Iteration 10000: Loss = -12455.1572265625
Iteration 10100: Loss = -12455.1064453125
Iteration 10200: Loss = -12455.0458984375
Iteration 10300: Loss = -12454.9892578125
Iteration 10400: Loss = -12454.9638671875
Iteration 10500: Loss = -12454.955078125
Iteration 10600: Loss = -12454.947265625
Iteration 10700: Loss = -12454.94140625
Iteration 10800: Loss = -12454.9365234375
Iteration 10900: Loss = -12454.9287109375
Iteration 11000: Loss = -12454.919921875
Iteration 11100: Loss = -12454.912109375
Iteration 11200: Loss = -12454.9072265625
Iteration 11300: Loss = -12454.904296875
Iteration 11400: Loss = -12454.9013671875
Iteration 11500: Loss = -12454.8984375
Iteration 11600: Loss = -12454.8994140625
1
Iteration 11700: Loss = -12454.8955078125
Iteration 11800: Loss = -12454.89453125
Iteration 11900: Loss = -12454.8935546875
Iteration 12000: Loss = -12454.8955078125
1
Iteration 12100: Loss = -12454.8916015625
Iteration 12200: Loss = -12454.890625
Iteration 12300: Loss = -12454.890625
Iteration 12400: Loss = -12454.888671875
Iteration 12500: Loss = -12454.8876953125
Iteration 12600: Loss = -12454.88671875
Iteration 12700: Loss = -12454.88671875
Iteration 12800: Loss = -12454.884765625
Iteration 12900: Loss = -12454.8837890625
Iteration 13000: Loss = -12454.8583984375
Iteration 13100: Loss = -12454.853515625
Iteration 13200: Loss = -12454.8525390625
Iteration 13300: Loss = -12454.8515625
Iteration 13400: Loss = -12454.8515625
Iteration 13500: Loss = -12454.8505859375
Iteration 13600: Loss = -12454.8486328125
Iteration 13700: Loss = -12454.849609375
1
Iteration 13800: Loss = -12454.8505859375
2
Iteration 13900: Loss = -12454.849609375
3
Iteration 14000: Loss = -12454.8466796875
Iteration 14100: Loss = -12454.8154296875
Iteration 14200: Loss = -12454.8125
Iteration 14300: Loss = -12454.8115234375
Iteration 14400: Loss = -12454.8115234375
Iteration 14500: Loss = -12454.8115234375
Iteration 14600: Loss = -12454.8095703125
Iteration 14700: Loss = -12454.8095703125
Iteration 14800: Loss = -12454.8095703125
Iteration 14900: Loss = -12454.8076171875
Iteration 15000: Loss = -12454.8095703125
1
Iteration 15100: Loss = -12454.8076171875
Iteration 15200: Loss = -12454.80859375
1
Iteration 15300: Loss = -12454.80859375
2
Iteration 15400: Loss = -12454.80859375
3
Iteration 15500: Loss = -12454.8076171875
Iteration 15600: Loss = -12454.8076171875
Iteration 15700: Loss = -12454.8076171875
Iteration 15800: Loss = -12454.806640625
Iteration 15900: Loss = -12454.806640625
Iteration 16000: Loss = -12454.806640625
Iteration 16100: Loss = -12454.806640625
Iteration 16200: Loss = -12454.80859375
1
Iteration 16300: Loss = -12454.806640625
Iteration 16400: Loss = -12454.806640625
Iteration 16500: Loss = -12454.8056640625
Iteration 16600: Loss = -12454.8056640625
Iteration 16700: Loss = -12454.8056640625
Iteration 16800: Loss = -12454.8076171875
1
Iteration 16900: Loss = -12454.8056640625
Iteration 17000: Loss = -12454.8056640625
Iteration 17100: Loss = -12454.8046875
Iteration 17200: Loss = -12454.8056640625
1
Iteration 17300: Loss = -12454.806640625
2
Iteration 17400: Loss = -12454.8056640625
3
Iteration 17500: Loss = -12454.8056640625
4
Iteration 17600: Loss = -12454.8046875
Iteration 17700: Loss = -12454.8056640625
1
Iteration 17800: Loss = -12454.8046875
Iteration 17900: Loss = -12454.8037109375
Iteration 18000: Loss = -12454.8076171875
1
Iteration 18100: Loss = -12454.8037109375
Iteration 18200: Loss = -12454.8046875
1
Iteration 18300: Loss = -12454.8046875
2
Iteration 18400: Loss = -12454.8046875
3
Iteration 18500: Loss = -12454.8056640625
4
Iteration 18600: Loss = -12454.8046875
5
Iteration 18700: Loss = -12454.806640625
6
Iteration 18800: Loss = -12454.8056640625
7
Iteration 18900: Loss = -12454.8037109375
Iteration 19000: Loss = -12454.8046875
1
Iteration 19100: Loss = -12454.8037109375
Iteration 19200: Loss = -12454.802734375
Iteration 19300: Loss = -12454.8037109375
1
Iteration 19400: Loss = -12453.1533203125
Iteration 19500: Loss = -12452.3701171875
Iteration 19600: Loss = -12451.77734375
Iteration 19700: Loss = -12450.46875
Iteration 19800: Loss = -12450.423828125
Iteration 19900: Loss = -12450.419921875
Iteration 20000: Loss = -12450.419921875
Iteration 20100: Loss = -12450.4189453125
Iteration 20200: Loss = -12450.419921875
1
Iteration 20300: Loss = -12450.4189453125
Iteration 20400: Loss = -12450.4189453125
Iteration 20500: Loss = -12450.419921875
1
Iteration 20600: Loss = -12450.4189453125
Iteration 20700: Loss = -12450.419921875
1
Iteration 20800: Loss = -12450.419921875
2
Iteration 20900: Loss = -12450.4169921875
Iteration 21000: Loss = -12450.4169921875
Iteration 21100: Loss = -12450.4189453125
1
Iteration 21200: Loss = -12450.4130859375
Iteration 21300: Loss = -12450.41015625
Iteration 21400: Loss = -12450.4140625
1
Iteration 21500: Loss = -12450.4091796875
Iteration 21600: Loss = -12450.4091796875
Iteration 21700: Loss = -12450.4091796875
Iteration 21800: Loss = -12450.408203125
Iteration 21900: Loss = -12450.408203125
Iteration 22000: Loss = -12450.4072265625
Iteration 22100: Loss = -12450.4072265625
Iteration 22200: Loss = -12450.40625
Iteration 22300: Loss = -12450.40625
Iteration 22400: Loss = -12450.408203125
1
Iteration 22500: Loss = -12450.40625
Iteration 22600: Loss = -12450.40625
Iteration 22700: Loss = -12450.4072265625
1
Iteration 22800: Loss = -12450.404296875
Iteration 22900: Loss = -12450.4072265625
1
Iteration 23000: Loss = -12450.40625
2
Iteration 23100: Loss = -12450.4052734375
3
Iteration 23200: Loss = -12450.404296875
Iteration 23300: Loss = -12450.40625
1
Iteration 23400: Loss = -12450.404296875
Iteration 23500: Loss = -12450.40625
1
Iteration 23600: Loss = -12450.3955078125
Iteration 23700: Loss = -12450.39453125
Iteration 23800: Loss = -12450.39453125
Iteration 23900: Loss = -12450.39453125
Iteration 24000: Loss = -12450.392578125
Iteration 24100: Loss = -12450.39453125
1
Iteration 24200: Loss = -12450.3935546875
2
Iteration 24300: Loss = -12450.3935546875
3
Iteration 24400: Loss = -12450.392578125
Iteration 24500: Loss = -12450.392578125
Iteration 24600: Loss = -12450.392578125
Iteration 24700: Loss = -12450.3916015625
Iteration 24800: Loss = -12450.392578125
1
Iteration 24900: Loss = -12450.392578125
2
Iteration 25000: Loss = -12450.390625
Iteration 25100: Loss = -12450.3916015625
1
Iteration 25200: Loss = -12450.392578125
2
Iteration 25300: Loss = -12450.3896484375
Iteration 25400: Loss = -12450.390625
1
Iteration 25500: Loss = -12450.392578125
2
Iteration 25600: Loss = -12450.3896484375
Iteration 25700: Loss = -12450.3896484375
Iteration 25800: Loss = -12450.390625
1
Iteration 25900: Loss = -12450.390625
2
Iteration 26000: Loss = -12450.3955078125
3
Iteration 26100: Loss = -12450.390625
4
Iteration 26200: Loss = -12450.390625
5
Iteration 26300: Loss = -12450.392578125
6
Iteration 26400: Loss = -12450.3896484375
Iteration 26500: Loss = -12450.390625
1
Iteration 26600: Loss = -12450.390625
2
Iteration 26700: Loss = -12450.390625
3
Iteration 26800: Loss = -12450.390625
4
Iteration 26900: Loss = -12450.390625
5
Iteration 27000: Loss = -12450.388671875
Iteration 27100: Loss = -12450.390625
1
Iteration 27200: Loss = -12450.388671875
Iteration 27300: Loss = -12450.3896484375
1
Iteration 27400: Loss = -12450.392578125
2
Iteration 27500: Loss = -12450.3896484375
3
Iteration 27600: Loss = -12450.3896484375
4
Iteration 27700: Loss = -12450.3916015625
5
Iteration 27800: Loss = -12450.390625
6
Iteration 27900: Loss = -12450.3916015625
7
Iteration 28000: Loss = -12450.3896484375
8
Iteration 28100: Loss = -12450.390625
9
Iteration 28200: Loss = -12450.390625
10
Iteration 28300: Loss = -12450.3896484375
11
Iteration 28400: Loss = -12450.3896484375
12
Iteration 28500: Loss = -12450.3896484375
13
Iteration 28600: Loss = -12450.390625
14
Iteration 28700: Loss = -12450.3896484375
15
Stopping early at iteration 28700 due to no improvement.
pi: tensor([[1.0000e+00, 8.8707e-07],
        [1.8230e-01, 8.1770e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9753, 0.0247], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2002, 0.2842],
         [0.3320, 0.7249]],

        [[0.9897, 0.1726],
         [0.8726, 0.0805]],

        [[0.1441, 0.3098],
         [0.0073, 0.3139]],

        [[0.9669, 0.2313],
         [0.8988, 0.2263]],

        [[0.8339, 0.2230],
         [0.9847, 0.0172]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: -0.010213452814961178
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: -0.00018519451173730755
Average Adjusted Rand Index: -0.0032617771531430724
[-0.00018519451173730755, -0.00018519451173730755] [-0.0032617771531430724, -0.0032617771531430724] [12450.3896484375, 12450.3896484375]
-------------------------------------
This iteration is 10
True Objective function: Loss = -11897.441056096859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33078.83203125
Iteration 100: Loss = -21085.515625
Iteration 200: Loss = -14491.1533203125
Iteration 300: Loss = -13213.794921875
Iteration 400: Loss = -12851.2421875
Iteration 500: Loss = -12701.2646484375
Iteration 600: Loss = -12634.2119140625
Iteration 700: Loss = -12581.201171875
Iteration 800: Loss = -12553.53515625
Iteration 900: Loss = -12530.03515625
Iteration 1000: Loss = -12503.30078125
Iteration 1100: Loss = -12493.1357421875
Iteration 1200: Loss = -12485.2548828125
Iteration 1300: Loss = -12477.9814453125
Iteration 1400: Loss = -12470.5693359375
Iteration 1500: Loss = -12462.771484375
Iteration 1600: Loss = -12456.9619140625
Iteration 1700: Loss = -12450.35546875
Iteration 1800: Loss = -12442.912109375
Iteration 1900: Loss = -12436.9296875
Iteration 2000: Loss = -12433.38671875
Iteration 2100: Loss = -12431.177734375
Iteration 2200: Loss = -12429.2509765625
Iteration 2300: Loss = -12427.8671875
Iteration 2400: Loss = -12426.8369140625
Iteration 2500: Loss = -12426.02734375
Iteration 2600: Loss = -12425.3525390625
Iteration 2700: Loss = -12424.7421875
Iteration 2800: Loss = -12422.2822265625
Iteration 2900: Loss = -12420.7724609375
Iteration 3000: Loss = -12420.2529296875
Iteration 3100: Loss = -12419.853515625
Iteration 3200: Loss = -12419.486328125
Iteration 3300: Loss = -12416.0771484375
Iteration 3400: Loss = -12415.44921875
Iteration 3500: Loss = -12415.09765625
Iteration 3600: Loss = -12414.8193359375
Iteration 3700: Loss = -12414.580078125
Iteration 3800: Loss = -12414.3369140625
Iteration 3900: Loss = -12411.9169921875
Iteration 4000: Loss = -12410.7802734375
Iteration 4100: Loss = -12410.419921875
Iteration 4200: Loss = -12410.1767578125
Iteration 4300: Loss = -12409.9873046875
Iteration 4400: Loss = -12409.826171875
Iteration 4500: Loss = -12409.6865234375
Iteration 4600: Loss = -12409.5498046875
Iteration 4700: Loss = -12409.435546875
Iteration 4800: Loss = -12409.33203125
Iteration 4900: Loss = -12409.23828125
Iteration 5000: Loss = -12409.1484375
Iteration 5100: Loss = -12409.064453125
Iteration 5200: Loss = -12408.98828125
Iteration 5300: Loss = -12408.916015625
Iteration 5400: Loss = -12408.8486328125
Iteration 5500: Loss = -12408.78515625
Iteration 5600: Loss = -12408.7265625
Iteration 5700: Loss = -12408.6669921875
Iteration 5800: Loss = -12408.615234375
Iteration 5900: Loss = -12408.5625
Iteration 6000: Loss = -12408.5107421875
Iteration 6100: Loss = -12408.46484375
Iteration 6200: Loss = -12408.421875
Iteration 6300: Loss = -12408.3525390625
Iteration 6400: Loss = -12404.3330078125
Iteration 6500: Loss = -12403.935546875
Iteration 6600: Loss = -12403.7763671875
Iteration 6700: Loss = -12403.66796875
Iteration 6800: Loss = -12403.583984375
Iteration 6900: Loss = -12403.513671875
Iteration 7000: Loss = -12403.453125
Iteration 7100: Loss = -12403.4033203125
Iteration 7200: Loss = -12403.3564453125
Iteration 7300: Loss = -12403.3173828125
Iteration 7400: Loss = -12403.2822265625
Iteration 7500: Loss = -12403.2490234375
Iteration 7600: Loss = -12403.2197265625
Iteration 7700: Loss = -12403.1923828125
Iteration 7800: Loss = -12403.1689453125
Iteration 7900: Loss = -12403.1484375
Iteration 8000: Loss = -12403.1279296875
Iteration 8100: Loss = -12403.109375
Iteration 8200: Loss = -12403.0927734375
Iteration 8300: Loss = -12403.0751953125
Iteration 8400: Loss = -12403.0634765625
Iteration 8500: Loss = -12403.048828125
Iteration 8600: Loss = -12403.037109375
Iteration 8700: Loss = -12403.025390625
Iteration 8800: Loss = -12403.01171875
Iteration 8900: Loss = -12403.001953125
Iteration 9000: Loss = -12402.994140625
Iteration 9100: Loss = -12402.9853515625
Iteration 9200: Loss = -12402.9755859375
Iteration 9300: Loss = -12402.96875
Iteration 9400: Loss = -12402.9609375
Iteration 9500: Loss = -12402.955078125
Iteration 9600: Loss = -12402.9482421875
Iteration 9700: Loss = -12402.94140625
Iteration 9800: Loss = -12402.9365234375
Iteration 9900: Loss = -12402.9296875
Iteration 10000: Loss = -12402.9267578125
Iteration 10100: Loss = -12402.921875
Iteration 10200: Loss = -12402.916015625
Iteration 10300: Loss = -12402.912109375
Iteration 10400: Loss = -12402.908203125
Iteration 10500: Loss = -12402.9033203125
Iteration 10600: Loss = -12402.900390625
Iteration 10700: Loss = -12402.8955078125
Iteration 10800: Loss = -12402.8916015625
Iteration 10900: Loss = -12402.8896484375
Iteration 11000: Loss = -12402.8857421875
Iteration 11100: Loss = -12402.8837890625
Iteration 11200: Loss = -12402.880859375
Iteration 11300: Loss = -12402.87890625
Iteration 11400: Loss = -12402.875
Iteration 11500: Loss = -12402.873046875
Iteration 11600: Loss = -12402.8701171875
Iteration 11700: Loss = -12402.8681640625
Iteration 11800: Loss = -12402.8671875
Iteration 11900: Loss = -12402.86328125
Iteration 12000: Loss = -12402.8603515625
Iteration 12100: Loss = -12402.8583984375
Iteration 12200: Loss = -12402.857421875
Iteration 12300: Loss = -12402.85546875
Iteration 12400: Loss = -12402.85546875
Iteration 12500: Loss = -12402.8515625
Iteration 12600: Loss = -12402.8505859375
Iteration 12700: Loss = -12402.849609375
Iteration 12800: Loss = -12402.845703125
Iteration 12900: Loss = -12402.8447265625
Iteration 13000: Loss = -12402.8447265625
Iteration 13100: Loss = -12402.841796875
Iteration 13200: Loss = -12402.8408203125
Iteration 13300: Loss = -12402.8388671875
Iteration 13400: Loss = -12402.837890625
Iteration 13500: Loss = -12402.8349609375
Iteration 13600: Loss = -12402.8349609375
Iteration 13700: Loss = -12402.8349609375
Iteration 13800: Loss = -12402.833984375
Iteration 13900: Loss = -12402.83203125
Iteration 14000: Loss = -12402.830078125
Iteration 14100: Loss = -12402.830078125
Iteration 14200: Loss = -12402.830078125
Iteration 14300: Loss = -12402.8291015625
Iteration 14400: Loss = -12402.8271484375
Iteration 14500: Loss = -12402.828125
1
Iteration 14600: Loss = -12402.826171875
Iteration 14700: Loss = -12402.8271484375
1
Iteration 14800: Loss = -12402.826171875
Iteration 14900: Loss = -12402.82421875
Iteration 15000: Loss = -12402.8232421875
Iteration 15100: Loss = -12402.822265625
Iteration 15200: Loss = -12402.8212890625
Iteration 15300: Loss = -12402.8193359375
Iteration 15400: Loss = -12402.8193359375
Iteration 15500: Loss = -12402.8193359375
Iteration 15600: Loss = -12402.8193359375
Iteration 15700: Loss = -12402.8193359375
Iteration 15800: Loss = -12402.8212890625
1
Iteration 15900: Loss = -12402.8193359375
Iteration 16000: Loss = -12402.818359375
Iteration 16100: Loss = -12402.818359375
Iteration 16200: Loss = -12402.8173828125
Iteration 16300: Loss = -12402.8173828125
Iteration 16400: Loss = -12402.81640625
Iteration 16500: Loss = -12402.81640625
Iteration 16600: Loss = -12402.81640625
Iteration 16700: Loss = -12402.814453125
Iteration 16800: Loss = -12402.8154296875
1
Iteration 16900: Loss = -12402.8193359375
2
Iteration 17000: Loss = -12402.8154296875
3
Iteration 17100: Loss = -12402.8154296875
4
Iteration 17200: Loss = -12402.814453125
Iteration 17300: Loss = -12402.814453125
Iteration 17400: Loss = -12402.8154296875
1
Iteration 17500: Loss = -12402.8134765625
Iteration 17600: Loss = -12402.814453125
1
Iteration 17700: Loss = -12402.8154296875
2
Iteration 17800: Loss = -12402.814453125
3
Iteration 17900: Loss = -12402.814453125
4
Iteration 18000: Loss = -12402.8125
Iteration 18100: Loss = -12402.8154296875
1
Iteration 18200: Loss = -12402.8154296875
2
Iteration 18300: Loss = -12402.8125
Iteration 18400: Loss = -12402.8134765625
1
Iteration 18500: Loss = -12402.8134765625
2
Iteration 18600: Loss = -12402.8134765625
3
Iteration 18700: Loss = -12402.8134765625
4
Iteration 18800: Loss = -12402.814453125
5
Iteration 18900: Loss = -12402.8134765625
6
Iteration 19000: Loss = -12402.8134765625
7
Iteration 19100: Loss = -12402.8134765625
8
Iteration 19200: Loss = -12402.8134765625
9
Iteration 19300: Loss = -12402.8115234375
Iteration 19400: Loss = -12402.8134765625
1
Iteration 19500: Loss = -12402.814453125
2
Iteration 19600: Loss = -12402.814453125
3
Iteration 19700: Loss = -12402.814453125
4
Iteration 19800: Loss = -12402.8134765625
5
Iteration 19900: Loss = -12402.8134765625
6
Iteration 20000: Loss = -12402.8134765625
7
Iteration 20100: Loss = -12402.8115234375
Iteration 20200: Loss = -12402.8125
1
Iteration 20300: Loss = -12402.8115234375
Iteration 20400: Loss = -12402.8125
1
Iteration 20500: Loss = -12402.8115234375
Iteration 20600: Loss = -12402.8115234375
Iteration 20700: Loss = -12402.8115234375
Iteration 20800: Loss = -12402.8134765625
1
Iteration 20900: Loss = -12402.8125
2
Iteration 21000: Loss = -12402.8125
3
Iteration 21100: Loss = -12402.8125
4
Iteration 21200: Loss = -12402.8125
5
Iteration 21300: Loss = -12402.8125
6
Iteration 21400: Loss = -12402.8125
7
Iteration 21500: Loss = -12402.8115234375
Iteration 21600: Loss = -12402.8125
1
Iteration 21700: Loss = -12402.8125
2
Iteration 21800: Loss = -12402.8134765625
3
Iteration 21900: Loss = -12402.8125
4
Iteration 22000: Loss = -12402.8125
5
Iteration 22100: Loss = -12402.8125
6
Iteration 22200: Loss = -12402.8125
7
Iteration 22300: Loss = -12402.8125
8
Iteration 22400: Loss = -12402.8125
9
Iteration 22500: Loss = -12402.8125
10
Iteration 22600: Loss = -12402.8125
11
Iteration 22700: Loss = -12402.8115234375
Iteration 22800: Loss = -12402.8125
1
Iteration 22900: Loss = -12402.8115234375
Iteration 23000: Loss = -12402.8125
1
Iteration 23100: Loss = -12402.8115234375
Iteration 23200: Loss = -12402.8115234375
Iteration 23300: Loss = -12402.8125
1
Iteration 23400: Loss = -12402.8125
2
Iteration 23500: Loss = -12402.810546875
Iteration 23600: Loss = -12402.8125
1
Iteration 23700: Loss = -12402.8134765625
2
Iteration 23800: Loss = -12402.8125
3
Iteration 23900: Loss = -12402.810546875
Iteration 24000: Loss = -12402.810546875
Iteration 24100: Loss = -12402.8125
1
Iteration 24200: Loss = -12402.8125
2
Iteration 24300: Loss = -12402.8125
3
Iteration 24400: Loss = -12402.8125
4
Iteration 24500: Loss = -12402.8115234375
5
Iteration 24600: Loss = -12402.8125
6
Iteration 24700: Loss = -12402.8125
7
Iteration 24800: Loss = -12402.8134765625
8
Iteration 24900: Loss = -12402.8125
9
Iteration 25000: Loss = -12402.8125
10
Iteration 25100: Loss = -12402.8134765625
11
Iteration 25200: Loss = -12402.8115234375
12
Iteration 25300: Loss = -12402.806640625
Iteration 25400: Loss = -12402.8076171875
1
Iteration 25500: Loss = -12402.806640625
Iteration 25600: Loss = -12402.8056640625
Iteration 25700: Loss = -12402.8046875
Iteration 25800: Loss = -12402.8037109375
Iteration 25900: Loss = -12402.794921875
Iteration 26000: Loss = -12402.7841796875
Iteration 26100: Loss = -12402.78125
Iteration 26200: Loss = -12402.7568359375
Iteration 26300: Loss = -12402.7421875
Iteration 26400: Loss = -12402.740234375
Iteration 26500: Loss = -12402.7431640625
1
Iteration 26600: Loss = -12402.7265625
Iteration 26700: Loss = -12402.7275390625
1
Iteration 26800: Loss = -12402.7216796875
Iteration 26900: Loss = -12402.724609375
1
Iteration 27000: Loss = -12402.7216796875
Iteration 27100: Loss = -12402.6630859375
Iteration 27200: Loss = -12402.66015625
Iteration 27300: Loss = -12402.662109375
1
Iteration 27400: Loss = -12402.6640625
2
Iteration 27500: Loss = -12402.6630859375
3
Iteration 27600: Loss = -12402.6630859375
4
Iteration 27700: Loss = -12402.662109375
5
Iteration 27800: Loss = -12402.658203125
Iteration 27900: Loss = -12402.6591796875
1
Iteration 28000: Loss = -12402.6591796875
2
Iteration 28100: Loss = -12402.6591796875
3
Iteration 28200: Loss = -12402.6591796875
4
Iteration 28300: Loss = -12402.66015625
5
Iteration 28400: Loss = -12402.6591796875
6
Iteration 28500: Loss = -12402.658203125
Iteration 28600: Loss = -12402.658203125
Iteration 28700: Loss = -12402.6591796875
1
Iteration 28800: Loss = -12402.66015625
2
Iteration 28900: Loss = -12402.66015625
3
Iteration 29000: Loss = -12402.66015625
4
Iteration 29100: Loss = -12402.66015625
5
Iteration 29200: Loss = -12402.6591796875
6
Iteration 29300: Loss = -12402.658203125
Iteration 29400: Loss = -12402.6591796875
1
Iteration 29500: Loss = -12402.66015625
2
Iteration 29600: Loss = -12402.6591796875
3
Iteration 29700: Loss = -12402.6591796875
4
Iteration 29800: Loss = -12402.6591796875
5
Iteration 29900: Loss = -12402.66015625
6
pi: tensor([[9.9434e-01, 5.6571e-03],
        [1.0000e+00, 1.2267e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 8.3013e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2001, 0.2095],
         [0.3621, 0.3488]],

        [[0.0725, 0.2058],
         [0.9900, 0.3410]],

        [[0.9667, 0.1306],
         [0.8022, 0.1728]],

        [[0.0924, 0.3287],
         [0.0361, 0.0612]],

        [[0.1721, 0.2653],
         [0.9604, 0.2109]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00039543502922895045
Average Adjusted Rand Index: -0.0003077958928485548
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22883.37109375
Iteration 100: Loss = -16817.41796875
Iteration 200: Loss = -13645.4052734375
Iteration 300: Loss = -12873.4384765625
Iteration 400: Loss = -12682.3173828125
Iteration 500: Loss = -12617.337890625
Iteration 600: Loss = -12579.548828125
Iteration 700: Loss = -12554.7802734375
Iteration 800: Loss = -12535.3671875
Iteration 900: Loss = -12517.8984375
Iteration 1000: Loss = -12502.8740234375
Iteration 1100: Loss = -12490.3388671875
Iteration 1200: Loss = -12476.3466796875
Iteration 1300: Loss = -12464.9990234375
Iteration 1400: Loss = -12456.306640625
Iteration 1500: Loss = -12446.9814453125
Iteration 1600: Loss = -12438.998046875
Iteration 1700: Loss = -12427.4033203125
Iteration 1800: Loss = -12417.6298828125
Iteration 1900: Loss = -12409.5458984375
Iteration 2000: Loss = -12402.267578125
Iteration 2100: Loss = -12397.7861328125
Iteration 2200: Loss = -12395.2802734375
Iteration 2300: Loss = -12393.556640625
Iteration 2400: Loss = -12391.6904296875
Iteration 2500: Loss = -12388.4111328125
Iteration 2600: Loss = -12383.2060546875
Iteration 2700: Loss = -12380.3857421875
Iteration 2800: Loss = -12374.73046875
Iteration 2900: Loss = -12373.8515625
Iteration 3000: Loss = -12373.19921875
Iteration 3100: Loss = -12372.5458984375
Iteration 3200: Loss = -12372.041015625
Iteration 3300: Loss = -12371.2412109375
Iteration 3400: Loss = -12366.7939453125
Iteration 3500: Loss = -12301.28515625
Iteration 3600: Loss = -12250.7265625
Iteration 3700: Loss = -12221.7646484375
Iteration 3800: Loss = -12213.5546875
Iteration 3900: Loss = -12212.6181640625
Iteration 4000: Loss = -12212.1005859375
Iteration 4100: Loss = -12211.7353515625
Iteration 4200: Loss = -12211.3427734375
Iteration 4300: Loss = -12201.806640625
Iteration 4400: Loss = -12195.7607421875
Iteration 4500: Loss = -12186.759765625
Iteration 4600: Loss = -12186.310546875
Iteration 4700: Loss = -12186.1044921875
Iteration 4800: Loss = -12185.947265625
Iteration 4900: Loss = -12179.5107421875
Iteration 5000: Loss = -12179.220703125
Iteration 5100: Loss = -12179.091796875
Iteration 5200: Loss = -12179.0263671875
Iteration 5300: Loss = -12178.9736328125
Iteration 5400: Loss = -12178.9287109375
Iteration 5500: Loss = -12178.8916015625
Iteration 5600: Loss = -12178.857421875
Iteration 5700: Loss = -12178.8251953125
Iteration 5800: Loss = -12178.7919921875
Iteration 5900: Loss = -12174.1552734375
Iteration 6000: Loss = -12164.2958984375
Iteration 6100: Loss = -12154.513671875
Iteration 6200: Loss = -12154.2900390625
Iteration 6300: Loss = -12154.2177734375
Iteration 6400: Loss = -12154.173828125
Iteration 6500: Loss = -12154.1435546875
Iteration 6600: Loss = -12154.1171875
Iteration 6700: Loss = -12154.0947265625
Iteration 6800: Loss = -12154.076171875
Iteration 6900: Loss = -12154.0625
Iteration 7000: Loss = -12154.046875
Iteration 7100: Loss = -12154.03515625
Iteration 7200: Loss = -12154.0224609375
Iteration 7300: Loss = -12154.013671875
Iteration 7400: Loss = -12154.001953125
Iteration 7500: Loss = -12153.994140625
Iteration 7600: Loss = -12153.986328125
Iteration 7700: Loss = -12153.978515625
Iteration 7800: Loss = -12153.9716796875
Iteration 7900: Loss = -12153.9658203125
Iteration 8000: Loss = -12153.958984375
Iteration 8100: Loss = -12153.953125
Iteration 8200: Loss = -12153.9482421875
Iteration 8300: Loss = -12153.9443359375
Iteration 8400: Loss = -12153.9384765625
Iteration 8500: Loss = -12153.9345703125
Iteration 8600: Loss = -12153.9296875
Iteration 8700: Loss = -12153.9267578125
Iteration 8800: Loss = -12153.9228515625
Iteration 8900: Loss = -12153.91796875
Iteration 9000: Loss = -12153.9072265625
Iteration 9100: Loss = -12146.6533203125
Iteration 9200: Loss = -12146.564453125
Iteration 9300: Loss = -12146.54296875
Iteration 9400: Loss = -12146.5302734375
Iteration 9500: Loss = -12146.5205078125
Iteration 9600: Loss = -12146.515625
Iteration 9700: Loss = -12146.51171875
Iteration 9800: Loss = -12146.5068359375
Iteration 9900: Loss = -12146.50390625
Iteration 10000: Loss = -12146.5009765625
Iteration 10100: Loss = -12146.4990234375
Iteration 10200: Loss = -12146.4970703125
Iteration 10300: Loss = -12146.494140625
Iteration 10400: Loss = -12146.4912109375
Iteration 10500: Loss = -12146.4892578125
Iteration 10600: Loss = -12146.486328125
Iteration 10700: Loss = -12146.4853515625
Iteration 10800: Loss = -12146.4833984375
Iteration 10900: Loss = -12146.482421875
Iteration 11000: Loss = -12146.4814453125
Iteration 11100: Loss = -12146.48046875
Iteration 11200: Loss = -12146.4765625
Iteration 11300: Loss = -12146.478515625
1
Iteration 11400: Loss = -12146.4775390625
2
Iteration 11500: Loss = -12146.4775390625
3
Iteration 11600: Loss = -12146.474609375
Iteration 11700: Loss = -12146.474609375
Iteration 11800: Loss = -12146.4736328125
Iteration 11900: Loss = -12146.4736328125
Iteration 12000: Loss = -12146.47265625
Iteration 12100: Loss = -12146.470703125
Iteration 12200: Loss = -12146.4697265625
Iteration 12300: Loss = -12146.470703125
1
Iteration 12400: Loss = -12146.4697265625
Iteration 12500: Loss = -12146.46875
Iteration 12600: Loss = -12146.4697265625
1
Iteration 12700: Loss = -12146.46875
Iteration 12800: Loss = -12146.4677734375
Iteration 12900: Loss = -12146.46875
1
Iteration 13000: Loss = -12146.4677734375
Iteration 13100: Loss = -12146.466796875
Iteration 13200: Loss = -12146.466796875
Iteration 13300: Loss = -12146.4658203125
Iteration 13400: Loss = -12146.4658203125
Iteration 13500: Loss = -12146.466796875
1
Iteration 13600: Loss = -12146.46484375
Iteration 13700: Loss = -12146.4658203125
1
Iteration 13800: Loss = -12146.46484375
Iteration 13900: Loss = -12146.46484375
Iteration 14000: Loss = -12146.462890625
Iteration 14100: Loss = -12146.4638671875
1
Iteration 14200: Loss = -12146.466796875
2
Iteration 14300: Loss = -12146.4638671875
3
Iteration 14400: Loss = -12146.462890625
Iteration 14500: Loss = -12146.4638671875
1
Iteration 14600: Loss = -12146.4638671875
2
Iteration 14700: Loss = -12146.462890625
Iteration 14800: Loss = -12146.462890625
Iteration 14900: Loss = -12146.462890625
Iteration 15000: Loss = -12146.4638671875
1
Iteration 15100: Loss = -12146.4619140625
Iteration 15200: Loss = -12146.462890625
1
Iteration 15300: Loss = -12146.4619140625
Iteration 15400: Loss = -12146.4609375
Iteration 15500: Loss = -12146.1923828125
Iteration 15600: Loss = -12145.240234375
Iteration 15700: Loss = -12145.1572265625
Iteration 15800: Loss = -12142.45703125
Iteration 15900: Loss = -12138.6328125
Iteration 16000: Loss = -12137.8388671875
Iteration 16100: Loss = -12137.748046875
Iteration 16200: Loss = -12135.9931640625
Iteration 16300: Loss = -12134.638671875
Iteration 16400: Loss = -12134.46875
Iteration 16500: Loss = -12134.169921875
Iteration 16600: Loss = -12133.9765625
Iteration 16700: Loss = -12133.974609375
Iteration 16800: Loss = -12133.9677734375
Iteration 16900: Loss = -12132.60546875
Iteration 17000: Loss = -12131.154296875
Iteration 17100: Loss = -12130.681640625
Iteration 17200: Loss = -12127.91796875
Iteration 17300: Loss = -12121.0400390625
Iteration 17400: Loss = -12116.037109375
Iteration 17500: Loss = -12097.5712890625
Iteration 17600: Loss = -12078.4228515625
Iteration 17700: Loss = -12044.4013671875
Iteration 17800: Loss = -12013.3291015625
Iteration 17900: Loss = -11988.0517578125
Iteration 18000: Loss = -11960.0595703125
Iteration 18100: Loss = -11947.8154296875
Iteration 18200: Loss = -11945.8896484375
Iteration 18300: Loss = -11945.71484375
Iteration 18400: Loss = -11945.6240234375
Iteration 18500: Loss = -11945.560546875
Iteration 18600: Loss = -11945.517578125
Iteration 18700: Loss = -11945.484375
Iteration 18800: Loss = -11945.4580078125
Iteration 18900: Loss = -11945.4365234375
Iteration 19000: Loss = -11945.419921875
Iteration 19100: Loss = -11945.4052734375
Iteration 19200: Loss = -11945.392578125
Iteration 19300: Loss = -11945.3828125
Iteration 19400: Loss = -11945.3740234375
Iteration 19500: Loss = -11945.3671875
Iteration 19600: Loss = -11945.359375
Iteration 19700: Loss = -11945.353515625
Iteration 19800: Loss = -11945.3486328125
Iteration 19900: Loss = -11945.34375
Iteration 20000: Loss = -11945.3388671875
Iteration 20100: Loss = -11945.3349609375
Iteration 20200: Loss = -11945.3310546875
Iteration 20300: Loss = -11945.328125
Iteration 20400: Loss = -11945.326171875
Iteration 20500: Loss = -11945.32421875
Iteration 20600: Loss = -11945.3212890625
Iteration 20700: Loss = -11945.318359375
Iteration 20800: Loss = -11945.3173828125
Iteration 20900: Loss = -11945.314453125
Iteration 21000: Loss = -11945.3154296875
1
Iteration 21100: Loss = -11945.3115234375
Iteration 21200: Loss = -11945.3095703125
Iteration 21300: Loss = -11945.3095703125
Iteration 21400: Loss = -11945.3076171875
Iteration 21500: Loss = -11945.3056640625
Iteration 21600: Loss = -11945.1982421875
Iteration 21700: Loss = -11945.1982421875
Iteration 21800: Loss = -11945.1962890625
Iteration 21900: Loss = -11945.1953125
Iteration 22000: Loss = -11945.1943359375
Iteration 22100: Loss = -11945.193359375
Iteration 22200: Loss = -11945.1943359375
1
Iteration 22300: Loss = -11945.193359375
Iteration 22400: Loss = -11945.19140625
Iteration 22500: Loss = -11945.1923828125
1
Iteration 22600: Loss = -11945.1923828125
2
Iteration 22700: Loss = -11945.1904296875
Iteration 22800: Loss = -11945.1904296875
Iteration 22900: Loss = -11945.189453125
Iteration 23000: Loss = -11945.1884765625
Iteration 23100: Loss = -11945.1875
Iteration 23200: Loss = -11945.189453125
1
Iteration 23300: Loss = -11945.1875
Iteration 23400: Loss = -11945.1875
Iteration 23500: Loss = -11945.1865234375
Iteration 23600: Loss = -11945.1875
1
Iteration 23700: Loss = -11945.1884765625
2
Iteration 23800: Loss = -11945.1865234375
Iteration 23900: Loss = -11945.185546875
Iteration 24000: Loss = -11945.1865234375
1
Iteration 24100: Loss = -11945.185546875
Iteration 24200: Loss = -11945.1845703125
Iteration 24300: Loss = -11936.04296875
Iteration 24400: Loss = -11935.8935546875
Iteration 24500: Loss = -11935.8662109375
Iteration 24600: Loss = -11935.853515625
Iteration 24700: Loss = -11935.84765625
Iteration 24800: Loss = -11935.8427734375
Iteration 24900: Loss = -11935.83984375
Iteration 25000: Loss = -11935.8349609375
Iteration 25100: Loss = -11935.833984375
Iteration 25200: Loss = -11935.8330078125
Iteration 25300: Loss = -11935.83203125
Iteration 25400: Loss = -11935.8310546875
Iteration 25500: Loss = -11935.830078125
Iteration 25600: Loss = -11935.8291015625
Iteration 25700: Loss = -11935.8291015625
Iteration 25800: Loss = -11935.8115234375
Iteration 25900: Loss = -11935.8056640625
Iteration 26000: Loss = -11935.8056640625
Iteration 26100: Loss = -11935.802734375
Iteration 26200: Loss = -11935.6552734375
Iteration 26300: Loss = -11935.65234375
Iteration 26400: Loss = -11920.283203125
Iteration 26500: Loss = -11919.9111328125
Iteration 26600: Loss = -11919.853515625
Iteration 26700: Loss = -11919.8212890625
Iteration 26800: Loss = -11919.80859375
Iteration 26900: Loss = -11919.798828125
Iteration 27000: Loss = -11919.7919921875
Iteration 27100: Loss = -11919.787109375
Iteration 27200: Loss = -11919.783203125
Iteration 27300: Loss = -11919.7802734375
Iteration 27400: Loss = -11919.77734375
Iteration 27500: Loss = -11919.775390625
Iteration 27600: Loss = -11919.7734375
Iteration 27700: Loss = -11919.7734375
Iteration 27800: Loss = -11919.7705078125
Iteration 27900: Loss = -11919.76953125
Iteration 28000: Loss = -11919.76953125
Iteration 28100: Loss = -11919.767578125
Iteration 28200: Loss = -11919.7666015625
Iteration 28300: Loss = -11919.767578125
1
Iteration 28400: Loss = -11919.7666015625
Iteration 28500: Loss = -11919.765625
Iteration 28600: Loss = -11919.7666015625
1
Iteration 28700: Loss = -11919.763671875
Iteration 28800: Loss = -11919.7646484375
1
Iteration 28900: Loss = -11919.7646484375
2
Iteration 29000: Loss = -11919.763671875
Iteration 29100: Loss = -11919.763671875
Iteration 29200: Loss = -11919.7626953125
Iteration 29300: Loss = -11919.7626953125
Iteration 29400: Loss = -11919.7626953125
Iteration 29500: Loss = -11919.7607421875
Iteration 29600: Loss = -11919.7626953125
1
Iteration 29700: Loss = -11919.76171875
2
Iteration 29800: Loss = -11919.7626953125
3
Iteration 29900: Loss = -11919.76171875
4
pi: tensor([[0.3726, 0.6274],
        [0.6607, 0.3393]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5188, 0.4812], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3039, 0.0986],
         [0.9797, 0.2982]],

        [[0.7013, 0.1031],
         [0.0697, 0.0074]],

        [[0.7556, 0.0961],
         [0.3751, 0.0688]],

        [[0.5036, 0.0951],
         [0.4489, 0.4097]],

        [[0.7766, 0.1111],
         [0.3857, 0.2716]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 3
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03192093449851545
Average Adjusted Rand Index: 0.968160299616196
[-0.00039543502922895045, 0.03192093449851545] [-0.0003077958928485548, 0.968160299616196] [12402.66015625, 11919.7607421875]
-------------------------------------
This iteration is 11
True Objective function: Loss = -11900.076634093703
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18041.087890625
Iteration 100: Loss = -13813.3828125
Iteration 200: Loss = -12708.8818359375
Iteration 300: Loss = -12522.232421875
Iteration 400: Loss = -12480.205078125
Iteration 500: Loss = -12461.4912109375
Iteration 600: Loss = -12452.595703125
Iteration 700: Loss = -12448.435546875
Iteration 800: Loss = -12446.119140625
Iteration 900: Loss = -12444.673828125
Iteration 1000: Loss = -12443.708984375
Iteration 1100: Loss = -12443.0107421875
Iteration 1200: Loss = -12442.4833984375
Iteration 1300: Loss = -12442.0751953125
Iteration 1400: Loss = -12441.751953125
Iteration 1500: Loss = -12441.4892578125
Iteration 1600: Loss = -12441.2705078125
Iteration 1700: Loss = -12441.08984375
Iteration 1800: Loss = -12440.9365234375
Iteration 1900: Loss = -12440.80859375
Iteration 2000: Loss = -12440.701171875
Iteration 2100: Loss = -12440.6044921875
Iteration 2200: Loss = -12440.521484375
Iteration 2300: Loss = -12440.4482421875
Iteration 2400: Loss = -12440.3818359375
Iteration 2500: Loss = -12440.3193359375
Iteration 2600: Loss = -12440.265625
Iteration 2700: Loss = -12440.212890625
Iteration 2800: Loss = -12440.1611328125
Iteration 2900: Loss = -12440.1142578125
Iteration 3000: Loss = -12440.072265625
Iteration 3100: Loss = -12440.02734375
Iteration 3200: Loss = -12439.984375
Iteration 3300: Loss = -12439.9423828125
Iteration 3400: Loss = -12439.8984375
Iteration 3500: Loss = -12439.8515625
Iteration 3600: Loss = -12439.80078125
Iteration 3700: Loss = -12439.7509765625
Iteration 3800: Loss = -12439.6982421875
Iteration 3900: Loss = -12439.6416015625
Iteration 4000: Loss = -12439.583984375
Iteration 4100: Loss = -12439.5263671875
Iteration 4200: Loss = -12439.4677734375
Iteration 4300: Loss = -12439.4189453125
Iteration 4400: Loss = -12439.375
Iteration 4500: Loss = -12439.3349609375
Iteration 4600: Loss = -12439.294921875
Iteration 4700: Loss = -12439.2578125
Iteration 4800: Loss = -12439.2236328125
Iteration 4900: Loss = -12439.1875
Iteration 5000: Loss = -12439.1513671875
Iteration 5100: Loss = -12439.1171875
Iteration 5200: Loss = -12439.080078125
Iteration 5300: Loss = -12439.04296875
Iteration 5400: Loss = -12439.0048828125
Iteration 5500: Loss = -12438.9697265625
Iteration 5600: Loss = -12438.9306640625
Iteration 5700: Loss = -12438.89453125
Iteration 5800: Loss = -12438.8603515625
Iteration 5900: Loss = -12438.8251953125
Iteration 6000: Loss = -12438.79296875
Iteration 6100: Loss = -12438.7607421875
Iteration 6200: Loss = -12438.7333984375
Iteration 6300: Loss = -12438.705078125
Iteration 6400: Loss = -12438.6796875
Iteration 6500: Loss = -12438.6591796875
Iteration 6600: Loss = -12438.63671875
Iteration 6700: Loss = -12438.619140625
Iteration 6800: Loss = -12438.6005859375
Iteration 6900: Loss = -12438.5849609375
Iteration 7000: Loss = -12438.5712890625
Iteration 7100: Loss = -12438.5595703125
Iteration 7200: Loss = -12438.548828125
Iteration 7300: Loss = -12438.5380859375
Iteration 7400: Loss = -12438.5322265625
Iteration 7500: Loss = -12438.521484375
Iteration 7600: Loss = -12438.515625
Iteration 7700: Loss = -12438.509765625
Iteration 7800: Loss = -12438.5029296875
Iteration 7900: Loss = -12438.4990234375
Iteration 8000: Loss = -12438.4931640625
Iteration 8100: Loss = -12438.4892578125
Iteration 8200: Loss = -12438.4853515625
Iteration 8300: Loss = -12438.482421875
Iteration 8400: Loss = -12438.4775390625
Iteration 8500: Loss = -12438.4755859375
Iteration 8600: Loss = -12438.47265625
Iteration 8700: Loss = -12438.46875
Iteration 8800: Loss = -12438.466796875
Iteration 8900: Loss = -12438.46484375
Iteration 9000: Loss = -12438.4609375
Iteration 9100: Loss = -12438.458984375
Iteration 9200: Loss = -12438.45703125
Iteration 9300: Loss = -12438.455078125
Iteration 9400: Loss = -12438.453125
Iteration 9500: Loss = -12438.453125
Iteration 9600: Loss = -12438.4501953125
Iteration 9700: Loss = -12438.44921875
Iteration 9800: Loss = -12438.4482421875
Iteration 9900: Loss = -12438.4453125
Iteration 10000: Loss = -12438.4453125
Iteration 10100: Loss = -12438.4423828125
Iteration 10200: Loss = -12438.4423828125
Iteration 10300: Loss = -12438.4404296875
Iteration 10400: Loss = -12438.4404296875
Iteration 10500: Loss = -12438.4384765625
Iteration 10600: Loss = -12438.4375
Iteration 10700: Loss = -12438.4365234375
Iteration 10800: Loss = -12438.4384765625
1
Iteration 10900: Loss = -12438.4365234375
Iteration 11000: Loss = -12438.435546875
Iteration 11100: Loss = -12438.43359375
Iteration 11200: Loss = -12438.4345703125
1
Iteration 11300: Loss = -12438.43359375
Iteration 11400: Loss = -12438.431640625
Iteration 11500: Loss = -12438.4326171875
1
Iteration 11600: Loss = -12438.4326171875
2
Iteration 11700: Loss = -12438.4306640625
Iteration 11800: Loss = -12438.4306640625
Iteration 11900: Loss = -12438.4306640625
Iteration 12000: Loss = -12438.4306640625
Iteration 12100: Loss = -12438.4296875
Iteration 12200: Loss = -12438.4296875
Iteration 12300: Loss = -12438.4287109375
Iteration 12400: Loss = -12438.427734375
Iteration 12500: Loss = -12438.427734375
Iteration 12600: Loss = -12438.4287109375
1
Iteration 12700: Loss = -12438.42578125
Iteration 12800: Loss = -12438.4267578125
1
Iteration 12900: Loss = -12438.4267578125
2
Iteration 13000: Loss = -12438.427734375
3
Iteration 13100: Loss = -12438.4267578125
4
Iteration 13200: Loss = -12438.4267578125
5
Iteration 13300: Loss = -12438.427734375
6
Iteration 13400: Loss = -12438.42578125
Iteration 13500: Loss = -12438.42578125
Iteration 13600: Loss = -12438.42578125
Iteration 13700: Loss = -12438.42578125
Iteration 13800: Loss = -12438.42578125
Iteration 13900: Loss = -12438.4248046875
Iteration 14000: Loss = -12438.4267578125
1
Iteration 14100: Loss = -12438.42578125
2
Iteration 14200: Loss = -12438.42578125
3
Iteration 14300: Loss = -12438.4267578125
4
Iteration 14400: Loss = -12438.42578125
5
Iteration 14500: Loss = -12438.4248046875
Iteration 14600: Loss = -12438.42578125
1
Iteration 14700: Loss = -12438.42578125
2
Iteration 14800: Loss = -12438.423828125
Iteration 14900: Loss = -12438.423828125
Iteration 15000: Loss = -12438.4228515625
Iteration 15100: Loss = -12438.423828125
1
Iteration 15200: Loss = -12438.423828125
2
Iteration 15300: Loss = -12438.4248046875
3
Iteration 15400: Loss = -12438.4248046875
4
Iteration 15500: Loss = -12438.4248046875
5
Iteration 15600: Loss = -12438.423828125
6
Iteration 15700: Loss = -12438.4248046875
7
Iteration 15800: Loss = -12438.4248046875
8
Iteration 15900: Loss = -12438.4248046875
9
Iteration 16000: Loss = -12438.423828125
10
Iteration 16100: Loss = -12438.4248046875
11
Iteration 16200: Loss = -12438.423828125
12
Iteration 16300: Loss = -12438.423828125
13
Iteration 16400: Loss = -12438.4228515625
Iteration 16500: Loss = -12438.423828125
1
Iteration 16600: Loss = -12438.423828125
2
Iteration 16700: Loss = -12438.423828125
3
Iteration 16800: Loss = -12438.423828125
4
Iteration 16900: Loss = -12438.4228515625
Iteration 17000: Loss = -12438.4248046875
1
Iteration 17100: Loss = -12438.423828125
2
Iteration 17200: Loss = -12438.423828125
3
Iteration 17300: Loss = -12438.423828125
4
Iteration 17400: Loss = -12438.423828125
5
Iteration 17500: Loss = -12438.4228515625
Iteration 17600: Loss = -12438.423828125
1
Iteration 17700: Loss = -12438.4228515625
Iteration 17800: Loss = -12438.4248046875
1
Iteration 17900: Loss = -12438.423828125
2
Iteration 18000: Loss = -12438.4248046875
3
Iteration 18100: Loss = -12438.423828125
4
Iteration 18200: Loss = -12438.423828125
5
Iteration 18300: Loss = -12438.423828125
6
Iteration 18400: Loss = -12438.421875
Iteration 18500: Loss = -12438.4248046875
1
Iteration 18600: Loss = -12438.423828125
2
Iteration 18700: Loss = -12438.421875
Iteration 18800: Loss = -12438.421875
Iteration 18900: Loss = -12438.4248046875
1
Iteration 19000: Loss = -12438.423828125
2
Iteration 19100: Loss = -12438.423828125
3
Iteration 19200: Loss = -12438.423828125
4
Iteration 19300: Loss = -12438.4228515625
5
Iteration 19400: Loss = -12438.423828125
6
Iteration 19500: Loss = -12438.423828125
7
Iteration 19600: Loss = -12438.423828125
8
Iteration 19700: Loss = -12438.423828125
9
Iteration 19800: Loss = -12438.4248046875
10
Iteration 19900: Loss = -12438.423828125
11
Iteration 20000: Loss = -12438.423828125
12
Iteration 20100: Loss = -12438.4228515625
13
Iteration 20200: Loss = -12438.42578125
14
Iteration 20300: Loss = -12438.4228515625
15
Stopping early at iteration 20300 due to no improvement.
pi: tensor([[0.9892, 0.0108],
        [0.9662, 0.0338]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0039, 0.9961], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2030, 0.1973],
         [0.8557, 0.1976]],

        [[0.0093, 0.2193],
         [0.0125, 0.6476]],

        [[0.0330, 0.0812],
         [0.9116, 0.9921]],

        [[0.9264, 0.2155],
         [0.9526, 0.9561]],

        [[0.0361, 0.1982],
         [0.2895, 0.9354]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.005453571602473584
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0005934547855384487
Average Adjusted Rand Index: 0.0010907143204947167
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -41056.71484375
Iteration 100: Loss = -24840.2109375
Iteration 200: Loss = -15916.412109375
Iteration 300: Loss = -13356.9013671875
Iteration 400: Loss = -12898.68359375
Iteration 500: Loss = -12738.0068359375
Iteration 600: Loss = -12654.3857421875
Iteration 700: Loss = -12594.9140625
Iteration 800: Loss = -12557.4912109375
Iteration 900: Loss = -12535.6923828125
Iteration 1000: Loss = -12517.9501953125
Iteration 1100: Loss = -12503.064453125
Iteration 1200: Loss = -12494.080078125
Iteration 1300: Loss = -12486.95703125
Iteration 1400: Loss = -12481.1796875
Iteration 1500: Loss = -12476.4140625
Iteration 1600: Loss = -12472.4248046875
Iteration 1700: Loss = -12469.0478515625
Iteration 1800: Loss = -12466.162109375
Iteration 1900: Loss = -12463.66796875
Iteration 2000: Loss = -12461.5009765625
Iteration 2100: Loss = -12459.6025390625
Iteration 2200: Loss = -12457.9296875
Iteration 2300: Loss = -12456.4482421875
Iteration 2400: Loss = -12455.126953125
Iteration 2500: Loss = -12453.947265625
Iteration 2600: Loss = -12452.8876953125
Iteration 2700: Loss = -12451.9306640625
Iteration 2800: Loss = -12451.0634765625
Iteration 2900: Loss = -12450.2783203125
Iteration 3000: Loss = -12449.564453125
Iteration 3100: Loss = -12448.912109375
Iteration 3200: Loss = -12448.3134765625
Iteration 3300: Loss = -12447.765625
Iteration 3400: Loss = -12447.265625
Iteration 3500: Loss = -12446.8017578125
Iteration 3600: Loss = -12446.3759765625
Iteration 3700: Loss = -12445.9814453125
Iteration 3800: Loss = -12445.6162109375
Iteration 3900: Loss = -12445.27734375
Iteration 4000: Loss = -12444.9638671875
Iteration 4100: Loss = -12444.6728515625
Iteration 4200: Loss = -12444.3994140625
Iteration 4300: Loss = -12444.1494140625
Iteration 4400: Loss = -12443.9140625
Iteration 4500: Loss = -12443.693359375
Iteration 4600: Loss = -12443.48828125
Iteration 4700: Loss = -12443.2958984375
Iteration 4800: Loss = -12443.1171875
Iteration 4900: Loss = -12442.94921875
Iteration 5000: Loss = -12442.791015625
Iteration 5100: Loss = -12442.6435546875
Iteration 5200: Loss = -12442.5048828125
Iteration 5300: Loss = -12442.375
Iteration 5400: Loss = -12442.2529296875
Iteration 5500: Loss = -12442.138671875
Iteration 5600: Loss = -12442.03125
Iteration 5700: Loss = -12441.9287109375
Iteration 5800: Loss = -12441.83203125
Iteration 5900: Loss = -12441.7431640625
Iteration 6000: Loss = -12441.65625
Iteration 6100: Loss = -12441.576171875
Iteration 6200: Loss = -12441.501953125
Iteration 6300: Loss = -12441.431640625
Iteration 6400: Loss = -12441.36328125
Iteration 6500: Loss = -12441.3017578125
Iteration 6600: Loss = -12441.2412109375
Iteration 6700: Loss = -12441.185546875
Iteration 6800: Loss = -12441.1298828125
Iteration 6900: Loss = -12441.0810546875
Iteration 7000: Loss = -12441.033203125
Iteration 7100: Loss = -12440.98828125
Iteration 7200: Loss = -12440.9443359375
Iteration 7300: Loss = -12440.904296875
Iteration 7400: Loss = -12440.8671875
Iteration 7500: Loss = -12440.8330078125
Iteration 7600: Loss = -12440.796875
Iteration 7700: Loss = -12440.7666015625
Iteration 7800: Loss = -12440.7353515625
Iteration 7900: Loss = -12440.7080078125
Iteration 8000: Loss = -12440.6796875
Iteration 8100: Loss = -12440.6552734375
Iteration 8200: Loss = -12440.6298828125
Iteration 8300: Loss = -12440.607421875
Iteration 8400: Loss = -12440.5859375
Iteration 8500: Loss = -12440.5634765625
Iteration 8600: Loss = -12440.5439453125
Iteration 8700: Loss = -12440.525390625
Iteration 8800: Loss = -12440.5087890625
Iteration 8900: Loss = -12440.4921875
Iteration 9000: Loss = -12440.478515625
Iteration 9100: Loss = -12440.462890625
Iteration 9200: Loss = -12440.4482421875
Iteration 9300: Loss = -12440.435546875
Iteration 9400: Loss = -12440.421875
Iteration 9500: Loss = -12440.4091796875
Iteration 9600: Loss = -12440.3974609375
Iteration 9700: Loss = -12440.3896484375
Iteration 9800: Loss = -12440.37890625
Iteration 9900: Loss = -12440.369140625
Iteration 10000: Loss = -12440.3583984375
Iteration 10100: Loss = -12440.3544921875
Iteration 10200: Loss = -12440.341796875
Iteration 10300: Loss = -12440.3349609375
Iteration 10400: Loss = -12440.328125
Iteration 10500: Loss = -12440.3212890625
Iteration 10600: Loss = -12440.3134765625
Iteration 10700: Loss = -12440.3076171875
Iteration 10800: Loss = -12440.302734375
Iteration 10900: Loss = -12440.2958984375
Iteration 11000: Loss = -12440.291015625
Iteration 11100: Loss = -12440.28515625
Iteration 11200: Loss = -12440.28125
Iteration 11300: Loss = -12440.27734375
Iteration 11400: Loss = -12440.2724609375
Iteration 11500: Loss = -12440.2685546875
Iteration 11600: Loss = -12440.2646484375
Iteration 11700: Loss = -12440.26171875
Iteration 11800: Loss = -12440.2587890625
Iteration 11900: Loss = -12440.2548828125
Iteration 12000: Loss = -12440.251953125
Iteration 12100: Loss = -12440.2470703125
Iteration 12200: Loss = -12440.24609375
Iteration 12300: Loss = -12440.2412109375
Iteration 12400: Loss = -12440.2333984375
Iteration 12500: Loss = -12439.9296875
Iteration 12600: Loss = -12439.6552734375
Iteration 12700: Loss = -12439.5712890625
Iteration 12800: Loss = -12439.521484375
Iteration 12900: Loss = -12439.4853515625
Iteration 13000: Loss = -12439.4638671875
Iteration 13100: Loss = -12439.4453125
Iteration 13200: Loss = -12439.4306640625
Iteration 13300: Loss = -12439.41796875
Iteration 13400: Loss = -12439.40625
Iteration 13500: Loss = -12439.39453125
Iteration 13600: Loss = -12439.3828125
Iteration 13700: Loss = -12439.3720703125
Iteration 13800: Loss = -12439.3583984375
Iteration 13900: Loss = -12439.345703125
Iteration 14000: Loss = -12439.3291015625
Iteration 14100: Loss = -12439.3154296875
Iteration 14200: Loss = -12439.2978515625
Iteration 14300: Loss = -12439.27734375
Iteration 14400: Loss = -12439.25390625
Iteration 14500: Loss = -12439.2255859375
Iteration 14600: Loss = -12439.189453125
Iteration 14700: Loss = -12439.150390625
Iteration 14800: Loss = -12439.1025390625
Iteration 14900: Loss = -12439.041015625
Iteration 15000: Loss = -12438.97265625
Iteration 15100: Loss = -12438.9072265625
Iteration 15200: Loss = -12438.8564453125
Iteration 15300: Loss = -12438.8271484375
Iteration 15400: Loss = -12438.8154296875
Iteration 15500: Loss = -12438.8115234375
Iteration 15600: Loss = -12438.8076171875
Iteration 15700: Loss = -12438.806640625
Iteration 15800: Loss = -12438.8056640625
Iteration 15900: Loss = -12438.8056640625
Iteration 16000: Loss = -12438.8056640625
Iteration 16100: Loss = -12438.8046875
Iteration 16200: Loss = -12438.8037109375
Iteration 16300: Loss = -12438.8037109375
Iteration 16400: Loss = -12438.802734375
Iteration 16500: Loss = -12438.802734375
Iteration 16600: Loss = -12438.8017578125
Iteration 16700: Loss = -12438.8017578125
Iteration 16800: Loss = -12438.7998046875
Iteration 16900: Loss = -12438.8017578125
1
Iteration 17000: Loss = -12438.802734375
2
Iteration 17100: Loss = -12438.802734375
3
Iteration 17200: Loss = -12438.8017578125
4
Iteration 17300: Loss = -12438.802734375
5
Iteration 17400: Loss = -12438.80078125
6
Iteration 17500: Loss = -12438.8037109375
7
Iteration 17600: Loss = -12438.798828125
Iteration 17700: Loss = -12438.7998046875
1
Iteration 17800: Loss = -12438.80078125
2
Iteration 17900: Loss = -12438.80078125
3
Iteration 18000: Loss = -12438.798828125
Iteration 18100: Loss = -12438.80078125
1
Iteration 18200: Loss = -12438.7978515625
Iteration 18300: Loss = -12438.7998046875
1
Iteration 18400: Loss = -12438.7998046875
2
Iteration 18500: Loss = -12438.798828125
3
Iteration 18600: Loss = -12438.798828125
4
Iteration 18700: Loss = -12438.80078125
5
Iteration 18800: Loss = -12438.798828125
6
Iteration 18900: Loss = -12438.798828125
7
Iteration 19000: Loss = -12438.798828125
8
Iteration 19100: Loss = -12438.80078125
9
Iteration 19200: Loss = -12438.7978515625
Iteration 19300: Loss = -12438.7998046875
1
Iteration 19400: Loss = -12438.7998046875
2
Iteration 19500: Loss = -12438.7998046875
3
Iteration 19600: Loss = -12438.7978515625
Iteration 19700: Loss = -12438.7978515625
Iteration 19800: Loss = -12438.7998046875
1
Iteration 19900: Loss = -12438.798828125
2
Iteration 20000: Loss = -12438.7978515625
Iteration 20100: Loss = -12438.798828125
1
Iteration 20200: Loss = -12438.798828125
2
Iteration 20300: Loss = -12438.798828125
3
Iteration 20400: Loss = -12438.7978515625
Iteration 20500: Loss = -12438.798828125
1
Iteration 20600: Loss = -12438.7978515625
Iteration 20700: Loss = -12438.798828125
1
Iteration 20800: Loss = -12438.798828125
2
Iteration 20900: Loss = -12438.7998046875
3
Iteration 21000: Loss = -12438.798828125
4
Iteration 21100: Loss = -12438.798828125
5
Iteration 21200: Loss = -12438.798828125
6
Iteration 21300: Loss = -12438.798828125
7
Iteration 21400: Loss = -12438.798828125
8
Iteration 21500: Loss = -12438.7978515625
Iteration 21600: Loss = -12438.798828125
1
Iteration 21700: Loss = -12438.7978515625
Iteration 21800: Loss = -12438.7978515625
Iteration 21900: Loss = -12438.796875
Iteration 22000: Loss = -12438.798828125
1
Iteration 22100: Loss = -12438.796875
Iteration 22200: Loss = -12438.798828125
1
Iteration 22300: Loss = -12438.798828125
2
Iteration 22400: Loss = -12438.798828125
3
Iteration 22500: Loss = -12438.798828125
4
Iteration 22600: Loss = -12438.7998046875
5
Iteration 22700: Loss = -12438.798828125
6
Iteration 22800: Loss = -12438.796875
Iteration 22900: Loss = -12438.796875
Iteration 23000: Loss = -12438.798828125
1
Iteration 23100: Loss = -12438.7978515625
2
Iteration 23200: Loss = -12438.7978515625
3
Iteration 23300: Loss = -12438.796875
Iteration 23400: Loss = -12438.7978515625
1
Iteration 23500: Loss = -12438.796875
Iteration 23600: Loss = -12438.798828125
1
Iteration 23700: Loss = -12438.7978515625
2
Iteration 23800: Loss = -12438.796875
Iteration 23900: Loss = -12438.796875
Iteration 24000: Loss = -12438.798828125
1
Iteration 24100: Loss = -12438.7978515625
2
Iteration 24200: Loss = -12438.798828125
3
Iteration 24300: Loss = -12438.798828125
4
Iteration 24400: Loss = -12438.796875
Iteration 24500: Loss = -12438.798828125
1
Iteration 24600: Loss = -12438.796875
Iteration 24700: Loss = -12438.796875
Iteration 24800: Loss = -12438.796875
Iteration 24900: Loss = -12438.796875
Iteration 25000: Loss = -12438.796875
Iteration 25100: Loss = -12438.798828125
1
Iteration 25200: Loss = -12438.796875
Iteration 25300: Loss = -12438.798828125
1
Iteration 25400: Loss = -12438.7978515625
2
Iteration 25500: Loss = -12438.796875
Iteration 25600: Loss = -12438.796875
Iteration 25700: Loss = -12438.798828125
1
Iteration 25800: Loss = -12438.798828125
2
Iteration 25900: Loss = -12438.796875
Iteration 26000: Loss = -12438.796875
Iteration 26100: Loss = -12438.798828125
1
Iteration 26200: Loss = -12438.796875
Iteration 26300: Loss = -12438.798828125
1
Iteration 26400: Loss = -12438.796875
Iteration 26500: Loss = -12438.796875
Iteration 26600: Loss = -12438.7978515625
1
Iteration 26700: Loss = -12438.796875
Iteration 26800: Loss = -12438.796875
Iteration 26900: Loss = -12438.7978515625
1
Iteration 27000: Loss = -12438.798828125
2
Iteration 27100: Loss = -12438.7978515625
3
Iteration 27200: Loss = -12438.7978515625
4
Iteration 27300: Loss = -12438.798828125
5
Iteration 27400: Loss = -12438.796875
Iteration 27500: Loss = -12438.798828125
1
Iteration 27600: Loss = -12438.7978515625
2
Iteration 27700: Loss = -12438.798828125
3
Iteration 27800: Loss = -12438.796875
Iteration 27900: Loss = -12438.7978515625
1
Iteration 28000: Loss = -12438.796875
Iteration 28100: Loss = -12438.7958984375
Iteration 28200: Loss = -12438.796875
1
Iteration 28300: Loss = -12438.796875
2
Iteration 28400: Loss = -12438.796875
3
Iteration 28500: Loss = -12438.798828125
4
Iteration 28600: Loss = -12438.798828125
5
Iteration 28700: Loss = -12438.798828125
6
Iteration 28800: Loss = -12438.7958984375
Iteration 28900: Loss = -12438.796875
1
Iteration 29000: Loss = -12438.796875
2
Iteration 29100: Loss = -12438.7978515625
3
Iteration 29200: Loss = -12438.7978515625
4
Iteration 29300: Loss = -12438.7978515625
5
Iteration 29400: Loss = -12438.798828125
6
Iteration 29500: Loss = -12438.796875
7
Iteration 29600: Loss = -12438.798828125
8
Iteration 29700: Loss = -12438.798828125
9
Iteration 29800: Loss = -12438.798828125
10
Iteration 29900: Loss = -12438.796875
11
pi: tensor([[9.8787e-01, 1.2130e-02],
        [9.9999e-01, 5.4997e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 5.4691e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2018, 0.1944],
         [0.2334, 0.2251]],

        [[0.0270, 0.2338],
         [0.8063, 0.0070]],

        [[0.8840, 0.0806],
         [0.7018, 0.7521]],

        [[0.6237, 0.2295],
         [0.9633, 0.9919]],

        [[0.9752, 0.2036],
         [0.6521, 0.0152]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.005453571602473584
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0007718940776607481
Average Adjusted Rand Index: 0.0010907143204947167
[0.0005934547855384487, 0.0007718940776607481] [0.0010907143204947167, 0.0010907143204947167] [12438.4228515625, 12438.796875]
-------------------------------------
This iteration is 12
True Objective function: Loss = -11757.709003650833
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -34464.53125
Iteration 100: Loss = -23464.1171875
Iteration 200: Loss = -14688.9482421875
Iteration 300: Loss = -13010.6787109375
Iteration 400: Loss = -12654.1953125
Iteration 500: Loss = -12556.1826171875
Iteration 600: Loss = -12508.53515625
Iteration 700: Loss = -12477.39453125
Iteration 800: Loss = -12455.44140625
Iteration 900: Loss = -12438.419921875
Iteration 1000: Loss = -12425.2607421875
Iteration 1100: Loss = -12412.0224609375
Iteration 1200: Loss = -12397.1044921875
Iteration 1300: Loss = -12387.22265625
Iteration 1400: Loss = -12375.4052734375
Iteration 1500: Loss = -12363.1318359375
Iteration 1600: Loss = -12352.9912109375
Iteration 1700: Loss = -12337.060546875
Iteration 1800: Loss = -12328.2646484375
Iteration 1900: Loss = -12318.482421875
Iteration 2000: Loss = -12313.1552734375
Iteration 2100: Loss = -12310.052734375
Iteration 2200: Loss = -12307.173828125
Iteration 2300: Loss = -12301.55078125
Iteration 2400: Loss = -12299.921875
Iteration 2500: Loss = -12298.6669921875
Iteration 2600: Loss = -12297.6328125
Iteration 2700: Loss = -12296.7568359375
Iteration 2800: Loss = -12296.001953125
Iteration 2900: Loss = -12295.34375
Iteration 3000: Loss = -12294.7646484375
Iteration 3100: Loss = -12294.2509765625
Iteration 3200: Loss = -12293.7919921875
Iteration 3300: Loss = -12293.376953125
Iteration 3400: Loss = -12293.0048828125
Iteration 3500: Loss = -12292.6650390625
Iteration 3600: Loss = -12292.3515625
Iteration 3700: Loss = -12292.0693359375
Iteration 3800: Loss = -12291.810546875
Iteration 3900: Loss = -12291.568359375
Iteration 4000: Loss = -12291.279296875
Iteration 4100: Loss = -12285.3076171875
Iteration 4200: Loss = -12284.900390625
Iteration 4300: Loss = -12284.6376953125
Iteration 4400: Loss = -12284.4140625
Iteration 4500: Loss = -12284.216796875
Iteration 4600: Loss = -12284.0390625
Iteration 4700: Loss = -12283.8759765625
Iteration 4800: Loss = -12283.7265625
Iteration 4900: Loss = -12283.58984375
Iteration 5000: Loss = -12283.462890625
Iteration 5100: Loss = -12283.34375
Iteration 5200: Loss = -12283.234375
Iteration 5300: Loss = -12283.1328125
Iteration 5400: Loss = -12283.03515625
Iteration 5500: Loss = -12282.947265625
Iteration 5600: Loss = -12282.86328125
Iteration 5700: Loss = -12282.7861328125
Iteration 5800: Loss = -12282.7109375
Iteration 5900: Loss = -12282.64453125
Iteration 6000: Loss = -12282.578125
Iteration 6100: Loss = -12282.5166015625
Iteration 6200: Loss = -12282.4609375
Iteration 6300: Loss = -12282.40625
Iteration 6400: Loss = -12282.35546875
Iteration 6500: Loss = -12282.30859375
Iteration 6600: Loss = -12282.265625
Iteration 6700: Loss = -12282.220703125
Iteration 6800: Loss = -12282.18359375
Iteration 6900: Loss = -12282.1455078125
Iteration 7000: Loss = -12282.109375
Iteration 7100: Loss = -12282.0751953125
Iteration 7200: Loss = -12282.0439453125
Iteration 7300: Loss = -12282.0146484375
Iteration 7400: Loss = -12281.9853515625
Iteration 7500: Loss = -12281.958984375
Iteration 7600: Loss = -12281.935546875
Iteration 7700: Loss = -12281.91015625
Iteration 7800: Loss = -12281.890625
Iteration 7900: Loss = -12281.8671875
Iteration 8000: Loss = -12281.84765625
Iteration 8100: Loss = -12281.826171875
Iteration 8200: Loss = -12281.8095703125
Iteration 8300: Loss = -12281.79296875
Iteration 8400: Loss = -12281.7763671875
Iteration 8500: Loss = -12281.759765625
Iteration 8600: Loss = -12281.744140625
Iteration 8700: Loss = -12281.73046875
Iteration 8800: Loss = -12281.71484375
Iteration 8900: Loss = -12281.703125
Iteration 9000: Loss = -12281.69140625
Iteration 9100: Loss = -12281.6787109375
Iteration 9200: Loss = -12281.666015625
Iteration 9300: Loss = -12281.6572265625
Iteration 9400: Loss = -12281.646484375
Iteration 9500: Loss = -12281.6357421875
Iteration 9600: Loss = -12281.625
Iteration 9700: Loss = -12281.6171875
Iteration 9800: Loss = -12281.607421875
Iteration 9900: Loss = -12281.59765625
Iteration 10000: Loss = -12281.5888671875
Iteration 10100: Loss = -12281.5810546875
Iteration 10200: Loss = -12281.572265625
Iteration 10300: Loss = -12281.5634765625
Iteration 10400: Loss = -12281.5537109375
Iteration 10500: Loss = -12281.5458984375
Iteration 10600: Loss = -12281.5380859375
Iteration 10700: Loss = -12281.53515625
Iteration 10800: Loss = -12281.5224609375
Iteration 10900: Loss = -12281.515625
Iteration 11000: Loss = -12281.509765625
Iteration 11100: Loss = -12281.5029296875
Iteration 11200: Loss = -12281.4951171875
Iteration 11300: Loss = -12281.48828125
Iteration 11400: Loss = -12281.4814453125
Iteration 11500: Loss = -12281.4755859375
Iteration 11600: Loss = -12281.46875
Iteration 11700: Loss = -12281.466796875
Iteration 11800: Loss = -12281.458984375
Iteration 11900: Loss = -12281.455078125
Iteration 12000: Loss = -12281.4482421875
Iteration 12100: Loss = -12281.4443359375
Iteration 12200: Loss = -12281.4404296875
Iteration 12300: Loss = -12281.435546875
Iteration 12400: Loss = -12281.4296875
Iteration 12500: Loss = -12281.427734375
Iteration 12600: Loss = -12281.423828125
Iteration 12700: Loss = -12281.4208984375
Iteration 12800: Loss = -12281.41796875
Iteration 12900: Loss = -12281.416015625
Iteration 13000: Loss = -12281.412109375
Iteration 13100: Loss = -12281.4091796875
Iteration 13200: Loss = -12281.40625
Iteration 13300: Loss = -12281.4052734375
Iteration 13400: Loss = -12281.40234375
Iteration 13500: Loss = -12281.3984375
Iteration 13600: Loss = -12281.3994140625
1
Iteration 13700: Loss = -12281.39453125
Iteration 13800: Loss = -12281.39453125
Iteration 13900: Loss = -12281.392578125
Iteration 14000: Loss = -12281.3916015625
Iteration 14100: Loss = -12281.390625
Iteration 14200: Loss = -12281.3876953125
Iteration 14300: Loss = -12281.38671875
Iteration 14400: Loss = -12281.38671875
Iteration 14500: Loss = -12281.384765625
Iteration 14600: Loss = -12281.3857421875
1
Iteration 14700: Loss = -12281.3837890625
Iteration 14800: Loss = -12281.3818359375
Iteration 14900: Loss = -12281.3818359375
Iteration 15000: Loss = -12281.3798828125
Iteration 15100: Loss = -12281.380859375
1
Iteration 15200: Loss = -12281.3798828125
Iteration 15300: Loss = -12281.3798828125
Iteration 15400: Loss = -12281.37890625
Iteration 15500: Loss = -12281.3779296875
Iteration 15600: Loss = -12281.376953125
Iteration 15700: Loss = -12281.3759765625
Iteration 15800: Loss = -12281.3759765625
Iteration 15900: Loss = -12281.375
Iteration 16000: Loss = -12281.375
Iteration 16100: Loss = -12281.3740234375
Iteration 16200: Loss = -12281.3740234375
Iteration 16300: Loss = -12281.3740234375
Iteration 16400: Loss = -12281.3740234375
Iteration 16500: Loss = -12281.373046875
Iteration 16600: Loss = -12281.373046875
Iteration 16700: Loss = -12281.3720703125
Iteration 16800: Loss = -12281.37109375
Iteration 16900: Loss = -12281.3740234375
1
Iteration 17000: Loss = -12281.3701171875
Iteration 17100: Loss = -12281.37109375
1
Iteration 17200: Loss = -12281.3720703125
2
Iteration 17300: Loss = -12281.3701171875
Iteration 17400: Loss = -12281.37109375
1
Iteration 17500: Loss = -12281.3701171875
Iteration 17600: Loss = -12281.37109375
1
Iteration 17700: Loss = -12281.3701171875
Iteration 17800: Loss = -12281.3701171875
Iteration 17900: Loss = -12281.3701171875
Iteration 18000: Loss = -12281.3701171875
Iteration 18100: Loss = -12281.37109375
1
Iteration 18200: Loss = -12281.3701171875
Iteration 18300: Loss = -12281.3701171875
Iteration 18400: Loss = -12281.369140625
Iteration 18500: Loss = -12281.369140625
Iteration 18600: Loss = -12281.369140625
Iteration 18700: Loss = -12281.3701171875
1
Iteration 18800: Loss = -12281.3681640625
Iteration 18900: Loss = -12281.369140625
1
Iteration 19000: Loss = -12281.3671875
Iteration 19100: Loss = -12281.3681640625
1
Iteration 19200: Loss = -12281.3681640625
2
Iteration 19300: Loss = -12281.369140625
3
Iteration 19400: Loss = -12281.369140625
4
Iteration 19500: Loss = -12281.3671875
Iteration 19600: Loss = -12281.3671875
Iteration 19700: Loss = -12281.3681640625
1
Iteration 19800: Loss = -12281.3681640625
2
Iteration 19900: Loss = -12281.3681640625
3
Iteration 20000: Loss = -12281.3681640625
4
Iteration 20100: Loss = -12281.3671875
Iteration 20200: Loss = -12281.369140625
1
Iteration 20300: Loss = -12281.3681640625
2
Iteration 20400: Loss = -12281.3681640625
3
Iteration 20500: Loss = -12281.3681640625
4
Iteration 20600: Loss = -12281.3681640625
5
Iteration 20700: Loss = -12281.3681640625
6
Iteration 20800: Loss = -12281.3681640625
7
Iteration 20900: Loss = -12281.3681640625
8
Iteration 21000: Loss = -12281.3681640625
9
Iteration 21100: Loss = -12281.3681640625
10
Iteration 21200: Loss = -12281.369140625
11
Iteration 21300: Loss = -12281.3701171875
12
Iteration 21400: Loss = -12281.369140625
13
Iteration 21500: Loss = -12281.3681640625
14
Iteration 21600: Loss = -12281.3662109375
Iteration 21700: Loss = -12281.3681640625
1
Iteration 21800: Loss = -12281.3671875
2
Iteration 21900: Loss = -12281.3681640625
3
Iteration 22000: Loss = -12281.3671875
4
Iteration 22100: Loss = -12281.3681640625
5
Iteration 22200: Loss = -12281.3681640625
6
Iteration 22300: Loss = -12281.3681640625
7
Iteration 22400: Loss = -12281.3681640625
8
Iteration 22500: Loss = -12281.3662109375
Iteration 22600: Loss = -12281.37109375
1
Iteration 22700: Loss = -12281.3671875
2
Iteration 22800: Loss = -12281.3701171875
3
Iteration 22900: Loss = -12281.3681640625
4
Iteration 23000: Loss = -12281.3662109375
Iteration 23100: Loss = -12281.3671875
1
Iteration 23200: Loss = -12281.3671875
2
Iteration 23300: Loss = -12281.369140625
3
Iteration 23400: Loss = -12281.3671875
4
Iteration 23500: Loss = -12281.3681640625
5
Iteration 23600: Loss = -12281.3681640625
6
Iteration 23700: Loss = -12281.3671875
7
Iteration 23800: Loss = -12281.3701171875
8
Iteration 23900: Loss = -12281.3671875
9
Iteration 24000: Loss = -12281.3681640625
10
Iteration 24100: Loss = -12281.3671875
11
Iteration 24200: Loss = -12281.3671875
12
Iteration 24300: Loss = -12281.3671875
13
Iteration 24400: Loss = -12281.3671875
14
Iteration 24500: Loss = -12281.369140625
15
Stopping early at iteration 24500 due to no improvement.
pi: tensor([[1.0000e+00, 1.1265e-06],
        [1.0000e+00, 1.7253e-07]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0064, 0.9936], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1960, 0.2006],
         [0.9831, 0.2010]],

        [[0.1348, 0.2366],
         [0.4209, 0.1465]],

        [[0.3378, 0.1882],
         [0.0853, 0.9245]],

        [[0.0380, 0.3222],
         [0.0645, 0.0458]],

        [[0.3627, 0.2330],
         [0.1487, 0.9462]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0022415863304625677
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -42738.41015625
Iteration 100: Loss = -27520.669921875
Iteration 200: Loss = -16872.208984375
Iteration 300: Loss = -13589.3603515625
Iteration 400: Loss = -12778.705078125
Iteration 500: Loss = -12570.7607421875
Iteration 600: Loss = -12493.744140625
Iteration 700: Loss = -12453.6044921875
Iteration 800: Loss = -12423.4541015625
Iteration 900: Loss = -12395.779296875
Iteration 1000: Loss = -12374.849609375
Iteration 1100: Loss = -12363.7724609375
Iteration 1200: Loss = -12346.0654296875
Iteration 1300: Loss = -12330.5126953125
Iteration 1400: Loss = -12322.5146484375
Iteration 1500: Loss = -12314.03125
Iteration 1600: Loss = -12306.123046875
Iteration 1700: Loss = -12297.8232421875
Iteration 1800: Loss = -12294.458984375
Iteration 1900: Loss = -12291.7373046875
Iteration 2000: Loss = -12289.1611328125
Iteration 2100: Loss = -12286.13671875
Iteration 2200: Loss = -12281.2607421875
Iteration 2300: Loss = -12276.2265625
Iteration 2400: Loss = -12271.8876953125
Iteration 2500: Loss = -12263.6435546875
Iteration 2600: Loss = -12252.849609375
Iteration 2700: Loss = -12232.087890625
Iteration 2800: Loss = -12206.96484375
Iteration 2900: Loss = -12193.7861328125
Iteration 3000: Loss = -12186.6318359375
Iteration 3100: Loss = -12178.029296875
Iteration 3200: Loss = -12172.8095703125
Iteration 3300: Loss = -12166.748046875
Iteration 3400: Loss = -12164.9755859375
Iteration 3500: Loss = -12148.84765625
Iteration 3600: Loss = -12142.98046875
Iteration 3700: Loss = -12141.1083984375
Iteration 3800: Loss = -12138.361328125
Iteration 3900: Loss = -12128.140625
Iteration 4000: Loss = -12127.353515625
Iteration 4100: Loss = -12126.9130859375
Iteration 4200: Loss = -12121.330078125
Iteration 4300: Loss = -12120.19921875
Iteration 4400: Loss = -12117.396484375
Iteration 4500: Loss = -12109.544921875
Iteration 4600: Loss = -12106.4638671875
Iteration 4700: Loss = -12104.9677734375
Iteration 4800: Loss = -12104.5634765625
Iteration 4900: Loss = -12104.201171875
Iteration 5000: Loss = -12102.099609375
Iteration 5100: Loss = -12100.6201171875
Iteration 5200: Loss = -12100.333984375
Iteration 5300: Loss = -12100.0966796875
Iteration 5400: Loss = -12099.8076171875
Iteration 5500: Loss = -12099.453125
Iteration 5600: Loss = -12099.18359375
Iteration 5700: Loss = -12099.0400390625
Iteration 5800: Loss = -12098.9501953125
Iteration 5900: Loss = -12098.8837890625
Iteration 6000: Loss = -12098.8291015625
Iteration 6100: Loss = -12098.779296875
Iteration 6200: Loss = -12098.734375
Iteration 6300: Loss = -12098.6953125
Iteration 6400: Loss = -12098.6611328125
Iteration 6500: Loss = -12098.6298828125
Iteration 6600: Loss = -12098.6005859375
Iteration 6700: Loss = -12098.5732421875
Iteration 6800: Loss = -12098.5478515625
Iteration 6900: Loss = -12098.525390625
Iteration 7000: Loss = -12098.501953125
Iteration 7100: Loss = -12098.4814453125
Iteration 7200: Loss = -12098.462890625
Iteration 7300: Loss = -12098.4443359375
Iteration 7400: Loss = -12098.427734375
Iteration 7500: Loss = -12098.4111328125
Iteration 7600: Loss = -12098.3955078125
Iteration 7700: Loss = -12098.3818359375
Iteration 7800: Loss = -12098.3671875
Iteration 7900: Loss = -12098.353515625
Iteration 8000: Loss = -12098.3369140625
Iteration 8100: Loss = -12098.3115234375
Iteration 8200: Loss = -12097.8994140625
Iteration 8300: Loss = -12097.267578125
Iteration 8400: Loss = -12097.2568359375
Iteration 8500: Loss = -12097.248046875
Iteration 8600: Loss = -12097.2392578125
Iteration 8700: Loss = -12097.2314453125
Iteration 8800: Loss = -12097.224609375
Iteration 8900: Loss = -12097.21875
Iteration 9000: Loss = -12097.2109375
Iteration 9100: Loss = -12097.205078125
Iteration 9200: Loss = -12097.19921875
Iteration 9300: Loss = -12097.1953125
Iteration 9400: Loss = -12097.1904296875
Iteration 9500: Loss = -12097.185546875
Iteration 9600: Loss = -12097.1796875
Iteration 9700: Loss = -12097.17578125
Iteration 9800: Loss = -12097.1708984375
Iteration 9900: Loss = -12097.16796875
Iteration 10000: Loss = -12097.1630859375
Iteration 10100: Loss = -12097.16015625
Iteration 10200: Loss = -12097.1552734375
Iteration 10300: Loss = -12097.154296875
Iteration 10400: Loss = -12097.150390625
Iteration 10500: Loss = -12097.1474609375
Iteration 10600: Loss = -12097.1455078125
Iteration 10700: Loss = -12097.142578125
Iteration 10800: Loss = -12097.140625
Iteration 10900: Loss = -12097.1376953125
Iteration 11000: Loss = -12097.1357421875
Iteration 11100: Loss = -12097.1328125
Iteration 11200: Loss = -12097.130859375
Iteration 11300: Loss = -12097.12890625
Iteration 11400: Loss = -12097.1259765625
Iteration 11500: Loss = -12097.1240234375
Iteration 11600: Loss = -12097.12109375
Iteration 11700: Loss = -12097.1201171875
Iteration 11800: Loss = -12097.1201171875
Iteration 11900: Loss = -12097.1181640625
Iteration 12000: Loss = -12097.1162109375
Iteration 12100: Loss = -12097.11328125
Iteration 12200: Loss = -12097.11328125
Iteration 12300: Loss = -12097.111328125
Iteration 12400: Loss = -12097.1103515625
Iteration 12500: Loss = -12097.1103515625
Iteration 12600: Loss = -12097.1083984375
Iteration 12700: Loss = -12097.109375
1
Iteration 12800: Loss = -12097.1064453125
Iteration 12900: Loss = -12097.1064453125
Iteration 13000: Loss = -12097.107421875
1
Iteration 13100: Loss = -12097.10546875
Iteration 13200: Loss = -12097.1044921875
Iteration 13300: Loss = -12097.103515625
Iteration 13400: Loss = -12097.103515625
Iteration 13500: Loss = -12097.1025390625
Iteration 13600: Loss = -12097.1015625
Iteration 13700: Loss = -12097.1025390625
1
Iteration 13800: Loss = -12097.1005859375
Iteration 13900: Loss = -12097.1015625
1
Iteration 14000: Loss = -12097.099609375
Iteration 14100: Loss = -12097.0986328125
Iteration 14200: Loss = -12097.09765625
Iteration 14300: Loss = -12097.0986328125
1
Iteration 14400: Loss = -12097.09765625
Iteration 14500: Loss = -12097.0966796875
Iteration 14600: Loss = -12097.09765625
1
Iteration 14700: Loss = -12097.095703125
Iteration 14800: Loss = -12097.095703125
Iteration 14900: Loss = -12097.0947265625
Iteration 15000: Loss = -12097.0859375
Iteration 15100: Loss = -12096.716796875
Iteration 15200: Loss = -12095.7880859375
Iteration 15300: Loss = -12095.62890625
Iteration 15400: Loss = -12095.3505859375
Iteration 15500: Loss = -12094.7705078125
Iteration 15600: Loss = -12094.7412109375
Iteration 15700: Loss = -12094.5361328125
Iteration 15800: Loss = -12094.5341796875
Iteration 15900: Loss = -12094.5009765625
Iteration 16000: Loss = -12094.37890625
Iteration 16100: Loss = -12094.2822265625
Iteration 16200: Loss = -12093.7578125
Iteration 16300: Loss = -12093.5361328125
Iteration 16400: Loss = -12092.69921875
Iteration 16500: Loss = -12091.6904296875
Iteration 16600: Loss = -12091.2802734375
Iteration 16700: Loss = -12089.7490234375
Iteration 16800: Loss = -12089.3642578125
Iteration 16900: Loss = -12087.9892578125
Iteration 17000: Loss = -12086.7314453125
Iteration 17100: Loss = -12061.255859375
Iteration 17200: Loss = -12029.306640625
Iteration 17300: Loss = -12017.69140625
Iteration 17400: Loss = -12017.4658203125
Iteration 17500: Loss = -12017.37890625
Iteration 17600: Loss = -12017.33203125
Iteration 17700: Loss = -12017.302734375
Iteration 17800: Loss = -12017.283203125
Iteration 17900: Loss = -12017.2626953125
Iteration 18000: Loss = -12015.9775390625
Iteration 18100: Loss = -12015.966796875
Iteration 18200: Loss = -12015.958984375
Iteration 18300: Loss = -12015.9541015625
Iteration 18400: Loss = -12015.9482421875
Iteration 18500: Loss = -12015.9453125
Iteration 18600: Loss = -12015.94140625
Iteration 18700: Loss = -12015.939453125
Iteration 18800: Loss = -12015.9365234375
Iteration 18900: Loss = -12015.9326171875
Iteration 19000: Loss = -12015.93359375
1
Iteration 19100: Loss = -12015.931640625
Iteration 19200: Loss = -12015.927734375
Iteration 19300: Loss = -12015.927734375
Iteration 19400: Loss = -12015.9267578125
Iteration 19500: Loss = -12015.92578125
Iteration 19600: Loss = -12015.9248046875
Iteration 19700: Loss = -12015.923828125
Iteration 19800: Loss = -12015.923828125
Iteration 19900: Loss = -12015.921875
Iteration 20000: Loss = -12015.921875
Iteration 20100: Loss = -12015.921875
Iteration 20200: Loss = -12015.9208984375
Iteration 20300: Loss = -12015.919921875
Iteration 20400: Loss = -12015.919921875
Iteration 20500: Loss = -12015.9189453125
Iteration 20600: Loss = -12015.9189453125
Iteration 20700: Loss = -12015.9189453125
Iteration 20800: Loss = -12015.91796875
Iteration 20900: Loss = -12015.91796875
Iteration 21000: Loss = -12015.9169921875
Iteration 21100: Loss = -12015.9169921875
Iteration 21200: Loss = -12015.9189453125
1
Iteration 21300: Loss = -12015.9169921875
Iteration 21400: Loss = -12015.9169921875
Iteration 21500: Loss = -12015.9150390625
Iteration 21600: Loss = -12015.916015625
1
Iteration 21700: Loss = -12015.9169921875
2
Iteration 21800: Loss = -12015.9150390625
Iteration 21900: Loss = -12015.9150390625
Iteration 22000: Loss = -12015.9150390625
Iteration 22100: Loss = -12015.9150390625
Iteration 22200: Loss = -12015.9140625
Iteration 22300: Loss = -12015.916015625
1
Iteration 22400: Loss = -12015.9150390625
2
Iteration 22500: Loss = -12015.9150390625
3
Iteration 22600: Loss = -12015.9150390625
4
Iteration 22700: Loss = -12015.9140625
Iteration 22800: Loss = -12015.9130859375
Iteration 22900: Loss = -12015.9130859375
Iteration 23000: Loss = -12015.9150390625
1
Iteration 23100: Loss = -12015.90234375
Iteration 23200: Loss = -12015.90234375
Iteration 23300: Loss = -12015.9033203125
1
Iteration 23400: Loss = -12015.90234375
Iteration 23500: Loss = -12015.90234375
Iteration 23600: Loss = -12015.9013671875
Iteration 23700: Loss = -12015.9033203125
1
Iteration 23800: Loss = -12015.9033203125
2
Iteration 23900: Loss = -12015.9033203125
3
Iteration 24000: Loss = -12015.9033203125
4
Iteration 24100: Loss = -12015.9013671875
Iteration 24200: Loss = -12015.90234375
1
Iteration 24300: Loss = -12015.90234375
2
Iteration 24400: Loss = -12015.904296875
3
Iteration 24500: Loss = -12015.9013671875
Iteration 24600: Loss = -12015.90234375
1
Iteration 24700: Loss = -12015.90234375
2
Iteration 24800: Loss = -12015.900390625
Iteration 24900: Loss = -12015.8984375
Iteration 25000: Loss = -12015.900390625
1
Iteration 25100: Loss = -12015.8994140625
2
Iteration 25200: Loss = -12015.900390625
3
Iteration 25300: Loss = -12015.7529296875
Iteration 25400: Loss = -12011.458984375
Iteration 25500: Loss = -12003.3798828125
Iteration 25600: Loss = -11999.8251953125
Iteration 25700: Loss = -11997.2509765625
Iteration 25800: Loss = -11995.3017578125
Iteration 25900: Loss = -11994.5654296875
Iteration 26000: Loss = -11987.2763671875
Iteration 26100: Loss = -11985.87109375
Iteration 26200: Loss = -11981.9130859375
Iteration 26300: Loss = -11981.833984375
Iteration 26400: Loss = -11981.7822265625
Iteration 26500: Loss = -11981.7392578125
Iteration 26600: Loss = -11981.7412109375
1
Iteration 26700: Loss = -11981.740234375
2
Iteration 26800: Loss = -11981.740234375
3
Iteration 26900: Loss = -11981.591796875
Iteration 27000: Loss = -11981.099609375
Iteration 27100: Loss = -11981.0966796875
Iteration 27200: Loss = -11981.0966796875
Iteration 27300: Loss = -11980.275390625
Iteration 27400: Loss = -11980.2607421875
Iteration 27500: Loss = -11980.2587890625
Iteration 27600: Loss = -11980.2587890625
Iteration 27700: Loss = -11980.255859375
Iteration 27800: Loss = -11980.2548828125
Iteration 27900: Loss = -11977.0986328125
Iteration 28000: Loss = -11977.0654296875
Iteration 28100: Loss = -11977.0595703125
Iteration 28200: Loss = -11973.876953125
Iteration 28300: Loss = -11970.771484375
Iteration 28400: Loss = -11964.1171875
Iteration 28500: Loss = -11963.9638671875
Iteration 28600: Loss = -11963.806640625
Iteration 28700: Loss = -11963.7080078125
Iteration 28800: Loss = -11963.4765625
Iteration 28900: Loss = -11962.9638671875
Iteration 29000: Loss = -11958.287109375
Iteration 29100: Loss = -11956.0078125
Iteration 29200: Loss = -11953.1884765625
Iteration 29300: Loss = -11945.376953125
Iteration 29400: Loss = -11941.412109375
Iteration 29500: Loss = -11928.26953125
Iteration 29600: Loss = -11927.93359375
Iteration 29700: Loss = -11925.2685546875
Iteration 29800: Loss = -11919.8525390625
Iteration 29900: Loss = -11906.490234375
pi: tensor([[0.6708, 0.3292],
        [0.3617, 0.6383]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4284, 0.5716], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2806, 0.1080],
         [0.9920, 0.2953]],

        [[0.9853, 0.0835],
         [0.0149, 0.9665]],

        [[0.3513, 0.0963],
         [0.1469, 0.0107]],

        [[0.3116, 0.1395],
         [0.9602, 0.7981]],

        [[0.0165, 0.1009],
         [0.9676, 0.6004]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 12
Adjusted Rand Index: 0.5734430082256169
time is 4
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.30331159823632386
Average Adjusted Rand Index: 0.9146886016451233
[-0.0022415863304625677, 0.30331159823632386] [0.0, 0.9146886016451233] [12281.369140625, 11903.1748046875]
-------------------------------------
This iteration is 13
True Objective function: Loss = -11913.675738954336
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25802.4453125
Iteration 100: Loss = -17240.119140625
Iteration 200: Loss = -13507.099609375
Iteration 300: Loss = -12690.9453125
Iteration 400: Loss = -12566.2841796875
Iteration 500: Loss = -12520.9638671875
Iteration 600: Loss = -12496.98828125
Iteration 700: Loss = -12482.0595703125
Iteration 800: Loss = -12472.041015625
Iteration 900: Loss = -12464.9052734375
Iteration 1000: Loss = -12459.6220703125
Iteration 1100: Loss = -12455.5732421875
Iteration 1200: Loss = -12452.3837890625
Iteration 1300: Loss = -12449.826171875
Iteration 1400: Loss = -12447.7412109375
Iteration 1500: Loss = -12446.0146484375
Iteration 1600: Loss = -12444.56640625
Iteration 1700: Loss = -12443.341796875
Iteration 1800: Loss = -12442.2939453125
Iteration 1900: Loss = -12441.3896484375
Iteration 2000: Loss = -12440.6044921875
Iteration 2100: Loss = -12439.9189453125
Iteration 2200: Loss = -12439.3154296875
Iteration 2300: Loss = -12438.7822265625
Iteration 2400: Loss = -12438.3056640625
Iteration 2500: Loss = -12437.8818359375
Iteration 2600: Loss = -12437.5
Iteration 2700: Loss = -12437.15625
Iteration 2800: Loss = -12436.8486328125
Iteration 2900: Loss = -12436.5673828125
Iteration 3000: Loss = -12436.310546875
Iteration 3100: Loss = -12436.078125
Iteration 3200: Loss = -12435.8681640625
Iteration 3300: Loss = -12435.6728515625
Iteration 3400: Loss = -12435.4892578125
Iteration 3500: Loss = -12435.32421875
Iteration 3600: Loss = -12435.171875
Iteration 3700: Loss = -12435.03125
Iteration 3800: Loss = -12434.9013671875
Iteration 3900: Loss = -12434.7802734375
Iteration 4000: Loss = -12434.6650390625
Iteration 4100: Loss = -12434.55859375
Iteration 4200: Loss = -12434.458984375
Iteration 4300: Loss = -12434.3642578125
Iteration 4400: Loss = -12434.2734375
Iteration 4500: Loss = -12434.1884765625
Iteration 4600: Loss = -12434.109375
Iteration 4700: Loss = -12434.0322265625
Iteration 4800: Loss = -12433.958984375
Iteration 4900: Loss = -12433.8896484375
Iteration 5000: Loss = -12433.8232421875
Iteration 5100: Loss = -12433.759765625
Iteration 5200: Loss = -12433.7001953125
Iteration 5300: Loss = -12433.642578125
Iteration 5400: Loss = -12433.5849609375
Iteration 5500: Loss = -12433.529296875
Iteration 5600: Loss = -12433.4736328125
Iteration 5700: Loss = -12433.4189453125
Iteration 5800: Loss = -12433.3623046875
Iteration 5900: Loss = -12433.3037109375
Iteration 6000: Loss = -12433.2470703125
Iteration 6100: Loss = -12433.19140625
Iteration 6200: Loss = -12433.1376953125
Iteration 6300: Loss = -12433.0849609375
Iteration 6400: Loss = -12433.0341796875
Iteration 6500: Loss = -12432.9853515625
Iteration 6600: Loss = -12432.93359375
Iteration 6700: Loss = -12432.890625
Iteration 6800: Loss = -12432.849609375
Iteration 6900: Loss = -12432.8115234375
Iteration 7000: Loss = -12432.7724609375
Iteration 7100: Loss = -12432.734375
Iteration 7200: Loss = -12432.6982421875
Iteration 7300: Loss = -12432.662109375
Iteration 7400: Loss = -12432.626953125
Iteration 7500: Loss = -12432.58984375
Iteration 7600: Loss = -12432.5517578125
Iteration 7700: Loss = -12432.5166015625
Iteration 7800: Loss = -12432.478515625
Iteration 7900: Loss = -12432.439453125
Iteration 8000: Loss = -12432.3984375
Iteration 8100: Loss = -12432.3564453125
Iteration 8200: Loss = -12432.3125
Iteration 8300: Loss = -12432.2705078125
Iteration 8400: Loss = -12432.224609375
Iteration 8500: Loss = -12432.1796875
Iteration 8600: Loss = -12432.1328125
Iteration 8700: Loss = -12432.0908203125
Iteration 8800: Loss = -12432.0478515625
Iteration 8900: Loss = -12432.0087890625
Iteration 9000: Loss = -12431.97265625
Iteration 9100: Loss = -12431.9404296875
Iteration 9200: Loss = -12431.9111328125
Iteration 9300: Loss = -12431.88671875
Iteration 9400: Loss = -12431.869140625
Iteration 9500: Loss = -12431.853515625
Iteration 9600: Loss = -12431.83984375
Iteration 9700: Loss = -12431.8291015625
Iteration 9800: Loss = -12431.8193359375
Iteration 9900: Loss = -12431.8134765625
Iteration 10000: Loss = -12431.806640625
Iteration 10100: Loss = -12431.8017578125
Iteration 10200: Loss = -12431.7958984375
Iteration 10300: Loss = -12431.7919921875
Iteration 10400: Loss = -12431.787109375
Iteration 10500: Loss = -12431.78515625
Iteration 10600: Loss = -12431.7802734375
Iteration 10700: Loss = -12431.7783203125
Iteration 10800: Loss = -12431.7763671875
Iteration 10900: Loss = -12431.7744140625
Iteration 11000: Loss = -12431.771484375
Iteration 11100: Loss = -12431.7705078125
Iteration 11200: Loss = -12431.767578125
Iteration 11300: Loss = -12431.7666015625
Iteration 11400: Loss = -12431.7646484375
Iteration 11500: Loss = -12431.7666015625
1
Iteration 11600: Loss = -12431.7626953125
Iteration 11700: Loss = -12431.76171875
Iteration 11800: Loss = -12431.7607421875
Iteration 11900: Loss = -12431.759765625
Iteration 12000: Loss = -12431.759765625
Iteration 12100: Loss = -12431.7587890625
Iteration 12200: Loss = -12431.759765625
1
Iteration 12300: Loss = -12431.7578125
Iteration 12400: Loss = -12431.755859375
Iteration 12500: Loss = -12431.755859375
Iteration 12600: Loss = -12431.755859375
Iteration 12700: Loss = -12431.755859375
Iteration 12800: Loss = -12431.75390625
Iteration 12900: Loss = -12431.7529296875
Iteration 13000: Loss = -12431.75390625
1
Iteration 13100: Loss = -12431.751953125
Iteration 13200: Loss = -12431.751953125
Iteration 13300: Loss = -12431.751953125
Iteration 13400: Loss = -12431.7509765625
Iteration 13500: Loss = -12431.751953125
1
Iteration 13600: Loss = -12431.7509765625
Iteration 13700: Loss = -12431.7490234375
Iteration 13800: Loss = -12431.75
1
Iteration 13900: Loss = -12431.75
2
Iteration 14000: Loss = -12431.75
3
Iteration 14100: Loss = -12431.7490234375
Iteration 14200: Loss = -12431.7490234375
Iteration 14300: Loss = -12431.7490234375
Iteration 14400: Loss = -12431.7490234375
Iteration 14500: Loss = -12431.7470703125
Iteration 14600: Loss = -12431.748046875
1
Iteration 14700: Loss = -12431.7470703125
Iteration 14800: Loss = -12431.748046875
1
Iteration 14900: Loss = -12431.7509765625
2
Iteration 15000: Loss = -12431.7470703125
Iteration 15100: Loss = -12431.748046875
1
Iteration 15200: Loss = -12431.74609375
Iteration 15300: Loss = -12431.748046875
1
Iteration 15400: Loss = -12431.74609375
Iteration 15500: Loss = -12431.7470703125
1
Iteration 15600: Loss = -12431.74609375
Iteration 15700: Loss = -12431.7451171875
Iteration 15800: Loss = -12431.74609375
1
Iteration 15900: Loss = -12431.74609375
2
Iteration 16000: Loss = -12431.74609375
3
Iteration 16100: Loss = -12431.7470703125
4
Iteration 16200: Loss = -12431.7451171875
Iteration 16300: Loss = -12431.7470703125
1
Iteration 16400: Loss = -12431.7470703125
2
Iteration 16500: Loss = -12431.744140625
Iteration 16600: Loss = -12431.7451171875
1
Iteration 16700: Loss = -12431.7451171875
2
Iteration 16800: Loss = -12431.7451171875
3
Iteration 16900: Loss = -12431.74609375
4
Iteration 17000: Loss = -12431.7451171875
5
Iteration 17100: Loss = -12431.748046875
6
Iteration 17200: Loss = -12431.74609375
7
Iteration 17300: Loss = -12431.7451171875
8
Iteration 17400: Loss = -12431.744140625
Iteration 17500: Loss = -12431.7451171875
1
Iteration 17600: Loss = -12431.744140625
Iteration 17700: Loss = -12431.7431640625
Iteration 17800: Loss = -12431.74609375
1
Iteration 17900: Loss = -12431.74609375
2
Iteration 18000: Loss = -12431.74609375
3
Iteration 18100: Loss = -12431.744140625
4
Iteration 18200: Loss = -12431.7451171875
5
Iteration 18300: Loss = -12431.74609375
6
Iteration 18400: Loss = -12431.7451171875
7
Iteration 18500: Loss = -12431.744140625
8
Iteration 18600: Loss = -12431.744140625
9
Iteration 18700: Loss = -12431.7431640625
Iteration 18800: Loss = -12431.7373046875
Iteration 18900: Loss = -12430.2666015625
Iteration 19000: Loss = -12430.236328125
Iteration 19100: Loss = -12429.009765625
Iteration 19200: Loss = -12427.6591796875
Iteration 19300: Loss = -12427.6357421875
Iteration 19400: Loss = -12427.626953125
Iteration 19500: Loss = -12427.626953125
Iteration 19600: Loss = -12427.626953125
Iteration 19700: Loss = -12427.625
Iteration 19800: Loss = -12427.625
Iteration 19900: Loss = -12427.6259765625
1
Iteration 20000: Loss = -12427.6240234375
Iteration 20100: Loss = -12427.625
1
Iteration 20200: Loss = -12427.625
2
Iteration 20300: Loss = -12427.625
3
Iteration 20400: Loss = -12427.625
4
Iteration 20500: Loss = -12427.6259765625
5
Iteration 20600: Loss = -12427.625
6
Iteration 20700: Loss = -12427.625
7
Iteration 20800: Loss = -12427.625
8
Iteration 20900: Loss = -12427.625
9
Iteration 21000: Loss = -12427.6240234375
Iteration 21100: Loss = -12427.6259765625
1
Iteration 21200: Loss = -12427.6259765625
2
Iteration 21300: Loss = -12427.6240234375
Iteration 21400: Loss = -12427.6240234375
Iteration 21500: Loss = -12427.6259765625
1
Iteration 21600: Loss = -12427.6259765625
2
Iteration 21700: Loss = -12427.6259765625
3
Iteration 21800: Loss = -12427.6240234375
Iteration 21900: Loss = -12427.625
1
Iteration 22000: Loss = -12427.625
2
Iteration 22100: Loss = -12427.6259765625
3
Iteration 22200: Loss = -12427.625
4
Iteration 22300: Loss = -12427.625
5
Iteration 22400: Loss = -12427.625
6
Iteration 22500: Loss = -12427.625
7
Iteration 22600: Loss = -12427.625
8
Iteration 22700: Loss = -12427.6240234375
Iteration 22800: Loss = -12427.626953125
1
Iteration 22900: Loss = -12427.6240234375
Iteration 23000: Loss = -12427.6259765625
1
Iteration 23100: Loss = -12427.6240234375
Iteration 23200: Loss = -12427.626953125
1
Iteration 23300: Loss = -12427.625
2
Iteration 23400: Loss = -12427.625
3
Iteration 23500: Loss = -12427.6259765625
4
Iteration 23600: Loss = -12427.6259765625
5
Iteration 23700: Loss = -12427.625
6
Iteration 23800: Loss = -12427.6259765625
7
Iteration 23900: Loss = -12427.626953125
8
Iteration 24000: Loss = -12427.625
9
Iteration 24100: Loss = -12427.625
10
Iteration 24200: Loss = -12427.626953125
11
Iteration 24300: Loss = -12427.6259765625
12
Iteration 24400: Loss = -12427.6259765625
13
Iteration 24500: Loss = -12427.625
14
Iteration 24600: Loss = -12427.6240234375
Iteration 24700: Loss = -12427.6259765625
1
Iteration 24800: Loss = -12427.6259765625
2
Iteration 24900: Loss = -12427.6240234375
Iteration 25000: Loss = -12427.6240234375
Iteration 25100: Loss = -12427.6259765625
1
Iteration 25200: Loss = -12427.6259765625
2
Iteration 25300: Loss = -12427.6259765625
3
Iteration 25400: Loss = -12427.625
4
Iteration 25500: Loss = -12427.625
5
Iteration 25600: Loss = -12427.6240234375
Iteration 25700: Loss = -12427.6259765625
1
Iteration 25800: Loss = -12427.6240234375
Iteration 25900: Loss = -12427.6259765625
1
Iteration 26000: Loss = -12427.6259765625
2
Iteration 26100: Loss = -12427.625
3
Iteration 26200: Loss = -12427.625
4
Iteration 26300: Loss = -12427.6259765625
5
Iteration 26400: Loss = -12427.626953125
6
Iteration 26500: Loss = -12427.625
7
Iteration 26600: Loss = -12427.625
8
Iteration 26700: Loss = -12427.6240234375
Iteration 26800: Loss = -12427.625
1
Iteration 26900: Loss = -12427.625
2
Iteration 27000: Loss = -12427.625
3
Iteration 27100: Loss = -12427.625
4
Iteration 27200: Loss = -12427.6240234375
Iteration 27300: Loss = -12427.6259765625
1
Iteration 27400: Loss = -12427.6259765625
2
Iteration 27500: Loss = -12427.6259765625
3
Iteration 27600: Loss = -12427.6240234375
Iteration 27700: Loss = -12427.6259765625
1
Iteration 27800: Loss = -12427.625
2
Iteration 27900: Loss = -12427.6240234375
Iteration 28000: Loss = -12427.625
1
Iteration 28100: Loss = -12427.6259765625
2
Iteration 28200: Loss = -12427.625
3
Iteration 28300: Loss = -12427.625
4
Iteration 28400: Loss = -12427.625
5
Iteration 28500: Loss = -12427.6259765625
6
Iteration 28600: Loss = -12427.6240234375
Iteration 28700: Loss = -12427.6240234375
Iteration 28800: Loss = -12427.625
1
Iteration 28900: Loss = -12427.625
2
Iteration 29000: Loss = -12427.6240234375
Iteration 29100: Loss = -12427.625
1
Iteration 29200: Loss = -12427.625
2
Iteration 29300: Loss = -12427.625
3
Iteration 29400: Loss = -12427.625
4
Iteration 29500: Loss = -12427.6240234375
Iteration 29600: Loss = -12427.6259765625
1
Iteration 29700: Loss = -12427.626953125
2
Iteration 29800: Loss = -12427.625
3
Iteration 29900: Loss = -12427.6259765625
4
pi: tensor([[6.1393e-06, 9.9999e-01],
        [5.8173e-03, 9.9418e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0599, 0.9401], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.6863, 0.1093],
         [0.9713, 0.2031]],

        [[0.9912, 0.2117],
         [0.7483, 0.8770]],

        [[0.0153, 0.2688],
         [0.3043, 0.8372]],

        [[0.5921, 0.2850],
         [0.8806, 0.9259]],

        [[0.9738, 0.1114],
         [0.8442, 0.9805]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 35
Adjusted Rand Index: 0.06446406470332265
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.008287845424679436
Average Adjusted Rand Index: 0.01289281294066453
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -41744.6796875
Iteration 100: Loss = -25255.4140625
Iteration 200: Loss = -15416.2470703125
Iteration 300: Loss = -12991.58203125
Iteration 400: Loss = -12655.6015625
Iteration 500: Loss = -12568.8935546875
Iteration 600: Loss = -12524.142578125
Iteration 700: Loss = -12503.51953125
Iteration 800: Loss = -12486.8125
Iteration 900: Loss = -12477.3759765625
Iteration 1000: Loss = -12470.787109375
Iteration 1100: Loss = -12465.7587890625
Iteration 1200: Loss = -12461.779296875
Iteration 1300: Loss = -12458.5185546875
Iteration 1400: Loss = -12455.685546875
Iteration 1500: Loss = -12452.580078125
Iteration 1600: Loss = -12447.953125
Iteration 1700: Loss = -12445.6044921875
Iteration 1800: Loss = -12444.1669921875
Iteration 1900: Loss = -12443.0224609375
Iteration 2000: Loss = -12442.0546875
Iteration 2100: Loss = -12441.2216796875
Iteration 2200: Loss = -12440.4951171875
Iteration 2300: Loss = -12439.85546875
Iteration 2400: Loss = -12439.2900390625
Iteration 2500: Loss = -12438.7900390625
Iteration 2600: Loss = -12438.34375
Iteration 2700: Loss = -12437.9443359375
Iteration 2800: Loss = -12437.5849609375
Iteration 2900: Loss = -12437.2626953125
Iteration 3000: Loss = -12436.9736328125
Iteration 3100: Loss = -12436.70703125
Iteration 3200: Loss = -12436.46875
Iteration 3300: Loss = -12436.25390625
Iteration 3400: Loss = -12436.0546875
Iteration 3500: Loss = -12435.8759765625
Iteration 3600: Loss = -12435.7119140625
Iteration 3700: Loss = -12435.5595703125
Iteration 3800: Loss = -12435.419921875
Iteration 3900: Loss = -12435.29296875
Iteration 4000: Loss = -12435.1748046875
Iteration 4100: Loss = -12435.0654296875
Iteration 4200: Loss = -12434.9638671875
Iteration 4300: Loss = -12434.8701171875
Iteration 4400: Loss = -12434.7822265625
Iteration 4500: Loss = -12434.7001953125
Iteration 4600: Loss = -12434.6220703125
Iteration 4700: Loss = -12434.5517578125
Iteration 4800: Loss = -12434.484375
Iteration 4900: Loss = -12434.4228515625
Iteration 5000: Loss = -12434.3662109375
Iteration 5100: Loss = -12434.30859375
Iteration 5200: Loss = -12434.2578125
Iteration 5300: Loss = -12434.2080078125
Iteration 5400: Loss = -12434.1630859375
Iteration 5500: Loss = -12434.119140625
Iteration 5600: Loss = -12434.0791015625
Iteration 5700: Loss = -12434.0390625
Iteration 5800: Loss = -12434.0029296875
Iteration 5900: Loss = -12433.96875
Iteration 6000: Loss = -12433.935546875
Iteration 6100: Loss = -12433.9052734375
Iteration 6200: Loss = -12433.8759765625
Iteration 6300: Loss = -12433.8486328125
Iteration 6400: Loss = -12433.8212890625
Iteration 6500: Loss = -12433.796875
Iteration 6600: Loss = -12433.7734375
Iteration 6700: Loss = -12433.75
Iteration 6800: Loss = -12433.73046875
Iteration 6900: Loss = -12433.7099609375
Iteration 7000: Loss = -12433.689453125
Iteration 7100: Loss = -12433.671875
Iteration 7200: Loss = -12433.6552734375
Iteration 7300: Loss = -12433.638671875
Iteration 7400: Loss = -12433.6220703125
Iteration 7500: Loss = -12433.6083984375
Iteration 7600: Loss = -12433.59375
Iteration 7700: Loss = -12433.580078125
Iteration 7800: Loss = -12433.56640625
Iteration 7900: Loss = -12433.5556640625
Iteration 8000: Loss = -12433.5419921875
Iteration 8100: Loss = -12433.5322265625
Iteration 8200: Loss = -12433.521484375
Iteration 8300: Loss = -12433.5107421875
Iteration 8400: Loss = -12433.5009765625
Iteration 8500: Loss = -12433.4921875
Iteration 8600: Loss = -12433.482421875
Iteration 8700: Loss = -12433.4736328125
Iteration 8800: Loss = -12433.4658203125
Iteration 8900: Loss = -12433.458984375
Iteration 9000: Loss = -12433.4501953125
Iteration 9100: Loss = -12433.4423828125
Iteration 9200: Loss = -12433.435546875
Iteration 9300: Loss = -12433.4296875
Iteration 9400: Loss = -12433.4228515625
Iteration 9500: Loss = -12433.4150390625
Iteration 9600: Loss = -12433.4091796875
Iteration 9700: Loss = -12433.4033203125
Iteration 9800: Loss = -12433.396484375
Iteration 9900: Loss = -12433.392578125
Iteration 10000: Loss = -12433.3876953125
Iteration 10100: Loss = -12433.3798828125
Iteration 10200: Loss = -12433.3759765625
Iteration 10300: Loss = -12433.3701171875
Iteration 10400: Loss = -12433.3623046875
Iteration 10500: Loss = -12433.3564453125
Iteration 10600: Loss = -12433.349609375
Iteration 10700: Loss = -12433.341796875
Iteration 10800: Loss = -12433.333984375
Iteration 10900: Loss = -12433.326171875
Iteration 11000: Loss = -12433.3115234375
Iteration 11100: Loss = -12433.26953125
Iteration 11200: Loss = -12432.3134765625
Iteration 11300: Loss = -12431.9345703125
Iteration 11400: Loss = -12430.158203125
Iteration 11500: Loss = -12429.75390625
Iteration 11600: Loss = -12428.361328125
Iteration 11700: Loss = -12428.0517578125
Iteration 11800: Loss = -12427.9599609375
Iteration 11900: Loss = -12427.921875
Iteration 12000: Loss = -12427.892578125
Iteration 12100: Loss = -12427.865234375
Iteration 12200: Loss = -12427.8466796875
Iteration 12300: Loss = -12427.8310546875
Iteration 12400: Loss = -12427.81640625
Iteration 12500: Loss = -12427.8017578125
Iteration 12600: Loss = -12427.7900390625
Iteration 12700: Loss = -12427.7802734375
Iteration 12800: Loss = -12427.7744140625
Iteration 12900: Loss = -12427.767578125
Iteration 13000: Loss = -12427.759765625
Iteration 13100: Loss = -12427.755859375
Iteration 13200: Loss = -12427.74609375
Iteration 13300: Loss = -12427.7392578125
Iteration 13400: Loss = -12427.732421875
Iteration 13500: Loss = -12427.7275390625
Iteration 13600: Loss = -12427.72265625
Iteration 13700: Loss = -12427.7177734375
Iteration 13800: Loss = -12427.7099609375
Iteration 13900: Loss = -12427.7060546875
Iteration 14000: Loss = -12427.7001953125
Iteration 14100: Loss = -12427.6982421875
Iteration 14200: Loss = -12427.6943359375
Iteration 14300: Loss = -12427.6904296875
Iteration 14400: Loss = -12427.6845703125
Iteration 14500: Loss = -12427.6826171875
Iteration 14600: Loss = -12427.6787109375
Iteration 14700: Loss = -12427.677734375
Iteration 14800: Loss = -12427.6748046875
Iteration 14900: Loss = -12427.673828125
Iteration 15000: Loss = -12427.6728515625
Iteration 15100: Loss = -12427.6728515625
Iteration 15200: Loss = -12427.671875
Iteration 15300: Loss = -12427.6708984375
Iteration 15400: Loss = -12427.669921875
Iteration 15500: Loss = -12427.669921875
Iteration 15600: Loss = -12427.66796875
Iteration 15700: Loss = -12427.6689453125
1
Iteration 15800: Loss = -12427.6669921875
Iteration 15900: Loss = -12427.669921875
1
Iteration 16000: Loss = -12427.66796875
2
Iteration 16100: Loss = -12427.66796875
3
Iteration 16200: Loss = -12427.666015625
Iteration 16300: Loss = -12427.666015625
Iteration 16400: Loss = -12427.666015625
Iteration 16500: Loss = -12427.6669921875
1
Iteration 16600: Loss = -12427.6669921875
2
Iteration 16700: Loss = -12427.6650390625
Iteration 16800: Loss = -12427.6630859375
Iteration 16900: Loss = -12427.6611328125
Iteration 17000: Loss = -12427.6611328125
Iteration 17100: Loss = -12427.6611328125
Iteration 17200: Loss = -12427.66015625
Iteration 17300: Loss = -12427.66015625
Iteration 17400: Loss = -12427.6611328125
1
Iteration 17500: Loss = -12427.66015625
Iteration 17600: Loss = -12427.6611328125
1
Iteration 17700: Loss = -12427.66015625
Iteration 17800: Loss = -12427.6611328125
1
Iteration 17900: Loss = -12427.6611328125
2
Iteration 18000: Loss = -12427.658203125
Iteration 18100: Loss = -12427.6572265625
Iteration 18200: Loss = -12427.658203125
1
Iteration 18300: Loss = -12427.6572265625
Iteration 18400: Loss = -12427.658203125
1
Iteration 18500: Loss = -12427.6572265625
Iteration 18600: Loss = -12427.658203125
1
Iteration 18700: Loss = -12427.6572265625
Iteration 18800: Loss = -12427.658203125
1
Iteration 18900: Loss = -12427.6572265625
Iteration 19000: Loss = -12427.658203125
1
Iteration 19100: Loss = -12427.658203125
2
Iteration 19200: Loss = -12427.658203125
3
Iteration 19300: Loss = -12427.65625
Iteration 19400: Loss = -12427.658203125
1
Iteration 19500: Loss = -12427.6572265625
2
Iteration 19600: Loss = -12427.6572265625
3
Iteration 19700: Loss = -12427.6572265625
4
Iteration 19800: Loss = -12427.658203125
5
Iteration 19900: Loss = -12427.658203125
6
Iteration 20000: Loss = -12427.658203125
7
Iteration 20100: Loss = -12427.65625
Iteration 20200: Loss = -12427.658203125
1
Iteration 20300: Loss = -12427.65625
Iteration 20400: Loss = -12427.65625
Iteration 20500: Loss = -12427.66015625
1
Iteration 20600: Loss = -12427.658203125
2
Iteration 20700: Loss = -12427.65625
Iteration 20800: Loss = -12427.654296875
Iteration 20900: Loss = -12427.65625
1
Iteration 21000: Loss = -12427.6572265625
2
Iteration 21100: Loss = -12427.65625
3
Iteration 21200: Loss = -12427.6552734375
4
Iteration 21300: Loss = -12427.6513671875
Iteration 21400: Loss = -12427.6533203125
1
Iteration 21500: Loss = -12427.65234375
2
Iteration 21600: Loss = -12427.6533203125
3
Iteration 21700: Loss = -12427.6494140625
Iteration 21800: Loss = -12427.6494140625
Iteration 21900: Loss = -12427.6494140625
Iteration 22000: Loss = -12427.6484375
Iteration 22100: Loss = -12427.6484375
Iteration 22200: Loss = -12427.6484375
Iteration 22300: Loss = -12427.6484375
Iteration 22400: Loss = -12427.6494140625
1
Iteration 22500: Loss = -12427.6484375
Iteration 22600: Loss = -12427.6484375
Iteration 22700: Loss = -12427.6484375
Iteration 22800: Loss = -12427.6484375
Iteration 22900: Loss = -12427.6484375
Iteration 23000: Loss = -12427.6484375
Iteration 23100: Loss = -12427.6474609375
Iteration 23200: Loss = -12427.6455078125
Iteration 23300: Loss = -12427.646484375
1
Iteration 23400: Loss = -12427.646484375
2
Iteration 23500: Loss = -12427.6435546875
Iteration 23600: Loss = -12427.64453125
1
Iteration 23700: Loss = -12427.6416015625
Iteration 23800: Loss = -12427.640625
Iteration 23900: Loss = -12427.6396484375
Iteration 24000: Loss = -12427.6357421875
Iteration 24100: Loss = -12427.6357421875
Iteration 24200: Loss = -12427.63671875
1
Iteration 24300: Loss = -12427.634765625
Iteration 24400: Loss = -12427.6357421875
1
Iteration 24500: Loss = -12427.6357421875
2
Iteration 24600: Loss = -12427.6357421875
3
Iteration 24700: Loss = -12427.634765625
Iteration 24800: Loss = -12427.63671875
1
Iteration 24900: Loss = -12427.634765625
Iteration 25000: Loss = -12427.6357421875
1
Iteration 25100: Loss = -12427.6357421875
2
Iteration 25200: Loss = -12427.6357421875
3
Iteration 25300: Loss = -12427.634765625
Iteration 25400: Loss = -12427.6357421875
1
Iteration 25500: Loss = -12427.63671875
2
Iteration 25600: Loss = -12427.6357421875
3
Iteration 25700: Loss = -12427.63671875
4
Iteration 25800: Loss = -12427.63671875
5
Iteration 25900: Loss = -12427.6357421875
6
Iteration 26000: Loss = -12427.63671875
7
Iteration 26100: Loss = -12427.6357421875
8
Iteration 26200: Loss = -12427.6357421875
9
Iteration 26300: Loss = -12427.63671875
10
Iteration 26400: Loss = -12427.6357421875
11
Iteration 26500: Loss = -12427.6357421875
12
Iteration 26600: Loss = -12427.6357421875
13
Iteration 26700: Loss = -12427.63671875
14
Iteration 26800: Loss = -12427.634765625
Iteration 26900: Loss = -12427.6357421875
1
Iteration 27000: Loss = -12427.6357421875
2
Iteration 27100: Loss = -12427.63671875
3
Iteration 27200: Loss = -12427.6357421875
4
Iteration 27300: Loss = -12427.634765625
Iteration 27400: Loss = -12427.634765625
Iteration 27500: Loss = -12427.63671875
1
Iteration 27600: Loss = -12427.6357421875
2
Iteration 27700: Loss = -12427.6357421875
3
Iteration 27800: Loss = -12427.634765625
Iteration 27900: Loss = -12427.6357421875
1
Iteration 28000: Loss = -12427.634765625
Iteration 28100: Loss = -12427.6376953125
1
Iteration 28200: Loss = -12427.6357421875
2
Iteration 28300: Loss = -12427.6337890625
Iteration 28400: Loss = -12427.6357421875
1
Iteration 28500: Loss = -12427.6357421875
2
Iteration 28600: Loss = -12427.6357421875
3
Iteration 28700: Loss = -12427.63671875
4
Iteration 28800: Loss = -12427.634765625
5
Iteration 28900: Loss = -12427.634765625
6
Iteration 29000: Loss = -12427.63671875
7
Iteration 29100: Loss = -12427.63671875
8
Iteration 29200: Loss = -12427.6357421875
9
Iteration 29300: Loss = -12427.6337890625
Iteration 29400: Loss = -12427.63671875
1
Iteration 29500: Loss = -12427.63671875
2
Iteration 29600: Loss = -12427.6357421875
3
Iteration 29700: Loss = -12427.6337890625
Iteration 29800: Loss = -12427.6357421875
1
Iteration 29900: Loss = -12427.6337890625
pi: tensor([[9.9433e-01, 5.6696e-03],
        [9.9999e-01, 6.8536e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9401, 0.0599], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2031, 0.1092],
         [0.1010, 0.6874]],

        [[0.9917, 0.2093],
         [0.9658, 0.0644]],

        [[0.2017, 0.2687],
         [0.8960, 0.0138]],

        [[0.0108, 0.2852],
         [0.0137, 0.7913]],

        [[0.5623, 0.1113],
         [0.0483, 0.1165]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 65
Adjusted Rand Index: 0.06446406470332265
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 61
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.008287845424679436
Average Adjusted Rand Index: 0.01289281294066453
[0.008287845424679436, 0.008287845424679436] [0.01289281294066453, 0.01289281294066453] [12427.625, 12427.634765625]
-------------------------------------
This iteration is 14
True Objective function: Loss = -11881.874937180504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -75792.4765625
Iteration 100: Loss = -55571.26953125
Iteration 200: Loss = -34522.4609375
Iteration 300: Loss = -20746.6484375
Iteration 400: Loss = -15351.6416015625
Iteration 500: Loss = -13443.6962890625
Iteration 600: Loss = -12806.7119140625
Iteration 700: Loss = -12600.3330078125
Iteration 800: Loss = -12532.5048828125
Iteration 900: Loss = -12505.134765625
Iteration 1000: Loss = -12488.408203125
Iteration 1100: Loss = -12477.6240234375
Iteration 1200: Loss = -12467.8916015625
Iteration 1300: Loss = -12458.31640625
Iteration 1400: Loss = -12451.484375
Iteration 1500: Loss = -12441.5908203125
Iteration 1600: Loss = -12436.970703125
Iteration 1700: Loss = -12433.341796875
Iteration 1800: Loss = -12430.546875
Iteration 1900: Loss = -12428.3212890625
Iteration 2000: Loss = -12426.478515625
Iteration 2100: Loss = -12424.9169921875
Iteration 2200: Loss = -12423.572265625
Iteration 2300: Loss = -12422.3974609375
Iteration 2400: Loss = -12421.3642578125
Iteration 2500: Loss = -12420.447265625
Iteration 2600: Loss = -12419.626953125
Iteration 2700: Loss = -12418.8955078125
Iteration 2800: Loss = -12418.232421875
Iteration 2900: Loss = -12417.6357421875
Iteration 3000: Loss = -12417.091796875
Iteration 3100: Loss = -12416.595703125
Iteration 3200: Loss = -12416.14453125
Iteration 3300: Loss = -12415.73046875
Iteration 3400: Loss = -12415.349609375
Iteration 3500: Loss = -12415.001953125
Iteration 3600: Loss = -12414.6787109375
Iteration 3700: Loss = -12414.380859375
Iteration 3800: Loss = -12414.1064453125
Iteration 3900: Loss = -12413.849609375
Iteration 4000: Loss = -12413.6123046875
Iteration 4100: Loss = -12413.3935546875
Iteration 4200: Loss = -12413.1884765625
Iteration 4300: Loss = -12412.9970703125
Iteration 4400: Loss = -12412.8173828125
Iteration 4500: Loss = -12412.65234375
Iteration 4600: Loss = -12412.4970703125
Iteration 4700: Loss = -12412.349609375
Iteration 4800: Loss = -12412.212890625
Iteration 4900: Loss = -12412.0830078125
Iteration 5000: Loss = -12411.962890625
Iteration 5100: Loss = -12411.8505859375
Iteration 5200: Loss = -12411.7451171875
Iteration 5300: Loss = -12411.64453125
Iteration 5400: Loss = -12411.55078125
Iteration 5500: Loss = -12411.4619140625
Iteration 5600: Loss = -12411.376953125
Iteration 5700: Loss = -12411.2978515625
Iteration 5800: Loss = -12411.2255859375
Iteration 5900: Loss = -12411.1552734375
Iteration 6000: Loss = -12411.0869140625
Iteration 6100: Loss = -12411.0244140625
Iteration 6200: Loss = -12410.9658203125
Iteration 6300: Loss = -12410.91015625
Iteration 6400: Loss = -12410.8564453125
Iteration 6500: Loss = -12410.80859375
Iteration 6600: Loss = -12410.759765625
Iteration 6700: Loss = -12410.7158203125
Iteration 6800: Loss = -12410.6708984375
Iteration 6900: Loss = -12410.6318359375
Iteration 7000: Loss = -12410.59375
Iteration 7100: Loss = -12410.5576171875
Iteration 7200: Loss = -12410.5244140625
Iteration 7300: Loss = -12410.4912109375
Iteration 7400: Loss = -12410.4609375
Iteration 7500: Loss = -12410.4326171875
Iteration 7600: Loss = -12410.404296875
Iteration 7700: Loss = -12410.3779296875
Iteration 7800: Loss = -12410.353515625
Iteration 7900: Loss = -12410.3291015625
Iteration 8000: Loss = -12410.306640625
Iteration 8100: Loss = -12410.283203125
Iteration 8200: Loss = -12410.2646484375
Iteration 8300: Loss = -12410.244140625
Iteration 8400: Loss = -12410.2255859375
Iteration 8500: Loss = -12410.2119140625
Iteration 8600: Loss = -12410.1943359375
Iteration 8700: Loss = -12410.177734375
Iteration 8800: Loss = -12410.1640625
Iteration 8900: Loss = -12410.150390625
Iteration 9000: Loss = -12410.13671875
Iteration 9100: Loss = -12410.123046875
Iteration 9200: Loss = -12410.109375
Iteration 9300: Loss = -12410.0986328125
Iteration 9400: Loss = -12410.0869140625
Iteration 9500: Loss = -12410.078125
Iteration 9600: Loss = -12410.0673828125
Iteration 9700: Loss = -12410.056640625
Iteration 9800: Loss = -12410.048828125
Iteration 9900: Loss = -12410.0400390625
Iteration 10000: Loss = -12410.0322265625
Iteration 10100: Loss = -12410.0224609375
Iteration 10200: Loss = -12410.015625
Iteration 10300: Loss = -12410.0078125
Iteration 10400: Loss = -12410.001953125
Iteration 10500: Loss = -12409.9951171875
Iteration 10600: Loss = -12409.9892578125
Iteration 10700: Loss = -12409.9833984375
Iteration 10800: Loss = -12409.978515625
Iteration 10900: Loss = -12409.9736328125
Iteration 11000: Loss = -12409.9677734375
Iteration 11100: Loss = -12409.962890625
Iteration 11200: Loss = -12409.9599609375
Iteration 11300: Loss = -12409.955078125
Iteration 11400: Loss = -12409.951171875
Iteration 11500: Loss = -12409.9482421875
Iteration 11600: Loss = -12409.943359375
Iteration 11700: Loss = -12409.9404296875
Iteration 11800: Loss = -12409.9365234375
Iteration 11900: Loss = -12409.93359375
Iteration 12000: Loss = -12409.9306640625
Iteration 12100: Loss = -12409.9267578125
Iteration 12200: Loss = -12409.9248046875
Iteration 12300: Loss = -12409.9228515625
Iteration 12400: Loss = -12409.919921875
Iteration 12500: Loss = -12409.9169921875
Iteration 12600: Loss = -12409.9169921875
Iteration 12700: Loss = -12409.912109375
Iteration 12800: Loss = -12409.9111328125
Iteration 12900: Loss = -12409.91015625
Iteration 13000: Loss = -12409.9072265625
Iteration 13100: Loss = -12409.90625
Iteration 13200: Loss = -12409.9052734375
Iteration 13300: Loss = -12409.90234375
Iteration 13400: Loss = -12409.90234375
Iteration 13500: Loss = -12409.900390625
Iteration 13600: Loss = -12409.8974609375
Iteration 13700: Loss = -12409.8955078125
Iteration 13800: Loss = -12409.8955078125
Iteration 13900: Loss = -12409.8955078125
Iteration 14000: Loss = -12409.892578125
Iteration 14100: Loss = -12409.892578125
Iteration 14200: Loss = -12409.8916015625
Iteration 14300: Loss = -12409.890625
Iteration 14400: Loss = -12409.8896484375
Iteration 14500: Loss = -12409.888671875
Iteration 14600: Loss = -12409.88671875
Iteration 14700: Loss = -12409.88671875
Iteration 14800: Loss = -12409.88671875
Iteration 14900: Loss = -12409.8857421875
Iteration 15000: Loss = -12409.884765625
Iteration 15100: Loss = -12409.8837890625
Iteration 15200: Loss = -12409.884765625
1
Iteration 15300: Loss = -12409.8837890625
Iteration 15400: Loss = -12409.8828125
Iteration 15500: Loss = -12409.880859375
Iteration 15600: Loss = -12409.8818359375
1
Iteration 15700: Loss = -12409.8798828125
Iteration 15800: Loss = -12409.880859375
1
Iteration 15900: Loss = -12409.8798828125
Iteration 16000: Loss = -12409.87890625
Iteration 16100: Loss = -12409.8798828125
1
Iteration 16200: Loss = -12409.880859375
2
Iteration 16300: Loss = -12409.87890625
Iteration 16400: Loss = -12409.8779296875
Iteration 16500: Loss = -12409.8779296875
Iteration 16600: Loss = -12409.8779296875
Iteration 16700: Loss = -12409.876953125
Iteration 16800: Loss = -12409.8759765625
Iteration 16900: Loss = -12409.8759765625
Iteration 17000: Loss = -12409.8759765625
Iteration 17100: Loss = -12409.8740234375
Iteration 17200: Loss = -12409.8740234375
Iteration 17300: Loss = -12409.8740234375
Iteration 17400: Loss = -12409.8740234375
Iteration 17500: Loss = -12409.8740234375
Iteration 17600: Loss = -12409.8740234375
Iteration 17700: Loss = -12409.8740234375
Iteration 17800: Loss = -12409.875
1
Iteration 17900: Loss = -12409.873046875
Iteration 18000: Loss = -12409.8740234375
1
Iteration 18100: Loss = -12409.875
2
Iteration 18200: Loss = -12409.8740234375
3
Iteration 18300: Loss = -12409.873046875
Iteration 18400: Loss = -12409.8720703125
Iteration 18500: Loss = -12409.8720703125
Iteration 18600: Loss = -12409.873046875
1
Iteration 18700: Loss = -12409.873046875
2
Iteration 18800: Loss = -12409.8740234375
3
Iteration 18900: Loss = -12409.8720703125
Iteration 19000: Loss = -12409.8720703125
Iteration 19100: Loss = -12409.8720703125
Iteration 19200: Loss = -12409.873046875
1
Iteration 19300: Loss = -12409.8720703125
Iteration 19400: Loss = -12409.873046875
1
Iteration 19500: Loss = -12409.8720703125
Iteration 19600: Loss = -12409.8720703125
Iteration 19700: Loss = -12409.8740234375
1
Iteration 19800: Loss = -12409.8720703125
Iteration 19900: Loss = -12409.8720703125
Iteration 20000: Loss = -12409.87109375
Iteration 20100: Loss = -12409.8720703125
1
Iteration 20200: Loss = -12409.8720703125
2
Iteration 20300: Loss = -12409.8720703125
3
Iteration 20400: Loss = -12409.8720703125
4
Iteration 20500: Loss = -12409.87109375
Iteration 20600: Loss = -12409.8720703125
1
Iteration 20700: Loss = -12409.87109375
Iteration 20800: Loss = -12409.87109375
Iteration 20900: Loss = -12409.8720703125
1
Iteration 21000: Loss = -12409.87109375
Iteration 21100: Loss = -12409.8720703125
1
Iteration 21200: Loss = -12409.876953125
2
Iteration 21300: Loss = -12409.8701171875
Iteration 21400: Loss = -12409.87109375
1
Iteration 21500: Loss = -12409.873046875
2
Iteration 21600: Loss = -12409.8720703125
3
Iteration 21700: Loss = -12409.87109375
4
Iteration 21800: Loss = -12409.87109375
5
Iteration 21900: Loss = -12409.873046875
6
Iteration 22000: Loss = -12409.8701171875
Iteration 22100: Loss = -12409.8720703125
1
Iteration 22200: Loss = -12409.87109375
2
Iteration 22300: Loss = -12409.873046875
3
Iteration 22400: Loss = -12409.8720703125
4
Iteration 22500: Loss = -12409.8701171875
Iteration 22600: Loss = -12409.87109375
1
Iteration 22700: Loss = -12409.873046875
2
Iteration 22800: Loss = -12409.873046875
3
Iteration 22900: Loss = -12409.8720703125
4
Iteration 23000: Loss = -12409.8701171875
Iteration 23100: Loss = -12409.8740234375
1
Iteration 23200: Loss = -12409.873046875
2
Iteration 23300: Loss = -12409.87109375
3
Iteration 23400: Loss = -12409.8720703125
4
Iteration 23500: Loss = -12409.873046875
5
Iteration 23600: Loss = -12409.8701171875
Iteration 23700: Loss = -12409.8720703125
1
Iteration 23800: Loss = -12409.87109375
2
Iteration 23900: Loss = -12409.87109375
3
Iteration 24000: Loss = -12409.8720703125
4
Iteration 24100: Loss = -12409.8720703125
5
Iteration 24200: Loss = -12409.8720703125
6
Iteration 24300: Loss = -12409.87109375
7
Iteration 24400: Loss = -12409.8720703125
8
Iteration 24500: Loss = -12409.873046875
9
Iteration 24600: Loss = -12409.87109375
10
Iteration 24700: Loss = -12409.8720703125
11
Iteration 24800: Loss = -12409.8720703125
12
Iteration 24900: Loss = -12409.8740234375
13
Iteration 25000: Loss = -12409.8720703125
14
Iteration 25100: Loss = -12409.873046875
15
Stopping early at iteration 25100 due to no improvement.
pi: tensor([[1.0000e+00, 1.9714e-06],
        [9.8980e-01, 1.0204e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 5.9637e-08], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2007, 0.3412],
         [0.9611, 0.9502]],

        [[0.9601, 0.1993],
         [0.0870, 0.9906]],

        [[0.0578, 0.2105],
         [0.4638, 0.9912]],

        [[0.4988, 0.7345],
         [0.9393, 0.1983]],

        [[0.0140, 0.7457],
         [0.4672, 0.9701]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -40009.15625
Iteration 100: Loss = -22232.94921875
Iteration 200: Loss = -14548.75
Iteration 300: Loss = -13038.68359375
Iteration 400: Loss = -12731.365234375
Iteration 500: Loss = -12621.7900390625
Iteration 600: Loss = -12551.6328125
Iteration 700: Loss = -12503.43359375
Iteration 800: Loss = -12477.2001953125
Iteration 900: Loss = -12464.701171875
Iteration 1000: Loss = -12454.431640625
Iteration 1100: Loss = -12444.228515625
Iteration 1200: Loss = -12439.15234375
Iteration 1300: Loss = -12435.146484375
Iteration 1400: Loss = -12431.9150390625
Iteration 1500: Loss = -12429.2998046875
Iteration 1600: Loss = -12427.1240234375
Iteration 1700: Loss = -12425.2880859375
Iteration 1800: Loss = -12423.71875
Iteration 1900: Loss = -12422.3720703125
Iteration 2000: Loss = -12421.2041015625
Iteration 2100: Loss = -12420.1826171875
Iteration 2200: Loss = -12419.2822265625
Iteration 2300: Loss = -12418.484375
Iteration 2400: Loss = -12417.7734375
Iteration 2500: Loss = -12417.13671875
Iteration 2600: Loss = -12416.56640625
Iteration 2700: Loss = -12416.046875
Iteration 2800: Loss = -12415.5771484375
Iteration 2900: Loss = -12415.1455078125
Iteration 3000: Loss = -12414.748046875
Iteration 3100: Loss = -12414.38671875
Iteration 3200: Loss = -12414.0556640625
Iteration 3300: Loss = -12413.7509765625
Iteration 3400: Loss = -12413.470703125
Iteration 3500: Loss = -12413.212890625
Iteration 3600: Loss = -12412.97265625
Iteration 3700: Loss = -12412.7490234375
Iteration 3800: Loss = -12412.541015625
Iteration 3900: Loss = -12412.349609375
Iteration 4000: Loss = -12412.1748046875
Iteration 4100: Loss = -12412.0126953125
Iteration 4200: Loss = -12411.8642578125
Iteration 4300: Loss = -12411.7255859375
Iteration 4400: Loss = -12411.6005859375
Iteration 4500: Loss = -12411.486328125
Iteration 4600: Loss = -12411.380859375
Iteration 4700: Loss = -12411.2822265625
Iteration 4800: Loss = -12411.1923828125
Iteration 4900: Loss = -12411.1083984375
Iteration 5000: Loss = -12411.029296875
Iteration 5100: Loss = -12410.958984375
Iteration 5200: Loss = -12410.8916015625
Iteration 5300: Loss = -12410.830078125
Iteration 5400: Loss = -12410.7744140625
Iteration 5500: Loss = -12410.724609375
Iteration 5600: Loss = -12410.67578125
Iteration 5700: Loss = -12410.630859375
Iteration 5800: Loss = -12410.5908203125
Iteration 5900: Loss = -12410.55078125
Iteration 6000: Loss = -12410.513671875
Iteration 6100: Loss = -12410.4794921875
Iteration 6200: Loss = -12410.4462890625
Iteration 6300: Loss = -12410.4169921875
Iteration 6400: Loss = -12410.3876953125
Iteration 6500: Loss = -12410.3603515625
Iteration 6600: Loss = -12410.3349609375
Iteration 6700: Loss = -12410.3115234375
Iteration 6800: Loss = -12410.2890625
Iteration 6900: Loss = -12410.2666015625
Iteration 7000: Loss = -12410.248046875
Iteration 7100: Loss = -12410.2265625
Iteration 7200: Loss = -12410.2080078125
Iteration 7300: Loss = -12410.1923828125
Iteration 7400: Loss = -12410.1748046875
Iteration 7500: Loss = -12410.16015625
Iteration 7600: Loss = -12410.1455078125
Iteration 7700: Loss = -12410.1328125
Iteration 7800: Loss = -12410.1171875
Iteration 7900: Loss = -12410.1044921875
Iteration 8000: Loss = -12410.0927734375
Iteration 8100: Loss = -12410.0830078125
Iteration 8200: Loss = -12410.0712890625
Iteration 8300: Loss = -12410.060546875
Iteration 8400: Loss = -12410.0517578125
Iteration 8500: Loss = -12410.0419921875
Iteration 8600: Loss = -12410.033203125
Iteration 8700: Loss = -12410.025390625
Iteration 8800: Loss = -12410.0166015625
Iteration 8900: Loss = -12410.009765625
Iteration 9000: Loss = -12410.0029296875
Iteration 9100: Loss = -12409.9931640625
Iteration 9200: Loss = -12409.98828125
Iteration 9300: Loss = -12409.9814453125
Iteration 9400: Loss = -12409.9765625
Iteration 9500: Loss = -12409.9697265625
Iteration 9600: Loss = -12409.96484375
Iteration 9700: Loss = -12409.9599609375
Iteration 9800: Loss = -12409.953125
Iteration 9900: Loss = -12409.9501953125
Iteration 10000: Loss = -12409.9443359375
Iteration 10100: Loss = -12409.94140625
Iteration 10200: Loss = -12409.935546875
Iteration 10300: Loss = -12409.9326171875
Iteration 10400: Loss = -12409.9287109375
Iteration 10500: Loss = -12409.923828125
Iteration 10600: Loss = -12409.919921875
Iteration 10700: Loss = -12409.9169921875
Iteration 10800: Loss = -12409.912109375
Iteration 10900: Loss = -12409.9091796875
Iteration 11000: Loss = -12409.90625
Iteration 11100: Loss = -12409.904296875
Iteration 11200: Loss = -12409.9013671875
Iteration 11300: Loss = -12409.8974609375
Iteration 11400: Loss = -12409.892578125
Iteration 11500: Loss = -12409.890625
Iteration 11600: Loss = -12409.8876953125
Iteration 11700: Loss = -12409.8837890625
Iteration 11800: Loss = -12409.8818359375
Iteration 11900: Loss = -12409.8779296875
Iteration 12000: Loss = -12409.876953125
Iteration 12100: Loss = -12409.875
Iteration 12200: Loss = -12409.87109375
Iteration 12300: Loss = -12409.87109375
Iteration 12400: Loss = -12409.8681640625
Iteration 12500: Loss = -12409.8681640625
Iteration 12600: Loss = -12409.8642578125
Iteration 12700: Loss = -12409.86328125
Iteration 12800: Loss = -12409.8623046875
Iteration 12900: Loss = -12409.8623046875
Iteration 13000: Loss = -12409.859375
Iteration 13100: Loss = -12409.859375
Iteration 13200: Loss = -12409.8564453125
Iteration 13300: Loss = -12409.8544921875
Iteration 13400: Loss = -12409.8564453125
1
Iteration 13500: Loss = -12409.853515625
Iteration 13600: Loss = -12409.853515625
Iteration 13700: Loss = -12409.8515625
Iteration 13800: Loss = -12409.8525390625
1
Iteration 13900: Loss = -12409.8505859375
Iteration 14000: Loss = -12409.8505859375
Iteration 14100: Loss = -12409.849609375
Iteration 14200: Loss = -12409.84765625
Iteration 14300: Loss = -12409.84765625
Iteration 14400: Loss = -12409.84765625
Iteration 14500: Loss = -12409.84765625
Iteration 14600: Loss = -12409.8447265625
Iteration 14700: Loss = -12409.8427734375
Iteration 14800: Loss = -12409.841796875
Iteration 14900: Loss = -12409.83984375
Iteration 15000: Loss = -12409.8388671875
Iteration 15100: Loss = -12409.833984375
Iteration 15200: Loss = -12409.421875
Iteration 15300: Loss = -12406.80078125
Iteration 15400: Loss = -12406.673828125
Iteration 15500: Loss = -12406.642578125
Iteration 15600: Loss = -12406.626953125
Iteration 15700: Loss = -12406.5029296875
Iteration 15800: Loss = -12406.4384765625
Iteration 15900: Loss = -12406.3828125
Iteration 16000: Loss = -12406.2978515625
Iteration 16100: Loss = -12406.2109375
Iteration 16200: Loss = -12406.1533203125
Iteration 16300: Loss = -12406.1201171875
Iteration 16400: Loss = -12406.095703125
Iteration 16500: Loss = -12406.08203125
Iteration 16600: Loss = -12406.068359375
Iteration 16700: Loss = -12406.0595703125
Iteration 16800: Loss = -12406.0537109375
Iteration 16900: Loss = -12406.0478515625
Iteration 17000: Loss = -12406.04296875
Iteration 17100: Loss = -12406.0390625
Iteration 17200: Loss = -12406.037109375
Iteration 17300: Loss = -12406.033203125
Iteration 17400: Loss = -12406.0322265625
Iteration 17500: Loss = -12406.0302734375
Iteration 17600: Loss = -12406.0283203125
Iteration 17700: Loss = -12406.0283203125
Iteration 17800: Loss = -12406.0244140625
Iteration 17900: Loss = -12406.025390625
1
Iteration 18000: Loss = -12406.0234375
Iteration 18100: Loss = -12406.021484375
Iteration 18200: Loss = -12406.021484375
Iteration 18300: Loss = -12406.0205078125
Iteration 18400: Loss = -12406.01953125
Iteration 18500: Loss = -12406.01953125
Iteration 18600: Loss = -12406.017578125
Iteration 18700: Loss = -12406.0185546875
1
Iteration 18800: Loss = -12406.017578125
Iteration 18900: Loss = -12406.015625
Iteration 19000: Loss = -12406.0166015625
1
Iteration 19100: Loss = -12406.015625
Iteration 19200: Loss = -12406.015625
Iteration 19300: Loss = -12406.017578125
1
Iteration 19400: Loss = -12406.015625
Iteration 19500: Loss = -12406.0146484375
Iteration 19600: Loss = -12406.015625
1
Iteration 19700: Loss = -12406.0146484375
Iteration 19800: Loss = -12406.013671875
Iteration 19900: Loss = -12406.013671875
Iteration 20000: Loss = -12406.013671875
Iteration 20100: Loss = -12406.0146484375
1
Iteration 20200: Loss = -12406.0146484375
2
Iteration 20300: Loss = -12406.013671875
Iteration 20400: Loss = -12406.013671875
Iteration 20500: Loss = -12406.0146484375
1
Iteration 20600: Loss = -12406.0126953125
Iteration 20700: Loss = -12406.01171875
Iteration 20800: Loss = -12406.013671875
1
Iteration 20900: Loss = -12406.01171875
Iteration 21000: Loss = -12406.015625
1
Iteration 21100: Loss = -12406.0126953125
2
Iteration 21200: Loss = -12406.0126953125
3
Iteration 21300: Loss = -12405.9794921875
Iteration 21400: Loss = -12405.9755859375
Iteration 21500: Loss = -12405.97265625
Iteration 21600: Loss = -12405.970703125
Iteration 21700: Loss = -12405.9716796875
1
Iteration 21800: Loss = -12405.96875
Iteration 21900: Loss = -12405.9677734375
Iteration 22000: Loss = -12405.9697265625
1
Iteration 22100: Loss = -12405.9697265625
2
Iteration 22200: Loss = -12405.9677734375
Iteration 22300: Loss = -12405.9697265625
1
Iteration 22400: Loss = -12405.9677734375
Iteration 22500: Loss = -12405.9677734375
Iteration 22600: Loss = -12405.96875
1
Iteration 22700: Loss = -12405.9677734375
Iteration 22800: Loss = -12405.9677734375
Iteration 22900: Loss = -12405.9677734375
Iteration 23000: Loss = -12405.9677734375
Iteration 23100: Loss = -12405.97265625
1
Iteration 23200: Loss = -12405.96875
2
Iteration 23300: Loss = -12405.966796875
Iteration 23400: Loss = -12405.9677734375
1
Iteration 23500: Loss = -12405.96875
2
Iteration 23600: Loss = -12405.9677734375
3
Iteration 23700: Loss = -12405.96875
4
Iteration 23800: Loss = -12405.96875
5
Iteration 23900: Loss = -12405.9677734375
6
Iteration 24000: Loss = -12405.9697265625
7
Iteration 24100: Loss = -12405.9677734375
8
Iteration 24200: Loss = -12405.966796875
Iteration 24300: Loss = -12405.9677734375
1
Iteration 24400: Loss = -12405.96875
2
Iteration 24500: Loss = -12405.9677734375
3
Iteration 24600: Loss = -12405.966796875
Iteration 24700: Loss = -12405.96875
1
Iteration 24800: Loss = -12405.9677734375
2
Iteration 24900: Loss = -12405.9677734375
3
Iteration 25000: Loss = -12405.966796875
Iteration 25100: Loss = -12405.966796875
Iteration 25200: Loss = -12405.966796875
Iteration 25300: Loss = -12405.9677734375
1
Iteration 25400: Loss = -12405.9677734375
2
Iteration 25500: Loss = -12405.9677734375
3
Iteration 25600: Loss = -12405.9677734375
4
Iteration 25700: Loss = -12405.9697265625
5
Iteration 25800: Loss = -12405.966796875
Iteration 25900: Loss = -12405.9677734375
1
Iteration 26000: Loss = -12405.96875
2
Iteration 26100: Loss = -12405.9697265625
3
Iteration 26200: Loss = -12405.9677734375
4
Iteration 26300: Loss = -12405.9677734375
5
Iteration 26400: Loss = -12405.96875
6
Iteration 26500: Loss = -12405.96875
7
Iteration 26600: Loss = -12405.9677734375
8
Iteration 26700: Loss = -12405.9677734375
9
Iteration 26800: Loss = -12405.966796875
Iteration 26900: Loss = -12405.966796875
Iteration 27000: Loss = -12405.9677734375
1
Iteration 27100: Loss = -12405.9697265625
2
Iteration 27200: Loss = -12405.96875
3
Iteration 27300: Loss = -12405.966796875
Iteration 27400: Loss = -12405.9677734375
1
Iteration 27500: Loss = -12405.96875
2
Iteration 27600: Loss = -12405.9677734375
3
Iteration 27700: Loss = -12405.978515625
4
Iteration 27800: Loss = -12405.9677734375
5
Iteration 27900: Loss = -12405.9677734375
6
Iteration 28000: Loss = -12405.9677734375
7
Iteration 28100: Loss = -12405.9677734375
8
Iteration 28200: Loss = -12405.96875
9
Iteration 28300: Loss = -12405.9697265625
10
Iteration 28400: Loss = -12405.966796875
Iteration 28500: Loss = -12405.96875
1
Iteration 28600: Loss = -12405.96875
2
Iteration 28700: Loss = -12405.9677734375
3
Iteration 28800: Loss = -12405.9697265625
4
Iteration 28900: Loss = -12405.9677734375
5
Iteration 29000: Loss = -12405.9677734375
6
Iteration 29100: Loss = -12405.9677734375
7
Iteration 29200: Loss = -12405.966796875
Iteration 29300: Loss = -12405.9677734375
1
Iteration 29400: Loss = -12405.96875
2
Iteration 29500: Loss = -12405.9677734375
3
Iteration 29600: Loss = -12405.9677734375
4
Iteration 29700: Loss = -12405.9677734375
5
Iteration 29800: Loss = -12405.9677734375
6
Iteration 29900: Loss = -12405.9697265625
7
pi: tensor([[0.9897, 0.0103],
        [0.1611, 0.8389]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9981, 0.0019], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2023, 0.1852],
         [0.9914, 0.9934]],

        [[0.5774, 0.0882],
         [0.1770, 0.5488]],

        [[0.0129, 0.2411],
         [0.0476, 0.4831]],

        [[0.0663, 0.1122],
         [0.9927, 0.0182]],

        [[0.2270, 0.1669],
         [0.8678, 0.0070]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.01171303074670571
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0070073061126442035
Global Adjusted Rand Index: -0.0017214713622936695
Average Adjusted Rand Index: -0.0005040094689878618
[0.0, -0.0017214713622936695] [0.0, -0.0005040094689878618] [12409.873046875, 12405.9677734375]
-------------------------------------
This iteration is 15
True Objective function: Loss = -11809.322912558191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35897.1328125
Iteration 100: Loss = -22117.359375
Iteration 200: Loss = -14594.70703125
Iteration 300: Loss = -13038.3125
Iteration 400: Loss = -12805.228515625
Iteration 500: Loss = -12720.4755859375
Iteration 600: Loss = -12676.06640625
Iteration 700: Loss = -12644.185546875
Iteration 800: Loss = -12620.6416015625
Iteration 900: Loss = -12604.267578125
Iteration 1000: Loss = -12588.8740234375
Iteration 1100: Loss = -12576.9453125
Iteration 1200: Loss = -12569.30859375
Iteration 1300: Loss = -12561.4326171875
Iteration 1400: Loss = -12555.6884765625
Iteration 1500: Loss = -12550.345703125
Iteration 1600: Loss = -12545.314453125
Iteration 1700: Loss = -12541.072265625
Iteration 1800: Loss = -12537.2197265625
Iteration 1900: Loss = -12533.224609375
Iteration 2000: Loss = -12530.1630859375
Iteration 2100: Loss = -12527.89453125
Iteration 2200: Loss = -12525.6396484375
Iteration 2300: Loss = -12523.181640625
Iteration 2400: Loss = -12521.599609375
Iteration 2500: Loss = -12519.80078125
Iteration 2600: Loss = -12517.3359375
Iteration 2700: Loss = -12515.75390625
Iteration 2800: Loss = -12514.2080078125
Iteration 2900: Loss = -12512.43359375
Iteration 3000: Loss = -12510.736328125
Iteration 3100: Loss = -12509.35546875
Iteration 3200: Loss = -12507.89453125
Iteration 3300: Loss = -12506.1142578125
Iteration 3400: Loss = -12504.8056640625
Iteration 3500: Loss = -12503.6875
Iteration 3600: Loss = -12502.796875
Iteration 3700: Loss = -12501.771484375
Iteration 3800: Loss = -12501.08984375
Iteration 3900: Loss = -12500.224609375
Iteration 4000: Loss = -12498.3583984375
Iteration 4100: Loss = -12492.55078125
Iteration 4200: Loss = -12487.2744140625
Iteration 4300: Loss = -12483.748046875
Iteration 4400: Loss = -12482.2783203125
Iteration 4500: Loss = -12480.3828125
Iteration 4600: Loss = -12477.787109375
Iteration 4700: Loss = -12471.98046875
Iteration 4800: Loss = -12468.662109375
Iteration 4900: Loss = -12464.3798828125
Iteration 5000: Loss = -12463.0302734375
Iteration 5100: Loss = -12459.119140625
Iteration 5200: Loss = -12454.1923828125
Iteration 5300: Loss = -12449.3623046875
Iteration 5400: Loss = -12442.7080078125
Iteration 5500: Loss = -12435.97265625
Iteration 5600: Loss = -12424.7998046875
Iteration 5700: Loss = -12412.552734375
Iteration 5800: Loss = -12401.595703125
Iteration 5900: Loss = -12397.0576171875
Iteration 6000: Loss = -12392.03515625
Iteration 6100: Loss = -12390.0888671875
Iteration 6200: Loss = -12387.611328125
Iteration 6300: Loss = -12384.8095703125
Iteration 6400: Loss = -12384.0849609375
Iteration 6500: Loss = -12383.7431640625
Iteration 6600: Loss = -12383.5205078125
Iteration 6700: Loss = -12383.3603515625
Iteration 6800: Loss = -12383.23828125
Iteration 6900: Loss = -12383.1376953125
Iteration 7000: Loss = -12383.0546875
Iteration 7100: Loss = -12382.9814453125
Iteration 7200: Loss = -12382.9208984375
Iteration 7300: Loss = -12382.8662109375
Iteration 7400: Loss = -12382.81640625
Iteration 7500: Loss = -12382.7744140625
Iteration 7600: Loss = -12382.7333984375
Iteration 7700: Loss = -12382.697265625
Iteration 7800: Loss = -12382.6650390625
Iteration 7900: Loss = -12382.634765625
Iteration 8000: Loss = -12382.6083984375
Iteration 8100: Loss = -12382.5830078125
Iteration 8200: Loss = -12382.556640625
Iteration 8300: Loss = -12382.53515625
Iteration 8400: Loss = -12382.515625
Iteration 8500: Loss = -12382.49609375
Iteration 8600: Loss = -12382.478515625
Iteration 8700: Loss = -12382.4619140625
Iteration 8800: Loss = -12382.4462890625
Iteration 8900: Loss = -12382.431640625
Iteration 9000: Loss = -12382.41796875
Iteration 9100: Loss = -12382.4052734375
Iteration 9200: Loss = -12382.39453125
Iteration 9300: Loss = -12382.3818359375
Iteration 9400: Loss = -12382.3720703125
Iteration 9500: Loss = -12382.3623046875
Iteration 9600: Loss = -12382.3525390625
Iteration 9700: Loss = -12382.341796875
Iteration 9800: Loss = -12382.3330078125
Iteration 9900: Loss = -12382.328125
Iteration 10000: Loss = -12382.3193359375
Iteration 10100: Loss = -12382.3125
Iteration 10200: Loss = -12382.3056640625
Iteration 10300: Loss = -12382.30078125
Iteration 10400: Loss = -12382.2939453125
Iteration 10500: Loss = -12382.287109375
Iteration 10600: Loss = -12382.2841796875
Iteration 10700: Loss = -12382.279296875
Iteration 10800: Loss = -12382.2744140625
Iteration 10900: Loss = -12382.26953125
Iteration 11000: Loss = -12382.263671875
Iteration 11100: Loss = -12382.2607421875
Iteration 11200: Loss = -12382.2568359375
Iteration 11300: Loss = -12382.2529296875
Iteration 11400: Loss = -12382.248046875
Iteration 11500: Loss = -12382.24609375
Iteration 11600: Loss = -12382.244140625
Iteration 11700: Loss = -12382.2412109375
Iteration 11800: Loss = -12382.2373046875
Iteration 11900: Loss = -12382.2373046875
Iteration 12000: Loss = -12382.2333984375
Iteration 12100: Loss = -12382.2294921875
Iteration 12200: Loss = -12382.228515625
Iteration 12300: Loss = -12382.2255859375
Iteration 12400: Loss = -12382.224609375
Iteration 12500: Loss = -12382.22265625
Iteration 12600: Loss = -12382.2197265625
Iteration 12700: Loss = -12382.21875
Iteration 12800: Loss = -12382.216796875
Iteration 12900: Loss = -12382.216796875
Iteration 13000: Loss = -12382.2138671875
Iteration 13100: Loss = -12382.2109375
Iteration 13200: Loss = -12382.2119140625
1
Iteration 13300: Loss = -12382.2080078125
Iteration 13400: Loss = -12382.208984375
1
Iteration 13500: Loss = -12382.208984375
2
Iteration 13600: Loss = -12382.205078125
Iteration 13700: Loss = -12382.205078125
Iteration 13800: Loss = -12382.2021484375
Iteration 13900: Loss = -12382.2041015625
1
Iteration 14000: Loss = -12382.203125
2
Iteration 14100: Loss = -12382.2021484375
Iteration 14200: Loss = -12382.201171875
Iteration 14300: Loss = -12382.201171875
Iteration 14400: Loss = -12382.2001953125
Iteration 14500: Loss = -12382.1982421875
Iteration 14600: Loss = -12382.2001953125
1
Iteration 14700: Loss = -12382.19921875
2
Iteration 14800: Loss = -12382.1953125
Iteration 14900: Loss = -12382.1962890625
1
Iteration 15000: Loss = -12382.1953125
Iteration 15100: Loss = -12382.1962890625
1
Iteration 15200: Loss = -12382.1962890625
2
Iteration 15300: Loss = -12382.193359375
Iteration 15400: Loss = -12382.1953125
1
Iteration 15500: Loss = -12382.1943359375
2
Iteration 15600: Loss = -12382.1943359375
3
Iteration 15700: Loss = -12382.1923828125
Iteration 15800: Loss = -12382.193359375
1
Iteration 15900: Loss = -12382.19140625
Iteration 16000: Loss = -12382.19140625
Iteration 16100: Loss = -12382.193359375
1
Iteration 16200: Loss = -12382.19140625
Iteration 16300: Loss = -12382.1904296875
Iteration 16400: Loss = -12382.1904296875
Iteration 16500: Loss = -12382.1904296875
Iteration 16600: Loss = -12382.19140625
1
Iteration 16700: Loss = -12382.1904296875
Iteration 16800: Loss = -12382.1884765625
Iteration 16900: Loss = -12382.1884765625
Iteration 17000: Loss = -12382.189453125
1
Iteration 17100: Loss = -12382.1904296875
2
Iteration 17200: Loss = -12382.1884765625
Iteration 17300: Loss = -12382.189453125
1
Iteration 17400: Loss = -12382.1875
Iteration 17500: Loss = -12382.1875
Iteration 17600: Loss = -12382.189453125
1
Iteration 17700: Loss = -12382.1884765625
2
Iteration 17800: Loss = -12382.1875
Iteration 17900: Loss = -12382.1884765625
1
Iteration 18000: Loss = -12382.1865234375
Iteration 18100: Loss = -12382.1884765625
1
Iteration 18200: Loss = -12382.1884765625
2
Iteration 18300: Loss = -12382.1884765625
3
Iteration 18400: Loss = -12382.1875
4
Iteration 18500: Loss = -12382.1865234375
Iteration 18600: Loss = -12382.1865234375
Iteration 18700: Loss = -12382.1875
1
Iteration 18800: Loss = -12382.1865234375
Iteration 18900: Loss = -12382.1845703125
Iteration 19000: Loss = -12382.1884765625
1
Iteration 19100: Loss = -12382.1865234375
2
Iteration 19200: Loss = -12382.1865234375
3
Iteration 19300: Loss = -12382.185546875
4
Iteration 19400: Loss = -12382.1884765625
5
Iteration 19500: Loss = -12382.1865234375
6
Iteration 19600: Loss = -12382.1865234375
7
Iteration 19700: Loss = -12382.1865234375
8
Iteration 19800: Loss = -12382.1865234375
9
Iteration 19900: Loss = -12382.1865234375
10
Iteration 20000: Loss = -12382.1875
11
Iteration 20100: Loss = -12382.185546875
12
Iteration 20200: Loss = -12382.185546875
13
Iteration 20300: Loss = -12382.185546875
14
Iteration 20400: Loss = -12382.1875
15
Stopping early at iteration 20400 due to no improvement.
pi: tensor([[9.9999e-01, 7.2585e-06],
        [9.8453e-01, 1.5474e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 8.9409e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1999, 0.2618],
         [0.7702, 0.1913]],

        [[0.0092, 0.2196],
         [0.0192, 0.0083]],

        [[0.4899, 0.1931],
         [0.0577, 0.9906]],

        [[0.9324, 0.6779],
         [0.0095, 0.4028]],

        [[0.9521, 0.2149],
         [0.9110, 0.0370]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -34145.6328125
Iteration 100: Loss = -23808.02734375
Iteration 200: Loss = -16005.50390625
Iteration 300: Loss = -13845.2197265625
Iteration 400: Loss = -13088.306640625
Iteration 500: Loss = -12778.73828125
Iteration 600: Loss = -12618.67578125
Iteration 700: Loss = -12540.13671875
Iteration 800: Loss = -12495.431640625
Iteration 900: Loss = -12473.859375
Iteration 1000: Loss = -12459.8330078125
Iteration 1100: Loss = -12450.447265625
Iteration 1200: Loss = -12440.8466796875
Iteration 1300: Loss = -12436.01171875
Iteration 1400: Loss = -12432.2412109375
Iteration 1500: Loss = -12428.5517578125
Iteration 1600: Loss = -12424.583984375
Iteration 1700: Loss = -12422.474609375
Iteration 1800: Loss = -12420.71484375
Iteration 1900: Loss = -12419.1806640625
Iteration 2000: Loss = -12417.7763671875
Iteration 2100: Loss = -12416.3046875
Iteration 2200: Loss = -12413.13671875
Iteration 2300: Loss = -12411.9326171875
Iteration 2400: Loss = -12410.7333984375
Iteration 2500: Loss = -12407.7685546875
Iteration 2600: Loss = -12406.3994140625
Iteration 2700: Loss = -12401.5673828125
Iteration 2800: Loss = -12400.78125
Iteration 2900: Loss = -12400.150390625
Iteration 3000: Loss = -12399.595703125
Iteration 3100: Loss = -12399.0849609375
Iteration 3200: Loss = -12398.5771484375
Iteration 3300: Loss = -12397.9462890625
Iteration 3400: Loss = -12396.041015625
Iteration 3500: Loss = -12394.87109375
Iteration 3600: Loss = -12394.251953125
Iteration 3700: Loss = -12393.8154296875
Iteration 3800: Loss = -12393.4736328125
Iteration 3900: Loss = -12393.1865234375
Iteration 4000: Loss = -12392.9326171875
Iteration 4100: Loss = -12392.6982421875
Iteration 4200: Loss = -12392.4755859375
Iteration 4300: Loss = -12392.26953125
Iteration 4400: Loss = -12392.0791015625
Iteration 4500: Loss = -12391.9013671875
Iteration 4600: Loss = -12391.7392578125
Iteration 4700: Loss = -12391.578125
Iteration 4800: Loss = -12391.4140625
Iteration 4900: Loss = -12391.2724609375
Iteration 5000: Loss = -12391.1044921875
Iteration 5100: Loss = -12390.96484375
Iteration 5200: Loss = -12390.8115234375
Iteration 5300: Loss = -12389.2158203125
Iteration 5400: Loss = -12387.7197265625
Iteration 5500: Loss = -12387.556640625
Iteration 5600: Loss = -12387.419921875
Iteration 5700: Loss = -12387.248046875
Iteration 5800: Loss = -12387.1728515625
Iteration 5900: Loss = -12387.1083984375
Iteration 6000: Loss = -12387.0458984375
Iteration 6100: Loss = -12386.9716796875
Iteration 6200: Loss = -12386.9189453125
Iteration 6300: Loss = -12386.8701171875
Iteration 6400: Loss = -12386.8232421875
Iteration 6500: Loss = -12386.779296875
Iteration 6600: Loss = -12386.73828125
Iteration 6700: Loss = -12386.6943359375
Iteration 6800: Loss = -12386.634765625
Iteration 6900: Loss = -12386.58203125
Iteration 7000: Loss = -12386.5439453125
Iteration 7100: Loss = -12386.5107421875
Iteration 7200: Loss = -12386.4794921875
Iteration 7300: Loss = -12386.451171875
Iteration 7400: Loss = -12386.4248046875
Iteration 7500: Loss = -12386.3974609375
Iteration 7600: Loss = -12386.3720703125
Iteration 7700: Loss = -12386.34765625
Iteration 7800: Loss = -12386.322265625
Iteration 7900: Loss = -12386.298828125
Iteration 8000: Loss = -12386.2724609375
Iteration 8100: Loss = -12386.24609375
Iteration 8200: Loss = -12386.22265625
Iteration 8300: Loss = -12386.1962890625
Iteration 8400: Loss = -12386.1767578125
Iteration 8500: Loss = -12386.1572265625
Iteration 8600: Loss = -12386.138671875
Iteration 8700: Loss = -12386.12109375
Iteration 8800: Loss = -12386.1083984375
Iteration 8900: Loss = -12386.0947265625
Iteration 9000: Loss = -12386.0810546875
Iteration 9100: Loss = -12386.0693359375
Iteration 9200: Loss = -12386.05859375
Iteration 9300: Loss = -12386.0478515625
Iteration 9400: Loss = -12386.033203125
Iteration 9500: Loss = -12386.0224609375
Iteration 9600: Loss = -12386.0126953125
Iteration 9700: Loss = -12386.00390625
Iteration 9800: Loss = -12385.99609375
Iteration 9900: Loss = -12385.98828125
Iteration 10000: Loss = -12385.9814453125
Iteration 10100: Loss = -12385.974609375
Iteration 10200: Loss = -12385.9677734375
Iteration 10300: Loss = -12385.962890625
Iteration 10400: Loss = -12385.95703125
Iteration 10500: Loss = -12385.951171875
Iteration 10600: Loss = -12385.9482421875
Iteration 10700: Loss = -12385.9443359375
Iteration 10800: Loss = -12385.939453125
Iteration 10900: Loss = -12385.9345703125
Iteration 11000: Loss = -12385.931640625
Iteration 11100: Loss = -12385.93359375
1
Iteration 11200: Loss = -12385.9248046875
Iteration 11300: Loss = -12385.921875
Iteration 11400: Loss = -12385.91796875
Iteration 11500: Loss = -12385.9150390625
Iteration 11600: Loss = -12385.912109375
Iteration 11700: Loss = -12385.9091796875
Iteration 11800: Loss = -12385.90625
Iteration 11900: Loss = -12385.90234375
Iteration 12000: Loss = -12385.888671875
Iteration 12100: Loss = -12385.8798828125
Iteration 12200: Loss = -12385.8779296875
Iteration 12300: Loss = -12385.1015625
Iteration 12400: Loss = -12383.6142578125
Iteration 12500: Loss = -12383.6103515625
Iteration 12600: Loss = -12383.603515625
Iteration 12700: Loss = -12383.6015625
Iteration 12800: Loss = -12383.6005859375
Iteration 12900: Loss = -12383.5927734375
Iteration 13000: Loss = -12383.578125
Iteration 13100: Loss = -12383.576171875
Iteration 13200: Loss = -12383.57421875
Iteration 13300: Loss = -12383.5732421875
Iteration 13400: Loss = -12383.5712890625
Iteration 13500: Loss = -12383.5703125
Iteration 13600: Loss = -12383.5712890625
1
Iteration 13700: Loss = -12383.5703125
Iteration 13800: Loss = -12383.5673828125
Iteration 13900: Loss = -12383.5673828125
Iteration 14000: Loss = -12383.5673828125
Iteration 14100: Loss = -12383.56640625
Iteration 14200: Loss = -12383.5654296875
Iteration 14300: Loss = -12383.564453125
Iteration 14400: Loss = -12383.564453125
Iteration 14500: Loss = -12383.533203125
Iteration 14600: Loss = -12383.5
Iteration 14700: Loss = -12383.40234375
Iteration 14800: Loss = -12382.4814453125
Iteration 14900: Loss = -12380.6142578125
Iteration 15000: Loss = -12380.59375
Iteration 15100: Loss = -12380.5927734375
Iteration 15200: Loss = -12380.5908203125
Iteration 15300: Loss = -12380.58984375
Iteration 15400: Loss = -12380.587890625
Iteration 15500: Loss = -12380.58984375
1
Iteration 15600: Loss = -12380.587890625
Iteration 15700: Loss = -12380.5869140625
Iteration 15800: Loss = -12380.5869140625
Iteration 15900: Loss = -12380.5869140625
Iteration 16000: Loss = -12380.5810546875
Iteration 16100: Loss = -12380.58203125
1
Iteration 16200: Loss = -12380.5810546875
Iteration 16300: Loss = -12380.5810546875
Iteration 16400: Loss = -12380.5810546875
Iteration 16500: Loss = -12380.5810546875
Iteration 16600: Loss = -12380.5810546875
Iteration 16700: Loss = -12380.5791015625
Iteration 16800: Loss = -12380.5791015625
Iteration 16900: Loss = -12380.5791015625
Iteration 17000: Loss = -12380.580078125
1
Iteration 17100: Loss = -12380.5791015625
Iteration 17200: Loss = -12380.5791015625
Iteration 17300: Loss = -12380.5791015625
Iteration 17400: Loss = -12380.5791015625
Iteration 17500: Loss = -12380.5791015625
Iteration 17600: Loss = -12380.5791015625
Iteration 17700: Loss = -12380.5791015625
Iteration 17800: Loss = -12380.578125
Iteration 17900: Loss = -12380.5791015625
1
Iteration 18000: Loss = -12380.578125
Iteration 18100: Loss = -12380.572265625
Iteration 18200: Loss = -12380.57421875
1
Iteration 18300: Loss = -12380.57421875
2
Iteration 18400: Loss = -12380.57421875
3
Iteration 18500: Loss = -12380.5751953125
4
Iteration 18600: Loss = -12380.57421875
5
Iteration 18700: Loss = -12380.5732421875
6
Iteration 18800: Loss = -12380.57421875
7
Iteration 18900: Loss = -12380.5751953125
8
Iteration 19000: Loss = -12380.57421875
9
Iteration 19100: Loss = -12380.5751953125
10
Iteration 19200: Loss = -12380.572265625
Iteration 19300: Loss = -12380.5732421875
1
Iteration 19400: Loss = -12380.572265625
Iteration 19500: Loss = -12380.5732421875
1
Iteration 19600: Loss = -12380.57421875
2
Iteration 19700: Loss = -12380.572265625
Iteration 19800: Loss = -12380.5732421875
1
Iteration 19900: Loss = -12380.572265625
Iteration 20000: Loss = -12380.5732421875
1
Iteration 20100: Loss = -12380.57421875
2
Iteration 20200: Loss = -12380.5732421875
3
Iteration 20300: Loss = -12380.572265625
Iteration 20400: Loss = -12380.572265625
Iteration 20500: Loss = -12380.5751953125
1
Iteration 20600: Loss = -12380.572265625
Iteration 20700: Loss = -12380.572265625
Iteration 20800: Loss = -12380.5712890625
Iteration 20900: Loss = -12380.5751953125
1
Iteration 21000: Loss = -12380.5732421875
2
Iteration 21100: Loss = -12380.572265625
3
Iteration 21200: Loss = -12380.5732421875
4
Iteration 21300: Loss = -12380.572265625
5
Iteration 21400: Loss = -12380.5751953125
6
Iteration 21500: Loss = -12380.57421875
7
Iteration 21600: Loss = -12380.572265625
8
Iteration 21700: Loss = -12380.572265625
9
Iteration 21800: Loss = -12380.572265625
10
Iteration 21900: Loss = -12380.57421875
11
Iteration 22000: Loss = -12380.572265625
12
Iteration 22100: Loss = -12380.572265625
13
Iteration 22200: Loss = -12380.572265625
14
Iteration 22300: Loss = -12380.572265625
15
Stopping early at iteration 22300 due to no improvement.
pi: tensor([[1.3153e-02, 9.8685e-01],
        [2.3623e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 6.2580e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1909, 0.2033],
         [0.6604, 0.2020]],

        [[0.7397, 0.2260],
         [0.2825, 0.0108]],

        [[0.9222, 0.7063],
         [0.9303, 0.0318]],

        [[0.9068, 0.1927],
         [0.2113, 0.3454]],

        [[0.1266, 0.2137],
         [0.9687, 0.6839]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0015885126575489166
Average Adjusted Rand Index: 0.0
[0.0, -0.0015885126575489166] [0.0, 0.0] [12382.1875, 12380.572265625]
-------------------------------------
This iteration is 16
True Objective function: Loss = -11822.951962475014
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23619.37109375
Iteration 100: Loss = -17220.001953125
Iteration 200: Loss = -13813.2578125
Iteration 300: Loss = -12725.884765625
Iteration 400: Loss = -12486.7763671875
Iteration 500: Loss = -12417.9697265625
Iteration 600: Loss = -12394.3662109375
Iteration 700: Loss = -12381.767578125
Iteration 800: Loss = -12372.9453125
Iteration 900: Loss = -12365.890625
Iteration 1000: Loss = -12358.7392578125
Iteration 1100: Loss = -12351.755859375
Iteration 1200: Loss = -12347.5390625
Iteration 1300: Loss = -12345.4169921875
Iteration 1400: Loss = -12344.19921875
Iteration 1500: Loss = -12343.4072265625
Iteration 1600: Loss = -12342.8505859375
Iteration 1700: Loss = -12342.447265625
Iteration 1800: Loss = -12342.1474609375
Iteration 1900: Loss = -12341.91015625
Iteration 2000: Loss = -12341.7177734375
Iteration 2100: Loss = -12341.5576171875
Iteration 2200: Loss = -12341.4248046875
Iteration 2300: Loss = -12341.3134765625
Iteration 2400: Loss = -12341.216796875
Iteration 2500: Loss = -12341.1337890625
Iteration 2600: Loss = -12341.0615234375
Iteration 2700: Loss = -12340.998046875
Iteration 2800: Loss = -12340.9404296875
Iteration 2900: Loss = -12340.8876953125
Iteration 3000: Loss = -12340.841796875
Iteration 3100: Loss = -12340.7978515625
Iteration 3200: Loss = -12340.759765625
Iteration 3300: Loss = -12340.7255859375
Iteration 3400: Loss = -12340.6962890625
Iteration 3500: Loss = -12340.666015625
Iteration 3600: Loss = -12340.6396484375
Iteration 3700: Loss = -12340.6162109375
Iteration 3800: Loss = -12340.59375
Iteration 3900: Loss = -12340.572265625
Iteration 4000: Loss = -12340.552734375
Iteration 4100: Loss = -12340.5341796875
Iteration 4200: Loss = -12340.515625
Iteration 4300: Loss = -12340.501953125
Iteration 4400: Loss = -12340.48828125
Iteration 4500: Loss = -12340.478515625
Iteration 4600: Loss = -12340.4658203125
Iteration 4700: Loss = -12340.455078125
Iteration 4800: Loss = -12340.4462890625
Iteration 4900: Loss = -12340.4365234375
Iteration 5000: Loss = -12340.4267578125
Iteration 5100: Loss = -12340.4169921875
Iteration 5200: Loss = -12340.408203125
Iteration 5300: Loss = -12340.3984375
Iteration 5400: Loss = -12340.3916015625
Iteration 5500: Loss = -12340.380859375
Iteration 5600: Loss = -12340.3720703125
Iteration 5700: Loss = -12340.3642578125
Iteration 5800: Loss = -12340.3564453125
Iteration 5900: Loss = -12340.3486328125
Iteration 6000: Loss = -12340.337890625
Iteration 6100: Loss = -12340.3271484375
Iteration 6200: Loss = -12340.318359375
Iteration 6300: Loss = -12340.30859375
Iteration 6400: Loss = -12340.298828125
Iteration 6500: Loss = -12340.287109375
Iteration 6600: Loss = -12340.2763671875
Iteration 6700: Loss = -12340.265625
Iteration 6800: Loss = -12340.251953125
Iteration 6900: Loss = -12340.2392578125
Iteration 7000: Loss = -12340.22265625
Iteration 7100: Loss = -12340.197265625
Iteration 7200: Loss = -12340.1748046875
Iteration 7300: Loss = -12340.1533203125
Iteration 7400: Loss = -12340.12890625
Iteration 7500: Loss = -12340.1025390625
Iteration 7600: Loss = -12340.0693359375
Iteration 7700: Loss = -12340.029296875
Iteration 7800: Loss = -12339.982421875
Iteration 7900: Loss = -12339.9228515625
Iteration 8000: Loss = -12339.853515625
Iteration 8100: Loss = -12339.771484375
Iteration 8200: Loss = -12339.685546875
Iteration 8300: Loss = -12339.615234375
Iteration 8400: Loss = -12339.5732421875
Iteration 8500: Loss = -12339.5078125
Iteration 8600: Loss = -12339.478515625
Iteration 8700: Loss = -12339.4521484375
Iteration 8800: Loss = -12339.431640625
Iteration 8900: Loss = -12339.412109375
Iteration 9000: Loss = -12339.396484375
Iteration 9100: Loss = -12339.3818359375
Iteration 9200: Loss = -12339.3701171875
Iteration 9300: Loss = -12339.359375
Iteration 9400: Loss = -12339.3525390625
Iteration 9500: Loss = -12339.345703125
Iteration 9600: Loss = -12339.341796875
Iteration 9700: Loss = -12339.3359375
Iteration 9800: Loss = -12339.3330078125
Iteration 9900: Loss = -12339.3310546875
Iteration 10000: Loss = -12339.328125
Iteration 10100: Loss = -12339.3251953125
Iteration 10200: Loss = -12339.322265625
Iteration 10300: Loss = -12339.3232421875
1
Iteration 10400: Loss = -12339.3203125
Iteration 10500: Loss = -12339.318359375
Iteration 10600: Loss = -12339.318359375
Iteration 10700: Loss = -12339.3173828125
Iteration 10800: Loss = -12339.31640625
Iteration 10900: Loss = -12339.31640625
Iteration 11000: Loss = -12339.314453125
Iteration 11100: Loss = -12339.314453125
Iteration 11200: Loss = -12339.3134765625
Iteration 11300: Loss = -12339.314453125
1
Iteration 11400: Loss = -12339.314453125
2
Iteration 11500: Loss = -12339.3134765625
Iteration 11600: Loss = -12339.3115234375
Iteration 11700: Loss = -12339.3125
1
Iteration 11800: Loss = -12339.3115234375
Iteration 11900: Loss = -12339.3134765625
1
Iteration 12000: Loss = -12339.314453125
2
Iteration 12100: Loss = -12339.3115234375
Iteration 12200: Loss = -12339.3115234375
Iteration 12300: Loss = -12339.3115234375
Iteration 12400: Loss = -12339.3115234375
Iteration 12500: Loss = -12339.3115234375
Iteration 12600: Loss = -12339.3095703125
Iteration 12700: Loss = -12339.310546875
1
Iteration 12800: Loss = -12339.310546875
2
Iteration 12900: Loss = -12339.30859375
Iteration 13000: Loss = -12339.3046875
Iteration 13100: Loss = -12339.30078125
Iteration 13200: Loss = -12339.2978515625
Iteration 13300: Loss = -12339.294921875
Iteration 13400: Loss = -12339.2919921875
Iteration 13500: Loss = -12339.2900390625
Iteration 13600: Loss = -12339.2919921875
1
Iteration 13700: Loss = -12339.2900390625
Iteration 13800: Loss = -12339.2900390625
Iteration 13900: Loss = -12339.2900390625
Iteration 14000: Loss = -12339.291015625
1
Iteration 14100: Loss = -12339.2900390625
Iteration 14200: Loss = -12339.2900390625
Iteration 14300: Loss = -12339.291015625
1
Iteration 14400: Loss = -12339.291015625
2
Iteration 14500: Loss = -12339.291015625
3
Iteration 14600: Loss = -12339.2900390625
Iteration 14700: Loss = -12339.291015625
1
Iteration 14800: Loss = -12339.291015625
2
Iteration 14900: Loss = -12339.291015625
3
Iteration 15000: Loss = -12339.2900390625
Iteration 15100: Loss = -12339.2900390625
Iteration 15200: Loss = -12339.291015625
1
Iteration 15300: Loss = -12339.291015625
2
Iteration 15400: Loss = -12339.2890625
Iteration 15500: Loss = -12339.2900390625
1
Iteration 15600: Loss = -12339.2890625
Iteration 15700: Loss = -12339.291015625
1
Iteration 15800: Loss = -12339.291015625
2
Iteration 15900: Loss = -12339.291015625
3
Iteration 16000: Loss = -12339.291015625
4
Iteration 16100: Loss = -12339.2900390625
5
Iteration 16200: Loss = -12339.291015625
6
Iteration 16300: Loss = -12339.2900390625
7
Iteration 16400: Loss = -12339.2919921875
8
Iteration 16500: Loss = -12339.2900390625
9
Iteration 16600: Loss = -12339.2900390625
10
Iteration 16700: Loss = -12339.2890625
Iteration 16800: Loss = -12339.2900390625
1
Iteration 16900: Loss = -12339.2939453125
2
Iteration 17000: Loss = -12339.2900390625
3
Iteration 17100: Loss = -12339.291015625
4
Iteration 17200: Loss = -12339.2900390625
5
Iteration 17300: Loss = -12339.2900390625
6
Iteration 17400: Loss = -12339.291015625
7
Iteration 17500: Loss = -12339.2900390625
8
Iteration 17600: Loss = -12339.2900390625
9
Iteration 17700: Loss = -12339.291015625
10
Iteration 17800: Loss = -12339.2939453125
11
Iteration 17900: Loss = -12339.2900390625
12
Iteration 18000: Loss = -12339.291015625
13
Iteration 18100: Loss = -12339.2900390625
14
Iteration 18200: Loss = -12339.291015625
15
Stopping early at iteration 18200 due to no improvement.
pi: tensor([[0.2648, 0.7352],
        [0.7056, 0.2944]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9967, 0.0033], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1997, 0.1987],
         [0.7991, 0.1972]],

        [[0.6031, 0.1962],
         [0.0740, 0.7362]],

        [[0.9914, 0.2034],
         [0.3125, 0.0210]],

        [[0.6282, 0.2050],
         [0.9778, 0.4283]],

        [[0.1176, 0.1897],
         [0.0577, 0.0072]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0024427480916030535
Global Adjusted Rand Index: -0.0017325615910433053
Average Adjusted Rand Index: 0.0004885496183206107
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -38479.671875
Iteration 100: Loss = -22316.68359375
Iteration 200: Loss = -14639.73828125
Iteration 300: Loss = -13168.9228515625
Iteration 400: Loss = -12807.978515625
Iteration 500: Loss = -12633.990234375
Iteration 600: Loss = -12545.2548828125
Iteration 700: Loss = -12479.3623046875
Iteration 800: Loss = -12434.0908203125
Iteration 900: Loss = -12410.904296875
Iteration 1000: Loss = -12394.439453125
Iteration 1100: Loss = -12385.05078125
Iteration 1200: Loss = -12377.4287109375
Iteration 1300: Loss = -12372.0
Iteration 1400: Loss = -12367.9228515625
Iteration 1500: Loss = -12364.6669921875
Iteration 1600: Loss = -12361.98046875
Iteration 1700: Loss = -12359.7216796875
Iteration 1800: Loss = -12357.798828125
Iteration 1900: Loss = -12356.1396484375
Iteration 2000: Loss = -12354.6982421875
Iteration 2100: Loss = -12353.435546875
Iteration 2200: Loss = -12352.3212890625
Iteration 2300: Loss = -12351.3359375
Iteration 2400: Loss = -12350.4541015625
Iteration 2500: Loss = -12349.6630859375
Iteration 2600: Loss = -12348.955078125
Iteration 2700: Loss = -12348.3134765625
Iteration 2800: Loss = -12347.7333984375
Iteration 2900: Loss = -12347.2060546875
Iteration 3000: Loss = -12346.724609375
Iteration 3100: Loss = -12346.28515625
Iteration 3200: Loss = -12345.880859375
Iteration 3300: Loss = -12345.51171875
Iteration 3400: Loss = -12345.1708984375
Iteration 3500: Loss = -12344.859375
Iteration 3600: Loss = -12344.5703125
Iteration 3700: Loss = -12344.3056640625
Iteration 3800: Loss = -12344.0595703125
Iteration 3900: Loss = -12343.830078125
Iteration 4000: Loss = -12343.6171875
Iteration 4100: Loss = -12343.4208984375
Iteration 4200: Loss = -12343.23828125
Iteration 4300: Loss = -12343.0673828125
Iteration 4400: Loss = -12342.908203125
Iteration 4500: Loss = -12342.7587890625
Iteration 4600: Loss = -12342.6201171875
Iteration 4700: Loss = -12342.490234375
Iteration 4800: Loss = -12342.369140625
Iteration 4900: Loss = -12342.2529296875
Iteration 5000: Loss = -12342.1474609375
Iteration 5100: Loss = -12342.044921875
Iteration 5200: Loss = -12341.9521484375
Iteration 5300: Loss = -12341.865234375
Iteration 5400: Loss = -12341.7802734375
Iteration 5500: Loss = -12341.703125
Iteration 5600: Loss = -12341.62890625
Iteration 5700: Loss = -12341.5595703125
Iteration 5800: Loss = -12341.4931640625
Iteration 5900: Loss = -12341.431640625
Iteration 6000: Loss = -12341.3720703125
Iteration 6100: Loss = -12341.3134765625
Iteration 6200: Loss = -12341.26171875
Iteration 6300: Loss = -12341.2109375
Iteration 6400: Loss = -12341.162109375
Iteration 6500: Loss = -12341.1171875
Iteration 6600: Loss = -12341.072265625
Iteration 6700: Loss = -12341.0302734375
Iteration 6800: Loss = -12340.9912109375
Iteration 6900: Loss = -12340.955078125
Iteration 7000: Loss = -12340.919921875
Iteration 7100: Loss = -12340.888671875
Iteration 7200: Loss = -12340.857421875
Iteration 7300: Loss = -12340.8310546875
Iteration 7400: Loss = -12340.8056640625
Iteration 7500: Loss = -12340.7802734375
Iteration 7600: Loss = -12340.7587890625
Iteration 7700: Loss = -12340.740234375
Iteration 7800: Loss = -12340.7216796875
Iteration 7900: Loss = -12340.7041015625
Iteration 8000: Loss = -12340.6884765625
Iteration 8100: Loss = -12340.673828125
Iteration 8200: Loss = -12340.66015625
Iteration 8300: Loss = -12340.6474609375
Iteration 8400: Loss = -12340.63671875
Iteration 8500: Loss = -12340.625
Iteration 8600: Loss = -12340.6171875
Iteration 8700: Loss = -12340.6044921875
Iteration 8800: Loss = -12340.5986328125
Iteration 8900: Loss = -12340.5888671875
Iteration 9000: Loss = -12340.5810546875
Iteration 9100: Loss = -12340.576171875
Iteration 9200: Loss = -12340.568359375
Iteration 9300: Loss = -12340.5634765625
Iteration 9400: Loss = -12340.556640625
Iteration 9500: Loss = -12340.552734375
Iteration 9600: Loss = -12340.5458984375
Iteration 9700: Loss = -12340.5419921875
Iteration 9800: Loss = -12340.5390625
Iteration 9900: Loss = -12340.5341796875
Iteration 10000: Loss = -12340.529296875
Iteration 10100: Loss = -12340.525390625
Iteration 10200: Loss = -12340.5234375
Iteration 10300: Loss = -12340.51953125
Iteration 10400: Loss = -12340.5166015625
Iteration 10500: Loss = -12340.513671875
Iteration 10600: Loss = -12340.5126953125
Iteration 10700: Loss = -12340.5107421875
Iteration 10800: Loss = -12340.5078125
Iteration 10900: Loss = -12340.5029296875
Iteration 11000: Loss = -12340.5009765625
Iteration 11100: Loss = -12340.5009765625
Iteration 11200: Loss = -12340.4990234375
Iteration 11300: Loss = -12340.498046875
Iteration 11400: Loss = -12340.49609375
Iteration 11500: Loss = -12340.494140625
Iteration 11600: Loss = -12340.4912109375
Iteration 11700: Loss = -12340.4892578125
Iteration 11800: Loss = -12340.4892578125
Iteration 11900: Loss = -12340.4873046875
Iteration 12000: Loss = -12340.48828125
1
Iteration 12100: Loss = -12340.486328125
Iteration 12200: Loss = -12340.4833984375
Iteration 12300: Loss = -12340.4833984375
Iteration 12400: Loss = -12340.482421875
Iteration 12500: Loss = -12340.48046875
Iteration 12600: Loss = -12340.48046875
Iteration 12700: Loss = -12340.4794921875
Iteration 12800: Loss = -12340.478515625
Iteration 12900: Loss = -12340.4765625
Iteration 13000: Loss = -12340.4775390625
1
Iteration 13100: Loss = -12340.474609375
Iteration 13200: Loss = -12340.474609375
Iteration 13300: Loss = -12340.474609375
Iteration 13400: Loss = -12340.4736328125
Iteration 13500: Loss = -12340.47265625
Iteration 13600: Loss = -12340.47265625
Iteration 13700: Loss = -12340.4697265625
Iteration 13800: Loss = -12340.470703125
1
Iteration 13900: Loss = -12340.46875
Iteration 14000: Loss = -12340.47265625
1
Iteration 14100: Loss = -12340.4677734375
Iteration 14200: Loss = -12340.466796875
Iteration 14300: Loss = -12340.4658203125
Iteration 14400: Loss = -12340.4658203125
Iteration 14500: Loss = -12340.4658203125
Iteration 14600: Loss = -12340.46484375
Iteration 14700: Loss = -12340.4638671875
Iteration 14800: Loss = -12340.4638671875
Iteration 14900: Loss = -12340.462890625
Iteration 15000: Loss = -12340.462890625
Iteration 15100: Loss = -12340.4619140625
Iteration 15200: Loss = -12340.4619140625
Iteration 15300: Loss = -12340.4619140625
Iteration 15400: Loss = -12340.4619140625
Iteration 15500: Loss = -12340.4599609375
Iteration 15600: Loss = -12340.4609375
1
Iteration 15700: Loss = -12340.4599609375
Iteration 15800: Loss = -12340.458984375
Iteration 15900: Loss = -12340.4580078125
Iteration 16000: Loss = -12340.4580078125
Iteration 16100: Loss = -12340.45703125
Iteration 16200: Loss = -12340.4560546875
Iteration 16300: Loss = -12340.455078125
Iteration 16400: Loss = -12340.453125
Iteration 16500: Loss = -12340.4521484375
Iteration 16600: Loss = -12340.44921875
Iteration 16700: Loss = -12340.4501953125
1
Iteration 16800: Loss = -12340.4462890625
Iteration 16900: Loss = -12340.443359375
Iteration 17000: Loss = -12340.44140625
Iteration 17100: Loss = -12340.439453125
Iteration 17200: Loss = -12340.4365234375
Iteration 17300: Loss = -12340.43359375
Iteration 17400: Loss = -12340.423828125
Iteration 17500: Loss = -12340.412109375
Iteration 17600: Loss = -12340.384765625
Iteration 17700: Loss = -12339.9091796875
Iteration 17800: Loss = -12339.6787109375
Iteration 17900: Loss = -12339.5283203125
Iteration 18000: Loss = -12339.4736328125
Iteration 18100: Loss = -12339.4267578125
Iteration 18200: Loss = -12339.3134765625
Iteration 18300: Loss = -12339.28515625
Iteration 18400: Loss = -12339.27734375
Iteration 18500: Loss = -12339.2705078125
Iteration 18600: Loss = -12339.2685546875
Iteration 18700: Loss = -12339.265625
Iteration 18800: Loss = -12339.2646484375
Iteration 18900: Loss = -12339.2646484375
Iteration 19000: Loss = -12339.26171875
Iteration 19100: Loss = -12339.263671875
1
Iteration 19200: Loss = -12339.26171875
Iteration 19300: Loss = -12339.26171875
Iteration 19400: Loss = -12339.2607421875
Iteration 19500: Loss = -12339.26171875
1
Iteration 19600: Loss = -12339.26171875
2
Iteration 19700: Loss = -12339.259765625
Iteration 19800: Loss = -12339.259765625
Iteration 19900: Loss = -12339.2587890625
Iteration 20000: Loss = -12339.2607421875
1
Iteration 20100: Loss = -12339.259765625
2
Iteration 20200: Loss = -12339.2607421875
3
Iteration 20300: Loss = -12339.259765625
4
Iteration 20400: Loss = -12339.259765625
5
Iteration 20500: Loss = -12339.2587890625
Iteration 20600: Loss = -12339.2587890625
Iteration 20700: Loss = -12339.2587890625
Iteration 20800: Loss = -12339.2607421875
1
Iteration 20900: Loss = -12339.2587890625
Iteration 21000: Loss = -12339.2578125
Iteration 21100: Loss = -12339.259765625
1
Iteration 21200: Loss = -12339.2578125
Iteration 21300: Loss = -12339.2578125
Iteration 21400: Loss = -12339.2607421875
1
Iteration 21500: Loss = -12339.2587890625
2
Iteration 21600: Loss = -12339.2587890625
3
Iteration 21700: Loss = -12339.2587890625
4
Iteration 21800: Loss = -12339.259765625
5
Iteration 21900: Loss = -12339.259765625
6
Iteration 22000: Loss = -12339.2587890625
7
Iteration 22100: Loss = -12339.2568359375
Iteration 22200: Loss = -12339.2587890625
1
Iteration 22300: Loss = -12339.259765625
2
Iteration 22400: Loss = -12339.2568359375
Iteration 22500: Loss = -12339.2587890625
1
Iteration 22600: Loss = -12339.259765625
2
Iteration 22700: Loss = -12339.259765625
3
Iteration 22800: Loss = -12339.2578125
4
Iteration 22900: Loss = -12339.2587890625
5
Iteration 23000: Loss = -12339.2578125
6
Iteration 23100: Loss = -12339.2568359375
Iteration 23200: Loss = -12339.2578125
1
Iteration 23300: Loss = -12339.259765625
2
Iteration 23400: Loss = -12339.2587890625
3
Iteration 23500: Loss = -12339.2568359375
Iteration 23600: Loss = -12339.2587890625
1
Iteration 23700: Loss = -12339.2587890625
2
Iteration 23800: Loss = -12339.2587890625
3
Iteration 23900: Loss = -12339.2587890625
4
Iteration 24000: Loss = -12339.259765625
5
Iteration 24100: Loss = -12339.259765625
6
Iteration 24200: Loss = -12339.2587890625
7
Iteration 24300: Loss = -12339.2568359375
Iteration 24400: Loss = -12339.2578125
1
Iteration 24500: Loss = -12339.2578125
2
Iteration 24600: Loss = -12339.2578125
3
Iteration 24700: Loss = -12339.2607421875
4
Iteration 24800: Loss = -12339.2578125
5
Iteration 24900: Loss = -12339.2587890625
6
Iteration 25000: Loss = -12339.2578125
7
Iteration 25100: Loss = -12339.2578125
8
Iteration 25200: Loss = -12339.2587890625
9
Iteration 25300: Loss = -12339.255859375
Iteration 25400: Loss = -12339.2578125
1
Iteration 25500: Loss = -12339.259765625
2
Iteration 25600: Loss = -12339.2587890625
3
Iteration 25700: Loss = -12339.2587890625
4
Iteration 25800: Loss = -12339.2587890625
5
Iteration 25900: Loss = -12339.2587890625
6
Iteration 26000: Loss = -12339.2587890625
7
Iteration 26100: Loss = -12339.2578125
8
Iteration 26200: Loss = -12339.2578125
9
Iteration 26300: Loss = -12339.2587890625
10
Iteration 26400: Loss = -12339.259765625
11
Iteration 26500: Loss = -12339.2578125
12
Iteration 26600: Loss = -12339.259765625
13
Iteration 26700: Loss = -12339.2578125
14
Iteration 26800: Loss = -12339.2578125
15
Stopping early at iteration 26800 due to no improvement.
pi: tensor([[9.9494e-01, 5.0649e-03],
        [1.2715e-04, 9.9987e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9954, 0.0046], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1984, 0.1658],
         [0.8521, 0.8252]],

        [[0.4578, 0.2200],
         [0.9502, 0.9848]],

        [[0.1646, 0.2844],
         [0.8424, 0.4701]],

        [[0.9843, 0.2515],
         [0.9419, 0.9924]],

        [[0.7349, 0.1283],
         [0.8307, 0.0313]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: -8.821581797071552e-05
Average Adjusted Rand Index: 0.0001728471963116079
[-0.0017325615910433053, -8.821581797071552e-05] [0.0004885496183206107, 0.0001728471963116079] [12339.291015625, 12339.2578125]
-------------------------------------
This iteration is 17
True Objective function: Loss = -11877.750482742535
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -32178.380859375
Iteration 100: Loss = -20977.18359375
Iteration 200: Loss = -14174.5380859375
Iteration 300: Loss = -13052.9375
Iteration 400: Loss = -12803.6982421875
Iteration 500: Loss = -12722.6123046875
Iteration 600: Loss = -12686.255859375
Iteration 700: Loss = -12662.9296875
Iteration 800: Loss = -12647.216796875
Iteration 900: Loss = -12633.462890625
Iteration 1000: Loss = -12623.21484375
Iteration 1100: Loss = -12612.052734375
Iteration 1200: Loss = -12601.8759765625
Iteration 1300: Loss = -12592.76171875
Iteration 1400: Loss = -12583.0419921875
Iteration 1500: Loss = -12576.8115234375
Iteration 1600: Loss = -12570.740234375
Iteration 1700: Loss = -12564.564453125
Iteration 1800: Loss = -12557.96484375
Iteration 1900: Loss = -12552.8251953125
Iteration 2000: Loss = -12548.0205078125
Iteration 2100: Loss = -12544.2265625
Iteration 2200: Loss = -12539.3837890625
Iteration 2300: Loss = -12535.697265625
Iteration 2400: Loss = -12533.1650390625
Iteration 2500: Loss = -12531.1123046875
Iteration 2600: Loss = -12529.0556640625
Iteration 2700: Loss = -12526.78125
Iteration 2800: Loss = -12524.7578125
Iteration 2900: Loss = -12522.541015625
Iteration 3000: Loss = -12520.5546875
Iteration 3100: Loss = -12518.681640625
Iteration 3200: Loss = -12517.2724609375
Iteration 3300: Loss = -12515.0400390625
Iteration 3400: Loss = -12513.3251953125
Iteration 3500: Loss = -12511.654296875
Iteration 3600: Loss = -12510.6357421875
Iteration 3700: Loss = -12509.8935546875
Iteration 3800: Loss = -12508.82421875
Iteration 3900: Loss = -12508.2744140625
Iteration 4000: Loss = -12507.7744140625
Iteration 4100: Loss = -12507.28125
Iteration 4200: Loss = -12506.8447265625
Iteration 4300: Loss = -12506.49609375
Iteration 4400: Loss = -12506.2109375
Iteration 4500: Loss = -12505.955078125
Iteration 4600: Loss = -12505.029296875
Iteration 4700: Loss = -12504.419921875
Iteration 4800: Loss = -12503.845703125
Iteration 4900: Loss = -12503.546875
Iteration 5000: Loss = -12502.1083984375
Iteration 5100: Loss = -12501.2705078125
Iteration 5200: Loss = -12500.962890625
Iteration 5300: Loss = -12500.7646484375
Iteration 5400: Loss = -12500.0419921875
Iteration 5500: Loss = -12499.8720703125
Iteration 5600: Loss = -12499.619140625
Iteration 5700: Loss = -12499.236328125
Iteration 5800: Loss = -12498.951171875
Iteration 5900: Loss = -12498.5341796875
Iteration 6000: Loss = -12497.3271484375
Iteration 6100: Loss = -12496.3974609375
Iteration 6200: Loss = -12496.0615234375
Iteration 6300: Loss = -12495.83984375
Iteration 6400: Loss = -12495.662109375
Iteration 6500: Loss = -12494.658203125
Iteration 6600: Loss = -12494.4501953125
Iteration 6700: Loss = -12494.2294921875
Iteration 6800: Loss = -12493.955078125
Iteration 6900: Loss = -12493.6357421875
Iteration 7000: Loss = -12493.328125
Iteration 7100: Loss = -12493.095703125
Iteration 7200: Loss = -12492.93359375
Iteration 7300: Loss = -12492.818359375
Iteration 7400: Loss = -12492.7275390625
Iteration 7500: Loss = -12492.537109375
Iteration 7600: Loss = -12488.7998046875
Iteration 7700: Loss = -12487.7294921875
Iteration 7800: Loss = -12485.2109375
Iteration 7900: Loss = -12484.9111328125
Iteration 8000: Loss = -12484.419921875
Iteration 8100: Loss = -12484.244140625
Iteration 8200: Loss = -12481.59375
Iteration 8300: Loss = -12479.5400390625
Iteration 8400: Loss = -12478.630859375
Iteration 8500: Loss = -12474.65625
Iteration 8600: Loss = -12473.3173828125
Iteration 8700: Loss = -12470.88671875
Iteration 8800: Loss = -12468.6572265625
Iteration 8900: Loss = -12465.28515625
Iteration 9000: Loss = -12465.1923828125
Iteration 9100: Loss = -12464.8623046875
Iteration 9200: Loss = -12462.0537109375
Iteration 9300: Loss = -12460.798828125
Iteration 9400: Loss = -12459.1630859375
Iteration 9500: Loss = -12457.2265625
Iteration 9600: Loss = -12452.4384765625
Iteration 9700: Loss = -12446.771484375
Iteration 9800: Loss = -12443.3212890625
Iteration 9900: Loss = -12440.166015625
Iteration 10000: Loss = -12432.4287109375
Iteration 10100: Loss = -12422.3544921875
Iteration 10200: Loss = -12415.2109375
Iteration 10300: Loss = -12407.3447265625
Iteration 10400: Loss = -12383.771484375
Iteration 10500: Loss = -12328.603515625
Iteration 10600: Loss = -12316.8134765625
Iteration 10700: Loss = -12313.8818359375
Iteration 10800: Loss = -12312.3671875
Iteration 10900: Loss = -12312.19140625
Iteration 11000: Loss = -12312.0703125
Iteration 11100: Loss = -12311.9619140625
Iteration 11200: Loss = -12304.6142578125
Iteration 11300: Loss = -12298.8505859375
Iteration 11400: Loss = -12290.8828125
Iteration 11500: Loss = -12275.6796875
Iteration 11600: Loss = -12275.3359375
Iteration 11700: Loss = -12275.2548828125
Iteration 11800: Loss = -12275.2119140625
Iteration 11900: Loss = -12275.18359375
Iteration 12000: Loss = -12275.138671875
Iteration 12100: Loss = -12267.4814453125
Iteration 12200: Loss = -12267.390625
Iteration 12300: Loss = -12267.34375
Iteration 12400: Loss = -12263.7275390625
Iteration 12500: Loss = -12262.5234375
Iteration 12600: Loss = -12260.6044921875
Iteration 12700: Loss = -12258.6630859375
Iteration 12800: Loss = -12257.7275390625
Iteration 12900: Loss = -12255.689453125
Iteration 13000: Loss = -12250.4169921875
Iteration 13100: Loss = -12239.240234375
Iteration 13200: Loss = -12237.8115234375
Iteration 13300: Loss = -12234.25390625
Iteration 13400: Loss = -12229.6201171875
Iteration 13500: Loss = -12216.298828125
Iteration 13600: Loss = -12206.6201171875
Iteration 13700: Loss = -12191.708984375
Iteration 13800: Loss = -12186.271484375
Iteration 13900: Loss = -12186.1376953125
Iteration 14000: Loss = -12186.0703125
Iteration 14100: Loss = -12164.2119140625
Iteration 14200: Loss = -12162.2734375
Iteration 14300: Loss = -12156.9541015625
Iteration 14400: Loss = -12153.78125
Iteration 14500: Loss = -12153.703125
Iteration 14600: Loss = -12144.4912109375
Iteration 14700: Loss = -12140.3486328125
Iteration 14800: Loss = -12129.0517578125
Iteration 14900: Loss = -12128.8408203125
Iteration 15000: Loss = -12128.7763671875
Iteration 15100: Loss = -12128.740234375
Iteration 15200: Loss = -12128.7177734375
Iteration 15300: Loss = -12128.7001953125
Iteration 15400: Loss = -12128.6865234375
Iteration 15500: Loss = -12128.67578125
Iteration 15600: Loss = -12128.66796875
Iteration 15700: Loss = -12128.6591796875
Iteration 15800: Loss = -12128.6533203125
Iteration 15900: Loss = -12128.6484375
Iteration 16000: Loss = -12128.6435546875
Iteration 16100: Loss = -12128.638671875
Iteration 16200: Loss = -12128.63671875
Iteration 16300: Loss = -12128.6328125
Iteration 16400: Loss = -12128.6298828125
Iteration 16500: Loss = -12128.626953125
Iteration 16600: Loss = -12128.625
Iteration 16700: Loss = -12128.623046875
Iteration 16800: Loss = -12128.6201171875
Iteration 16900: Loss = -12128.62109375
1
Iteration 17000: Loss = -12128.6171875
Iteration 17100: Loss = -12128.6162109375
Iteration 17200: Loss = -12128.615234375
Iteration 17300: Loss = -12128.61328125
Iteration 17400: Loss = -12128.61328125
Iteration 17500: Loss = -12128.611328125
Iteration 17600: Loss = -12128.6103515625
Iteration 17700: Loss = -12128.609375
Iteration 17800: Loss = -12128.6083984375
Iteration 17900: Loss = -12128.607421875
Iteration 18000: Loss = -12128.6064453125
Iteration 18100: Loss = -12128.6064453125
Iteration 18200: Loss = -12128.60546875
Iteration 18300: Loss = -12128.6064453125
1
Iteration 18400: Loss = -12128.6044921875
Iteration 18500: Loss = -12128.6044921875
Iteration 18600: Loss = -12128.603515625
Iteration 18700: Loss = -12128.6044921875
1
Iteration 18800: Loss = -12128.60546875
2
Iteration 18900: Loss = -12128.6025390625
Iteration 19000: Loss = -12128.6015625
Iteration 19100: Loss = -12128.6015625
Iteration 19200: Loss = -12128.6025390625
1
Iteration 19300: Loss = -12128.603515625
2
Iteration 19400: Loss = -12128.6005859375
Iteration 19500: Loss = -12128.6015625
1
Iteration 19600: Loss = -12128.599609375
Iteration 19700: Loss = -12128.599609375
Iteration 19800: Loss = -12128.599609375
Iteration 19900: Loss = -12128.599609375
Iteration 20000: Loss = -12128.6005859375
1
Iteration 20100: Loss = -12128.599609375
Iteration 20200: Loss = -12128.599609375
Iteration 20300: Loss = -12128.5986328125
Iteration 20400: Loss = -12128.59765625
Iteration 20500: Loss = -12117.6630859375
Iteration 20600: Loss = -12117.5048828125
Iteration 20700: Loss = -12117.4697265625
Iteration 20800: Loss = -12117.4521484375
Iteration 20900: Loss = -12117.4443359375
Iteration 21000: Loss = -12117.4384765625
Iteration 21100: Loss = -12117.43359375
Iteration 21200: Loss = -12117.4296875
Iteration 21300: Loss = -12117.427734375
Iteration 21400: Loss = -12117.4267578125
Iteration 21500: Loss = -12117.423828125
Iteration 21600: Loss = -12117.4228515625
Iteration 21700: Loss = -12117.4228515625
Iteration 21800: Loss = -12117.4228515625
Iteration 21900: Loss = -12117.4208984375
Iteration 22000: Loss = -12117.419921875
Iteration 22100: Loss = -12117.4189453125
Iteration 22200: Loss = -12117.419921875
1
Iteration 22300: Loss = -12117.41796875
Iteration 22400: Loss = -12117.41796875
Iteration 22500: Loss = -12117.41796875
Iteration 22600: Loss = -12117.4169921875
Iteration 22700: Loss = -12116.7177734375
Iteration 22800: Loss = -12112.2548828125
Iteration 22900: Loss = -12112.236328125
Iteration 23000: Loss = -12112.2294921875
Iteration 23100: Loss = -12112.2255859375
Iteration 23200: Loss = -12112.224609375
Iteration 23300: Loss = -12112.2236328125
Iteration 23400: Loss = -12112.2216796875
Iteration 23500: Loss = -12112.220703125
Iteration 23600: Loss = -12112.2197265625
Iteration 23700: Loss = -12112.220703125
1
Iteration 23800: Loss = -12112.2197265625
Iteration 23900: Loss = -12112.2197265625
Iteration 24000: Loss = -12112.220703125
1
Iteration 24100: Loss = -12112.2177734375
Iteration 24200: Loss = -12112.2197265625
1
Iteration 24300: Loss = -12112.216796875
Iteration 24400: Loss = -12112.21875
1
Iteration 24500: Loss = -12112.2197265625
2
Iteration 24600: Loss = -12112.216796875
Iteration 24700: Loss = -12112.21875
1
Iteration 24800: Loss = -12112.216796875
Iteration 24900: Loss = -12112.216796875
Iteration 25000: Loss = -12112.21875
1
Iteration 25100: Loss = -12112.21875
2
Iteration 25200: Loss = -12112.2177734375
3
Iteration 25300: Loss = -12112.2177734375
4
Iteration 25400: Loss = -12112.21875
5
Iteration 25500: Loss = -12112.2177734375
6
Iteration 25600: Loss = -12112.216796875
Iteration 25700: Loss = -12112.2177734375
1
Iteration 25800: Loss = -12112.216796875
Iteration 25900: Loss = -12112.216796875
Iteration 26000: Loss = -12112.2177734375
1
Iteration 26100: Loss = -12112.216796875
Iteration 26200: Loss = -12112.216796875
Iteration 26300: Loss = -12112.2158203125
Iteration 26400: Loss = -12112.2177734375
1
Iteration 26500: Loss = -12112.216796875
2
Iteration 26600: Loss = -12112.216796875
3
Iteration 26700: Loss = -12112.216796875
4
Iteration 26800: Loss = -12112.216796875
5
Iteration 26900: Loss = -12112.216796875
6
Iteration 27000: Loss = -12112.2177734375
7
Iteration 27100: Loss = -12112.2177734375
8
Iteration 27200: Loss = -12112.2177734375
9
Iteration 27300: Loss = -12112.2158203125
Iteration 27400: Loss = -12112.20703125
Iteration 27500: Loss = -12104.8203125
Iteration 27600: Loss = -12104.7421875
Iteration 27700: Loss = -12104.72265625
Iteration 27800: Loss = -12104.71484375
Iteration 27900: Loss = -12104.7109375
Iteration 28000: Loss = -12104.7080078125
Iteration 28100: Loss = -12104.70703125
Iteration 28200: Loss = -12104.705078125
Iteration 28300: Loss = -12104.7041015625
Iteration 28400: Loss = -12104.703125
Iteration 28500: Loss = -12104.701171875
Iteration 28600: Loss = -12104.7021484375
1
Iteration 28700: Loss = -12104.701171875
Iteration 28800: Loss = -12104.701171875
Iteration 28900: Loss = -12104.7001953125
Iteration 29000: Loss = -12104.701171875
1
Iteration 29100: Loss = -12104.7001953125
Iteration 29200: Loss = -12104.69921875
Iteration 29300: Loss = -12104.666015625
Iteration 29400: Loss = -12104.6669921875
1
Iteration 29500: Loss = -12104.6669921875
2
Iteration 29600: Loss = -12104.666015625
Iteration 29700: Loss = -12104.6669921875
1
Iteration 29800: Loss = -12104.6669921875
2
Iteration 29900: Loss = -12104.6669921875
3
pi: tensor([[0.7766, 0.2234],
        [0.5573, 0.4427]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5023, 0.4977], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2558, 0.0998],
         [0.5924, 0.2879]],

        [[0.0409, 0.1038],
         [0.2560, 0.0466]],

        [[0.8650, 0.7527],
         [0.7395, 0.0088]],

        [[0.8083, 0.0964],
         [0.0196, 0.9796]],

        [[0.7749, 0.0956],
         [0.1300, 0.9488]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 4
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9598877889442053
Global Adjusted Rand Index: 0.6645200563994271
Average Adjusted Rand Index: 0.7839747044911609
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37926.0859375
Iteration 100: Loss = -21457.380859375
Iteration 200: Loss = -13933.54296875
Iteration 300: Loss = -12824.63671875
Iteration 400: Loss = -12601.6484375
Iteration 500: Loss = -12497.4091796875
Iteration 600: Loss = -12455.3828125
Iteration 700: Loss = -12431.5712890625
Iteration 800: Loss = -12414.7646484375
Iteration 900: Loss = -12401.9853515625
Iteration 1000: Loss = -12389.2314453125
Iteration 1100: Loss = -12383.0322265625
Iteration 1200: Loss = -12378.9873046875
Iteration 1300: Loss = -12375.9453125
Iteration 1400: Loss = -12373.5537109375
Iteration 1500: Loss = -12371.6171875
Iteration 1600: Loss = -12370.0146484375
Iteration 1700: Loss = -12368.666015625
Iteration 1800: Loss = -12367.51171875
Iteration 1900: Loss = -12366.5107421875
Iteration 2000: Loss = -12365.6357421875
Iteration 2100: Loss = -12364.8623046875
Iteration 2200: Loss = -12364.171875
Iteration 2300: Loss = -12363.5546875
Iteration 2400: Loss = -12362.998046875
Iteration 2500: Loss = -12362.494140625
Iteration 2600: Loss = -12362.0341796875
Iteration 2700: Loss = -12361.615234375
Iteration 2800: Loss = -12361.23046875
Iteration 2900: Loss = -12360.87890625
Iteration 3000: Loss = -12360.5537109375
Iteration 3100: Loss = -12360.255859375
Iteration 3200: Loss = -12359.9775390625
Iteration 3300: Loss = -12359.720703125
Iteration 3400: Loss = -12359.482421875
Iteration 3500: Loss = -12359.2607421875
Iteration 3600: Loss = -12359.0546875
Iteration 3700: Loss = -12358.86328125
Iteration 3800: Loss = -12358.6826171875
Iteration 3900: Loss = -12358.5146484375
Iteration 4000: Loss = -12358.357421875
Iteration 4100: Loss = -12358.2099609375
Iteration 4200: Loss = -12358.0732421875
Iteration 4300: Loss = -12357.9423828125
Iteration 4400: Loss = -12357.8212890625
Iteration 4500: Loss = -12357.70703125
Iteration 4600: Loss = -12357.599609375
Iteration 4700: Loss = -12357.4970703125
Iteration 4800: Loss = -12357.404296875
Iteration 4900: Loss = -12357.314453125
Iteration 5000: Loss = -12357.23046875
Iteration 5100: Loss = -12357.1494140625
Iteration 5200: Loss = -12357.076171875
Iteration 5300: Loss = -12357.0048828125
Iteration 5400: Loss = -12356.9384765625
Iteration 5500: Loss = -12356.876953125
Iteration 5600: Loss = -12356.8154296875
Iteration 5700: Loss = -12356.759765625
Iteration 5800: Loss = -12356.70703125
Iteration 5900: Loss = -12356.65625
Iteration 6000: Loss = -12356.6103515625
Iteration 6100: Loss = -12356.5654296875
Iteration 6200: Loss = -12356.5234375
Iteration 6300: Loss = -12356.4814453125
Iteration 6400: Loss = -12356.4443359375
Iteration 6500: Loss = -12356.408203125
Iteration 6600: Loss = -12356.375
Iteration 6700: Loss = -12356.34375
Iteration 6800: Loss = -12356.3115234375
Iteration 6900: Loss = -12356.283203125
Iteration 7000: Loss = -12356.2548828125
Iteration 7100: Loss = -12356.2294921875
Iteration 7200: Loss = -12356.2041015625
Iteration 7300: Loss = -12356.1826171875
Iteration 7400: Loss = -12356.158203125
Iteration 7500: Loss = -12356.13671875
Iteration 7600: Loss = -12356.1171875
Iteration 7700: Loss = -12356.0966796875
Iteration 7800: Loss = -12356.078125
Iteration 7900: Loss = -12356.0625
Iteration 8000: Loss = -12356.0458984375
Iteration 8100: Loss = -12356.0302734375
Iteration 8200: Loss = -12356.0146484375
Iteration 8300: Loss = -12356.0009765625
Iteration 8400: Loss = -12355.9853515625
Iteration 8500: Loss = -12355.9736328125
Iteration 8600: Loss = -12355.9638671875
Iteration 8700: Loss = -12355.9482421875
Iteration 8800: Loss = -12355.9365234375
Iteration 8900: Loss = -12355.923828125
Iteration 9000: Loss = -12355.9130859375
Iteration 9100: Loss = -12355.9033203125
Iteration 9200: Loss = -12355.8916015625
Iteration 9300: Loss = -12355.8818359375
Iteration 9400: Loss = -12355.8720703125
Iteration 9500: Loss = -12355.8603515625
Iteration 9600: Loss = -12355.8525390625
Iteration 9700: Loss = -12355.841796875
Iteration 9800: Loss = -12355.8291015625
Iteration 9900: Loss = -12355.8193359375
Iteration 10000: Loss = -12355.8076171875
Iteration 10100: Loss = -12355.794921875
Iteration 10200: Loss = -12355.7822265625
Iteration 10300: Loss = -12355.763671875
Iteration 10400: Loss = -12355.7490234375
Iteration 10500: Loss = -12355.7333984375
Iteration 10600: Loss = -12355.7177734375
Iteration 10700: Loss = -12355.705078125
Iteration 10800: Loss = -12355.693359375
Iteration 10900: Loss = -12355.6826171875
Iteration 11000: Loss = -12355.6708984375
Iteration 11100: Loss = -12355.6611328125
Iteration 11200: Loss = -12355.6513671875
Iteration 11300: Loss = -12355.63671875
Iteration 11400: Loss = -12355.6279296875
Iteration 11500: Loss = -12355.615234375
Iteration 11600: Loss = -12355.6064453125
Iteration 11700: Loss = -12355.59375
Iteration 11800: Loss = -12355.5810546875
Iteration 11900: Loss = -12355.5673828125
Iteration 12000: Loss = -12355.5556640625
Iteration 12100: Loss = -12355.5419921875
Iteration 12200: Loss = -12355.5302734375
Iteration 12300: Loss = -12355.517578125
Iteration 12400: Loss = -12355.5048828125
Iteration 12500: Loss = -12355.48828125
Iteration 12600: Loss = -12355.47265625
Iteration 12700: Loss = -12355.455078125
Iteration 12800: Loss = -12355.4365234375
Iteration 12900: Loss = -12355.4111328125
Iteration 13000: Loss = -12355.3828125
Iteration 13100: Loss = -12355.3447265625
Iteration 13200: Loss = -12355.2939453125
Iteration 13300: Loss = -12355.228515625
Iteration 13400: Loss = -12355.1201171875
Iteration 13500: Loss = -12354.9091796875
Iteration 13600: Loss = -12352.86328125
Iteration 13700: Loss = -12352.076171875
Iteration 13800: Loss = -12351.62109375
Iteration 13900: Loss = -12350.5908203125
Iteration 14000: Loss = -12349.9755859375
Iteration 14100: Loss = -12349.8662109375
Iteration 14200: Loss = -12349.7626953125
Iteration 14300: Loss = -12349.7138671875
Iteration 14400: Loss = -12349.66796875
Iteration 14500: Loss = -12349.623046875
Iteration 14600: Loss = -12349.580078125
Iteration 14700: Loss = -12349.5458984375
Iteration 14800: Loss = -12349.525390625
Iteration 14900: Loss = -12349.5166015625
Iteration 15000: Loss = -12349.509765625
Iteration 15100: Loss = -12349.5048828125
Iteration 15200: Loss = -12349.5
Iteration 15300: Loss = -12349.498046875
Iteration 15400: Loss = -12349.49609375
Iteration 15500: Loss = -12349.4931640625
Iteration 15600: Loss = -12349.4931640625
Iteration 15700: Loss = -12349.4921875
Iteration 15800: Loss = -12349.4912109375
Iteration 15900: Loss = -12349.490234375
Iteration 16000: Loss = -12349.48828125
Iteration 16100: Loss = -12349.48828125
Iteration 16200: Loss = -12349.48828125
Iteration 16300: Loss = -12349.4873046875
Iteration 16400: Loss = -12349.48828125
1
Iteration 16500: Loss = -12349.4873046875
Iteration 16600: Loss = -12349.4873046875
Iteration 16700: Loss = -12349.486328125
Iteration 16800: Loss = -12349.486328125
Iteration 16900: Loss = -12349.486328125
Iteration 17000: Loss = -12349.486328125
Iteration 17100: Loss = -12349.486328125
Iteration 17200: Loss = -12349.484375
Iteration 17300: Loss = -12349.4853515625
1
Iteration 17400: Loss = -12349.484375
Iteration 17500: Loss = -12349.4833984375
Iteration 17600: Loss = -12349.482421875
Iteration 17700: Loss = -12349.4833984375
1
Iteration 17800: Loss = -12349.4873046875
2
Iteration 17900: Loss = -12349.482421875
Iteration 18000: Loss = -12349.4814453125
Iteration 18100: Loss = -12349.4833984375
1
Iteration 18200: Loss = -12349.4833984375
2
Iteration 18300: Loss = -12349.4853515625
3
Iteration 18400: Loss = -12349.482421875
4
Iteration 18500: Loss = -12349.482421875
5
Iteration 18600: Loss = -12349.482421875
6
Iteration 18700: Loss = -12349.4814453125
Iteration 18800: Loss = -12349.4833984375
1
Iteration 18900: Loss = -12349.4833984375
2
Iteration 19000: Loss = -12349.482421875
3
Iteration 19100: Loss = -12349.482421875
4
Iteration 19200: Loss = -12349.4814453125
Iteration 19300: Loss = -12349.4833984375
1
Iteration 19400: Loss = -12349.482421875
2
Iteration 19500: Loss = -12349.4814453125
Iteration 19600: Loss = -12349.482421875
1
Iteration 19700: Loss = -12349.4814453125
Iteration 19800: Loss = -12349.482421875
1
Iteration 19900: Loss = -12349.482421875
2
Iteration 20000: Loss = -12349.4814453125
Iteration 20100: Loss = -12349.482421875
1
Iteration 20200: Loss = -12349.4814453125
Iteration 20300: Loss = -12349.482421875
1
Iteration 20400: Loss = -12349.482421875
2
Iteration 20500: Loss = -12349.48046875
Iteration 20600: Loss = -12349.4814453125
1
Iteration 20700: Loss = -12349.48046875
Iteration 20800: Loss = -12349.48046875
Iteration 20900: Loss = -12349.4814453125
1
Iteration 21000: Loss = -12349.4833984375
2
Iteration 21100: Loss = -12349.482421875
3
Iteration 21200: Loss = -12349.4814453125
4
Iteration 21300: Loss = -12349.4814453125
5
Iteration 21400: Loss = -12349.48046875
Iteration 21500: Loss = -12349.48046875
Iteration 21600: Loss = -12349.484375
1
Iteration 21700: Loss = -12349.48046875
Iteration 21800: Loss = -12349.4833984375
1
Iteration 21900: Loss = -12349.48046875
Iteration 22000: Loss = -12349.4814453125
1
Iteration 22100: Loss = -12349.48046875
Iteration 22200: Loss = -12349.4814453125
1
Iteration 22300: Loss = -12349.482421875
2
Iteration 22400: Loss = -12349.482421875
3
Iteration 22500: Loss = -12349.4814453125
4
Iteration 22600: Loss = -12349.48046875
Iteration 22700: Loss = -12349.482421875
1
Iteration 22800: Loss = -12349.4814453125
2
Iteration 22900: Loss = -12349.4814453125
3
Iteration 23000: Loss = -12349.4833984375
4
Iteration 23100: Loss = -12349.48046875
Iteration 23200: Loss = -12349.4814453125
1
Iteration 23300: Loss = -12349.48046875
Iteration 23400: Loss = -12349.482421875
1
Iteration 23500: Loss = -12349.4814453125
2
Iteration 23600: Loss = -12349.48046875
Iteration 23700: Loss = -12349.482421875
1
Iteration 23800: Loss = -12349.4814453125
2
Iteration 23900: Loss = -12349.4814453125
3
Iteration 24000: Loss = -12349.4814453125
4
Iteration 24100: Loss = -12349.482421875
5
Iteration 24200: Loss = -12349.482421875
6
Iteration 24300: Loss = -12349.4814453125
7
Iteration 24400: Loss = -12349.48046875
Iteration 24500: Loss = -12349.4814453125
1
Iteration 24600: Loss = -12349.482421875
2
Iteration 24700: Loss = -12349.4814453125
3
Iteration 24800: Loss = -12349.482421875
4
Iteration 24900: Loss = -12349.48046875
Iteration 25000: Loss = -12349.482421875
1
Iteration 25100: Loss = -12349.4814453125
2
Iteration 25200: Loss = -12349.4814453125
3
Iteration 25300: Loss = -12349.4814453125
4
Iteration 25400: Loss = -12349.4814453125
5
Iteration 25500: Loss = -12349.4814453125
6
Iteration 25600: Loss = -12349.4814453125
7
Iteration 25700: Loss = -12349.482421875
8
Iteration 25800: Loss = -12349.4814453125
9
Iteration 25900: Loss = -12349.482421875
10
Iteration 26000: Loss = -12349.4814453125
11
Iteration 26100: Loss = -12349.48046875
Iteration 26200: Loss = -12349.482421875
1
Iteration 26300: Loss = -12349.48046875
Iteration 26400: Loss = -12349.48046875
Iteration 26500: Loss = -12349.482421875
1
Iteration 26600: Loss = -12349.48046875
Iteration 26700: Loss = -12349.48046875
Iteration 26800: Loss = -12349.4814453125
1
Iteration 26900: Loss = -12349.48046875
Iteration 27000: Loss = -12349.4814453125
1
Iteration 27100: Loss = -12349.4814453125
2
Iteration 27200: Loss = -12349.48046875
Iteration 27300: Loss = -12349.482421875
1
Iteration 27400: Loss = -12349.482421875
2
Iteration 27500: Loss = -12349.482421875
3
Iteration 27600: Loss = -12349.48046875
Iteration 27700: Loss = -12349.4814453125
1
Iteration 27800: Loss = -12349.482421875
2
Iteration 27900: Loss = -12349.482421875
3
Iteration 28000: Loss = -12349.48046875
Iteration 28100: Loss = -12349.4814453125
1
Iteration 28200: Loss = -12349.4794921875
Iteration 28300: Loss = -12349.4814453125
1
Iteration 28400: Loss = -12349.4814453125
2
Iteration 28500: Loss = -12349.48046875
3
Iteration 28600: Loss = -12349.48046875
4
Iteration 28700: Loss = -12349.4814453125
5
Iteration 28800: Loss = -12349.4814453125
6
Iteration 28900: Loss = -12349.482421875
7
Iteration 29000: Loss = -12349.482421875
8
Iteration 29100: Loss = -12349.4814453125
9
Iteration 29200: Loss = -12349.48046875
10
Iteration 29300: Loss = -12349.4814453125
11
Iteration 29400: Loss = -12349.4814453125
12
Iteration 29500: Loss = -12349.4814453125
13
Iteration 29600: Loss = -12349.4814453125
14
Iteration 29700: Loss = -12349.4814453125
15
Stopping early at iteration 29700 due to no improvement.
pi: tensor([[6.7286e-06, 9.9999e-01],
        [1.0652e-01, 8.9348e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0091, 0.9909], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3755, 0.1409],
         [0.1102, 0.1973]],

        [[0.9624, 0.1149],
         [0.9822, 0.0798]],

        [[0.9751, 0.2238],
         [0.1886, 0.4227]],

        [[0.9901, 0.2426],
         [0.9705, 0.1795]],

        [[0.9212, 0.2465],
         [0.2239, 0.9577]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 29
Adjusted Rand Index: 0.15255865002057895
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.01717781179455718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: -0.025916162480371957
Global Adjusted Rand Index: 0.006461389076221106
Average Adjusted Rand Index: 0.02189293514912996
[0.6645200563994271, 0.006461389076221106] [0.7839747044911609, 0.02189293514912996] [12104.666015625, 12349.4814453125]
-------------------------------------
This iteration is 18
True Objective function: Loss = -11820.840347016367
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17557.20703125
Iteration 100: Loss = -13765.9658203125
Iteration 200: Loss = -12636.779296875
Iteration 300: Loss = -12414.3056640625
Iteration 400: Loss = -12359.1181640625
Iteration 500: Loss = -12331.6240234375
Iteration 600: Loss = -12317.181640625
Iteration 700: Loss = -12310.357421875
Iteration 800: Loss = -12307.109375
Iteration 900: Loss = -12304.2431640625
Iteration 1000: Loss = -12301.9482421875
Iteration 1100: Loss = -12300.994140625
Iteration 1200: Loss = -12300.3583984375
Iteration 1300: Loss = -12299.89453125
Iteration 1400: Loss = -12299.5556640625
Iteration 1500: Loss = -12299.291015625
Iteration 1600: Loss = -12299.076171875
Iteration 1700: Loss = -12298.9189453125
Iteration 1800: Loss = -12298.8115234375
Iteration 1900: Loss = -12298.734375
Iteration 2000: Loss = -12298.67578125
Iteration 2100: Loss = -12298.6298828125
Iteration 2200: Loss = -12298.58984375
Iteration 2300: Loss = -12298.552734375
Iteration 2400: Loss = -12298.5224609375
Iteration 2500: Loss = -12298.494140625
Iteration 2600: Loss = -12298.4677734375
Iteration 2700: Loss = -12298.4453125
Iteration 2800: Loss = -12298.423828125
Iteration 2900: Loss = -12298.3994140625
Iteration 3000: Loss = -12298.3779296875
Iteration 3100: Loss = -12298.357421875
Iteration 3200: Loss = -12298.33984375
Iteration 3300: Loss = -12298.322265625
Iteration 3400: Loss = -12298.3046875
Iteration 3500: Loss = -12298.2900390625
Iteration 3600: Loss = -12298.2734375
Iteration 3700: Loss = -12298.2587890625
Iteration 3800: Loss = -12298.244140625
Iteration 3900: Loss = -12298.228515625
Iteration 4000: Loss = -12298.212890625
Iteration 4100: Loss = -12298.197265625
Iteration 4200: Loss = -12298.1806640625
Iteration 4300: Loss = -12298.169921875
Iteration 4400: Loss = -12298.1572265625
Iteration 4500: Loss = -12298.1455078125
Iteration 4600: Loss = -12298.134765625
Iteration 4700: Loss = -12298.125
Iteration 4800: Loss = -12298.1162109375
Iteration 4900: Loss = -12298.107421875
Iteration 5000: Loss = -12298.09765625
Iteration 5100: Loss = -12298.0859375
Iteration 5200: Loss = -12298.0751953125
Iteration 5300: Loss = -12298.0654296875
Iteration 5400: Loss = -12298.0537109375
Iteration 5500: Loss = -12298.044921875
Iteration 5600: Loss = -12298.0322265625
Iteration 5700: Loss = -12298.0185546875
Iteration 5800: Loss = -12298.005859375
Iteration 5900: Loss = -12297.9892578125
Iteration 6000: Loss = -12297.96875
Iteration 6100: Loss = -12297.943359375
Iteration 6200: Loss = -12297.9111328125
Iteration 6300: Loss = -12297.8681640625
Iteration 6400: Loss = -12297.814453125
Iteration 6500: Loss = -12297.74609375
Iteration 6600: Loss = -12297.6708984375
Iteration 6700: Loss = -12297.5966796875
Iteration 6800: Loss = -12297.52734375
Iteration 6900: Loss = -12297.46875
Iteration 7000: Loss = -12297.416015625
Iteration 7100: Loss = -12297.3720703125
Iteration 7200: Loss = -12297.33203125
Iteration 7300: Loss = -12297.296875
Iteration 7400: Loss = -12297.26171875
Iteration 7500: Loss = -12297.234375
Iteration 7600: Loss = -12297.2080078125
Iteration 7700: Loss = -12297.1845703125
Iteration 7800: Loss = -12297.1650390625
Iteration 7900: Loss = -12297.146484375
Iteration 8000: Loss = -12297.130859375
Iteration 8100: Loss = -12297.1171875
Iteration 8200: Loss = -12297.1015625
Iteration 8300: Loss = -12297.091796875
Iteration 8400: Loss = -12297.080078125
Iteration 8500: Loss = -12297.0712890625
Iteration 8600: Loss = -12297.0625
Iteration 8700: Loss = -12297.0546875
Iteration 8800: Loss = -12297.0478515625
Iteration 8900: Loss = -12297.0419921875
Iteration 9000: Loss = -12297.03515625
Iteration 9100: Loss = -12297.0302734375
Iteration 9200: Loss = -12297.025390625
Iteration 9300: Loss = -12297.021484375
Iteration 9400: Loss = -12297.017578125
Iteration 9500: Loss = -12297.013671875
Iteration 9600: Loss = -12297.009765625
Iteration 9700: Loss = -12297.005859375
Iteration 9800: Loss = -12297.00390625
Iteration 9900: Loss = -12297.001953125
Iteration 10000: Loss = -12296.998046875
Iteration 10100: Loss = -12296.99609375
Iteration 10200: Loss = -12296.9921875
Iteration 10300: Loss = -12296.9912109375
Iteration 10400: Loss = -12296.990234375
Iteration 10500: Loss = -12296.98828125
Iteration 10600: Loss = -12296.986328125
Iteration 10700: Loss = -12296.984375
Iteration 10800: Loss = -12296.984375
Iteration 10900: Loss = -12296.982421875
Iteration 11000: Loss = -12296.98046875
Iteration 11100: Loss = -12296.9794921875
Iteration 11200: Loss = -12296.9775390625
Iteration 11300: Loss = -12296.9775390625
Iteration 11400: Loss = -12296.9765625
Iteration 11500: Loss = -12296.9755859375
Iteration 11600: Loss = -12296.97265625
Iteration 11700: Loss = -12296.9736328125
1
Iteration 11800: Loss = -12296.97265625
Iteration 11900: Loss = -12296.970703125
Iteration 12000: Loss = -12296.970703125
Iteration 12100: Loss = -12296.96875
Iteration 12200: Loss = -12296.970703125
1
Iteration 12300: Loss = -12296.96875
Iteration 12400: Loss = -12296.966796875
Iteration 12500: Loss = -12296.9658203125
Iteration 12600: Loss = -12296.9677734375
1
Iteration 12700: Loss = -12296.966796875
2
Iteration 12800: Loss = -12296.9658203125
Iteration 12900: Loss = -12296.966796875
1
Iteration 13000: Loss = -12296.9658203125
Iteration 13100: Loss = -12296.96484375
Iteration 13200: Loss = -12296.96484375
Iteration 13300: Loss = -12296.9638671875
Iteration 13400: Loss = -12296.9638671875
Iteration 13500: Loss = -12296.966796875
1
Iteration 13600: Loss = -12296.962890625
Iteration 13700: Loss = -12296.962890625
Iteration 13800: Loss = -12296.962890625
Iteration 13900: Loss = -12296.9609375
Iteration 14000: Loss = -12296.9609375
Iteration 14100: Loss = -12296.9619140625
1
Iteration 14200: Loss = -12296.9619140625
2
Iteration 14300: Loss = -12296.9619140625
3
Iteration 14400: Loss = -12296.9619140625
4
Iteration 14500: Loss = -12296.9609375
Iteration 14600: Loss = -12296.9609375
Iteration 14700: Loss = -12296.9599609375
Iteration 14800: Loss = -12296.9599609375
Iteration 14900: Loss = -12296.9609375
1
Iteration 15000: Loss = -12296.9609375
2
Iteration 15100: Loss = -12296.958984375
Iteration 15200: Loss = -12296.9599609375
1
Iteration 15300: Loss = -12296.958984375
Iteration 15400: Loss = -12296.958984375
Iteration 15500: Loss = -12296.9599609375
1
Iteration 15600: Loss = -12296.9599609375
2
Iteration 15700: Loss = -12296.958984375
Iteration 15800: Loss = -12296.958984375
Iteration 15900: Loss = -12296.958984375
Iteration 16000: Loss = -12296.958984375
Iteration 16100: Loss = -12296.958984375
Iteration 16200: Loss = -12296.9599609375
1
Iteration 16300: Loss = -12296.9580078125
Iteration 16400: Loss = -12296.958984375
1
Iteration 16500: Loss = -12296.9580078125
Iteration 16600: Loss = -12296.9580078125
Iteration 16700: Loss = -12296.9580078125
Iteration 16800: Loss = -12296.9580078125
Iteration 16900: Loss = -12296.9580078125
Iteration 17000: Loss = -12296.9580078125
Iteration 17100: Loss = -12296.9599609375
1
Iteration 17200: Loss = -12296.958984375
2
Iteration 17300: Loss = -12296.9580078125
Iteration 17400: Loss = -12296.9580078125
Iteration 17500: Loss = -12296.9580078125
Iteration 17600: Loss = -12296.9580078125
Iteration 17700: Loss = -12296.95703125
Iteration 17800: Loss = -12296.95703125
Iteration 17900: Loss = -12296.9580078125
1
Iteration 18000: Loss = -12296.9580078125
2
Iteration 18100: Loss = -12296.95703125
Iteration 18200: Loss = -12296.95703125
Iteration 18300: Loss = -12296.9580078125
1
Iteration 18400: Loss = -12296.9580078125
2
Iteration 18500: Loss = -12296.9580078125
3
Iteration 18600: Loss = -12296.95703125
Iteration 18700: Loss = -12296.9580078125
1
Iteration 18800: Loss = -12296.9580078125
2
Iteration 18900: Loss = -12296.95703125
Iteration 19000: Loss = -12296.95703125
Iteration 19100: Loss = -12296.95703125
Iteration 19200: Loss = -12296.9580078125
1
Iteration 19300: Loss = -12296.95703125
Iteration 19400: Loss = -12296.9580078125
1
Iteration 19500: Loss = -12296.95703125
Iteration 19600: Loss = -12296.95703125
Iteration 19700: Loss = -12296.9580078125
1
Iteration 19800: Loss = -12296.95703125
Iteration 19900: Loss = -12296.95703125
Iteration 20000: Loss = -12296.9560546875
Iteration 20100: Loss = -12296.9580078125
1
Iteration 20200: Loss = -12296.9580078125
2
Iteration 20300: Loss = -12296.95703125
3
Iteration 20400: Loss = -12296.9609375
4
Iteration 20500: Loss = -12296.9560546875
Iteration 20600: Loss = -12296.958984375
1
Iteration 20700: Loss = -12296.9580078125
2
Iteration 20800: Loss = -12296.95703125
3
Iteration 20900: Loss = -12296.95703125
4
Iteration 21000: Loss = -12296.958984375
5
Iteration 21100: Loss = -12296.9560546875
Iteration 21200: Loss = -12296.95703125
1
Iteration 21300: Loss = -12296.958984375
2
Iteration 21400: Loss = -12296.9580078125
3
Iteration 21500: Loss = -12296.9580078125
4
Iteration 21600: Loss = -12296.958984375
5
Iteration 21700: Loss = -12296.95703125
6
Iteration 21800: Loss = -12296.9580078125
7
Iteration 21900: Loss = -12296.95703125
8
Iteration 22000: Loss = -12296.9599609375
9
Iteration 22100: Loss = -12296.95703125
10
Iteration 22200: Loss = -12296.9580078125
11
Iteration 22300: Loss = -12296.95703125
12
Iteration 22400: Loss = -12296.95703125
13
Iteration 22500: Loss = -12296.958984375
14
Iteration 22600: Loss = -12296.9580078125
15
Stopping early at iteration 22600 due to no improvement.
pi: tensor([[9.9997e-01, 2.6672e-05],
        [2.8356e-05, 9.9997e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0614, 0.9386], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1841, 0.1958],
         [0.0220, 0.1984]],

        [[0.9788, 0.2177],
         [0.1358, 0.9630]],

        [[0.8828, 0.1506],
         [0.5565, 0.3212]],

        [[0.1482, 0.1603],
         [0.6310, 0.7190]],

        [[0.1696, 0.2303],
         [0.6709, 0.3889]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.001829943009345947
Global Adjusted Rand Index: -0.0007145885191866497
Average Adjusted Rand Index: 0.0011270107514989706
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18003.298828125
Iteration 100: Loss = -14227.443359375
Iteration 200: Loss = -12830.45703125
Iteration 300: Loss = -12459.29296875
Iteration 400: Loss = -12357.3984375
Iteration 500: Loss = -12327.5751953125
Iteration 600: Loss = -12315.431640625
Iteration 700: Loss = -12308.828125
Iteration 800: Loss = -12305.5947265625
Iteration 900: Loss = -12303.46875
Iteration 1000: Loss = -12301.9990234375
Iteration 1100: Loss = -12301.0029296875
Iteration 1200: Loss = -12300.359375
Iteration 1300: Loss = -12299.9169921875
Iteration 1400: Loss = -12299.5947265625
Iteration 1500: Loss = -12299.3466796875
Iteration 1600: Loss = -12299.15234375
Iteration 1700: Loss = -12298.9990234375
Iteration 1800: Loss = -12298.8759765625
Iteration 1900: Loss = -12298.7744140625
Iteration 2000: Loss = -12298.6904296875
Iteration 2100: Loss = -12298.6181640625
Iteration 2200: Loss = -12298.5537109375
Iteration 2300: Loss = -12298.4970703125
Iteration 2400: Loss = -12298.4482421875
Iteration 2500: Loss = -12298.4111328125
Iteration 2600: Loss = -12298.376953125
Iteration 2700: Loss = -12298.3505859375
Iteration 2800: Loss = -12298.32421875
Iteration 2900: Loss = -12298.302734375
Iteration 3000: Loss = -12298.283203125
Iteration 3100: Loss = -12298.2646484375
Iteration 3200: Loss = -12298.2490234375
Iteration 3300: Loss = -12298.2333984375
Iteration 3400: Loss = -12298.21875
Iteration 3500: Loss = -12298.20703125
Iteration 3600: Loss = -12298.193359375
Iteration 3700: Loss = -12298.181640625
Iteration 3800: Loss = -12298.171875
Iteration 3900: Loss = -12298.162109375
Iteration 4000: Loss = -12298.1513671875
Iteration 4100: Loss = -12298.1416015625
Iteration 4200: Loss = -12298.1328125
Iteration 4300: Loss = -12298.123046875
Iteration 4400: Loss = -12298.1162109375
Iteration 4500: Loss = -12298.1064453125
Iteration 4600: Loss = -12298.09765625
Iteration 4700: Loss = -12298.0908203125
Iteration 4800: Loss = -12298.08203125
Iteration 4900: Loss = -12298.0732421875
Iteration 5000: Loss = -12298.064453125
Iteration 5100: Loss = -12298.0517578125
Iteration 5200: Loss = -12298.0390625
Iteration 5300: Loss = -12298.0224609375
Iteration 5400: Loss = -12298.0107421875
Iteration 5500: Loss = -12298.0
Iteration 5600: Loss = -12297.99609375
Iteration 5700: Loss = -12297.9921875
Iteration 5800: Loss = -12297.9873046875
Iteration 5900: Loss = -12297.9833984375
Iteration 6000: Loss = -12297.9794921875
Iteration 6100: Loss = -12297.9765625
Iteration 6200: Loss = -12297.9716796875
Iteration 6300: Loss = -12297.9697265625
Iteration 6400: Loss = -12297.966796875
Iteration 6500: Loss = -12297.9619140625
Iteration 6600: Loss = -12297.9599609375
Iteration 6700: Loss = -12297.9580078125
Iteration 6800: Loss = -12297.9560546875
Iteration 6900: Loss = -12297.9541015625
Iteration 7000: Loss = -12297.951171875
Iteration 7100: Loss = -12297.9482421875
Iteration 7200: Loss = -12297.9462890625
Iteration 7300: Loss = -12297.9453125
Iteration 7400: Loss = -12297.943359375
Iteration 7500: Loss = -12297.9423828125
Iteration 7600: Loss = -12297.939453125
Iteration 7700: Loss = -12297.939453125
Iteration 7800: Loss = -12297.9384765625
Iteration 7900: Loss = -12297.9375
Iteration 8000: Loss = -12297.935546875
Iteration 8100: Loss = -12297.9345703125
Iteration 8200: Loss = -12297.9326171875
Iteration 8300: Loss = -12297.931640625
Iteration 8400: Loss = -12297.9296875
Iteration 8500: Loss = -12297.931640625
1
Iteration 8600: Loss = -12297.9306640625
2
Iteration 8700: Loss = -12297.9296875
Iteration 8800: Loss = -12297.9296875
Iteration 8900: Loss = -12297.9267578125
Iteration 9000: Loss = -12297.927734375
1
Iteration 9100: Loss = -12297.92578125
Iteration 9200: Loss = -12297.9248046875
Iteration 9300: Loss = -12297.92578125
1
Iteration 9400: Loss = -12297.9248046875
Iteration 9500: Loss = -12297.92578125
1
Iteration 9600: Loss = -12297.9228515625
Iteration 9700: Loss = -12297.923828125
1
Iteration 9800: Loss = -12297.9228515625
Iteration 9900: Loss = -12297.9228515625
Iteration 10000: Loss = -12297.9228515625
Iteration 10100: Loss = -12297.921875
Iteration 10200: Loss = -12297.921875
Iteration 10300: Loss = -12297.921875
Iteration 10400: Loss = -12297.9208984375
Iteration 10500: Loss = -12297.919921875
Iteration 10600: Loss = -12297.9208984375
1
Iteration 10700: Loss = -12297.919921875
Iteration 10800: Loss = -12297.9189453125
Iteration 10900: Loss = -12297.919921875
1
Iteration 11000: Loss = -12297.9189453125
Iteration 11100: Loss = -12297.916015625
Iteration 11200: Loss = -12297.9150390625
Iteration 11300: Loss = -12297.912109375
Iteration 11400: Loss = -12297.9091796875
Iteration 11500: Loss = -12297.8974609375
Iteration 11600: Loss = -12297.833984375
Iteration 11700: Loss = -12296.0859375
Iteration 11800: Loss = -12295.3828125
Iteration 11900: Loss = -12295.2763671875
Iteration 12000: Loss = -12295.2373046875
Iteration 12100: Loss = -12295.216796875
Iteration 12200: Loss = -12295.20703125
Iteration 12300: Loss = -12295.1982421875
Iteration 12400: Loss = -12295.19140625
Iteration 12500: Loss = -12295.1875
Iteration 12600: Loss = -12295.1845703125
Iteration 12700: Loss = -12295.1806640625
Iteration 12800: Loss = -12295.1787109375
Iteration 12900: Loss = -12295.177734375
Iteration 13000: Loss = -12295.17578125
Iteration 13100: Loss = -12295.1748046875
Iteration 13200: Loss = -12295.1748046875
Iteration 13300: Loss = -12295.1728515625
Iteration 13400: Loss = -12295.1728515625
Iteration 13500: Loss = -12295.1708984375
Iteration 13600: Loss = -12295.1708984375
Iteration 13700: Loss = -12295.169921875
Iteration 13800: Loss = -12295.171875
1
Iteration 13900: Loss = -12295.169921875
Iteration 14000: Loss = -12295.1689453125
Iteration 14100: Loss = -12295.16796875
Iteration 14200: Loss = -12295.16796875
Iteration 14300: Loss = -12295.1669921875
Iteration 14400: Loss = -12295.1669921875
Iteration 14500: Loss = -12295.1669921875
Iteration 14600: Loss = -12295.1669921875
Iteration 14700: Loss = -12295.166015625
Iteration 14800: Loss = -12295.1669921875
1
Iteration 14900: Loss = -12295.166015625
Iteration 15000: Loss = -12295.166015625
Iteration 15100: Loss = -12295.166015625
Iteration 15200: Loss = -12295.166015625
Iteration 15300: Loss = -12295.1640625
Iteration 15400: Loss = -12295.166015625
1
Iteration 15500: Loss = -12295.1650390625
2
Iteration 15600: Loss = -12295.1650390625
3
Iteration 15700: Loss = -12295.1640625
Iteration 15800: Loss = -12295.1650390625
1
Iteration 15900: Loss = -12295.1640625
Iteration 16000: Loss = -12295.1650390625
1
Iteration 16100: Loss = -12295.1640625
Iteration 16200: Loss = -12295.1640625
Iteration 16300: Loss = -12295.1630859375
Iteration 16400: Loss = -12295.1640625
1
Iteration 16500: Loss = -12295.1650390625
2
Iteration 16600: Loss = -12295.1640625
3
Iteration 16700: Loss = -12295.1640625
4
Iteration 16800: Loss = -12295.1640625
5
Iteration 16900: Loss = -12295.1640625
6
Iteration 17000: Loss = -12295.1630859375
Iteration 17100: Loss = -12295.1630859375
Iteration 17200: Loss = -12295.1630859375
Iteration 17300: Loss = -12295.1630859375
Iteration 17400: Loss = -12295.1650390625
1
Iteration 17500: Loss = -12295.1630859375
Iteration 17600: Loss = -12295.1630859375
Iteration 17700: Loss = -12295.1650390625
1
Iteration 17800: Loss = -12295.1640625
2
Iteration 17900: Loss = -12295.1640625
3
Iteration 18000: Loss = -12295.1630859375
Iteration 18100: Loss = -12295.1640625
1
Iteration 18200: Loss = -12295.1630859375
Iteration 18300: Loss = -12295.1640625
1
Iteration 18400: Loss = -12295.1640625
2
Iteration 18500: Loss = -12295.162109375
Iteration 18600: Loss = -12295.162109375
Iteration 18700: Loss = -12295.1640625
1
Iteration 18800: Loss = -12295.1630859375
2
Iteration 18900: Loss = -12295.1630859375
3
Iteration 19000: Loss = -12295.1640625
4
Iteration 19100: Loss = -12295.1640625
5
Iteration 19200: Loss = -12295.1630859375
6
Iteration 19300: Loss = -12295.1630859375
7
Iteration 19400: Loss = -12295.1640625
8
Iteration 19500: Loss = -12295.1640625
9
Iteration 19600: Loss = -12295.1630859375
10
Iteration 19700: Loss = -12295.1630859375
11
Iteration 19800: Loss = -12295.162109375
Iteration 19900: Loss = -12295.1630859375
1
Iteration 20000: Loss = -12295.1630859375
2
Iteration 20100: Loss = -12295.1630859375
3
Iteration 20200: Loss = -12295.1650390625
4
Iteration 20300: Loss = -12295.1630859375
5
Iteration 20400: Loss = -12295.162109375
Iteration 20500: Loss = -12295.1640625
1
Iteration 20600: Loss = -12295.1630859375
2
Iteration 20700: Loss = -12295.1630859375
3
Iteration 20800: Loss = -12295.162109375
Iteration 20900: Loss = -12295.1630859375
1
Iteration 21000: Loss = -12295.1640625
2
Iteration 21100: Loss = -12295.1650390625
3
Iteration 21200: Loss = -12295.162109375
Iteration 21300: Loss = -12295.1650390625
1
Iteration 21400: Loss = -12295.1630859375
2
Iteration 21500: Loss = -12295.1630859375
3
Iteration 21600: Loss = -12295.1630859375
4
Iteration 21700: Loss = -12295.1630859375
5
Iteration 21800: Loss = -12295.1630859375
6
Iteration 21900: Loss = -12295.1630859375
7
Iteration 22000: Loss = -12295.1630859375
8
Iteration 22100: Loss = -12295.1640625
9
Iteration 22200: Loss = -12295.162109375
Iteration 22300: Loss = -12295.1630859375
1
Iteration 22400: Loss = -12295.1630859375
2
Iteration 22500: Loss = -12295.162109375
Iteration 22600: Loss = -12295.1630859375
1
Iteration 22700: Loss = -12295.1630859375
2
Iteration 22800: Loss = -12295.1640625
3
Iteration 22900: Loss = -12295.1630859375
4
Iteration 23000: Loss = -12295.162109375
Iteration 23100: Loss = -12295.1630859375
1
Iteration 23200: Loss = -12295.1630859375
2
Iteration 23300: Loss = -12295.1640625
3
Iteration 23400: Loss = -12295.1640625
4
Iteration 23500: Loss = -12295.1630859375
5
Iteration 23600: Loss = -12295.162109375
Iteration 23700: Loss = -12295.1630859375
1
Iteration 23800: Loss = -12295.162109375
Iteration 23900: Loss = -12295.1630859375
1
Iteration 24000: Loss = -12295.162109375
Iteration 24100: Loss = -12295.1630859375
1
Iteration 24200: Loss = -12295.162109375
Iteration 24300: Loss = -12295.1650390625
1
Iteration 24400: Loss = -12295.1630859375
2
Iteration 24500: Loss = -12295.162109375
Iteration 24600: Loss = -12295.162109375
Iteration 24700: Loss = -12295.1630859375
1
Iteration 24800: Loss = -12295.1630859375
2
Iteration 24900: Loss = -12295.1611328125
Iteration 25000: Loss = -12295.162109375
1
Iteration 25100: Loss = -12295.1630859375
2
Iteration 25200: Loss = -12295.1640625
3
Iteration 25300: Loss = -12295.1630859375
4
Iteration 25400: Loss = -12295.162109375
5
Iteration 25500: Loss = -12295.162109375
6
Iteration 25600: Loss = -12295.1630859375
7
Iteration 25700: Loss = -12295.1640625
8
Iteration 25800: Loss = -12295.1640625
9
Iteration 25900: Loss = -12295.162109375
10
Iteration 26000: Loss = -12295.162109375
11
Iteration 26100: Loss = -12295.1630859375
12
Iteration 26200: Loss = -12295.1630859375
13
Iteration 26300: Loss = -12295.162109375
14
Iteration 26400: Loss = -12295.1630859375
15
Stopping early at iteration 26400 due to no improvement.
pi: tensor([[1.0309e-05, 9.9999e-01],
        [1.0000e+00, 4.8286e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9340, 0.0660], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2033, 0.1966],
         [0.7793, 0.1891]],

        [[0.9454, 0.2632],
         [0.9807, 0.9465]],

        [[0.0810, 0.1534],
         [0.1774, 0.9177]],

        [[0.2315, 0.1932],
         [0.0334, 0.7150]],

        [[0.2926, 0.1782],
         [0.8998, 0.8444]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.0013070675007002147
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.004212316740111065
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0017137835707030447
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.002911726262720568
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.003578912575773538
Global Adjusted Rand Index: -0.001882278421723635
Average Adjusted Rand Index: -0.000627682871411053
[-0.0007145885191866497, -0.001882278421723635] [0.0011270107514989706, -0.000627682871411053] [12296.9580078125, 12295.1630859375]
-------------------------------------
This iteration is 19
True Objective function: Loss = -11792.456516906203
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -42545.0859375
Iteration 100: Loss = -26848.576171875
Iteration 200: Loss = -15915.47265625
Iteration 300: Loss = -13359.921875
Iteration 400: Loss = -12922.7666015625
Iteration 500: Loss = -12736.4814453125
Iteration 600: Loss = -12629.9013671875
Iteration 700: Loss = -12555.689453125
Iteration 800: Loss = -12507.294921875
Iteration 900: Loss = -12469.0380859375
Iteration 1000: Loss = -12438.23828125
Iteration 1100: Loss = -12408.505859375
Iteration 1200: Loss = -12388.0810546875
Iteration 1300: Loss = -12373.8818359375
Iteration 1400: Loss = -12361.8779296875
Iteration 1500: Loss = -12353.232421875
Iteration 1600: Loss = -12345.689453125
Iteration 1700: Loss = -12338.931640625
Iteration 1800: Loss = -12332.6796875
Iteration 1900: Loss = -12326.3388671875
Iteration 2000: Loss = -12320.8486328125
Iteration 2100: Loss = -12314.4208984375
Iteration 2200: Loss = -12308.03125
Iteration 2300: Loss = -12303.896484375
Iteration 2400: Loss = -12301.3740234375
Iteration 2500: Loss = -12298.97265625
Iteration 2600: Loss = -12293.2802734375
Iteration 2700: Loss = -12291.23828125
Iteration 2800: Loss = -12289.7763671875
Iteration 2900: Loss = -12288.599609375
Iteration 3000: Loss = -12287.6005859375
Iteration 3100: Loss = -12286.7197265625
Iteration 3200: Loss = -12285.9423828125
Iteration 3300: Loss = -12285.24609375
Iteration 3400: Loss = -12284.619140625
Iteration 3500: Loss = -12284.048828125
Iteration 3600: Loss = -12283.5263671875
Iteration 3700: Loss = -12283.0439453125
Iteration 3800: Loss = -12282.599609375
Iteration 3900: Loss = -12282.19140625
Iteration 4000: Loss = -12281.8115234375
Iteration 4100: Loss = -12281.4580078125
Iteration 4200: Loss = -12281.125
Iteration 4300: Loss = -12280.8173828125
Iteration 4400: Loss = -12280.533203125
Iteration 4500: Loss = -12280.2666015625
Iteration 4600: Loss = -12280.0126953125
Iteration 4700: Loss = -12279.7734375
Iteration 4800: Loss = -12279.5537109375
Iteration 4900: Loss = -12279.349609375
Iteration 5000: Loss = -12279.15625
Iteration 5100: Loss = -12278.9736328125
Iteration 5200: Loss = -12278.7998046875
Iteration 5300: Loss = -12278.62109375
Iteration 5400: Loss = -12278.4619140625
Iteration 5500: Loss = -12278.314453125
Iteration 5600: Loss = -12278.17578125
Iteration 5700: Loss = -12278.0419921875
Iteration 5800: Loss = -12277.9140625
Iteration 5900: Loss = -12277.7939453125
Iteration 6000: Loss = -12277.677734375
Iteration 6100: Loss = -12277.5673828125
Iteration 6200: Loss = -12277.458984375
Iteration 6300: Loss = -12277.357421875
Iteration 6400: Loss = -12277.2578125
Iteration 6500: Loss = -12277.1630859375
Iteration 6600: Loss = -12277.0712890625
Iteration 6700: Loss = -12276.9814453125
Iteration 6800: Loss = -12276.8466796875
Iteration 6900: Loss = -12276.765625
Iteration 7000: Loss = -12276.6826171875
Iteration 7100: Loss = -12276.6083984375
Iteration 7200: Loss = -12276.53515625
Iteration 7300: Loss = -12276.46484375
Iteration 7400: Loss = -12276.3955078125
Iteration 7500: Loss = -12276.3251953125
Iteration 7600: Loss = -12276.2607421875
Iteration 7700: Loss = -12276.2001953125
Iteration 7800: Loss = -12276.1376953125
Iteration 7900: Loss = -12276.076171875
Iteration 8000: Loss = -12276.0244140625
Iteration 8100: Loss = -12275.974609375
Iteration 8200: Loss = -12275.9287109375
Iteration 8300: Loss = -12275.884765625
Iteration 8400: Loss = -12275.83984375
Iteration 8500: Loss = -12271.92578125
Iteration 8600: Loss = -12271.8486328125
Iteration 8700: Loss = -12271.802734375
Iteration 8800: Loss = -12271.7587890625
Iteration 8900: Loss = -12271.71875
Iteration 9000: Loss = -12271.68359375
Iteration 9100: Loss = -12271.6484375
Iteration 9200: Loss = -12271.6162109375
Iteration 9300: Loss = -12271.5869140625
Iteration 9400: Loss = -12271.5595703125
Iteration 9500: Loss = -12271.53125
Iteration 9600: Loss = -12270.74609375
Iteration 9700: Loss = -12267.5302734375
Iteration 9800: Loss = -12267.4951171875
Iteration 9900: Loss = -12267.46875
Iteration 10000: Loss = -12262.671875
Iteration 10100: Loss = -12262.572265625
Iteration 10200: Loss = -12262.53515625
Iteration 10300: Loss = -12262.513671875
Iteration 10400: Loss = -12262.4970703125
Iteration 10500: Loss = -12262.48828125
Iteration 10600: Loss = -12257.2822265625
Iteration 10700: Loss = -12256.8232421875
Iteration 10800: Loss = -12256.7158203125
Iteration 10900: Loss = -12256.654296875
Iteration 11000: Loss = -12256.61328125
Iteration 11100: Loss = -12256.580078125
Iteration 11200: Loss = -12256.556640625
Iteration 11300: Loss = -12256.537109375
Iteration 11400: Loss = -12256.51953125
Iteration 11500: Loss = -12256.5048828125
Iteration 11600: Loss = -12256.4921875
Iteration 11700: Loss = -12256.4814453125
Iteration 11800: Loss = -12256.47265625
Iteration 11900: Loss = -12256.4638671875
Iteration 12000: Loss = -12256.455078125
Iteration 12100: Loss = -12256.4482421875
Iteration 12200: Loss = -12256.4404296875
Iteration 12300: Loss = -12256.435546875
Iteration 12400: Loss = -12256.4306640625
Iteration 12500: Loss = -12256.4267578125
Iteration 12600: Loss = -12256.4189453125
Iteration 12700: Loss = -12256.416015625
Iteration 12800: Loss = -12256.4111328125
Iteration 12900: Loss = -12256.4072265625
Iteration 13000: Loss = -12256.4052734375
Iteration 13100: Loss = -12256.4013671875
Iteration 13200: Loss = -12256.3994140625
Iteration 13300: Loss = -12256.39453125
Iteration 13400: Loss = -12256.3916015625
Iteration 13500: Loss = -12256.3896484375
Iteration 13600: Loss = -12256.38671875
Iteration 13700: Loss = -12256.3857421875
Iteration 13800: Loss = -12256.3828125
Iteration 13900: Loss = -12256.380859375
Iteration 14000: Loss = -12256.37890625
Iteration 14100: Loss = -12256.376953125
Iteration 14200: Loss = -12256.375
Iteration 14300: Loss = -12256.3740234375
Iteration 14400: Loss = -12256.3720703125
Iteration 14500: Loss = -12256.37109375
Iteration 14600: Loss = -12256.3701171875
Iteration 14700: Loss = -12256.3681640625
Iteration 14800: Loss = -12256.369140625
1
Iteration 14900: Loss = -12256.365234375
Iteration 15000: Loss = -12256.3662109375
1
Iteration 15100: Loss = -12256.3623046875
Iteration 15200: Loss = -12256.36328125
1
Iteration 15300: Loss = -12256.3623046875
Iteration 15400: Loss = -12256.361328125
Iteration 15500: Loss = -12256.3603515625
Iteration 15600: Loss = -12256.3603515625
Iteration 15700: Loss = -12256.357421875
Iteration 15800: Loss = -12256.359375
1
Iteration 15900: Loss = -12256.3583984375
2
Iteration 16000: Loss = -12256.3564453125
Iteration 16100: Loss = -12256.357421875
1
Iteration 16200: Loss = -12256.3564453125
Iteration 16300: Loss = -12256.35546875
Iteration 16400: Loss = -12256.35546875
Iteration 16500: Loss = -12256.3544921875
Iteration 16600: Loss = -12256.3544921875
Iteration 16700: Loss = -12256.353515625
Iteration 16800: Loss = -12256.3525390625
Iteration 16900: Loss = -12256.3525390625
Iteration 17000: Loss = -12256.3515625
Iteration 17100: Loss = -12256.3525390625
1
Iteration 17200: Loss = -12256.349609375
Iteration 17300: Loss = -12256.349609375
Iteration 17400: Loss = -12256.3505859375
1
Iteration 17500: Loss = -12256.3515625
2
Iteration 17600: Loss = -12256.3515625
3
Iteration 17700: Loss = -12256.3515625
4
Iteration 17800: Loss = -12256.3515625
5
Iteration 17900: Loss = -12256.3505859375
6
Iteration 18000: Loss = -12256.349609375
Iteration 18100: Loss = -12256.349609375
Iteration 18200: Loss = -12256.3525390625
1
Iteration 18300: Loss = -12256.349609375
Iteration 18400: Loss = -12256.349609375
Iteration 18500: Loss = -12256.3486328125
Iteration 18600: Loss = -12256.3486328125
Iteration 18700: Loss = -12256.3486328125
Iteration 18800: Loss = -12256.3486328125
Iteration 18900: Loss = -12256.3486328125
Iteration 19000: Loss = -12256.3486328125
Iteration 19100: Loss = -12256.34765625
Iteration 19200: Loss = -12256.34765625
Iteration 19300: Loss = -12256.3466796875
Iteration 19400: Loss = -12256.3486328125
1
Iteration 19500: Loss = -12256.3486328125
2
Iteration 19600: Loss = -12256.34765625
3
Iteration 19700: Loss = -12256.34765625
4
Iteration 19800: Loss = -12256.34765625
5
Iteration 19900: Loss = -12256.34765625
6
Iteration 20000: Loss = -12256.3466796875
Iteration 20100: Loss = -12256.34765625
1
Iteration 20200: Loss = -12256.3466796875
Iteration 20300: Loss = -12256.34765625
1
Iteration 20400: Loss = -12256.34765625
2
Iteration 20500: Loss = -12256.3466796875
Iteration 20600: Loss = -12256.3564453125
1
Iteration 20700: Loss = -12256.34765625
2
Iteration 20800: Loss = -12256.34765625
3
Iteration 20900: Loss = -12256.34765625
4
Iteration 21000: Loss = -12256.3486328125
5
Iteration 21100: Loss = -12256.345703125
Iteration 21200: Loss = -12256.3466796875
1
Iteration 21300: Loss = -12256.34765625
2
Iteration 21400: Loss = -12256.34765625
3
Iteration 21500: Loss = -12256.3466796875
4
Iteration 21600: Loss = -12256.3466796875
5
Iteration 21700: Loss = -12256.3466796875
6
Iteration 21800: Loss = -12256.34765625
7
Iteration 21900: Loss = -12256.3466796875
8
Iteration 22000: Loss = -12256.34765625
9
Iteration 22100: Loss = -12256.34765625
10
Iteration 22200: Loss = -12256.3466796875
11
Iteration 22300: Loss = -12256.3466796875
12
Iteration 22400: Loss = -12256.34765625
13
Iteration 22500: Loss = -12256.3486328125
14
Iteration 22600: Loss = -12256.3466796875
15
Stopping early at iteration 22600 due to no improvement.
pi: tensor([[1.0000e+00, 4.1128e-06],
        [9.9482e-01, 5.1785e-03]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 3.9414e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1963, 0.2705],
         [0.8890, 0.1227]],

        [[0.6244, 0.1956],
         [0.9903, 0.0083]],

        [[0.9701, 0.2216],
         [0.9808, 0.0281]],

        [[0.8980, 0.1956],
         [0.9263, 0.9876]],

        [[0.4896, 0.2996],
         [0.0695, 0.9297]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30878.48046875
Iteration 100: Loss = -18194.2890625
Iteration 200: Loss = -13259.251953125
Iteration 300: Loss = -12459.0166015625
Iteration 400: Loss = -12352.9033203125
Iteration 500: Loss = -12316.7333984375
Iteration 600: Loss = -12299.640625
Iteration 700: Loss = -12288.7080078125
Iteration 800: Loss = -12281.390625
Iteration 900: Loss = -12276.388671875
Iteration 1000: Loss = -12272.7958984375
Iteration 1100: Loss = -12270.12109375
Iteration 1200: Loss = -12268.0556640625
Iteration 1300: Loss = -12266.4150390625
Iteration 1400: Loss = -12265.080078125
Iteration 1500: Loss = -12263.9697265625
Iteration 1600: Loss = -12263.013671875
Iteration 1700: Loss = -12262.2041015625
Iteration 1800: Loss = -12261.5478515625
Iteration 1900: Loss = -12260.990234375
Iteration 2000: Loss = -12260.5087890625
Iteration 2100: Loss = -12260.0927734375
Iteration 2200: Loss = -12259.7314453125
Iteration 2300: Loss = -12259.416015625
Iteration 2400: Loss = -12259.140625
Iteration 2500: Loss = -12258.8955078125
Iteration 2600: Loss = -12258.681640625
Iteration 2700: Loss = -12258.490234375
Iteration 2800: Loss = -12258.3203125
Iteration 2900: Loss = -12258.169921875
Iteration 3000: Loss = -12258.033203125
Iteration 3100: Loss = -12257.912109375
Iteration 3200: Loss = -12257.8017578125
Iteration 3300: Loss = -12257.7021484375
Iteration 3400: Loss = -12257.609375
Iteration 3500: Loss = -12257.5244140625
Iteration 3600: Loss = -12257.4462890625
Iteration 3700: Loss = -12257.373046875
Iteration 3800: Loss = -12257.30859375
Iteration 3900: Loss = -12257.2470703125
Iteration 4000: Loss = -12257.1923828125
Iteration 4100: Loss = -12257.140625
Iteration 4200: Loss = -12257.09375
Iteration 4300: Loss = -12257.048828125
Iteration 4400: Loss = -12257.0087890625
Iteration 4500: Loss = -12256.9716796875
Iteration 4600: Loss = -12256.9345703125
Iteration 4700: Loss = -12256.9013671875
Iteration 4800: Loss = -12256.8701171875
Iteration 4900: Loss = -12256.8408203125
Iteration 5000: Loss = -12256.8125
Iteration 5100: Loss = -12256.7890625
Iteration 5200: Loss = -12256.763671875
Iteration 5300: Loss = -12256.740234375
Iteration 5400: Loss = -12256.71875
Iteration 5500: Loss = -12256.6982421875
Iteration 5600: Loss = -12256.6806640625
Iteration 5700: Loss = -12256.6611328125
Iteration 5800: Loss = -12256.64453125
Iteration 5900: Loss = -12256.6279296875
Iteration 6000: Loss = -12256.6142578125
Iteration 6100: Loss = -12256.5966796875
Iteration 6200: Loss = -12256.5849609375
Iteration 6300: Loss = -12256.5703125
Iteration 6400: Loss = -12256.5615234375
Iteration 6500: Loss = -12256.5478515625
Iteration 6600: Loss = -12256.537109375
Iteration 6700: Loss = -12256.5283203125
Iteration 6800: Loss = -12256.517578125
Iteration 6900: Loss = -12256.5078125
Iteration 7000: Loss = -12256.4990234375
Iteration 7100: Loss = -12256.4931640625
Iteration 7200: Loss = -12256.484375
Iteration 7300: Loss = -12256.4765625
Iteration 7400: Loss = -12256.46875
Iteration 7500: Loss = -12256.4638671875
Iteration 7600: Loss = -12256.45703125
Iteration 7700: Loss = -12256.451171875
Iteration 7800: Loss = -12256.4443359375
Iteration 7900: Loss = -12256.4423828125
Iteration 8000: Loss = -12256.4365234375
Iteration 8100: Loss = -12256.4306640625
Iteration 8200: Loss = -12256.4248046875
Iteration 8300: Loss = -12256.4228515625
Iteration 8400: Loss = -12256.41796875
Iteration 8500: Loss = -12256.4140625
Iteration 8600: Loss = -12256.4111328125
Iteration 8700: Loss = -12256.408203125
Iteration 8800: Loss = -12256.404296875
Iteration 8900: Loss = -12256.3994140625
Iteration 9000: Loss = -12256.3974609375
Iteration 9100: Loss = -12256.39453125
Iteration 9200: Loss = -12256.3935546875
Iteration 9300: Loss = -12256.388671875
Iteration 9400: Loss = -12256.384765625
Iteration 9500: Loss = -12256.3857421875
1
Iteration 9600: Loss = -12256.3828125
Iteration 9700: Loss = -12256.380859375
Iteration 9800: Loss = -12256.3779296875
Iteration 9900: Loss = -12256.3779296875
Iteration 10000: Loss = -12256.375
Iteration 10100: Loss = -12256.3740234375
Iteration 10200: Loss = -12256.373046875
Iteration 10300: Loss = -12256.3701171875
Iteration 10400: Loss = -12256.3671875
Iteration 10500: Loss = -12256.3671875
Iteration 10600: Loss = -12256.365234375
Iteration 10700: Loss = -12256.3642578125
Iteration 10800: Loss = -12256.36328125
Iteration 10900: Loss = -12256.3623046875
Iteration 11000: Loss = -12256.3623046875
Iteration 11100: Loss = -12256.3603515625
Iteration 11200: Loss = -12256.359375
Iteration 11300: Loss = -12256.359375
Iteration 11400: Loss = -12256.3583984375
Iteration 11500: Loss = -12256.3564453125
Iteration 11600: Loss = -12256.3564453125
Iteration 11700: Loss = -12256.35546875
Iteration 11800: Loss = -12256.3564453125
1
Iteration 11900: Loss = -12256.3544921875
Iteration 12000: Loss = -12256.3544921875
Iteration 12100: Loss = -12256.353515625
Iteration 12200: Loss = -12256.3544921875
1
Iteration 12300: Loss = -12256.3525390625
Iteration 12400: Loss = -12256.3544921875
1
Iteration 12500: Loss = -12256.3515625
Iteration 12600: Loss = -12256.3525390625
1
Iteration 12700: Loss = -12256.3515625
Iteration 12800: Loss = -12256.3505859375
Iteration 12900: Loss = -12256.349609375
Iteration 13000: Loss = -12256.3505859375
1
Iteration 13100: Loss = -12256.349609375
Iteration 13200: Loss = -12256.349609375
Iteration 13300: Loss = -12256.3486328125
Iteration 13400: Loss = -12256.34765625
Iteration 13500: Loss = -12256.3486328125
1
Iteration 13600: Loss = -12256.349609375
2
Iteration 13700: Loss = -12256.349609375
3
Iteration 13800: Loss = -12256.3486328125
4
Iteration 13900: Loss = -12256.3466796875
Iteration 14000: Loss = -12256.34765625
1
Iteration 14100: Loss = -12256.3466796875
Iteration 14200: Loss = -12256.34765625
1
Iteration 14300: Loss = -12256.3486328125
2
Iteration 14400: Loss = -12256.34765625
3
Iteration 14500: Loss = -12256.345703125
Iteration 14600: Loss = -12256.3505859375
1
Iteration 14700: Loss = -12256.3486328125
2
Iteration 14800: Loss = -12256.345703125
Iteration 14900: Loss = -12256.345703125
Iteration 15000: Loss = -12256.345703125
Iteration 15100: Loss = -12256.345703125
Iteration 15200: Loss = -12256.3447265625
Iteration 15300: Loss = -12256.3466796875
1
Iteration 15400: Loss = -12256.3466796875
2
Iteration 15500: Loss = -12256.345703125
3
Iteration 15600: Loss = -12256.3447265625
Iteration 15700: Loss = -12256.3466796875
1
Iteration 15800: Loss = -12256.345703125
2
Iteration 15900: Loss = -12256.3447265625
Iteration 16000: Loss = -12256.3447265625
Iteration 16100: Loss = -12256.3447265625
Iteration 16200: Loss = -12256.345703125
1
Iteration 16300: Loss = -12256.3447265625
Iteration 16400: Loss = -12256.345703125
1
Iteration 16500: Loss = -12256.3466796875
2
Iteration 16600: Loss = -12256.345703125
3
Iteration 16700: Loss = -12256.3447265625
Iteration 16800: Loss = -12256.3447265625
Iteration 16900: Loss = -12256.3427734375
Iteration 17000: Loss = -12256.3427734375
Iteration 17100: Loss = -12256.34375
1
Iteration 17200: Loss = -12256.345703125
2
Iteration 17300: Loss = -12256.3447265625
3
Iteration 17400: Loss = -12256.3447265625
4
Iteration 17500: Loss = -12256.3447265625
5
Iteration 17600: Loss = -12256.34375
6
Iteration 17700: Loss = -12256.341796875
Iteration 17800: Loss = -12256.341796875
Iteration 17900: Loss = -12256.341796875
Iteration 18000: Loss = -12256.3408203125
Iteration 18100: Loss = -12256.341796875
1
Iteration 18200: Loss = -12256.3408203125
Iteration 18300: Loss = -12256.3369140625
Iteration 18400: Loss = -12256.33203125
Iteration 18500: Loss = -12256.306640625
Iteration 18600: Loss = -12253.6015625
Iteration 18700: Loss = -12253.5546875
Iteration 18800: Loss = -12253.5390625
Iteration 18900: Loss = -12253.5341796875
Iteration 19000: Loss = -12253.5146484375
Iteration 19100: Loss = -12253.5126953125
Iteration 19200: Loss = -12253.509765625
Iteration 19300: Loss = -12253.5068359375
Iteration 19400: Loss = -12253.5029296875
Iteration 19500: Loss = -12253.4990234375
Iteration 19600: Loss = -12253.498046875
Iteration 19700: Loss = -12253.498046875
Iteration 19800: Loss = -12253.49609375
Iteration 19900: Loss = -12253.4970703125
1
Iteration 20000: Loss = -12253.4951171875
Iteration 20100: Loss = -12253.494140625
Iteration 20200: Loss = -12253.4970703125
1
Iteration 20300: Loss = -12253.4951171875
2
Iteration 20400: Loss = -12253.494140625
Iteration 20500: Loss = -12253.4931640625
Iteration 20600: Loss = -12253.4951171875
1
Iteration 20700: Loss = -12253.4931640625
Iteration 20800: Loss = -12253.4931640625
Iteration 20900: Loss = -12253.494140625
1
Iteration 21000: Loss = -12253.4921875
Iteration 21100: Loss = -12253.4921875
Iteration 21200: Loss = -12253.4931640625
1
Iteration 21300: Loss = -12253.4931640625
2
Iteration 21400: Loss = -12253.494140625
3
Iteration 21500: Loss = -12253.4921875
Iteration 21600: Loss = -12253.490234375
Iteration 21700: Loss = -12253.4912109375
1
Iteration 21800: Loss = -12253.4912109375
2
Iteration 21900: Loss = -12253.4912109375
3
Iteration 22000: Loss = -12253.4921875
4
Iteration 22100: Loss = -12253.4921875
5
Iteration 22200: Loss = -12253.4921875
6
Iteration 22300: Loss = -12253.4912109375
7
Iteration 22400: Loss = -12253.490234375
Iteration 22500: Loss = -12253.490234375
Iteration 22600: Loss = -12253.4921875
1
Iteration 22700: Loss = -12253.4921875
2
Iteration 22800: Loss = -12253.4912109375
3
Iteration 22900: Loss = -12253.4912109375
4
Iteration 23000: Loss = -12253.490234375
Iteration 23100: Loss = -12253.4912109375
1
Iteration 23200: Loss = -12253.4912109375
2
Iteration 23300: Loss = -12253.490234375
Iteration 23400: Loss = -12253.490234375
Iteration 23500: Loss = -12253.4912109375
1
Iteration 23600: Loss = -12253.4931640625
2
Iteration 23700: Loss = -12253.4912109375
3
Iteration 23800: Loss = -12253.4912109375
4
Iteration 23900: Loss = -12253.490234375
Iteration 24000: Loss = -12253.4912109375
1
Iteration 24100: Loss = -12253.4912109375
2
Iteration 24200: Loss = -12253.490234375
Iteration 24300: Loss = -12253.490234375
Iteration 24400: Loss = -12253.490234375
Iteration 24500: Loss = -12253.490234375
Iteration 24600: Loss = -12253.490234375
Iteration 24700: Loss = -12253.490234375
Iteration 24800: Loss = -12253.4892578125
Iteration 24900: Loss = -12253.4912109375
1
Iteration 25000: Loss = -12253.490234375
2
Iteration 25100: Loss = -12253.4912109375
3
Iteration 25200: Loss = -12253.4892578125
Iteration 25300: Loss = -12253.4912109375
1
Iteration 25400: Loss = -12253.4912109375
2
Iteration 25500: Loss = -12253.490234375
3
Iteration 25600: Loss = -12253.4912109375
4
Iteration 25700: Loss = -12253.490234375
5
Iteration 25800: Loss = -12253.490234375
6
Iteration 25900: Loss = -12253.4921875
7
Iteration 26000: Loss = -12253.490234375
8
Iteration 26100: Loss = -12253.4921875
9
Iteration 26200: Loss = -12253.4912109375
10
Iteration 26300: Loss = -12253.4892578125
Iteration 26400: Loss = -12253.4912109375
1
Iteration 26500: Loss = -12253.4912109375
2
Iteration 26600: Loss = -12253.4892578125
Iteration 26700: Loss = -12253.490234375
1
Iteration 26800: Loss = -12253.4912109375
2
Iteration 26900: Loss = -12253.4912109375
3
Iteration 27000: Loss = -12253.490234375
4
Iteration 27100: Loss = -12253.490234375
5
Iteration 27200: Loss = -12253.494140625
6
Iteration 27300: Loss = -12253.490234375
7
Iteration 27400: Loss = -12253.4912109375
8
Iteration 27500: Loss = -12253.490234375
9
Iteration 27600: Loss = -12253.4892578125
Iteration 27700: Loss = -12253.4892578125
Iteration 27800: Loss = -12253.4912109375
1
Iteration 27900: Loss = -12253.490234375
2
Iteration 28000: Loss = -12253.490234375
3
Iteration 28100: Loss = -12253.4892578125
Iteration 28200: Loss = -12253.4912109375
1
Iteration 28300: Loss = -12253.4912109375
2
Iteration 28400: Loss = -12253.4912109375
3
Iteration 28500: Loss = -12253.490234375
4
Iteration 28600: Loss = -12253.490234375
5
Iteration 28700: Loss = -12253.490234375
6
Iteration 28800: Loss = -12253.4892578125
Iteration 28900: Loss = -12253.4912109375
1
Iteration 29000: Loss = -12253.490234375
2
Iteration 29100: Loss = -12253.4892578125
Iteration 29200: Loss = -12253.490234375
1
Iteration 29300: Loss = -12253.490234375
2
Iteration 29400: Loss = -12253.4931640625
3
Iteration 29500: Loss = -12253.4912109375
4
Iteration 29600: Loss = -12253.490234375
5
Iteration 29700: Loss = -12253.490234375
6
Iteration 29800: Loss = -12253.4912109375
7
Iteration 29900: Loss = -12253.490234375
8
pi: tensor([[1.0000e+00, 1.9419e-06],
        [5.2305e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9789, 0.0211], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1950, 0.2791],
         [0.0561, 0.3527]],

        [[0.0865, 0.1383],
         [0.9859, 0.5019]],

        [[0.5748, 0.2466],
         [0.1576, 0.7063]],

        [[0.4778, 0.1888],
         [0.6139, 0.9327]],

        [[0.9928, 0.2654],
         [0.9916, 0.2000]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
Global Adjusted Rand Index: 0.00035999121612196195
Average Adjusted Rand Index: -0.0007048448738251247
[0.0, 0.00035999121612196195] [0.0, -0.0007048448738251247] [12256.3466796875, 12253.490234375]
-------------------------------------
This iteration is 20
True Objective function: Loss = -11737.20908574616
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -44626.34765625
Iteration 100: Loss = -24447.76171875
Iteration 200: Loss = -14938.205078125
Iteration 300: Loss = -13259.8291015625
Iteration 400: Loss = -12932.244140625
Iteration 500: Loss = -12765.125
Iteration 600: Loss = -12623.16015625
Iteration 700: Loss = -12522.4560546875
Iteration 800: Loss = -12462.6865234375
Iteration 900: Loss = -12434.2353515625
Iteration 1000: Loss = -12406.234375
Iteration 1100: Loss = -12380.9443359375
Iteration 1200: Loss = -12358.80859375
Iteration 1300: Loss = -12348.9736328125
Iteration 1400: Loss = -12338.759765625
Iteration 1500: Loss = -12333.2724609375
Iteration 1600: Loss = -12328.9716796875
Iteration 1700: Loss = -12312.4052734375
Iteration 1800: Loss = -12308.9794921875
Iteration 1900: Loss = -12306.46875
Iteration 2000: Loss = -12304.375
Iteration 2100: Loss = -12302.5703125
Iteration 2200: Loss = -12300.994140625
Iteration 2300: Loss = -12299.5986328125
Iteration 2400: Loss = -12298.3583984375
Iteration 2500: Loss = -12297.244140625
Iteration 2600: Loss = -12296.2421875
Iteration 2700: Loss = -12295.3310546875
Iteration 2800: Loss = -12294.5029296875
Iteration 2900: Loss = -12293.7470703125
Iteration 3000: Loss = -12293.0517578125
Iteration 3100: Loss = -12292.400390625
Iteration 3200: Loss = -12291.779296875
Iteration 3300: Loss = -12291.197265625
Iteration 3400: Loss = -12287.966796875
Iteration 3500: Loss = -12280.0224609375
Iteration 3600: Loss = -12279.400390625
Iteration 3700: Loss = -12278.974609375
Iteration 3800: Loss = -12278.607421875
Iteration 3900: Loss = -12278.2734375
Iteration 4000: Loss = -12277.9677734375
Iteration 4100: Loss = -12277.6845703125
Iteration 4200: Loss = -12277.4208984375
Iteration 4300: Loss = -12277.173828125
Iteration 4400: Loss = -12276.943359375
Iteration 4500: Loss = -12276.7255859375
Iteration 4600: Loss = -12276.5244140625
Iteration 4700: Loss = -12276.3369140625
Iteration 4800: Loss = -12276.1630859375
Iteration 4900: Loss = -12276.0009765625
Iteration 5000: Loss = -12275.849609375
Iteration 5100: Loss = -12275.708984375
Iteration 5200: Loss = -12275.576171875
Iteration 5300: Loss = -12275.451171875
Iteration 5400: Loss = -12275.33203125
Iteration 5500: Loss = -12275.2158203125
Iteration 5600: Loss = -12275.1005859375
Iteration 5700: Loss = -12275.00390625
Iteration 5800: Loss = -12274.912109375
Iteration 5900: Loss = -12274.8271484375
Iteration 6000: Loss = -12274.74609375
Iteration 6100: Loss = -12274.6689453125
Iteration 6200: Loss = -12274.595703125
Iteration 6300: Loss = -12274.5263671875
Iteration 6400: Loss = -12274.4619140625
Iteration 6500: Loss = -12274.3984375
Iteration 6600: Loss = -12274.3388671875
Iteration 6700: Loss = -12274.283203125
Iteration 6800: Loss = -12274.23046875
Iteration 6900: Loss = -12274.1796875
Iteration 7000: Loss = -12274.123046875
Iteration 7100: Loss = -12269.05078125
Iteration 7200: Loss = -12268.75390625
Iteration 7300: Loss = -12268.630859375
Iteration 7400: Loss = -12268.5478515625
Iteration 7500: Loss = -12268.4765625
Iteration 7600: Loss = -12268.4189453125
Iteration 7700: Loss = -12268.3681640625
Iteration 7800: Loss = -12268.322265625
Iteration 7900: Loss = -12268.2802734375
Iteration 8000: Loss = -12268.2412109375
Iteration 8100: Loss = -12268.2060546875
Iteration 8200: Loss = -12268.1728515625
Iteration 8300: Loss = -12268.142578125
Iteration 8400: Loss = -12268.1142578125
Iteration 8500: Loss = -12268.0888671875
Iteration 8600: Loss = -12268.0625
Iteration 8700: Loss = -12268.0400390625
Iteration 8800: Loss = -12268.01953125
Iteration 8900: Loss = -12267.998046875
Iteration 9000: Loss = -12267.978515625
Iteration 9100: Loss = -12267.958984375
Iteration 9200: Loss = -12267.9443359375
Iteration 9300: Loss = -12267.92578125
Iteration 9400: Loss = -12267.91015625
Iteration 9500: Loss = -12267.8955078125
Iteration 9600: Loss = -12267.8828125
Iteration 9700: Loss = -12267.8681640625
Iteration 9800: Loss = -12267.8564453125
Iteration 9900: Loss = -12267.84375
Iteration 10000: Loss = -12267.8330078125
Iteration 10100: Loss = -12267.822265625
Iteration 10200: Loss = -12267.810546875
Iteration 10300: Loss = -12267.8017578125
Iteration 10400: Loss = -12267.79296875
Iteration 10500: Loss = -12267.7841796875
Iteration 10600: Loss = -12267.7783203125
Iteration 10700: Loss = -12267.7685546875
Iteration 10800: Loss = -12267.76171875
Iteration 10900: Loss = -12267.75390625
Iteration 11000: Loss = -12267.748046875
Iteration 11100: Loss = -12267.7412109375
Iteration 11200: Loss = -12267.734375
Iteration 11300: Loss = -12267.728515625
Iteration 11400: Loss = -12267.72265625
Iteration 11500: Loss = -12267.7177734375
Iteration 11600: Loss = -12267.712890625
Iteration 11700: Loss = -12267.7080078125
Iteration 11800: Loss = -12267.7041015625
Iteration 11900: Loss = -12267.69921875
Iteration 12000: Loss = -12267.6953125
Iteration 12100: Loss = -12267.6904296875
Iteration 12200: Loss = -12267.6865234375
Iteration 12300: Loss = -12267.6826171875
Iteration 12400: Loss = -12267.6787109375
Iteration 12500: Loss = -12267.67578125
Iteration 12600: Loss = -12267.673828125
Iteration 12700: Loss = -12267.6689453125
Iteration 12800: Loss = -12267.6669921875
Iteration 12900: Loss = -12267.669921875
1
Iteration 13000: Loss = -12267.6630859375
Iteration 13100: Loss = -12267.6591796875
Iteration 13200: Loss = -12267.66015625
1
Iteration 13300: Loss = -12267.6552734375
Iteration 13400: Loss = -12267.654296875
Iteration 13500: Loss = -12267.6533203125
Iteration 13600: Loss = -12267.6484375
Iteration 13700: Loss = -12267.6474609375
Iteration 13800: Loss = -12267.646484375
Iteration 13900: Loss = -12267.6435546875
Iteration 14000: Loss = -12267.642578125
Iteration 14100: Loss = -12267.6416015625
Iteration 14200: Loss = -12267.638671875
Iteration 14300: Loss = -12267.638671875
Iteration 14400: Loss = -12267.638671875
Iteration 14500: Loss = -12267.63671875
Iteration 14600: Loss = -12267.634765625
Iteration 14700: Loss = -12267.63671875
1
Iteration 14800: Loss = -12267.6328125
Iteration 14900: Loss = -12267.6318359375
Iteration 15000: Loss = -12267.6298828125
Iteration 15100: Loss = -12267.6298828125
Iteration 15200: Loss = -12267.62890625
Iteration 15300: Loss = -12267.62890625
Iteration 15400: Loss = -12267.6259765625
Iteration 15500: Loss = -12267.6259765625
Iteration 15600: Loss = -12267.6259765625
Iteration 15700: Loss = -12267.6240234375
Iteration 15800: Loss = -12267.6259765625
1
Iteration 15900: Loss = -12267.6240234375
Iteration 16000: Loss = -12267.625
1
Iteration 16100: Loss = -12267.6240234375
Iteration 16200: Loss = -12267.623046875
Iteration 16300: Loss = -12267.6220703125
Iteration 16400: Loss = -12267.6220703125
Iteration 16500: Loss = -12267.6220703125
Iteration 16600: Loss = -12267.62109375
Iteration 16700: Loss = -12267.6201171875
Iteration 16800: Loss = -12267.6181640625
Iteration 16900: Loss = -12267.619140625
1
Iteration 17000: Loss = -12267.6162109375
Iteration 17100: Loss = -12267.6162109375
Iteration 17200: Loss = -12267.61328125
Iteration 17300: Loss = -12267.61328125
Iteration 17400: Loss = -12267.6123046875
Iteration 17500: Loss = -12267.6103515625
Iteration 17600: Loss = -12267.607421875
Iteration 17700: Loss = -12267.5927734375
Iteration 17800: Loss = -12267.5693359375
Iteration 17900: Loss = -12267.5458984375
Iteration 18000: Loss = -12267.5107421875
Iteration 18100: Loss = -12267.3125
Iteration 18200: Loss = -12266.56640625
Iteration 18300: Loss = -12253.578125
Iteration 18400: Loss = -12102.7734375
Iteration 18500: Loss = -12033.923828125
Iteration 18600: Loss = -11990.4912109375
Iteration 18700: Loss = -11894.712890625
Iteration 18800: Loss = -11857.509765625
Iteration 18900: Loss = -11842.52734375
Iteration 19000: Loss = -11816.9873046875
Iteration 19100: Loss = -11796.220703125
Iteration 19200: Loss = -11795.646484375
Iteration 19300: Loss = -11786.076171875
Iteration 19400: Loss = -11778.53125
Iteration 19500: Loss = -11774.36328125
Iteration 19600: Loss = -11774.2109375
Iteration 19700: Loss = -11774.11328125
Iteration 19800: Loss = -11774.0400390625
Iteration 19900: Loss = -11773.9814453125
Iteration 20000: Loss = -11765.072265625
Iteration 20100: Loss = -11764.8896484375
Iteration 20200: Loss = -11764.830078125
Iteration 20300: Loss = -11764.7900390625
Iteration 20400: Loss = -11764.759765625
Iteration 20500: Loss = -11764.734375
Iteration 20600: Loss = -11764.7138671875
Iteration 20700: Loss = -11764.6962890625
Iteration 20800: Loss = -11764.6796875
Iteration 20900: Loss = -11764.666015625
Iteration 21000: Loss = -11764.65234375
Iteration 21100: Loss = -11764.6416015625
Iteration 21200: Loss = -11764.630859375
Iteration 21300: Loss = -11764.623046875
Iteration 21400: Loss = -11764.6142578125
Iteration 21500: Loss = -11764.607421875
Iteration 21600: Loss = -11764.6005859375
Iteration 21700: Loss = -11764.58203125
Iteration 21800: Loss = -11764.57421875
Iteration 21900: Loss = -11764.5693359375
Iteration 22000: Loss = -11764.564453125
Iteration 22100: Loss = -11764.5595703125
Iteration 22200: Loss = -11764.556640625
Iteration 22300: Loss = -11764.552734375
Iteration 22400: Loss = -11764.548828125
Iteration 22500: Loss = -11764.544921875
Iteration 22600: Loss = -11764.541015625
Iteration 22700: Loss = -11764.5380859375
Iteration 22800: Loss = -11764.5361328125
Iteration 22900: Loss = -11764.53515625
Iteration 23000: Loss = -11764.53125
Iteration 23100: Loss = -11764.5283203125
Iteration 23200: Loss = -11764.5283203125
Iteration 23300: Loss = -11764.525390625
Iteration 23400: Loss = -11764.5244140625
Iteration 23500: Loss = -11764.5224609375
Iteration 23600: Loss = -11764.5205078125
Iteration 23700: Loss = -11764.5185546875
Iteration 23800: Loss = -11764.517578125
Iteration 23900: Loss = -11764.515625
Iteration 24000: Loss = -11764.515625
Iteration 24100: Loss = -11764.513671875
Iteration 24200: Loss = -11764.5126953125
Iteration 24300: Loss = -11764.5126953125
Iteration 24400: Loss = -11764.51171875
Iteration 24500: Loss = -11764.5087890625
Iteration 24600: Loss = -11764.5087890625
Iteration 24700: Loss = -11764.5078125
Iteration 24800: Loss = -11764.5078125
Iteration 24900: Loss = -11764.5029296875
Iteration 25000: Loss = -11764.5029296875
Iteration 25100: Loss = -11764.501953125
Iteration 25200: Loss = -11764.5
Iteration 25300: Loss = -11764.5
Iteration 25400: Loss = -11764.5009765625
1
Iteration 25500: Loss = -11764.498046875
Iteration 25600: Loss = -11764.5
1
Iteration 25700: Loss = -11764.5
2
Iteration 25800: Loss = -11764.498046875
Iteration 25900: Loss = -11764.498046875
Iteration 26000: Loss = -11764.4990234375
1
Iteration 26100: Loss = -11764.4970703125
Iteration 26200: Loss = -11764.498046875
1
Iteration 26300: Loss = -11764.49609375
Iteration 26400: Loss = -11764.49609375
Iteration 26500: Loss = -11764.49609375
Iteration 26600: Loss = -11764.49609375
Iteration 26700: Loss = -11764.4951171875
Iteration 26800: Loss = -11764.49609375
1
Iteration 26900: Loss = -11764.494140625
Iteration 27000: Loss = -11764.4951171875
1
Iteration 27100: Loss = -11764.4951171875
2
Iteration 27200: Loss = -11764.494140625
Iteration 27300: Loss = -11764.4931640625
Iteration 27400: Loss = -11764.4873046875
Iteration 27500: Loss = -11764.48828125
1
Iteration 27600: Loss = -11764.48828125
2
Iteration 27700: Loss = -11764.48828125
3
Iteration 27800: Loss = -11764.4892578125
4
Iteration 27900: Loss = -11764.4873046875
Iteration 28000: Loss = -11764.48828125
1
Iteration 28100: Loss = -11764.48828125
2
Iteration 28200: Loss = -11764.4873046875
Iteration 28300: Loss = -11764.48828125
1
Iteration 28400: Loss = -11764.4873046875
Iteration 28500: Loss = -11764.48828125
1
Iteration 28600: Loss = -11764.4873046875
Iteration 28700: Loss = -11764.486328125
Iteration 28800: Loss = -11764.48828125
1
Iteration 28900: Loss = -11764.48828125
2
Iteration 29000: Loss = -11764.486328125
Iteration 29100: Loss = -11764.4853515625
Iteration 29200: Loss = -11764.4873046875
1
Iteration 29300: Loss = -11764.4853515625
Iteration 29400: Loss = -11764.486328125
1
Iteration 29500: Loss = -11764.486328125
2
Iteration 29600: Loss = -11764.486328125
3
Iteration 29700: Loss = -11764.4873046875
4
Iteration 29800: Loss = -11764.48828125
5
Iteration 29900: Loss = -11764.486328125
6
pi: tensor([[0.7981, 0.2019],
        [0.1786, 0.8214]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5602, 0.4398], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2879, 0.1135],
         [0.6493, 0.2995]],

        [[0.9930, 0.0984],
         [0.9218, 0.6539]],

        [[0.9628, 0.0981],
         [0.9648, 0.0426]],

        [[0.6346, 0.0887],
         [0.3881, 0.9115]],

        [[0.5888, 0.1039],
         [0.4642, 0.7146]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080890789891884
time is 1
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9524808260462735
Average Adjusted Rand Index: 0.953617627730974
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -40640.67578125
Iteration 100: Loss = -26282.630859375
Iteration 200: Loss = -16707.640625
Iteration 300: Loss = -13686.6962890625
Iteration 400: Loss = -12884.2646484375
Iteration 500: Loss = -12647.40625
Iteration 600: Loss = -12511.79296875
Iteration 700: Loss = -12446.5361328125
Iteration 800: Loss = -12402.3349609375
Iteration 900: Loss = -12365.8076171875
Iteration 1000: Loss = -12347.0869140625
Iteration 1100: Loss = -12334.91015625
Iteration 1200: Loss = -12325.9580078125
Iteration 1300: Loss = -12318.8876953125
Iteration 1400: Loss = -12313.142578125
Iteration 1500: Loss = -12308.4365234375
Iteration 1600: Loss = -12304.6025390625
Iteration 1700: Loss = -12301.4794921875
Iteration 1800: Loss = -12298.865234375
Iteration 1900: Loss = -12292.53515625
Iteration 2000: Loss = -12289.6494140625
Iteration 2100: Loss = -12287.43359375
Iteration 2200: Loss = -12285.3310546875
Iteration 2300: Loss = -12279.7236328125
Iteration 2400: Loss = -12278.2353515625
Iteration 2500: Loss = -12276.9384765625
Iteration 2600: Loss = -12275.708984375
Iteration 2700: Loss = -12274.30859375
Iteration 2800: Loss = -12272.00390625
Iteration 2900: Loss = -12268.5595703125
Iteration 3000: Loss = -12266.8701171875
Iteration 3100: Loss = -12265.3134765625
Iteration 3200: Loss = -12263.673828125
Iteration 3300: Loss = -12262.216796875
Iteration 3400: Loss = -12260.8671875
Iteration 3500: Loss = -12259.51953125
Iteration 3600: Loss = -12257.94140625
Iteration 3700: Loss = -12256.2822265625
Iteration 3800: Loss = -12254.1337890625
Iteration 3900: Loss = -12250.357421875
Iteration 4000: Loss = -12246.63671875
Iteration 4100: Loss = -12244.5107421875
Iteration 4200: Loss = -12241.1708984375
Iteration 4300: Loss = -12239.177734375
Iteration 4400: Loss = -12237.462890625
Iteration 4500: Loss = -12235.462890625
Iteration 4600: Loss = -12233.5693359375
Iteration 4700: Loss = -12230.263671875
Iteration 4800: Loss = -12228.9287109375
Iteration 4900: Loss = -12227.537109375
Iteration 5000: Loss = -12226.474609375
Iteration 5100: Loss = -12225.63671875
Iteration 5200: Loss = -12225.27734375
Iteration 5300: Loss = -12224.869140625
Iteration 5400: Loss = -12224.0078125
Iteration 5500: Loss = -12223.2626953125
Iteration 5600: Loss = -12222.99609375
Iteration 5700: Loss = -12222.8193359375
Iteration 5800: Loss = -12222.353515625
Iteration 5900: Loss = -12219.5791015625
Iteration 6000: Loss = -12219.3720703125
Iteration 6100: Loss = -12218.7578125
Iteration 6200: Loss = -12214.4208984375
Iteration 6300: Loss = -12214.01953125
Iteration 6400: Loss = -12213.890625
Iteration 6500: Loss = -12213.806640625
Iteration 6600: Loss = -12213.7421875
Iteration 6700: Loss = -12213.6884765625
Iteration 6800: Loss = -12213.6396484375
Iteration 6900: Loss = -12213.595703125
Iteration 7000: Loss = -12213.5556640625
Iteration 7100: Loss = -12213.51953125
Iteration 7200: Loss = -12213.48828125
Iteration 7300: Loss = -12213.4560546875
Iteration 7400: Loss = -12213.427734375
Iteration 7500: Loss = -12213.4013671875
Iteration 7600: Loss = -12213.3779296875
Iteration 7700: Loss = -12213.353515625
Iteration 7800: Loss = -12213.3330078125
Iteration 7900: Loss = -12213.3125
Iteration 8000: Loss = -12213.2919921875
Iteration 8100: Loss = -12213.2734375
Iteration 8200: Loss = -12213.2568359375
Iteration 8300: Loss = -12213.240234375
Iteration 8400: Loss = -12213.2255859375
Iteration 8500: Loss = -12213.2109375
Iteration 8600: Loss = -12213.1962890625
Iteration 8700: Loss = -12213.1845703125
Iteration 8800: Loss = -12213.173828125
Iteration 8900: Loss = -12213.16015625
Iteration 9000: Loss = -12213.150390625
Iteration 9100: Loss = -12213.1416015625
Iteration 9200: Loss = -12213.1298828125
Iteration 9300: Loss = -12213.12109375
Iteration 9400: Loss = -12213.1123046875
Iteration 9500: Loss = -12213.1044921875
Iteration 9600: Loss = -12213.095703125
Iteration 9700: Loss = -12213.0888671875
Iteration 9800: Loss = -12213.0810546875
Iteration 9900: Loss = -12213.076171875
Iteration 10000: Loss = -12213.068359375
Iteration 10100: Loss = -12213.0634765625
Iteration 10200: Loss = -12213.0576171875
Iteration 10300: Loss = -12213.052734375
Iteration 10400: Loss = -12213.0478515625
Iteration 10500: Loss = -12213.04296875
Iteration 10600: Loss = -12213.0390625
Iteration 10700: Loss = -12213.03515625
Iteration 10800: Loss = -12213.03125
Iteration 10900: Loss = -12213.0263671875
Iteration 11000: Loss = -12213.0244140625
Iteration 11100: Loss = -12213.021484375
Iteration 11200: Loss = -12213.0166015625
Iteration 11300: Loss = -12213.015625
Iteration 11400: Loss = -12213.01171875
Iteration 11500: Loss = -12213.0078125
Iteration 11600: Loss = -12213.0068359375
Iteration 11700: Loss = -12213.00390625
Iteration 11800: Loss = -12213.001953125
Iteration 11900: Loss = -12213.0
Iteration 12000: Loss = -12212.998046875
Iteration 12100: Loss = -12212.99609375
Iteration 12200: Loss = -12212.994140625
Iteration 12300: Loss = -12212.9921875
Iteration 12400: Loss = -12212.990234375
Iteration 12500: Loss = -12212.9892578125
Iteration 12600: Loss = -12212.9892578125
Iteration 12700: Loss = -12212.986328125
Iteration 12800: Loss = -12212.986328125
Iteration 12900: Loss = -12212.984375
Iteration 13000: Loss = -12212.982421875
Iteration 13100: Loss = -12212.982421875
Iteration 13200: Loss = -12212.98046875
Iteration 13300: Loss = -12212.978515625
Iteration 13400: Loss = -12212.978515625
Iteration 13500: Loss = -12212.9775390625
Iteration 13600: Loss = -12212.9755859375
Iteration 13700: Loss = -12212.9755859375
Iteration 13800: Loss = -12212.9736328125
Iteration 13900: Loss = -12212.9736328125
Iteration 14000: Loss = -12212.9736328125
Iteration 14100: Loss = -12212.9716796875
Iteration 14200: Loss = -12212.970703125
Iteration 14300: Loss = -12212.970703125
Iteration 14400: Loss = -12212.9697265625
Iteration 14500: Loss = -12212.970703125
1
Iteration 14600: Loss = -12212.9697265625
Iteration 14700: Loss = -12212.96875
Iteration 14800: Loss = -12212.9677734375
Iteration 14900: Loss = -12212.970703125
1
Iteration 15000: Loss = -12212.966796875
Iteration 15100: Loss = -12212.9677734375
1
Iteration 15200: Loss = -12212.9677734375
2
Iteration 15300: Loss = -12212.9658203125
Iteration 15400: Loss = -12212.96484375
Iteration 15500: Loss = -12212.96484375
Iteration 15600: Loss = -12212.9638671875
Iteration 15700: Loss = -12212.962890625
Iteration 15800: Loss = -12212.9638671875
1
Iteration 15900: Loss = -12212.9609375
Iteration 16000: Loss = -12212.9521484375
Iteration 16100: Loss = -12212.8037109375
Iteration 16200: Loss = -12211.5986328125
Iteration 16300: Loss = -12211.4365234375
Iteration 16400: Loss = -12211.3623046875
Iteration 16500: Loss = -12211.330078125
Iteration 16600: Loss = -12211.3037109375
Iteration 16700: Loss = -12211.2939453125
Iteration 16800: Loss = -12211.291015625
Iteration 16900: Loss = -12211.2900390625
Iteration 17000: Loss = -12211.2880859375
Iteration 17100: Loss = -12211.2880859375
Iteration 17200: Loss = -12211.287109375
Iteration 17300: Loss = -12211.2890625
1
Iteration 17400: Loss = -12211.2861328125
Iteration 17500: Loss = -12211.28515625
Iteration 17600: Loss = -12211.28515625
Iteration 17700: Loss = -12211.2861328125
1
Iteration 17800: Loss = -12211.2841796875
Iteration 17900: Loss = -12211.2841796875
Iteration 18000: Loss = -12211.2861328125
1
Iteration 18100: Loss = -12211.28515625
2
Iteration 18200: Loss = -12211.2841796875
Iteration 18300: Loss = -12211.283203125
Iteration 18400: Loss = -12211.28515625
1
Iteration 18500: Loss = -12211.283203125
Iteration 18600: Loss = -12211.2841796875
1
Iteration 18700: Loss = -12211.2841796875
2
Iteration 18800: Loss = -12211.283203125
Iteration 18900: Loss = -12211.283203125
Iteration 19000: Loss = -12211.283203125
Iteration 19100: Loss = -12211.283203125
Iteration 19200: Loss = -12211.2841796875
1
Iteration 19300: Loss = -12211.2841796875
2
Iteration 19400: Loss = -12211.2841796875
3
Iteration 19500: Loss = -12211.2841796875
4
Iteration 19600: Loss = -12211.2841796875
5
Iteration 19700: Loss = -12211.283203125
Iteration 19800: Loss = -12211.283203125
Iteration 19900: Loss = -12211.2822265625
Iteration 20000: Loss = -12211.2822265625
Iteration 20100: Loss = -12211.2822265625
Iteration 20200: Loss = -12211.283203125
1
Iteration 20300: Loss = -12211.283203125
2
Iteration 20400: Loss = -12211.283203125
3
Iteration 20500: Loss = -12211.2822265625
Iteration 20600: Loss = -12211.283203125
1
Iteration 20700: Loss = -12211.2822265625
Iteration 20800: Loss = -12211.2822265625
Iteration 20900: Loss = -12211.283203125
1
Iteration 21000: Loss = -12211.283203125
2
Iteration 21100: Loss = -12211.28515625
3
Iteration 21200: Loss = -12211.2822265625
Iteration 21300: Loss = -12211.2822265625
Iteration 21400: Loss = -12211.283203125
1
Iteration 21500: Loss = -12211.283203125
2
Iteration 21600: Loss = -12211.283203125
3
Iteration 21700: Loss = -12211.283203125
4
Iteration 21800: Loss = -12211.2822265625
Iteration 21900: Loss = -12211.28515625
1
Iteration 22000: Loss = -12211.283203125
2
Iteration 22100: Loss = -12211.283203125
3
Iteration 22200: Loss = -12211.283203125
4
Iteration 22300: Loss = -12211.2841796875
5
Iteration 22400: Loss = -12211.28125
Iteration 22500: Loss = -12211.2841796875
1
Iteration 22600: Loss = -12211.28125
Iteration 22700: Loss = -12211.0439453125
Iteration 22800: Loss = -12211.0341796875
Iteration 22900: Loss = -12211.0341796875
Iteration 23000: Loss = -12211.0341796875
Iteration 23100: Loss = -12211.03515625
1
Iteration 23200: Loss = -12211.03515625
2
Iteration 23300: Loss = -12211.0341796875
Iteration 23400: Loss = -12211.025390625
Iteration 23500: Loss = -12211.0087890625
Iteration 23600: Loss = -12211.0078125
Iteration 23700: Loss = -12211.0
Iteration 23800: Loss = -12211.001953125
1
Iteration 23900: Loss = -12210.96484375
Iteration 24000: Loss = -12210.9658203125
1
Iteration 24100: Loss = -12210.9560546875
Iteration 24200: Loss = -12210.9560546875
Iteration 24300: Loss = -12210.9501953125
Iteration 24400: Loss = -12210.94921875
Iteration 24500: Loss = -12210.951171875
1
Iteration 24600: Loss = -12210.9541015625
2
Iteration 24700: Loss = -12210.94921875
Iteration 24800: Loss = -12210.947265625
Iteration 24900: Loss = -12210.9482421875
1
Iteration 25000: Loss = -12210.94921875
2
Iteration 25100: Loss = -12210.9482421875
3
Iteration 25200: Loss = -12210.9482421875
4
Iteration 25300: Loss = -12210.947265625
Iteration 25400: Loss = -12210.9453125
Iteration 25500: Loss = -12210.9443359375
Iteration 25600: Loss = -12210.9462890625
1
Iteration 25700: Loss = -12210.9443359375
Iteration 25800: Loss = -12210.9443359375
Iteration 25900: Loss = -12210.943359375
Iteration 26000: Loss = -12210.9453125
1
Iteration 26100: Loss = -12210.9453125
2
Iteration 26200: Loss = -12210.9453125
3
Iteration 26300: Loss = -12210.9443359375
4
Iteration 26400: Loss = -12210.9453125
5
Iteration 26500: Loss = -12210.9453125
6
Iteration 26600: Loss = -12210.943359375
Iteration 26700: Loss = -12210.943359375
Iteration 26800: Loss = -12210.943359375
Iteration 26900: Loss = -12210.9443359375
1
Iteration 27000: Loss = -12210.9443359375
2
Iteration 27100: Loss = -12210.9443359375
3
Iteration 27200: Loss = -12210.943359375
Iteration 27300: Loss = -12210.94140625
Iteration 27400: Loss = -12210.943359375
1
Iteration 27500: Loss = -12210.943359375
2
Iteration 27600: Loss = -12210.943359375
3
Iteration 27700: Loss = -12210.9423828125
4
Iteration 27800: Loss = -12210.9423828125
5
Iteration 27900: Loss = -12210.9423828125
6
Iteration 28000: Loss = -12210.943359375
7
Iteration 28100: Loss = -12210.9443359375
8
Iteration 28200: Loss = -12210.9423828125
9
Iteration 28300: Loss = -12210.943359375
10
Iteration 28400: Loss = -12210.943359375
11
Iteration 28500: Loss = -12210.943359375
12
Iteration 28600: Loss = -12210.94140625
Iteration 28700: Loss = -12210.9443359375
1
Iteration 28800: Loss = -12210.943359375
2
Iteration 28900: Loss = -12210.9423828125
3
Iteration 29000: Loss = -12210.9423828125
4
Iteration 29100: Loss = -12210.94140625
Iteration 29200: Loss = -12210.9423828125
1
Iteration 29300: Loss = -12210.943359375
2
Iteration 29400: Loss = -12210.9443359375
3
Iteration 29500: Loss = -12210.94140625
Iteration 29600: Loss = -12210.943359375
1
Iteration 29700: Loss = -12210.9423828125
2
Iteration 29800: Loss = -12210.9423828125
3
Iteration 29900: Loss = -12210.943359375
4
pi: tensor([[5.3871e-06, 9.9999e-01],
        [1.2135e-02, 9.8787e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4753, 0.5247], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3046, 0.1025],
         [0.4608, 0.2033]],

        [[0.9058, 0.2471],
         [0.0257, 0.9894]],

        [[0.0162, 0.2917],
         [0.1905, 0.6678]],

        [[0.1910, 0.0871],
         [0.0414, 0.0302]],

        [[0.0328, 0.1299],
         [0.0339, 0.1931]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.027163932315055546
Average Adjusted Rand Index: 0.19194193980842583
[0.9524808260462735, 0.027163932315055546] [0.953617627730974, 0.19194193980842583] [11764.45703125, 12210.9423828125]
-------------------------------------
This iteration is 21
True Objective function: Loss = -11716.715613826182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33503.5390625
Iteration 100: Loss = -22688.42578125
Iteration 200: Loss = -16116.68359375
Iteration 300: Loss = -13311.3095703125
Iteration 400: Loss = -12528.2763671875
Iteration 500: Loss = -12349.89453125
Iteration 600: Loss = -12291.3125
Iteration 700: Loss = -12268.658203125
Iteration 800: Loss = -12252.248046875
Iteration 900: Loss = -12229.6162109375
Iteration 1000: Loss = -12203.75390625
Iteration 1100: Loss = -12182.8662109375
Iteration 1200: Loss = -12161.546875
Iteration 1300: Loss = -12144.6884765625
Iteration 1400: Loss = -12134.837890625
Iteration 1500: Loss = -12120.947265625
Iteration 1600: Loss = -12108.806640625
Iteration 1700: Loss = -12088.48046875
Iteration 1800: Loss = -12069.53515625
Iteration 1900: Loss = -12050.306640625
Iteration 2000: Loss = -12027.271484375
Iteration 2100: Loss = -12019.12109375
Iteration 2200: Loss = -12017.1572265625
Iteration 2300: Loss = -12013.7734375
Iteration 2400: Loss = -12010.6962890625
Iteration 2500: Loss = -12010.1826171875
Iteration 2600: Loss = -12009.80078125
Iteration 2700: Loss = -12009.48828125
Iteration 2800: Loss = -12009.21484375
Iteration 2900: Loss = -12008.962890625
Iteration 3000: Loss = -12008.7119140625
Iteration 3100: Loss = -12008.4423828125
Iteration 3200: Loss = -12008.1240234375
Iteration 3300: Loss = -12007.7119140625
Iteration 3400: Loss = -12007.125
Iteration 3500: Loss = -12006.125
Iteration 3600: Loss = -12004.2783203125
Iteration 3700: Loss = -12002.1279296875
Iteration 3800: Loss = -11994.79296875
Iteration 3900: Loss = -11990.9150390625
Iteration 4000: Loss = -11985.126953125
Iteration 4100: Loss = -11972.205078125
Iteration 4200: Loss = -11970.8076171875
Iteration 4300: Loss = -11960.4296875
Iteration 4400: Loss = -11922.6875
Iteration 4500: Loss = -11882.8486328125
Iteration 4600: Loss = -11844.318359375
Iteration 4700: Loss = -11834.71875
Iteration 4800: Loss = -11824.00390625
Iteration 4900: Loss = -11816.865234375
Iteration 5000: Loss = -11816.5
Iteration 5100: Loss = -11795.611328125
Iteration 5200: Loss = -11792.080078125
Iteration 5300: Loss = -11791.8232421875
Iteration 5400: Loss = -11782.9638671875
Iteration 5500: Loss = -11782.61328125
Iteration 5600: Loss = -11782.466796875
Iteration 5700: Loss = -11782.212890625
Iteration 5800: Loss = -11782.1474609375
Iteration 5900: Loss = -11767.6962890625
Iteration 6000: Loss = -11766.6142578125
Iteration 6100: Loss = -11766.4833984375
Iteration 6200: Loss = -11766.365234375
Iteration 6300: Loss = -11766.2958984375
Iteration 6400: Loss = -11766.2626953125
Iteration 6500: Loss = -11766.2333984375
Iteration 6600: Loss = -11766.2109375
Iteration 6700: Loss = -11766.1884765625
Iteration 6800: Loss = -11766.1708984375
Iteration 6900: Loss = -11766.15625
Iteration 7000: Loss = -11766.1416015625
Iteration 7100: Loss = -11766.1279296875
Iteration 7200: Loss = -11766.1171875
Iteration 7300: Loss = -11766.1064453125
Iteration 7400: Loss = -11766.0966796875
Iteration 7500: Loss = -11766.0888671875
Iteration 7600: Loss = -11766.080078125
Iteration 7700: Loss = -11766.072265625
Iteration 7800: Loss = -11766.06640625
Iteration 7900: Loss = -11766.0595703125
Iteration 8000: Loss = -11766.052734375
Iteration 8100: Loss = -11766.0478515625
Iteration 8200: Loss = -11766.04296875
Iteration 8300: Loss = -11766.0380859375
Iteration 8400: Loss = -11766.033203125
Iteration 8500: Loss = -11766.0302734375
Iteration 8600: Loss = -11766.0244140625
Iteration 8700: Loss = -11766.021484375
Iteration 8800: Loss = -11766.017578125
Iteration 8900: Loss = -11766.0146484375
Iteration 9000: Loss = -11766.01171875
Iteration 9100: Loss = -11766.0078125
Iteration 9200: Loss = -11766.0048828125
Iteration 9300: Loss = -11766.0029296875
Iteration 9400: Loss = -11766.0
Iteration 9500: Loss = -11765.9951171875
Iteration 9600: Loss = -11765.994140625
Iteration 9700: Loss = -11765.98828125
Iteration 9800: Loss = -11765.978515625
Iteration 9900: Loss = -11765.9697265625
Iteration 10000: Loss = -11765.9677734375
Iteration 10100: Loss = -11765.966796875
Iteration 10200: Loss = -11765.96484375
Iteration 10300: Loss = -11765.962890625
Iteration 10400: Loss = -11765.9619140625
Iteration 10500: Loss = -11765.9609375
Iteration 10600: Loss = -11765.9599609375
Iteration 10700: Loss = -11765.958984375
Iteration 10800: Loss = -11765.95703125
Iteration 10900: Loss = -11765.95703125
Iteration 11000: Loss = -11765.9560546875
Iteration 11100: Loss = -11765.955078125
Iteration 11200: Loss = -11765.9541015625
Iteration 11300: Loss = -11765.953125
Iteration 11400: Loss = -11765.953125
Iteration 11500: Loss = -11765.953125
Iteration 11600: Loss = -11765.951171875
Iteration 11700: Loss = -11765.9501953125
Iteration 11800: Loss = -11765.94921875
Iteration 11900: Loss = -11765.9482421875
Iteration 12000: Loss = -11765.9482421875
Iteration 12100: Loss = -11765.94921875
1
Iteration 12200: Loss = -11765.9482421875
Iteration 12300: Loss = -11765.9482421875
Iteration 12400: Loss = -11765.9482421875
Iteration 12500: Loss = -11765.9462890625
Iteration 12600: Loss = -11765.947265625
1
Iteration 12700: Loss = -11765.9462890625
Iteration 12800: Loss = -11765.9453125
Iteration 12900: Loss = -11765.9462890625
1
Iteration 13000: Loss = -11765.9462890625
2
Iteration 13100: Loss = -11765.9453125
Iteration 13200: Loss = -11765.9443359375
Iteration 13300: Loss = -11765.9443359375
Iteration 13400: Loss = -11765.9453125
1
Iteration 13500: Loss = -11765.9443359375
Iteration 13600: Loss = -11765.9443359375
Iteration 13700: Loss = -11765.9443359375
Iteration 13800: Loss = -11765.943359375
Iteration 13900: Loss = -11765.9443359375
1
Iteration 14000: Loss = -11765.943359375
Iteration 14100: Loss = -11765.943359375
Iteration 14200: Loss = -11765.9443359375
1
Iteration 14300: Loss = -11765.9423828125
Iteration 14400: Loss = -11765.94140625
Iteration 14500: Loss = -11765.9423828125
1
Iteration 14600: Loss = -11765.94140625
Iteration 14700: Loss = -11765.94140625
Iteration 14800: Loss = -11765.9423828125
1
Iteration 14900: Loss = -11765.9423828125
2
Iteration 15000: Loss = -11765.9423828125
3
Iteration 15100: Loss = -11765.943359375
4
Iteration 15200: Loss = -11765.94140625
Iteration 15300: Loss = -11765.9423828125
1
Iteration 15400: Loss = -11765.9404296875
Iteration 15500: Loss = -11765.94140625
1
Iteration 15600: Loss = -11765.9423828125
2
Iteration 15700: Loss = -11765.939453125
Iteration 15800: Loss = -11765.94140625
1
Iteration 15900: Loss = -11765.94140625
2
Iteration 16000: Loss = -11765.94140625
3
Iteration 16100: Loss = -11765.9423828125
4
Iteration 16200: Loss = -11765.94140625
5
Iteration 16300: Loss = -11765.9404296875
6
Iteration 16400: Loss = -11765.94140625
7
Iteration 16500: Loss = -11765.94140625
8
Iteration 16600: Loss = -11765.9404296875
9
Iteration 16700: Loss = -11765.939453125
Iteration 16800: Loss = -11765.9404296875
1
Iteration 16900: Loss = -11765.94140625
2
Iteration 17000: Loss = -11765.9404296875
3
Iteration 17100: Loss = -11765.9404296875
4
Iteration 17200: Loss = -11765.94140625
5
Iteration 17300: Loss = -11765.94140625
6
Iteration 17400: Loss = -11765.939453125
Iteration 17500: Loss = -11765.939453125
Iteration 17600: Loss = -11765.939453125
Iteration 17700: Loss = -11765.939453125
Iteration 17800: Loss = -11765.9384765625
Iteration 17900: Loss = -11765.9384765625
Iteration 18000: Loss = -11765.9384765625
Iteration 18100: Loss = -11765.939453125
1
Iteration 18200: Loss = -11765.939453125
2
Iteration 18300: Loss = -11765.9404296875
3
Iteration 18400: Loss = -11765.939453125
4
Iteration 18500: Loss = -11765.939453125
5
Iteration 18600: Loss = -11765.939453125
6
Iteration 18700: Loss = -11765.9384765625
Iteration 18800: Loss = -11765.939453125
1
Iteration 18900: Loss = -11765.939453125
2
Iteration 19000: Loss = -11765.9384765625
Iteration 19100: Loss = -11765.939453125
1
Iteration 19200: Loss = -11765.9384765625
Iteration 19300: Loss = -11765.9384765625
Iteration 19400: Loss = -11765.9365234375
Iteration 19500: Loss = -11765.9375
1
Iteration 19600: Loss = -11765.9384765625
2
Iteration 19700: Loss = -11765.9375
3
Iteration 19800: Loss = -11765.9384765625
4
Iteration 19900: Loss = -11765.9384765625
5
Iteration 20000: Loss = -11765.9375
6
Iteration 20100: Loss = -11765.9375
7
Iteration 20200: Loss = -11765.9375
8
Iteration 20300: Loss = -11765.9384765625
9
Iteration 20400: Loss = -11765.9375
10
Iteration 20500: Loss = -11765.9375
11
Iteration 20600: Loss = -11765.9384765625
12
Iteration 20700: Loss = -11765.9384765625
13
Iteration 20800: Loss = -11765.9384765625
14
Iteration 20900: Loss = -11765.9384765625
15
Stopping early at iteration 20900 due to no improvement.
pi: tensor([[0.5062, 0.4938],
        [0.4983, 0.5017]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4317, 0.5683], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3032, 0.0966],
         [0.0122, 0.2920]],

        [[0.0803, 0.0938],
         [0.7866, 0.0074]],

        [[0.8299, 0.0971],
         [0.9851, 0.9773]],

        [[0.3290, 0.0965],
         [0.0272, 0.0181]],

        [[0.0071, 0.0955],
         [0.7052, 0.9919]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.03649118202185019
Average Adjusted Rand Index: 0.9919998119331364
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33379.4765625
Iteration 100: Loss = -21354.482421875
Iteration 200: Loss = -14211.291015625
Iteration 300: Loss = -12811.6884765625
Iteration 400: Loss = -12524.3525390625
Iteration 500: Loss = -12416.9658203125
Iteration 600: Loss = -12371.546875
Iteration 700: Loss = -12343.8505859375
Iteration 800: Loss = -12325.1435546875
Iteration 900: Loss = -12304.6318359375
Iteration 1000: Loss = -12290.935546875
Iteration 1100: Loss = -12281.4404296875
Iteration 1200: Loss = -12275.9921875
Iteration 1300: Loss = -12272.3955078125
Iteration 1400: Loss = -12269.330078125
Iteration 1500: Loss = -12266.7197265625
Iteration 1600: Loss = -12264.689453125
Iteration 1700: Loss = -12263.1240234375
Iteration 1800: Loss = -12261.9267578125
Iteration 1900: Loss = -12260.962890625
Iteration 2000: Loss = -12260.1328125
Iteration 2100: Loss = -12259.4794921875
Iteration 2200: Loss = -12258.9228515625
Iteration 2300: Loss = -12258.44921875
Iteration 2400: Loss = -12258.033203125
Iteration 2500: Loss = -12257.666015625
Iteration 2600: Loss = -12257.3427734375
Iteration 2700: Loss = -12257.0517578125
Iteration 2800: Loss = -12256.791015625
Iteration 2900: Loss = -12256.5556640625
Iteration 3000: Loss = -12256.34375
Iteration 3100: Loss = -12256.150390625
Iteration 3200: Loss = -12255.9755859375
Iteration 3300: Loss = -12255.81640625
Iteration 3400: Loss = -12255.6708984375
Iteration 3500: Loss = -12255.5361328125
Iteration 3600: Loss = -12255.4130859375
Iteration 3700: Loss = -12255.2998046875
Iteration 3800: Loss = -12255.1943359375
Iteration 3900: Loss = -12255.095703125
Iteration 4000: Loss = -12255.0048828125
Iteration 4100: Loss = -12254.919921875
Iteration 4200: Loss = -12254.8408203125
Iteration 4300: Loss = -12254.767578125
Iteration 4400: Loss = -12254.69921875
Iteration 4500: Loss = -12254.6337890625
Iteration 4600: Loss = -12254.572265625
Iteration 4700: Loss = -12254.5166015625
Iteration 4800: Loss = -12254.462890625
Iteration 4900: Loss = -12254.4150390625
Iteration 5000: Loss = -12254.3671875
Iteration 5100: Loss = -12254.3232421875
Iteration 5200: Loss = -12254.2822265625
Iteration 5300: Loss = -12254.2421875
Iteration 5400: Loss = -12254.205078125
Iteration 5500: Loss = -12254.169921875
Iteration 5600: Loss = -12254.1376953125
Iteration 5700: Loss = -12254.1064453125
Iteration 5800: Loss = -12254.0771484375
Iteration 5900: Loss = -12254.0478515625
Iteration 6000: Loss = -12254.0224609375
Iteration 6100: Loss = -12253.998046875
Iteration 6200: Loss = -12253.97265625
Iteration 6300: Loss = -12253.951171875
Iteration 6400: Loss = -12253.9296875
Iteration 6500: Loss = -12253.9091796875
Iteration 6600: Loss = -12253.8916015625
Iteration 6700: Loss = -12253.873046875
Iteration 6800: Loss = -12253.85546875
Iteration 6900: Loss = -12253.83984375
Iteration 7000: Loss = -12253.8232421875
Iteration 7100: Loss = -12253.8095703125
Iteration 7200: Loss = -12253.7958984375
Iteration 7300: Loss = -12253.783203125
Iteration 7400: Loss = -12253.7685546875
Iteration 7500: Loss = -12253.7578125
Iteration 7600: Loss = -12253.7470703125
Iteration 7700: Loss = -12253.736328125
Iteration 7800: Loss = -12253.7265625
Iteration 7900: Loss = -12253.7158203125
Iteration 8000: Loss = -12253.7060546875
Iteration 8100: Loss = -12253.697265625
Iteration 8200: Loss = -12253.6904296875
Iteration 8300: Loss = -12253.681640625
Iteration 8400: Loss = -12253.673828125
Iteration 8500: Loss = -12253.6669921875
Iteration 8600: Loss = -12253.6591796875
Iteration 8700: Loss = -12253.6533203125
Iteration 8800: Loss = -12253.646484375
Iteration 8900: Loss = -12253.640625
Iteration 9000: Loss = -12253.6357421875
Iteration 9100: Loss = -12253.630859375
Iteration 9200: Loss = -12253.625
Iteration 9300: Loss = -12253.62109375
Iteration 9400: Loss = -12253.6162109375
Iteration 9500: Loss = -12253.611328125
Iteration 9600: Loss = -12253.60546875
Iteration 9700: Loss = -12253.603515625
Iteration 9800: Loss = -12253.599609375
Iteration 9900: Loss = -12253.5966796875
Iteration 10000: Loss = -12253.5927734375
Iteration 10100: Loss = -12253.5888671875
Iteration 10200: Loss = -12253.5859375
Iteration 10300: Loss = -12253.583984375
Iteration 10400: Loss = -12253.5810546875
Iteration 10500: Loss = -12253.5791015625
Iteration 10600: Loss = -12253.5751953125
Iteration 10700: Loss = -12253.5712890625
Iteration 10800: Loss = -12253.5703125
Iteration 10900: Loss = -12253.5693359375
Iteration 11000: Loss = -12253.5673828125
Iteration 11100: Loss = -12253.564453125
Iteration 11200: Loss = -12253.5634765625
Iteration 11300: Loss = -12253.560546875
Iteration 11400: Loss = -12253.55859375
Iteration 11500: Loss = -12253.5576171875
Iteration 11600: Loss = -12253.556640625
Iteration 11700: Loss = -12253.5537109375
Iteration 11800: Loss = -12253.5537109375
Iteration 11900: Loss = -12253.552734375
Iteration 12000: Loss = -12253.548828125
Iteration 12100: Loss = -12253.548828125
Iteration 12200: Loss = -12253.548828125
Iteration 12300: Loss = -12253.5458984375
Iteration 12400: Loss = -12253.5478515625
1
Iteration 12500: Loss = -12253.544921875
Iteration 12600: Loss = -12253.54296875
Iteration 12700: Loss = -12253.5439453125
1
Iteration 12800: Loss = -12253.5439453125
2
Iteration 12900: Loss = -12253.541015625
Iteration 13000: Loss = -12253.541015625
Iteration 13100: Loss = -12253.5400390625
Iteration 13200: Loss = -12253.541015625
1
Iteration 13300: Loss = -12253.5390625
Iteration 13400: Loss = -12253.5380859375
Iteration 13500: Loss = -12253.5380859375
Iteration 13600: Loss = -12253.5361328125
Iteration 13700: Loss = -12253.5380859375
1
Iteration 13800: Loss = -12253.5361328125
Iteration 13900: Loss = -12253.53515625
Iteration 14000: Loss = -12253.53515625
Iteration 14100: Loss = -12253.53515625
Iteration 14200: Loss = -12253.5341796875
Iteration 14300: Loss = -12253.5341796875
Iteration 14400: Loss = -12253.5341796875
Iteration 14500: Loss = -12253.5341796875
Iteration 14600: Loss = -12253.5341796875
Iteration 14700: Loss = -12253.5322265625
Iteration 14800: Loss = -12253.5341796875
1
Iteration 14900: Loss = -12253.5341796875
2
Iteration 15000: Loss = -12253.53125
Iteration 15100: Loss = -12253.5302734375
Iteration 15200: Loss = -12253.5302734375
Iteration 15300: Loss = -12253.529296875
Iteration 15400: Loss = -12253.5302734375
1
Iteration 15500: Loss = -12253.5302734375
2
Iteration 15600: Loss = -12253.5302734375
3
Iteration 15700: Loss = -12253.5302734375
4
Iteration 15800: Loss = -12253.529296875
Iteration 15900: Loss = -12253.5283203125
Iteration 16000: Loss = -12253.529296875
1
Iteration 16100: Loss = -12253.529296875
2
Iteration 16200: Loss = -12253.5302734375
3
Iteration 16300: Loss = -12253.52734375
Iteration 16400: Loss = -12253.529296875
1
Iteration 16500: Loss = -12253.52734375
Iteration 16600: Loss = -12253.529296875
1
Iteration 16700: Loss = -12253.52734375
Iteration 16800: Loss = -12253.5302734375
1
Iteration 16900: Loss = -12253.5283203125
2
Iteration 17000: Loss = -12253.5283203125
3
Iteration 17100: Loss = -12253.5263671875
Iteration 17200: Loss = -12253.5263671875
Iteration 17300: Loss = -12253.52734375
1
Iteration 17400: Loss = -12253.52734375
2
Iteration 17500: Loss = -12253.52734375
3
Iteration 17600: Loss = -12253.5263671875
Iteration 17700: Loss = -12253.525390625
Iteration 17800: Loss = -12253.525390625
Iteration 17900: Loss = -12253.525390625
Iteration 18000: Loss = -12253.5283203125
1
Iteration 18100: Loss = -12253.5263671875
2
Iteration 18200: Loss = -12253.5263671875
3
Iteration 18300: Loss = -12253.5263671875
4
Iteration 18400: Loss = -12253.525390625
Iteration 18500: Loss = -12253.5263671875
1
Iteration 18600: Loss = -12253.525390625
Iteration 18700: Loss = -12253.525390625
Iteration 18800: Loss = -12253.5263671875
1
Iteration 18900: Loss = -12253.525390625
Iteration 19000: Loss = -12253.5263671875
1
Iteration 19100: Loss = -12253.5283203125
2
Iteration 19200: Loss = -12253.525390625
Iteration 19300: Loss = -12253.5263671875
1
Iteration 19400: Loss = -12253.5244140625
Iteration 19500: Loss = -12253.5234375
Iteration 19600: Loss = -12253.5322265625
1
Iteration 19700: Loss = -12253.5263671875
2
Iteration 19800: Loss = -12253.5234375
Iteration 19900: Loss = -12253.525390625
1
Iteration 20000: Loss = -12253.5263671875
2
Iteration 20100: Loss = -12253.5234375
Iteration 20200: Loss = -12253.5234375
Iteration 20300: Loss = -12253.5234375
Iteration 20400: Loss = -12253.521484375
Iteration 20500: Loss = -12253.513671875
Iteration 20600: Loss = -12253.50390625
Iteration 20700: Loss = -12253.50390625
Iteration 20800: Loss = -12253.50390625
Iteration 20900: Loss = -12253.50390625
Iteration 21000: Loss = -12253.505859375
1
Iteration 21100: Loss = -12253.505859375
2
Iteration 21200: Loss = -12253.5048828125
3
Iteration 21300: Loss = -12253.5048828125
4
Iteration 21400: Loss = -12253.5048828125
5
Iteration 21500: Loss = -12253.50390625
Iteration 21600: Loss = -12253.5048828125
1
Iteration 21700: Loss = -12253.50390625
Iteration 21800: Loss = -12253.50390625
Iteration 21900: Loss = -12253.505859375
1
Iteration 22000: Loss = -12253.5048828125
2
Iteration 22100: Loss = -12253.50390625
Iteration 22200: Loss = -12253.50390625
Iteration 22300: Loss = -12253.5029296875
Iteration 22400: Loss = -12253.5048828125
1
Iteration 22500: Loss = -12253.5048828125
2
Iteration 22600: Loss = -12253.5029296875
Iteration 22700: Loss = -12253.50390625
1
Iteration 22800: Loss = -12253.50390625
2
Iteration 22900: Loss = -12253.505859375
3
Iteration 23000: Loss = -12253.5048828125
4
Iteration 23100: Loss = -12253.50390625
5
Iteration 23200: Loss = -12253.50390625
6
Iteration 23300: Loss = -12253.50390625
7
Iteration 23400: Loss = -12253.50390625
8
Iteration 23500: Loss = -12253.50390625
9
Iteration 23600: Loss = -12253.5029296875
Iteration 23700: Loss = -12253.505859375
1
Iteration 23800: Loss = -12253.50390625
2
Iteration 23900: Loss = -12253.50390625
3
Iteration 24000: Loss = -12253.505859375
4
Iteration 24100: Loss = -12253.50390625
5
Iteration 24200: Loss = -12253.5048828125
6
Iteration 24300: Loss = -12253.505859375
7
Iteration 24400: Loss = -12253.50390625
8
Iteration 24500: Loss = -12253.5048828125
9
Iteration 24600: Loss = -12253.505859375
10
Iteration 24700: Loss = -12253.505859375
11
Iteration 24800: Loss = -12253.50390625
12
Iteration 24900: Loss = -12253.50390625
13
Iteration 25000: Loss = -12253.501953125
Iteration 25100: Loss = -12253.5048828125
1
Iteration 25200: Loss = -12253.5048828125
2
Iteration 25300: Loss = -12253.505859375
3
Iteration 25400: Loss = -12253.5048828125
4
Iteration 25500: Loss = -12253.50390625
5
Iteration 25600: Loss = -12253.505859375
6
Iteration 25700: Loss = -12253.505859375
7
Iteration 25800: Loss = -12253.50390625
8
Iteration 25900: Loss = -12253.50390625
9
Iteration 26000: Loss = -12253.50390625
10
Iteration 26100: Loss = -12253.5048828125
11
Iteration 26200: Loss = -12253.5048828125
12
Iteration 26300: Loss = -12253.5048828125
13
Iteration 26400: Loss = -12253.50390625
14
Iteration 26500: Loss = -12253.5048828125
15
Stopping early at iteration 26500 due to no improvement.
pi: tensor([[1.0000e+00, 2.9169e-06],
        [1.0000e+00, 2.4600e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9720, 0.0280], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1965, 0.1732],
         [0.9919, 0.1544]],

        [[0.0595, 0.2224],
         [0.0142, 0.9765]],

        [[0.1852, 0.2067],
         [0.6788, 0.8995]],

        [[0.0129, 0.1951],
         [0.2016, 0.9663]],

        [[0.3503, 0.2024],
         [0.0191, 0.0752]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.03649118202185019, 0.0] [0.9919998119331364, 0.0] [11765.9384765625, 12253.5048828125]
-------------------------------------
This iteration is 22
True Objective function: Loss = -11844.99499279651
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -44451.6875
Iteration 100: Loss = -25963.482421875
Iteration 200: Loss = -15346.353515625
Iteration 300: Loss = -13474.62109375
Iteration 400: Loss = -13052.3115234375
Iteration 500: Loss = -12788.5107421875
Iteration 600: Loss = -12651.982421875
Iteration 700: Loss = -12554.71484375
Iteration 800: Loss = -12505.2041015625
Iteration 900: Loss = -12482.2412109375
Iteration 1000: Loss = -12455.244140625
Iteration 1100: Loss = -12431.9443359375
Iteration 1200: Loss = -12420.9951171875
Iteration 1300: Loss = -12408.69921875
Iteration 1400: Loss = -12403.779296875
Iteration 1500: Loss = -12399.5615234375
Iteration 1600: Loss = -12395.556640625
Iteration 1700: Loss = -12392.2958984375
Iteration 1800: Loss = -12389.8798828125
Iteration 1900: Loss = -12387.9462890625
Iteration 2000: Loss = -12386.310546875
Iteration 2100: Loss = -12384.8974609375
Iteration 2200: Loss = -12383.6591796875
Iteration 2300: Loss = -12382.5576171875
Iteration 2400: Loss = -12381.5595703125
Iteration 2500: Loss = -12380.6826171875
Iteration 2600: Loss = -12379.8935546875
Iteration 2700: Loss = -12379.1748046875
Iteration 2800: Loss = -12378.5166015625
Iteration 2900: Loss = -12377.9150390625
Iteration 3000: Loss = -12377.3408203125
Iteration 3100: Loss = -12376.7314453125
Iteration 3200: Loss = -12374.7734375
Iteration 3300: Loss = -12371.0810546875
Iteration 3400: Loss = -12370.3271484375
Iteration 3500: Loss = -12369.83984375
Iteration 3600: Loss = -12369.4423828125
Iteration 3700: Loss = -12369.091796875
Iteration 3800: Loss = -12368.77734375
Iteration 3900: Loss = -12368.4912109375
Iteration 4000: Loss = -12368.228515625
Iteration 4100: Loss = -12367.982421875
Iteration 4200: Loss = -12367.7568359375
Iteration 4300: Loss = -12367.548828125
Iteration 4400: Loss = -12367.3544921875
Iteration 4500: Loss = -12367.1728515625
Iteration 4600: Loss = -12367.0048828125
Iteration 4700: Loss = -12366.845703125
Iteration 4800: Loss = -12366.69921875
Iteration 4900: Loss = -12366.560546875
Iteration 5000: Loss = -12366.4296875
Iteration 5100: Loss = -12366.3076171875
Iteration 5200: Loss = -12366.193359375
Iteration 5300: Loss = -12366.0849609375
Iteration 5400: Loss = -12365.9833984375
Iteration 5500: Loss = -12365.8876953125
Iteration 5600: Loss = -12365.798828125
Iteration 5700: Loss = -12365.712890625
Iteration 5800: Loss = -12365.6318359375
Iteration 5900: Loss = -12365.5556640625
Iteration 6000: Loss = -12365.482421875
Iteration 6100: Loss = -12365.4150390625
Iteration 6200: Loss = -12365.349609375
Iteration 6300: Loss = -12365.2890625
Iteration 6400: Loss = -12365.2294921875
Iteration 6500: Loss = -12365.173828125
Iteration 6600: Loss = -12365.12109375
Iteration 6700: Loss = -12365.0712890625
Iteration 6800: Loss = -12365.0205078125
Iteration 6900: Loss = -12364.970703125
Iteration 7000: Loss = -12364.90234375
Iteration 7100: Loss = -12364.751953125
Iteration 7200: Loss = -12364.6279296875
Iteration 7300: Loss = -12364.556640625
Iteration 7400: Loss = -12364.494140625
Iteration 7500: Loss = -12364.4423828125
Iteration 7600: Loss = -12364.3935546875
Iteration 7700: Loss = -12364.353515625
Iteration 7800: Loss = -12364.3154296875
Iteration 7900: Loss = -12364.28125
Iteration 8000: Loss = -12364.2490234375
Iteration 8100: Loss = -12364.2197265625
Iteration 8200: Loss = -12364.1904296875
Iteration 8300: Loss = -12364.1669921875
Iteration 8400: Loss = -12364.1435546875
Iteration 8500: Loss = -12364.1220703125
Iteration 8600: Loss = -12364.103515625
Iteration 8700: Loss = -12364.0830078125
Iteration 8800: Loss = -12364.0654296875
Iteration 8900: Loss = -12364.0498046875
Iteration 9000: Loss = -12364.03515625
Iteration 9100: Loss = -12364.01953125
Iteration 9200: Loss = -12364.005859375
Iteration 9300: Loss = -12363.9931640625
Iteration 9400: Loss = -12363.9794921875
Iteration 9500: Loss = -12363.9677734375
Iteration 9600: Loss = -12363.95703125
Iteration 9700: Loss = -12363.9462890625
Iteration 9800: Loss = -12363.935546875
Iteration 9900: Loss = -12363.9267578125
Iteration 10000: Loss = -12363.916015625
Iteration 10100: Loss = -12363.908203125
Iteration 10200: Loss = -12363.9013671875
Iteration 10300: Loss = -12363.8935546875
Iteration 10400: Loss = -12363.884765625
Iteration 10500: Loss = -12363.87890625
Iteration 10600: Loss = -12363.8720703125
Iteration 10700: Loss = -12363.865234375
Iteration 10800: Loss = -12363.857421875
Iteration 10900: Loss = -12363.8525390625
Iteration 11000: Loss = -12363.8466796875
Iteration 11100: Loss = -12363.8388671875
Iteration 11200: Loss = -12363.8349609375
Iteration 11300: Loss = -12363.830078125
Iteration 11400: Loss = -12363.82421875
Iteration 11500: Loss = -12363.8212890625
Iteration 11600: Loss = -12363.814453125
Iteration 11700: Loss = -12363.810546875
Iteration 11800: Loss = -12363.8076171875
Iteration 11900: Loss = -12363.802734375
Iteration 12000: Loss = -12363.7978515625
Iteration 12100: Loss = -12363.7939453125
Iteration 12200: Loss = -12363.791015625
Iteration 12300: Loss = -12363.787109375
Iteration 12400: Loss = -12363.7841796875
Iteration 12500: Loss = -12363.78125
Iteration 12600: Loss = -12363.7783203125
Iteration 12700: Loss = -12363.7744140625
Iteration 12800: Loss = -12363.7705078125
Iteration 12900: Loss = -12363.7666015625
Iteration 13000: Loss = -12363.7646484375
Iteration 13100: Loss = -12363.7626953125
Iteration 13200: Loss = -12363.7587890625
Iteration 13300: Loss = -12363.7568359375
Iteration 13400: Loss = -12363.75390625
Iteration 13500: Loss = -12363.75
Iteration 13600: Loss = -12363.7490234375
Iteration 13700: Loss = -12363.7470703125
Iteration 13800: Loss = -12363.7421875
Iteration 13900: Loss = -12363.740234375
Iteration 14000: Loss = -12363.73828125
Iteration 14100: Loss = -12363.734375
Iteration 14200: Loss = -12363.7333984375
Iteration 14300: Loss = -12363.73046875
Iteration 14400: Loss = -12363.7275390625
Iteration 14500: Loss = -12363.7255859375
Iteration 14600: Loss = -12363.72265625
Iteration 14700: Loss = -12363.7197265625
Iteration 14800: Loss = -12363.7158203125
Iteration 14900: Loss = -12363.7138671875
Iteration 15000: Loss = -12363.708984375
Iteration 15100: Loss = -12363.703125
Iteration 15200: Loss = -12363.7021484375
Iteration 15300: Loss = -12363.6962890625
Iteration 15400: Loss = -12363.6923828125
Iteration 15500: Loss = -12363.689453125
Iteration 15600: Loss = -12363.685546875
Iteration 15700: Loss = -12363.6767578125
Iteration 15800: Loss = -12363.6728515625
Iteration 15900: Loss = -12363.666015625
Iteration 16000: Loss = -12363.66015625
Iteration 16100: Loss = -12363.654296875
Iteration 16200: Loss = -12363.64453125
Iteration 16300: Loss = -12363.6376953125
Iteration 16400: Loss = -12363.6328125
Iteration 16500: Loss = -12363.625
Iteration 16600: Loss = -12363.6171875
Iteration 16700: Loss = -12363.6123046875
Iteration 16800: Loss = -12363.609375
Iteration 16900: Loss = -12363.6015625
Iteration 17000: Loss = -12363.5986328125
Iteration 17100: Loss = -12363.595703125
Iteration 17200: Loss = -12363.58984375
Iteration 17300: Loss = -12363.5576171875
Iteration 17400: Loss = -12363.2607421875
Iteration 17500: Loss = -12363.103515625
Iteration 17600: Loss = -12363.06640625
Iteration 17700: Loss = -12363.060546875
Iteration 17800: Loss = -12363.0576171875
Iteration 17900: Loss = -12363.056640625
Iteration 18000: Loss = -12363.0556640625
Iteration 18100: Loss = -12363.0537109375
Iteration 18200: Loss = -12363.0556640625
1
Iteration 18300: Loss = -12363.0546875
2
Iteration 18400: Loss = -12363.0546875
3
Iteration 18500: Loss = -12363.0537109375
Iteration 18600: Loss = -12363.0546875
1
Iteration 18700: Loss = -12363.052734375
Iteration 18800: Loss = -12363.0537109375
1
Iteration 18900: Loss = -12363.052734375
Iteration 19000: Loss = -12363.052734375
Iteration 19100: Loss = -12363.0537109375
1
Iteration 19200: Loss = -12363.052734375
Iteration 19300: Loss = -12363.0537109375
1
Iteration 19400: Loss = -12363.0517578125
Iteration 19500: Loss = -12363.052734375
1
Iteration 19600: Loss = -12363.0537109375
2
Iteration 19700: Loss = -12363.052734375
3
Iteration 19800: Loss = -12363.05078125
Iteration 19900: Loss = -12363.0498046875
Iteration 20000: Loss = -12362.96484375
Iteration 20100: Loss = -12362.9609375
Iteration 20200: Loss = -12362.9599609375
Iteration 20300: Loss = -12362.9609375
1
Iteration 20400: Loss = -12362.9619140625
2
Iteration 20500: Loss = -12362.9609375
3
Iteration 20600: Loss = -12362.958984375
Iteration 20700: Loss = -12362.947265625
Iteration 20800: Loss = -12362.939453125
Iteration 20900: Loss = -12362.9375
Iteration 21000: Loss = -12362.9375
Iteration 21100: Loss = -12362.927734375
Iteration 21200: Loss = -12362.9248046875
Iteration 21300: Loss = -12362.9033203125
Iteration 21400: Loss = -12362.8984375
Iteration 21500: Loss = -12362.8876953125
Iteration 21600: Loss = -12362.876953125
Iteration 21700: Loss = -12362.8583984375
Iteration 21800: Loss = -12362.849609375
Iteration 21900: Loss = -12362.8388671875
Iteration 22000: Loss = -12362.830078125
Iteration 22100: Loss = -12362.8134765625
Iteration 22200: Loss = -12362.802734375
Iteration 22300: Loss = -12362.802734375
Iteration 22400: Loss = -12362.7939453125
Iteration 22500: Loss = -12362.7900390625
Iteration 22600: Loss = -12362.7822265625
Iteration 22700: Loss = -12362.7783203125
Iteration 22800: Loss = -12362.771484375
Iteration 22900: Loss = -12362.7568359375
Iteration 23000: Loss = -12362.75
Iteration 23100: Loss = -12362.7333984375
Iteration 23200: Loss = -12362.73046875
Iteration 23300: Loss = -12362.7255859375
Iteration 23400: Loss = -12362.70703125
Iteration 23500: Loss = -12362.7060546875
Iteration 23600: Loss = -12362.6953125
Iteration 23700: Loss = -12362.677734375
Iteration 23800: Loss = -12362.6728515625
Iteration 23900: Loss = -12362.671875
Iteration 24000: Loss = -12362.6689453125
Iteration 24100: Loss = -12362.6533203125
Iteration 24200: Loss = -12362.65234375
Iteration 24300: Loss = -12362.6494140625
Iteration 24400: Loss = -12362.6513671875
1
Iteration 24500: Loss = -12362.6513671875
2
Iteration 24600: Loss = -12362.6513671875
3
Iteration 24700: Loss = -12362.65234375
4
Iteration 24800: Loss = -12362.6513671875
5
Iteration 24900: Loss = -12362.6494140625
Iteration 25000: Loss = -12362.65234375
1
Iteration 25100: Loss = -12362.6533203125
2
Iteration 25200: Loss = -12362.6513671875
3
Iteration 25300: Loss = -12362.650390625
4
Iteration 25400: Loss = -12362.6513671875
5
Iteration 25500: Loss = -12362.650390625
6
Iteration 25600: Loss = -12362.642578125
Iteration 25700: Loss = -12362.6455078125
1
Iteration 25800: Loss = -12362.64453125
2
Iteration 25900: Loss = -12362.642578125
Iteration 26000: Loss = -12362.64453125
1
Iteration 26100: Loss = -12362.6435546875
2
Iteration 26200: Loss = -12362.64453125
3
Iteration 26300: Loss = -12362.6416015625
Iteration 26400: Loss = -12362.64453125
1
Iteration 26500: Loss = -12362.642578125
2
Iteration 26600: Loss = -12362.6435546875
3
Iteration 26700: Loss = -12362.6416015625
Iteration 26800: Loss = -12362.642578125
1
Iteration 26900: Loss = -12362.6435546875
2
Iteration 27000: Loss = -12362.6435546875
3
Iteration 27100: Loss = -12362.642578125
4
Iteration 27200: Loss = -12362.642578125
5
Iteration 27300: Loss = -12362.6455078125
6
Iteration 27400: Loss = -12362.6416015625
Iteration 27500: Loss = -12362.642578125
1
Iteration 27600: Loss = -12362.642578125
2
Iteration 27700: Loss = -12362.646484375
3
Iteration 27800: Loss = -12362.6435546875
4
Iteration 27900: Loss = -12362.642578125
5
Iteration 28000: Loss = -12362.642578125
6
Iteration 28100: Loss = -12362.6416015625
Iteration 28200: Loss = -12362.642578125
1
Iteration 28300: Loss = -12362.6455078125
2
Iteration 28400: Loss = -12362.6435546875
3
Iteration 28500: Loss = -12362.642578125
4
Iteration 28600: Loss = -12362.6435546875
5
Iteration 28700: Loss = -12362.642578125
6
Iteration 28800: Loss = -12362.64453125
7
Iteration 28900: Loss = -12362.6416015625
Iteration 29000: Loss = -12362.642578125
1
Iteration 29100: Loss = -12362.64453125
2
Iteration 29200: Loss = -12362.6416015625
Iteration 29300: Loss = -12362.642578125
1
Iteration 29400: Loss = -12362.642578125
2
Iteration 29500: Loss = -12362.64453125
3
Iteration 29600: Loss = -12362.6435546875
4
Iteration 29700: Loss = -12362.6435546875
5
Iteration 29800: Loss = -12362.642578125
6
Iteration 29900: Loss = -12362.642578125
7
pi: tensor([[9.9134e-01, 8.6578e-03],
        [1.0000e+00, 2.3678e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9674, 0.0326], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2001, 0.1282],
         [0.3527, 0.0870]],

        [[0.8157, 0.3403],
         [0.7302, 0.1765]],

        [[0.2912, 0.1500],
         [0.4855, 0.8240]],

        [[0.7421, 0.1852],
         [0.9800, 0.9604]],

        [[0.4707, 0.1823],
         [0.7095, 0.7671]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0002480639543726333
Average Adjusted Rand Index: -0.0005926784880256398
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36563.94921875
Iteration 100: Loss = -20658.94921875
Iteration 200: Loss = -14217.4482421875
Iteration 300: Loss = -13016.1337890625
Iteration 400: Loss = -12754.49609375
Iteration 500: Loss = -12617.873046875
Iteration 600: Loss = -12542.70703125
Iteration 700: Loss = -12489.255859375
Iteration 800: Loss = -12454.658203125
Iteration 900: Loss = -12433.1328125
Iteration 1000: Loss = -12419.2939453125
Iteration 1100: Loss = -12410.3486328125
Iteration 1200: Loss = -12403.1396484375
Iteration 1300: Loss = -12399.4326171875
Iteration 1400: Loss = -12394.7255859375
Iteration 1500: Loss = -12389.806640625
Iteration 1600: Loss = -12386.201171875
Iteration 1700: Loss = -12384.4697265625
Iteration 1800: Loss = -12383.0244140625
Iteration 1900: Loss = -12381.78125
Iteration 2000: Loss = -12380.705078125
Iteration 2100: Loss = -12379.763671875
Iteration 2200: Loss = -12378.927734375
Iteration 2300: Loss = -12378.1865234375
Iteration 2400: Loss = -12377.5234375
Iteration 2500: Loss = -12376.9296875
Iteration 2600: Loss = -12376.390625
Iteration 2700: Loss = -12375.9072265625
Iteration 2800: Loss = -12375.466796875
Iteration 2900: Loss = -12375.064453125
Iteration 3000: Loss = -12374.6962890625
Iteration 3100: Loss = -12374.359375
Iteration 3200: Loss = -12374.0478515625
Iteration 3300: Loss = -12373.7666015625
Iteration 3400: Loss = -12373.5
Iteration 3500: Loss = -12373.2412109375
Iteration 3600: Loss = -12368.3466796875
Iteration 3700: Loss = -12367.6923828125
Iteration 3800: Loss = -12367.3857421875
Iteration 3900: Loss = -12367.1416015625
Iteration 4000: Loss = -12366.92578125
Iteration 4100: Loss = -12366.7373046875
Iteration 4200: Loss = -12366.5634765625
Iteration 4300: Loss = -12366.40625
Iteration 4400: Loss = -12366.259765625
Iteration 4500: Loss = -12366.1279296875
Iteration 4600: Loss = -12366.0029296875
Iteration 4700: Loss = -12365.8857421875
Iteration 4800: Loss = -12365.7783203125
Iteration 4900: Loss = -12365.6767578125
Iteration 5000: Loss = -12365.583984375
Iteration 5100: Loss = -12365.494140625
Iteration 5200: Loss = -12365.412109375
Iteration 5300: Loss = -12365.3310546875
Iteration 5400: Loss = -12365.2568359375
Iteration 5500: Loss = -12365.1796875
Iteration 5600: Loss = -12365.04296875
Iteration 5700: Loss = -12364.9111328125
Iteration 5800: Loss = -12364.7958984375
Iteration 5900: Loss = -12364.6923828125
Iteration 6000: Loss = -12364.5986328125
Iteration 6100: Loss = -12364.515625
Iteration 6200: Loss = -12364.4404296875
Iteration 6300: Loss = -12364.3701171875
Iteration 6400: Loss = -12364.3056640625
Iteration 6500: Loss = -12364.240234375
Iteration 6600: Loss = -12364.1826171875
Iteration 6700: Loss = -12364.1240234375
Iteration 6800: Loss = -12364.07421875
Iteration 6900: Loss = -12364.0283203125
Iteration 7000: Loss = -12363.9833984375
Iteration 7100: Loss = -12363.943359375
Iteration 7200: Loss = -12363.9013671875
Iteration 7300: Loss = -12363.873046875
Iteration 7400: Loss = -12363.833984375
Iteration 7500: Loss = -12363.8056640625
Iteration 7600: Loss = -12363.78125
Iteration 7700: Loss = -12363.7578125
Iteration 7800: Loss = -12363.740234375
Iteration 7900: Loss = -12363.7216796875
Iteration 8000: Loss = -12363.7060546875
Iteration 8100: Loss = -12363.6923828125
Iteration 8200: Loss = -12363.67578125
Iteration 8300: Loss = -12363.6640625
Iteration 8400: Loss = -12363.650390625
Iteration 8500: Loss = -12363.638671875
Iteration 8600: Loss = -12363.6259765625
Iteration 8700: Loss = -12363.6162109375
Iteration 8800: Loss = -12363.60546875
Iteration 8900: Loss = -12363.5947265625
Iteration 9000: Loss = -12363.583984375
Iteration 9100: Loss = -12363.5771484375
Iteration 9200: Loss = -12363.5693359375
Iteration 9300: Loss = -12363.5595703125
Iteration 9400: Loss = -12363.5517578125
Iteration 9500: Loss = -12363.5439453125
Iteration 9600: Loss = -12363.537109375
Iteration 9700: Loss = -12363.529296875
Iteration 9800: Loss = -12363.5205078125
Iteration 9900: Loss = -12363.5146484375
Iteration 10000: Loss = -12363.505859375
Iteration 10100: Loss = -12363.4990234375
Iteration 10200: Loss = -12363.4931640625
Iteration 10300: Loss = -12363.486328125
Iteration 10400: Loss = -12363.4814453125
Iteration 10500: Loss = -12363.4736328125
Iteration 10600: Loss = -12363.466796875
Iteration 10700: Loss = -12363.4619140625
Iteration 10800: Loss = -12363.45703125
Iteration 10900: Loss = -12363.451171875
Iteration 11000: Loss = -12363.4443359375
Iteration 11100: Loss = -12363.4384765625
Iteration 11200: Loss = -12363.4306640625
Iteration 11300: Loss = -12363.4267578125
Iteration 11400: Loss = -12363.4208984375
Iteration 11500: Loss = -12363.4140625
Iteration 11600: Loss = -12363.408203125
Iteration 11700: Loss = -12363.400390625
Iteration 11800: Loss = -12363.392578125
Iteration 11900: Loss = -12363.3837890625
Iteration 12000: Loss = -12363.3779296875
Iteration 12100: Loss = -12363.3701171875
Iteration 12200: Loss = -12363.3623046875
Iteration 12300: Loss = -12363.3505859375
Iteration 12400: Loss = -12363.3427734375
Iteration 12500: Loss = -12363.3310546875
Iteration 12600: Loss = -12363.3203125
Iteration 12700: Loss = -12363.306640625
Iteration 12800: Loss = -12363.29296875
Iteration 12900: Loss = -12363.2763671875
Iteration 13000: Loss = -12363.2607421875
Iteration 13100: Loss = -12363.2412109375
Iteration 13200: Loss = -12363.2216796875
Iteration 13300: Loss = -12363.1962890625
Iteration 13400: Loss = -12363.171875
Iteration 13500: Loss = -12363.1396484375
Iteration 13600: Loss = -12363.1005859375
Iteration 13700: Loss = -12363.033203125
Iteration 13800: Loss = -12362.904296875
Iteration 13900: Loss = -12362.802734375
Iteration 14000: Loss = -12362.68359375
Iteration 14100: Loss = -12362.5615234375
Iteration 14200: Loss = -12362.458984375
Iteration 14300: Loss = -12362.3955078125
Iteration 14400: Loss = -12362.365234375
Iteration 14500: Loss = -12362.3466796875
Iteration 14600: Loss = -12362.337890625
Iteration 14700: Loss = -12362.3310546875
Iteration 14800: Loss = -12362.3271484375
Iteration 14900: Loss = -12362.3232421875
Iteration 15000: Loss = -12362.3203125
Iteration 15100: Loss = -12362.318359375
Iteration 15200: Loss = -12362.31640625
Iteration 15300: Loss = -12362.3154296875
Iteration 15400: Loss = -12362.3154296875
Iteration 15500: Loss = -12362.3125
Iteration 15600: Loss = -12362.3125
Iteration 15700: Loss = -12362.3125
Iteration 15800: Loss = -12362.3115234375
Iteration 15900: Loss = -12362.3115234375
Iteration 16000: Loss = -12362.310546875
Iteration 16100: Loss = -12362.3115234375
1
Iteration 16200: Loss = -12362.3095703125
Iteration 16300: Loss = -12362.3095703125
Iteration 16400: Loss = -12362.3115234375
1
Iteration 16500: Loss = -12362.30859375
Iteration 16600: Loss = -12362.30859375
Iteration 16700: Loss = -12362.3076171875
Iteration 16800: Loss = -12362.3095703125
1
Iteration 16900: Loss = -12362.306640625
Iteration 17000: Loss = -12362.3076171875
1
Iteration 17100: Loss = -12362.306640625
Iteration 17200: Loss = -12362.3076171875
1
Iteration 17300: Loss = -12362.3076171875
2
Iteration 17400: Loss = -12362.3056640625
Iteration 17500: Loss = -12362.306640625
1
Iteration 17600: Loss = -12362.3056640625
Iteration 17700: Loss = -12362.306640625
1
Iteration 17800: Loss = -12362.3076171875
2
Iteration 17900: Loss = -12362.306640625
3
Iteration 18000: Loss = -12362.3056640625
Iteration 18100: Loss = -12362.3046875
Iteration 18200: Loss = -12362.306640625
1
Iteration 18300: Loss = -12362.3046875
Iteration 18400: Loss = -12362.3056640625
1
Iteration 18500: Loss = -12362.3046875
Iteration 18600: Loss = -12362.3046875
Iteration 18700: Loss = -12362.3056640625
1
Iteration 18800: Loss = -12362.3046875
Iteration 18900: Loss = -12362.302734375
Iteration 19000: Loss = -12362.3046875
1
Iteration 19100: Loss = -12362.3046875
2
Iteration 19200: Loss = -12362.3115234375
3
Iteration 19300: Loss = -12362.3037109375
4
Iteration 19400: Loss = -12362.3037109375
5
Iteration 19500: Loss = -12362.3037109375
6
Iteration 19600: Loss = -12362.3046875
7
Iteration 19700: Loss = -12362.3046875
8
Iteration 19800: Loss = -12362.3037109375
9
Iteration 19900: Loss = -12362.3056640625
10
Iteration 20000: Loss = -12362.3046875
11
Iteration 20100: Loss = -12362.3037109375
12
Iteration 20200: Loss = -12362.302734375
Iteration 20300: Loss = -12362.3037109375
1
Iteration 20400: Loss = -12362.3046875
2
Iteration 20500: Loss = -12362.302734375
Iteration 20600: Loss = -12362.3037109375
1
Iteration 20700: Loss = -12362.3037109375
2
Iteration 20800: Loss = -12362.302734375
Iteration 20900: Loss = -12362.3017578125
Iteration 21000: Loss = -12362.302734375
1
Iteration 21100: Loss = -12362.302734375
2
Iteration 21200: Loss = -12362.3037109375
3
Iteration 21300: Loss = -12362.302734375
4
Iteration 21400: Loss = -12362.3037109375
5
Iteration 21500: Loss = -12362.3046875
6
Iteration 21600: Loss = -12362.302734375
7
Iteration 21700: Loss = -12362.3037109375
8
Iteration 21800: Loss = -12362.3037109375
9
Iteration 21900: Loss = -12362.30078125
Iteration 22000: Loss = -12362.302734375
1
Iteration 22100: Loss = -12362.302734375
2
Iteration 22200: Loss = -12362.3037109375
3
Iteration 22300: Loss = -12362.302734375
4
Iteration 22400: Loss = -12362.302734375
5
Iteration 22500: Loss = -12362.302734375
6
Iteration 22600: Loss = -12362.302734375
7
Iteration 22700: Loss = -12362.3037109375
8
Iteration 22800: Loss = -12362.302734375
9
Iteration 22900: Loss = -12362.302734375
10
Iteration 23000: Loss = -12362.3017578125
11
Iteration 23100: Loss = -12362.3017578125
12
Iteration 23200: Loss = -12362.3037109375
13
Iteration 23300: Loss = -12362.3017578125
14
Iteration 23400: Loss = -12362.3037109375
15
Stopping early at iteration 23400 due to no improvement.
pi: tensor([[9.8522e-01, 1.4785e-02],
        [9.9998e-01, 2.4952e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9673, 0.0327], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2001, 0.1293],
         [0.8291, 0.1144]],

        [[0.2441, 0.3286],
         [0.9468, 0.0323]],

        [[0.6323, 0.1519],
         [0.0120, 0.0244]],

        [[0.9475, 0.1915],
         [0.1461, 0.9242]],

        [[0.1797, 0.1826],
         [0.0097, 0.8119]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0002480639543726333
Average Adjusted Rand Index: -0.0005926784880256398
[-0.0002480639543726333, -0.0002480639543726333] [-0.0005926784880256398, -0.0005926784880256398] [12362.6416015625, 12362.3037109375]
-------------------------------------
This iteration is 23
True Objective function: Loss = -11823.619180424665
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -47641.3125
Iteration 100: Loss = -27757.2109375
Iteration 200: Loss = -15892.365234375
Iteration 300: Loss = -13424.7744140625
Iteration 400: Loss = -12946.8095703125
Iteration 500: Loss = -12769.3330078125
Iteration 600: Loss = -12671.3291015625
Iteration 700: Loss = -12606.0439453125
Iteration 800: Loss = -12538.2373046875
Iteration 900: Loss = -12494.0185546875
Iteration 1000: Loss = -12462.3427734375
Iteration 1100: Loss = -12440.083984375
Iteration 1200: Loss = -12422.6640625
Iteration 1300: Loss = -12414.697265625
Iteration 1400: Loss = -12408.3828125
Iteration 1500: Loss = -12403.0380859375
Iteration 1600: Loss = -12398.333984375
Iteration 1700: Loss = -12394.3935546875
Iteration 1800: Loss = -12388.060546875
Iteration 1900: Loss = -12384.826171875
Iteration 2000: Loss = -12382.1982421875
Iteration 2100: Loss = -12378.5908203125
Iteration 2200: Loss = -12375.8359375
Iteration 2300: Loss = -12373.4482421875
Iteration 2400: Loss = -12371.4599609375
Iteration 2500: Loss = -12369.7763671875
Iteration 2600: Loss = -12368.3125
Iteration 2700: Loss = -12366.8193359375
Iteration 2800: Loss = -12363.734375
Iteration 2900: Loss = -12361.365234375
Iteration 3000: Loss = -12360.02734375
Iteration 3100: Loss = -12359.02734375
Iteration 3200: Loss = -12358.2119140625
Iteration 3300: Loss = -12357.513671875
Iteration 3400: Loss = -12356.8984375
Iteration 3500: Loss = -12356.3505859375
Iteration 3600: Loss = -12355.853515625
Iteration 3700: Loss = -12355.4033203125
Iteration 3800: Loss = -12354.9912109375
Iteration 3900: Loss = -12354.615234375
Iteration 4000: Loss = -12354.26953125
Iteration 4100: Loss = -12353.953125
Iteration 4200: Loss = -12353.658203125
Iteration 4300: Loss = -12353.388671875
Iteration 4400: Loss = -12353.13671875
Iteration 4500: Loss = -12352.90625
Iteration 4600: Loss = -12352.689453125
Iteration 4700: Loss = -12352.4892578125
Iteration 4800: Loss = -12352.3046875
Iteration 4900: Loss = -12352.1318359375
Iteration 5000: Loss = -12351.9716796875
Iteration 5100: Loss = -12351.8203125
Iteration 5200: Loss = -12351.6806640625
Iteration 5300: Loss = -12351.5498046875
Iteration 5400: Loss = -12351.427734375
Iteration 5500: Loss = -12351.3134765625
Iteration 5600: Loss = -12351.208984375
Iteration 5700: Loss = -12351.1103515625
Iteration 5800: Loss = -12351.0185546875
Iteration 5900: Loss = -12350.9345703125
Iteration 6000: Loss = -12350.8525390625
Iteration 6100: Loss = -12350.77734375
Iteration 6200: Loss = -12350.7080078125
Iteration 6300: Loss = -12350.6396484375
Iteration 6400: Loss = -12350.5771484375
Iteration 6500: Loss = -12350.5146484375
Iteration 6600: Loss = -12350.4560546875
Iteration 6700: Loss = -12350.3984375
Iteration 6800: Loss = -12350.345703125
Iteration 6900: Loss = -12350.2919921875
Iteration 7000: Loss = -12350.23828125
Iteration 7100: Loss = -12350.1923828125
Iteration 7200: Loss = -12350.1337890625
Iteration 7300: Loss = -12350.0927734375
Iteration 7400: Loss = -12350.056640625
Iteration 7500: Loss = -12350.025390625
Iteration 7600: Loss = -12349.9931640625
Iteration 7700: Loss = -12349.966796875
Iteration 7800: Loss = -12349.94140625
Iteration 7900: Loss = -12349.91796875
Iteration 8000: Loss = -12349.8935546875
Iteration 8100: Loss = -12349.8740234375
Iteration 8200: Loss = -12349.8525390625
Iteration 8300: Loss = -12349.830078125
Iteration 8400: Loss = -12349.8134765625
Iteration 8500: Loss = -12349.79296875
Iteration 8600: Loss = -12349.7763671875
Iteration 8700: Loss = -12349.759765625
Iteration 8800: Loss = -12349.744140625
Iteration 8900: Loss = -12349.7294921875
Iteration 9000: Loss = -12349.712890625
Iteration 9100: Loss = -12349.6982421875
Iteration 9200: Loss = -12349.68359375
Iteration 9300: Loss = -12349.66796875
Iteration 9400: Loss = -12349.6513671875
Iteration 9500: Loss = -12349.63671875
Iteration 9600: Loss = -12349.6259765625
Iteration 9700: Loss = -12349.615234375
Iteration 9800: Loss = -12349.60546875
Iteration 9900: Loss = -12349.5947265625
Iteration 10000: Loss = -12349.583984375
Iteration 10100: Loss = -12349.5751953125
Iteration 10200: Loss = -12349.56640625
Iteration 10300: Loss = -12349.556640625
Iteration 10400: Loss = -12349.5478515625
Iteration 10500: Loss = -12349.5380859375
Iteration 10600: Loss = -12349.53125
Iteration 10700: Loss = -12349.5234375
Iteration 10800: Loss = -12349.513671875
Iteration 10900: Loss = -12349.5087890625
Iteration 11000: Loss = -12349.5009765625
Iteration 11100: Loss = -12349.4912109375
Iteration 11200: Loss = -12349.4853515625
Iteration 11300: Loss = -12349.4794921875
Iteration 11400: Loss = -12349.4736328125
Iteration 11500: Loss = -12349.462890625
Iteration 11600: Loss = -12349.453125
Iteration 11700: Loss = -12349.447265625
Iteration 11800: Loss = -12349.4404296875
Iteration 11900: Loss = -12349.435546875
Iteration 12000: Loss = -12349.4306640625
Iteration 12100: Loss = -12349.4267578125
Iteration 12200: Loss = -12349.421875
Iteration 12300: Loss = -12349.4169921875
Iteration 12400: Loss = -12349.4140625
Iteration 12500: Loss = -12349.4091796875
Iteration 12600: Loss = -12349.4033203125
Iteration 12700: Loss = -12349.3994140625
Iteration 12800: Loss = -12349.3955078125
Iteration 12900: Loss = -12349.392578125
Iteration 13000: Loss = -12349.388671875
Iteration 13100: Loss = -12349.384765625
Iteration 13200: Loss = -12349.380859375
Iteration 13300: Loss = -12349.376953125
Iteration 13400: Loss = -12349.373046875
Iteration 13500: Loss = -12349.369140625
Iteration 13600: Loss = -12349.3662109375
Iteration 13700: Loss = -12349.3623046875
Iteration 13800: Loss = -12349.357421875
Iteration 13900: Loss = -12349.3525390625
Iteration 14000: Loss = -12349.349609375
Iteration 14100: Loss = -12349.3447265625
Iteration 14200: Loss = -12349.33984375
Iteration 14300: Loss = -12349.3349609375
Iteration 14400: Loss = -12349.3310546875
Iteration 14500: Loss = -12349.3310546875
Iteration 14600: Loss = -12349.32421875
Iteration 14700: Loss = -12349.3203125
Iteration 14800: Loss = -12349.3173828125
Iteration 14900: Loss = -12349.30859375
Iteration 15000: Loss = -12349.3046875
Iteration 15100: Loss = -12349.2998046875
Iteration 15200: Loss = -12349.298828125
Iteration 15300: Loss = -12349.2958984375
Iteration 15400: Loss = -12349.2939453125
Iteration 15500: Loss = -12349.29296875
Iteration 15600: Loss = -12349.2890625
Iteration 15700: Loss = -12349.287109375
Iteration 15800: Loss = -12349.28515625
Iteration 15900: Loss = -12349.2861328125
1
Iteration 16000: Loss = -12349.2822265625
Iteration 16100: Loss = -12349.28125
Iteration 16200: Loss = -12349.275390625
Iteration 16300: Loss = -12349.275390625
Iteration 16400: Loss = -12349.2734375
Iteration 16500: Loss = -12349.26953125
Iteration 16600: Loss = -12349.2705078125
1
Iteration 16700: Loss = -12349.2705078125
2
Iteration 16800: Loss = -12349.2705078125
3
Iteration 16900: Loss = -12349.2685546875
Iteration 17000: Loss = -12349.2685546875
Iteration 17100: Loss = -12349.2666015625
Iteration 17200: Loss = -12349.267578125
1
Iteration 17300: Loss = -12349.2666015625
Iteration 17400: Loss = -12349.267578125
1
Iteration 17500: Loss = -12349.265625
Iteration 17600: Loss = -12349.265625
Iteration 17700: Loss = -12349.265625
Iteration 17800: Loss = -12349.2666015625
1
Iteration 17900: Loss = -12349.265625
Iteration 18000: Loss = -12349.265625
Iteration 18100: Loss = -12349.2666015625
1
Iteration 18200: Loss = -12349.2646484375
Iteration 18300: Loss = -12349.2646484375
Iteration 18400: Loss = -12349.2646484375
Iteration 18500: Loss = -12349.2646484375
Iteration 18600: Loss = -12349.2626953125
Iteration 18700: Loss = -12349.2607421875
Iteration 18800: Loss = -12349.2578125
Iteration 18900: Loss = -12349.2568359375
Iteration 19000: Loss = -12349.2548828125
Iteration 19100: Loss = -12349.2490234375
Iteration 19200: Loss = -12349.2470703125
Iteration 19300: Loss = -12349.2412109375
Iteration 19400: Loss = -12349.2138671875
Iteration 19500: Loss = -12348.9873046875
Iteration 19600: Loss = -12348.6748046875
Iteration 19700: Loss = -12348.4208984375
Iteration 19800: Loss = -12347.50390625
Iteration 19900: Loss = -12347.4794921875
Iteration 20000: Loss = -12347.470703125
Iteration 20100: Loss = -12347.466796875
Iteration 20200: Loss = -12347.466796875
Iteration 20300: Loss = -12347.4619140625
Iteration 20400: Loss = -12347.4619140625
Iteration 20500: Loss = -12347.4609375
Iteration 20600: Loss = -12347.458984375
Iteration 20700: Loss = -12347.4599609375
1
Iteration 20800: Loss = -12347.458984375
Iteration 20900: Loss = -12347.4580078125
Iteration 21000: Loss = -12347.45703125
Iteration 21100: Loss = -12347.4560546875
Iteration 21200: Loss = -12347.4580078125
1
Iteration 21300: Loss = -12347.45703125
2
Iteration 21400: Loss = -12347.45703125
3
Iteration 21500: Loss = -12347.4560546875
Iteration 21600: Loss = -12347.45703125
1
Iteration 21700: Loss = -12347.455078125
Iteration 21800: Loss = -12347.4560546875
1
Iteration 21900: Loss = -12347.4560546875
2
Iteration 22000: Loss = -12347.4560546875
3
Iteration 22100: Loss = -12347.45703125
4
Iteration 22200: Loss = -12347.4560546875
5
Iteration 22300: Loss = -12347.45703125
6
Iteration 22400: Loss = -12347.455078125
Iteration 22500: Loss = -12347.4560546875
1
Iteration 22600: Loss = -12347.4541015625
Iteration 22700: Loss = -12347.4541015625
Iteration 22800: Loss = -12347.4560546875
1
Iteration 22900: Loss = -12347.4541015625
Iteration 23000: Loss = -12347.45703125
1
Iteration 23100: Loss = -12347.4560546875
2
Iteration 23200: Loss = -12347.4541015625
Iteration 23300: Loss = -12347.455078125
1
Iteration 23400: Loss = -12347.4541015625
Iteration 23500: Loss = -12347.455078125
1
Iteration 23600: Loss = -12347.4541015625
Iteration 23700: Loss = -12347.455078125
1
Iteration 23800: Loss = -12347.4560546875
2
Iteration 23900: Loss = -12347.4541015625
Iteration 24000: Loss = -12347.4541015625
Iteration 24100: Loss = -12347.4560546875
1
Iteration 24200: Loss = -12347.4560546875
2
Iteration 24300: Loss = -12347.455078125
3
Iteration 24400: Loss = -12347.4560546875
4
Iteration 24500: Loss = -12347.455078125
5
Iteration 24600: Loss = -12347.3583984375
Iteration 24700: Loss = -12347.359375
1
Iteration 24800: Loss = -12347.359375
2
Iteration 24900: Loss = -12347.359375
3
Iteration 25000: Loss = -12347.359375
4
Iteration 25100: Loss = -12347.359375
5
Iteration 25200: Loss = -12347.359375
6
Iteration 25300: Loss = -12347.359375
7
Iteration 25400: Loss = -12347.359375
8
Iteration 25500: Loss = -12347.365234375
9
Iteration 25600: Loss = -12347.357421875
Iteration 25700: Loss = -12347.3583984375
1
Iteration 25800: Loss = -12347.359375
2
Iteration 25900: Loss = -12347.3583984375
3
Iteration 26000: Loss = -12347.3603515625
4
Iteration 26100: Loss = -12347.3564453125
Iteration 26200: Loss = -12347.3564453125
Iteration 26300: Loss = -12347.35546875
Iteration 26400: Loss = -12347.357421875
1
Iteration 26500: Loss = -12347.35546875
Iteration 26600: Loss = -12347.35546875
Iteration 26700: Loss = -12347.3564453125
1
Iteration 26800: Loss = -12347.357421875
2
Iteration 26900: Loss = -12347.3564453125
3
Iteration 27000: Loss = -12347.3564453125
4
Iteration 27100: Loss = -12347.35546875
Iteration 27200: Loss = -12347.3564453125
1
Iteration 27300: Loss = -12347.35546875
Iteration 27400: Loss = -12347.3564453125
1
Iteration 27500: Loss = -12347.357421875
2
Iteration 27600: Loss = -12347.35546875
Iteration 27700: Loss = -12347.3564453125
1
Iteration 27800: Loss = -12347.35546875
Iteration 27900: Loss = -12347.3564453125
1
Iteration 28000: Loss = -12347.35546875
Iteration 28100: Loss = -12347.357421875
1
Iteration 28200: Loss = -12347.3564453125
2
Iteration 28300: Loss = -12347.35546875
Iteration 28400: Loss = -12347.3564453125
1
Iteration 28500: Loss = -12347.3564453125
2
Iteration 28600: Loss = -12347.357421875
3
Iteration 28700: Loss = -12347.3564453125
4
Iteration 28800: Loss = -12347.35546875
Iteration 28900: Loss = -12347.3564453125
1
Iteration 29000: Loss = -12347.3564453125
2
Iteration 29100: Loss = -12347.357421875
3
Iteration 29200: Loss = -12347.3564453125
4
Iteration 29300: Loss = -12347.3544921875
Iteration 29400: Loss = -12347.3583984375
1
Iteration 29500: Loss = -12347.357421875
2
Iteration 29600: Loss = -12347.3564453125
3
Iteration 29700: Loss = -12347.3564453125
4
Iteration 29800: Loss = -12347.35546875
5
Iteration 29900: Loss = -12347.3564453125
6
pi: tensor([[2.2214e-01, 7.7786e-01],
        [1.1279e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9983e-01, 1.6513e-04], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1934, 0.1954],
         [0.5272, 0.2015]],

        [[0.9278, 0.1877],
         [0.0656, 0.6699]],

        [[0.0078, 0.1651],
         [0.4159, 0.8199]],

        [[0.0826, 0.3740],
         [0.9576, 0.2378]],

        [[0.9904, 0.2716],
         [0.4441, 0.9201]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0006439247262699942
Average Adjusted Rand Index: -0.0013099363851887004
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22231.853515625
Iteration 100: Loss = -16155.435546875
Iteration 200: Loss = -13203.8173828125
Iteration 300: Loss = -12622.3994140625
Iteration 400: Loss = -12512.8935546875
Iteration 500: Loss = -12464.3134765625
Iteration 600: Loss = -12435.0439453125
Iteration 700: Loss = -12416.5625
Iteration 800: Loss = -12399.8544921875
Iteration 900: Loss = -12388.5361328125
Iteration 1000: Loss = -12380.060546875
Iteration 1100: Loss = -12375.3193359375
Iteration 1200: Loss = -12369.7373046875
Iteration 1300: Loss = -12365.9501953125
Iteration 1400: Loss = -12363.58984375
Iteration 1500: Loss = -12361.595703125
Iteration 1600: Loss = -12359.83203125
Iteration 1700: Loss = -12358.1455078125
Iteration 1800: Loss = -12357.0068359375
Iteration 1900: Loss = -12356.2412109375
Iteration 2000: Loss = -12355.658203125
Iteration 2100: Loss = -12355.189453125
Iteration 2200: Loss = -12354.8037109375
Iteration 2300: Loss = -12354.4775390625
Iteration 2400: Loss = -12354.1982421875
Iteration 2500: Loss = -12353.9580078125
Iteration 2600: Loss = -12353.751953125
Iteration 2700: Loss = -12352.1298828125
Iteration 2800: Loss = -12351.251953125
Iteration 2900: Loss = -12351.0625
Iteration 3000: Loss = -12350.9072265625
Iteration 3100: Loss = -12350.7734375
Iteration 3200: Loss = -12350.6533203125
Iteration 3300: Loss = -12350.5419921875
Iteration 3400: Loss = -12350.4306640625
Iteration 3500: Loss = -12350.3125
Iteration 3600: Loss = -12350.24609375
Iteration 3700: Loss = -12350.1845703125
Iteration 3800: Loss = -12350.12890625
Iteration 3900: Loss = -12350.076171875
Iteration 4000: Loss = -12350.0302734375
Iteration 4100: Loss = -12349.984375
Iteration 4200: Loss = -12349.9423828125
Iteration 4300: Loss = -12349.9033203125
Iteration 4400: Loss = -12349.8681640625
Iteration 4500: Loss = -12349.8349609375
Iteration 4600: Loss = -12349.80078125
Iteration 4700: Loss = -12349.7705078125
Iteration 4800: Loss = -12349.740234375
Iteration 4900: Loss = -12349.7119140625
Iteration 5000: Loss = -12349.6875
Iteration 5100: Loss = -12349.6630859375
Iteration 5200: Loss = -12349.6416015625
Iteration 5300: Loss = -12349.62109375
Iteration 5400: Loss = -12349.6025390625
Iteration 5500: Loss = -12349.5859375
Iteration 5600: Loss = -12349.5693359375
Iteration 5700: Loss = -12349.5546875
Iteration 5800: Loss = -12349.541015625
Iteration 5900: Loss = -12349.5263671875
Iteration 6000: Loss = -12349.513671875
Iteration 6100: Loss = -12349.5029296875
Iteration 6200: Loss = -12349.490234375
Iteration 6300: Loss = -12349.4794921875
Iteration 6400: Loss = -12349.4697265625
Iteration 6500: Loss = -12349.4609375
Iteration 6600: Loss = -12349.451171875
Iteration 6700: Loss = -12349.4404296875
Iteration 6800: Loss = -12349.431640625
Iteration 6900: Loss = -12349.4189453125
Iteration 7000: Loss = -12349.408203125
Iteration 7100: Loss = -12349.4013671875
Iteration 7200: Loss = -12349.392578125
Iteration 7300: Loss = -12349.388671875
Iteration 7400: Loss = -12349.3828125
Iteration 7500: Loss = -12349.375
Iteration 7600: Loss = -12349.37109375
Iteration 7700: Loss = -12349.3662109375
Iteration 7800: Loss = -12349.3603515625
Iteration 7900: Loss = -12349.35546875
Iteration 8000: Loss = -12349.3515625
Iteration 8100: Loss = -12349.34765625
Iteration 8200: Loss = -12349.3447265625
Iteration 8300: Loss = -12349.3388671875
Iteration 8400: Loss = -12349.3369140625
Iteration 8500: Loss = -12349.3330078125
Iteration 8600: Loss = -12349.3310546875
Iteration 8700: Loss = -12349.3251953125
Iteration 8800: Loss = -12349.3232421875
Iteration 8900: Loss = -12349.3232421875
Iteration 9000: Loss = -12349.318359375
Iteration 9100: Loss = -12349.314453125
Iteration 9200: Loss = -12349.3134765625
Iteration 9300: Loss = -12349.30859375
Iteration 9400: Loss = -12349.306640625
Iteration 9500: Loss = -12349.3037109375
Iteration 9600: Loss = -12349.3017578125
Iteration 9700: Loss = -12349.298828125
Iteration 9800: Loss = -12349.298828125
Iteration 9900: Loss = -12349.2958984375
Iteration 10000: Loss = -12349.2919921875
Iteration 10100: Loss = -12349.2900390625
Iteration 10200: Loss = -12349.2880859375
Iteration 10300: Loss = -12349.2880859375
Iteration 10400: Loss = -12349.28515625
Iteration 10500: Loss = -12349.283203125
Iteration 10600: Loss = -12349.2802734375
Iteration 10700: Loss = -12349.2783203125
Iteration 10800: Loss = -12349.2783203125
Iteration 10900: Loss = -12349.2763671875
Iteration 11000: Loss = -12349.2724609375
Iteration 11100: Loss = -12349.267578125
Iteration 11200: Loss = -12349.265625
Iteration 11300: Loss = -12349.259765625
Iteration 11400: Loss = -12349.2470703125
Iteration 11500: Loss = -12349.2158203125
Iteration 11600: Loss = -12349.150390625
Iteration 11700: Loss = -12349.0732421875
Iteration 11800: Loss = -12349.0517578125
Iteration 11900: Loss = -12349.025390625
Iteration 12000: Loss = -12348.9892578125
Iteration 12100: Loss = -12348.9755859375
Iteration 12200: Loss = -12348.9677734375
Iteration 12300: Loss = -12348.935546875
Iteration 12400: Loss = -12348.900390625
Iteration 12500: Loss = -12348.837890625
Iteration 12600: Loss = -12348.8056640625
Iteration 12700: Loss = -12348.7734375
Iteration 12800: Loss = -12348.748046875
Iteration 12900: Loss = -12348.720703125
Iteration 13000: Loss = -12348.712890625
Iteration 13100: Loss = -12348.7080078125
Iteration 13200: Loss = -12348.6982421875
Iteration 13300: Loss = -12348.677734375
Iteration 13400: Loss = -12348.66796875
Iteration 13500: Loss = -12348.662109375
Iteration 13600: Loss = -12348.6474609375
Iteration 13700: Loss = -12348.6337890625
Iteration 13800: Loss = -12348.6220703125
Iteration 13900: Loss = -12348.611328125
Iteration 14000: Loss = -12348.595703125
Iteration 14100: Loss = -12348.591796875
Iteration 14200: Loss = -12348.5771484375
Iteration 14300: Loss = -12348.5673828125
Iteration 14400: Loss = -12348.5654296875
Iteration 14500: Loss = -12348.5615234375
Iteration 14600: Loss = -12348.560546875
Iteration 14700: Loss = -12348.5576171875
Iteration 14800: Loss = -12348.5546875
Iteration 14900: Loss = -12348.5478515625
Iteration 15000: Loss = -12348.5478515625
Iteration 15100: Loss = -12348.546875
Iteration 15200: Loss = -12348.546875
Iteration 15300: Loss = -12348.5458984375
Iteration 15400: Loss = -12348.5439453125
Iteration 15500: Loss = -12348.5390625
Iteration 15600: Loss = -12348.537109375
Iteration 15700: Loss = -12348.53515625
Iteration 15800: Loss = -12348.5322265625
Iteration 15900: Loss = -12348.53125
Iteration 16000: Loss = -12348.5322265625
1
Iteration 16100: Loss = -12348.5283203125
Iteration 16200: Loss = -12348.52734375
Iteration 16300: Loss = -12348.5146484375
Iteration 16400: Loss = -12348.5126953125
Iteration 16500: Loss = -12348.51171875
Iteration 16600: Loss = -12348.51171875
Iteration 16700: Loss = -12348.509765625
Iteration 16800: Loss = -12348.5126953125
1
Iteration 16900: Loss = -12348.5107421875
2
Iteration 17000: Loss = -12348.5126953125
3
Iteration 17100: Loss = -12348.51171875
4
Iteration 17200: Loss = -12348.51171875
5
Iteration 17300: Loss = -12348.5126953125
6
Iteration 17400: Loss = -12348.5107421875
7
Iteration 17500: Loss = -12348.5107421875
8
Iteration 17600: Loss = -12348.5107421875
9
Iteration 17700: Loss = -12348.5107421875
10
Iteration 17800: Loss = -12348.51171875
11
Iteration 17900: Loss = -12348.5107421875
12
Iteration 18000: Loss = -12348.5107421875
13
Iteration 18100: Loss = -12348.509765625
Iteration 18200: Loss = -12348.5068359375
Iteration 18300: Loss = -12348.505859375
Iteration 18400: Loss = -12348.509765625
1
Iteration 18500: Loss = -12348.505859375
Iteration 18600: Loss = -12348.5068359375
1
Iteration 18700: Loss = -12348.50390625
Iteration 18800: Loss = -12348.505859375
1
Iteration 18900: Loss = -12348.50390625
Iteration 19000: Loss = -12348.5029296875
Iteration 19100: Loss = -12348.5009765625
Iteration 19200: Loss = -12348.5009765625
Iteration 19300: Loss = -12348.5
Iteration 19400: Loss = -12348.501953125
1
Iteration 19500: Loss = -12348.501953125
2
Iteration 19600: Loss = -12348.5009765625
3
Iteration 19700: Loss = -12348.5
Iteration 19800: Loss = -12348.5009765625
1
Iteration 19900: Loss = -12348.5009765625
2
Iteration 20000: Loss = -12348.501953125
3
Iteration 20100: Loss = -12348.5009765625
4
Iteration 20200: Loss = -12348.5009765625
5
Iteration 20300: Loss = -12348.5009765625
6
Iteration 20400: Loss = -12348.5009765625
7
Iteration 20500: Loss = -12348.5009765625
8
Iteration 20600: Loss = -12348.5029296875
9
Iteration 20700: Loss = -12348.5009765625
10
Iteration 20800: Loss = -12348.5009765625
11
Iteration 20900: Loss = -12348.501953125
12
Iteration 21000: Loss = -12348.501953125
13
Iteration 21100: Loss = -12348.5009765625
14
Iteration 21200: Loss = -12348.501953125
15
Stopping early at iteration 21200 due to no improvement.
pi: tensor([[9.8026e-05, 9.9990e-01],
        [1.1516e-02, 9.8848e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0190, 0.9810], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1841, 0.1815],
         [0.9674, 0.1985]],

        [[0.6726, 0.1577],
         [0.9831, 0.8652]],

        [[0.8400, 0.2182],
         [0.9790, 0.6904]],

        [[0.9717, 0.3861],
         [0.0306, 0.0579]],

        [[0.8302, 0.2024],
         [0.9050, 0.0335]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00039543502922895045
Average Adjusted Rand Index: -0.0008569898232458489
[-0.0006439247262699942, -0.00039543502922895045] [-0.0013099363851887004, -0.0008569898232458489] [12347.3564453125, 12348.501953125]
-------------------------------------
This iteration is 24
True Objective function: Loss = -11827.733561380155
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -32533.890625
Iteration 100: Loss = -22441.97265625
Iteration 200: Loss = -15187.3388671875
Iteration 300: Loss = -13251.181640625
Iteration 400: Loss = -12829.5654296875
Iteration 500: Loss = -12706.7412109375
Iteration 600: Loss = -12658.9619140625
Iteration 700: Loss = -12625.416015625
Iteration 800: Loss = -12595.296875
Iteration 900: Loss = -12577.09375
Iteration 1000: Loss = -12565.810546875
Iteration 1100: Loss = -12556.6044921875
Iteration 1200: Loss = -12548.94140625
Iteration 1300: Loss = -12542.662109375
Iteration 1400: Loss = -12537.7822265625
Iteration 1500: Loss = -12532.4248046875
Iteration 1600: Loss = -12526.0009765625
Iteration 1700: Loss = -12520.1728515625
Iteration 1800: Loss = -12514.5751953125
Iteration 1900: Loss = -12508.5068359375
Iteration 2000: Loss = -12503.095703125
Iteration 2100: Loss = -12498.728515625
Iteration 2200: Loss = -12496.4365234375
Iteration 2300: Loss = -12493.9609375
Iteration 2400: Loss = -12491.4169921875
Iteration 2500: Loss = -12489.109375
Iteration 2600: Loss = -12487.6171875
Iteration 2700: Loss = -12485.921875
Iteration 2800: Loss = -12484.4150390625
Iteration 2900: Loss = -12482.98828125
Iteration 3000: Loss = -12481.677734375
Iteration 3100: Loss = -12479.8125
Iteration 3200: Loss = -12478.8193359375
Iteration 3300: Loss = -12477.373046875
Iteration 3400: Loss = -12476.732421875
Iteration 3500: Loss = -12475.6298828125
Iteration 3600: Loss = -12475.2021484375
Iteration 3700: Loss = -12474.87109375
Iteration 3800: Loss = -12474.587890625
Iteration 3900: Loss = -12474.3408203125
Iteration 4000: Loss = -12474.1181640625
Iteration 4100: Loss = -12473.9140625
Iteration 4200: Loss = -12473.7236328125
Iteration 4300: Loss = -12473.5322265625
Iteration 4400: Loss = -12473.2783203125
Iteration 4500: Loss = -12472.5615234375
Iteration 4600: Loss = -12472.2490234375
Iteration 4700: Loss = -12471.9013671875
Iteration 4800: Loss = -12470.560546875
Iteration 4900: Loss = -12469.5576171875
Iteration 5000: Loss = -12468.8603515625
Iteration 5100: Loss = -12467.7763671875
Iteration 5200: Loss = -12463.7646484375
Iteration 5300: Loss = -12437.3662109375
Iteration 5400: Loss = -12415.783203125
Iteration 5500: Loss = -12390.705078125
Iteration 5600: Loss = -12369.837890625
Iteration 5700: Loss = -12360.5595703125
Iteration 5800: Loss = -12358.830078125
Iteration 5900: Loss = -12357.9560546875
Iteration 6000: Loss = -12357.423828125
Iteration 6100: Loss = -12356.935546875
Iteration 6200: Loss = -12349.6689453125
Iteration 6300: Loss = -12347.716796875
Iteration 6400: Loss = -12347.1982421875
Iteration 6500: Loss = -12346.8564453125
Iteration 6600: Loss = -12346.6025390625
Iteration 6700: Loss = -12346.4013671875
Iteration 6800: Loss = -12346.2392578125
Iteration 6900: Loss = -12346.1025390625
Iteration 7000: Loss = -12345.98828125
Iteration 7100: Loss = -12345.888671875
Iteration 7200: Loss = -12345.8037109375
Iteration 7300: Loss = -12345.728515625
Iteration 7400: Loss = -12345.6630859375
Iteration 7500: Loss = -12345.603515625
Iteration 7600: Loss = -12345.55078125
Iteration 7700: Loss = -12345.5048828125
Iteration 7800: Loss = -12345.4599609375
Iteration 7900: Loss = -12345.421875
Iteration 8000: Loss = -12345.384765625
Iteration 8100: Loss = -12345.353515625
Iteration 8200: Loss = -12345.32421875
Iteration 8300: Loss = -12345.296875
Iteration 8400: Loss = -12345.271484375
Iteration 8500: Loss = -12345.248046875
Iteration 8600: Loss = -12345.2275390625
Iteration 8700: Loss = -12345.20703125
Iteration 8800: Loss = -12345.1875
Iteration 8900: Loss = -12345.1708984375
Iteration 9000: Loss = -12345.154296875
Iteration 9100: Loss = -12345.140625
Iteration 9200: Loss = -12345.126953125
Iteration 9300: Loss = -12345.115234375
Iteration 9400: Loss = -12345.1005859375
Iteration 9500: Loss = -12345.08984375
Iteration 9600: Loss = -12345.0791015625
Iteration 9700: Loss = -12345.0693359375
Iteration 9800: Loss = -12345.0595703125
Iteration 9900: Loss = -12345.0517578125
Iteration 10000: Loss = -12345.0419921875
Iteration 10100: Loss = -12345.03515625
Iteration 10200: Loss = -12345.0283203125
Iteration 10300: Loss = -12345.0224609375
Iteration 10400: Loss = -12345.0146484375
Iteration 10500: Loss = -12345.0087890625
Iteration 10600: Loss = -12345.0029296875
Iteration 10700: Loss = -12344.998046875
Iteration 10800: Loss = -12344.9912109375
Iteration 10900: Loss = -12344.9873046875
Iteration 11000: Loss = -12344.9833984375
Iteration 11100: Loss = -12344.978515625
Iteration 11200: Loss = -12344.9755859375
Iteration 11300: Loss = -12344.970703125
Iteration 11400: Loss = -12344.966796875
Iteration 11500: Loss = -12344.962890625
Iteration 11600: Loss = -12344.9599609375
Iteration 11700: Loss = -12344.95703125
Iteration 11800: Loss = -12344.953125
Iteration 11900: Loss = -12344.9521484375
Iteration 12000: Loss = -12344.9482421875
Iteration 12100: Loss = -12344.9462890625
Iteration 12200: Loss = -12344.943359375
Iteration 12300: Loss = -12344.9423828125
Iteration 12400: Loss = -12344.939453125
Iteration 12500: Loss = -12344.9375
Iteration 12600: Loss = -12344.9375
Iteration 12700: Loss = -12344.9345703125
Iteration 12800: Loss = -12344.9326171875
Iteration 12900: Loss = -12344.9296875
Iteration 13000: Loss = -12344.9296875
Iteration 13100: Loss = -12344.927734375
Iteration 13200: Loss = -12344.9267578125
Iteration 13300: Loss = -12344.92578125
Iteration 13400: Loss = -12344.9228515625
Iteration 13500: Loss = -12344.9228515625
Iteration 13600: Loss = -12344.921875
Iteration 13700: Loss = -12344.9208984375
Iteration 13800: Loss = -12344.91796875
Iteration 13900: Loss = -12344.9189453125
1
Iteration 14000: Loss = -12344.9169921875
Iteration 14100: Loss = -12344.91796875
1
Iteration 14200: Loss = -12344.9150390625
Iteration 14300: Loss = -12344.9150390625
Iteration 14400: Loss = -12344.9150390625
Iteration 14500: Loss = -12344.9140625
Iteration 14600: Loss = -12344.9140625
Iteration 14700: Loss = -12344.9111328125
Iteration 14800: Loss = -12344.9111328125
Iteration 14900: Loss = -12344.9130859375
1
Iteration 15000: Loss = -12344.91015625
Iteration 15100: Loss = -12344.91015625
Iteration 15200: Loss = -12344.91015625
Iteration 15300: Loss = -12344.91015625
Iteration 15400: Loss = -12344.9091796875
Iteration 15500: Loss = -12344.9091796875
Iteration 15600: Loss = -12344.9072265625
Iteration 15700: Loss = -12344.908203125
1
Iteration 15800: Loss = -12344.908203125
2
Iteration 15900: Loss = -12344.908203125
3
Iteration 16000: Loss = -12344.9072265625
Iteration 16100: Loss = -12344.9052734375
Iteration 16200: Loss = -12344.9072265625
1
Iteration 16300: Loss = -12344.9052734375
Iteration 16400: Loss = -12344.9072265625
1
Iteration 16500: Loss = -12344.9052734375
Iteration 16600: Loss = -12344.904296875
Iteration 16700: Loss = -12344.904296875
Iteration 16800: Loss = -12344.904296875
Iteration 16900: Loss = -12344.90625
1
Iteration 17000: Loss = -12344.904296875
Iteration 17100: Loss = -12344.904296875
Iteration 17200: Loss = -12344.904296875
Iteration 17300: Loss = -12344.9033203125
Iteration 17400: Loss = -12344.9033203125
Iteration 17500: Loss = -12344.9033203125
Iteration 17600: Loss = -12344.9033203125
Iteration 17700: Loss = -12344.9033203125
Iteration 17800: Loss = -12344.9052734375
1
Iteration 17900: Loss = -12344.9033203125
Iteration 18000: Loss = -12344.9033203125
Iteration 18100: Loss = -12344.904296875
1
Iteration 18200: Loss = -12344.9033203125
Iteration 18300: Loss = -12344.90234375
Iteration 18400: Loss = -12344.90234375
Iteration 18500: Loss = -12344.900390625
Iteration 18600: Loss = -12344.9013671875
1
Iteration 18700: Loss = -12344.90234375
2
Iteration 18800: Loss = -12344.9013671875
3
Iteration 18900: Loss = -12344.90234375
4
Iteration 19000: Loss = -12344.90234375
5
Iteration 19100: Loss = -12344.9033203125
6
Iteration 19200: Loss = -12344.904296875
7
Iteration 19300: Loss = -12344.90234375
8
Iteration 19400: Loss = -12344.9013671875
9
Iteration 19500: Loss = -12344.9013671875
10
Iteration 19600: Loss = -12344.9013671875
11
Iteration 19700: Loss = -12344.90234375
12
Iteration 19800: Loss = -12344.900390625
Iteration 19900: Loss = -12344.9013671875
1
Iteration 20000: Loss = -12344.9013671875
2
Iteration 20100: Loss = -12344.900390625
Iteration 20200: Loss = -12344.90234375
1
Iteration 20300: Loss = -12344.9033203125
2
Iteration 20400: Loss = -12344.90234375
3
Iteration 20500: Loss = -12344.90234375
4
Iteration 20600: Loss = -12344.90234375
5
Iteration 20700: Loss = -12344.90234375
6
Iteration 20800: Loss = -12344.9013671875
7
Iteration 20900: Loss = -12344.9013671875
8
Iteration 21000: Loss = -12344.90234375
9
Iteration 21100: Loss = -12344.90234375
10
Iteration 21200: Loss = -12344.9013671875
11
Iteration 21300: Loss = -12344.900390625
Iteration 21400: Loss = -12344.9013671875
1
Iteration 21500: Loss = -12344.90234375
2
Iteration 21600: Loss = -12344.90234375
3
Iteration 21700: Loss = -12344.90234375
4
Iteration 21800: Loss = -12344.9013671875
5
Iteration 21900: Loss = -12344.900390625
Iteration 22000: Loss = -12344.90234375
1
Iteration 22100: Loss = -12344.9013671875
2
Iteration 22200: Loss = -12344.9013671875
3
Iteration 22300: Loss = -12344.90234375
4
Iteration 22400: Loss = -12344.9013671875
5
Iteration 22500: Loss = -12344.900390625
Iteration 22600: Loss = -12344.90234375
1
Iteration 22700: Loss = -12344.90234375
2
Iteration 22800: Loss = -12344.90234375
3
Iteration 22900: Loss = -12344.90234375
4
Iteration 23000: Loss = -12344.9013671875
5
Iteration 23100: Loss = -12344.9013671875
6
Iteration 23200: Loss = -12344.9013671875
7
Iteration 23300: Loss = -12344.9013671875
8
Iteration 23400: Loss = -12344.900390625
Iteration 23500: Loss = -12344.90234375
1
Iteration 23600: Loss = -12344.900390625
Iteration 23700: Loss = -12344.90234375
1
Iteration 23800: Loss = -12344.900390625
Iteration 23900: Loss = -12344.90234375
1
Iteration 24000: Loss = -12344.9013671875
2
Iteration 24100: Loss = -12344.90234375
3
Iteration 24200: Loss = -12344.90234375
4
Iteration 24300: Loss = -12344.90234375
5
Iteration 24400: Loss = -12344.9013671875
6
Iteration 24500: Loss = -12344.90234375
7
Iteration 24600: Loss = -12344.9013671875
8
Iteration 24700: Loss = -12344.900390625
Iteration 24800: Loss = -12344.8994140625
Iteration 24900: Loss = -12344.90234375
1
Iteration 25000: Loss = -12344.8994140625
Iteration 25100: Loss = -12344.8994140625
Iteration 25200: Loss = -12344.9013671875
1
Iteration 25300: Loss = -12344.8994140625
Iteration 25400: Loss = -12344.9013671875
1
Iteration 25500: Loss = -12344.900390625
2
Iteration 25600: Loss = -12344.9013671875
3
Iteration 25700: Loss = -12344.8994140625
Iteration 25800: Loss = -12344.8994140625
Iteration 25900: Loss = -12344.900390625
1
Iteration 26000: Loss = -12344.9013671875
2
Iteration 26100: Loss = -12344.900390625
3
Iteration 26200: Loss = -12344.9033203125
4
Iteration 26300: Loss = -12344.9013671875
5
Iteration 26400: Loss = -12344.8994140625
Iteration 26500: Loss = -12344.900390625
1
Iteration 26600: Loss = -12344.900390625
2
Iteration 26700: Loss = -12344.9013671875
3
Iteration 26800: Loss = -12344.900390625
4
Iteration 26900: Loss = -12344.9013671875
5
Iteration 27000: Loss = -12344.8994140625
Iteration 27100: Loss = -12344.900390625
1
Iteration 27200: Loss = -12344.8994140625
Iteration 27300: Loss = -12344.9013671875
1
Iteration 27400: Loss = -12344.8994140625
Iteration 27500: Loss = -12344.900390625
1
Iteration 27600: Loss = -12344.900390625
2
Iteration 27700: Loss = -12344.8994140625
Iteration 27800: Loss = -12344.8994140625
Iteration 27900: Loss = -12344.9013671875
1
Iteration 28000: Loss = -12344.9013671875
2
Iteration 28100: Loss = -12344.900390625
3
Iteration 28200: Loss = -12344.900390625
4
Iteration 28300: Loss = -12344.9013671875
5
Iteration 28400: Loss = -12344.900390625
6
Iteration 28500: Loss = -12344.8994140625
Iteration 28600: Loss = -12344.8994140625
Iteration 28700: Loss = -12344.9013671875
1
Iteration 28800: Loss = -12344.900390625
2
Iteration 28900: Loss = -12344.900390625
3
Iteration 29000: Loss = -12344.900390625
4
Iteration 29100: Loss = -12344.9013671875
5
Iteration 29200: Loss = -12344.9013671875
6
Iteration 29300: Loss = -12344.9013671875
7
Iteration 29400: Loss = -12344.8994140625
Iteration 29500: Loss = -12344.9013671875
1
Iteration 29600: Loss = -12344.9013671875
2
Iteration 29700: Loss = -12344.9013671875
3
Iteration 29800: Loss = -12344.900390625
4
Iteration 29900: Loss = -12344.9013671875
5
pi: tensor([[1.6534e-06, 1.0000e+00],
        [1.0000e+00, 8.0485e-07]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 2.9810e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1931, 0.1877],
         [0.4870, 0.2077]],

        [[0.9558, 0.2186],
         [0.9137, 0.1237]],

        [[0.3445, 0.6052],
         [0.7660, 0.7234]],

        [[0.9745, 0.2005],
         [0.0976, 0.9446]],

        [[0.9909, 0.2018],
         [0.9700, 0.4233]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0017322075719022284
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27344.90234375
Iteration 100: Loss = -18189.357421875
Iteration 200: Loss = -13459.5712890625
Iteration 300: Loss = -12729.751953125
Iteration 400: Loss = -12587.87109375
Iteration 500: Loss = -12538.109375
Iteration 600: Loss = -12509.701171875
Iteration 700: Loss = -12492.4580078125
Iteration 800: Loss = -12478.7158203125
Iteration 900: Loss = -12465.640625
Iteration 1000: Loss = -12454.5009765625
Iteration 1100: Loss = -12442.81640625
Iteration 1200: Loss = -12434.693359375
Iteration 1300: Loss = -12428.267578125
Iteration 1400: Loss = -12419.8115234375
Iteration 1500: Loss = -12415.2880859375
Iteration 1600: Loss = -12411.8525390625
Iteration 1700: Loss = -12408.4775390625
Iteration 1800: Loss = -12405.361328125
Iteration 1900: Loss = -12401.7255859375
Iteration 2000: Loss = -12396.9345703125
Iteration 2100: Loss = -12392.5986328125
Iteration 2200: Loss = -12389.236328125
Iteration 2300: Loss = -12386.564453125
Iteration 2400: Loss = -12384.6591796875
Iteration 2500: Loss = -12383.1708984375
Iteration 2600: Loss = -12381.7294921875
Iteration 2700: Loss = -12380.4052734375
Iteration 2800: Loss = -12378.67578125
Iteration 2900: Loss = -12375.6669921875
Iteration 3000: Loss = -12373.35546875
Iteration 3100: Loss = -12371.412109375
Iteration 3200: Loss = -12367.3525390625
Iteration 3300: Loss = -12365.4560546875
Iteration 3400: Loss = -12364.109375
Iteration 3500: Loss = -12362.58203125
Iteration 3600: Loss = -12361.064453125
Iteration 3700: Loss = -12359.525390625
Iteration 3800: Loss = -12358.9140625
Iteration 3900: Loss = -12358.505859375
Iteration 4000: Loss = -12358.1865234375
Iteration 4100: Loss = -12357.916015625
Iteration 4200: Loss = -12357.673828125
Iteration 4300: Loss = -12357.427734375
Iteration 4400: Loss = -12357.07421875
Iteration 4500: Loss = -12355.9873046875
Iteration 4600: Loss = -12355.20703125
Iteration 4700: Loss = -12354.91796875
Iteration 4800: Loss = -12354.7294921875
Iteration 4900: Loss = -12354.5869140625
Iteration 5000: Loss = -12354.4658203125
Iteration 5100: Loss = -12354.3564453125
Iteration 5200: Loss = -12354.24609375
Iteration 5300: Loss = -12354.0703125
Iteration 5400: Loss = -12352.0634765625
Iteration 5500: Loss = -12351.763671875
Iteration 5600: Loss = -12351.66796875
Iteration 5700: Loss = -12351.5986328125
Iteration 5800: Loss = -12351.537109375
Iteration 5900: Loss = -12351.482421875
Iteration 6000: Loss = -12351.4267578125
Iteration 6100: Loss = -12350.388671875
Iteration 6200: Loss = -12347.9296875
Iteration 6300: Loss = -12347.6806640625
Iteration 6400: Loss = -12347.55078125
Iteration 6500: Loss = -12347.46484375
Iteration 6600: Loss = -12347.3984375
Iteration 6700: Loss = -12347.3408203125
Iteration 6800: Loss = -12347.291015625
Iteration 6900: Loss = -12347.244140625
Iteration 7000: Loss = -12347.205078125
Iteration 7100: Loss = -12347.1708984375
Iteration 7200: Loss = -12347.140625
Iteration 7300: Loss = -12347.1142578125
Iteration 7400: Loss = -12347.0908203125
Iteration 7500: Loss = -12347.0703125
Iteration 7600: Loss = -12347.0498046875
Iteration 7700: Loss = -12347.0322265625
Iteration 7800: Loss = -12347.017578125
Iteration 7900: Loss = -12347.0029296875
Iteration 8000: Loss = -12346.98828125
Iteration 8100: Loss = -12346.9755859375
Iteration 8200: Loss = -12346.96484375
Iteration 8300: Loss = -12346.9541015625
Iteration 8400: Loss = -12346.9423828125
Iteration 8500: Loss = -12346.9326171875
Iteration 8600: Loss = -12346.923828125
Iteration 8700: Loss = -12346.9150390625
Iteration 8800: Loss = -12346.908203125
Iteration 8900: Loss = -12346.900390625
Iteration 9000: Loss = -12346.8935546875
Iteration 9100: Loss = -12346.8857421875
Iteration 9200: Loss = -12346.87890625
Iteration 9300: Loss = -12346.875
Iteration 9400: Loss = -12346.8681640625
Iteration 9500: Loss = -12346.861328125
Iteration 9600: Loss = -12346.857421875
Iteration 9700: Loss = -12346.8515625
Iteration 9800: Loss = -12346.84765625
Iteration 9900: Loss = -12346.84375
Iteration 10000: Loss = -12346.8408203125
Iteration 10100: Loss = -12346.8349609375
Iteration 10200: Loss = -12346.8310546875
Iteration 10300: Loss = -12346.8291015625
Iteration 10400: Loss = -12346.8251953125
Iteration 10500: Loss = -12346.822265625
Iteration 10600: Loss = -12346.8193359375
Iteration 10700: Loss = -12346.8154296875
Iteration 10800: Loss = -12346.8134765625
Iteration 10900: Loss = -12346.810546875
Iteration 11000: Loss = -12346.8076171875
Iteration 11100: Loss = -12346.8046875
Iteration 11200: Loss = -12346.802734375
Iteration 11300: Loss = -12346.8017578125
Iteration 11400: Loss = -12346.7998046875
Iteration 11500: Loss = -12346.796875
Iteration 11600: Loss = -12346.794921875
Iteration 11700: Loss = -12346.7939453125
Iteration 11800: Loss = -12346.7919921875
Iteration 11900: Loss = -12346.7890625
Iteration 12000: Loss = -12346.7880859375
Iteration 12100: Loss = -12346.7861328125
Iteration 12200: Loss = -12346.7841796875
Iteration 12300: Loss = -12346.7783203125
Iteration 12400: Loss = -12346.7763671875
Iteration 12500: Loss = -12346.7734375
Iteration 12600: Loss = -12346.771484375
Iteration 12700: Loss = -12346.76953125
Iteration 12800: Loss = -12346.767578125
Iteration 12900: Loss = -12346.767578125
Iteration 13000: Loss = -12346.765625
Iteration 13100: Loss = -12346.7646484375
Iteration 13200: Loss = -12346.765625
1
Iteration 13300: Loss = -12346.76171875
Iteration 13400: Loss = -12346.763671875
1
Iteration 13500: Loss = -12346.76171875
Iteration 13600: Loss = -12346.759765625
Iteration 13700: Loss = -12346.7587890625
Iteration 13800: Loss = -12346.7578125
Iteration 13900: Loss = -12346.7587890625
1
Iteration 14000: Loss = -12346.7568359375
Iteration 14100: Loss = -12346.7568359375
Iteration 14200: Loss = -12346.755859375
Iteration 14300: Loss = -12346.7548828125
Iteration 14400: Loss = -12346.7548828125
Iteration 14500: Loss = -12346.75390625
Iteration 14600: Loss = -12346.75390625
Iteration 14700: Loss = -12346.75390625
Iteration 14800: Loss = -12346.751953125
Iteration 14900: Loss = -12346.751953125
Iteration 15000: Loss = -12346.751953125
Iteration 15100: Loss = -12346.751953125
Iteration 15200: Loss = -12346.7529296875
1
Iteration 15300: Loss = -12346.7509765625
Iteration 15400: Loss = -12346.75
Iteration 15500: Loss = -12346.75
Iteration 15600: Loss = -12346.751953125
1
Iteration 15700: Loss = -12346.7490234375
Iteration 15800: Loss = -12346.75
1
Iteration 15900: Loss = -12346.75
2
Iteration 16000: Loss = -12346.7490234375
Iteration 16100: Loss = -12346.75
1
Iteration 16200: Loss = -12346.7509765625
2
Iteration 16300: Loss = -12346.7490234375
Iteration 16400: Loss = -12346.7470703125
Iteration 16500: Loss = -12346.748046875
1
Iteration 16600: Loss = -12346.748046875
2
Iteration 16700: Loss = -12346.748046875
3
Iteration 16800: Loss = -12346.7490234375
4
Iteration 16900: Loss = -12346.748046875
5
Iteration 17000: Loss = -12346.7470703125
Iteration 17100: Loss = -12346.7470703125
Iteration 17200: Loss = -12346.748046875
1
Iteration 17300: Loss = -12346.7470703125
Iteration 17400: Loss = -12346.7470703125
Iteration 17500: Loss = -12346.7470703125
Iteration 17600: Loss = -12346.7451171875
Iteration 17700: Loss = -12346.74609375
1
Iteration 17800: Loss = -12346.7470703125
2
Iteration 17900: Loss = -12346.74609375
3
Iteration 18000: Loss = -12346.74609375
4
Iteration 18100: Loss = -12346.7470703125
5
Iteration 18200: Loss = -12346.74609375
6
Iteration 18300: Loss = -12346.74609375
7
Iteration 18400: Loss = -12346.7470703125
8
Iteration 18500: Loss = -12346.74609375
9
Iteration 18600: Loss = -12346.7470703125
10
Iteration 18700: Loss = -12346.748046875
11
Iteration 18800: Loss = -12346.74609375
12
Iteration 18900: Loss = -12346.74609375
13
Iteration 19000: Loss = -12346.74609375
14
Iteration 19100: Loss = -12346.74609375
15
Stopping early at iteration 19100 due to no improvement.
pi: tensor([[4.5239e-02, 9.5476e-01],
        [3.9191e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9997e-01, 2.7188e-05], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1925, 0.1785],
         [0.8354, 0.1990]],

        [[0.9459, 0.2705],
         [0.2786, 0.9876]],

        [[0.2192, 0.7787],
         [0.0095, 0.9334]],

        [[0.2033, 0.2028],
         [0.8966, 0.9634]],

        [[0.5617, 0.1812],
         [0.1643, 0.9694]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0017176225429760765
Average Adjusted Rand Index: -0.000982071485668608
[-0.0017322075719022284, -0.0017176225429760765] [0.0, -0.000982071485668608] [12344.9013671875, 12346.74609375]
-------------------------------------
This iteration is 25
True Objective function: Loss = -11962.98414798985
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19697.771484375
Iteration 100: Loss = -14749.947265625
Iteration 200: Loss = -12951.93359375
Iteration 300: Loss = -12630.75390625
Iteration 400: Loss = -12571.7138671875
Iteration 500: Loss = -12547.3193359375
Iteration 600: Loss = -12531.9697265625
Iteration 700: Loss = -12523.669921875
Iteration 800: Loss = -12519.279296875
Iteration 900: Loss = -12516.609375
Iteration 1000: Loss = -12514.80078125
Iteration 1100: Loss = -12513.48046875
Iteration 1200: Loss = -12512.470703125
Iteration 1300: Loss = -12511.68359375
Iteration 1400: Loss = -12511.056640625
Iteration 1500: Loss = -12510.544921875
Iteration 1600: Loss = -12510.1279296875
Iteration 1700: Loss = -12509.7822265625
Iteration 1800: Loss = -12509.4951171875
Iteration 1900: Loss = -12509.2568359375
Iteration 2000: Loss = -12509.0546875
Iteration 2100: Loss = -12508.8876953125
Iteration 2200: Loss = -12508.7431640625
Iteration 2300: Loss = -12508.625
Iteration 2400: Loss = -12508.5234375
Iteration 2500: Loss = -12508.439453125
Iteration 2600: Loss = -12508.37109375
Iteration 2700: Loss = -12508.3154296875
Iteration 2800: Loss = -12508.26953125
Iteration 2900: Loss = -12508.2333984375
Iteration 3000: Loss = -12508.201171875
Iteration 3100: Loss = -12508.17578125
Iteration 3200: Loss = -12508.1533203125
Iteration 3300: Loss = -12508.1337890625
Iteration 3400: Loss = -12508.1142578125
Iteration 3500: Loss = -12508.09765625
Iteration 3600: Loss = -12508.080078125
Iteration 3700: Loss = -12508.0625
Iteration 3800: Loss = -12508.0439453125
Iteration 3900: Loss = -12508.0234375
Iteration 4000: Loss = -12508.0
Iteration 4100: Loss = -12507.978515625
Iteration 4200: Loss = -12507.95703125
Iteration 4300: Loss = -12507.9375
Iteration 4400: Loss = -12507.919921875
Iteration 4500: Loss = -12507.90234375
Iteration 4600: Loss = -12507.8828125
Iteration 4700: Loss = -12507.86328125
Iteration 4800: Loss = -12507.8447265625
Iteration 4900: Loss = -12507.82421875
Iteration 5000: Loss = -12507.8037109375
Iteration 5100: Loss = -12507.783203125
Iteration 5200: Loss = -12507.7646484375
Iteration 5300: Loss = -12507.74609375
Iteration 5400: Loss = -12507.7265625
Iteration 5500: Loss = -12507.708984375
Iteration 5600: Loss = -12507.69140625
Iteration 5700: Loss = -12507.6767578125
Iteration 5800: Loss = -12507.66015625
Iteration 5900: Loss = -12507.64453125
Iteration 6000: Loss = -12507.6318359375
Iteration 6100: Loss = -12507.6201171875
Iteration 6200: Loss = -12507.607421875
Iteration 6300: Loss = -12507.595703125
Iteration 6400: Loss = -12507.5849609375
Iteration 6500: Loss = -12507.5771484375
Iteration 6600: Loss = -12507.5673828125
Iteration 6700: Loss = -12507.5595703125
Iteration 6800: Loss = -12507.5546875
Iteration 6900: Loss = -12507.55078125
Iteration 7000: Loss = -12507.5458984375
Iteration 7100: Loss = -12507.541015625
Iteration 7200: Loss = -12507.537109375
Iteration 7300: Loss = -12507.5361328125
Iteration 7400: Loss = -12507.5322265625
Iteration 7500: Loss = -12507.529296875
Iteration 7600: Loss = -12507.5263671875
Iteration 7700: Loss = -12507.5244140625
Iteration 7800: Loss = -12507.5205078125
Iteration 7900: Loss = -12507.5185546875
Iteration 8000: Loss = -12507.513671875
Iteration 8100: Loss = -12507.5126953125
Iteration 8200: Loss = -12507.5068359375
Iteration 8300: Loss = -12507.5029296875
Iteration 8400: Loss = -12507.5
Iteration 8500: Loss = -12507.494140625
Iteration 8600: Loss = -12507.4892578125
Iteration 8700: Loss = -12507.4833984375
Iteration 8800: Loss = -12507.4765625
Iteration 8900: Loss = -12507.46875
Iteration 9000: Loss = -12507.4619140625
Iteration 9100: Loss = -12507.453125
Iteration 9200: Loss = -12507.4423828125
Iteration 9300: Loss = -12507.423828125
Iteration 9400: Loss = -12507.396484375
Iteration 9500: Loss = -12507.3447265625
Iteration 9600: Loss = -12507.19140625
Iteration 9700: Loss = -12506.4140625
Iteration 9800: Loss = -12504.8076171875
Iteration 9900: Loss = -12504.4404296875
Iteration 10000: Loss = -12504.3173828125
Iteration 10100: Loss = -12504.267578125
Iteration 10200: Loss = -12504.236328125
Iteration 10300: Loss = -12504.2099609375
Iteration 10400: Loss = -12504.1826171875
Iteration 10500: Loss = -12504.1513671875
Iteration 10600: Loss = -12504.14453125
Iteration 10700: Loss = -12504.1376953125
Iteration 10800: Loss = -12504.134765625
Iteration 10900: Loss = -12504.12890625
Iteration 11000: Loss = -12504.123046875
Iteration 11100: Loss = -12504.12109375
Iteration 11200: Loss = -12504.1171875
Iteration 11300: Loss = -12504.1171875
Iteration 11400: Loss = -12504.1142578125
Iteration 11500: Loss = -12504.1103515625
Iteration 11600: Loss = -12504.109375
Iteration 11700: Loss = -12504.107421875
Iteration 11800: Loss = -12504.107421875
Iteration 11900: Loss = -12504.1044921875
Iteration 12000: Loss = -12504.10546875
1
Iteration 12100: Loss = -12504.1025390625
Iteration 12200: Loss = -12504.1025390625
Iteration 12300: Loss = -12504.1005859375
Iteration 12400: Loss = -12504.1015625
1
Iteration 12500: Loss = -12504.0986328125
Iteration 12600: Loss = -12504.099609375
1
Iteration 12700: Loss = -12504.099609375
2
Iteration 12800: Loss = -12504.09765625
Iteration 12900: Loss = -12504.09765625
Iteration 13000: Loss = -12504.0966796875
Iteration 13100: Loss = -12504.095703125
Iteration 13200: Loss = -12504.0947265625
Iteration 13300: Loss = -12504.0966796875
1
Iteration 13400: Loss = -12504.0947265625
Iteration 13500: Loss = -12504.0947265625
Iteration 13600: Loss = -12504.0947265625
Iteration 13700: Loss = -12504.09375
Iteration 13800: Loss = -12504.09375
Iteration 13900: Loss = -12504.091796875
Iteration 14000: Loss = -12504.091796875
Iteration 14100: Loss = -12504.0927734375
1
Iteration 14200: Loss = -12504.091796875
Iteration 14300: Loss = -12504.0908203125
Iteration 14400: Loss = -12504.0908203125
Iteration 14500: Loss = -12504.0908203125
Iteration 14600: Loss = -12504.0908203125
Iteration 14700: Loss = -12504.09375
1
Iteration 14800: Loss = -12504.0908203125
Iteration 14900: Loss = -12504.09375
1
Iteration 15000: Loss = -12504.08984375
Iteration 15100: Loss = -12504.08984375
Iteration 15200: Loss = -12504.08984375
Iteration 15300: Loss = -12504.09375
1
Iteration 15400: Loss = -12504.0888671875
Iteration 15500: Loss = -12504.0908203125
1
Iteration 15600: Loss = -12504.0888671875
Iteration 15700: Loss = -12504.0908203125
1
Iteration 15800: Loss = -12504.0888671875
Iteration 15900: Loss = -12504.0888671875
Iteration 16000: Loss = -12504.08984375
1
Iteration 16100: Loss = -12504.08984375
2
Iteration 16200: Loss = -12504.08984375
3
Iteration 16300: Loss = -12504.087890625
Iteration 16400: Loss = -12504.08984375
1
Iteration 16500: Loss = -12504.087890625
Iteration 16600: Loss = -12504.08984375
1
Iteration 16700: Loss = -12504.0888671875
2
Iteration 16800: Loss = -12504.0888671875
3
Iteration 16900: Loss = -12504.0888671875
4
Iteration 17000: Loss = -12504.087890625
Iteration 17100: Loss = -12504.0888671875
1
Iteration 17200: Loss = -12504.0888671875
2
Iteration 17300: Loss = -12504.0888671875
3
Iteration 17400: Loss = -12504.0888671875
4
Iteration 17500: Loss = -12504.0888671875
5
Iteration 17600: Loss = -12504.08984375
6
Iteration 17700: Loss = -12504.0888671875
7
Iteration 17800: Loss = -12504.0888671875
8
Iteration 17900: Loss = -12504.0888671875
9
Iteration 18000: Loss = -12504.08984375
10
Iteration 18100: Loss = -12504.087890625
Iteration 18200: Loss = -12504.0888671875
1
Iteration 18300: Loss = -12504.087890625
Iteration 18400: Loss = -12504.0888671875
1
Iteration 18500: Loss = -12504.0888671875
2
Iteration 18600: Loss = -12504.0888671875
3
Iteration 18700: Loss = -12504.08984375
4
Iteration 18800: Loss = -12504.087890625
Iteration 18900: Loss = -12504.0888671875
1
Iteration 19000: Loss = -12504.08984375
2
Iteration 19100: Loss = -12504.0869140625
Iteration 19200: Loss = -12504.087890625
1
Iteration 19300: Loss = -12504.087890625
2
Iteration 19400: Loss = -12504.087890625
3
Iteration 19500: Loss = -12504.08984375
4
Iteration 19600: Loss = -12504.0888671875
5
Iteration 19700: Loss = -12504.087890625
6
Iteration 19800: Loss = -12504.0888671875
7
Iteration 19900: Loss = -12504.0888671875
8
Iteration 20000: Loss = -12504.0869140625
Iteration 20100: Loss = -12504.08984375
1
Iteration 20200: Loss = -12504.0888671875
2
Iteration 20300: Loss = -12504.08984375
3
Iteration 20400: Loss = -12504.0888671875
4
Iteration 20500: Loss = -12504.087890625
5
Iteration 20600: Loss = -12504.0888671875
6
Iteration 20700: Loss = -12504.087890625
7
Iteration 20800: Loss = -12504.087890625
8
Iteration 20900: Loss = -12504.0888671875
9
Iteration 21000: Loss = -12504.0888671875
10
Iteration 21100: Loss = -12504.087890625
11
Iteration 21200: Loss = -12504.0888671875
12
Iteration 21300: Loss = -12504.087890625
13
Iteration 21400: Loss = -12504.0869140625
Iteration 21500: Loss = -12504.087890625
1
Iteration 21600: Loss = -12504.0869140625
Iteration 21700: Loss = -12504.0888671875
1
Iteration 21800: Loss = -12504.0888671875
2
Iteration 21900: Loss = -12504.0888671875
3
Iteration 22000: Loss = -12504.0888671875
4
Iteration 22100: Loss = -12504.0888671875
5
Iteration 22200: Loss = -12504.0888671875
6
Iteration 22300: Loss = -12504.087890625
7
Iteration 22400: Loss = -12504.087890625
8
Iteration 22500: Loss = -12504.0888671875
9
Iteration 22600: Loss = -12504.0888671875
10
Iteration 22700: Loss = -12504.08984375
11
Iteration 22800: Loss = -12504.0888671875
12
Iteration 22900: Loss = -12504.0888671875
13
Iteration 23000: Loss = -12504.08984375
14
Iteration 23100: Loss = -12504.087890625
15
Stopping early at iteration 23100 due to no improvement.
pi: tensor([[9.9999e-01, 1.1736e-05],
        [2.9178e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0802, 0.9198], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3843, 0.1913],
         [0.9904, 0.1999]],

        [[0.0359, 0.2398],
         [0.0592, 0.0964]],

        [[0.6195, 0.2483],
         [0.0325, 0.8264]],

        [[0.7237, 0.2365],
         [0.0099, 0.9740]],

        [[0.2474, 0.1729],
         [0.0072, 0.0448]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.00411480160152051
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.016623577451856865
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.016623577451856865
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.005914731726761669
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.016623577451856865
Global Adjusted Rand Index: -0.008746299870922047
Average Adjusted Rand Index: -0.011980053136770554
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -48023.921875
Iteration 100: Loss = -27023.94921875
Iteration 200: Loss = -16072.837890625
Iteration 300: Loss = -13647.3876953125
Iteration 400: Loss = -13144.0986328125
Iteration 500: Loss = -12874.763671875
Iteration 600: Loss = -12747.513671875
Iteration 700: Loss = -12680.822265625
Iteration 800: Loss = -12637.1220703125
Iteration 900: Loss = -12612.388671875
Iteration 1000: Loss = -12588.560546875
Iteration 1100: Loss = -12576.125
Iteration 1200: Loss = -12566.2470703125
Iteration 1300: Loss = -12552.3056640625
Iteration 1400: Loss = -12546.4697265625
Iteration 1500: Loss = -12541.8955078125
Iteration 1600: Loss = -12538.1240234375
Iteration 1700: Loss = -12534.9580078125
Iteration 1800: Loss = -12532.265625
Iteration 1900: Loss = -12529.953125
Iteration 2000: Loss = -12527.9482421875
Iteration 2100: Loss = -12526.201171875
Iteration 2200: Loss = -12524.6640625
Iteration 2300: Loss = -12523.3076171875
Iteration 2400: Loss = -12522.103515625
Iteration 2500: Loss = -12521.02734375
Iteration 2600: Loss = -12520.06640625
Iteration 2700: Loss = -12519.2001953125
Iteration 2800: Loss = -12518.41796875
Iteration 2900: Loss = -12517.7119140625
Iteration 3000: Loss = -12517.07421875
Iteration 3100: Loss = -12516.49609375
Iteration 3200: Loss = -12515.96875
Iteration 3300: Loss = -12515.490234375
Iteration 3400: Loss = -12515.0498046875
Iteration 3500: Loss = -12514.6474609375
Iteration 3600: Loss = -12514.279296875
Iteration 3700: Loss = -12513.939453125
Iteration 3800: Loss = -12513.623046875
Iteration 3900: Loss = -12513.3349609375
Iteration 4000: Loss = -12513.0673828125
Iteration 4100: Loss = -12512.8203125
Iteration 4200: Loss = -12512.5888671875
Iteration 4300: Loss = -12512.373046875
Iteration 4400: Loss = -12512.173828125
Iteration 4500: Loss = -12511.986328125
Iteration 4600: Loss = -12511.810546875
Iteration 4700: Loss = -12511.6474609375
Iteration 4800: Loss = -12511.490234375
Iteration 4900: Loss = -12511.3466796875
Iteration 5000: Loss = -12511.208984375
Iteration 5100: Loss = -12511.08203125
Iteration 5200: Loss = -12510.9599609375
Iteration 5300: Loss = -12510.841796875
Iteration 5400: Loss = -12510.732421875
Iteration 5500: Loss = -12510.6279296875
Iteration 5600: Loss = -12510.5322265625
Iteration 5700: Loss = -12510.44140625
Iteration 5800: Loss = -12510.3544921875
Iteration 5900: Loss = -12510.2724609375
Iteration 6000: Loss = -12510.1962890625
Iteration 6100: Loss = -12510.1220703125
Iteration 6200: Loss = -12510.0537109375
Iteration 6300: Loss = -12509.986328125
Iteration 6400: Loss = -12509.9248046875
Iteration 6500: Loss = -12509.8662109375
Iteration 6600: Loss = -12509.8076171875
Iteration 6700: Loss = -12509.7548828125
Iteration 6800: Loss = -12509.705078125
Iteration 6900: Loss = -12509.6533203125
Iteration 7000: Loss = -12509.609375
Iteration 7100: Loss = -12509.56640625
Iteration 7200: Loss = -12509.5234375
Iteration 7300: Loss = -12509.484375
Iteration 7400: Loss = -12509.447265625
Iteration 7500: Loss = -12509.4130859375
Iteration 7600: Loss = -12509.37890625
Iteration 7700: Loss = -12509.3466796875
Iteration 7800: Loss = -12509.3154296875
Iteration 7900: Loss = -12509.28515625
Iteration 8000: Loss = -12509.259765625
Iteration 8100: Loss = -12509.2333984375
Iteration 8200: Loss = -12509.2080078125
Iteration 8300: Loss = -12509.18359375
Iteration 8400: Loss = -12509.1640625
Iteration 8500: Loss = -12509.1396484375
Iteration 8600: Loss = -12509.119140625
Iteration 8700: Loss = -12509.0986328125
Iteration 8800: Loss = -12509.080078125
Iteration 8900: Loss = -12509.0634765625
Iteration 9000: Loss = -12509.046875
Iteration 9100: Loss = -12509.0302734375
Iteration 9200: Loss = -12509.015625
Iteration 9300: Loss = -12508.9990234375
Iteration 9400: Loss = -12508.9853515625
Iteration 9500: Loss = -12508.97265625
Iteration 9600: Loss = -12508.9599609375
Iteration 9700: Loss = -12508.947265625
Iteration 9800: Loss = -12508.935546875
Iteration 9900: Loss = -12508.92578125
Iteration 10000: Loss = -12508.9169921875
Iteration 10100: Loss = -12508.90625
Iteration 10200: Loss = -12508.8955078125
Iteration 10300: Loss = -12508.8876953125
Iteration 10400: Loss = -12508.87890625
Iteration 10500: Loss = -12508.8701171875
Iteration 10600: Loss = -12508.8623046875
Iteration 10700: Loss = -12508.85546875
Iteration 10800: Loss = -12508.8486328125
Iteration 10900: Loss = -12508.841796875
Iteration 11000: Loss = -12508.8349609375
Iteration 11100: Loss = -12508.8330078125
Iteration 11200: Loss = -12508.822265625
Iteration 11300: Loss = -12508.8193359375
Iteration 11400: Loss = -12508.8115234375
Iteration 11500: Loss = -12508.8076171875
Iteration 11600: Loss = -12508.802734375
Iteration 11700: Loss = -12508.798828125
Iteration 11800: Loss = -12508.794921875
Iteration 11900: Loss = -12508.791015625
Iteration 12000: Loss = -12508.7861328125
Iteration 12100: Loss = -12508.783203125
Iteration 12200: Loss = -12508.779296875
Iteration 12300: Loss = -12508.7763671875
Iteration 12400: Loss = -12508.7724609375
Iteration 12500: Loss = -12508.7685546875
Iteration 12600: Loss = -12508.7666015625
Iteration 12700: Loss = -12508.7626953125
Iteration 12800: Loss = -12508.7607421875
Iteration 12900: Loss = -12508.7578125
Iteration 13000: Loss = -12508.755859375
Iteration 13100: Loss = -12508.75390625
Iteration 13200: Loss = -12508.7509765625
Iteration 13300: Loss = -12508.7490234375
Iteration 13400: Loss = -12508.748046875
Iteration 13500: Loss = -12508.7451171875
Iteration 13600: Loss = -12508.7431640625
Iteration 13700: Loss = -12508.7431640625
Iteration 13800: Loss = -12508.7392578125
Iteration 13900: Loss = -12508.73828125
Iteration 14000: Loss = -12508.7373046875
Iteration 14100: Loss = -12508.7353515625
Iteration 14200: Loss = -12508.7333984375
Iteration 14300: Loss = -12508.732421875
Iteration 14400: Loss = -12508.732421875
Iteration 14500: Loss = -12508.7294921875
Iteration 14600: Loss = -12508.728515625
Iteration 14700: Loss = -12508.728515625
Iteration 14800: Loss = -12508.7275390625
Iteration 14900: Loss = -12508.7255859375
Iteration 15000: Loss = -12508.7255859375
Iteration 15100: Loss = -12508.7236328125
Iteration 15200: Loss = -12508.72265625
Iteration 15300: Loss = -12508.7216796875
Iteration 15400: Loss = -12508.72265625
1
Iteration 15500: Loss = -12508.7197265625
Iteration 15600: Loss = -12508.7216796875
1
Iteration 15700: Loss = -12508.72265625
2
Iteration 15800: Loss = -12508.7197265625
Iteration 15900: Loss = -12508.71875
Iteration 16000: Loss = -12508.7177734375
Iteration 16100: Loss = -12508.7177734375
Iteration 16200: Loss = -12508.716796875
Iteration 16300: Loss = -12508.716796875
Iteration 16400: Loss = -12508.71484375
Iteration 16500: Loss = -12508.7216796875
1
Iteration 16600: Loss = -12508.7138671875
Iteration 16700: Loss = -12508.71484375
1
Iteration 16800: Loss = -12508.7138671875
Iteration 16900: Loss = -12508.712890625
Iteration 17000: Loss = -12508.7138671875
1
Iteration 17100: Loss = -12508.7119140625
Iteration 17200: Loss = -12508.7138671875
1
Iteration 17300: Loss = -12508.71484375
2
Iteration 17400: Loss = -12508.7119140625
Iteration 17500: Loss = -12508.7119140625
Iteration 17600: Loss = -12508.7119140625
Iteration 17700: Loss = -12508.712890625
1
Iteration 17800: Loss = -12508.7119140625
Iteration 17900: Loss = -12508.7109375
Iteration 18000: Loss = -12508.7099609375
Iteration 18100: Loss = -12508.7099609375
Iteration 18200: Loss = -12508.7099609375
Iteration 18300: Loss = -12508.7099609375
Iteration 18400: Loss = -12508.7119140625
1
Iteration 18500: Loss = -12508.7099609375
Iteration 18600: Loss = -12508.7099609375
Iteration 18700: Loss = -12508.7109375
1
Iteration 18800: Loss = -12508.7119140625
2
Iteration 18900: Loss = -12508.708984375
Iteration 19000: Loss = -12508.70703125
Iteration 19100: Loss = -12508.7080078125
1
Iteration 19200: Loss = -12508.7080078125
2
Iteration 19300: Loss = -12508.70703125
Iteration 19400: Loss = -12508.708984375
1
Iteration 19500: Loss = -12508.708984375
2
Iteration 19600: Loss = -12508.70703125
Iteration 19700: Loss = -12508.7080078125
1
Iteration 19800: Loss = -12508.7060546875
Iteration 19900: Loss = -12508.7060546875
Iteration 20000: Loss = -12508.69921875
Iteration 20100: Loss = -12508.376953125
Iteration 20200: Loss = -12508.0576171875
Iteration 20300: Loss = -12508.0478515625
Iteration 20400: Loss = -12508.04296875
Iteration 20500: Loss = -12508.041015625
Iteration 20600: Loss = -12508.0390625
Iteration 20700: Loss = -12508.0390625
Iteration 20800: Loss = -12508.0361328125
Iteration 20900: Loss = -12508.0361328125
Iteration 21000: Loss = -12508.0341796875
Iteration 21100: Loss = -12508.0341796875
Iteration 21200: Loss = -12508.0341796875
Iteration 21300: Loss = -12508.0341796875
Iteration 21400: Loss = -12508.0341796875
Iteration 21500: Loss = -12508.0322265625
Iteration 21600: Loss = -12508.033203125
1
Iteration 21700: Loss = -12508.0341796875
2
Iteration 21800: Loss = -12508.0341796875
3
Iteration 21900: Loss = -12508.0341796875
4
Iteration 22000: Loss = -12508.033203125
5
Iteration 22100: Loss = -12508.033203125
6
Iteration 22200: Loss = -12508.033203125
7
Iteration 22300: Loss = -12508.0322265625
Iteration 22400: Loss = -12508.021484375
Iteration 22500: Loss = -12508.01953125
Iteration 22600: Loss = -12508.017578125
Iteration 22700: Loss = -12508.017578125
Iteration 22800: Loss = -12508.01953125
1
Iteration 22900: Loss = -12508.0185546875
2
Iteration 23000: Loss = -12508.0166015625
Iteration 23100: Loss = -12508.0048828125
Iteration 23200: Loss = -12507.7373046875
Iteration 23300: Loss = -12507.7373046875
Iteration 23400: Loss = -12507.5888671875
Iteration 23500: Loss = -12506.1201171875
Iteration 23600: Loss = -12506.0234375
Iteration 23700: Loss = -12506.005859375
Iteration 23800: Loss = -12505.998046875
Iteration 23900: Loss = -12505.994140625
Iteration 24000: Loss = -12505.9912109375
Iteration 24100: Loss = -12505.98828125
Iteration 24200: Loss = -12505.98828125
Iteration 24300: Loss = -12505.9892578125
1
Iteration 24400: Loss = -12505.9873046875
Iteration 24500: Loss = -12505.986328125
Iteration 24600: Loss = -12505.9853515625
Iteration 24700: Loss = -12505.9853515625
Iteration 24800: Loss = -12505.984375
Iteration 24900: Loss = -12505.984375
Iteration 25000: Loss = -12505.9833984375
Iteration 25100: Loss = -12505.9853515625
1
Iteration 25200: Loss = -12505.982421875
Iteration 25300: Loss = -12505.982421875
Iteration 25400: Loss = -12505.982421875
Iteration 25500: Loss = -12505.982421875
Iteration 25600: Loss = -12505.9814453125
Iteration 25700: Loss = -12505.9833984375
1
Iteration 25800: Loss = -12505.98046875
Iteration 25900: Loss = -12505.9814453125
1
Iteration 26000: Loss = -12505.98046875
Iteration 26100: Loss = -12505.9814453125
1
Iteration 26200: Loss = -12505.982421875
2
Iteration 26300: Loss = -12505.9814453125
3
Iteration 26400: Loss = -12505.982421875
4
Iteration 26500: Loss = -12505.9814453125
5
Iteration 26600: Loss = -12505.9814453125
6
Iteration 26700: Loss = -12505.9833984375
7
Iteration 26800: Loss = -12505.9814453125
8
Iteration 26900: Loss = -12505.982421875
9
Iteration 27000: Loss = -12505.98046875
Iteration 27100: Loss = -12505.98046875
Iteration 27200: Loss = -12505.9814453125
1
Iteration 27300: Loss = -12505.9814453125
2
Iteration 27400: Loss = -12505.982421875
3
Iteration 27500: Loss = -12505.9814453125
4
Iteration 27600: Loss = -12505.9814453125
5
Iteration 27700: Loss = -12505.9814453125
6
Iteration 27800: Loss = -12505.98046875
Iteration 27900: Loss = -12505.9814453125
1
Iteration 28000: Loss = -12505.98046875
Iteration 28100: Loss = -12505.982421875
1
Iteration 28200: Loss = -12505.982421875
2
Iteration 28300: Loss = -12505.9814453125
3
Iteration 28400: Loss = -12505.98046875
Iteration 28500: Loss = -12505.9814453125
1
Iteration 28600: Loss = -12505.9814453125
2
Iteration 28700: Loss = -12505.982421875
3
Iteration 28800: Loss = -12505.9833984375
4
Iteration 28900: Loss = -12505.98046875
Iteration 29000: Loss = -12505.982421875
1
Iteration 29100: Loss = -12505.98046875
Iteration 29200: Loss = -12505.98046875
Iteration 29300: Loss = -12505.9814453125
1
Iteration 29400: Loss = -12505.9814453125
2
Iteration 29500: Loss = -12505.982421875
3
Iteration 29600: Loss = -12505.9814453125
4
Iteration 29700: Loss = -12505.98046875
Iteration 29800: Loss = -12505.98046875
Iteration 29900: Loss = -12505.9814453125
1
pi: tensor([[1.0000e+00, 5.9900e-08],
        [1.7981e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9802, 0.0198], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2024, 0.1885],
         [0.0140, 1.0000]],

        [[0.9919, 0.2705],
         [0.9916, 0.8188]],

        [[0.0909, 0.2506],
         [0.1648, 0.9235]],

        [[0.0134, 0.2351],
         [0.2448, 0.5732]],

        [[0.3338, 0.2041],
         [0.0263, 0.8621]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: -0.009009494263563287
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: -0.009009494263563287
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0007748402262652058
Global Adjusted Rand Index: -0.0038912575267512446
Average Adjusted Rand Index: -0.003960302570392354
[-0.008746299870922047, -0.0038912575267512446] [-0.011980053136770554, -0.003960302570392354] [12504.087890625, 12505.982421875]
-------------------------------------
This iteration is 26
True Objective function: Loss = -11952.214575692185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35009.890625
Iteration 100: Loss = -24272.216796875
Iteration 200: Loss = -16368.0498046875
Iteration 300: Loss = -13238.3017578125
Iteration 400: Loss = -12709.171875
Iteration 500: Loss = -12617.822265625
Iteration 600: Loss = -12572.86328125
Iteration 700: Loss = -12541.2265625
Iteration 800: Loss = -12522.57421875
Iteration 900: Loss = -12506.8955078125
Iteration 1000: Loss = -12495.423828125
Iteration 1100: Loss = -12484.244140625
Iteration 1200: Loss = -12476.474609375
Iteration 1300: Loss = -12469.490234375
Iteration 1400: Loss = -12462.3837890625
Iteration 1500: Loss = -12458.435546875
Iteration 1600: Loss = -12454.6962890625
Iteration 1700: Loss = -12451.310546875
Iteration 1800: Loss = -12448.8330078125
Iteration 1900: Loss = -12444.9736328125
Iteration 2000: Loss = -12441.71875
Iteration 2100: Loss = -12440.087890625
Iteration 2200: Loss = -12438.7265625
Iteration 2300: Loss = -12437.2998046875
Iteration 2400: Loss = -12436.1669921875
Iteration 2500: Loss = -12435.412109375
Iteration 2600: Loss = -12434.6533203125
Iteration 2700: Loss = -12433.5478515625
Iteration 2800: Loss = -12432.7431640625
Iteration 2900: Loss = -12432.162109375
Iteration 3000: Loss = -12430.8408203125
Iteration 3100: Loss = -12430.3291015625
Iteration 3200: Loss = -12429.958984375
Iteration 3300: Loss = -12429.5947265625
Iteration 3400: Loss = -12429.216796875
Iteration 3500: Loss = -12428.857421875
Iteration 3600: Loss = -12428.5595703125
Iteration 3700: Loss = -12428.3251953125
Iteration 3800: Loss = -12428.125
Iteration 3900: Loss = -12427.7626953125
Iteration 4000: Loss = -12426.6357421875
Iteration 4100: Loss = -12426.2890625
Iteration 4200: Loss = -12424.2919921875
Iteration 4300: Loss = -12424.12109375
Iteration 4400: Loss = -12423.9814453125
Iteration 4500: Loss = -12422.5869140625
Iteration 4600: Loss = -12422.4951171875
Iteration 4700: Loss = -12422.4140625
Iteration 4800: Loss = -12422.337890625
Iteration 4900: Loss = -12422.2666015625
Iteration 5000: Loss = -12422.2001953125
Iteration 5100: Loss = -12422.1357421875
Iteration 5200: Loss = -12422.0654296875
Iteration 5300: Loss = -12421.6474609375
Iteration 5400: Loss = -12420.7392578125
Iteration 5500: Loss = -12420.640625
Iteration 5600: Loss = -12420.5361328125
Iteration 5700: Loss = -12419.9580078125
Iteration 5800: Loss = -12418.8212890625
Iteration 5900: Loss = -12416.9111328125
Iteration 6000: Loss = -12415.443359375
Iteration 6100: Loss = -12415.392578125
Iteration 6200: Loss = -12415.3330078125
Iteration 6300: Loss = -12415.15234375
Iteration 6400: Loss = -12415.080078125
Iteration 6500: Loss = -12415.0517578125
Iteration 6600: Loss = -12415.02734375
Iteration 6700: Loss = -12415.00390625
Iteration 6800: Loss = -12414.98046875
Iteration 6900: Loss = -12414.9599609375
Iteration 7000: Loss = -12414.9404296875
Iteration 7100: Loss = -12414.921875
Iteration 7200: Loss = -12414.90625
Iteration 7300: Loss = -12414.890625
Iteration 7400: Loss = -12414.875
Iteration 7500: Loss = -12414.8583984375
Iteration 7600: Loss = -12414.845703125
Iteration 7700: Loss = -12414.8310546875
Iteration 7800: Loss = -12414.8203125
Iteration 7900: Loss = -12414.8076171875
Iteration 8000: Loss = -12414.7958984375
Iteration 8100: Loss = -12414.7861328125
Iteration 8200: Loss = -12414.77734375
Iteration 8300: Loss = -12414.7685546875
Iteration 8400: Loss = -12414.7587890625
Iteration 8500: Loss = -12414.7509765625
Iteration 8600: Loss = -12414.7431640625
Iteration 8700: Loss = -12414.7353515625
Iteration 8800: Loss = -12414.7265625
Iteration 8900: Loss = -12414.7197265625
Iteration 9000: Loss = -12414.416015625
Iteration 9100: Loss = -12413.5625
Iteration 9200: Loss = -12413.5546875
Iteration 9300: Loss = -12413.5498046875
Iteration 9400: Loss = -12413.544921875
Iteration 9500: Loss = -12413.5400390625
Iteration 9600: Loss = -12413.53515625
Iteration 9700: Loss = -12413.533203125
Iteration 9800: Loss = -12413.5263671875
Iteration 9900: Loss = -12413.5234375
Iteration 10000: Loss = -12413.5205078125
Iteration 10100: Loss = -12413.517578125
Iteration 10200: Loss = -12413.513671875
Iteration 10300: Loss = -12413.5087890625
Iteration 10400: Loss = -12413.5068359375
Iteration 10500: Loss = -12413.50390625
Iteration 10600: Loss = -12413.5
Iteration 10700: Loss = -12413.498046875
Iteration 10800: Loss = -12413.49609375
Iteration 10900: Loss = -12413.4931640625
Iteration 11000: Loss = -12413.4912109375
Iteration 11100: Loss = -12413.48828125
Iteration 11200: Loss = -12413.4833984375
Iteration 11300: Loss = -12412.306640625
Iteration 11400: Loss = -12412.2958984375
Iteration 11500: Loss = -12412.1923828125
Iteration 11600: Loss = -12411.0400390625
Iteration 11700: Loss = -12411.037109375
Iteration 11800: Loss = -12411.0361328125
Iteration 11900: Loss = -12411.033203125
Iteration 12000: Loss = -12409.9033203125
Iteration 12100: Loss = -12409.89453125
Iteration 12200: Loss = -12409.890625
Iteration 12300: Loss = -12409.888671875
Iteration 12400: Loss = -12409.8876953125
Iteration 12500: Loss = -12409.88671875
Iteration 12600: Loss = -12409.884765625
Iteration 12700: Loss = -12409.8818359375
Iteration 12800: Loss = -12409.8798828125
Iteration 12900: Loss = -12407.044921875
Iteration 13000: Loss = -12407.017578125
Iteration 13100: Loss = -12407.013671875
Iteration 13200: Loss = -12407.0126953125
Iteration 13300: Loss = -12407.0126953125
Iteration 13400: Loss = -12407.01171875
Iteration 13500: Loss = -12407.0126953125
1
Iteration 13600: Loss = -12407.0126953125
2
Iteration 13700: Loss = -12407.0107421875
Iteration 13800: Loss = -12406.970703125
Iteration 13900: Loss = -12405.640625
Iteration 14000: Loss = -12405.638671875
Iteration 14100: Loss = -12405.638671875
Iteration 14200: Loss = -12405.6376953125
Iteration 14300: Loss = -12405.6357421875
Iteration 14400: Loss = -12405.6357421875
Iteration 14500: Loss = -12405.63671875
1
Iteration 14600: Loss = -12405.634765625
Iteration 14700: Loss = -12405.6357421875
1
Iteration 14800: Loss = -12405.6337890625
Iteration 14900: Loss = -12405.6328125
Iteration 15000: Loss = -12405.6337890625
1
Iteration 15100: Loss = -12405.6328125
Iteration 15200: Loss = -12405.6337890625
1
Iteration 15300: Loss = -12405.6328125
Iteration 15400: Loss = -12404.0341796875
Iteration 15500: Loss = -12404.0322265625
Iteration 15600: Loss = -12404.03125
Iteration 15700: Loss = -12404.0302734375
Iteration 15800: Loss = -12404.0302734375
Iteration 15900: Loss = -12404.029296875
Iteration 16000: Loss = -12404.029296875
Iteration 16100: Loss = -12404.0302734375
1
Iteration 16200: Loss = -12404.0302734375
2
Iteration 16300: Loss = -12404.029296875
Iteration 16400: Loss = -12404.03125
1
Iteration 16500: Loss = -12404.029296875
Iteration 16600: Loss = -12404.029296875
Iteration 16700: Loss = -12404.029296875
Iteration 16800: Loss = -12404.0283203125
Iteration 16900: Loss = -12404.029296875
1
Iteration 17000: Loss = -12404.029296875
2
Iteration 17100: Loss = -12404.029296875
3
Iteration 17200: Loss = -12404.029296875
4
Iteration 17300: Loss = -12404.0283203125
Iteration 17400: Loss = -12404.0302734375
1
Iteration 17500: Loss = -12404.0283203125
Iteration 17600: Loss = -12404.029296875
1
Iteration 17700: Loss = -12404.02734375
Iteration 17800: Loss = -12404.029296875
1
Iteration 17900: Loss = -12404.02734375
Iteration 18000: Loss = -12404.0283203125
1
Iteration 18100: Loss = -12404.0283203125
2
Iteration 18200: Loss = -12404.02734375
Iteration 18300: Loss = -12404.0283203125
1
Iteration 18400: Loss = -12404.02734375
Iteration 18500: Loss = -12404.02734375
Iteration 18600: Loss = -12404.0283203125
1
Iteration 18700: Loss = -12403.90234375
Iteration 18800: Loss = -12401.6083984375
Iteration 18900: Loss = -12401.6015625
Iteration 19000: Loss = -12401.5986328125
Iteration 19100: Loss = -12401.5986328125
Iteration 19200: Loss = -12401.5986328125
Iteration 19300: Loss = -12401.59765625
Iteration 19400: Loss = -12401.5986328125
1
Iteration 19500: Loss = -12401.5986328125
2
Iteration 19600: Loss = -12401.59765625
Iteration 19700: Loss = -12401.59765625
Iteration 19800: Loss = -12401.59765625
Iteration 19900: Loss = -12401.5986328125
1
Iteration 20000: Loss = -12401.5986328125
2
Iteration 20100: Loss = -12401.59765625
Iteration 20200: Loss = -12401.5986328125
1
Iteration 20300: Loss = -12401.59765625
Iteration 20400: Loss = -12401.59765625
Iteration 20500: Loss = -12401.5966796875
Iteration 20600: Loss = -12401.5966796875
Iteration 20700: Loss = -12401.5986328125
1
Iteration 20800: Loss = -12401.599609375
2
Iteration 20900: Loss = -12401.5966796875
Iteration 21000: Loss = -12401.59765625
1
Iteration 21100: Loss = -12401.595703125
Iteration 21200: Loss = -12401.6005859375
1
Iteration 21300: Loss = -12401.5986328125
2
Iteration 21400: Loss = -12401.59765625
3
Iteration 21500: Loss = -12401.5986328125
4
Iteration 21600: Loss = -12401.5986328125
5
Iteration 21700: Loss = -12401.59765625
6
Iteration 21800: Loss = -12401.59765625
7
Iteration 21900: Loss = -12401.5966796875
8
Iteration 22000: Loss = -12401.5966796875
9
Iteration 22100: Loss = -12401.59765625
10
Iteration 22200: Loss = -12401.59765625
11
Iteration 22300: Loss = -12401.59765625
12
Iteration 22400: Loss = -12401.59765625
13
Iteration 22500: Loss = -12401.5986328125
14
Iteration 22600: Loss = -12401.5986328125
15
Stopping early at iteration 22600 due to no improvement.
pi: tensor([[1.7283e-01, 8.2717e-01],
        [6.5505e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 1.0229e-05], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2035, 0.2226],
         [0.7082, 0.2012]],

        [[0.9837, 0.1906],
         [0.0156, 0.3087]],

        [[0.6756, 0.1471],
         [0.5858, 0.0311]],

        [[0.0099, 0.5024],
         [0.0387, 0.7644]],

        [[0.9185, 0.3617],
         [0.7288, 0.4743]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.001576224575319747
Average Adjusted Rand Index: 0.0001633280491905654
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27578.76953125
Iteration 100: Loss = -18268.984375
Iteration 200: Loss = -14195.7890625
Iteration 300: Loss = -13037.9658203125
Iteration 400: Loss = -12667.70703125
Iteration 500: Loss = -12561.4130859375
Iteration 600: Loss = -12506.9365234375
Iteration 700: Loss = -12475.1865234375
Iteration 800: Loss = -12455.9404296875
Iteration 900: Loss = -12443.1875
Iteration 1000: Loss = -12436.248046875
Iteration 1100: Loss = -12432.12890625
Iteration 1200: Loss = -12429.0302734375
Iteration 1300: Loss = -12426.3173828125
Iteration 1400: Loss = -12423.845703125
Iteration 1500: Loss = -12421.6982421875
Iteration 1600: Loss = -12419.9521484375
Iteration 1700: Loss = -12418.5732421875
Iteration 1800: Loss = -12417.26171875
Iteration 1900: Loss = -12413.1767578125
Iteration 2000: Loss = -12409.623046875
Iteration 2100: Loss = -12407.736328125
Iteration 2200: Loss = -12406.615234375
Iteration 2300: Loss = -12405.802734375
Iteration 2400: Loss = -12405.169921875
Iteration 2500: Loss = -12404.65234375
Iteration 2600: Loss = -12404.22265625
Iteration 2700: Loss = -12403.85546875
Iteration 2800: Loss = -12403.5380859375
Iteration 2900: Loss = -12403.2587890625
Iteration 3000: Loss = -12403.0126953125
Iteration 3100: Loss = -12402.79296875
Iteration 3200: Loss = -12402.5966796875
Iteration 3300: Loss = -12402.4208984375
Iteration 3400: Loss = -12402.259765625
Iteration 3500: Loss = -12402.1162109375
Iteration 3600: Loss = -12401.982421875
Iteration 3700: Loss = -12401.861328125
Iteration 3800: Loss = -12401.75
Iteration 3900: Loss = -12401.646484375
Iteration 4000: Loss = -12401.552734375
Iteration 4100: Loss = -12401.46484375
Iteration 4200: Loss = -12401.3828125
Iteration 4300: Loss = -12401.3076171875
Iteration 4400: Loss = -12401.236328125
Iteration 4500: Loss = -12401.16796875
Iteration 4600: Loss = -12401.107421875
Iteration 4700: Loss = -12401.05078125
Iteration 4800: Loss = -12400.9951171875
Iteration 4900: Loss = -12400.9462890625
Iteration 5000: Loss = -12400.8974609375
Iteration 5100: Loss = -12400.8525390625
Iteration 5200: Loss = -12400.8095703125
Iteration 5300: Loss = -12400.7705078125
Iteration 5400: Loss = -12400.734375
Iteration 5500: Loss = -12400.69921875
Iteration 5600: Loss = -12400.6669921875
Iteration 5700: Loss = -12400.634765625
Iteration 5800: Loss = -12400.60546875
Iteration 5900: Loss = -12400.578125
Iteration 6000: Loss = -12400.5537109375
Iteration 6100: Loss = -12400.52734375
Iteration 6200: Loss = -12400.505859375
Iteration 6300: Loss = -12400.4833984375
Iteration 6400: Loss = -12400.462890625
Iteration 6500: Loss = -12400.4443359375
Iteration 6600: Loss = -12400.4267578125
Iteration 6700: Loss = -12400.41015625
Iteration 6800: Loss = -12400.3935546875
Iteration 6900: Loss = -12400.3779296875
Iteration 7000: Loss = -12400.361328125
Iteration 7100: Loss = -12400.3486328125
Iteration 7200: Loss = -12400.3369140625
Iteration 7300: Loss = -12400.3232421875
Iteration 7400: Loss = -12400.3115234375
Iteration 7500: Loss = -12400.3017578125
Iteration 7600: Loss = -12400.291015625
Iteration 7700: Loss = -12400.2802734375
Iteration 7800: Loss = -12400.2705078125
Iteration 7900: Loss = -12400.26171875
Iteration 8000: Loss = -12400.25390625
Iteration 8100: Loss = -12400.24609375
Iteration 8200: Loss = -12400.2373046875
Iteration 8300: Loss = -12400.23046875
Iteration 8400: Loss = -12400.2216796875
Iteration 8500: Loss = -12400.21484375
Iteration 8600: Loss = -12400.2080078125
Iteration 8700: Loss = -12400.201171875
Iteration 8800: Loss = -12400.1953125
Iteration 8900: Loss = -12400.1875
Iteration 9000: Loss = -12400.1826171875
Iteration 9100: Loss = -12400.17578125
Iteration 9200: Loss = -12400.1708984375
Iteration 9300: Loss = -12400.1630859375
Iteration 9400: Loss = -12400.15625
Iteration 9500: Loss = -12400.1484375
Iteration 9600: Loss = -12400.1396484375
Iteration 9700: Loss = -12400.1318359375
Iteration 9800: Loss = -12400.1220703125
Iteration 9900: Loss = -12400.103515625
Iteration 10000: Loss = -12400.0703125
Iteration 10100: Loss = -12399.9716796875
Iteration 10200: Loss = -12399.7568359375
Iteration 10300: Loss = -12399.5888671875
Iteration 10400: Loss = -12399.421875
Iteration 10500: Loss = -12399.2998046875
Iteration 10600: Loss = -12399.22265625
Iteration 10700: Loss = -12399.1728515625
Iteration 10800: Loss = -12399.130859375
Iteration 10900: Loss = -12399.0947265625
Iteration 11000: Loss = -12399.033203125
Iteration 11100: Loss = -12398.9990234375
Iteration 11200: Loss = -12398.9140625
Iteration 11300: Loss = -12398.87890625
Iteration 11400: Loss = -12398.8076171875
Iteration 11500: Loss = -12398.7783203125
Iteration 11600: Loss = -12398.7275390625
Iteration 11700: Loss = -12398.6904296875
Iteration 11800: Loss = -12398.6640625
Iteration 11900: Loss = -12398.65234375
Iteration 12000: Loss = -12398.6337890625
Iteration 12100: Loss = -12398.5908203125
Iteration 12200: Loss = -12398.556640625
Iteration 12300: Loss = -12398.5009765625
Iteration 12400: Loss = -12398.455078125
Iteration 12500: Loss = -12398.400390625
Iteration 12600: Loss = -12398.3515625
Iteration 12700: Loss = -12398.328125
Iteration 12800: Loss = -12398.2548828125
Iteration 12900: Loss = -12398.1435546875
Iteration 13000: Loss = -12398.107421875
Iteration 13100: Loss = -12398.0849609375
Iteration 13200: Loss = -12398.0634765625
Iteration 13300: Loss = -12398.029296875
Iteration 13400: Loss = -12398.0009765625
Iteration 13500: Loss = -12397.97265625
Iteration 13600: Loss = -12397.9140625
Iteration 13700: Loss = -12397.8798828125
Iteration 13800: Loss = -12397.8603515625
Iteration 13900: Loss = -12397.8349609375
Iteration 14000: Loss = -12397.79296875
Iteration 14100: Loss = -12397.7587890625
Iteration 14200: Loss = -12397.7216796875
Iteration 14300: Loss = -12397.7099609375
Iteration 14400: Loss = -12397.7001953125
Iteration 14500: Loss = -12397.67578125
Iteration 14600: Loss = -12397.658203125
Iteration 14700: Loss = -12397.63671875
Iteration 14800: Loss = -12397.6328125
Iteration 14900: Loss = -12397.6298828125
Iteration 15000: Loss = -12397.626953125
Iteration 15100: Loss = -12397.626953125
Iteration 15200: Loss = -12397.626953125
Iteration 15300: Loss = -12397.6259765625
Iteration 15400: Loss = -12397.6279296875
1
Iteration 15500: Loss = -12397.626953125
2
Iteration 15600: Loss = -12397.6279296875
3
Iteration 15700: Loss = -12397.626953125
4
Iteration 15800: Loss = -12397.599609375
Iteration 15900: Loss = -12397.56640625
Iteration 16000: Loss = -12397.564453125
Iteration 16100: Loss = -12397.564453125
Iteration 16200: Loss = -12397.5654296875
1
Iteration 16300: Loss = -12397.5654296875
2
Iteration 16400: Loss = -12397.5654296875
3
Iteration 16500: Loss = -12397.5654296875
4
Iteration 16600: Loss = -12397.56640625
5
Iteration 16700: Loss = -12397.5654296875
6
Iteration 16800: Loss = -12397.5654296875
7
Iteration 16900: Loss = -12397.5654296875
8
Iteration 17000: Loss = -12397.564453125
Iteration 17100: Loss = -12397.5654296875
1
Iteration 17200: Loss = -12397.564453125
Iteration 17300: Loss = -12397.5654296875
1
Iteration 17400: Loss = -12397.5654296875
2
Iteration 17500: Loss = -12397.5634765625
Iteration 17600: Loss = -12397.564453125
1
Iteration 17700: Loss = -12397.56640625
2
Iteration 17800: Loss = -12397.564453125
3
Iteration 17900: Loss = -12397.5654296875
4
Iteration 18000: Loss = -12397.56640625
5
Iteration 18100: Loss = -12397.5654296875
6
Iteration 18200: Loss = -12397.5654296875
7
Iteration 18300: Loss = -12397.5654296875
8
Iteration 18400: Loss = -12397.5654296875
9
Iteration 18500: Loss = -12397.5654296875
10
Iteration 18600: Loss = -12397.564453125
11
Iteration 18700: Loss = -12397.5673828125
12
Iteration 18800: Loss = -12397.5654296875
13
Iteration 18900: Loss = -12397.5654296875
14
Iteration 19000: Loss = -12397.56640625
15
Stopping early at iteration 19000 due to no improvement.
pi: tensor([[0.0024, 0.9976],
        [0.0723, 0.9277]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1950, 0.8050], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2352, 0.2148],
         [0.0266, 0.1968]],

        [[0.0344, 0.1946],
         [0.1571, 0.4669]],

        [[0.9933, 0.1759],
         [0.9572, 0.0745]],

        [[0.9735, 0.2464],
         [0.6495, 0.6334]],

        [[0.0600, 0.2525],
         [0.9058, 0.9924]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 59
Adjusted Rand Index: -0.006658343736995423
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0012662455124815629
Global Adjusted Rand Index: -0.0005986414753463625
Average Adjusted Rand Index: -0.001078419644902772
[0.001576224575319747, -0.0005986414753463625] [0.0001633280491905654, -0.001078419644902772] [12401.5986328125, 12397.56640625]
-------------------------------------
This iteration is 27
True Objective function: Loss = -11902.109106942185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36512.85546875
Iteration 100: Loss = -24134.96484375
Iteration 200: Loss = -15644.9013671875
Iteration 300: Loss = -13537.7275390625
Iteration 400: Loss = -13084.8916015625
Iteration 500: Loss = -12881.4970703125
Iteration 600: Loss = -12766.2470703125
Iteration 700: Loss = -12706.6376953125
Iteration 800: Loss = -12673.703125
Iteration 900: Loss = -12650.2548828125
Iteration 1000: Loss = -12634.1083984375
Iteration 1100: Loss = -12622.3837890625
Iteration 1200: Loss = -12613.4541015625
Iteration 1300: Loss = -12606.4833984375
Iteration 1400: Loss = -12600.7294921875
Iteration 1500: Loss = -12596.2333984375
Iteration 1600: Loss = -12592.66015625
Iteration 1700: Loss = -12589.7578125
Iteration 1800: Loss = -12587.3076171875
Iteration 1900: Loss = -12585.09765625
Iteration 2000: Loss = -12583.240234375
Iteration 2100: Loss = -12581.6005859375
Iteration 2200: Loss = -12580.208984375
Iteration 2300: Loss = -12578.9130859375
Iteration 2400: Loss = -12577.76171875
Iteration 2500: Loss = -12576.6572265625
Iteration 2600: Loss = -12575.5556640625
Iteration 2700: Loss = -12574.435546875
Iteration 2800: Loss = -12573.4951171875
Iteration 2900: Loss = -12572.5576171875
Iteration 3000: Loss = -12571.5771484375
Iteration 3100: Loss = -12570.607421875
Iteration 3200: Loss = -12569.654296875
Iteration 3300: Loss = -12568.6962890625
Iteration 3400: Loss = -12567.8056640625
Iteration 3500: Loss = -12566.857421875
Iteration 3600: Loss = -12565.890625
Iteration 3700: Loss = -12565.1259765625
Iteration 3800: Loss = -12564.462890625
Iteration 3900: Loss = -12563.8232421875
Iteration 4000: Loss = -12563.1552734375
Iteration 4100: Loss = -12562.41796875
Iteration 4200: Loss = -12561.681640625
Iteration 4300: Loss = -12560.9775390625
Iteration 4400: Loss = -12560.3046875
Iteration 4500: Loss = -12559.6953125
Iteration 4600: Loss = -12558.7001953125
Iteration 4700: Loss = -12556.7568359375
Iteration 4800: Loss = -12556.0087890625
Iteration 4900: Loss = -12555.3447265625
Iteration 5000: Loss = -12554.8857421875
Iteration 5100: Loss = -12554.5380859375
Iteration 5200: Loss = -12554.265625
Iteration 5300: Loss = -12554.0087890625
Iteration 5400: Loss = -12553.7080078125
Iteration 5500: Loss = -12553.4658203125
Iteration 5600: Loss = -12553.244140625
Iteration 5700: Loss = -12552.84375
Iteration 5800: Loss = -12552.640625
Iteration 5900: Loss = -12552.5322265625
Iteration 6000: Loss = -12552.4501953125
Iteration 6100: Loss = -12552.37890625
Iteration 6200: Loss = -12552.31640625
Iteration 6300: Loss = -12552.2578125
Iteration 6400: Loss = -12551.6181640625
Iteration 6500: Loss = -12550.888671875
Iteration 6600: Loss = -12550.736328125
Iteration 6700: Loss = -12550.6826171875
Iteration 6800: Loss = -12550.6376953125
Iteration 6900: Loss = -12550.5986328125
Iteration 7000: Loss = -12550.5615234375
Iteration 7100: Loss = -12550.26953125
Iteration 7200: Loss = -12549.9150390625
Iteration 7300: Loss = -12549.876953125
Iteration 7400: Loss = -12549.84765625
Iteration 7500: Loss = -12549.8203125
Iteration 7600: Loss = -12549.7958984375
Iteration 7700: Loss = -12549.7734375
Iteration 7800: Loss = -12549.7529296875
Iteration 7900: Loss = -12549.732421875
Iteration 8000: Loss = -12549.7138671875
Iteration 8100: Loss = -12549.6953125
Iteration 8200: Loss = -12549.677734375
Iteration 8300: Loss = -12549.66015625
Iteration 8400: Loss = -12549.2412109375
Iteration 8500: Loss = -12549.1005859375
Iteration 8600: Loss = -12549.0849609375
Iteration 8700: Loss = -12549.068359375
Iteration 8800: Loss = -12548.419921875
Iteration 8900: Loss = -12547.912109375
Iteration 9000: Loss = -12547.8916015625
Iteration 9100: Loss = -12547.876953125
Iteration 9200: Loss = -12547.1494140625
Iteration 9300: Loss = -12547.1357421875
Iteration 9400: Loss = -12547.1259765625
Iteration 9500: Loss = -12546.517578125
Iteration 9600: Loss = -12546.4892578125
Iteration 9700: Loss = -12546.48046875
Iteration 9800: Loss = -12546.4765625
Iteration 9900: Loss = -12546.46875
Iteration 10000: Loss = -12546.4619140625
Iteration 10100: Loss = -12546.4560546875
Iteration 10200: Loss = -12546.451171875
Iteration 10300: Loss = -12546.4453125
Iteration 10400: Loss = -12546.4404296875
Iteration 10500: Loss = -12546.4375
Iteration 10600: Loss = -12546.431640625
Iteration 10700: Loss = -12546.4267578125
Iteration 10800: Loss = -12546.423828125
Iteration 10900: Loss = -12546.421875
Iteration 11000: Loss = -12546.416015625
Iteration 11100: Loss = -12546.4140625
Iteration 11200: Loss = -12546.41015625
Iteration 11300: Loss = -12546.408203125
Iteration 11400: Loss = -12545.7587890625
Iteration 11500: Loss = -12545.7353515625
Iteration 11600: Loss = -12545.73046875
Iteration 11700: Loss = -12545.7275390625
Iteration 11800: Loss = -12545.7265625
Iteration 11900: Loss = -12545.72265625
Iteration 12000: Loss = -12545.72265625
Iteration 12100: Loss = -12545.7197265625
Iteration 12200: Loss = -12545.666015625
Iteration 12300: Loss = -12544.9677734375
Iteration 12400: Loss = -12544.966796875
Iteration 12500: Loss = -12544.966796875
Iteration 12600: Loss = -12544.9638671875
Iteration 12700: Loss = -12544.962890625
Iteration 12800: Loss = -12544.9619140625
Iteration 12900: Loss = -12544.958984375
Iteration 13000: Loss = -12544.9580078125
Iteration 13100: Loss = -12544.958984375
1
Iteration 13200: Loss = -12544.9580078125
Iteration 13300: Loss = -12544.95703125
Iteration 13400: Loss = -12544.9560546875
Iteration 13500: Loss = -12544.9541015625
Iteration 13600: Loss = -12544.9541015625
Iteration 13700: Loss = -12544.953125
Iteration 13800: Loss = -12544.9521484375
Iteration 13900: Loss = -12544.951171875
Iteration 14000: Loss = -12544.951171875
Iteration 14100: Loss = -12544.94921875
Iteration 14200: Loss = -12544.947265625
Iteration 14300: Loss = -12542.8173828125
Iteration 14400: Loss = -12542.275390625
Iteration 14500: Loss = -12540.609375
Iteration 14600: Loss = -12539.8515625
Iteration 14700: Loss = -12539.3974609375
Iteration 14800: Loss = -12537.8232421875
Iteration 14900: Loss = -12535.041015625
Iteration 15000: Loss = -12535.0224609375
Iteration 15100: Loss = -12533.48828125
Iteration 15200: Loss = -12532.029296875
Iteration 15300: Loss = -12528.2646484375
Iteration 15400: Loss = -12525.53515625
Iteration 15500: Loss = -12523.0947265625
Iteration 15600: Loss = -12518.8828125
Iteration 15700: Loss = -12512.998046875
Iteration 15800: Loss = -12509.568359375
Iteration 15900: Loss = -12504.4521484375
Iteration 16000: Loss = -12499.8720703125
Iteration 16100: Loss = -12493.1435546875
Iteration 16200: Loss = -12487.3193359375
Iteration 16300: Loss = -12480.171875
Iteration 16400: Loss = -12475.357421875
Iteration 16500: Loss = -12472.4599609375
Iteration 16600: Loss = -12468.7568359375
Iteration 16700: Loss = -12463.328125
Iteration 16800: Loss = -12461.16015625
Iteration 16900: Loss = -12459.353515625
Iteration 17000: Loss = -12455.4130859375
Iteration 17100: Loss = -12452.5634765625
Iteration 17200: Loss = -12450.3662109375
Iteration 17300: Loss = -12449.1484375
Iteration 17400: Loss = -12448.40625
Iteration 17500: Loss = -12445.1103515625
Iteration 17600: Loss = -12443.7705078125
Iteration 17700: Loss = -12440.5322265625
Iteration 17800: Loss = -12439.7900390625
Iteration 17900: Loss = -12437.6748046875
Iteration 18000: Loss = -12436.068359375
Iteration 18100: Loss = -12434.103515625
Iteration 18200: Loss = -12430.5126953125
Iteration 18300: Loss = -12430.458984375
Iteration 18400: Loss = -12428.9267578125
Iteration 18500: Loss = -12428.849609375
Iteration 18600: Loss = -12428.849609375
Iteration 18700: Loss = -12428.8486328125
Iteration 18800: Loss = -12428.8466796875
Iteration 18900: Loss = -12428.84375
Iteration 19000: Loss = -12428.83984375
Iteration 19100: Loss = -12428.7998046875
Iteration 19200: Loss = -12428.0625
Iteration 19300: Loss = -12425.96875
Iteration 19400: Loss = -12425.3046875
Iteration 19500: Loss = -12420.92578125
Iteration 19600: Loss = -12420.75
Iteration 19700: Loss = -12420.6787109375
Iteration 19800: Loss = -12420.6552734375
Iteration 19900: Loss = -12420.6455078125
Iteration 20000: Loss = -12420.640625
Iteration 20100: Loss = -12420.6376953125
Iteration 20200: Loss = -12420.634765625
Iteration 20300: Loss = -12420.6337890625
Iteration 20400: Loss = -12420.6337890625
Iteration 20500: Loss = -12420.6318359375
Iteration 20600: Loss = -12420.6298828125
Iteration 20700: Loss = -12420.630859375
1
Iteration 20800: Loss = -12420.6298828125
Iteration 20900: Loss = -12420.630859375
1
Iteration 21000: Loss = -12420.6298828125
Iteration 21100: Loss = -12420.6298828125
Iteration 21200: Loss = -12420.630859375
1
Iteration 21300: Loss = -12420.6298828125
Iteration 21400: Loss = -12420.62890625
Iteration 21500: Loss = -12420.6279296875
Iteration 21600: Loss = -12420.62890625
1
Iteration 21700: Loss = -12420.62890625
2
Iteration 21800: Loss = -12420.6279296875
Iteration 21900: Loss = -12420.6279296875
Iteration 22000: Loss = -12420.626953125
Iteration 22100: Loss = -12420.626953125
Iteration 22200: Loss = -12420.626953125
Iteration 22300: Loss = -12420.626953125
Iteration 22400: Loss = -12420.626953125
Iteration 22500: Loss = -12420.6259765625
Iteration 22600: Loss = -12420.626953125
1
Iteration 22700: Loss = -12420.626953125
2
Iteration 22800: Loss = -12420.6259765625
Iteration 22900: Loss = -12420.6259765625
Iteration 23000: Loss = -12420.6259765625
Iteration 23100: Loss = -12420.6259765625
Iteration 23200: Loss = -12420.626953125
1
Iteration 23300: Loss = -12420.6259765625
Iteration 23400: Loss = -12420.626953125
1
Iteration 23500: Loss = -12420.626953125
2
Iteration 23600: Loss = -12420.626953125
3
Iteration 23700: Loss = -12420.6279296875
4
Iteration 23800: Loss = -12420.626953125
5
Iteration 23900: Loss = -12420.626953125
6
Iteration 24000: Loss = -12420.6259765625
Iteration 24100: Loss = -12420.626953125
1
Iteration 24200: Loss = -12420.6279296875
2
Iteration 24300: Loss = -12420.6259765625
Iteration 24400: Loss = -12420.625
Iteration 24500: Loss = -12420.6259765625
1
Iteration 24600: Loss = -12420.6259765625
2
Iteration 24700: Loss = -12420.6259765625
3
Iteration 24800: Loss = -12420.625
Iteration 24900: Loss = -12420.6259765625
1
Iteration 25000: Loss = -12420.626953125
2
Iteration 25100: Loss = -12420.626953125
3
Iteration 25200: Loss = -12420.626953125
4
Iteration 25300: Loss = -12420.6259765625
5
Iteration 25400: Loss = -12420.626953125
6
Iteration 25500: Loss = -12420.625
Iteration 25600: Loss = -12420.626953125
1
Iteration 25700: Loss = -12420.6259765625
2
Iteration 25800: Loss = -12420.625
Iteration 25900: Loss = -12420.625
Iteration 26000: Loss = -12420.6240234375
Iteration 26100: Loss = -12420.625
1
Iteration 26200: Loss = -12420.625
2
Iteration 26300: Loss = -12420.6240234375
Iteration 26400: Loss = -12420.6259765625
1
Iteration 26500: Loss = -12420.6259765625
2
Iteration 26600: Loss = -12420.625
3
Iteration 26700: Loss = -12420.625
4
Iteration 26800: Loss = -12420.619140625
Iteration 26900: Loss = -12420.6083984375
Iteration 27000: Loss = -12420.6064453125
Iteration 27100: Loss = -12420.6064453125
Iteration 27200: Loss = -12420.6064453125
Iteration 27300: Loss = -12420.609375
1
Iteration 27400: Loss = -12420.6064453125
Iteration 27500: Loss = -12420.607421875
1
Iteration 27600: Loss = -12420.6064453125
Iteration 27700: Loss = -12420.607421875
1
Iteration 27800: Loss = -12420.60546875
Iteration 27900: Loss = -12420.607421875
1
Iteration 28000: Loss = -12420.6064453125
2
Iteration 28100: Loss = -12420.6064453125
3
Iteration 28200: Loss = -12420.6064453125
4
Iteration 28300: Loss = -12420.607421875
5
Iteration 28400: Loss = -12420.607421875
6
Iteration 28500: Loss = -12420.6064453125
7
Iteration 28600: Loss = -12420.6064453125
8
Iteration 28700: Loss = -12420.6064453125
9
Iteration 28800: Loss = -12420.6083984375
10
Iteration 28900: Loss = -12420.6064453125
11
Iteration 29000: Loss = -12420.60546875
Iteration 29100: Loss = -12420.6083984375
1
Iteration 29200: Loss = -12420.60546875
Iteration 29300: Loss = -12420.607421875
1
Iteration 29400: Loss = -12420.6064453125
2
Iteration 29500: Loss = -12420.6064453125
3
Iteration 29600: Loss = -12420.6064453125
4
Iteration 29700: Loss = -12420.6064453125
5
Iteration 29800: Loss = -12420.6064453125
6
Iteration 29900: Loss = -12420.607421875
7
pi: tensor([[0.0097, 0.9903],
        [0.9705, 0.0295]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.2540e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2004, 0.2002],
         [0.1215, 0.2078]],

        [[0.0252, 0.1242],
         [0.9769, 0.8953]],

        [[0.7382, 0.1504],
         [0.2091, 0.8959]],

        [[0.4222, 0.1990],
         [0.7538, 0.8059]],

        [[0.0454, 0.1415],
         [0.0537, 0.0100]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.007431593898976406
time is 3
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.012864505300896736
Global Adjusted Rand Index: -0.0009334548946437693
Average Adjusted Rand Index: 0.004562014524930944
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -44836.03515625
Iteration 100: Loss = -28301.1796875
Iteration 200: Loss = -17553.228515625
Iteration 300: Loss = -13693.771484375
Iteration 400: Loss = -12790.138671875
Iteration 500: Loss = -12580.4326171875
Iteration 600: Loss = -12521.884765625
Iteration 700: Loss = -12481.900390625
Iteration 800: Loss = -12470.8857421875
Iteration 900: Loss = -12463.015625
Iteration 1000: Loss = -12456.9501953125
Iteration 1100: Loss = -12452.5361328125
Iteration 1200: Loss = -12449.0400390625
Iteration 1300: Loss = -12445.9501953125
Iteration 1400: Loss = -12443.154296875
Iteration 1500: Loss = -12440.8564453125
Iteration 1600: Loss = -12434.90234375
Iteration 1700: Loss = -12433.25
Iteration 1800: Loss = -12432.0908203125
Iteration 1900: Loss = -12431.1787109375
Iteration 2000: Loss = -12430.421875
Iteration 2100: Loss = -12429.7783203125
Iteration 2200: Loss = -12429.224609375
Iteration 2300: Loss = -12428.740234375
Iteration 2400: Loss = -12428.318359375
Iteration 2500: Loss = -12427.943359375
Iteration 2600: Loss = -12427.609375
Iteration 2700: Loss = -12427.3095703125
Iteration 2800: Loss = -12427.0380859375
Iteration 2900: Loss = -12426.794921875
Iteration 3000: Loss = -12426.5751953125
Iteration 3100: Loss = -12426.376953125
Iteration 3200: Loss = -12426.1953125
Iteration 3300: Loss = -12426.02734375
Iteration 3400: Loss = -12425.876953125
Iteration 3500: Loss = -12425.7373046875
Iteration 3600: Loss = -12425.609375
Iteration 3700: Loss = -12425.490234375
Iteration 3800: Loss = -12425.3798828125
Iteration 3900: Loss = -12425.279296875
Iteration 4000: Loss = -12425.1845703125
Iteration 4100: Loss = -12425.0986328125
Iteration 4200: Loss = -12425.013671875
Iteration 4300: Loss = -12424.9384765625
Iteration 4400: Loss = -12424.8662109375
Iteration 4500: Loss = -12424.798828125
Iteration 4600: Loss = -12424.7353515625
Iteration 4700: Loss = -12424.6767578125
Iteration 4800: Loss = -12424.62109375
Iteration 4900: Loss = -12424.5712890625
Iteration 5000: Loss = -12424.5234375
Iteration 5100: Loss = -12424.4775390625
Iteration 5200: Loss = -12424.435546875
Iteration 5300: Loss = -12424.3955078125
Iteration 5400: Loss = -12424.357421875
Iteration 5500: Loss = -12424.322265625
Iteration 5600: Loss = -12424.2880859375
Iteration 5700: Loss = -12424.2578125
Iteration 5800: Loss = -12424.2275390625
Iteration 5900: Loss = -12424.1982421875
Iteration 6000: Loss = -12424.1728515625
Iteration 6100: Loss = -12424.146484375
Iteration 6200: Loss = -12424.1240234375
Iteration 6300: Loss = -12424.1015625
Iteration 6400: Loss = -12424.078125
Iteration 6500: Loss = -12424.0595703125
Iteration 6600: Loss = -12424.0380859375
Iteration 6700: Loss = -12424.0205078125
Iteration 6800: Loss = -12424.0048828125
Iteration 6900: Loss = -12423.9873046875
Iteration 7000: Loss = -12423.9716796875
Iteration 7100: Loss = -12423.9560546875
Iteration 7200: Loss = -12423.9404296875
Iteration 7300: Loss = -12423.9287109375
Iteration 7400: Loss = -12423.916015625
Iteration 7500: Loss = -12423.90625
Iteration 7600: Loss = -12423.892578125
Iteration 7700: Loss = -12423.880859375
Iteration 7800: Loss = -12423.869140625
Iteration 7900: Loss = -12423.8603515625
Iteration 8000: Loss = -12423.849609375
Iteration 8100: Loss = -12423.841796875
Iteration 8200: Loss = -12423.8330078125
Iteration 8300: Loss = -12423.82421875
Iteration 8400: Loss = -12423.8134765625
Iteration 8500: Loss = -12423.8056640625
Iteration 8600: Loss = -12423.7978515625
Iteration 8700: Loss = -12423.7880859375
Iteration 8800: Loss = -12423.7783203125
Iteration 8900: Loss = -12423.767578125
Iteration 9000: Loss = -12423.7568359375
Iteration 9100: Loss = -12423.75
Iteration 9200: Loss = -12423.7373046875
Iteration 9300: Loss = -12423.7265625
Iteration 9400: Loss = -12423.716796875
Iteration 9500: Loss = -12423.7080078125
Iteration 9600: Loss = -12423.701171875
Iteration 9700: Loss = -12423.6953125
Iteration 9800: Loss = -12423.6923828125
Iteration 9900: Loss = -12423.689453125
Iteration 10000: Loss = -12423.6884765625
Iteration 10100: Loss = -12423.6865234375
Iteration 10200: Loss = -12423.6826171875
Iteration 10300: Loss = -12423.6806640625
Iteration 10400: Loss = -12423.6767578125
Iteration 10500: Loss = -12423.673828125
Iteration 10600: Loss = -12423.6689453125
Iteration 10700: Loss = -12423.6669921875
Iteration 10800: Loss = -12423.6640625
Iteration 10900: Loss = -12423.662109375
Iteration 11000: Loss = -12423.66015625
Iteration 11100: Loss = -12423.6572265625
Iteration 11200: Loss = -12423.6552734375
Iteration 11300: Loss = -12423.654296875
Iteration 11400: Loss = -12423.65234375
Iteration 11500: Loss = -12423.6484375
Iteration 11600: Loss = -12423.6484375
Iteration 11700: Loss = -12423.64453125
Iteration 11800: Loss = -12423.6396484375
Iteration 11900: Loss = -12423.634765625
Iteration 12000: Loss = -12423.630859375
Iteration 12100: Loss = -12423.6259765625
Iteration 12200: Loss = -12423.6171875
Iteration 12300: Loss = -12423.5986328125
Iteration 12400: Loss = -12422.8798828125
Iteration 12500: Loss = -12422.4404296875
Iteration 12600: Loss = -12422.3271484375
Iteration 12700: Loss = -12422.2666015625
Iteration 12800: Loss = -12422.140625
Iteration 12900: Loss = -12422.1015625
Iteration 13000: Loss = -12422.0830078125
Iteration 13100: Loss = -12422.0703125
Iteration 13200: Loss = -12422.0615234375
Iteration 13300: Loss = -12422.0546875
Iteration 13400: Loss = -12422.048828125
Iteration 13500: Loss = -12422.041015625
Iteration 13600: Loss = -12422.0361328125
Iteration 13700: Loss = -12422.03125
Iteration 13800: Loss = -12422.0283203125
Iteration 13900: Loss = -12422.025390625
Iteration 14000: Loss = -12422.021484375
Iteration 14100: Loss = -12422.017578125
Iteration 14200: Loss = -12422.0166015625
Iteration 14300: Loss = -12422.0146484375
Iteration 14400: Loss = -12422.01171875
Iteration 14500: Loss = -12422.0107421875
Iteration 14600: Loss = -12422.009765625
Iteration 14700: Loss = -12422.0078125
Iteration 14800: Loss = -12422.0068359375
Iteration 14900: Loss = -12422.0068359375
Iteration 15000: Loss = -12422.005859375
Iteration 15100: Loss = -12422.0029296875
Iteration 15200: Loss = -12422.0029296875
Iteration 15300: Loss = -12422.001953125
Iteration 15400: Loss = -12422.0009765625
Iteration 15500: Loss = -12422.001953125
1
Iteration 15600: Loss = -12422.0009765625
Iteration 15700: Loss = -12421.9990234375
Iteration 15800: Loss = -12421.9990234375
Iteration 15900: Loss = -12421.9970703125
Iteration 16000: Loss = -12421.99609375
Iteration 16100: Loss = -12421.998046875
1
Iteration 16200: Loss = -12421.9970703125
2
Iteration 16300: Loss = -12421.9970703125
3
Iteration 16400: Loss = -12421.99609375
Iteration 16500: Loss = -12421.9951171875
Iteration 16600: Loss = -12421.99609375
1
Iteration 16700: Loss = -12421.994140625
Iteration 16800: Loss = -12421.9951171875
1
Iteration 16900: Loss = -12421.9931640625
Iteration 17000: Loss = -12421.994140625
1
Iteration 17100: Loss = -12421.9951171875
2
Iteration 17200: Loss = -12421.9951171875
3
Iteration 17300: Loss = -12421.9921875
Iteration 17400: Loss = -12421.9921875
Iteration 17500: Loss = -12421.9921875
Iteration 17600: Loss = -12421.994140625
1
Iteration 17700: Loss = -12421.9921875
Iteration 17800: Loss = -12421.9931640625
1
Iteration 17900: Loss = -12421.9912109375
Iteration 18000: Loss = -12421.9921875
1
Iteration 18100: Loss = -12421.990234375
Iteration 18200: Loss = -12421.9912109375
1
Iteration 18300: Loss = -12421.9912109375
2
Iteration 18400: Loss = -12421.9912109375
3
Iteration 18500: Loss = -12421.9921875
4
Iteration 18600: Loss = -12421.9921875
5
Iteration 18700: Loss = -12421.9892578125
Iteration 18800: Loss = -12421.9873046875
Iteration 18900: Loss = -12421.9873046875
Iteration 19000: Loss = -12421.98828125
1
Iteration 19100: Loss = -12421.98828125
2
Iteration 19200: Loss = -12421.9873046875
Iteration 19300: Loss = -12421.9873046875
Iteration 19400: Loss = -12421.9873046875
Iteration 19500: Loss = -12421.9873046875
Iteration 19600: Loss = -12421.98828125
1
Iteration 19700: Loss = -12421.98828125
2
Iteration 19800: Loss = -12421.9873046875
Iteration 19900: Loss = -12421.9873046875
Iteration 20000: Loss = -12421.986328125
Iteration 20100: Loss = -12421.986328125
Iteration 20200: Loss = -12421.9853515625
Iteration 20300: Loss = -12421.9892578125
1
Iteration 20400: Loss = -12421.986328125
2
Iteration 20500: Loss = -12421.9853515625
Iteration 20600: Loss = -12421.9853515625
Iteration 20700: Loss = -12421.9853515625
Iteration 20800: Loss = -12421.9873046875
1
Iteration 20900: Loss = -12421.986328125
2
Iteration 21000: Loss = -12421.9873046875
3
Iteration 21100: Loss = -12421.986328125
4
Iteration 21200: Loss = -12421.9873046875
5
Iteration 21300: Loss = -12421.9873046875
6
Iteration 21400: Loss = -12421.986328125
7
Iteration 21500: Loss = -12421.9853515625
Iteration 21600: Loss = -12421.984375
Iteration 21700: Loss = -12421.9833984375
Iteration 21800: Loss = -12421.9853515625
1
Iteration 21900: Loss = -12421.986328125
2
Iteration 22000: Loss = -12421.9853515625
3
Iteration 22100: Loss = -12421.984375
4
Iteration 22200: Loss = -12421.984375
5
Iteration 22300: Loss = -12421.9833984375
Iteration 22400: Loss = -12421.984375
1
Iteration 22500: Loss = -12421.9853515625
2
Iteration 22600: Loss = -12421.9833984375
Iteration 22700: Loss = -12421.984375
1
Iteration 22800: Loss = -12421.9853515625
2
Iteration 22900: Loss = -12421.9853515625
3
Iteration 23000: Loss = -12421.9853515625
4
Iteration 23100: Loss = -12421.9833984375
Iteration 23200: Loss = -12421.984375
1
Iteration 23300: Loss = -12421.9853515625
2
Iteration 23400: Loss = -12421.9853515625
3
Iteration 23500: Loss = -12421.984375
4
Iteration 23600: Loss = -12421.984375
5
Iteration 23700: Loss = -12421.984375
6
Iteration 23800: Loss = -12421.984375
7
Iteration 23900: Loss = -12421.984375
8
Iteration 24000: Loss = -12421.986328125
9
Iteration 24100: Loss = -12421.9853515625
10
Iteration 24200: Loss = -12421.984375
11
Iteration 24300: Loss = -12421.984375
12
Iteration 24400: Loss = -12421.9853515625
13
Iteration 24500: Loss = -12421.9853515625
14
Iteration 24600: Loss = -12421.9853515625
15
Stopping early at iteration 24600 due to no improvement.
pi: tensor([[9.9982e-01, 1.8283e-04],
        [6.5934e-03, 9.9341e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([2.2452e-06, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.9072, 0.2008],
         [0.5880, 0.1996]],

        [[0.3208, 0.1958],
         [0.9306, 0.5545]],

        [[0.5157, 0.2191],
         [0.3703, 0.0151]],

        [[0.0297, 0.2633],
         [0.9879, 0.0920]],

        [[0.8028, 0.2898],
         [0.0070, 0.9913]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
Global Adjusted Rand Index: 0.0007683893919480408
Average Adjusted Rand Index: 0.00012381852885091918
[-0.0009334548946437693, 0.0007683893919480408] [0.004562014524930944, 0.00012381852885091918] [12420.607421875, 12421.9853515625]
-------------------------------------
This iteration is 28
True Objective function: Loss = -11547.594581647678
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -14410.8134765625
Iteration 100: Loss = -12678.7109375
Iteration 200: Loss = -12154.9755859375
Iteration 300: Loss = -11987.8681640625
Iteration 400: Loss = -11757.66796875
Iteration 500: Loss = -11660.396484375
Iteration 600: Loss = -11583.5869140625
Iteration 700: Loss = -11554.6162109375
Iteration 800: Loss = -11547.642578125
Iteration 900: Loss = -11544.365234375
Iteration 1000: Loss = -11542.4248046875
Iteration 1100: Loss = -11541.142578125
Iteration 1200: Loss = -11540.2333984375
Iteration 1300: Loss = -11539.5595703125
Iteration 1400: Loss = -11539.0390625
Iteration 1500: Loss = -11538.630859375
Iteration 1600: Loss = -11538.30078125
Iteration 1700: Loss = -11538.029296875
Iteration 1800: Loss = -11537.8046875
Iteration 1900: Loss = -11537.6142578125
Iteration 2000: Loss = -11537.453125
Iteration 2100: Loss = -11537.3125
Iteration 2200: Loss = -11537.1923828125
Iteration 2300: Loss = -11537.0869140625
Iteration 2400: Loss = -11536.994140625
Iteration 2500: Loss = -11536.9130859375
Iteration 2600: Loss = -11536.8408203125
Iteration 2700: Loss = -11536.7744140625
Iteration 2800: Loss = -11536.716796875
Iteration 2900: Loss = -11536.6640625
Iteration 3000: Loss = -11536.6181640625
Iteration 3100: Loss = -11536.5751953125
Iteration 3200: Loss = -11536.537109375
Iteration 3300: Loss = -11536.501953125
Iteration 3400: Loss = -11536.46875
Iteration 3500: Loss = -11536.4384765625
Iteration 3600: Loss = -11536.412109375
Iteration 3700: Loss = -11536.3857421875
Iteration 3800: Loss = -11536.3642578125
Iteration 3900: Loss = -11536.3427734375
Iteration 4000: Loss = -11536.3232421875
Iteration 4100: Loss = -11536.3056640625
Iteration 4200: Loss = -11536.2900390625
Iteration 4300: Loss = -11536.2734375
Iteration 4400: Loss = -11536.259765625
Iteration 4500: Loss = -11536.24609375
Iteration 4600: Loss = -11536.2333984375
Iteration 4700: Loss = -11536.22265625
Iteration 4800: Loss = -11536.2099609375
Iteration 4900: Loss = -11536.201171875
Iteration 5000: Loss = -11536.19140625
Iteration 5100: Loss = -11536.1826171875
Iteration 5200: Loss = -11536.173828125
Iteration 5300: Loss = -11536.166015625
Iteration 5400: Loss = -11536.1591796875
Iteration 5500: Loss = -11536.15234375
Iteration 5600: Loss = -11536.146484375
Iteration 5700: Loss = -11536.1396484375
Iteration 5800: Loss = -11536.1337890625
Iteration 5900: Loss = -11536.12890625
Iteration 6000: Loss = -11536.123046875
Iteration 6100: Loss = -11536.1181640625
Iteration 6200: Loss = -11536.1142578125
Iteration 6300: Loss = -11536.111328125
Iteration 6400: Loss = -11536.107421875
Iteration 6500: Loss = -11536.1025390625
Iteration 6600: Loss = -11536.099609375
Iteration 6700: Loss = -11536.0966796875
Iteration 6800: Loss = -11536.0927734375
Iteration 6900: Loss = -11536.08984375
Iteration 7000: Loss = -11536.0859375
Iteration 7100: Loss = -11536.083984375
Iteration 7200: Loss = -11536.08203125
Iteration 7300: Loss = -11536.080078125
Iteration 7400: Loss = -11536.078125
Iteration 7500: Loss = -11536.0751953125
Iteration 7600: Loss = -11536.0732421875
Iteration 7700: Loss = -11536.072265625
Iteration 7800: Loss = -11536.0703125
Iteration 7900: Loss = -11536.0673828125
Iteration 8000: Loss = -11536.0673828125
Iteration 8100: Loss = -11536.0654296875
Iteration 8200: Loss = -11536.064453125
Iteration 8300: Loss = -11536.0634765625
Iteration 8400: Loss = -11536.0625
Iteration 8500: Loss = -11536.0595703125
Iteration 8600: Loss = -11536.0595703125
Iteration 8700: Loss = -11536.0576171875
Iteration 8800: Loss = -11536.0576171875
Iteration 8900: Loss = -11536.0556640625
Iteration 9000: Loss = -11536.0546875
Iteration 9100: Loss = -11536.0546875
Iteration 9200: Loss = -11536.0517578125
Iteration 9300: Loss = -11536.0537109375
1
Iteration 9400: Loss = -11536.0517578125
Iteration 9500: Loss = -11536.0517578125
Iteration 9600: Loss = -11536.05078125
Iteration 9700: Loss = -11536.048828125
Iteration 9800: Loss = -11536.048828125
Iteration 9900: Loss = -11536.048828125
Iteration 10000: Loss = -11536.0478515625
Iteration 10100: Loss = -11536.0478515625
Iteration 10200: Loss = -11536.0478515625
Iteration 10300: Loss = -11536.044921875
Iteration 10400: Loss = -11536.0458984375
1
Iteration 10500: Loss = -11536.044921875
Iteration 10600: Loss = -11536.044921875
Iteration 10700: Loss = -11536.0458984375
1
Iteration 10800: Loss = -11536.0439453125
Iteration 10900: Loss = -11536.044921875
1
Iteration 11000: Loss = -11536.0439453125
Iteration 11100: Loss = -11536.0439453125
Iteration 11200: Loss = -11536.04296875
Iteration 11300: Loss = -11536.0439453125
1
Iteration 11400: Loss = -11536.0419921875
Iteration 11500: Loss = -11536.04296875
1
Iteration 11600: Loss = -11536.041015625
Iteration 11700: Loss = -11536.0419921875
1
Iteration 11800: Loss = -11536.04296875
2
Iteration 11900: Loss = -11536.041015625
Iteration 12000: Loss = -11536.0419921875
1
Iteration 12100: Loss = -11536.041015625
Iteration 12200: Loss = -11536.0400390625
Iteration 12300: Loss = -11536.0400390625
Iteration 12400: Loss = -11536.0400390625
Iteration 12500: Loss = -11536.0390625
Iteration 12600: Loss = -11536.0390625
Iteration 12700: Loss = -11536.0400390625
1
Iteration 12800: Loss = -11536.0400390625
2
Iteration 12900: Loss = -11536.0390625
Iteration 13000: Loss = -11536.0400390625
1
Iteration 13100: Loss = -11536.0390625
Iteration 13200: Loss = -11536.0390625
Iteration 13300: Loss = -11536.0380859375
Iteration 13400: Loss = -11536.0400390625
1
Iteration 13500: Loss = -11536.0390625
2
Iteration 13600: Loss = -11536.0390625
3
Iteration 13700: Loss = -11536.0390625
4
Iteration 13800: Loss = -11536.0390625
5
Iteration 13900: Loss = -11536.0400390625
6
Iteration 14000: Loss = -11536.0380859375
Iteration 14100: Loss = -11536.0380859375
Iteration 14200: Loss = -11536.0380859375
Iteration 14300: Loss = -11536.0390625
1
Iteration 14400: Loss = -11536.0380859375
Iteration 14500: Loss = -11536.0380859375
Iteration 14600: Loss = -11536.0390625
1
Iteration 14700: Loss = -11536.0380859375
Iteration 14800: Loss = -11536.0390625
1
Iteration 14900: Loss = -11536.0361328125
Iteration 15000: Loss = -11536.0380859375
1
Iteration 15100: Loss = -11536.037109375
2
Iteration 15200: Loss = -11536.037109375
3
Iteration 15300: Loss = -11536.0380859375
4
Iteration 15400: Loss = -11536.0380859375
5
Iteration 15500: Loss = -11536.0380859375
6
Iteration 15600: Loss = -11536.0380859375
7
Iteration 15700: Loss = -11536.0390625
8
Iteration 15800: Loss = -11536.037109375
9
Iteration 15900: Loss = -11536.0380859375
10
Iteration 16000: Loss = -11536.037109375
11
Iteration 16100: Loss = -11536.0380859375
12
Iteration 16200: Loss = -11536.0380859375
13
Iteration 16300: Loss = -11536.037109375
14
Iteration 16400: Loss = -11536.037109375
15
Stopping early at iteration 16400 due to no improvement.
pi: tensor([[0.7668, 0.2332],
        [0.2386, 0.7614]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4603, 0.5397], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3012, 0.0919],
         [0.0152, 0.2901]],

        [[0.9866, 0.0871],
         [0.9108, 0.0916]],

        [[0.9427, 0.0840],
         [0.2743, 0.9909]],

        [[0.9692, 0.1005],
         [0.0486, 0.9676]],

        [[0.0171, 0.0909],
         [0.9587, 0.0079]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17947.537109375
Iteration 100: Loss = -14068.25390625
Iteration 200: Loss = -12558.689453125
Iteration 300: Loss = -12193.0390625
Iteration 400: Loss = -12025.931640625
Iteration 500: Loss = -11762.138671875
Iteration 600: Loss = -11629.849609375
Iteration 700: Loss = -11594.740234375
Iteration 800: Loss = -11585.109375
Iteration 900: Loss = -11580.990234375
Iteration 1000: Loss = -11578.5849609375
Iteration 1100: Loss = -11577.0029296875
Iteration 1200: Loss = -11575.8828125
Iteration 1300: Loss = -11575.0537109375
Iteration 1400: Loss = -11574.41796875
Iteration 1500: Loss = -11573.9169921875
Iteration 1600: Loss = -11573.515625
Iteration 1700: Loss = -11573.185546875
Iteration 1800: Loss = -11572.91015625
Iteration 1900: Loss = -11572.6806640625
Iteration 2000: Loss = -11572.4833984375
Iteration 2100: Loss = -11572.3154296875
Iteration 2200: Loss = -11572.169921875
Iteration 2300: Loss = -11572.0419921875
Iteration 2400: Loss = -11571.931640625
Iteration 2500: Loss = -11571.8310546875
Iteration 2600: Loss = -11571.7451171875
Iteration 2700: Loss = -11571.666015625
Iteration 2800: Loss = -11571.5966796875
Iteration 2900: Loss = -11571.5341796875
Iteration 3000: Loss = -11571.4775390625
Iteration 3100: Loss = -11571.4248046875
Iteration 3200: Loss = -11571.37890625
Iteration 3300: Loss = -11571.3369140625
Iteration 3400: Loss = -11571.296875
Iteration 3500: Loss = -11571.26171875
Iteration 3600: Loss = -11571.228515625
Iteration 3700: Loss = -11571.2001953125
Iteration 3800: Loss = -11571.1728515625
Iteration 3900: Loss = -11571.146484375
Iteration 4000: Loss = -11571.123046875
Iteration 4100: Loss = -11571.1015625
Iteration 4200: Loss = -11571.08203125
Iteration 4300: Loss = -11571.0615234375
Iteration 4400: Loss = -11571.044921875
Iteration 4500: Loss = -11571.029296875
Iteration 4600: Loss = -11571.0146484375
Iteration 4700: Loss = -11571.0009765625
Iteration 4800: Loss = -11570.9873046875
Iteration 4900: Loss = -11570.9765625
Iteration 5000: Loss = -11570.9638671875
Iteration 5100: Loss = -11570.953125
Iteration 5200: Loss = -11570.9443359375
Iteration 5300: Loss = -11570.9345703125
Iteration 5400: Loss = -11570.9267578125
Iteration 5500: Loss = -11570.91796875
Iteration 5600: Loss = -11570.91015625
Iteration 5700: Loss = -11570.904296875
Iteration 5800: Loss = -11570.8974609375
Iteration 5900: Loss = -11570.8896484375
Iteration 6000: Loss = -11570.884765625
Iteration 6100: Loss = -11570.87890625
Iteration 6200: Loss = -11570.873046875
Iteration 6300: Loss = -11570.8681640625
Iteration 6400: Loss = -11570.8642578125
Iteration 6500: Loss = -11570.859375
Iteration 6600: Loss = -11570.85546875
Iteration 6700: Loss = -11570.8515625
Iteration 6800: Loss = -11570.84765625
Iteration 6900: Loss = -11570.8447265625
Iteration 7000: Loss = -11570.83984375
Iteration 7100: Loss = -11570.837890625
Iteration 7200: Loss = -11570.8359375
Iteration 7300: Loss = -11570.83203125
Iteration 7400: Loss = -11570.8291015625
Iteration 7500: Loss = -11570.826171875
Iteration 7600: Loss = -11570.826171875
Iteration 7700: Loss = -11570.8232421875
Iteration 7800: Loss = -11570.8193359375
Iteration 7900: Loss = -11570.8173828125
Iteration 8000: Loss = -11570.81640625
Iteration 8100: Loss = -11570.8154296875
Iteration 8200: Loss = -11570.814453125
Iteration 8300: Loss = -11570.8115234375
Iteration 8400: Loss = -11570.810546875
Iteration 8500: Loss = -11570.80859375
Iteration 8600: Loss = -11570.80859375
Iteration 8700: Loss = -11570.8056640625
Iteration 8800: Loss = -11570.8046875
Iteration 8900: Loss = -11570.8046875
Iteration 9000: Loss = -11570.8037109375
Iteration 9100: Loss = -11570.802734375
Iteration 9200: Loss = -11570.80078125
Iteration 9300: Loss = -11570.7998046875
Iteration 9400: Loss = -11570.798828125
Iteration 9500: Loss = -11570.798828125
Iteration 9600: Loss = -11570.7978515625
Iteration 9700: Loss = -11570.796875
Iteration 9800: Loss = -11570.7958984375
Iteration 9900: Loss = -11570.796875
1
Iteration 10000: Loss = -11570.7958984375
Iteration 10100: Loss = -11570.794921875
Iteration 10200: Loss = -11570.7939453125
Iteration 10300: Loss = -11570.7939453125
Iteration 10400: Loss = -11570.79296875
Iteration 10500: Loss = -11570.79296875
Iteration 10600: Loss = -11570.7919921875
Iteration 10700: Loss = -11570.7919921875
Iteration 10800: Loss = -11570.791015625
Iteration 10900: Loss = -11570.791015625
Iteration 11000: Loss = -11570.7900390625
Iteration 11100: Loss = -11570.791015625
1
Iteration 11200: Loss = -11570.7890625
Iteration 11300: Loss = -11570.7890625
Iteration 11400: Loss = -11570.7890625
Iteration 11500: Loss = -11570.7900390625
1
Iteration 11600: Loss = -11570.7890625
Iteration 11700: Loss = -11570.7890625
Iteration 11800: Loss = -11570.787109375
Iteration 11900: Loss = -11570.7890625
1
Iteration 12000: Loss = -11570.787109375
Iteration 12100: Loss = -11570.787109375
Iteration 12200: Loss = -11570.787109375
Iteration 12300: Loss = -11570.787109375
Iteration 12400: Loss = -11570.787109375
Iteration 12500: Loss = -11570.78515625
Iteration 12600: Loss = -11570.7861328125
1
Iteration 12700: Loss = -11570.78515625
Iteration 12800: Loss = -11570.78515625
Iteration 12900: Loss = -11570.78515625
Iteration 13000: Loss = -11570.78515625
Iteration 13100: Loss = -11570.7861328125
1
Iteration 13200: Loss = -11570.7861328125
2
Iteration 13300: Loss = -11570.78515625
Iteration 13400: Loss = -11570.78515625
Iteration 13500: Loss = -11570.7841796875
Iteration 13600: Loss = -11570.7841796875
Iteration 13700: Loss = -11570.7841796875
Iteration 13800: Loss = -11570.783203125
Iteration 13900: Loss = -11570.78515625
1
Iteration 14000: Loss = -11570.78515625
2
Iteration 14100: Loss = -11570.783203125
Iteration 14200: Loss = -11570.783203125
Iteration 14300: Loss = -11570.783203125
Iteration 14400: Loss = -11570.7841796875
1
Iteration 14500: Loss = -11570.7841796875
2
Iteration 14600: Loss = -11570.7841796875
3
Iteration 14700: Loss = -11570.783203125
Iteration 14800: Loss = -11570.783203125
Iteration 14900: Loss = -11570.7841796875
1
Iteration 15000: Loss = -11570.783203125
Iteration 15100: Loss = -11570.7841796875
1
Iteration 15200: Loss = -11570.7841796875
2
Iteration 15300: Loss = -11570.783203125
Iteration 15400: Loss = -11570.7841796875
1
Iteration 15500: Loss = -11570.7841796875
2
Iteration 15600: Loss = -11570.7841796875
3
Iteration 15700: Loss = -11570.783203125
Iteration 15800: Loss = -11570.7841796875
1
Iteration 15900: Loss = -11570.7841796875
2
Iteration 16000: Loss = -11570.7841796875
3
Iteration 16100: Loss = -11570.7841796875
4
Iteration 16200: Loss = -11570.783203125
Iteration 16300: Loss = -11570.7822265625
Iteration 16400: Loss = -11570.783203125
1
Iteration 16500: Loss = -11570.7822265625
Iteration 16600: Loss = -11570.783203125
1
Iteration 16700: Loss = -11570.783203125
2
Iteration 16800: Loss = -11570.783203125
3
Iteration 16900: Loss = -11570.7822265625
Iteration 17000: Loss = -11570.7822265625
Iteration 17100: Loss = -11570.78515625
1
Iteration 17200: Loss = -11570.7822265625
Iteration 17300: Loss = -11570.783203125
1
Iteration 17400: Loss = -11570.78515625
2
Iteration 17500: Loss = -11570.7822265625
Iteration 17600: Loss = -11570.7822265625
Iteration 17700: Loss = -11570.783203125
1
Iteration 17800: Loss = -11570.783203125
2
Iteration 17900: Loss = -11570.7822265625
Iteration 18000: Loss = -11570.7822265625
Iteration 18100: Loss = -11570.783203125
1
Iteration 18200: Loss = -11570.783203125
2
Iteration 18300: Loss = -11570.783203125
3
Iteration 18400: Loss = -11570.7822265625
Iteration 18500: Loss = -11570.7841796875
1
Iteration 18600: Loss = -11570.78125
Iteration 18700: Loss = -11570.7822265625
1
Iteration 18800: Loss = -11570.7822265625
2
Iteration 18900: Loss = -11570.7822265625
3
Iteration 19000: Loss = -11570.783203125
4
Iteration 19100: Loss = -11570.783203125
5
Iteration 19200: Loss = -11570.7822265625
6
Iteration 19300: Loss = -11570.783203125
7
Iteration 19400: Loss = -11570.7822265625
8
Iteration 19500: Loss = -11570.783203125
9
Iteration 19600: Loss = -11570.7841796875
10
Iteration 19700: Loss = -11570.7822265625
11
Iteration 19800: Loss = -11570.7822265625
12
Iteration 19900: Loss = -11570.783203125
13
Iteration 20000: Loss = -11570.7822265625
14
Iteration 20100: Loss = -11570.7822265625
15
Stopping early at iteration 20100 due to no improvement.
pi: tensor([[0.3272, 0.6728],
        [0.6701, 0.3299]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5398, 0.4602], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2936, 0.0918],
         [0.0210, 0.2970]],

        [[0.9291, 0.0871],
         [0.4810, 0.2114]],

        [[0.4238, 0.0838],
         [0.0374, 0.0345]],

        [[0.9811, 0.1007],
         [0.9913, 0.8193]],

        [[0.1087, 0.0909],
         [0.9567, 0.3533]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03648670057561497
Average Adjusted Rand Index: 0.9919998119331364
[1.0, 0.03648670057561497] [1.0, 0.9919998119331364] [11536.037109375, 11570.7822265625]
-------------------------------------
This iteration is 29
True Objective function: Loss = -11905.383438909359
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21229.412109375
Iteration 100: Loss = -15191.107421875
Iteration 200: Loss = -12994.66796875
Iteration 300: Loss = -12613.2060546875
Iteration 400: Loss = -12518.92578125
Iteration 500: Loss = -12482.25
Iteration 600: Loss = -12457.279296875
Iteration 700: Loss = -12442.9208984375
Iteration 800: Loss = -12428.646484375
Iteration 900: Loss = -12414.107421875
Iteration 1000: Loss = -12398.041015625
Iteration 1100: Loss = -12384.0546875
Iteration 1200: Loss = -12374.9814453125
Iteration 1300: Loss = -12365.4853515625
Iteration 1400: Loss = -12357.6083984375
Iteration 1500: Loss = -12345.9482421875
Iteration 1600: Loss = -12330.662109375
Iteration 1700: Loss = -12317.37890625
Iteration 1800: Loss = -12283.380859375
Iteration 1900: Loss = -12266.728515625
Iteration 2000: Loss = -12249.8720703125
Iteration 2100: Loss = -12240.5986328125
Iteration 2200: Loss = -12229.3642578125
Iteration 2300: Loss = -12200.685546875
Iteration 2400: Loss = -12192.9462890625
Iteration 2500: Loss = -12175.9599609375
Iteration 2600: Loss = -12174.5107421875
Iteration 2700: Loss = -12173.9033203125
Iteration 2800: Loss = -12173.4638671875
Iteration 2900: Loss = -12173.07421875
Iteration 3000: Loss = -12172.212890625
Iteration 3100: Loss = -12170.9033203125
Iteration 3200: Loss = -12169.84765625
Iteration 3300: Loss = -12158.2783203125
Iteration 3400: Loss = -12156.015625
Iteration 3500: Loss = -12155.7646484375
Iteration 3600: Loss = -12155.603515625
Iteration 3700: Loss = -12155.4814453125
Iteration 3800: Loss = -12155.380859375
Iteration 3900: Loss = -12155.294921875
Iteration 4000: Loss = -12155.2216796875
Iteration 4100: Loss = -12155.15625
Iteration 4200: Loss = -12155.095703125
Iteration 4300: Loss = -12155.04296875
Iteration 4400: Loss = -12154.99609375
Iteration 4500: Loss = -12154.9521484375
Iteration 4600: Loss = -12154.9091796875
Iteration 4700: Loss = -12154.8740234375
Iteration 4800: Loss = -12154.8408203125
Iteration 4900: Loss = -12154.8115234375
Iteration 5000: Loss = -12154.7822265625
Iteration 5100: Loss = -12154.7568359375
Iteration 5200: Loss = -12154.732421875
Iteration 5300: Loss = -12154.7099609375
Iteration 5400: Loss = -12154.6884765625
Iteration 5500: Loss = -12154.669921875
Iteration 5600: Loss = -12154.65234375
Iteration 5700: Loss = -12154.6357421875
Iteration 5800: Loss = -12154.6201171875
Iteration 5900: Loss = -12154.60546875
Iteration 6000: Loss = -12154.591796875
Iteration 6100: Loss = -12154.578125
Iteration 6200: Loss = -12154.56640625
Iteration 6300: Loss = -12154.5556640625
Iteration 6400: Loss = -12154.5458984375
Iteration 6500: Loss = -12154.53515625
Iteration 6600: Loss = -12154.525390625
Iteration 6700: Loss = -12154.517578125
Iteration 6800: Loss = -12154.5087890625
Iteration 6900: Loss = -12154.5
Iteration 7000: Loss = -12154.494140625
Iteration 7100: Loss = -12154.4853515625
Iteration 7200: Loss = -12154.478515625
Iteration 7300: Loss = -12154.47265625
Iteration 7400: Loss = -12154.466796875
Iteration 7500: Loss = -12154.4609375
Iteration 7600: Loss = -12154.4560546875
Iteration 7700: Loss = -12154.4501953125
Iteration 7800: Loss = -12154.4462890625
Iteration 7900: Loss = -12154.44140625
Iteration 8000: Loss = -12154.4365234375
Iteration 8100: Loss = -12154.43359375
Iteration 8200: Loss = -12154.4306640625
Iteration 8300: Loss = -12154.42578125
Iteration 8400: Loss = -12154.4228515625
Iteration 8500: Loss = -12154.419921875
Iteration 8600: Loss = -12154.416015625
Iteration 8700: Loss = -12154.4140625
Iteration 8800: Loss = -12154.4111328125
Iteration 8900: Loss = -12154.408203125
Iteration 9000: Loss = -12154.40625
Iteration 9100: Loss = -12154.4033203125
Iteration 9200: Loss = -12154.40234375
Iteration 9300: Loss = -12154.400390625
Iteration 9400: Loss = -12154.3974609375
Iteration 9500: Loss = -12154.3974609375
Iteration 9600: Loss = -12154.39453125
Iteration 9700: Loss = -12154.3935546875
Iteration 9800: Loss = -12154.3916015625
Iteration 9900: Loss = -12154.3896484375
Iteration 10000: Loss = -12154.388671875
Iteration 10100: Loss = -12154.3876953125
Iteration 10200: Loss = -12154.38671875
Iteration 10300: Loss = -12154.384765625
Iteration 10400: Loss = -12154.3837890625
Iteration 10500: Loss = -12154.3818359375
Iteration 10600: Loss = -12154.380859375
Iteration 10700: Loss = -12154.37890625
Iteration 10800: Loss = -12154.3779296875
Iteration 10900: Loss = -12154.375
Iteration 11000: Loss = -12154.37109375
Iteration 11100: Loss = -12154.3525390625
Iteration 11200: Loss = -12153.3544921875
Iteration 11300: Loss = -12152.1640625
Iteration 11400: Loss = -12150.87109375
Iteration 11500: Loss = -12150.0400390625
Iteration 11600: Loss = -12148.89453125
Iteration 11700: Loss = -12148.5634765625
Iteration 11800: Loss = -12148.0234375
Iteration 11900: Loss = -12146.6591796875
Iteration 12000: Loss = -12144.6689453125
Iteration 12100: Loss = -12142.4384765625
Iteration 12200: Loss = -12140.2744140625
Iteration 12300: Loss = -12136.4248046875
Iteration 12400: Loss = -12127.0322265625
Iteration 12500: Loss = -12109.7490234375
Iteration 12600: Loss = -12101.3515625
Iteration 12700: Loss = -12053.2763671875
Iteration 12800: Loss = -12007.9169921875
Iteration 12900: Loss = -11973.7041015625
Iteration 13000: Loss = -11956.6064453125
Iteration 13100: Loss = -11947.2734375
Iteration 13200: Loss = -11939.1015625
Iteration 13300: Loss = -11938.8701171875
Iteration 13400: Loss = -11938.744140625
Iteration 13500: Loss = -11938.6669921875
Iteration 13600: Loss = -11938.6142578125
Iteration 13700: Loss = -11938.5732421875
Iteration 13800: Loss = -11938.5419921875
Iteration 13900: Loss = -11938.5146484375
Iteration 14000: Loss = -11938.48828125
Iteration 14100: Loss = -11938.453125
Iteration 14200: Loss = -11938.439453125
Iteration 14300: Loss = -11938.42578125
Iteration 14400: Loss = -11938.4140625
Iteration 14500: Loss = -11938.4052734375
Iteration 14600: Loss = -11938.396484375
Iteration 14700: Loss = -11938.3896484375
Iteration 14800: Loss = -11938.3828125
Iteration 14900: Loss = -11938.3779296875
Iteration 15000: Loss = -11938.37109375
Iteration 15100: Loss = -11938.3681640625
Iteration 15200: Loss = -11938.36328125
Iteration 15300: Loss = -11938.359375
Iteration 15400: Loss = -11938.3564453125
Iteration 15500: Loss = -11938.353515625
Iteration 15600: Loss = -11938.3505859375
Iteration 15700: Loss = -11938.34765625
Iteration 15800: Loss = -11938.345703125
Iteration 15900: Loss = -11938.3427734375
Iteration 16000: Loss = -11938.3408203125
Iteration 16100: Loss = -11938.33984375
Iteration 16200: Loss = -11938.337890625
Iteration 16300: Loss = -11938.3349609375
Iteration 16400: Loss = -11938.333984375
Iteration 16500: Loss = -11938.3330078125
Iteration 16600: Loss = -11938.3330078125
Iteration 16700: Loss = -11938.330078125
Iteration 16800: Loss = -11938.3291015625
Iteration 16900: Loss = -11938.328125
Iteration 17000: Loss = -11938.328125
Iteration 17100: Loss = -11938.326171875
Iteration 17200: Loss = -11938.326171875
Iteration 17300: Loss = -11938.3251953125
Iteration 17400: Loss = -11938.32421875
Iteration 17500: Loss = -11938.32421875
Iteration 17600: Loss = -11938.3232421875
Iteration 17700: Loss = -11938.3232421875
Iteration 17800: Loss = -11938.3212890625
Iteration 17900: Loss = -11938.3212890625
Iteration 18000: Loss = -11938.3203125
Iteration 18100: Loss = -11938.318359375
Iteration 18200: Loss = -11938.3203125
1
Iteration 18300: Loss = -11938.3193359375
2
Iteration 18400: Loss = -11938.3193359375
3
Iteration 18500: Loss = -11938.318359375
Iteration 18600: Loss = -11938.318359375
Iteration 18700: Loss = -11938.3173828125
Iteration 18800: Loss = -11938.31640625
Iteration 18900: Loss = -11938.31640625
Iteration 19000: Loss = -11938.3173828125
1
Iteration 19100: Loss = -11938.318359375
2
Iteration 19200: Loss = -11938.3173828125
3
Iteration 19300: Loss = -11938.3154296875
Iteration 19400: Loss = -11938.3154296875
Iteration 19500: Loss = -11938.31640625
1
Iteration 19600: Loss = -11938.314453125
Iteration 19700: Loss = -11938.3154296875
1
Iteration 19800: Loss = -11938.31640625
2
Iteration 19900: Loss = -11938.3154296875
3
Iteration 20000: Loss = -11938.3154296875
4
Iteration 20100: Loss = -11938.3154296875
5
Iteration 20200: Loss = -11938.314453125
Iteration 20300: Loss = -11938.3154296875
1
Iteration 20400: Loss = -11938.314453125
Iteration 20500: Loss = -11938.3154296875
1
Iteration 20600: Loss = -11938.314453125
Iteration 20700: Loss = -11938.3134765625
Iteration 20800: Loss = -11938.3134765625
Iteration 20900: Loss = -11938.314453125
1
Iteration 21000: Loss = -11938.3134765625
Iteration 21100: Loss = -11938.314453125
1
Iteration 21200: Loss = -11938.3154296875
2
Iteration 21300: Loss = -11938.3134765625
Iteration 21400: Loss = -11938.3134765625
Iteration 21500: Loss = -11938.3134765625
Iteration 21600: Loss = -11938.3154296875
1
Iteration 21700: Loss = -11938.314453125
2
Iteration 21800: Loss = -11938.3134765625
Iteration 21900: Loss = -11938.3134765625
Iteration 22000: Loss = -11938.3134765625
Iteration 22100: Loss = -11938.314453125
1
Iteration 22200: Loss = -11938.3125
Iteration 22300: Loss = -11938.3134765625
1
Iteration 22400: Loss = -11938.31640625
2
Iteration 22500: Loss = -11938.314453125
3
Iteration 22600: Loss = -11938.3125
Iteration 22700: Loss = -11938.314453125
1
Iteration 22800: Loss = -11938.3125
Iteration 22900: Loss = -11938.3134765625
1
Iteration 23000: Loss = -11938.3134765625
2
Iteration 23100: Loss = -11938.3125
Iteration 23200: Loss = -11938.3125
Iteration 23300: Loss = -11938.3134765625
1
Iteration 23400: Loss = -11938.3125
Iteration 23500: Loss = -11938.3134765625
1
Iteration 23600: Loss = -11938.3125
Iteration 23700: Loss = -11938.3125
Iteration 23800: Loss = -11938.3125
Iteration 23900: Loss = -11938.314453125
1
Iteration 24000: Loss = -11938.3125
Iteration 24100: Loss = -11938.3134765625
1
Iteration 24200: Loss = -11938.3115234375
Iteration 24300: Loss = -11938.3125
1
Iteration 24400: Loss = -11938.3115234375
Iteration 24500: Loss = -11938.3125
1
Iteration 24600: Loss = -11938.3134765625
2
Iteration 24700: Loss = -11938.3134765625
3
Iteration 24800: Loss = -11938.3125
4
Iteration 24900: Loss = -11938.3125
5
Iteration 25000: Loss = -11938.310546875
Iteration 25100: Loss = -11938.3125
1
Iteration 25200: Loss = -11938.3125
2
Iteration 25300: Loss = -11938.310546875
Iteration 25400: Loss = -11938.3125
1
Iteration 25500: Loss = -11938.3115234375
2
Iteration 25600: Loss = -11938.3125
3
Iteration 25700: Loss = -11938.3125
4
Iteration 25800: Loss = -11938.3115234375
5
Iteration 25900: Loss = -11938.3125
6
Iteration 26000: Loss = -11938.314453125
7
Iteration 26100: Loss = -11938.3115234375
8
Iteration 26200: Loss = -11938.3115234375
9
Iteration 26300: Loss = -11938.3115234375
10
Iteration 26400: Loss = -11938.3125
11
Iteration 26500: Loss = -11938.3125
12
Iteration 26600: Loss = -11938.3125
13
Iteration 26700: Loss = -11938.3125
14
Iteration 26800: Loss = -11938.3115234375
15
Stopping early at iteration 26800 due to no improvement.
pi: tensor([[0.3651, 0.6349],
        [0.5828, 0.4172]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5018, 0.4982], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3031, 0.0967],
         [0.8305, 0.2988]],

        [[0.0398, 0.1078],
         [0.9906, 0.1672]],

        [[0.2454, 0.1011],
         [0.1002, 0.6599]],

        [[0.0222, 0.0976],
         [0.0438, 0.0276]],

        [[0.0364, 0.1038],
         [0.7367, 0.9771]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03807427327330621
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28513.5234375
Iteration 100: Loss = -18291.947265625
Iteration 200: Loss = -13657.373046875
Iteration 300: Loss = -12879.99609375
Iteration 400: Loss = -12709.1259765625
Iteration 500: Loss = -12615.794921875
Iteration 600: Loss = -12569.3974609375
Iteration 700: Loss = -12534.8046875
Iteration 800: Loss = -12499.1005859375
Iteration 900: Loss = -12471.12109375
Iteration 1000: Loss = -12461.9150390625
Iteration 1100: Loss = -12455.7861328125
Iteration 1200: Loss = -12450.833984375
Iteration 1300: Loss = -12444.884765625
Iteration 1400: Loss = -12439.79296875
Iteration 1500: Loss = -12437.1943359375
Iteration 1600: Loss = -12435.078125
Iteration 1700: Loss = -12433.185546875
Iteration 1800: Loss = -12431.6064453125
Iteration 1900: Loss = -12430.27734375
Iteration 2000: Loss = -12428.015625
Iteration 2100: Loss = -12424.9638671875
Iteration 2200: Loss = -12423.8212890625
Iteration 2300: Loss = -12422.8037109375
Iteration 2400: Loss = -12420.54296875
Iteration 2500: Loss = -12416.8720703125
Iteration 2600: Loss = -12412.0986328125
Iteration 2700: Loss = -12410.51171875
Iteration 2800: Loss = -12409.6337890625
Iteration 2900: Loss = -12409.0205078125
Iteration 3000: Loss = -12408.53515625
Iteration 3100: Loss = -12408.1494140625
Iteration 3200: Loss = -12407.8193359375
Iteration 3300: Loss = -12407.53515625
Iteration 3400: Loss = -12407.2861328125
Iteration 3500: Loss = -12407.0625
Iteration 3600: Loss = -12406.8623046875
Iteration 3700: Loss = -12406.6826171875
Iteration 3800: Loss = -12406.51953125
Iteration 3900: Loss = -12406.369140625
Iteration 4000: Loss = -12406.232421875
Iteration 4100: Loss = -12406.107421875
Iteration 4200: Loss = -12405.9921875
Iteration 4300: Loss = -12405.8837890625
Iteration 4400: Loss = -12405.78515625
Iteration 4500: Loss = -12405.6923828125
Iteration 4600: Loss = -12405.6083984375
Iteration 4700: Loss = -12405.5283203125
Iteration 4800: Loss = -12405.44140625
Iteration 4900: Loss = -12405.373046875
Iteration 5000: Loss = -12405.30859375
Iteration 5100: Loss = -12405.248046875
Iteration 5200: Loss = -12405.1904296875
Iteration 5300: Loss = -12405.13671875
Iteration 5400: Loss = -12405.087890625
Iteration 5500: Loss = -12405.041015625
Iteration 5600: Loss = -12404.998046875
Iteration 5700: Loss = -12404.955078125
Iteration 5800: Loss = -12404.9150390625
Iteration 5900: Loss = -12404.8779296875
Iteration 6000: Loss = -12404.8447265625
Iteration 6100: Loss = -12404.810546875
Iteration 6200: Loss = -12404.779296875
Iteration 6300: Loss = -12404.751953125
Iteration 6400: Loss = -12404.724609375
Iteration 6500: Loss = -12404.6982421875
Iteration 6600: Loss = -12404.671875
Iteration 6700: Loss = -12404.6494140625
Iteration 6800: Loss = -12404.626953125
Iteration 6900: Loss = -12404.607421875
Iteration 7000: Loss = -12404.583984375
Iteration 7100: Loss = -12404.56640625
Iteration 7200: Loss = -12404.55078125
Iteration 7300: Loss = -12404.533203125
Iteration 7400: Loss = -12404.517578125
Iteration 7500: Loss = -12404.5029296875
Iteration 7600: Loss = -12404.4892578125
Iteration 7700: Loss = -12404.4755859375
Iteration 7800: Loss = -12404.4638671875
Iteration 7900: Loss = -12404.44921875
Iteration 8000: Loss = -12404.4384765625
Iteration 8100: Loss = -12404.427734375
Iteration 8200: Loss = -12404.4150390625
Iteration 8300: Loss = -12404.400390625
Iteration 8400: Loss = -12404.3828125
Iteration 8500: Loss = -12404.287109375
Iteration 8600: Loss = -12403.5634765625
Iteration 8700: Loss = -12403.4052734375
Iteration 8800: Loss = -12403.314453125
Iteration 8900: Loss = -12403.255859375
Iteration 9000: Loss = -12403.21484375
Iteration 9100: Loss = -12403.1806640625
Iteration 9200: Loss = -12403.15234375
Iteration 9300: Loss = -12403.1279296875
Iteration 9400: Loss = -12403.1083984375
Iteration 9500: Loss = -12403.08984375
Iteration 9600: Loss = -12403.0732421875
Iteration 9700: Loss = -12403.0595703125
Iteration 9800: Loss = -12403.0439453125
Iteration 9900: Loss = -12403.0322265625
Iteration 10000: Loss = -12403.0205078125
Iteration 10100: Loss = -12403.0078125
Iteration 10200: Loss = -12403.0
Iteration 10300: Loss = -12402.98828125
Iteration 10400: Loss = -12402.9794921875
Iteration 10500: Loss = -12402.97265625
Iteration 10600: Loss = -12402.9638671875
Iteration 10700: Loss = -12402.95703125
Iteration 10800: Loss = -12402.9482421875
Iteration 10900: Loss = -12402.943359375
Iteration 11000: Loss = -12402.935546875
Iteration 11100: Loss = -12402.9306640625
Iteration 11200: Loss = -12402.9248046875
Iteration 11300: Loss = -12402.9189453125
Iteration 11400: Loss = -12402.9130859375
Iteration 11500: Loss = -12402.9072265625
Iteration 11600: Loss = -12402.9033203125
Iteration 11700: Loss = -12402.8974609375
Iteration 11800: Loss = -12402.89453125
Iteration 11900: Loss = -12402.8876953125
Iteration 12000: Loss = -12402.8828125
Iteration 12100: Loss = -12402.87890625
Iteration 12200: Loss = -12402.8740234375
Iteration 12300: Loss = -12402.8701171875
Iteration 12400: Loss = -12402.8662109375
Iteration 12500: Loss = -12402.861328125
Iteration 12600: Loss = -12402.8583984375
Iteration 12700: Loss = -12402.853515625
Iteration 12800: Loss = -12402.849609375
Iteration 12900: Loss = -12402.8447265625
Iteration 13000: Loss = -12402.8427734375
Iteration 13100: Loss = -12402.8388671875
Iteration 13200: Loss = -12402.8330078125
Iteration 13300: Loss = -12402.830078125
Iteration 13400: Loss = -12402.8291015625
Iteration 13500: Loss = -12402.826171875
Iteration 13600: Loss = -12402.8193359375
Iteration 13700: Loss = -12402.818359375
Iteration 13800: Loss = -12402.8125
Iteration 13900: Loss = -12402.8115234375
Iteration 14000: Loss = -12402.8046875
Iteration 14100: Loss = -12402.80078125
Iteration 14200: Loss = -12402.7978515625
Iteration 14300: Loss = -12402.79296875
Iteration 14400: Loss = -12402.7841796875
Iteration 14500: Loss = -12402.7724609375
Iteration 14600: Loss = -12402.75390625
Iteration 14700: Loss = -12402.73046875
Iteration 14800: Loss = -12402.7216796875
Iteration 14900: Loss = -12402.708984375
Iteration 15000: Loss = -12402.70703125
Iteration 15100: Loss = -12402.697265625
Iteration 15200: Loss = -12402.6962890625
Iteration 15300: Loss = -12402.6962890625
Iteration 15400: Loss = -12402.6904296875
Iteration 15500: Loss = -12402.673828125
Iteration 15600: Loss = -12402.662109375
Iteration 15700: Loss = -12402.6591796875
Iteration 15800: Loss = -12402.64453125
Iteration 15900: Loss = -12402.638671875
Iteration 16000: Loss = -12402.634765625
Iteration 16100: Loss = -12402.6240234375
Iteration 16200: Loss = -12402.5966796875
Iteration 16300: Loss = -12402.576171875
Iteration 16400: Loss = -12402.541015625
Iteration 16500: Loss = -12402.4833984375
Iteration 16600: Loss = -12402.4130859375
Iteration 16700: Loss = -12402.365234375
Iteration 16800: Loss = -12402.3056640625
Iteration 16900: Loss = -12402.275390625
Iteration 17000: Loss = -12402.2470703125
Iteration 17100: Loss = -12402.22265625
Iteration 17200: Loss = -12402.12109375
Iteration 17300: Loss = -12402.041015625
Iteration 17400: Loss = -12401.990234375
Iteration 17500: Loss = -12401.9833984375
Iteration 17600: Loss = -12401.9326171875
Iteration 17700: Loss = -12401.8564453125
Iteration 17800: Loss = -12401.7958984375
Iteration 17900: Loss = -12401.75
Iteration 18000: Loss = -12401.71875
Iteration 18100: Loss = -12401.703125
Iteration 18200: Loss = -12401.666015625
Iteration 18300: Loss = -12401.6494140625
Iteration 18400: Loss = -12401.650390625
1
Iteration 18500: Loss = -12401.6494140625
Iteration 18600: Loss = -12401.6513671875
1
Iteration 18700: Loss = -12401.6494140625
Iteration 18800: Loss = -12401.6513671875
1
Iteration 18900: Loss = -12401.650390625
2
Iteration 19000: Loss = -12401.6494140625
Iteration 19100: Loss = -12401.6484375
Iteration 19200: Loss = -12401.6484375
Iteration 19300: Loss = -12401.6513671875
1
Iteration 19400: Loss = -12401.6513671875
2
Iteration 19500: Loss = -12401.650390625
3
Iteration 19600: Loss = -12401.6494140625
4
Iteration 19700: Loss = -12401.650390625
5
Iteration 19800: Loss = -12401.650390625
6
Iteration 19900: Loss = -12401.650390625
7
Iteration 20000: Loss = -12401.650390625
8
Iteration 20100: Loss = -12401.650390625
9
Iteration 20200: Loss = -12401.650390625
10
Iteration 20300: Loss = -12401.6494140625
11
Iteration 20400: Loss = -12401.650390625
12
Iteration 20500: Loss = -12401.6494140625
13
Iteration 20600: Loss = -12401.6484375
Iteration 20700: Loss = -12401.650390625
1
Iteration 20800: Loss = -12401.650390625
2
Iteration 20900: Loss = -12401.6484375
Iteration 21000: Loss = -12401.650390625
1
Iteration 21100: Loss = -12401.650390625
2
Iteration 21200: Loss = -12401.6494140625
3
Iteration 21300: Loss = -12401.650390625
4
Iteration 21400: Loss = -12401.650390625
5
Iteration 21500: Loss = -12401.6494140625
6
Iteration 21600: Loss = -12401.6484375
Iteration 21700: Loss = -12401.6494140625
1
Iteration 21800: Loss = -12401.650390625
2
Iteration 21900: Loss = -12401.6494140625
3
Iteration 22000: Loss = -12401.6494140625
4
Iteration 22100: Loss = -12401.6494140625
5
Iteration 22200: Loss = -12401.650390625
6
Iteration 22300: Loss = -12401.650390625
7
Iteration 22400: Loss = -12401.650390625
8
Iteration 22500: Loss = -12401.6494140625
9
Iteration 22600: Loss = -12401.6494140625
10
Iteration 22700: Loss = -12401.650390625
11
Iteration 22800: Loss = -12401.6484375
Iteration 22900: Loss = -12401.6494140625
1
Iteration 23000: Loss = -12401.650390625
2
Iteration 23100: Loss = -12401.650390625
3
Iteration 23200: Loss = -12401.6494140625
4
Iteration 23300: Loss = -12401.6484375
Iteration 23400: Loss = -12401.6494140625
1
Iteration 23500: Loss = -12401.6494140625
2
Iteration 23600: Loss = -12401.650390625
3
Iteration 23700: Loss = -12401.6494140625
4
Iteration 23800: Loss = -12401.6484375
Iteration 23900: Loss = -12401.6494140625
1
Iteration 24000: Loss = -12401.650390625
2
Iteration 24100: Loss = -12401.6513671875
3
Iteration 24200: Loss = -12401.6484375
Iteration 24300: Loss = -12401.6494140625
1
Iteration 24400: Loss = -12401.650390625
2
Iteration 24500: Loss = -12401.6494140625
3
Iteration 24600: Loss = -12401.6484375
Iteration 24700: Loss = -12401.6494140625
1
Iteration 24800: Loss = -12401.6484375
Iteration 24900: Loss = -12401.650390625
1
Iteration 25000: Loss = -12401.6484375
Iteration 25100: Loss = -12401.650390625
1
Iteration 25200: Loss = -12401.6474609375
Iteration 25300: Loss = -12401.6484375
1
Iteration 25400: Loss = -12401.650390625
2
Iteration 25500: Loss = -12401.6484375
3
Iteration 25600: Loss = -12401.650390625
4
Iteration 25700: Loss = -12401.650390625
5
Iteration 25800: Loss = -12401.650390625
6
Iteration 25900: Loss = -12401.6484375
7
Iteration 26000: Loss = -12401.6513671875
8
Iteration 26100: Loss = -12401.6484375
9
Iteration 26200: Loss = -12401.6494140625
10
Iteration 26300: Loss = -12401.6494140625
11
Iteration 26400: Loss = -12401.650390625
12
Iteration 26500: Loss = -12401.6494140625
13
Iteration 26600: Loss = -12401.6494140625
14
Iteration 26700: Loss = -12401.6494140625
15
Stopping early at iteration 26700 due to no improvement.
pi: tensor([[1.2365e-05, 9.9999e-01],
        [3.4678e-02, 9.6532e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0126, 0.9874], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2551, 0.0837],
         [0.2000, 0.1997]],

        [[0.9800, 0.2499],
         [0.7367, 0.0163]],

        [[0.4088, 0.1898],
         [0.3460, 0.6887]],

        [[0.1835, 0.1750],
         [0.1517, 0.9909]],

        [[0.0382, 0.2791],
         [0.5639, 0.6943]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: -0.00028166856108405595
Average Adjusted Rand Index: -0.0004529465619428516
[0.03807427327330621, -0.00028166856108405595] [1.0, -0.0004529465619428516] [11938.3115234375, 12401.6494140625]
-------------------------------------
This iteration is 30
True Objective function: Loss = -11761.135535701182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24429.392578125
Iteration 100: Loss = -17650.005859375
Iteration 200: Loss = -13437.2119140625
Iteration 300: Loss = -12646.5556640625
Iteration 400: Loss = -12462.7421875
Iteration 500: Loss = -12386.546875
Iteration 600: Loss = -12341.35546875
Iteration 700: Loss = -12322.248046875
Iteration 800: Loss = -12310.015625
Iteration 900: Loss = -12301.001953125
Iteration 1000: Loss = -12293.68359375
Iteration 1100: Loss = -12287.38671875
Iteration 1200: Loss = -12282.771484375
Iteration 1300: Loss = -12279.53125
Iteration 1400: Loss = -12276.7783203125
Iteration 1500: Loss = -12272.6552734375
Iteration 1600: Loss = -12269.0087890625
Iteration 1700: Loss = -12267.064453125
Iteration 1800: Loss = -12265.3759765625
Iteration 1900: Loss = -12263.9287109375
Iteration 2000: Loss = -12262.8642578125
Iteration 2100: Loss = -12262.087890625
Iteration 2200: Loss = -12261.4697265625
Iteration 2300: Loss = -12260.9599609375
Iteration 2400: Loss = -12260.5361328125
Iteration 2500: Loss = -12260.171875
Iteration 2600: Loss = -12259.8544921875
Iteration 2700: Loss = -12259.5751953125
Iteration 2800: Loss = -12259.3291015625
Iteration 2900: Loss = -12259.107421875
Iteration 3000: Loss = -12258.9072265625
Iteration 3100: Loss = -12258.7265625
Iteration 3200: Loss = -12258.5615234375
Iteration 3300: Loss = -12258.412109375
Iteration 3400: Loss = -12258.2763671875
Iteration 3500: Loss = -12258.1513671875
Iteration 3600: Loss = -12258.0361328125
Iteration 3700: Loss = -12257.9306640625
Iteration 3800: Loss = -12257.833984375
Iteration 3900: Loss = -12257.7451171875
Iteration 4000: Loss = -12257.6630859375
Iteration 4100: Loss = -12257.5869140625
Iteration 4200: Loss = -12257.5126953125
Iteration 4300: Loss = -12257.4443359375
Iteration 4400: Loss = -12257.3837890625
Iteration 4500: Loss = -12257.32421875
Iteration 4600: Loss = -12257.2705078125
Iteration 4700: Loss = -12257.2177734375
Iteration 4800: Loss = -12257.16796875
Iteration 4900: Loss = -12257.1240234375
Iteration 5000: Loss = -12257.0810546875
Iteration 5100: Loss = -12257.041015625
Iteration 5200: Loss = -12257.0048828125
Iteration 5300: Loss = -12256.9677734375
Iteration 5400: Loss = -12256.9345703125
Iteration 5500: Loss = -12256.90234375
Iteration 5600: Loss = -12256.8740234375
Iteration 5700: Loss = -12256.84765625
Iteration 5800: Loss = -12256.8203125
Iteration 5900: Loss = -12256.7958984375
Iteration 6000: Loss = -12256.7724609375
Iteration 6100: Loss = -12256.75
Iteration 6200: Loss = -12256.7294921875
Iteration 6300: Loss = -12256.7080078125
Iteration 6400: Loss = -12256.6904296875
Iteration 6500: Loss = -12256.673828125
Iteration 6600: Loss = -12256.65625
Iteration 6700: Loss = -12256.640625
Iteration 6800: Loss = -12256.626953125
Iteration 6900: Loss = -12256.6123046875
Iteration 7000: Loss = -12256.5986328125
Iteration 7100: Loss = -12256.587890625
Iteration 7200: Loss = -12256.57421875
Iteration 7300: Loss = -12256.5654296875
Iteration 7400: Loss = -12256.55078125
Iteration 7500: Loss = -12256.541015625
Iteration 7600: Loss = -12256.53125
Iteration 7700: Loss = -12256.5224609375
Iteration 7800: Loss = -12256.513671875
Iteration 7900: Loss = -12256.50390625
Iteration 8000: Loss = -12256.49609375
Iteration 8100: Loss = -12256.490234375
Iteration 8200: Loss = -12256.482421875
Iteration 8300: Loss = -12256.4736328125
Iteration 8400: Loss = -12256.46875
Iteration 8500: Loss = -12256.462890625
Iteration 8600: Loss = -12256.45703125
Iteration 8700: Loss = -12256.44921875
Iteration 8800: Loss = -12256.447265625
Iteration 8900: Loss = -12256.439453125
Iteration 9000: Loss = -12256.4365234375
Iteration 9100: Loss = -12256.431640625
Iteration 9200: Loss = -12256.4267578125
Iteration 9300: Loss = -12256.423828125
Iteration 9400: Loss = -12256.41796875
Iteration 9500: Loss = -12256.4169921875
Iteration 9600: Loss = -12256.4130859375
Iteration 9700: Loss = -12256.41015625
Iteration 9800: Loss = -12256.40625
Iteration 9900: Loss = -12256.40234375
Iteration 10000: Loss = -12256.4013671875
Iteration 10100: Loss = -12256.3984375
Iteration 10200: Loss = -12256.3955078125
Iteration 10300: Loss = -12256.392578125
Iteration 10400: Loss = -12256.3896484375
Iteration 10500: Loss = -12256.38671875
Iteration 10600: Loss = -12256.3857421875
Iteration 10700: Loss = -12256.3837890625
Iteration 10800: Loss = -12256.380859375
Iteration 10900: Loss = -12256.3798828125
Iteration 11000: Loss = -12256.3779296875
Iteration 11100: Loss = -12256.3759765625
Iteration 11200: Loss = -12256.375
Iteration 11300: Loss = -12256.3740234375
Iteration 11400: Loss = -12256.3720703125
Iteration 11500: Loss = -12256.37109375
Iteration 11600: Loss = -12256.3681640625
Iteration 11700: Loss = -12256.3681640625
Iteration 11800: Loss = -12256.3681640625
Iteration 11900: Loss = -12256.365234375
Iteration 12000: Loss = -12256.365234375
Iteration 12100: Loss = -12256.3642578125
Iteration 12200: Loss = -12256.36328125
Iteration 12300: Loss = -12256.361328125
Iteration 12400: Loss = -12256.359375
Iteration 12500: Loss = -12256.359375
Iteration 12600: Loss = -12256.357421875
Iteration 12700: Loss = -12256.357421875
Iteration 12800: Loss = -12256.353515625
Iteration 12900: Loss = -12256.3525390625
Iteration 13000: Loss = -12256.3447265625
Iteration 13100: Loss = -12256.3291015625
Iteration 13200: Loss = -12256.2841796875
Iteration 13300: Loss = -12256.2685546875
Iteration 13400: Loss = -12256.2607421875
Iteration 13500: Loss = -12256.2548828125
Iteration 13600: Loss = -12256.2509765625
Iteration 13700: Loss = -12256.248046875
Iteration 13800: Loss = -12256.2470703125
Iteration 13900: Loss = -12256.244140625
Iteration 14000: Loss = -12256.244140625
Iteration 14100: Loss = -12256.2431640625
Iteration 14200: Loss = -12256.240234375
Iteration 14300: Loss = -12256.240234375
Iteration 14400: Loss = -12256.2392578125
Iteration 14500: Loss = -12256.2392578125
Iteration 14600: Loss = -12256.23828125
Iteration 14700: Loss = -12256.236328125
Iteration 14800: Loss = -12256.2373046875
1
Iteration 14900: Loss = -12256.23828125
2
Iteration 15000: Loss = -12256.236328125
Iteration 15100: Loss = -12256.236328125
Iteration 15200: Loss = -12256.2353515625
Iteration 15300: Loss = -12256.234375
Iteration 15400: Loss = -12256.236328125
1
Iteration 15500: Loss = -12256.2353515625
2
Iteration 15600: Loss = -12256.2353515625
3
Iteration 15700: Loss = -12256.2333984375
Iteration 15800: Loss = -12256.2353515625
1
Iteration 15900: Loss = -12256.234375
2
Iteration 16000: Loss = -12256.2353515625
3
Iteration 16100: Loss = -12256.234375
4
Iteration 16200: Loss = -12256.2333984375
Iteration 16300: Loss = -12256.234375
1
Iteration 16400: Loss = -12256.232421875
Iteration 16500: Loss = -12256.234375
1
Iteration 16600: Loss = -12256.2333984375
2
Iteration 16700: Loss = -12256.2353515625
3
Iteration 16800: Loss = -12256.232421875
Iteration 16900: Loss = -12256.232421875
Iteration 17000: Loss = -12256.2333984375
1
Iteration 17100: Loss = -12256.2333984375
2
Iteration 17200: Loss = -12256.232421875
Iteration 17300: Loss = -12256.2333984375
1
Iteration 17400: Loss = -12256.2333984375
2
Iteration 17500: Loss = -12256.232421875
Iteration 17600: Loss = -12256.2314453125
Iteration 17700: Loss = -12256.2333984375
1
Iteration 17800: Loss = -12256.232421875
2
Iteration 17900: Loss = -12256.2333984375
3
Iteration 18000: Loss = -12256.2314453125
Iteration 18100: Loss = -12256.2314453125
Iteration 18200: Loss = -12256.232421875
1
Iteration 18300: Loss = -12256.232421875
2
Iteration 18400: Loss = -12256.2314453125
Iteration 18500: Loss = -12256.232421875
1
Iteration 18600: Loss = -12256.232421875
2
Iteration 18700: Loss = -12256.23046875
Iteration 18800: Loss = -12256.232421875
1
Iteration 18900: Loss = -12256.23046875
Iteration 19000: Loss = -12256.2314453125
1
Iteration 19100: Loss = -12256.2314453125
2
Iteration 19200: Loss = -12256.2314453125
3
Iteration 19300: Loss = -12256.2314453125
4
Iteration 19400: Loss = -12256.2314453125
5
Iteration 19500: Loss = -12256.232421875
6
Iteration 19600: Loss = -12256.23046875
Iteration 19700: Loss = -12256.232421875
1
Iteration 19800: Loss = -12256.232421875
2
Iteration 19900: Loss = -12256.2314453125
3
Iteration 20000: Loss = -12256.2314453125
4
Iteration 20100: Loss = -12256.2314453125
5
Iteration 20200: Loss = -12256.234375
6
Iteration 20300: Loss = -12256.2333984375
7
Iteration 20400: Loss = -12256.232421875
8
Iteration 20500: Loss = -12256.232421875
9
Iteration 20600: Loss = -12256.23046875
Iteration 20700: Loss = -12256.232421875
1
Iteration 20800: Loss = -12256.232421875
2
Iteration 20900: Loss = -12256.23046875
Iteration 21000: Loss = -12256.2333984375
1
Iteration 21100: Loss = -12256.232421875
2
Iteration 21200: Loss = -12256.2333984375
3
Iteration 21300: Loss = -12256.232421875
4
Iteration 21400: Loss = -12256.2333984375
5
Iteration 21500: Loss = -12256.232421875
6
Iteration 21600: Loss = -12256.2314453125
7
Iteration 21700: Loss = -12256.232421875
8
Iteration 21800: Loss = -12256.232421875
9
Iteration 21900: Loss = -12256.2314453125
10
Iteration 22000: Loss = -12256.232421875
11
Iteration 22100: Loss = -12256.234375
12
Iteration 22200: Loss = -12256.232421875
13
Iteration 22300: Loss = -12256.232421875
14
Iteration 22400: Loss = -12256.234375
15
Stopping early at iteration 22400 due to no improvement.
pi: tensor([[8.9680e-06, 9.9999e-01],
        [5.5363e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0240, 0.9760], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0902, 0.1462],
         [0.7696, 0.1966]],

        [[0.9921, 0.2609],
         [0.0737, 0.7618]],

        [[0.9817, 0.1991],
         [0.1001, 0.7242]],

        [[0.9633, 0.1968],
         [0.4142, 0.7470]],

        [[0.2659, 0.1974],
         [0.0152, 0.0562]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27866.626953125
Iteration 100: Loss = -18587.849609375
Iteration 200: Loss = -13957.8427734375
Iteration 300: Loss = -12621.0888671875
Iteration 400: Loss = -12385.6025390625
Iteration 500: Loss = -12329.576171875
Iteration 600: Loss = -12310.1943359375
Iteration 700: Loss = -12295.564453125
Iteration 800: Loss = -12286.861328125
Iteration 900: Loss = -12280.61328125
Iteration 1000: Loss = -12274.3115234375
Iteration 1100: Loss = -12271.1728515625
Iteration 1200: Loss = -12268.658203125
Iteration 1300: Loss = -12266.28125
Iteration 1400: Loss = -12263.943359375
Iteration 1500: Loss = -12261.875
Iteration 1600: Loss = -12259.6572265625
Iteration 1700: Loss = -12256.974609375
Iteration 1800: Loss = -12253.8173828125
Iteration 1900: Loss = -12250.3974609375
Iteration 2000: Loss = -12246.404296875
Iteration 2100: Loss = -12242.8349609375
Iteration 2200: Loss = -12240.6640625
Iteration 2300: Loss = -12238.8662109375
Iteration 2400: Loss = -12235.62109375
Iteration 2500: Loss = -12229.6455078125
Iteration 2600: Loss = -12221.0654296875
Iteration 2700: Loss = -12207.9453125
Iteration 2800: Loss = -12192.958984375
Iteration 2900: Loss = -12172.25
Iteration 3000: Loss = -12141.4599609375
Iteration 3100: Loss = -12120.6357421875
Iteration 3200: Loss = -12111.3046875
Iteration 3300: Loss = -12108.548828125
Iteration 3400: Loss = -12106.7294921875
Iteration 3500: Loss = -12099.9501953125
Iteration 3600: Loss = -12096.6015625
Iteration 3700: Loss = -12089.2763671875
Iteration 3800: Loss = -12086.7236328125
Iteration 3900: Loss = -12083.8544921875
Iteration 4000: Loss = -12079.5625
Iteration 4100: Loss = -12077.6875
Iteration 4200: Loss = -12073.201171875
Iteration 4300: Loss = -12064.630859375
Iteration 4400: Loss = -12061.1767578125
Iteration 4500: Loss = -12057.1103515625
Iteration 4600: Loss = -12050.4033203125
Iteration 4700: Loss = -12043.1328125
Iteration 4800: Loss = -12029.63671875
Iteration 4900: Loss = -12023.7802734375
Iteration 5000: Loss = -12002.654296875
Iteration 5100: Loss = -12000.8896484375
Iteration 5200: Loss = -11988.53515625
Iteration 5300: Loss = -11978.572265625
Iteration 5400: Loss = -11959.2646484375
Iteration 5500: Loss = -11957.4501953125
Iteration 5600: Loss = -11948.2021484375
Iteration 5700: Loss = -11942.783203125
Iteration 5800: Loss = -11927.345703125
Iteration 5900: Loss = -11921.4296875
Iteration 6000: Loss = -11914.22265625
Iteration 6100: Loss = -11899.83203125
Iteration 6200: Loss = -11899.4423828125
Iteration 6300: Loss = -11898.5859375
Iteration 6400: Loss = -11888.201171875
Iteration 6500: Loss = -11886.7861328125
Iteration 6600: Loss = -11886.685546875
Iteration 6700: Loss = -11877.8173828125
Iteration 6800: Loss = -11877.6591796875
Iteration 6900: Loss = -11877.59765625
Iteration 7000: Loss = -11877.5576171875
Iteration 7100: Loss = -11877.525390625
Iteration 7200: Loss = -11877.2919921875
Iteration 7300: Loss = -11857.56640625
Iteration 7400: Loss = -11857.2724609375
Iteration 7500: Loss = -11857.19140625
Iteration 7600: Loss = -11857.1484375
Iteration 7700: Loss = -11857.1171875
Iteration 7800: Loss = -11857.08984375
Iteration 7900: Loss = -11849.75390625
Iteration 8000: Loss = -11848.65234375
Iteration 8100: Loss = -11848.595703125
Iteration 8200: Loss = -11848.5654296875
Iteration 8300: Loss = -11848.544921875
Iteration 8400: Loss = -11848.5302734375
Iteration 8500: Loss = -11848.5126953125
Iteration 8600: Loss = -11840.8173828125
Iteration 8700: Loss = -11840.216796875
Iteration 8800: Loss = -11840.1728515625
Iteration 8900: Loss = -11840.1513671875
Iteration 9000: Loss = -11840.13671875
Iteration 9100: Loss = -11840.1259765625
Iteration 9200: Loss = -11840.1171875
Iteration 9300: Loss = -11840.1103515625
Iteration 9400: Loss = -11840.103515625
Iteration 9500: Loss = -11840.099609375
Iteration 9600: Loss = -11840.09375
Iteration 9700: Loss = -11840.0888671875
Iteration 9800: Loss = -11840.0859375
Iteration 9900: Loss = -11840.08203125
Iteration 10000: Loss = -11840.0791015625
Iteration 10100: Loss = -11840.0751953125
Iteration 10200: Loss = -11840.0751953125
Iteration 10300: Loss = -11840.0693359375
Iteration 10400: Loss = -11840.0439453125
Iteration 10500: Loss = -11839.740234375
Iteration 10600: Loss = -11839.73828125
Iteration 10700: Loss = -11839.7353515625
Iteration 10800: Loss = -11839.734375
Iteration 10900: Loss = -11839.728515625
Iteration 11000: Loss = -11828.767578125
Iteration 11100: Loss = -11827.9404296875
Iteration 11200: Loss = -11827.8896484375
Iteration 11300: Loss = -11827.8681640625
Iteration 11400: Loss = -11827.8525390625
Iteration 11500: Loss = -11826.5341796875
Iteration 11600: Loss = -11822.4296875
Iteration 11700: Loss = -11822.4033203125
Iteration 11800: Loss = -11821.470703125
Iteration 11900: Loss = -11815.095703125
Iteration 12000: Loss = -11815.0546875
Iteration 12100: Loss = -11815.037109375
Iteration 12200: Loss = -11815.029296875
Iteration 12300: Loss = -11815.0234375
Iteration 12400: Loss = -11815.017578125
Iteration 12500: Loss = -11815.015625
Iteration 12600: Loss = -11815.0068359375
Iteration 12700: Loss = -11815.0029296875
Iteration 12800: Loss = -11815.0009765625
Iteration 12900: Loss = -11815.0
Iteration 13000: Loss = -11814.998046875
Iteration 13100: Loss = -11814.99609375
Iteration 13200: Loss = -11814.9951171875
Iteration 13300: Loss = -11814.9951171875
Iteration 13400: Loss = -11814.9921875
Iteration 13500: Loss = -11809.052734375
Iteration 13600: Loss = -11806.359375
Iteration 13700: Loss = -11806.3203125
Iteration 13800: Loss = -11806.306640625
Iteration 13900: Loss = -11806.2978515625
Iteration 14000: Loss = -11806.2919921875
Iteration 14100: Loss = -11806.287109375
Iteration 14200: Loss = -11795.5390625
Iteration 14300: Loss = -11795.1494140625
Iteration 14400: Loss = -11795.107421875
Iteration 14500: Loss = -11795.08984375
Iteration 14600: Loss = -11795.078125
Iteration 14700: Loss = -11795.0732421875
Iteration 14800: Loss = -11795.06640625
Iteration 14900: Loss = -11788.0380859375
Iteration 15000: Loss = -11787.7265625
Iteration 15100: Loss = -11787.69921875
Iteration 15200: Loss = -11787.6865234375
Iteration 15300: Loss = -11787.6796875
Iteration 15400: Loss = -11787.673828125
Iteration 15500: Loss = -11787.6708984375
Iteration 15600: Loss = -11787.6669921875
Iteration 15700: Loss = -11785.5
Iteration 15800: Loss = -11784.59765625
Iteration 15900: Loss = -11784.5966796875
Iteration 16000: Loss = -11784.5947265625
Iteration 16100: Loss = -11784.5927734375
Iteration 16200: Loss = -11784.58984375
Iteration 16300: Loss = -11784.58984375
Iteration 16400: Loss = -11784.5888671875
Iteration 16500: Loss = -11784.587890625
Iteration 16600: Loss = -11784.5869140625
Iteration 16700: Loss = -11784.5869140625
Iteration 16800: Loss = -11784.5859375
Iteration 16900: Loss = -11784.5859375
Iteration 17000: Loss = -11784.5859375
Iteration 17100: Loss = -11784.583984375
Iteration 17200: Loss = -11784.5849609375
1
Iteration 17300: Loss = -11784.583984375
Iteration 17400: Loss = -11784.5830078125
Iteration 17500: Loss = -11784.583984375
1
Iteration 17600: Loss = -11784.5830078125
Iteration 17700: Loss = -11784.583984375
1
Iteration 17800: Loss = -11784.583984375
2
Iteration 17900: Loss = -11784.58203125
Iteration 18000: Loss = -11784.5830078125
1
Iteration 18100: Loss = -11784.5830078125
2
Iteration 18200: Loss = -11784.5830078125
3
Iteration 18300: Loss = -11784.580078125
Iteration 18400: Loss = -11784.5810546875
1
Iteration 18500: Loss = -11784.5810546875
2
Iteration 18600: Loss = -11784.5810546875
3
Iteration 18700: Loss = -11780.4853515625
Iteration 18800: Loss = -11780.4404296875
Iteration 18900: Loss = -11780.435546875
Iteration 19000: Loss = -11780.4345703125
Iteration 19100: Loss = -11780.43359375
Iteration 19200: Loss = -11780.4033203125
Iteration 19300: Loss = -11765.76953125
Iteration 19400: Loss = -11757.314453125
Iteration 19500: Loss = -11757.2080078125
Iteration 19600: Loss = -11757.1640625
Iteration 19700: Loss = -11757.1416015625
Iteration 19800: Loss = -11757.126953125
Iteration 19900: Loss = -11757.1162109375
Iteration 20000: Loss = -11757.109375
Iteration 20100: Loss = -11757.1044921875
Iteration 20200: Loss = -11757.0986328125
Iteration 20300: Loss = -11757.095703125
Iteration 20400: Loss = -11757.0927734375
Iteration 20500: Loss = -11757.0908203125
Iteration 20600: Loss = -11757.0888671875
Iteration 20700: Loss = -11757.0869140625
Iteration 20800: Loss = -11757.083984375
Iteration 20900: Loss = -11757.0830078125
Iteration 21000: Loss = -11757.0830078125
Iteration 21100: Loss = -11757.080078125
Iteration 21200: Loss = -11757.080078125
Iteration 21300: Loss = -11757.0791015625
Iteration 21400: Loss = -11757.078125
Iteration 21500: Loss = -11757.0771484375
Iteration 21600: Loss = -11757.076171875
Iteration 21700: Loss = -11757.0751953125
Iteration 21800: Loss = -11757.076171875
1
Iteration 21900: Loss = -11757.0751953125
Iteration 22000: Loss = -11757.07421875
Iteration 22100: Loss = -11757.07421875
Iteration 22200: Loss = -11757.0751953125
1
Iteration 22300: Loss = -11757.07421875
Iteration 22400: Loss = -11757.07421875
Iteration 22500: Loss = -11757.068359375
Iteration 22600: Loss = -11749.7900390625
Iteration 22700: Loss = -11749.6923828125
Iteration 22800: Loss = -11749.67578125
Iteration 22900: Loss = -11749.666015625
Iteration 23000: Loss = -11749.6630859375
Iteration 23100: Loss = -11749.6591796875
Iteration 23200: Loss = -11749.65625
Iteration 23300: Loss = -11749.65625
Iteration 23400: Loss = -11749.6533203125
Iteration 23500: Loss = -11749.65234375
Iteration 23600: Loss = -11749.65234375
Iteration 23700: Loss = -11749.6513671875
Iteration 23800: Loss = -11749.650390625
Iteration 23900: Loss = -11749.650390625
Iteration 24000: Loss = -11749.650390625
Iteration 24100: Loss = -11749.6494140625
Iteration 24200: Loss = -11749.6484375
Iteration 24300: Loss = -11749.6484375
Iteration 24400: Loss = -11749.6494140625
1
Iteration 24500: Loss = -11749.6474609375
Iteration 24600: Loss = -11749.6474609375
Iteration 24700: Loss = -11749.6484375
1
Iteration 24800: Loss = -11749.6474609375
Iteration 24900: Loss = -11749.6474609375
Iteration 25000: Loss = -11749.646484375
Iteration 25100: Loss = -11749.6474609375
1
Iteration 25200: Loss = -11749.6474609375
2
Iteration 25300: Loss = -11749.646484375
Iteration 25400: Loss = -11749.6455078125
Iteration 25500: Loss = -11749.646484375
1
Iteration 25600: Loss = -11749.646484375
2
Iteration 25700: Loss = -11749.6474609375
3
Iteration 25800: Loss = -11749.646484375
4
Iteration 25900: Loss = -11749.646484375
5
Iteration 26000: Loss = -11749.646484375
6
Iteration 26100: Loss = -11749.6474609375
7
Iteration 26200: Loss = -11749.646484375
8
Iteration 26300: Loss = -11749.646484375
9
Iteration 26400: Loss = -11749.646484375
10
Iteration 26500: Loss = -11749.646484375
11
Iteration 26600: Loss = -11749.6474609375
12
Iteration 26700: Loss = -11749.6474609375
13
Iteration 26800: Loss = -11749.6455078125
Iteration 26900: Loss = -11749.6455078125
Iteration 27000: Loss = -11749.646484375
1
Iteration 27100: Loss = -11749.646484375
2
Iteration 27200: Loss = -11749.6455078125
Iteration 27300: Loss = -11749.646484375
1
Iteration 27400: Loss = -11749.6455078125
Iteration 27500: Loss = -11749.6455078125
Iteration 27600: Loss = -11749.64453125
Iteration 27700: Loss = -11749.6455078125
1
Iteration 27800: Loss = -11749.64453125
Iteration 27900: Loss = -11749.6455078125
1
Iteration 28000: Loss = -11749.646484375
2
Iteration 28100: Loss = -11749.6474609375
3
Iteration 28200: Loss = -11749.6474609375
4
Iteration 28300: Loss = -11749.6455078125
5
Iteration 28400: Loss = -11749.6455078125
6
Iteration 28500: Loss = -11749.646484375
7
Iteration 28600: Loss = -11749.6455078125
8
Iteration 28700: Loss = -11749.6455078125
9
Iteration 28800: Loss = -11749.646484375
10
Iteration 28900: Loss = -11749.646484375
11
Iteration 29000: Loss = -11749.6455078125
12
Iteration 29100: Loss = -11749.6455078125
13
Iteration 29200: Loss = -11749.64453125
Iteration 29300: Loss = -11749.6455078125
1
Iteration 29400: Loss = -11749.6455078125
2
Iteration 29500: Loss = -11749.6455078125
3
Iteration 29600: Loss = -11749.64453125
Iteration 29700: Loss = -11749.646484375
1
Iteration 29800: Loss = -11749.6455078125
2
Iteration 29900: Loss = -11749.64453125
pi: tensor([[0.7730, 0.2270],
        [0.2757, 0.7243]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5193, 0.4807], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2940, 0.0930],
         [0.9851, 0.2976]],

        [[0.9831, 0.0948],
         [0.8697, 0.8538]],

        [[0.9329, 0.1034],
         [0.0151, 0.5463]],

        [[0.5207, 0.0952],
         [0.9917, 0.1119]],

        [[0.8832, 0.1029],
         [0.1291, 0.9774]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760960701200192
Average Adjusted Rand Index: 0.9759989342602624
[0.0, 0.9760960701200192] [0.0, 0.9759989342602624] [12256.234375, 11749.6455078125]
-------------------------------------
This iteration is 31
True Objective function: Loss = -12060.596185867535
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21392.044921875
Iteration 100: Loss = -15857.9560546875
Iteration 200: Loss = -13320.0
Iteration 300: Loss = -12728.484375
Iteration 400: Loss = -12629.99609375
Iteration 500: Loss = -12599.05078125
Iteration 600: Loss = -12582.65625
Iteration 700: Loss = -12570.37109375
Iteration 800: Loss = -12560.7265625
Iteration 900: Loss = -12555.84765625
Iteration 1000: Loss = -12553.2138671875
Iteration 1100: Loss = -12551.421875
Iteration 1200: Loss = -12550.2763671875
Iteration 1300: Loss = -12549.546875
Iteration 1400: Loss = -12549.0361328125
Iteration 1500: Loss = -12548.6572265625
Iteration 1600: Loss = -12548.3662109375
Iteration 1700: Loss = -12548.1337890625
Iteration 1800: Loss = -12547.947265625
Iteration 1900: Loss = -12547.79296875
Iteration 2000: Loss = -12547.666015625
Iteration 2100: Loss = -12547.556640625
Iteration 2200: Loss = -12547.4619140625
Iteration 2300: Loss = -12547.37890625
Iteration 2400: Loss = -12547.306640625
Iteration 2500: Loss = -12547.2421875
Iteration 2600: Loss = -12547.185546875
Iteration 2700: Loss = -12547.1328125
Iteration 2800: Loss = -12547.0859375
Iteration 2900: Loss = -12547.0380859375
Iteration 3000: Loss = -12546.994140625
Iteration 3100: Loss = -12546.94921875
Iteration 3200: Loss = -12546.9072265625
Iteration 3300: Loss = -12546.86328125
Iteration 3400: Loss = -12546.8173828125
Iteration 3500: Loss = -12546.7744140625
Iteration 3600: Loss = -12546.728515625
Iteration 3700: Loss = -12546.6806640625
Iteration 3800: Loss = -12546.6298828125
Iteration 3900: Loss = -12546.57421875
Iteration 4000: Loss = -12546.51953125
Iteration 4100: Loss = -12546.4619140625
Iteration 4200: Loss = -12546.40234375
Iteration 4300: Loss = -12546.341796875
Iteration 4400: Loss = -12546.2822265625
Iteration 4500: Loss = -12546.228515625
Iteration 4600: Loss = -12546.1767578125
Iteration 4700: Loss = -12546.1298828125
Iteration 4800: Loss = -12546.087890625
Iteration 4900: Loss = -12546.052734375
Iteration 5000: Loss = -12546.0205078125
Iteration 5100: Loss = -12545.9931640625
Iteration 5200: Loss = -12545.9677734375
Iteration 5300: Loss = -12545.9482421875
Iteration 5400: Loss = -12545.9306640625
Iteration 5500: Loss = -12545.916015625
Iteration 5600: Loss = -12545.8896484375
Iteration 5700: Loss = -12545.875
Iteration 5800: Loss = -12545.8671875
Iteration 5900: Loss = -12545.8603515625
Iteration 6000: Loss = -12545.8544921875
Iteration 6100: Loss = -12545.8486328125
Iteration 6200: Loss = -12545.84375
Iteration 6300: Loss = -12545.8369140625
Iteration 6400: Loss = -12545.8349609375
Iteration 6500: Loss = -12545.8310546875
Iteration 6600: Loss = -12545.8251953125
Iteration 6700: Loss = -12545.82421875
Iteration 6800: Loss = -12545.8193359375
Iteration 6900: Loss = -12545.810546875
Iteration 7000: Loss = -12545.80859375
Iteration 7100: Loss = -12545.8046875
Iteration 7200: Loss = -12545.798828125
Iteration 7300: Loss = -12545.7939453125
Iteration 7400: Loss = -12545.787109375
Iteration 7500: Loss = -12545.7763671875
Iteration 7600: Loss = -12545.7626953125
Iteration 7700: Loss = -12545.7451171875
Iteration 7800: Loss = -12545.7236328125
Iteration 7900: Loss = -12545.7060546875
Iteration 8000: Loss = -12545.689453125
Iteration 8100: Loss = -12545.666015625
Iteration 8200: Loss = -12545.62890625
Iteration 8300: Loss = -12545.5478515625
Iteration 8400: Loss = -12545.2744140625
Iteration 8500: Loss = -12544.650390625
Iteration 8600: Loss = -12544.341796875
Iteration 8700: Loss = -12544.1845703125
Iteration 8800: Loss = -12544.0888671875
Iteration 8900: Loss = -12544.025390625
Iteration 9000: Loss = -12543.9833984375
Iteration 9100: Loss = -12543.951171875
Iteration 9200: Loss = -12543.9267578125
Iteration 9300: Loss = -12543.908203125
Iteration 9400: Loss = -12543.8896484375
Iteration 9500: Loss = -12543.876953125
Iteration 9600: Loss = -12543.8671875
Iteration 9700: Loss = -12543.857421875
Iteration 9800: Loss = -12543.84765625
Iteration 9900: Loss = -12543.841796875
Iteration 10000: Loss = -12543.8349609375
Iteration 10100: Loss = -12543.83203125
Iteration 10200: Loss = -12543.828125
Iteration 10300: Loss = -12543.822265625
Iteration 10400: Loss = -12543.8193359375
Iteration 10500: Loss = -12543.81640625
Iteration 10600: Loss = -12543.8134765625
Iteration 10700: Loss = -12543.8115234375
Iteration 10800: Loss = -12543.8076171875
Iteration 10900: Loss = -12543.806640625
Iteration 11000: Loss = -12543.8046875
Iteration 11100: Loss = -12543.8037109375
Iteration 11200: Loss = -12543.802734375
Iteration 11300: Loss = -12543.798828125
Iteration 11400: Loss = -12543.798828125
Iteration 11500: Loss = -12543.7978515625
Iteration 11600: Loss = -12543.7958984375
Iteration 11700: Loss = -12543.794921875
Iteration 11800: Loss = -12543.7939453125
Iteration 11900: Loss = -12543.79296875
Iteration 12000: Loss = -12543.7919921875
Iteration 12100: Loss = -12543.7900390625
Iteration 12200: Loss = -12543.7900390625
Iteration 12300: Loss = -12543.7900390625
Iteration 12400: Loss = -12543.7880859375
Iteration 12500: Loss = -12543.7861328125
Iteration 12600: Loss = -12543.787109375
1
Iteration 12700: Loss = -12543.787109375
2
Iteration 12800: Loss = -12543.78515625
Iteration 12900: Loss = -12543.78515625
Iteration 13000: Loss = -12543.7841796875
Iteration 13100: Loss = -12543.7841796875
Iteration 13200: Loss = -12543.7841796875
Iteration 13300: Loss = -12543.7841796875
Iteration 13400: Loss = -12543.7822265625
Iteration 13500: Loss = -12543.783203125
1
Iteration 13600: Loss = -12543.78125
Iteration 13700: Loss = -12543.7841796875
1
Iteration 13800: Loss = -12543.78125
Iteration 13900: Loss = -12543.7822265625
1
Iteration 14000: Loss = -12543.78125
Iteration 14100: Loss = -12543.779296875
Iteration 14200: Loss = -12543.7822265625
1
Iteration 14300: Loss = -12543.7802734375
2
Iteration 14400: Loss = -12543.7802734375
3
Iteration 14500: Loss = -12543.78125
4
Iteration 14600: Loss = -12543.779296875
Iteration 14700: Loss = -12543.7802734375
1
Iteration 14800: Loss = -12543.779296875
Iteration 14900: Loss = -12543.7802734375
1
Iteration 15000: Loss = -12543.7783203125
Iteration 15100: Loss = -12543.7783203125
Iteration 15200: Loss = -12543.779296875
1
Iteration 15300: Loss = -12543.7783203125
Iteration 15400: Loss = -12543.77734375
Iteration 15500: Loss = -12543.7783203125
1
Iteration 15600: Loss = -12543.7783203125
2
Iteration 15700: Loss = -12543.7783203125
3
Iteration 15800: Loss = -12543.779296875
4
Iteration 15900: Loss = -12543.7783203125
5
Iteration 16000: Loss = -12543.77734375
Iteration 16100: Loss = -12543.7783203125
1
Iteration 16200: Loss = -12543.7783203125
2
Iteration 16300: Loss = -12543.7763671875
Iteration 16400: Loss = -12543.77734375
1
Iteration 16500: Loss = -12543.77734375
2
Iteration 16600: Loss = -12543.7783203125
3
Iteration 16700: Loss = -12543.7783203125
4
Iteration 16800: Loss = -12543.7783203125
5
Iteration 16900: Loss = -12543.7783203125
6
Iteration 17000: Loss = -12543.77734375
7
Iteration 17100: Loss = -12543.77734375
8
Iteration 17200: Loss = -12543.7763671875
Iteration 17300: Loss = -12543.7763671875
Iteration 17400: Loss = -12543.7783203125
1
Iteration 17500: Loss = -12543.7783203125
2
Iteration 17600: Loss = -12543.77734375
3
Iteration 17700: Loss = -12543.7763671875
Iteration 17800: Loss = -12543.7783203125
1
Iteration 17900: Loss = -12543.7763671875
Iteration 18000: Loss = -12543.77734375
1
Iteration 18100: Loss = -12543.779296875
2
Iteration 18200: Loss = -12543.7763671875
Iteration 18300: Loss = -12543.77734375
1
Iteration 18400: Loss = -12543.7763671875
Iteration 18500: Loss = -12543.7783203125
1
Iteration 18600: Loss = -12543.77734375
2
Iteration 18700: Loss = -12543.7763671875
Iteration 18800: Loss = -12543.7763671875
Iteration 18900: Loss = -12543.77734375
1
Iteration 19000: Loss = -12543.7763671875
Iteration 19100: Loss = -12543.7763671875
Iteration 19200: Loss = -12543.7763671875
Iteration 19300: Loss = -12543.77734375
1
Iteration 19400: Loss = -12543.7744140625
Iteration 19500: Loss = -12543.77734375
1
Iteration 19600: Loss = -12543.775390625
2
Iteration 19700: Loss = -12543.77734375
3
Iteration 19800: Loss = -12543.77734375
4
Iteration 19900: Loss = -12543.7763671875
5
Iteration 20000: Loss = -12543.775390625
6
Iteration 20100: Loss = -12543.77734375
7
Iteration 20200: Loss = -12543.775390625
8
Iteration 20300: Loss = -12543.7763671875
9
Iteration 20400: Loss = -12543.7763671875
10
Iteration 20500: Loss = -12543.7783203125
11
Iteration 20600: Loss = -12543.7763671875
12
Iteration 20700: Loss = -12543.7763671875
13
Iteration 20800: Loss = -12543.7763671875
14
Iteration 20900: Loss = -12543.7763671875
15
Stopping early at iteration 20900 due to no improvement.
pi: tensor([[9.9999e-01, 8.2148e-06],
        [3.4517e-05, 9.9997e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9708, 0.0292], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2058, 0.1456],
         [0.7662, 0.2261]],

        [[0.0936, 0.2728],
         [0.7724, 0.0301]],

        [[0.0252, 0.2175],
         [0.1823, 0.6947]],

        [[0.0219, 0.1345],
         [0.1908, 0.1556]],

        [[0.1017, 0.1706],
         [0.5617, 0.5678]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0010149900615556472
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.0041478895259491316
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0031985518644306096
Global Adjusted Rand Index: 0.0004364466111431728
Average Adjusted Rand Index: -0.0011528046186962486
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -45946.98046875
Iteration 100: Loss = -27985.802734375
Iteration 200: Loss = -16909.0390625
Iteration 300: Loss = -13562.7490234375
Iteration 400: Loss = -13033.875
Iteration 500: Loss = -12843.701171875
Iteration 600: Loss = -12751.189453125
Iteration 700: Loss = -12674.1279296875
Iteration 800: Loss = -12646.4228515625
Iteration 900: Loss = -12627.884765625
Iteration 1000: Loss = -12609.798828125
Iteration 1100: Loss = -12596.046875
Iteration 1200: Loss = -12589.3984375
Iteration 1300: Loss = -12584.1884765625
Iteration 1400: Loss = -12579.9560546875
Iteration 1500: Loss = -12574.986328125
Iteration 1600: Loss = -12568.2890625
Iteration 1700: Loss = -12565.8388671875
Iteration 1800: Loss = -12563.798828125
Iteration 1900: Loss = -12562.0810546875
Iteration 2000: Loss = -12560.6123046875
Iteration 2100: Loss = -12559.33984375
Iteration 2200: Loss = -12558.236328125
Iteration 2300: Loss = -12557.271484375
Iteration 2400: Loss = -12556.4189453125
Iteration 2500: Loss = -12555.6591796875
Iteration 2600: Loss = -12554.982421875
Iteration 2700: Loss = -12554.37109375
Iteration 2800: Loss = -12553.8203125
Iteration 2900: Loss = -12553.322265625
Iteration 3000: Loss = -12552.8671875
Iteration 3100: Loss = -12552.4541015625
Iteration 3200: Loss = -12552.0751953125
Iteration 3300: Loss = -12551.7275390625
Iteration 3400: Loss = -12551.4091796875
Iteration 3500: Loss = -12551.1162109375
Iteration 3600: Loss = -12550.84375
Iteration 3700: Loss = -12550.5927734375
Iteration 3800: Loss = -12550.3623046875
Iteration 3900: Loss = -12550.1494140625
Iteration 4000: Loss = -12549.9482421875
Iteration 4100: Loss = -12549.7607421875
Iteration 4200: Loss = -12549.5888671875
Iteration 4300: Loss = -12549.427734375
Iteration 4400: Loss = -12549.2763671875
Iteration 4500: Loss = -12549.1337890625
Iteration 4600: Loss = -12549.001953125
Iteration 4700: Loss = -12548.8779296875
Iteration 4800: Loss = -12548.76171875
Iteration 4900: Loss = -12548.654296875
Iteration 5000: Loss = -12548.5517578125
Iteration 5100: Loss = -12548.4541015625
Iteration 5200: Loss = -12548.365234375
Iteration 5300: Loss = -12548.279296875
Iteration 5400: Loss = -12548.19921875
Iteration 5500: Loss = -12548.123046875
Iteration 5600: Loss = -12548.0517578125
Iteration 5700: Loss = -12547.9853515625
Iteration 5800: Loss = -12547.9208984375
Iteration 5900: Loss = -12547.861328125
Iteration 6000: Loss = -12547.802734375
Iteration 6100: Loss = -12547.75
Iteration 6200: Loss = -12547.69921875
Iteration 6300: Loss = -12547.6484375
Iteration 6400: Loss = -12547.603515625
Iteration 6500: Loss = -12547.5576171875
Iteration 6600: Loss = -12547.5166015625
Iteration 6700: Loss = -12547.478515625
Iteration 6800: Loss = -12547.439453125
Iteration 6900: Loss = -12547.40234375
Iteration 7000: Loss = -12547.369140625
Iteration 7100: Loss = -12547.3369140625
Iteration 7200: Loss = -12547.306640625
Iteration 7300: Loss = -12547.2763671875
Iteration 7400: Loss = -12547.2470703125
Iteration 7500: Loss = -12547.21875
Iteration 7600: Loss = -12547.1923828125
Iteration 7700: Loss = -12547.1650390625
Iteration 7800: Loss = -12547.140625
Iteration 7900: Loss = -12547.1181640625
Iteration 8000: Loss = -12547.099609375
Iteration 8100: Loss = -12547.080078125
Iteration 8200: Loss = -12547.064453125
Iteration 8300: Loss = -12547.046875
Iteration 8400: Loss = -12547.0341796875
Iteration 8500: Loss = -12547.0205078125
Iteration 8600: Loss = -12547.0087890625
Iteration 8700: Loss = -12546.9990234375
Iteration 8800: Loss = -12546.9873046875
Iteration 8900: Loss = -12546.9794921875
Iteration 9000: Loss = -12546.970703125
Iteration 9100: Loss = -12546.9609375
Iteration 9200: Loss = -12546.9541015625
Iteration 9300: Loss = -12546.9462890625
Iteration 9400: Loss = -12546.939453125
Iteration 9500: Loss = -12546.9326171875
Iteration 9600: Loss = -12546.92578125
Iteration 9700: Loss = -12546.9189453125
Iteration 9800: Loss = -12546.9140625
Iteration 9900: Loss = -12546.908203125
Iteration 10000: Loss = -12546.9033203125
Iteration 10100: Loss = -12546.8974609375
Iteration 10200: Loss = -12546.892578125
Iteration 10300: Loss = -12546.8876953125
Iteration 10400: Loss = -12546.8828125
Iteration 10500: Loss = -12546.8779296875
Iteration 10600: Loss = -12546.8759765625
Iteration 10700: Loss = -12546.8720703125
Iteration 10800: Loss = -12546.8701171875
Iteration 10900: Loss = -12546.8642578125
Iteration 11000: Loss = -12546.8603515625
Iteration 11100: Loss = -12546.859375
Iteration 11200: Loss = -12546.85546875
Iteration 11300: Loss = -12546.8515625
Iteration 11400: Loss = -12546.8486328125
Iteration 11500: Loss = -12546.845703125
Iteration 11600: Loss = -12546.841796875
Iteration 11700: Loss = -12546.83984375
Iteration 11800: Loss = -12546.8359375
Iteration 11900: Loss = -12546.8330078125
Iteration 12000: Loss = -12546.8310546875
Iteration 12100: Loss = -12546.8291015625
Iteration 12200: Loss = -12546.826171875
Iteration 12300: Loss = -12546.8232421875
Iteration 12400: Loss = -12546.8212890625
Iteration 12500: Loss = -12546.8203125
Iteration 12600: Loss = -12546.8173828125
Iteration 12700: Loss = -12546.818359375
1
Iteration 12800: Loss = -12546.8154296875
Iteration 12900: Loss = -12546.814453125
Iteration 13000: Loss = -12546.814453125
Iteration 13100: Loss = -12546.8115234375
Iteration 13200: Loss = -12546.8115234375
Iteration 13300: Loss = -12546.810546875
Iteration 13400: Loss = -12546.810546875
Iteration 13500: Loss = -12546.80859375
Iteration 13600: Loss = -12546.806640625
Iteration 13700: Loss = -12546.8076171875
1
Iteration 13800: Loss = -12546.806640625
Iteration 13900: Loss = -12546.8076171875
1
Iteration 14000: Loss = -12546.8056640625
Iteration 14100: Loss = -12546.8046875
Iteration 14200: Loss = -12546.8037109375
Iteration 14300: Loss = -12546.8046875
1
Iteration 14400: Loss = -12546.802734375
Iteration 14500: Loss = -12546.802734375
Iteration 14600: Loss = -12546.802734375
Iteration 14700: Loss = -12546.8017578125
Iteration 14800: Loss = -12546.802734375
1
Iteration 14900: Loss = -12546.802734375
2
Iteration 15000: Loss = -12546.8017578125
Iteration 15100: Loss = -12546.8017578125
Iteration 15200: Loss = -12546.7998046875
Iteration 15300: Loss = -12546.798828125
Iteration 15400: Loss = -12546.798828125
Iteration 15500: Loss = -12546.80078125
1
Iteration 15600: Loss = -12546.798828125
Iteration 15700: Loss = -12546.798828125
Iteration 15800: Loss = -12546.798828125
Iteration 15900: Loss = -12546.7998046875
1
Iteration 16000: Loss = -12546.798828125
Iteration 16100: Loss = -12546.798828125
Iteration 16200: Loss = -12546.796875
Iteration 16300: Loss = -12546.7978515625
1
Iteration 16400: Loss = -12546.798828125
2
Iteration 16500: Loss = -12546.798828125
3
Iteration 16600: Loss = -12546.7978515625
4
Iteration 16700: Loss = -12546.796875
Iteration 16800: Loss = -12546.7978515625
1
Iteration 16900: Loss = -12546.7978515625
2
Iteration 17000: Loss = -12546.796875
Iteration 17100: Loss = -12546.7978515625
1
Iteration 17200: Loss = -12546.796875
Iteration 17300: Loss = -12546.796875
Iteration 17400: Loss = -12546.7978515625
1
Iteration 17500: Loss = -12546.796875
Iteration 17600: Loss = -12546.796875
Iteration 17700: Loss = -12546.798828125
1
Iteration 17800: Loss = -12546.796875
Iteration 17900: Loss = -12546.796875
Iteration 18000: Loss = -12546.7958984375
Iteration 18100: Loss = -12546.7978515625
1
Iteration 18200: Loss = -12546.7958984375
Iteration 18300: Loss = -12546.7958984375
Iteration 18400: Loss = -12546.7958984375
Iteration 18500: Loss = -12546.796875
1
Iteration 18600: Loss = -12546.796875
2
Iteration 18700: Loss = -12546.7958984375
Iteration 18800: Loss = -12546.794921875
Iteration 18900: Loss = -12546.796875
1
Iteration 19000: Loss = -12546.794921875
Iteration 19100: Loss = -12546.794921875
Iteration 19200: Loss = -12546.794921875
Iteration 19300: Loss = -12546.794921875
Iteration 19400: Loss = -12546.7958984375
1
Iteration 19500: Loss = -12546.796875
2
Iteration 19600: Loss = -12546.7958984375
3
Iteration 19700: Loss = -12546.798828125
4
Iteration 19800: Loss = -12546.796875
5
Iteration 19900: Loss = -12546.794921875
Iteration 20000: Loss = -12546.7939453125
Iteration 20100: Loss = -12546.796875
1
Iteration 20200: Loss = -12546.794921875
2
Iteration 20300: Loss = -12546.794921875
3
Iteration 20400: Loss = -12546.7939453125
Iteration 20500: Loss = -12546.7939453125
Iteration 20600: Loss = -12546.7958984375
1
Iteration 20700: Loss = -12546.794921875
2
Iteration 20800: Loss = -12546.7939453125
Iteration 20900: Loss = -12546.7939453125
Iteration 21000: Loss = -12546.794921875
1
Iteration 21100: Loss = -12546.794921875
2
Iteration 21200: Loss = -12546.794921875
3
Iteration 21300: Loss = -12546.7939453125
Iteration 21400: Loss = -12546.7939453125
Iteration 21500: Loss = -12546.7939453125
Iteration 21600: Loss = -12546.7958984375
1
Iteration 21700: Loss = -12546.794921875
2
Iteration 21800: Loss = -12546.794921875
3
Iteration 21900: Loss = -12546.796875
4
Iteration 22000: Loss = -12546.794921875
5
Iteration 22100: Loss = -12546.7939453125
Iteration 22200: Loss = -12546.794921875
1
Iteration 22300: Loss = -12546.7939453125
Iteration 22400: Loss = -12546.794921875
1
Iteration 22500: Loss = -12546.7939453125
Iteration 22600: Loss = -12546.7939453125
Iteration 22700: Loss = -12546.7939453125
Iteration 22800: Loss = -12546.7958984375
1
Iteration 22900: Loss = -12546.794921875
2
Iteration 23000: Loss = -12546.7958984375
3
Iteration 23100: Loss = -12546.7939453125
Iteration 23200: Loss = -12546.7958984375
1
Iteration 23300: Loss = -12546.7958984375
2
Iteration 23400: Loss = -12546.794921875
3
Iteration 23500: Loss = -12546.7939453125
Iteration 23600: Loss = -12546.794921875
1
Iteration 23700: Loss = -12546.7939453125
Iteration 23800: Loss = -12546.794921875
1
Iteration 23900: Loss = -12546.79296875
Iteration 24000: Loss = -12546.794921875
1
Iteration 24100: Loss = -12546.794921875
2
Iteration 24200: Loss = -12546.7939453125
3
Iteration 24300: Loss = -12546.7939453125
4
Iteration 24400: Loss = -12546.794921875
5
Iteration 24500: Loss = -12546.7939453125
6
Iteration 24600: Loss = -12546.794921875
7
Iteration 24700: Loss = -12546.7939453125
8
Iteration 24800: Loss = -12546.7939453125
9
Iteration 24900: Loss = -12546.7939453125
10
Iteration 25000: Loss = -12546.7939453125
11
Iteration 25100: Loss = -12546.7958984375
12
Iteration 25200: Loss = -12546.7958984375
13
Iteration 25300: Loss = -12546.794921875
14
Iteration 25400: Loss = -12546.7958984375
15
Stopping early at iteration 25400 due to no improvement.
pi: tensor([[9.9999e-01, 1.3959e-05],
        [9.8414e-01, 1.5861e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9985e-01, 1.4753e-04], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2048, 0.2106],
         [0.4738, 0.8006]],

        [[0.0137, 0.2016],
         [0.6946, 0.0455]],

        [[0.7806, 0.2151],
         [0.0294, 0.9794]],

        [[0.9491, 0.2117],
         [0.9850, 0.8040]],

        [[0.3612, 0.2225],
         [0.6872, 0.0114]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.0004364466111431728, 0.0] [-0.0011528046186962486, 0.0] [12543.7763671875, 12546.7958984375]
-------------------------------------
This iteration is 32
True Objective function: Loss = -11652.378966573497
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -41294.19921875
Iteration 100: Loss = -26852.85546875
Iteration 200: Loss = -17191.33203125
Iteration 300: Loss = -13622.07421875
Iteration 400: Loss = -12621.0185546875
Iteration 500: Loss = -12372.48828125
Iteration 600: Loss = -12306.5986328125
Iteration 700: Loss = -12270.873046875
Iteration 800: Loss = -12248.306640625
Iteration 900: Loss = -12232.87109375
Iteration 1000: Loss = -12221.7431640625
Iteration 1100: Loss = -12213.4033203125
Iteration 1200: Loss = -12206.9638671875
Iteration 1300: Loss = -12201.875
Iteration 1400: Loss = -12197.76953125
Iteration 1500: Loss = -12194.408203125
Iteration 1600: Loss = -12191.6123046875
Iteration 1700: Loss = -12189.2626953125
Iteration 1800: Loss = -12187.2666015625
Iteration 1900: Loss = -12185.5546875
Iteration 2000: Loss = -12184.07421875
Iteration 2100: Loss = -12182.78515625
Iteration 2200: Loss = -12181.654296875
Iteration 2300: Loss = -12180.65625
Iteration 2400: Loss = -12179.7724609375
Iteration 2500: Loss = -12178.9833984375
Iteration 2600: Loss = -12178.2763671875
Iteration 2700: Loss = -12177.642578125
Iteration 2800: Loss = -12177.068359375
Iteration 2900: Loss = -12176.548828125
Iteration 3000: Loss = -12176.078125
Iteration 3100: Loss = -12175.6484375
Iteration 3200: Loss = -12175.255859375
Iteration 3300: Loss = -12174.8974609375
Iteration 3400: Loss = -12174.5673828125
Iteration 3500: Loss = -12174.263671875
Iteration 3600: Loss = -12173.9833984375
Iteration 3700: Loss = -12173.7255859375
Iteration 3800: Loss = -12173.4873046875
Iteration 3900: Loss = -12173.2646484375
Iteration 4000: Loss = -12173.060546875
Iteration 4100: Loss = -12172.869140625
Iteration 4200: Loss = -12172.69140625
Iteration 4300: Loss = -12172.5263671875
Iteration 4400: Loss = -12172.373046875
Iteration 4500: Loss = -12172.2265625
Iteration 4600: Loss = -12172.0927734375
Iteration 4700: Loss = -12171.9677734375
Iteration 4800: Loss = -12171.84765625
Iteration 4900: Loss = -12171.7373046875
Iteration 5000: Loss = -12171.6337890625
Iteration 5100: Loss = -12171.5380859375
Iteration 5200: Loss = -12171.4443359375
Iteration 5300: Loss = -12171.359375
Iteration 5400: Loss = -12171.27734375
Iteration 5500: Loss = -12171.203125
Iteration 5600: Loss = -12171.1298828125
Iteration 5700: Loss = -12171.064453125
Iteration 5800: Loss = -12171.0009765625
Iteration 5900: Loss = -12170.939453125
Iteration 6000: Loss = -12170.8818359375
Iteration 6100: Loss = -12170.830078125
Iteration 6200: Loss = -12170.779296875
Iteration 6300: Loss = -12170.732421875
Iteration 6400: Loss = -12170.6875
Iteration 6500: Loss = -12170.6455078125
Iteration 6600: Loss = -12170.6044921875
Iteration 6700: Loss = -12170.568359375
Iteration 6800: Loss = -12170.53125
Iteration 6900: Loss = -12170.4970703125
Iteration 7000: Loss = -12170.4658203125
Iteration 7100: Loss = -12170.4345703125
Iteration 7200: Loss = -12170.40625
Iteration 7300: Loss = -12170.37890625
Iteration 7400: Loss = -12170.349609375
Iteration 7500: Loss = -12170.3291015625
Iteration 7600: Loss = -12170.3046875
Iteration 7700: Loss = -12170.283203125
Iteration 7800: Loss = -12170.2626953125
Iteration 7900: Loss = -12170.2431640625
Iteration 8000: Loss = -12170.224609375
Iteration 8100: Loss = -12170.20703125
Iteration 8200: Loss = -12170.189453125
Iteration 8300: Loss = -12170.1748046875
Iteration 8400: Loss = -12170.16015625
Iteration 8500: Loss = -12170.1455078125
Iteration 8600: Loss = -12170.130859375
Iteration 8700: Loss = -12170.1181640625
Iteration 8800: Loss = -12170.10546875
Iteration 8900: Loss = -12170.095703125
Iteration 9000: Loss = -12170.08203125
Iteration 9100: Loss = -12170.07421875
Iteration 9200: Loss = -12170.064453125
Iteration 9300: Loss = -12170.0537109375
Iteration 9400: Loss = -12170.0439453125
Iteration 9500: Loss = -12170.0361328125
Iteration 9600: Loss = -12170.0263671875
Iteration 9700: Loss = -12170.021484375
Iteration 9800: Loss = -12170.013671875
Iteration 9900: Loss = -12170.005859375
Iteration 10000: Loss = -12170.0009765625
Iteration 10100: Loss = -12169.9931640625
Iteration 10200: Loss = -12169.986328125
Iteration 10300: Loss = -12169.982421875
Iteration 10400: Loss = -12169.978515625
Iteration 10500: Loss = -12169.97265625
Iteration 10600: Loss = -12169.9677734375
Iteration 10700: Loss = -12169.9609375
Iteration 10800: Loss = -12169.9580078125
Iteration 10900: Loss = -12169.953125
Iteration 11000: Loss = -12169.9501953125
Iteration 11100: Loss = -12169.947265625
Iteration 11200: Loss = -12169.9423828125
Iteration 11300: Loss = -12169.939453125
Iteration 11400: Loss = -12169.935546875
Iteration 11500: Loss = -12169.9326171875
Iteration 11600: Loss = -12169.9296875
Iteration 11700: Loss = -12169.9267578125
Iteration 11800: Loss = -12169.92578125
Iteration 11900: Loss = -12169.9228515625
Iteration 12000: Loss = -12169.91796875
Iteration 12100: Loss = -12169.9169921875
Iteration 12200: Loss = -12169.916015625
Iteration 12300: Loss = -12169.9130859375
Iteration 12400: Loss = -12169.912109375
Iteration 12500: Loss = -12169.9111328125
Iteration 12600: Loss = -12169.9091796875
Iteration 12700: Loss = -12169.9072265625
Iteration 12800: Loss = -12169.904296875
Iteration 12900: Loss = -12169.9033203125
Iteration 13000: Loss = -12169.9013671875
Iteration 13100: Loss = -12169.900390625
Iteration 13200: Loss = -12169.8984375
Iteration 13300: Loss = -12169.896484375
Iteration 13400: Loss = -12169.8955078125
Iteration 13500: Loss = -12169.8955078125
Iteration 13600: Loss = -12169.89453125
Iteration 13700: Loss = -12169.892578125
Iteration 13800: Loss = -12169.892578125
Iteration 13900: Loss = -12169.892578125
Iteration 14000: Loss = -12169.890625
Iteration 14100: Loss = -12169.8896484375
Iteration 14200: Loss = -12169.888671875
Iteration 14300: Loss = -12169.888671875
Iteration 14400: Loss = -12169.8876953125
Iteration 14500: Loss = -12169.8896484375
1
Iteration 14600: Loss = -12169.88671875
Iteration 14700: Loss = -12169.8857421875
Iteration 14800: Loss = -12169.888671875
1
Iteration 14900: Loss = -12169.88671875
2
Iteration 15000: Loss = -12169.884765625
Iteration 15100: Loss = -12169.8857421875
1
Iteration 15200: Loss = -12169.8837890625
Iteration 15300: Loss = -12169.8828125
Iteration 15400: Loss = -12169.884765625
1
Iteration 15500: Loss = -12169.8837890625
2
Iteration 15600: Loss = -12169.8837890625
3
Iteration 15700: Loss = -12169.8828125
Iteration 15800: Loss = -12169.8828125
Iteration 15900: Loss = -12169.8828125
Iteration 16000: Loss = -12169.8828125
Iteration 16100: Loss = -12169.8837890625
1
Iteration 16200: Loss = -12169.880859375
Iteration 16300: Loss = -12169.8818359375
1
Iteration 16400: Loss = -12169.880859375
Iteration 16500: Loss = -12169.8818359375
1
Iteration 16600: Loss = -12169.880859375
Iteration 16700: Loss = -12169.8798828125
Iteration 16800: Loss = -12169.8798828125
Iteration 16900: Loss = -12169.8818359375
1
Iteration 17000: Loss = -12169.8798828125
Iteration 17100: Loss = -12169.8798828125
Iteration 17200: Loss = -12169.87890625
Iteration 17300: Loss = -12169.8798828125
1
Iteration 17400: Loss = -12169.8828125
2
Iteration 17500: Loss = -12169.8779296875
Iteration 17600: Loss = -12169.8798828125
1
Iteration 17700: Loss = -12169.8779296875
Iteration 17800: Loss = -12169.87890625
1
Iteration 17900: Loss = -12169.87890625
2
Iteration 18000: Loss = -12169.87890625
3
Iteration 18100: Loss = -12169.87890625
4
Iteration 18200: Loss = -12169.8779296875
Iteration 18300: Loss = -12169.8779296875
Iteration 18400: Loss = -12169.8779296875
Iteration 18500: Loss = -12169.876953125
Iteration 18600: Loss = -12169.87890625
1
Iteration 18700: Loss = -12169.87890625
2
Iteration 18800: Loss = -12169.8779296875
3
Iteration 18900: Loss = -12169.87890625
4
Iteration 19000: Loss = -12169.8779296875
5
Iteration 19100: Loss = -12169.87890625
6
Iteration 19200: Loss = -12169.876953125
Iteration 19300: Loss = -12169.8779296875
1
Iteration 19400: Loss = -12169.8779296875
2
Iteration 19500: Loss = -12169.8779296875
3
Iteration 19600: Loss = -12169.8779296875
4
Iteration 19700: Loss = -12169.87890625
5
Iteration 19800: Loss = -12169.87890625
6
Iteration 19900: Loss = -12169.8779296875
7
Iteration 20000: Loss = -12169.8779296875
8
Iteration 20100: Loss = -12169.87890625
9
Iteration 20200: Loss = -12169.8779296875
10
Iteration 20300: Loss = -12169.8779296875
11
Iteration 20400: Loss = -12169.8779296875
12
Iteration 20500: Loss = -12169.8779296875
13
Iteration 20600: Loss = -12169.8779296875
14
Iteration 20700: Loss = -12169.87890625
15
Stopping early at iteration 20700 due to no improvement.
pi: tensor([[9.9999e-01, 1.2450e-05],
        [7.1320e-01, 2.8680e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 6.4611e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1938, 0.3191],
         [0.9905, 0.5747]],

        [[0.0905, 0.3617],
         [0.9879, 0.0526]],

        [[0.2312, 0.2402],
         [0.1227, 0.0770]],

        [[0.8350, 0.1543],
         [0.0144, 0.7135]],

        [[0.1897, 0.1197],
         [0.1264, 0.0077]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -49413.66796875
Iteration 100: Loss = -29960.390625
Iteration 200: Loss = -17468.599609375
Iteration 300: Loss = -13393.8701171875
Iteration 400: Loss = -12603.271484375
Iteration 500: Loss = -12413.1328125
Iteration 600: Loss = -12337.837890625
Iteration 700: Loss = -12295.6982421875
Iteration 800: Loss = -12268.267578125
Iteration 900: Loss = -12249.1357421875
Iteration 1000: Loss = -12235.1884765625
Iteration 1100: Loss = -12224.6767578125
Iteration 1200: Loss = -12216.5341796875
Iteration 1300: Loss = -12210.0849609375
Iteration 1400: Loss = -12204.8779296875
Iteration 1500: Loss = -12200.611328125
Iteration 1600: Loss = -12197.0673828125
Iteration 1700: Loss = -12194.087890625
Iteration 1800: Loss = -12191.5546875
Iteration 1900: Loss = -12189.3828125
Iteration 2000: Loss = -12187.5068359375
Iteration 2100: Loss = -12185.8740234375
Iteration 2200: Loss = -12184.44140625
Iteration 2300: Loss = -12183.1787109375
Iteration 2400: Loss = -12182.0595703125
Iteration 2500: Loss = -12181.0625
Iteration 2600: Loss = -12180.171875
Iteration 2700: Loss = -12179.3720703125
Iteration 2800: Loss = -12178.650390625
Iteration 2900: Loss = -12177.9951171875
Iteration 3000: Loss = -12177.4033203125
Iteration 3100: Loss = -12176.86328125
Iteration 3200: Loss = -12176.3720703125
Iteration 3300: Loss = -12175.923828125
Iteration 3400: Loss = -12175.5087890625
Iteration 3500: Loss = -12175.130859375
Iteration 3600: Loss = -12174.7802734375
Iteration 3700: Loss = -12174.4599609375
Iteration 3800: Loss = -12174.162109375
Iteration 3900: Loss = -12173.8876953125
Iteration 4000: Loss = -12173.6328125
Iteration 4100: Loss = -12173.3955078125
Iteration 4200: Loss = -12173.1748046875
Iteration 4300: Loss = -12172.970703125
Iteration 4400: Loss = -12172.78125
Iteration 4500: Loss = -12172.6044921875
Iteration 4600: Loss = -12172.4384765625
Iteration 4700: Loss = -12172.283203125
Iteration 4800: Loss = -12172.1376953125
Iteration 4900: Loss = -12172.001953125
Iteration 5000: Loss = -12171.8759765625
Iteration 5100: Loss = -12171.7578125
Iteration 5200: Loss = -12171.646484375
Iteration 5300: Loss = -12171.5419921875
Iteration 5400: Loss = -12171.4443359375
Iteration 5500: Loss = -12171.3515625
Iteration 5600: Loss = -12171.265625
Iteration 5700: Loss = -12171.185546875
Iteration 5800: Loss = -12171.107421875
Iteration 5900: Loss = -12171.0361328125
Iteration 6000: Loss = -12170.96875
Iteration 6100: Loss = -12170.904296875
Iteration 6200: Loss = -12170.8466796875
Iteration 6300: Loss = -12170.7890625
Iteration 6400: Loss = -12170.7373046875
Iteration 6500: Loss = -12170.6865234375
Iteration 6600: Loss = -12170.6396484375
Iteration 6700: Loss = -12170.59375
Iteration 6800: Loss = -12170.552734375
Iteration 6900: Loss = -12170.513671875
Iteration 7000: Loss = -12170.4755859375
Iteration 7100: Loss = -12170.4404296875
Iteration 7200: Loss = -12170.40625
Iteration 7300: Loss = -12170.3740234375
Iteration 7400: Loss = -12170.34375
Iteration 7500: Loss = -12170.314453125
Iteration 7600: Loss = -12170.2890625
Iteration 7700: Loss = -12170.2626953125
Iteration 7800: Loss = -12170.2392578125
Iteration 7900: Loss = -12170.2158203125
Iteration 8000: Loss = -12170.1943359375
Iteration 8100: Loss = -12170.173828125
Iteration 8200: Loss = -12170.154296875
Iteration 8300: Loss = -12170.134765625
Iteration 8400: Loss = -12170.1181640625
Iteration 8500: Loss = -12170.099609375
Iteration 8600: Loss = -12170.0830078125
Iteration 8700: Loss = -12170.0654296875
Iteration 8800: Loss = -12170.0517578125
Iteration 8900: Loss = -12170.0361328125
Iteration 9000: Loss = -12170.0205078125
Iteration 9100: Loss = -12170.0048828125
Iteration 9200: Loss = -12169.9931640625
Iteration 9300: Loss = -12169.98046875
Iteration 9400: Loss = -12169.966796875
Iteration 9500: Loss = -12169.9541015625
Iteration 9600: Loss = -12169.9453125
Iteration 9700: Loss = -12169.93359375
Iteration 9800: Loss = -12169.9228515625
Iteration 9900: Loss = -12169.9140625
Iteration 10000: Loss = -12169.9033203125
Iteration 10100: Loss = -12169.89453125
Iteration 10200: Loss = -12169.88671875
Iteration 10300: Loss = -12169.8779296875
Iteration 10400: Loss = -12169.869140625
Iteration 10500: Loss = -12169.861328125
Iteration 10600: Loss = -12169.853515625
Iteration 10700: Loss = -12169.84375
Iteration 10800: Loss = -12169.8359375
Iteration 10900: Loss = -12169.8271484375
Iteration 11000: Loss = -12169.8193359375
Iteration 11100: Loss = -12169.8115234375
Iteration 11200: Loss = -12169.8046875
Iteration 11300: Loss = -12169.7978515625
Iteration 11400: Loss = -12169.791015625
Iteration 11500: Loss = -12169.783203125
Iteration 11600: Loss = -12169.7763671875
Iteration 11700: Loss = -12169.771484375
Iteration 11800: Loss = -12169.765625
Iteration 11900: Loss = -12169.759765625
Iteration 12000: Loss = -12169.7548828125
Iteration 12100: Loss = -12169.75
Iteration 12200: Loss = -12169.7451171875
Iteration 12300: Loss = -12169.7392578125
Iteration 12400: Loss = -12169.7353515625
Iteration 12500: Loss = -12169.73046875
Iteration 12600: Loss = -12169.724609375
Iteration 12700: Loss = -12169.720703125
Iteration 12800: Loss = -12169.7158203125
Iteration 12900: Loss = -12169.7109375
Iteration 13000: Loss = -12169.705078125
Iteration 13100: Loss = -12169.7001953125
Iteration 13200: Loss = -12169.693359375
Iteration 13300: Loss = -12169.6884765625
Iteration 13400: Loss = -12169.677734375
Iteration 13500: Loss = -12169.671875
Iteration 13600: Loss = -12169.6640625
Iteration 13700: Loss = -12169.6513671875
Iteration 13800: Loss = -12169.6396484375
Iteration 13900: Loss = -12169.623046875
Iteration 14000: Loss = -12169.6005859375
Iteration 14100: Loss = -12169.5673828125
Iteration 14200: Loss = -12169.265625
Iteration 14300: Loss = -12168.9775390625
Iteration 14400: Loss = -12168.6044921875
Iteration 14500: Loss = -12168.1318359375
Iteration 14600: Loss = -12167.8408203125
Iteration 14700: Loss = -12167.712890625
Iteration 14800: Loss = -12167.6171875
Iteration 14900: Loss = -12167.5205078125
Iteration 15000: Loss = -12167.44921875
Iteration 15100: Loss = -12167.4130859375
Iteration 15200: Loss = -12167.384765625
Iteration 15300: Loss = -12167.36328125
Iteration 15400: Loss = -12167.345703125
Iteration 15500: Loss = -12167.3359375
Iteration 15600: Loss = -12167.3310546875
Iteration 15700: Loss = -12167.3291015625
Iteration 15800: Loss = -12167.3271484375
Iteration 15900: Loss = -12167.3271484375
Iteration 16000: Loss = -12167.32421875
Iteration 16100: Loss = -12167.32421875
Iteration 16200: Loss = -12167.3232421875
Iteration 16300: Loss = -12167.3212890625
Iteration 16400: Loss = -12167.3212890625
Iteration 16500: Loss = -12167.3212890625
Iteration 16600: Loss = -12167.3193359375
Iteration 16700: Loss = -12167.3193359375
Iteration 16800: Loss = -12167.3193359375
Iteration 16900: Loss = -12167.3203125
1
Iteration 17000: Loss = -12167.3193359375
Iteration 17100: Loss = -12167.318359375
Iteration 17200: Loss = -12167.3203125
1
Iteration 17300: Loss = -12167.3193359375
2
Iteration 17400: Loss = -12167.3173828125
Iteration 17500: Loss = -12167.3193359375
1
Iteration 17600: Loss = -12167.31640625
Iteration 17700: Loss = -12167.318359375
1
Iteration 17800: Loss = -12167.3193359375
2
Iteration 17900: Loss = -12167.318359375
3
Iteration 18000: Loss = -12167.3212890625
4
Iteration 18100: Loss = -12167.3173828125
5
Iteration 18200: Loss = -12167.3203125
6
Iteration 18300: Loss = -12167.3193359375
7
Iteration 18400: Loss = -12167.318359375
8
Iteration 18500: Loss = -12167.3173828125
9
Iteration 18600: Loss = -12167.3173828125
10
Iteration 18700: Loss = -12167.3173828125
11
Iteration 18800: Loss = -12167.31640625
Iteration 18900: Loss = -12167.3173828125
1
Iteration 19000: Loss = -12167.31640625
Iteration 19100: Loss = -12167.3173828125
1
Iteration 19200: Loss = -12167.31640625
Iteration 19300: Loss = -12167.31640625
Iteration 19400: Loss = -12167.3173828125
1
Iteration 19500: Loss = -12167.31640625
Iteration 19600: Loss = -12167.3154296875
Iteration 19700: Loss = -12167.31640625
1
Iteration 19800: Loss = -12167.3173828125
2
Iteration 19900: Loss = -12167.31640625
3
Iteration 20000: Loss = -12167.31640625
4
Iteration 20100: Loss = -12167.3173828125
5
Iteration 20200: Loss = -12167.3154296875
Iteration 20300: Loss = -12167.31640625
1
Iteration 20400: Loss = -12167.31640625
2
Iteration 20500: Loss = -12167.31640625
3
Iteration 20600: Loss = -12167.31640625
4
Iteration 20700: Loss = -12167.31640625
5
Iteration 20800: Loss = -12167.3154296875
Iteration 20900: Loss = -12167.3173828125
1
Iteration 21000: Loss = -12167.31640625
2
Iteration 21100: Loss = -12167.3154296875
Iteration 21200: Loss = -12167.3154296875
Iteration 21300: Loss = -12167.314453125
Iteration 21400: Loss = -12167.3154296875
1
Iteration 21500: Loss = -12167.3154296875
2
Iteration 21600: Loss = -12167.31640625
3
Iteration 21700: Loss = -12167.31640625
4
Iteration 21800: Loss = -12167.3154296875
5
Iteration 21900: Loss = -12167.3173828125
6
Iteration 22000: Loss = -12167.31640625
7
Iteration 22100: Loss = -12167.3154296875
8
Iteration 22200: Loss = -12167.314453125
Iteration 22300: Loss = -12167.31640625
1
Iteration 22400: Loss = -12167.3154296875
2
Iteration 22500: Loss = -12167.3154296875
3
Iteration 22600: Loss = -12167.31640625
4
Iteration 22700: Loss = -12167.3154296875
5
Iteration 22800: Loss = -12167.3173828125
6
Iteration 22900: Loss = -12167.3154296875
7
Iteration 23000: Loss = -12167.314453125
Iteration 23100: Loss = -12167.31640625
1
Iteration 23200: Loss = -12167.3154296875
2
Iteration 23300: Loss = -12167.3154296875
3
Iteration 23400: Loss = -12167.31640625
4
Iteration 23500: Loss = -12167.3154296875
5
Iteration 23600: Loss = -12167.314453125
Iteration 23700: Loss = -12167.31640625
1
Iteration 23800: Loss = -12167.31640625
2
Iteration 23900: Loss = -12167.31640625
3
Iteration 24000: Loss = -12167.3154296875
4
Iteration 24100: Loss = -12167.3154296875
5
Iteration 24200: Loss = -12167.314453125
Iteration 24300: Loss = -12167.3173828125
1
Iteration 24400: Loss = -12167.3154296875
2
Iteration 24500: Loss = -12167.3154296875
3
Iteration 24600: Loss = -12167.314453125
Iteration 24700: Loss = -12167.3154296875
1
Iteration 24800: Loss = -12167.3154296875
2
Iteration 24900: Loss = -12167.31640625
3
Iteration 25000: Loss = -12167.3154296875
4
Iteration 25100: Loss = -12167.3154296875
5
Iteration 25200: Loss = -12167.3154296875
6
Iteration 25300: Loss = -12167.3154296875
7
Iteration 25400: Loss = -12167.31640625
8
Iteration 25500: Loss = -12167.3154296875
9
Iteration 25600: Loss = -12167.314453125
Iteration 25700: Loss = -12167.31640625
1
Iteration 25800: Loss = -12167.3154296875
2
Iteration 25900: Loss = -12167.31640625
3
Iteration 26000: Loss = -12167.3154296875
4
Iteration 26100: Loss = -12167.314453125
Iteration 26200: Loss = -12167.314453125
Iteration 26300: Loss = -12167.31640625
1
Iteration 26400: Loss = -12167.3154296875
2
Iteration 26500: Loss = -12167.3154296875
3
Iteration 26600: Loss = -12167.31640625
4
Iteration 26700: Loss = -12167.31640625
5
Iteration 26800: Loss = -12167.31640625
6
Iteration 26900: Loss = -12167.3134765625
Iteration 27000: Loss = -12167.3154296875
1
Iteration 27100: Loss = -12167.3154296875
2
Iteration 27200: Loss = -12167.31640625
3
Iteration 27300: Loss = -12167.31640625
4
Iteration 27400: Loss = -12167.31640625
5
Iteration 27500: Loss = -12167.3154296875
6
Iteration 27600: Loss = -12167.3154296875
7
Iteration 27700: Loss = -12167.31640625
8
Iteration 27800: Loss = -12167.31640625
9
Iteration 27900: Loss = -12167.31640625
10
Iteration 28000: Loss = -12167.3154296875
11
Iteration 28100: Loss = -12167.3154296875
12
Iteration 28200: Loss = -12167.3154296875
13
Iteration 28300: Loss = -12167.3154296875
14
Iteration 28400: Loss = -12167.3173828125
15
Stopping early at iteration 28400 due to no improvement.
pi: tensor([[9.5516e-01, 4.4843e-02],
        [9.9999e-01, 1.1343e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 9.1186e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1951, 0.2047],
         [0.9885, 0.2197]],

        [[0.0645, 0.2499],
         [0.8610, 0.0069]],

        [[0.9379, 0.2011],
         [0.3201, 0.9676]],

        [[0.6626, 0.1348],
         [0.7594, 0.9800]],

        [[0.7331, 0.1114],
         [0.9915, 0.0158]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.002056465888134684
Global Adjusted Rand Index: -0.0003140670332584823
Average Adjusted Rand Index: 0.0004112931776269368
[0.0, -0.0003140670332584823] [0.0, 0.0004112931776269368] [12169.87890625, 12167.3173828125]
-------------------------------------
This iteration is 33
True Objective function: Loss = -11753.646503650833
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26875.001953125
Iteration 100: Loss = -18706.279296875
Iteration 200: Loss = -14123.021484375
Iteration 300: Loss = -13119.953125
Iteration 400: Loss = -12783.3935546875
Iteration 500: Loss = -12652.0
Iteration 600: Loss = -12605.171875
Iteration 700: Loss = -12576.19921875
Iteration 800: Loss = -12554.3525390625
Iteration 900: Loss = -12533.5576171875
Iteration 1000: Loss = -12502.91796875
Iteration 1100: Loss = -12473.5
Iteration 1200: Loss = -12460.111328125
Iteration 1300: Loss = -12451.4423828125
Iteration 1400: Loss = -12443.9423828125
Iteration 1500: Loss = -12435.97265625
Iteration 1600: Loss = -12429.5595703125
Iteration 1700: Loss = -12423.513671875
Iteration 1800: Loss = -12417.50390625
Iteration 1900: Loss = -12410.7880859375
Iteration 2000: Loss = -12405.109375
Iteration 2100: Loss = -12399.8818359375
Iteration 2200: Loss = -12393.5166015625
Iteration 2300: Loss = -12384.544921875
Iteration 2400: Loss = -12377.1904296875
Iteration 2500: Loss = -12368.3076171875
Iteration 2600: Loss = -12358.64453125
Iteration 2700: Loss = -12348.6328125
Iteration 2800: Loss = -12338.1767578125
Iteration 2900: Loss = -12328.0205078125
Iteration 3000: Loss = -12309.791015625
Iteration 3100: Loss = -12297.943359375
Iteration 3200: Loss = -12277.8798828125
Iteration 3300: Loss = -12272.337890625
Iteration 3400: Loss = -12269.03125
Iteration 3500: Loss = -12266.8388671875
Iteration 3600: Loss = -12258.8935546875
Iteration 3700: Loss = -12257.1923828125
Iteration 3800: Loss = -12255.474609375
Iteration 3900: Loss = -12254.1328125
Iteration 4000: Loss = -12252.88671875
Iteration 4100: Loss = -12252.0625
Iteration 4200: Loss = -12251.390625
Iteration 4300: Loss = -12250.73828125
Iteration 4400: Loss = -12250.1220703125
Iteration 4500: Loss = -12248.62109375
Iteration 4600: Loss = -12242.81640625
Iteration 4700: Loss = -12220.84375
Iteration 4800: Loss = -12185.2626953125
Iteration 4900: Loss = -12167.064453125
Iteration 5000: Loss = -12147.8408203125
Iteration 5100: Loss = -12138.6435546875
Iteration 5200: Loss = -12130.4365234375
Iteration 5300: Loss = -12120.857421875
Iteration 5400: Loss = -12119.5751953125
Iteration 5500: Loss = -12095.5068359375
Iteration 5600: Loss = -12093.6171875
Iteration 5700: Loss = -12086.1416015625
Iteration 5800: Loss = -12065.2236328125
Iteration 5900: Loss = -12053.501953125
Iteration 6000: Loss = -12050.4736328125
Iteration 6100: Loss = -12031.740234375
Iteration 6200: Loss = -12031.2060546875
Iteration 6300: Loss = -12031.0078125
Iteration 6400: Loss = -12030.8837890625
Iteration 6500: Loss = -12030.7900390625
Iteration 6600: Loss = -12030.7197265625
Iteration 6700: Loss = -12030.658203125
Iteration 6800: Loss = -12030.6103515625
Iteration 6900: Loss = -12030.5673828125
Iteration 7000: Loss = -12030.529296875
Iteration 7100: Loss = -12030.498046875
Iteration 7200: Loss = -12030.46875
Iteration 7300: Loss = -12030.443359375
Iteration 7400: Loss = -12030.419921875
Iteration 7500: Loss = -12030.3984375
Iteration 7600: Loss = -12030.3798828125
Iteration 7700: Loss = -12030.3623046875
Iteration 7800: Loss = -12030.34765625
Iteration 7900: Loss = -12030.33203125
Iteration 8000: Loss = -12030.318359375
Iteration 8100: Loss = -12030.306640625
Iteration 8200: Loss = -12030.294921875
Iteration 8300: Loss = -12030.2841796875
Iteration 8400: Loss = -12030.275390625
Iteration 8500: Loss = -12030.265625
Iteration 8600: Loss = -12030.2568359375
Iteration 8700: Loss = -12030.2470703125
Iteration 8800: Loss = -12030.240234375
Iteration 8900: Loss = -12030.2314453125
Iteration 9000: Loss = -12030.22265625
Iteration 9100: Loss = -12030.201171875
Iteration 9200: Loss = -12021.57421875
Iteration 9300: Loss = -12021.03125
Iteration 9400: Loss = -12020.98828125
Iteration 9500: Loss = -12020.96484375
Iteration 9600: Loss = -12020.9521484375
Iteration 9700: Loss = -12020.9423828125
Iteration 9800: Loss = -12020.9326171875
Iteration 9900: Loss = -12020.9267578125
Iteration 10000: Loss = -12020.919921875
Iteration 10100: Loss = -12020.916015625
Iteration 10200: Loss = -12020.91015625
Iteration 10300: Loss = -12020.9072265625
Iteration 10400: Loss = -12020.90234375
Iteration 10500: Loss = -12020.8994140625
Iteration 10600: Loss = -12020.8974609375
Iteration 10700: Loss = -12020.892578125
Iteration 10800: Loss = -12020.890625
Iteration 10900: Loss = -12020.8876953125
Iteration 11000: Loss = -12020.884765625
Iteration 11100: Loss = -12020.8828125
Iteration 11200: Loss = -12020.8818359375
Iteration 11300: Loss = -12020.8798828125
Iteration 11400: Loss = -12020.8779296875
Iteration 11500: Loss = -12020.8740234375
Iteration 11600: Loss = -12020.875
1
Iteration 11700: Loss = -12020.87109375
Iteration 11800: Loss = -12020.87109375
Iteration 11900: Loss = -12020.869140625
Iteration 12000: Loss = -12020.8671875
Iteration 12100: Loss = -12020.8671875
Iteration 12200: Loss = -12020.8662109375
Iteration 12300: Loss = -12020.86328125
Iteration 12400: Loss = -12020.86328125
Iteration 12500: Loss = -12020.86328125
Iteration 12600: Loss = -12020.8603515625
Iteration 12700: Loss = -12020.859375
Iteration 12800: Loss = -12020.859375
Iteration 12900: Loss = -12020.857421875
Iteration 13000: Loss = -12020.859375
1
Iteration 13100: Loss = -12020.8583984375
2
Iteration 13200: Loss = -12020.857421875
Iteration 13300: Loss = -12020.8564453125
Iteration 13400: Loss = -12020.85546875
Iteration 13500: Loss = -12020.85546875
Iteration 13600: Loss = -12020.8544921875
Iteration 13700: Loss = -12020.8544921875
Iteration 13800: Loss = -12020.8525390625
Iteration 13900: Loss = -12020.8544921875
1
Iteration 14000: Loss = -12020.8515625
Iteration 14100: Loss = -12020.8525390625
1
Iteration 14200: Loss = -12020.8515625
Iteration 14300: Loss = -12020.8505859375
Iteration 14400: Loss = -12020.8505859375
Iteration 14500: Loss = -12020.8505859375
Iteration 14600: Loss = -12020.849609375
Iteration 14700: Loss = -12020.849609375
Iteration 14800: Loss = -12020.849609375
Iteration 14900: Loss = -12020.849609375
Iteration 15000: Loss = -12020.849609375
Iteration 15100: Loss = -12020.84765625
Iteration 15200: Loss = -12018.650390625
Iteration 15300: Loss = -12015.943359375
Iteration 15400: Loss = -12015.9296875
Iteration 15500: Loss = -12015.9248046875
Iteration 15600: Loss = -12015.921875
Iteration 15700: Loss = -12015.9208984375
Iteration 15800: Loss = -12015.919921875
Iteration 15900: Loss = -12015.9189453125
Iteration 16000: Loss = -12015.9189453125
Iteration 16100: Loss = -12015.91796875
Iteration 16200: Loss = -12015.91796875
Iteration 16300: Loss = -12015.91796875
Iteration 16400: Loss = -12015.91796875
Iteration 16500: Loss = -12015.916015625
Iteration 16600: Loss = -12015.9169921875
1
Iteration 16700: Loss = -12015.9169921875
2
Iteration 16800: Loss = -12015.91796875
3
Iteration 16900: Loss = -12015.91796875
4
Iteration 17000: Loss = -12015.9169921875
5
Iteration 17100: Loss = -12015.916015625
Iteration 17200: Loss = -12015.91796875
1
Iteration 17300: Loss = -12015.9169921875
2
Iteration 17400: Loss = -12015.9169921875
3
Iteration 17500: Loss = -12015.9150390625
Iteration 17600: Loss = -12015.9169921875
1
Iteration 17700: Loss = -12015.9169921875
2
Iteration 17800: Loss = -12015.916015625
3
Iteration 17900: Loss = -12015.9169921875
4
Iteration 18000: Loss = -12015.916015625
5
Iteration 18100: Loss = -12015.916015625
6
Iteration 18200: Loss = -12015.9150390625
Iteration 18300: Loss = -12015.916015625
1
Iteration 18400: Loss = -12015.9169921875
2
Iteration 18500: Loss = -12015.9150390625
Iteration 18600: Loss = -12015.916015625
1
Iteration 18700: Loss = -12015.9150390625
Iteration 18800: Loss = -12015.916015625
1
Iteration 18900: Loss = -12015.9169921875
2
Iteration 19000: Loss = -12015.916015625
3
Iteration 19100: Loss = -12015.9140625
Iteration 19200: Loss = -12014.5009765625
Iteration 19300: Loss = -12014.49609375
Iteration 19400: Loss = -12014.498046875
1
Iteration 19500: Loss = -12014.49609375
Iteration 19600: Loss = -12014.4970703125
1
Iteration 19700: Loss = -12014.49609375
Iteration 19800: Loss = -12014.4970703125
1
Iteration 19900: Loss = -12014.49609375
Iteration 20000: Loss = -12014.4970703125
1
Iteration 20100: Loss = -12014.49609375
Iteration 20200: Loss = -12014.49609375
Iteration 20300: Loss = -12014.498046875
1
Iteration 20400: Loss = -12014.49609375
Iteration 20500: Loss = -12014.49609375
Iteration 20600: Loss = -12014.49609375
Iteration 20700: Loss = -12014.49609375
Iteration 20800: Loss = -12014.49609375
Iteration 20900: Loss = -12014.49609375
Iteration 21000: Loss = -12014.49609375
Iteration 21100: Loss = -12014.49609375
Iteration 21200: Loss = -12014.49609375
Iteration 21300: Loss = -12014.49609375
Iteration 21400: Loss = -12014.49609375
Iteration 21500: Loss = -12014.49609375
Iteration 21600: Loss = -12014.4970703125
1
Iteration 21700: Loss = -12014.4970703125
2
Iteration 21800: Loss = -12014.4951171875
Iteration 21900: Loss = -12014.4951171875
Iteration 22000: Loss = -12014.49609375
1
Iteration 22100: Loss = -12014.4951171875
Iteration 22200: Loss = -12014.4970703125
1
Iteration 22300: Loss = -12014.49609375
2
Iteration 22400: Loss = -12014.49609375
3
Iteration 22500: Loss = -12014.4951171875
Iteration 22600: Loss = -12014.4970703125
1
Iteration 22700: Loss = -12014.4951171875
Iteration 22800: Loss = -12014.49609375
1
Iteration 22900: Loss = -12014.4951171875
Iteration 23000: Loss = -12014.4951171875
Iteration 23100: Loss = -12014.4951171875
Iteration 23200: Loss = -12014.4951171875
Iteration 23300: Loss = -12014.498046875
1
Iteration 23400: Loss = -12014.494140625
Iteration 23500: Loss = -12014.49609375
1
Iteration 23600: Loss = -12014.49609375
2
Iteration 23700: Loss = -12014.4970703125
3
Iteration 23800: Loss = -12014.49609375
4
Iteration 23900: Loss = -12014.49609375
5
Iteration 24000: Loss = -12014.4970703125
6
Iteration 24100: Loss = -12014.49609375
7
Iteration 24200: Loss = -12014.49609375
8
Iteration 24300: Loss = -12014.49609375
9
Iteration 24400: Loss = -12014.494140625
Iteration 24500: Loss = -12014.49609375
1
Iteration 24600: Loss = -12014.380859375
Iteration 24700: Loss = -12012.853515625
Iteration 24800: Loss = -12006.1904296875
Iteration 24900: Loss = -11999.255859375
Iteration 25000: Loss = -11998.2529296875
Iteration 25100: Loss = -11996.490234375
Iteration 25200: Loss = -11995.7646484375
Iteration 25300: Loss = -11995.6669921875
Iteration 25400: Loss = -11995.666015625
Iteration 25500: Loss = -11995.2939453125
Iteration 25600: Loss = -11992.6552734375
Iteration 25700: Loss = -11992.6416015625
Iteration 25800: Loss = -11992.6396484375
Iteration 25900: Loss = -11992.642578125
1
Iteration 26000: Loss = -11992.6416015625
2
Iteration 26100: Loss = -11992.640625
3
Iteration 26200: Loss = -11992.638671875
Iteration 26300: Loss = -11992.638671875
Iteration 26400: Loss = -11992.6396484375
1
Iteration 26500: Loss = -11992.6337890625
Iteration 26600: Loss = -11992.6298828125
Iteration 26700: Loss = -11992.630859375
1
Iteration 26800: Loss = -11992.6279296875
Iteration 26900: Loss = -11992.560546875
Iteration 27000: Loss = -11992.5595703125
Iteration 27100: Loss = -11992.5595703125
Iteration 27200: Loss = -11992.5576171875
Iteration 27300: Loss = -11992.5595703125
1
Iteration 27400: Loss = -11992.5595703125
2
Iteration 27500: Loss = -11992.5595703125
3
Iteration 27600: Loss = -11992.55859375
4
Iteration 27700: Loss = -11992.1708984375
Iteration 27800: Loss = -11991.8125
Iteration 27900: Loss = -11991.8125
Iteration 28000: Loss = -11991.7763671875
Iteration 28100: Loss = -11991.7744140625
Iteration 28200: Loss = -11991.7734375
Iteration 28300: Loss = -11991.771484375
Iteration 28400: Loss = -11991.7724609375
1
Iteration 28500: Loss = -11991.771484375
Iteration 28600: Loss = -11991.771484375
Iteration 28700: Loss = -11991.7724609375
1
Iteration 28800: Loss = -11991.771484375
Iteration 28900: Loss = -11991.7705078125
Iteration 29000: Loss = -11991.7685546875
Iteration 29100: Loss = -11991.76953125
1
Iteration 29200: Loss = -11991.7685546875
Iteration 29300: Loss = -11991.76953125
1
Iteration 29400: Loss = -11991.7685546875
Iteration 29500: Loss = -11991.7685546875
Iteration 29600: Loss = -11991.76953125
1
Iteration 29700: Loss = -11991.76953125
2
Iteration 29800: Loss = -11991.7685546875
Iteration 29900: Loss = -11991.7685546875
pi: tensor([[0.2127, 0.7873],
        [0.5155, 0.4845]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4827, 0.5173], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2961, 0.0914],
         [0.5798, 0.2636]],

        [[0.9865, 0.1047],
         [0.9638, 0.3913]],

        [[0.5965, 0.0982],
         [0.2003, 0.6524]],

        [[0.9882, 0.1888],
         [0.9587, 0.0234]],

        [[0.4899, 0.0898],
         [0.4895, 0.9455]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.0017656651454180695
time is 4
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: -0.001698576894490235
Average Adjusted Rand Index: 0.7836459892980424
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -41837.23828125
Iteration 100: Loss = -27452.759765625
Iteration 200: Loss = -16637.041015625
Iteration 300: Loss = -13304.828125
Iteration 400: Loss = -12790.4736328125
Iteration 500: Loss = -12648.3310546875
Iteration 600: Loss = -12554.255859375
Iteration 700: Loss = -12503.615234375
Iteration 800: Loss = -12464.9775390625
Iteration 900: Loss = -12430.34375
Iteration 1000: Loss = -12413.1689453125
Iteration 1100: Loss = -12398.515625
Iteration 1200: Loss = -12389.419921875
Iteration 1300: Loss = -12379.6708984375
Iteration 1400: Loss = -12373.4345703125
Iteration 1500: Loss = -12368.646484375
Iteration 1600: Loss = -12362.6611328125
Iteration 1700: Loss = -12358.6044921875
Iteration 1800: Loss = -12351.1494140625
Iteration 1900: Loss = -12347.66796875
Iteration 2000: Loss = -12345.5703125
Iteration 2100: Loss = -12343.890625
Iteration 2200: Loss = -12342.4775390625
Iteration 2300: Loss = -12341.2744140625
Iteration 2400: Loss = -12340.2255859375
Iteration 2500: Loss = -12339.2998046875
Iteration 2600: Loss = -12338.474609375
Iteration 2700: Loss = -12337.7333984375
Iteration 2800: Loss = -12337.0673828125
Iteration 2900: Loss = -12336.4755859375
Iteration 3000: Loss = -12335.94140625
Iteration 3100: Loss = -12335.4560546875
Iteration 3200: Loss = -12335.015625
Iteration 3300: Loss = -12334.6123046875
Iteration 3400: Loss = -12334.244140625
Iteration 3500: Loss = -12333.90625
Iteration 3600: Loss = -12333.5966796875
Iteration 3700: Loss = -12333.30859375
Iteration 3800: Loss = -12333.0439453125
Iteration 3900: Loss = -12332.80078125
Iteration 4000: Loss = -12332.5732421875
Iteration 4100: Loss = -12332.3623046875
Iteration 4200: Loss = -12332.1669921875
Iteration 4300: Loss = -12331.9853515625
Iteration 4400: Loss = -12331.81640625
Iteration 4500: Loss = -12331.6572265625
Iteration 4600: Loss = -12331.509765625
Iteration 4700: Loss = -12331.37109375
Iteration 4800: Loss = -12331.2412109375
Iteration 4900: Loss = -12331.1201171875
Iteration 5000: Loss = -12331.005859375
Iteration 5100: Loss = -12330.900390625
Iteration 5200: Loss = -12330.7998046875
Iteration 5300: Loss = -12330.7060546875
Iteration 5400: Loss = -12330.6181640625
Iteration 5500: Loss = -12330.53515625
Iteration 5600: Loss = -12330.458984375
Iteration 5700: Loss = -12330.3837890625
Iteration 5800: Loss = -12330.314453125
Iteration 5900: Loss = -12330.24609375
Iteration 6000: Loss = -12330.1865234375
Iteration 6100: Loss = -12330.125
Iteration 6200: Loss = -12329.9287109375
Iteration 6300: Loss = -12325.1767578125
Iteration 6400: Loss = -12324.970703125
Iteration 6500: Loss = -12324.853515625
Iteration 6600: Loss = -12324.7646484375
Iteration 6700: Loss = -12324.6904296875
Iteration 6800: Loss = -12324.626953125
Iteration 6900: Loss = -12324.5693359375
Iteration 7000: Loss = -12324.5166015625
Iteration 7100: Loss = -12324.4697265625
Iteration 7200: Loss = -12324.4267578125
Iteration 7300: Loss = -12324.38671875
Iteration 7400: Loss = -12324.349609375
Iteration 7500: Loss = -12324.314453125
Iteration 7600: Loss = -12324.2802734375
Iteration 7700: Loss = -12324.2509765625
Iteration 7800: Loss = -12324.22265625
Iteration 7900: Loss = -12324.1943359375
Iteration 8000: Loss = -12324.169921875
Iteration 8100: Loss = -12324.1455078125
Iteration 8200: Loss = -12324.1220703125
Iteration 8300: Loss = -12324.1015625
Iteration 8400: Loss = -12324.0810546875
Iteration 8500: Loss = -12324.0634765625
Iteration 8600: Loss = -12324.0439453125
Iteration 8700: Loss = -12324.0263671875
Iteration 8800: Loss = -12324.0126953125
Iteration 8900: Loss = -12323.998046875
Iteration 9000: Loss = -12323.982421875
Iteration 9100: Loss = -12323.970703125
Iteration 9200: Loss = -12323.9560546875
Iteration 9300: Loss = -12323.9423828125
Iteration 9400: Loss = -12323.9326171875
Iteration 9500: Loss = -12323.9208984375
Iteration 9600: Loss = -12323.91015625
Iteration 9700: Loss = -12323.9013671875
Iteration 9800: Loss = -12323.890625
Iteration 9900: Loss = -12323.8837890625
Iteration 10000: Loss = -12323.875
Iteration 10100: Loss = -12323.8662109375
Iteration 10200: Loss = -12323.8583984375
Iteration 10300: Loss = -12323.8525390625
Iteration 10400: Loss = -12323.8447265625
Iteration 10500: Loss = -12323.837890625
Iteration 10600: Loss = -12323.83203125
Iteration 10700: Loss = -12323.8271484375
Iteration 10800: Loss = -12323.8203125
Iteration 10900: Loss = -12323.8154296875
Iteration 11000: Loss = -12323.8095703125
Iteration 11100: Loss = -12323.8056640625
Iteration 11200: Loss = -12323.802734375
Iteration 11300: Loss = -12323.7978515625
Iteration 11400: Loss = -12323.7939453125
Iteration 11500: Loss = -12323.7890625
Iteration 11600: Loss = -12323.78515625
Iteration 11700: Loss = -12323.783203125
Iteration 11800: Loss = -12323.779296875
Iteration 11900: Loss = -12323.7744140625
Iteration 12000: Loss = -12323.7734375
Iteration 12100: Loss = -12323.76953125
Iteration 12200: Loss = -12323.767578125
Iteration 12300: Loss = -12323.7646484375
Iteration 12400: Loss = -12323.7626953125
Iteration 12500: Loss = -12323.7587890625
Iteration 12600: Loss = -12323.7587890625
Iteration 12700: Loss = -12323.755859375
Iteration 12800: Loss = -12323.75390625
Iteration 12900: Loss = -12323.7509765625
Iteration 13000: Loss = -12323.75
Iteration 13100: Loss = -12323.7470703125
Iteration 13200: Loss = -12323.74609375
Iteration 13300: Loss = -12323.744140625
Iteration 13400: Loss = -12323.7451171875
1
Iteration 13500: Loss = -12323.7431640625
Iteration 13600: Loss = -12323.7412109375
Iteration 13700: Loss = -12323.73828125
Iteration 13800: Loss = -12323.73828125
Iteration 13900: Loss = -12323.7373046875
Iteration 14000: Loss = -12323.7353515625
Iteration 14100: Loss = -12323.7353515625
Iteration 14200: Loss = -12323.7353515625
Iteration 14300: Loss = -12323.734375
Iteration 14400: Loss = -12323.732421875
Iteration 14500: Loss = -12323.7333984375
1
Iteration 14600: Loss = -12323.732421875
Iteration 14700: Loss = -12323.7294921875
Iteration 14800: Loss = -12323.73828125
1
Iteration 14900: Loss = -12323.7275390625
Iteration 15000: Loss = -12323.7275390625
Iteration 15100: Loss = -12323.728515625
1
Iteration 15200: Loss = -12323.7255859375
Iteration 15300: Loss = -12323.7275390625
1
Iteration 15400: Loss = -12323.7255859375
Iteration 15500: Loss = -12323.7265625
1
Iteration 15600: Loss = -12323.7255859375
Iteration 15700: Loss = -12323.7255859375
Iteration 15800: Loss = -12323.7236328125
Iteration 15900: Loss = -12323.7236328125
Iteration 16000: Loss = -12323.724609375
1
Iteration 16100: Loss = -12323.7236328125
Iteration 16200: Loss = -12323.7216796875
Iteration 16300: Loss = -12323.7236328125
1
Iteration 16400: Loss = -12323.7236328125
2
Iteration 16500: Loss = -12323.72265625
3
Iteration 16600: Loss = -12323.72265625
4
Iteration 16700: Loss = -12323.7216796875
Iteration 16800: Loss = -12323.7216796875
Iteration 16900: Loss = -12323.7216796875
Iteration 17000: Loss = -12323.7216796875
Iteration 17100: Loss = -12323.72265625
1
Iteration 17200: Loss = -12323.7216796875
Iteration 17300: Loss = -12323.72265625
1
Iteration 17400: Loss = -12323.7197265625
Iteration 17500: Loss = -12323.720703125
1
Iteration 17600: Loss = -12323.7197265625
Iteration 17700: Loss = -12323.720703125
1
Iteration 17800: Loss = -12323.720703125
2
Iteration 17900: Loss = -12323.720703125
3
Iteration 18000: Loss = -12323.7177734375
Iteration 18100: Loss = -12323.7197265625
1
Iteration 18200: Loss = -12323.7197265625
2
Iteration 18300: Loss = -12323.7197265625
3
Iteration 18400: Loss = -12323.7177734375
Iteration 18500: Loss = -12323.7197265625
1
Iteration 18600: Loss = -12323.72265625
2
Iteration 18700: Loss = -12323.7197265625
3
Iteration 18800: Loss = -12323.7197265625
4
Iteration 18900: Loss = -12323.7197265625
5
Iteration 19000: Loss = -12323.71875
6
Iteration 19100: Loss = -12323.71875
7
Iteration 19200: Loss = -12323.71875
8
Iteration 19300: Loss = -12323.71875
9
Iteration 19400: Loss = -12323.71875
10
Iteration 19500: Loss = -12323.71875
11
Iteration 19600: Loss = -12323.7177734375
Iteration 19700: Loss = -12323.71875
1
Iteration 19800: Loss = -12323.7177734375
Iteration 19900: Loss = -12323.7197265625
1
Iteration 20000: Loss = -12323.7177734375
Iteration 20100: Loss = -12323.7197265625
1
Iteration 20200: Loss = -12323.71875
2
Iteration 20300: Loss = -12323.71875
3
Iteration 20400: Loss = -12323.71875
4
Iteration 20500: Loss = -12323.7177734375
Iteration 20600: Loss = -12323.71875
1
Iteration 20700: Loss = -12323.71875
2
Iteration 20800: Loss = -12323.71875
3
Iteration 20900: Loss = -12323.71875
4
Iteration 21000: Loss = -12323.71875
5
Iteration 21100: Loss = -12323.71875
6
Iteration 21200: Loss = -12323.71875
7
Iteration 21300: Loss = -12323.7197265625
8
Iteration 21400: Loss = -12323.7177734375
Iteration 21500: Loss = -12323.7177734375
Iteration 21600: Loss = -12323.7177734375
Iteration 21700: Loss = -12323.7197265625
1
Iteration 21800: Loss = -12323.7177734375
Iteration 21900: Loss = -12323.7177734375
Iteration 22000: Loss = -12323.7177734375
Iteration 22100: Loss = -12323.7177734375
Iteration 22200: Loss = -12323.7177734375
Iteration 22300: Loss = -12323.7177734375
Iteration 22400: Loss = -12323.7177734375
Iteration 22500: Loss = -12323.7177734375
Iteration 22600: Loss = -12323.716796875
Iteration 22700: Loss = -12323.7177734375
1
Iteration 22800: Loss = -12323.716796875
Iteration 22900: Loss = -12323.71875
1
Iteration 23000: Loss = -12323.7177734375
2
Iteration 23100: Loss = -12323.7177734375
3
Iteration 23200: Loss = -12323.716796875
Iteration 23300: Loss = -12323.7177734375
1
Iteration 23400: Loss = -12323.7177734375
2
Iteration 23500: Loss = -12323.7197265625
3
Iteration 23600: Loss = -12323.7177734375
4
Iteration 23700: Loss = -12323.7177734375
5
Iteration 23800: Loss = -12323.71875
6
Iteration 23900: Loss = -12323.7177734375
7
Iteration 24000: Loss = -12323.7177734375
8
Iteration 24100: Loss = -12323.7177734375
9
Iteration 24200: Loss = -12323.7197265625
10
Iteration 24300: Loss = -12323.7177734375
11
Iteration 24400: Loss = -12323.7158203125
Iteration 24500: Loss = -12323.716796875
1
Iteration 24600: Loss = -12323.71875
2
Iteration 24700: Loss = -12323.7177734375
3
Iteration 24800: Loss = -12323.716796875
4
Iteration 24900: Loss = -12323.7177734375
5
Iteration 25000: Loss = -12323.7177734375
6
Iteration 25100: Loss = -12323.7177734375
7
Iteration 25200: Loss = -12323.7197265625
8
Iteration 25300: Loss = -12323.7177734375
9
Iteration 25400: Loss = -12323.71875
10
Iteration 25500: Loss = -12323.7197265625
11
Iteration 25600: Loss = -12323.7177734375
12
Iteration 25700: Loss = -12323.71875
13
Iteration 25800: Loss = -12323.71875
14
Iteration 25900: Loss = -12323.7177734375
15
Stopping early at iteration 25900 due to no improvement.
pi: tensor([[9.9999e-01, 5.4184e-06],
        [9.9001e-01, 9.9866e-03]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 3.3745e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1982, 0.2113],
         [0.0081, 0.6225]],

        [[0.9920, 0.2116],
         [0.9910, 0.9902]],

        [[0.6901, 0.2677],
         [0.0799, 0.0844]],

        [[0.9412, 0.2197],
         [0.0511, 0.8008]],

        [[0.4676, 0.1630],
         [0.8492, 0.9346]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[-0.001698576894490235, 0.0] [0.7836459892980424, 0.0] [11991.7685546875, 12323.7177734375]
-------------------------------------
This iteration is 34
True Objective function: Loss = -11944.854408844172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22032.98046875
Iteration 100: Loss = -15745.998046875
Iteration 200: Loss = -13338.5390625
Iteration 300: Loss = -12787.7607421875
Iteration 400: Loss = -12688.521484375
Iteration 500: Loss = -12655.302734375
Iteration 600: Loss = -12632.9296875
Iteration 700: Loss = -12614.0703125
Iteration 800: Loss = -12599.9033203125
Iteration 900: Loss = -12589.1953125
Iteration 1000: Loss = -12581.4921875
Iteration 1100: Loss = -12573.3349609375
Iteration 1200: Loss = -12563.0751953125
Iteration 1300: Loss = -12550.4619140625
Iteration 1400: Loss = -12537.09375
Iteration 1500: Loss = -12520.560546875
Iteration 1600: Loss = -12506.01171875
Iteration 1700: Loss = -12492.419921875
Iteration 1800: Loss = -12485.8037109375
Iteration 1900: Loss = -12481.498046875
Iteration 2000: Loss = -12478.203125
Iteration 2100: Loss = -12476.5263671875
Iteration 2200: Loss = -12475.4306640625
Iteration 2300: Loss = -12474.6220703125
Iteration 2400: Loss = -12473.9990234375
Iteration 2500: Loss = -12473.4951171875
Iteration 2600: Loss = -12473.0791015625
Iteration 2700: Loss = -12472.7314453125
Iteration 2800: Loss = -12472.435546875
Iteration 2900: Loss = -12472.1787109375
Iteration 3000: Loss = -12471.9580078125
Iteration 3100: Loss = -12471.763671875
Iteration 3200: Loss = -12471.5908203125
Iteration 3300: Loss = -12471.435546875
Iteration 3400: Loss = -12471.294921875
Iteration 3500: Loss = -12471.171875
Iteration 3600: Loss = -12471.0625
Iteration 3700: Loss = -12470.9619140625
Iteration 3800: Loss = -12470.87109375
Iteration 3900: Loss = -12470.7880859375
Iteration 4000: Loss = -12470.7119140625
Iteration 4100: Loss = -12470.6416015625
Iteration 4200: Loss = -12470.5771484375
Iteration 4300: Loss = -12470.517578125
Iteration 4400: Loss = -12470.4638671875
Iteration 4500: Loss = -12470.4130859375
Iteration 4600: Loss = -12470.3671875
Iteration 4700: Loss = -12470.3193359375
Iteration 4800: Loss = -12470.2802734375
Iteration 4900: Loss = -12470.2392578125
Iteration 5000: Loss = -12470.201171875
Iteration 5100: Loss = -12470.1640625
Iteration 5200: Loss = -12470.12890625
Iteration 5300: Loss = -12470.0966796875
Iteration 5400: Loss = -12470.0625
Iteration 5500: Loss = -12470.0302734375
Iteration 5600: Loss = -12470.0009765625
Iteration 5700: Loss = -12469.9736328125
Iteration 5800: Loss = -12469.9501953125
Iteration 5900: Loss = -12469.9287109375
Iteration 6000: Loss = -12469.908203125
Iteration 6100: Loss = -12469.8896484375
Iteration 6200: Loss = -12469.875
Iteration 6300: Loss = -12469.8603515625
Iteration 6400: Loss = -12469.849609375
Iteration 6500: Loss = -12469.837890625
Iteration 6600: Loss = -12469.828125
Iteration 6700: Loss = -12469.818359375
Iteration 6800: Loss = -12469.810546875
Iteration 6900: Loss = -12469.8037109375
Iteration 7000: Loss = -12469.7958984375
Iteration 7100: Loss = -12469.7890625
Iteration 7200: Loss = -12469.783203125
Iteration 7300: Loss = -12469.77734375
Iteration 7400: Loss = -12469.7724609375
Iteration 7500: Loss = -12469.765625
Iteration 7600: Loss = -12469.76171875
Iteration 7700: Loss = -12469.7568359375
Iteration 7800: Loss = -12469.751953125
Iteration 7900: Loss = -12469.7451171875
Iteration 8000: Loss = -12469.7412109375
Iteration 8100: Loss = -12469.73828125
Iteration 8200: Loss = -12469.7333984375
Iteration 8300: Loss = -12469.7275390625
Iteration 8400: Loss = -12469.7236328125
Iteration 8500: Loss = -12469.7216796875
Iteration 8600: Loss = -12469.716796875
Iteration 8700: Loss = -12469.712890625
Iteration 8800: Loss = -12469.708984375
Iteration 8900: Loss = -12469.7041015625
Iteration 9000: Loss = -12469.701171875
Iteration 9100: Loss = -12469.697265625
Iteration 9200: Loss = -12469.6923828125
Iteration 9300: Loss = -12469.6865234375
Iteration 9400: Loss = -12469.6806640625
Iteration 9500: Loss = -12469.6728515625
Iteration 9600: Loss = -12469.6669921875
Iteration 9700: Loss = -12469.6572265625
Iteration 9800: Loss = -12469.646484375
Iteration 9900: Loss = -12469.63671875
Iteration 10000: Loss = -12469.630859375
Iteration 10100: Loss = -12469.6240234375
Iteration 10200: Loss = -12469.6181640625
Iteration 10300: Loss = -12469.6103515625
Iteration 10400: Loss = -12469.603515625
Iteration 10500: Loss = -12469.5986328125
Iteration 10600: Loss = -12469.5927734375
Iteration 10700: Loss = -12469.58984375
Iteration 10800: Loss = -12469.583984375
Iteration 10900: Loss = -12469.580078125
Iteration 11000: Loss = -12469.572265625
Iteration 11100: Loss = -12469.5703125
Iteration 11200: Loss = -12469.568359375
Iteration 11300: Loss = -12469.5634765625
Iteration 11400: Loss = -12469.55859375
Iteration 11500: Loss = -12469.5576171875
Iteration 11600: Loss = -12469.5556640625
Iteration 11700: Loss = -12469.5517578125
Iteration 11800: Loss = -12469.544921875
Iteration 11900: Loss = -12469.5419921875
Iteration 12000: Loss = -12469.5322265625
Iteration 12100: Loss = -12469.5263671875
Iteration 12200: Loss = -12469.5205078125
Iteration 12300: Loss = -12469.51953125
Iteration 12400: Loss = -12469.521484375
1
Iteration 12500: Loss = -12469.5205078125
2
Iteration 12600: Loss = -12469.5185546875
Iteration 12700: Loss = -12469.5166015625
Iteration 12800: Loss = -12469.5107421875
Iteration 12900: Loss = -12469.509765625
Iteration 13000: Loss = -12469.5087890625
Iteration 13100: Loss = -12469.509765625
1
Iteration 13200: Loss = -12469.509765625
2
Iteration 13300: Loss = -12469.5087890625
Iteration 13400: Loss = -12469.5087890625
Iteration 13500: Loss = -12469.5078125
Iteration 13600: Loss = -12469.5078125
Iteration 13700: Loss = -12469.505859375
Iteration 13800: Loss = -12469.5029296875
Iteration 13900: Loss = -12469.484375
Iteration 14000: Loss = -12469.4775390625
Iteration 14100: Loss = -12469.4775390625
Iteration 14200: Loss = -12469.4765625
Iteration 14300: Loss = -12469.474609375
Iteration 14400: Loss = -12469.46484375
Iteration 14500: Loss = -12469.46484375
Iteration 14600: Loss = -12469.46484375
Iteration 14700: Loss = -12469.4501953125
Iteration 14800: Loss = -12469.4423828125
Iteration 14900: Loss = -12469.44140625
Iteration 15000: Loss = -12469.44140625
Iteration 15100: Loss = -12469.44140625
Iteration 15200: Loss = -12469.44140625
Iteration 15300: Loss = -12469.44140625
Iteration 15400: Loss = -12469.4423828125
1
Iteration 15500: Loss = -12469.44140625
Iteration 15600: Loss = -12469.4404296875
Iteration 15700: Loss = -12469.4404296875
Iteration 15800: Loss = -12469.44140625
1
Iteration 15900: Loss = -12469.4404296875
Iteration 16000: Loss = -12469.4404296875
Iteration 16100: Loss = -12469.4267578125
Iteration 16200: Loss = -12469.427734375
1
Iteration 16300: Loss = -12469.4287109375
2
Iteration 16400: Loss = -12469.4287109375
3
Iteration 16500: Loss = -12469.4287109375
4
Iteration 16600: Loss = -12469.4150390625
Iteration 16700: Loss = -12469.4140625
Iteration 16800: Loss = -12469.4140625
Iteration 16900: Loss = -12469.4033203125
Iteration 17000: Loss = -12469.4013671875
Iteration 17100: Loss = -12469.4013671875
Iteration 17200: Loss = -12469.3994140625
Iteration 17300: Loss = -12469.4013671875
1
Iteration 17400: Loss = -12469.4013671875
2
Iteration 17500: Loss = -12469.3896484375
Iteration 17600: Loss = -12469.37109375
Iteration 17700: Loss = -12469.3720703125
1
Iteration 17800: Loss = -12469.369140625
Iteration 17900: Loss = -12469.369140625
Iteration 18000: Loss = -12469.345703125
Iteration 18100: Loss = -12469.33203125
Iteration 18200: Loss = -12469.3330078125
1
Iteration 18300: Loss = -12469.33203125
Iteration 18400: Loss = -12469.33203125
Iteration 18500: Loss = -12469.33203125
Iteration 18600: Loss = -12469.3330078125
1
Iteration 18700: Loss = -12469.3330078125
2
Iteration 18800: Loss = -12469.3310546875
Iteration 18900: Loss = -12469.33203125
1
Iteration 19000: Loss = -12469.33203125
2
Iteration 19100: Loss = -12469.3310546875
Iteration 19200: Loss = -12469.3310546875
Iteration 19300: Loss = -12469.33203125
1
Iteration 19400: Loss = -12469.33203125
2
Iteration 19500: Loss = -12469.33203125
3
Iteration 19600: Loss = -12469.3310546875
Iteration 19700: Loss = -12469.33203125
1
Iteration 19800: Loss = -12469.3193359375
Iteration 19900: Loss = -12468.6826171875
Iteration 20000: Loss = -12468.4951171875
Iteration 20100: Loss = -12468.4755859375
Iteration 20200: Loss = -12468.4609375
Iteration 20300: Loss = -12468.4189453125
Iteration 20400: Loss = -12468.146484375
Iteration 20500: Loss = -12467.75
Iteration 20600: Loss = -12467.6376953125
Iteration 20700: Loss = -12467.615234375
Iteration 20800: Loss = -12467.6064453125
Iteration 20900: Loss = -12467.599609375
Iteration 21000: Loss = -12467.59765625
Iteration 21100: Loss = -12467.595703125
Iteration 21200: Loss = -12467.5947265625
Iteration 21300: Loss = -12467.59375
Iteration 21400: Loss = -12467.59375
Iteration 21500: Loss = -12467.59375
Iteration 21600: Loss = -12467.5927734375
Iteration 21700: Loss = -12467.58984375
Iteration 21800: Loss = -12467.591796875
1
Iteration 21900: Loss = -12467.591796875
2
Iteration 22000: Loss = -12467.591796875
3
Iteration 22100: Loss = -12467.5908203125
4
Iteration 22200: Loss = -12467.591796875
5
Iteration 22300: Loss = -12467.5888671875
Iteration 22400: Loss = -12467.5908203125
1
Iteration 22500: Loss = -12467.58984375
2
Iteration 22600: Loss = -12467.58984375
3
Iteration 22700: Loss = -12467.587890625
Iteration 22800: Loss = -12467.587890625
Iteration 22900: Loss = -12467.5888671875
1
Iteration 23000: Loss = -12467.587890625
Iteration 23100: Loss = -12467.5888671875
1
Iteration 23200: Loss = -12467.5888671875
2
Iteration 23300: Loss = -12467.5888671875
3
Iteration 23400: Loss = -12467.58984375
4
Iteration 23500: Loss = -12467.58984375
5
Iteration 23600: Loss = -12467.5908203125
6
Iteration 23700: Loss = -12467.5888671875
7
Iteration 23800: Loss = -12467.5908203125
8
Iteration 23900: Loss = -12467.5888671875
9
Iteration 24000: Loss = -12467.587890625
Iteration 24100: Loss = -12467.5888671875
1
Iteration 24200: Loss = -12467.587890625
Iteration 24300: Loss = -12467.587890625
Iteration 24400: Loss = -12467.587890625
Iteration 24500: Loss = -12467.5869140625
Iteration 24600: Loss = -12467.5888671875
1
Iteration 24700: Loss = -12467.5888671875
2
Iteration 24800: Loss = -12467.587890625
3
Iteration 24900: Loss = -12467.5888671875
4
Iteration 25000: Loss = -12467.5888671875
5
Iteration 25100: Loss = -12467.587890625
6
Iteration 25200: Loss = -12467.587890625
7
Iteration 25300: Loss = -12467.587890625
8
Iteration 25400: Loss = -12467.587890625
9
Iteration 25500: Loss = -12467.591796875
10
Iteration 25600: Loss = -12467.58984375
11
Iteration 25700: Loss = -12467.58984375
12
Iteration 25800: Loss = -12467.587890625
13
Iteration 25900: Loss = -12467.587890625
14
Iteration 26000: Loss = -12467.587890625
15
Stopping early at iteration 26000 due to no improvement.
pi: tensor([[9.9949e-01, 5.1113e-04],
        [1.3112e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0104, 0.9896], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2933, 0.2712],
         [0.4027, 0.2016]],

        [[0.5564, 0.1924],
         [0.5041, 0.8714]],

        [[0.9798, 0.1547],
         [0.5094, 0.4816]],

        [[0.4881, 0.2719],
         [0.0093, 0.9283]],

        [[0.4227, 0.3288],
         [0.8389, 0.8354]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: -0.005513710431743218
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.007614171548597778
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: -0.005513710431743218
Global Adjusted Rand Index: -0.0019575189348408355
Average Adjusted Rand Index: -0.0008395728906314123
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24870.083984375
Iteration 100: Loss = -17052.447265625
Iteration 200: Loss = -13590.9111328125
Iteration 300: Loss = -12762.1533203125
Iteration 400: Loss = -12618.1748046875
Iteration 500: Loss = -12561.671875
Iteration 600: Loss = -12534.9658203125
Iteration 700: Loss = -12519.67578125
Iteration 800: Loss = -12505.7978515625
Iteration 900: Loss = -12496.8681640625
Iteration 1000: Loss = -12491.9951171875
Iteration 1100: Loss = -12488.4130859375
Iteration 1200: Loss = -12485.6748046875
Iteration 1300: Loss = -12483.5185546875
Iteration 1400: Loss = -12481.78515625
Iteration 1500: Loss = -12480.37109375
Iteration 1600: Loss = -12479.1943359375
Iteration 1700: Loss = -12478.201171875
Iteration 1800: Loss = -12477.3544921875
Iteration 1900: Loss = -12476.626953125
Iteration 2000: Loss = -12475.9921875
Iteration 2100: Loss = -12475.431640625
Iteration 2200: Loss = -12474.93359375
Iteration 2300: Loss = -12474.4921875
Iteration 2400: Loss = -12474.095703125
Iteration 2500: Loss = -12473.740234375
Iteration 2600: Loss = -12473.423828125
Iteration 2700: Loss = -12473.1376953125
Iteration 2800: Loss = -12472.8798828125
Iteration 2900: Loss = -12472.6474609375
Iteration 3000: Loss = -12472.4345703125
Iteration 3100: Loss = -12472.2421875
Iteration 3200: Loss = -12472.0654296875
Iteration 3300: Loss = -12471.90234375
Iteration 3400: Loss = -12471.740234375
Iteration 3500: Loss = -12471.5986328125
Iteration 3600: Loss = -12471.4697265625
Iteration 3700: Loss = -12471.34765625
Iteration 3800: Loss = -12471.23046875
Iteration 3900: Loss = -12471.1201171875
Iteration 4000: Loss = -12471.015625
Iteration 4100: Loss = -12470.9072265625
Iteration 4200: Loss = -12470.8076171875
Iteration 4300: Loss = -12470.7109375
Iteration 4400: Loss = -12470.611328125
Iteration 4500: Loss = -12470.5146484375
Iteration 4600: Loss = -12470.4189453125
Iteration 4700: Loss = -12470.3232421875
Iteration 4800: Loss = -12470.2314453125
Iteration 4900: Loss = -12470.13671875
Iteration 5000: Loss = -12470.0439453125
Iteration 5100: Loss = -12469.953125
Iteration 5200: Loss = -12469.865234375
Iteration 5300: Loss = -12469.7861328125
Iteration 5400: Loss = -12469.7060546875
Iteration 5500: Loss = -12469.6318359375
Iteration 5600: Loss = -12469.560546875
Iteration 5700: Loss = -12469.4931640625
Iteration 5800: Loss = -12469.4287109375
Iteration 5900: Loss = -12469.3671875
Iteration 6000: Loss = -12469.30859375
Iteration 6100: Loss = -12469.2529296875
Iteration 6200: Loss = -12469.1982421875
Iteration 6300: Loss = -12469.1474609375
Iteration 6400: Loss = -12469.09765625
Iteration 6500: Loss = -12469.0498046875
Iteration 6600: Loss = -12469.0
Iteration 6700: Loss = -12468.9521484375
Iteration 6800: Loss = -12468.900390625
Iteration 6900: Loss = -12468.8525390625
Iteration 7000: Loss = -12468.80859375
Iteration 7100: Loss = -12468.7666015625
Iteration 7200: Loss = -12468.7255859375
Iteration 7300: Loss = -12468.6865234375
Iteration 7400: Loss = -12468.650390625
Iteration 7500: Loss = -12468.6142578125
Iteration 7600: Loss = -12468.5810546875
Iteration 7700: Loss = -12468.55078125
Iteration 7800: Loss = -12468.5205078125
Iteration 7900: Loss = -12468.494140625
Iteration 8000: Loss = -12468.470703125
Iteration 8100: Loss = -12468.44921875
Iteration 8200: Loss = -12468.427734375
Iteration 8300: Loss = -12468.4111328125
Iteration 8400: Loss = -12468.396484375
Iteration 8500: Loss = -12468.3798828125
Iteration 8600: Loss = -12468.3662109375
Iteration 8700: Loss = -12468.35546875
Iteration 8800: Loss = -12468.3427734375
Iteration 8900: Loss = -12468.333984375
Iteration 9000: Loss = -12468.3251953125
Iteration 9100: Loss = -12468.3154296875
Iteration 9200: Loss = -12468.3076171875
Iteration 9300: Loss = -12468.3017578125
Iteration 9400: Loss = -12468.29296875
Iteration 9500: Loss = -12468.287109375
Iteration 9600: Loss = -12468.28125
Iteration 9700: Loss = -12468.275390625
Iteration 9800: Loss = -12468.2724609375
Iteration 9900: Loss = -12468.267578125
Iteration 10000: Loss = -12468.2626953125
Iteration 10100: Loss = -12468.2578125
Iteration 10200: Loss = -12468.2529296875
Iteration 10300: Loss = -12468.25
Iteration 10400: Loss = -12468.2470703125
Iteration 10500: Loss = -12468.2421875
Iteration 10600: Loss = -12468.2412109375
Iteration 10700: Loss = -12468.236328125
Iteration 10800: Loss = -12468.2353515625
Iteration 10900: Loss = -12468.232421875
Iteration 11000: Loss = -12468.228515625
Iteration 11100: Loss = -12468.2275390625
Iteration 11200: Loss = -12468.2236328125
Iteration 11300: Loss = -12468.2236328125
Iteration 11400: Loss = -12468.2197265625
Iteration 11500: Loss = -12468.21875
Iteration 11600: Loss = -12468.2216796875
1
Iteration 11700: Loss = -12468.216796875
Iteration 11800: Loss = -12468.2158203125
Iteration 11900: Loss = -12468.212890625
Iteration 12000: Loss = -12468.2119140625
Iteration 12100: Loss = -12468.2119140625
Iteration 12200: Loss = -12468.2080078125
Iteration 12300: Loss = -12468.20703125
Iteration 12400: Loss = -12468.20703125
Iteration 12500: Loss = -12468.20703125
Iteration 12600: Loss = -12468.2041015625
Iteration 12700: Loss = -12468.2041015625
Iteration 12800: Loss = -12468.203125
Iteration 12900: Loss = -12468.2021484375
Iteration 13000: Loss = -12468.201171875
Iteration 13100: Loss = -12468.201171875
Iteration 13200: Loss = -12468.2001953125
Iteration 13300: Loss = -12468.2001953125
Iteration 13400: Loss = -12468.2001953125
Iteration 13500: Loss = -12468.1982421875
Iteration 13600: Loss = -12468.197265625
Iteration 13700: Loss = -12468.197265625
Iteration 13800: Loss = -12468.1962890625
Iteration 13900: Loss = -12468.1953125
Iteration 14000: Loss = -12468.1943359375
Iteration 14100: Loss = -12468.1943359375
Iteration 14200: Loss = -12468.1943359375
Iteration 14300: Loss = -12468.197265625
1
Iteration 14400: Loss = -12468.1943359375
Iteration 14500: Loss = -12468.197265625
1
Iteration 14600: Loss = -12468.1943359375
Iteration 14700: Loss = -12468.1923828125
Iteration 14800: Loss = -12468.1923828125
Iteration 14900: Loss = -12468.19140625
Iteration 15000: Loss = -12468.1923828125
1
Iteration 15100: Loss = -12468.1904296875
Iteration 15200: Loss = -12468.189453125
Iteration 15300: Loss = -12468.193359375
1
Iteration 15400: Loss = -12468.1904296875
2
Iteration 15500: Loss = -12468.1904296875
3
Iteration 15600: Loss = -12468.189453125
Iteration 15700: Loss = -12468.189453125
Iteration 15800: Loss = -12468.1884765625
Iteration 15900: Loss = -12468.1884765625
Iteration 16000: Loss = -12468.189453125
1
Iteration 16100: Loss = -12468.189453125
2
Iteration 16200: Loss = -12468.1884765625
Iteration 16300: Loss = -12468.1884765625
Iteration 16400: Loss = -12468.189453125
1
Iteration 16500: Loss = -12468.1884765625
Iteration 16600: Loss = -12468.189453125
1
Iteration 16700: Loss = -12468.1884765625
Iteration 16800: Loss = -12468.1875
Iteration 16900: Loss = -12468.1884765625
1
Iteration 17000: Loss = -12468.189453125
2
Iteration 17100: Loss = -12468.19140625
3
Iteration 17200: Loss = -12468.1875
Iteration 17300: Loss = -12468.1875
Iteration 17400: Loss = -12468.1875
Iteration 17500: Loss = -12468.1875
Iteration 17600: Loss = -12468.1875
Iteration 17700: Loss = -12468.1875
Iteration 17800: Loss = -12468.1875
Iteration 17900: Loss = -12468.1865234375
Iteration 18000: Loss = -12468.1875
1
Iteration 18100: Loss = -12468.1884765625
2
Iteration 18200: Loss = -12468.1875
3
Iteration 18300: Loss = -12468.1865234375
Iteration 18400: Loss = -12468.1865234375
Iteration 18500: Loss = -12468.1865234375
Iteration 18600: Loss = -12468.1875
1
Iteration 18700: Loss = -12468.1865234375
Iteration 18800: Loss = -12468.1875
1
Iteration 18900: Loss = -12468.185546875
Iteration 19000: Loss = -12468.1865234375
1
Iteration 19100: Loss = -12468.1865234375
2
Iteration 19200: Loss = -12468.1865234375
3
Iteration 19300: Loss = -12468.185546875
Iteration 19400: Loss = -12468.185546875
Iteration 19500: Loss = -12468.1865234375
1
Iteration 19600: Loss = -12468.1875
2
Iteration 19700: Loss = -12468.1865234375
3
Iteration 19800: Loss = -12468.1865234375
4
Iteration 19900: Loss = -12468.185546875
Iteration 20000: Loss = -12468.1865234375
1
Iteration 20100: Loss = -12468.1875
2
Iteration 20200: Loss = -12468.1875
3
Iteration 20300: Loss = -12468.1865234375
4
Iteration 20400: Loss = -12468.1865234375
5
Iteration 20500: Loss = -12468.185546875
Iteration 20600: Loss = -12468.185546875
Iteration 20700: Loss = -12468.1865234375
1
Iteration 20800: Loss = -12468.1875
2
Iteration 20900: Loss = -12468.1845703125
Iteration 21000: Loss = -12468.1845703125
Iteration 21100: Loss = -12468.1875
1
Iteration 21200: Loss = -12468.185546875
2
Iteration 21300: Loss = -12468.1845703125
Iteration 21400: Loss = -12468.1865234375
1
Iteration 21500: Loss = -12468.1865234375
2
Iteration 21600: Loss = -12468.1875
3
Iteration 21700: Loss = -12468.1865234375
4
Iteration 21800: Loss = -12468.185546875
5
Iteration 21900: Loss = -12468.1865234375
6
Iteration 22000: Loss = -12468.185546875
7
Iteration 22100: Loss = -12468.1875
8
Iteration 22200: Loss = -12468.185546875
9
Iteration 22300: Loss = -12468.185546875
10
Iteration 22400: Loss = -12468.185546875
11
Iteration 22500: Loss = -12468.1865234375
12
Iteration 22600: Loss = -12468.1865234375
13
Iteration 22700: Loss = -12468.185546875
14
Iteration 22800: Loss = -12468.1875
15
Stopping early at iteration 22800 due to no improvement.
pi: tensor([[9.8568e-01, 1.4316e-02],
        [2.4602e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 9.2385e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1997, 0.2212],
         [0.2179, 0.5317]],

        [[0.8685, 0.2021],
         [0.2899, 0.7665]],

        [[0.3774, 0.2258],
         [0.9385, 0.0236]],

        [[0.0422, 0.2602],
         [0.8337, 0.0348]],

        [[0.8669, 0.2591],
         [0.0264, 0.0086]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.008987974340494725
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.01717781179455718
Global Adjusted Rand Index: -0.003062120166439967
Average Adjusted Rand Index: -0.005069829177819817
[-0.0019575189348408355, -0.003062120166439967] [-0.0008395728906314123, -0.005069829177819817] [12467.587890625, 12468.1875]
-------------------------------------
This iteration is 35
True Objective function: Loss = -11795.264236712863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -38999.85546875
Iteration 100: Loss = -23509.48046875
Iteration 200: Loss = -15145.1689453125
Iteration 300: Loss = -13362.880859375
Iteration 400: Loss = -12947.2568359375
Iteration 500: Loss = -12729.0537109375
Iteration 600: Loss = -12623.365234375
Iteration 700: Loss = -12524.609375
Iteration 800: Loss = -12490.5908203125
Iteration 900: Loss = -12452.107421875
Iteration 1000: Loss = -12433.8603515625
Iteration 1100: Loss = -12416.513671875
Iteration 1200: Loss = -12406.208984375
Iteration 1300: Loss = -12386.7568359375
Iteration 1400: Loss = -12378.0859375
Iteration 1500: Loss = -12371.76953125
Iteration 1600: Loss = -12366.2021484375
Iteration 1700: Loss = -12362.568359375
Iteration 1800: Loss = -12355.978515625
Iteration 1900: Loss = -12353.5283203125
Iteration 2000: Loss = -12351.1982421875
Iteration 2100: Loss = -12348.52734375
Iteration 2200: Loss = -12346.2841796875
Iteration 2300: Loss = -12344.501953125
Iteration 2400: Loss = -12343.1171875
Iteration 2500: Loss = -12341.990234375
Iteration 2600: Loss = -12341.03515625
Iteration 2700: Loss = -12340.203125
Iteration 2800: Loss = -12339.466796875
Iteration 2900: Loss = -12338.806640625
Iteration 3000: Loss = -12338.216796875
Iteration 3100: Loss = -12337.6796875
Iteration 3200: Loss = -12337.193359375
Iteration 3300: Loss = -12336.7490234375
Iteration 3400: Loss = -12336.3408203125
Iteration 3500: Loss = -12335.970703125
Iteration 3600: Loss = -12335.6259765625
Iteration 3700: Loss = -12335.310546875
Iteration 3800: Loss = -12335.01953125
Iteration 3900: Loss = -12334.744140625
Iteration 4000: Loss = -12334.494140625
Iteration 4100: Loss = -12334.2626953125
Iteration 4200: Loss = -12334.0458984375
Iteration 4300: Loss = -12333.845703125
Iteration 4400: Loss = -12333.658203125
Iteration 4500: Loss = -12333.4853515625
Iteration 4600: Loss = -12333.3212890625
Iteration 4700: Loss = -12333.16796875
Iteration 4800: Loss = -12333.0263671875
Iteration 4900: Loss = -12332.8935546875
Iteration 5000: Loss = -12332.7685546875
Iteration 5100: Loss = -12332.650390625
Iteration 5200: Loss = -12332.5400390625
Iteration 5300: Loss = -12332.4375
Iteration 5400: Loss = -12332.341796875
Iteration 5500: Loss = -12332.25
Iteration 5600: Loss = -12332.1640625
Iteration 5700: Loss = -12332.0830078125
Iteration 5800: Loss = -12332.0068359375
Iteration 5900: Loss = -12331.9345703125
Iteration 6000: Loss = -12331.8701171875
Iteration 6100: Loss = -12331.8046875
Iteration 6200: Loss = -12331.7470703125
Iteration 6300: Loss = -12331.6884765625
Iteration 6400: Loss = -12331.634765625
Iteration 6500: Loss = -12331.5849609375
Iteration 6600: Loss = -12331.5380859375
Iteration 6700: Loss = -12331.494140625
Iteration 6800: Loss = -12331.451171875
Iteration 6900: Loss = -12331.4091796875
Iteration 7000: Loss = -12331.373046875
Iteration 7100: Loss = -12331.3369140625
Iteration 7200: Loss = -12331.3017578125
Iteration 7300: Loss = -12331.26953125
Iteration 7400: Loss = -12331.2412109375
Iteration 7500: Loss = -12331.212890625
Iteration 7600: Loss = -12331.1845703125
Iteration 7700: Loss = -12331.16015625
Iteration 7800: Loss = -12331.1357421875
Iteration 7900: Loss = -12331.1123046875
Iteration 8000: Loss = -12331.0908203125
Iteration 8100: Loss = -12331.0693359375
Iteration 8200: Loss = -12331.0498046875
Iteration 8300: Loss = -12331.03125
Iteration 8400: Loss = -12331.015625
Iteration 8500: Loss = -12330.9990234375
Iteration 8600: Loss = -12330.9833984375
Iteration 8700: Loss = -12330.96875
Iteration 8800: Loss = -12330.955078125
Iteration 8900: Loss = -12330.94140625
Iteration 9000: Loss = -12330.927734375
Iteration 9100: Loss = -12330.916015625
Iteration 9200: Loss = -12330.904296875
Iteration 9300: Loss = -12330.8935546875
Iteration 9400: Loss = -12330.8828125
Iteration 9500: Loss = -12330.875
Iteration 9600: Loss = -12330.8642578125
Iteration 9700: Loss = -12330.85546875
Iteration 9800: Loss = -12330.8486328125
Iteration 9900: Loss = -12330.8388671875
Iteration 10000: Loss = -12330.8330078125
Iteration 10100: Loss = -12330.8251953125
Iteration 10200: Loss = -12330.8193359375
Iteration 10300: Loss = -12330.8134765625
Iteration 10400: Loss = -12330.806640625
Iteration 10500: Loss = -12330.80078125
Iteration 10600: Loss = -12330.796875
Iteration 10700: Loss = -12330.791015625
Iteration 10800: Loss = -12330.787109375
Iteration 10900: Loss = -12330.78125
Iteration 11000: Loss = -12330.7783203125
Iteration 11100: Loss = -12330.7744140625
Iteration 11200: Loss = -12330.7705078125
Iteration 11300: Loss = -12330.7666015625
Iteration 11400: Loss = -12330.763671875
Iteration 11500: Loss = -12330.7607421875
Iteration 11600: Loss = -12330.7568359375
Iteration 11700: Loss = -12330.7529296875
Iteration 11800: Loss = -12330.7529296875
Iteration 11900: Loss = -12330.7490234375
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 36%|███▌      | 36/100 [28:03:07<50:28:33, 2839.27s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 37%|███▋      | 37/100 [28:51:19<49:57:51, 2855.10s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 38%|███▊      | 38/100 [29:46:01<51:22:37, 2983.18s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 39%|███▉      | 39/100 [30:36:27<50:46:11, 2996.25s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 40%|████      | 40/100 [31:26:05<49:50:38, 2990.65s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 41%|████      | 41/100 [32:21:23<50:37:29, 3088.98s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 42%|████▏     | 42/100 [33:10:42<49:08:16, 3049.95s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 43%|████▎     | 43/100 [33:53:15<45:55:51, 2900.91s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 44%|████▍     | 44/100 [34:31:51<42:23:36, 2725.29s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 45%|████▌     | 45/100 [35:19:12<42:10:09, 2760.17s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 46%|████▌     | 46/100 [36:03:28<40:55:57, 2728.84s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 47%|████▋     | 47/100 [36:44:21<38:57:16, 2645.97s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 48%|████▊     | 48/100 [37:32:34<39:17:20, 2720.01s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 49%|████▉     | 49/100 [38:27:16<40:55:22, 2888.69s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 50%|█████     | 50/100 [39:20:07<41:17:51, 2973.43s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 51%|█████     | 51/100 [40:09:15<40:22:05, 2965.82s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 52%|█████▏    | 52/100 [41:00:02<39:52:06, 2990.14s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 53%|█████▎    | 53/100 [41:42:55<37:24:09, 2864.88s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 54%|█████▍    | 54/100 [42:37:30<38:10:56, 2988.18s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 55%|█████▌    | 55/100 [43:31:03<38:11:30, 3055.35s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 56%|█████▌    | 56/100 [44:25:23<38:05:43, 3116.91s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 57%|█████▋    | 57/100 [45:16:32<37:03:29, 3102.54s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 58%|█████▊    | 58/100 [46:00:33<34:34:55, 2964.17s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 59%|█████▉    | 59/100 [46:53:25<34:28:07, 3026.53s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 60%|██████    | 60/100 [47:36:18<32:06:54, 2890.35s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 61%|██████    | 61/100 [48:22:04<30:50:31, 2846.96s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 62%|██████▏   | 62/100 [49:04:53<29:10:15, 2763.58s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 63%|██████▎   | 63/100 [49:53:25<28:51:40, 2808.11s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 64%|██████▍   | 64/100 [50:47:15<29:20:46, 2934.64s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 65%|██████▌   | 65/100 [51:23:27<26:18:27, 2705.91s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 66%|██████▌   | 66/100 [52:12:04<26:09:16, 2769.31s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 67%|██████▋   | 67/100 [52:57:25<25:15:09, 2754.83s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 68%|██████▊   | 68/100 [53:43:40<24:32:28, 2760.89s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 69%|██████▉   | 69/100 [54:33:06<24:18:09, 2822.24s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
Iteration 12000: Loss = -12330.748046875
Iteration 12100: Loss = -12330.744140625
Iteration 12200: Loss = -12330.7431640625
Iteration 12300: Loss = -12330.7392578125
Iteration 12400: Loss = -12330.73828125
Iteration 12500: Loss = -12330.7353515625
Iteration 12600: Loss = -12330.734375
Iteration 12700: Loss = -12330.7333984375
Iteration 12800: Loss = -12330.732421875
Iteration 12900: Loss = -12330.73046875
Iteration 13000: Loss = -12330.7294921875
Iteration 13100: Loss = -12330.7255859375
Iteration 13200: Loss = -12330.7255859375
Iteration 13300: Loss = -12330.7236328125
Iteration 13400: Loss = -12330.72265625
Iteration 13500: Loss = -12330.720703125
Iteration 13600: Loss = -12330.720703125
Iteration 13700: Loss = -12330.7197265625
Iteration 13800: Loss = -12330.71875
Iteration 13900: Loss = -12330.7177734375
Iteration 14000: Loss = -12330.7158203125
Iteration 14100: Loss = -12330.716796875
1
Iteration 14200: Loss = -12330.7138671875
Iteration 14300: Loss = -12330.7138671875
Iteration 14400: Loss = -12330.7138671875
Iteration 14500: Loss = -12330.7119140625
Iteration 14600: Loss = -12330.7119140625
Iteration 14700: Loss = -12330.7109375
Iteration 14800: Loss = -12330.7109375
Iteration 14900: Loss = -12330.7099609375
Iteration 15000: Loss = -12330.7109375
1
Iteration 15100: Loss = -12330.7080078125
Iteration 15200: Loss = -12330.7080078125
Iteration 15300: Loss = -12330.7080078125
Iteration 15400: Loss = -12330.7080078125
Iteration 15500: Loss = -12330.7080078125
Iteration 15600: Loss = -12330.7080078125
Iteration 15700: Loss = -12330.7060546875
Iteration 15800: Loss = -12330.70703125
1
Iteration 15900: Loss = -12330.705078125
Iteration 16000: Loss = -12330.70703125
1
Iteration 16100: Loss = -12330.70703125
2
Iteration 16200: Loss = -12330.70703125
3
Iteration 16300: Loss = -12330.705078125
Iteration 16400: Loss = -12330.705078125
Iteration 16500: Loss = -12330.705078125
Iteration 16600: Loss = -12330.7041015625
Iteration 16700: Loss = -12330.7060546875
1
Iteration 16800: Loss = -12330.7060546875
2
Iteration 16900: Loss = -12330.7041015625
Iteration 17000: Loss = -12330.7041015625
Iteration 17100: Loss = -12330.703125
Iteration 17200: Loss = -12330.703125
Iteration 17300: Loss = -12330.7041015625
1
Iteration 17400: Loss = -12330.7041015625
2
Iteration 17500: Loss = -12330.7041015625
3
Iteration 17600: Loss = -12330.7080078125
4
Iteration 17700: Loss = -12330.703125
Iteration 17800: Loss = -12330.703125
Iteration 17900: Loss = -12330.703125
Iteration 18000: Loss = -12330.703125
Iteration 18100: Loss = -12330.701171875
Iteration 18200: Loss = -12330.703125
1
Iteration 18300: Loss = -12330.701171875
Iteration 18400: Loss = -12330.7021484375
1
Iteration 18500: Loss = -12330.7021484375
2
Iteration 18600: Loss = -12330.7021484375
3
Iteration 18700: Loss = -12330.7021484375
4
Iteration 18800: Loss = -12330.701171875
Iteration 18900: Loss = -12330.7021484375
1
Iteration 19000: Loss = -12330.7021484375
2
Iteration 19100: Loss = -12330.7021484375
3
Iteration 19200: Loss = -12330.703125
4
Iteration 19300: Loss = -12330.7021484375
5
Iteration 19400: Loss = -12330.703125
6
Iteration 19500: Loss = -12330.7021484375
7
Iteration 19600: Loss = -12330.7021484375
8
Iteration 19700: Loss = -12330.703125
9
Iteration 19800: Loss = -12330.703125
10
Iteration 19900: Loss = -12330.701171875
Iteration 20000: Loss = -12330.701171875
Iteration 20100: Loss = -12330.703125
1
Iteration 20200: Loss = -12330.701171875
Iteration 20300: Loss = -12330.703125
1
Iteration 20400: Loss = -12330.703125
2
Iteration 20500: Loss = -12330.701171875
Iteration 20600: Loss = -12330.703125
1
Iteration 20700: Loss = -12330.703125
2
Iteration 20800: Loss = -12330.703125
3
Iteration 20900: Loss = -12327.4736328125
Iteration 21000: Loss = -12327.40234375
Iteration 21100: Loss = -12327.38671875
Iteration 21200: Loss = -12327.37109375
Iteration 21300: Loss = -12327.345703125
Iteration 21400: Loss = -12327.326171875
Iteration 21500: Loss = -12327.3154296875
Iteration 21600: Loss = -12327.2919921875
Iteration 21700: Loss = -12327.2802734375
Iteration 21800: Loss = -12327.275390625
Iteration 21900: Loss = -12327.2724609375
Iteration 22000: Loss = -12327.2705078125
Iteration 22100: Loss = -12327.2685546875
Iteration 22200: Loss = -12327.267578125
Iteration 22300: Loss = -12327.267578125
Iteration 22400: Loss = -12327.267578125
Iteration 22500: Loss = -12327.26171875
Iteration 22600: Loss = -12327.2626953125
1
Iteration 22700: Loss = -12327.26171875
Iteration 22800: Loss = -12327.2607421875
Iteration 22900: Loss = -12327.2587890625
Iteration 23000: Loss = -12327.259765625
1
Iteration 23100: Loss = -12327.259765625
2
Iteration 23200: Loss = -12327.2587890625
Iteration 23300: Loss = -12327.259765625
1
Iteration 23400: Loss = -12327.2578125
Iteration 23500: Loss = -12327.2578125
Iteration 23600: Loss = -12327.2578125
Iteration 23700: Loss = -12327.2578125
Iteration 23800: Loss = -12327.2587890625
1
Iteration 23900: Loss = -12327.2470703125
Iteration 24000: Loss = -12327.24609375
Iteration 24100: Loss = -12327.2470703125
1
Iteration 24200: Loss = -12327.2470703125
2
Iteration 24300: Loss = -12327.24609375
Iteration 24400: Loss = -12327.2470703125
1
Iteration 24500: Loss = -12327.2470703125
2
Iteration 24600: Loss = -12327.2451171875
Iteration 24700: Loss = -12327.2490234375
1
Iteration 24800: Loss = -12327.244140625
Iteration 24900: Loss = -12327.244140625
Iteration 25000: Loss = -12327.2451171875
1
Iteration 25100: Loss = -12327.2451171875
2
Iteration 25200: Loss = -12327.2451171875
3
Iteration 25300: Loss = -12327.2451171875
4
Iteration 25400: Loss = -12327.2451171875
5
Iteration 25500: Loss = -12327.24609375
6
Iteration 25600: Loss = -12327.2451171875
7
Iteration 25700: Loss = -12327.24609375
8
Iteration 25800: Loss = -12327.2451171875
9
Iteration 25900: Loss = -12327.2451171875
10
Iteration 26000: Loss = -12327.244140625
Iteration 26100: Loss = -12327.24609375
1
Iteration 26200: Loss = -12327.244140625
Iteration 26300: Loss = -12327.24609375
1
Iteration 26400: Loss = -12327.2451171875
2
Iteration 26500: Loss = -12327.24609375
3
Iteration 26600: Loss = -12327.2451171875
4
Iteration 26700: Loss = -12327.2451171875
5
Iteration 26800: Loss = -12327.24609375
6
Iteration 26900: Loss = -12327.244140625
Iteration 27000: Loss = -12327.24609375
1
Iteration 27100: Loss = -12327.2451171875
2
Iteration 27200: Loss = -12327.2451171875
3
Iteration 27300: Loss = -12327.244140625
Iteration 27400: Loss = -12327.24609375
1
Iteration 27500: Loss = -12327.2451171875
2
Iteration 27600: Loss = -12327.24609375
3
Iteration 27700: Loss = -12327.2451171875
4
Iteration 27800: Loss = -12327.2451171875
5
Iteration 27900: Loss = -12327.2431640625
Iteration 28000: Loss = -12327.244140625
1
Iteration 28100: Loss = -12327.2431640625
Iteration 28200: Loss = -12327.244140625
1
Iteration 28300: Loss = -12327.244140625
2
Iteration 28400: Loss = -12327.2431640625
Iteration 28500: Loss = -12327.244140625
1
Iteration 28600: Loss = -12327.2431640625
Iteration 28700: Loss = -12327.2431640625
Iteration 28800: Loss = -12327.244140625
1
Iteration 28900: Loss = -12327.244140625
2
Iteration 29000: Loss = -12327.2431640625
Iteration 29100: Loss = -12327.2421875
Iteration 29200: Loss = -12327.244140625
1
Iteration 29300: Loss = -12327.244140625
2
Iteration 29400: Loss = -12327.2431640625
3
Iteration 29500: Loss = -12327.2431640625
4
Iteration 29600: Loss = -12327.2421875
Iteration 29700: Loss = -12327.2431640625
1
Iteration 29800: Loss = -12327.2431640625
2
Iteration 29900: Loss = -12327.2421875
pi: tensor([[1.0000e+00, 1.0904e-06],
        [5.6125e-05, 9.9994e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9836, 0.0164], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.9943e-01, 1.6495e-01],
         [3.4271e-01, 5.8195e-04]],

        [[3.7263e-01, 8.0229e-02],
         [2.4589e-01, 2.0606e-01]],

        [[2.1032e-01, 1.8348e-01],
         [1.2159e-01, 3.4903e-01]],

        [[4.9148e-02, 2.3872e-01],
         [9.7705e-01, 3.8386e-01]],

        [[8.9293e-01, 1.9490e-01],
         [9.7141e-01, 3.3342e-01]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
Global Adjusted Rand Index: 0.00035999121612196195
Average Adjusted Rand Index: -0.0001015014715569809
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -52398.3046875
Iteration 100: Loss = -30837.787109375
Iteration 200: Loss = -17115.68359375
Iteration 300: Loss = -13595.568359375
Iteration 400: Loss = -12983.166015625
Iteration 500: Loss = -12734.2265625
Iteration 600: Loss = -12602.744140625
Iteration 700: Loss = -12508.4033203125
Iteration 800: Loss = -12449.9609375
Iteration 900: Loss = -12422.041015625
Iteration 1000: Loss = -12404.7236328125
Iteration 1100: Loss = -12393.529296875
Iteration 1200: Loss = -12384.6015625
Iteration 1300: Loss = -12376.64453125
Iteration 1400: Loss = -12369.19140625
Iteration 1500: Loss = -12363.705078125
Iteration 1600: Loss = -12359.728515625
Iteration 1700: Loss = -12356.5478515625
Iteration 1800: Loss = -12353.890625
Iteration 1900: Loss = -12351.62109375
Iteration 2000: Loss = -12349.6640625
Iteration 2100: Loss = -12347.9541015625
Iteration 2200: Loss = -12346.451171875
Iteration 2300: Loss = -12345.125
Iteration 2400: Loss = -12343.9443359375
Iteration 2500: Loss = -12342.8916015625
Iteration 2600: Loss = -12341.9453125
Iteration 2700: Loss = -12341.0947265625
Iteration 2800: Loss = -12340.3251953125
Iteration 2900: Loss = -12339.6259765625
Iteration 3000: Loss = -12338.9912109375
Iteration 3100: Loss = -12338.4111328125
Iteration 3200: Loss = -12337.8818359375
Iteration 3300: Loss = -12337.3955078125
Iteration 3400: Loss = -12336.94921875
Iteration 3500: Loss = -12336.5390625
Iteration 3600: Loss = -12336.1591796875
Iteration 3700: Loss = -12335.810546875
Iteration 3800: Loss = -12335.486328125
Iteration 3900: Loss = -12335.185546875
Iteration 4000: Loss = -12334.9091796875
Iteration 4100: Loss = -12334.650390625
Iteration 4200: Loss = -12334.4091796875
Iteration 4300: Loss = -12334.1845703125
Iteration 4400: Loss = -12333.9765625
Iteration 4500: Loss = -12333.7822265625
Iteration 4600: Loss = -12333.6005859375
Iteration 4700: Loss = -12333.431640625
Iteration 4800: Loss = -12333.271484375
Iteration 4900: Loss = -12333.123046875
Iteration 5000: Loss = -12332.9814453125
Iteration 5100: Loss = -12332.8515625
Iteration 5200: Loss = -12332.728515625
Iteration 5300: Loss = -12332.6123046875
Iteration 5400: Loss = -12332.5048828125
Iteration 5500: Loss = -12332.40234375
Iteration 5600: Loss = -12332.3056640625
Iteration 5700: Loss = -12332.2177734375
Iteration 5800: Loss = -12332.1298828125
Iteration 5900: Loss = -12332.0498046875
Iteration 6000: Loss = -12331.9755859375
Iteration 6100: Loss = -12331.9033203125
Iteration 6200: Loss = -12331.8369140625
Iteration 6300: Loss = -12331.7744140625
Iteration 6400: Loss = -12331.712890625
Iteration 6500: Loss = -12331.658203125
Iteration 6600: Loss = -12331.60546875
Iteration 6700: Loss = -12331.5546875
Iteration 6800: Loss = -12331.5068359375
Iteration 6900: Loss = -12331.4638671875
Iteration 7000: Loss = -12331.421875
Iteration 7100: Loss = -12331.3818359375
Iteration 7200: Loss = -12331.3447265625
Iteration 7300: Loss = -12331.310546875
Iteration 7400: Loss = -12331.2783203125
Iteration 7500: Loss = -12331.2470703125
Iteration 7600: Loss = -12331.2177734375
Iteration 7700: Loss = -12331.19140625
Iteration 7800: Loss = -12331.1640625
Iteration 7900: Loss = -12331.1396484375
Iteration 8000: Loss = -12331.1171875
Iteration 8100: Loss = -12331.0947265625
Iteration 8200: Loss = -12331.07421875
Iteration 8300: Loss = -12331.0546875
Iteration 8400: Loss = -12331.0390625
Iteration 8500: Loss = -12331.0205078125
Iteration 8600: Loss = -12331.005859375
Iteration 8700: Loss = -12330.9921875
Iteration 8800: Loss = -12330.9765625
Iteration 8900: Loss = -12330.962890625
Iteration 9000: Loss = -12330.9501953125
Iteration 9100: Loss = -12330.9384765625
Iteration 9200: Loss = -12330.927734375
Iteration 9300: Loss = -12330.9169921875
Iteration 9400: Loss = -12330.9072265625
Iteration 9500: Loss = -12330.8974609375
Iteration 9600: Loss = -12330.8876953125
Iteration 9700: Loss = -12330.8798828125
Iteration 9800: Loss = -12330.8720703125
Iteration 9900: Loss = -12330.86328125
Iteration 10000: Loss = -12330.857421875
Iteration 10100: Loss = -12330.8486328125
Iteration 10200: Loss = -12330.8427734375
Iteration 10300: Loss = -12330.837890625
Iteration 10400: Loss = -12330.8291015625
Iteration 10500: Loss = -12330.82421875
Iteration 10600: Loss = -12330.8193359375
Iteration 10700: Loss = -12330.814453125
Iteration 10800: Loss = -12330.8076171875
Iteration 10900: Loss = -12330.8046875
Iteration 11000: Loss = -12330.7978515625
Iteration 11100: Loss = -12330.7958984375
Iteration 11200: Loss = -12330.7900390625
Iteration 11300: Loss = -12330.787109375
Iteration 11400: Loss = -12330.78125
Iteration 11500: Loss = -12330.78125
Iteration 11600: Loss = -12330.7763671875
Iteration 11700: Loss = -12330.771484375
Iteration 11800: Loss = -12330.76953125
Iteration 11900: Loss = -12330.7646484375
Iteration 12000: Loss = -12330.7626953125
Iteration 12100: Loss = -12330.759765625
Iteration 12200: Loss = -12330.7568359375
Iteration 12300: Loss = -12330.75390625
Iteration 12400: Loss = -12330.75390625
Iteration 12500: Loss = -12330.75
Iteration 12600: Loss = -12330.7470703125
Iteration 12700: Loss = -12330.7451171875
Iteration 12800: Loss = -12330.7431640625
Iteration 12900: Loss = -12330.7421875
Iteration 13000: Loss = -12330.7421875
Iteration 13100: Loss = -12330.7392578125
Iteration 13200: Loss = -12330.736328125
Iteration 13300: Loss = -12330.734375
Iteration 13400: Loss = -12330.734375
Iteration 13500: Loss = -12330.7314453125
Iteration 13600: Loss = -12330.73046875
Iteration 13700: Loss = -12330.73046875
Iteration 13800: Loss = -12330.728515625
Iteration 13900: Loss = -12330.7255859375
Iteration 14000: Loss = -12330.724609375
Iteration 14100: Loss = -12330.7236328125
Iteration 14200: Loss = -12330.7236328125
Iteration 14300: Loss = -12330.72265625
Iteration 14400: Loss = -12330.7216796875
Iteration 14500: Loss = -12330.7216796875
Iteration 14600: Loss = -12330.7197265625
Iteration 14700: Loss = -12330.71875
Iteration 14800: Loss = -12330.716796875
Iteration 14900: Loss = -12330.716796875
Iteration 15000: Loss = -12330.7177734375
1
Iteration 15100: Loss = -12330.71484375
Iteration 15200: Loss = -12330.71484375
Iteration 15300: Loss = -12330.71484375
Iteration 15400: Loss = -12330.712890625
Iteration 15500: Loss = -12330.712890625
Iteration 15600: Loss = -12330.712890625
Iteration 15700: Loss = -12330.712890625
Iteration 15800: Loss = -12330.712890625
Iteration 15900: Loss = -12330.7119140625
Iteration 16000: Loss = -12330.7099609375
Iteration 16100: Loss = -12330.7099609375
Iteration 16200: Loss = -12330.7080078125
Iteration 16300: Loss = -12330.708984375
1
Iteration 16400: Loss = -12330.7099609375
2
Iteration 16500: Loss = -12330.7099609375
3
Iteration 16600: Loss = -12330.7080078125
Iteration 16700: Loss = -12330.7080078125
Iteration 16800: Loss = -12330.70703125
Iteration 16900: Loss = -12330.7060546875
Iteration 17000: Loss = -12330.70703125
1
Iteration 17100: Loss = -12330.7060546875
Iteration 17200: Loss = -12330.70703125
1
Iteration 17300: Loss = -12330.7060546875
Iteration 17400: Loss = -12330.7060546875
Iteration 17500: Loss = -12330.705078125
Iteration 17600: Loss = -12330.7060546875
1
Iteration 17700: Loss = -12330.705078125
Iteration 17800: Loss = -12330.7041015625
Iteration 17900: Loss = -12330.7041015625
Iteration 18000: Loss = -12330.703125
Iteration 18100: Loss = -12330.7041015625
1
Iteration 18200: Loss = -12330.7041015625
2
Iteration 18300: Loss = -12330.701171875
Iteration 18400: Loss = -12330.703125
1
Iteration 18500: Loss = -12330.7021484375
2
Iteration 18600: Loss = -12330.703125
3
Iteration 18700: Loss = -12330.701171875
Iteration 18800: Loss = -12330.703125
1
Iteration 18900: Loss = -12330.7021484375
2
Iteration 19000: Loss = -12330.705078125
3
Iteration 19100: Loss = -12330.7021484375
4
Iteration 19200: Loss = -12330.701171875
Iteration 19300: Loss = -12330.7021484375
1
Iteration 19400: Loss = -12330.701171875
Iteration 19500: Loss = -12330.7021484375
1
Iteration 19600: Loss = -12330.703125
2
Iteration 19700: Loss = -12330.703125
3
Iteration 19800: Loss = -12330.7021484375
4
Iteration 19900: Loss = -12330.7021484375
5
Iteration 20000: Loss = -12330.7021484375
6
Iteration 20100: Loss = -12330.703125
7
Iteration 20200: Loss = -12330.7021484375
8
Iteration 20300: Loss = -12330.701171875
Iteration 20400: Loss = -12330.7021484375
1
Iteration 20500: Loss = -12330.7021484375
2
Iteration 20600: Loss = -12330.703125
3
Iteration 20700: Loss = -12330.7021484375
4
Iteration 20800: Loss = -12330.7021484375
5
Iteration 20900: Loss = -12330.703125
6
Iteration 21000: Loss = -12330.701171875
Iteration 21100: Loss = -12330.7021484375
1
Iteration 21200: Loss = -12330.701171875
Iteration 21300: Loss = -12330.7021484375
1
Iteration 21400: Loss = -12330.7001953125
Iteration 21500: Loss = -12330.7021484375
1
Iteration 21600: Loss = -12330.701171875
2
Iteration 21700: Loss = -12330.7021484375
3
Iteration 21800: Loss = -12330.7021484375
4
Iteration 21900: Loss = -12330.7021484375
5
Iteration 22000: Loss = -12330.703125
6
Iteration 22100: Loss = -12330.703125
7
Iteration 22200: Loss = -12330.703125
8
Iteration 22300: Loss = -12330.7021484375
9
Iteration 22400: Loss = -12330.701171875
10
Iteration 22500: Loss = -12330.701171875
11
Iteration 22600: Loss = -12330.7021484375
12
Iteration 22700: Loss = -12330.7021484375
13
Iteration 22800: Loss = -12330.7021484375
14
Iteration 22900: Loss = -12330.7021484375
15
Stopping early at iteration 22900 due to no improvement.
pi: tensor([[9.9999e-01, 9.0572e-06],
        [9.9541e-01, 4.5891e-03]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9987e-01, 1.3088e-04], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1984, 0.1896],
         [0.7915, 0.9441]],

        [[0.0513, 0.2459],
         [0.8232, 0.8732]],

        [[0.1956, 0.1979],
         [0.0245, 0.9766]],

        [[0.9871, 0.2158],
         [0.9712, 0.0492]],

        [[0.0222, 0.1868],
         [0.8909, 0.5311]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.00035999121612196195, 0.0] [-0.0001015014715569809, 0.0] [12327.244140625, 12330.7021484375]
-------------------------------------
This iteration is 36
True Objective function: Loss = -11989.560258293353
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -42800.36328125
Iteration 100: Loss = -28443.513671875
Iteration 200: Loss = -18482.05859375
Iteration 300: Loss = -13962.560546875
Iteration 400: Loss = -12830.89453125
Iteration 500: Loss = -12592.109375
Iteration 600: Loss = -12532.94921875
Iteration 700: Loss = -12512.12890625
Iteration 800: Loss = -12501.7490234375
Iteration 900: Loss = -12495.1982421875
Iteration 1000: Loss = -12490.1455078125
Iteration 1100: Loss = -12480.91796875
Iteration 1200: Loss = -12477.3935546875
Iteration 1300: Loss = -12475.98828125
Iteration 1400: Loss = -12474.9580078125
Iteration 1500: Loss = -12474.1572265625
Iteration 1600: Loss = -12473.5146484375
Iteration 1700: Loss = -12472.986328125
Iteration 1800: Loss = -12472.548828125
Iteration 1900: Loss = -12472.1806640625
Iteration 2000: Loss = -12471.8701171875
Iteration 2100: Loss = -12471.603515625
Iteration 2200: Loss = -12471.375
Iteration 2300: Loss = -12471.1728515625
Iteration 2400: Loss = -12470.9970703125
Iteration 2500: Loss = -12470.8408203125
Iteration 2600: Loss = -12470.6884765625
Iteration 2700: Loss = -12470.56640625
Iteration 2800: Loss = -12470.455078125
Iteration 2900: Loss = -12470.353515625
Iteration 3000: Loss = -12470.26171875
Iteration 3100: Loss = -12470.1787109375
Iteration 3200: Loss = -12470.1025390625
Iteration 3300: Loss = -12470.0322265625
Iteration 3400: Loss = -12469.96484375
Iteration 3500: Loss = -12469.90625
Iteration 3600: Loss = -12469.8505859375
Iteration 3700: Loss = -12469.798828125
Iteration 3800: Loss = -12469.751953125
Iteration 3900: Loss = -12469.70703125
Iteration 4000: Loss = -12469.6650390625
Iteration 4100: Loss = -12469.6259765625
Iteration 4200: Loss = -12469.58984375
Iteration 4300: Loss = -12469.556640625
Iteration 4400: Loss = -12469.5244140625
Iteration 4500: Loss = -12469.494140625
Iteration 4600: Loss = -12469.466796875
Iteration 4700: Loss = -12469.439453125
Iteration 4800: Loss = -12469.4140625
Iteration 4900: Loss = -12469.390625
Iteration 5000: Loss = -12469.369140625
Iteration 5100: Loss = -12469.34765625
Iteration 5200: Loss = -12469.326171875
Iteration 5300: Loss = -12469.3076171875
Iteration 5400: Loss = -12469.2900390625
Iteration 5500: Loss = -12469.2724609375
Iteration 5600: Loss = -12469.25390625
Iteration 5700: Loss = -12469.2392578125
Iteration 5800: Loss = -12469.22265625
Iteration 5900: Loss = -12469.20703125
Iteration 6000: Loss = -12469.1884765625
Iteration 6100: Loss = -12469.169921875
Iteration 6200: Loss = -12469.15234375
Iteration 6300: Loss = -12469.130859375
Iteration 6400: Loss = -12469.1044921875
Iteration 6500: Loss = -12469.0693359375
Iteration 6600: Loss = -12469.015625
Iteration 6700: Loss = -12468.931640625
Iteration 6800: Loss = -12468.8134765625
Iteration 6900: Loss = -12468.69140625
Iteration 7000: Loss = -12468.5966796875
Iteration 7100: Loss = -12468.5263671875
Iteration 7200: Loss = -12468.46875
Iteration 7300: Loss = -12468.421875
Iteration 7400: Loss = -12468.3837890625
Iteration 7500: Loss = -12468.3486328125
Iteration 7600: Loss = -12468.3193359375
Iteration 7700: Loss = -12468.2939453125
Iteration 7800: Loss = -12468.271484375
Iteration 7900: Loss = -12468.2509765625
Iteration 8000: Loss = -12468.2333984375
Iteration 8100: Loss = -12468.21484375
Iteration 8200: Loss = -12468.2001953125
Iteration 8300: Loss = -12468.1845703125
Iteration 8400: Loss = -12468.171875
Iteration 8500: Loss = -12468.1591796875
Iteration 8600: Loss = -12468.1474609375
Iteration 8700: Loss = -12468.1337890625
Iteration 8800: Loss = -12468.1220703125
Iteration 8900: Loss = -12468.109375
Iteration 9000: Loss = -12468.0986328125
Iteration 9100: Loss = -12468.0869140625
Iteration 9200: Loss = -12468.076171875
Iteration 9300: Loss = -12468.0625
Iteration 9400: Loss = -12468.0517578125
Iteration 9500: Loss = -12468.0390625
Iteration 9600: Loss = -12468.025390625
Iteration 9700: Loss = -12468.0146484375
Iteration 9800: Loss = -12467.9990234375
Iteration 9900: Loss = -12467.984375
Iteration 10000: Loss = -12467.9697265625
Iteration 10100: Loss = -12467.9521484375
Iteration 10200: Loss = -12467.9345703125
Iteration 10300: Loss = -12467.9169921875
Iteration 10400: Loss = -12467.8974609375
Iteration 10500: Loss = -12467.8759765625
Iteration 10600: Loss = -12467.8525390625
Iteration 10700: Loss = -12467.8271484375
Iteration 10800: Loss = -12467.7998046875
Iteration 10900: Loss = -12467.7724609375
Iteration 11000: Loss = -12467.736328125
Iteration 11100: Loss = -12467.7001953125
Iteration 11200: Loss = -12467.658203125
Iteration 11300: Loss = -12467.611328125
Iteration 11400: Loss = -12467.5576171875
Iteration 11500: Loss = -12467.498046875
Iteration 11600: Loss = -12467.427734375
Iteration 11700: Loss = -12467.3466796875
Iteration 11800: Loss = -12467.251953125
Iteration 11900: Loss = -12467.1396484375
Iteration 12000: Loss = -12467.0
Iteration 12100: Loss = -12466.826171875
Iteration 12200: Loss = -12466.6123046875
Iteration 12300: Loss = -12466.365234375
Iteration 12400: Loss = -12466.109375
Iteration 12500: Loss = -12465.8818359375
Iteration 12600: Loss = -12465.724609375
Iteration 12700: Loss = -12465.6591796875
Iteration 12800: Loss = -12465.6240234375
Iteration 12900: Loss = -12465.599609375
Iteration 13000: Loss = -12465.578125
Iteration 13100: Loss = -12465.552734375
Iteration 13200: Loss = -12465.533203125
Iteration 13300: Loss = -12465.51953125
Iteration 13400: Loss = -12465.5078125
Iteration 13500: Loss = -12465.4990234375
Iteration 13600: Loss = -12465.486328125
Iteration 13700: Loss = -12465.4775390625
Iteration 13800: Loss = -12465.470703125
Iteration 13900: Loss = -12465.4658203125
Iteration 14000: Loss = -12465.45703125
Iteration 14100: Loss = -12465.453125
Iteration 14200: Loss = -12465.4521484375
Iteration 14300: Loss = -12465.4482421875
Iteration 14400: Loss = -12465.4462890625
Iteration 14500: Loss = -12465.4443359375
Iteration 14600: Loss = -12465.4423828125
Iteration 14700: Loss = -12465.44140625
Iteration 14800: Loss = -12465.4404296875
Iteration 14900: Loss = -12465.4384765625
Iteration 15000: Loss = -12465.4384765625
Iteration 15100: Loss = -12465.439453125
1
Iteration 15200: Loss = -12465.4375
Iteration 15300: Loss = -12465.435546875
Iteration 15400: Loss = -12465.4375
1
Iteration 15500: Loss = -12465.435546875
Iteration 15600: Loss = -12465.435546875
Iteration 15700: Loss = -12465.435546875
Iteration 15800: Loss = -12465.4345703125
Iteration 15900: Loss = -12465.4365234375
1
Iteration 16000: Loss = -12465.43359375
Iteration 16100: Loss = -12465.4365234375
1
Iteration 16200: Loss = -12465.435546875
2
Iteration 16300: Loss = -12465.43359375
Iteration 16400: Loss = -12465.43359375
Iteration 16500: Loss = -12465.435546875
1
Iteration 16600: Loss = -12465.43359375
Iteration 16700: Loss = -12465.4345703125
1
Iteration 16800: Loss = -12465.43359375
Iteration 16900: Loss = -12465.4326171875
Iteration 17000: Loss = -12465.4326171875
Iteration 17100: Loss = -12465.4345703125
1
Iteration 17200: Loss = -12465.4326171875
Iteration 17300: Loss = -12465.43359375
1
Iteration 17400: Loss = -12465.43359375
2
Iteration 17500: Loss = -12465.431640625
Iteration 17600: Loss = -12465.43359375
1
Iteration 17700: Loss = -12465.4326171875
2
Iteration 17800: Loss = -12465.4326171875
3
Iteration 17900: Loss = -12465.4326171875
4
Iteration 18000: Loss = -12465.4345703125
5
Iteration 18100: Loss = -12465.4326171875
6
Iteration 18200: Loss = -12465.431640625
Iteration 18300: Loss = -12465.4306640625
Iteration 18400: Loss = -12465.4326171875
1
Iteration 18500: Loss = -12465.4326171875
2
Iteration 18600: Loss = -12465.4326171875
3
Iteration 18700: Loss = -12465.43359375
4
Iteration 18800: Loss = -12465.4326171875
5
Iteration 18900: Loss = -12465.4326171875
6
Iteration 19000: Loss = -12465.4306640625
Iteration 19100: Loss = -12465.431640625
1
Iteration 19200: Loss = -12465.4365234375
2
Iteration 19300: Loss = -12465.4326171875
3
Iteration 19400: Loss = -12465.431640625
4
Iteration 19500: Loss = -12465.4326171875
5
Iteration 19600: Loss = -12465.4326171875
6
Iteration 19700: Loss = -12465.431640625
7
Iteration 19800: Loss = -12465.431640625
8
Iteration 19900: Loss = -12465.4326171875
9
Iteration 20000: Loss = -12465.431640625
10
Iteration 20100: Loss = -12465.4345703125
11
Iteration 20200: Loss = -12465.431640625
12
Iteration 20300: Loss = -12465.431640625
13
Iteration 20400: Loss = -12465.4306640625
Iteration 20500: Loss = -12465.431640625
1
Iteration 20600: Loss = -12465.431640625
2
Iteration 20700: Loss = -12465.4326171875
3
Iteration 20800: Loss = -12465.431640625
4
Iteration 20900: Loss = -12465.431640625
5
Iteration 21000: Loss = -12465.431640625
6
Iteration 21100: Loss = -12465.431640625
7
Iteration 21200: Loss = -12465.431640625
8
Iteration 21300: Loss = -12465.431640625
9
Iteration 21400: Loss = -12465.431640625
10
Iteration 21500: Loss = -12465.431640625
11
Iteration 21600: Loss = -12465.4306640625
Iteration 21700: Loss = -12465.4306640625
Iteration 21800: Loss = -12465.431640625
1
Iteration 21900: Loss = -12465.431640625
2
Iteration 22000: Loss = -12465.431640625
3
Iteration 22100: Loss = -12465.431640625
4
Iteration 22200: Loss = -12465.431640625
5
Iteration 22300: Loss = -12465.4296875
Iteration 22400: Loss = -12465.431640625
1
Iteration 22500: Loss = -12465.4306640625
2
Iteration 22600: Loss = -12465.4326171875
3
Iteration 22700: Loss = -12465.431640625
4
Iteration 22800: Loss = -12465.4306640625
5
Iteration 22900: Loss = -12465.431640625
6
Iteration 23000: Loss = -12465.431640625
7
Iteration 23100: Loss = -12465.4306640625
8
Iteration 23200: Loss = -12465.4306640625
9
Iteration 23300: Loss = -12465.4306640625
10
Iteration 23400: Loss = -12465.4306640625
11
Iteration 23500: Loss = -12465.431640625
12
Iteration 23600: Loss = -12465.431640625
13
Iteration 23700: Loss = -12465.431640625
14
Iteration 23800: Loss = -12465.431640625
15
Stopping early at iteration 23800 due to no improvement.
pi: tensor([[5.3821e-05, 9.9995e-01],
        [1.2766e-02, 9.8723e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0759, 0.9241], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0995, 0.1305],
         [0.6418, 0.2048]],

        [[0.0959, 0.2204],
         [0.7191, 0.0477]],

        [[0.9744, 0.2394],
         [0.9864, 0.0100]],

        [[0.0088, 0.1718],
         [0.9639, 0.7812]],

        [[0.6102, 0.1670],
         [0.2363, 0.2935]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.003506908785360844
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 62
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0021235046098361083
Average Adjusted Rand Index: 0.0007013817570721688
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -42812.859375
Iteration 100: Loss = -25096.193359375
Iteration 200: Loss = -15286.931640625
Iteration 300: Loss = -13207.1904296875
Iteration 400: Loss = -12827.375
Iteration 500: Loss = -12697.7841796875
Iteration 600: Loss = -12624.0234375
Iteration 700: Loss = -12585.3134765625
Iteration 800: Loss = -12559.6728515625
Iteration 900: Loss = -12542.9228515625
Iteration 1000: Loss = -12530.916015625
Iteration 1100: Loss = -12520.158203125
Iteration 1200: Loss = -12509.8017578125
Iteration 1300: Loss = -12504.0185546875
Iteration 1400: Loss = -12499.4833984375
Iteration 1500: Loss = -12495.791015625
Iteration 1600: Loss = -12492.7275390625
Iteration 1700: Loss = -12490.1474609375
Iteration 1800: Loss = -12487.9501953125
Iteration 1900: Loss = -12486.0615234375
Iteration 2000: Loss = -12484.4228515625
Iteration 2100: Loss = -12482.9931640625
Iteration 2200: Loss = -12481.7353515625
Iteration 2300: Loss = -12480.623046875
Iteration 2400: Loss = -12479.634765625
Iteration 2500: Loss = -12478.75390625
Iteration 2600: Loss = -12477.96484375
Iteration 2700: Loss = -12477.2548828125
Iteration 2800: Loss = -12476.6142578125
Iteration 2900: Loss = -12476.03515625
Iteration 3000: Loss = -12475.5078125
Iteration 3100: Loss = -12475.029296875
Iteration 3200: Loss = -12474.5888671875
Iteration 3300: Loss = -12474.1904296875
Iteration 3400: Loss = -12473.8232421875
Iteration 3500: Loss = -12473.4833984375
Iteration 3600: Loss = -12473.173828125
Iteration 3700: Loss = -12472.888671875
Iteration 3800: Loss = -12472.62109375
Iteration 3900: Loss = -12472.3759765625
Iteration 4000: Loss = -12472.1494140625
Iteration 4100: Loss = -12471.9384765625
Iteration 4200: Loss = -12471.7431640625
Iteration 4300: Loss = -12471.55859375
Iteration 4400: Loss = -12471.38671875
Iteration 4500: Loss = -12471.2255859375
Iteration 4600: Loss = -12471.0791015625
Iteration 4700: Loss = -12470.939453125
Iteration 4800: Loss = -12470.810546875
Iteration 4900: Loss = -12470.6884765625
Iteration 5000: Loss = -12470.576171875
Iteration 5100: Loss = -12470.4697265625
Iteration 5200: Loss = -12470.37109375
Iteration 5300: Loss = -12470.27734375
Iteration 5400: Loss = -12470.189453125
Iteration 5500: Loss = -12470.1083984375
Iteration 5600: Loss = -12470.033203125
Iteration 5700: Loss = -12469.9609375
Iteration 5800: Loss = -12469.8955078125
Iteration 5900: Loss = -12469.833984375
Iteration 6000: Loss = -12469.7744140625
Iteration 6100: Loss = -12469.7197265625
Iteration 6200: Loss = -12469.66796875
Iteration 6300: Loss = -12469.6240234375
Iteration 6400: Loss = -12469.5791015625
Iteration 6500: Loss = -12469.537109375
Iteration 6600: Loss = -12469.5
Iteration 6700: Loss = -12469.4658203125
Iteration 6800: Loss = -12469.435546875
Iteration 6900: Loss = -12469.408203125
Iteration 7000: Loss = -12469.380859375
Iteration 7100: Loss = -12469.3583984375
Iteration 7200: Loss = -12469.3369140625
Iteration 7300: Loss = -12469.3154296875
Iteration 7400: Loss = -12469.2939453125
Iteration 7500: Loss = -12469.27734375
Iteration 7600: Loss = -12469.2607421875
Iteration 7700: Loss = -12469.244140625
Iteration 7800: Loss = -12469.2314453125
Iteration 7900: Loss = -12469.216796875
Iteration 8000: Loss = -12469.205078125
Iteration 8100: Loss = -12469.1943359375
Iteration 8200: Loss = -12469.1826171875
Iteration 8300: Loss = -12469.173828125
Iteration 8400: Loss = -12469.166015625
Iteration 8500: Loss = -12469.158203125
Iteration 8600: Loss = -12469.1494140625
Iteration 8700: Loss = -12469.142578125
Iteration 8800: Loss = -12469.1357421875
Iteration 8900: Loss = -12469.130859375
Iteration 9000: Loss = -12469.1240234375
Iteration 9100: Loss = -12469.1201171875
Iteration 9200: Loss = -12469.115234375
Iteration 9300: Loss = -12469.1103515625
Iteration 9400: Loss = -12469.1064453125
Iteration 9500: Loss = -12469.1025390625
Iteration 9600: Loss = -12469.1015625
Iteration 9700: Loss = -12469.095703125
Iteration 9800: Loss = -12469.0927734375
Iteration 9900: Loss = -12469.0908203125
Iteration 10000: Loss = -12469.0888671875
Iteration 10100: Loss = -12469.0869140625
Iteration 10200: Loss = -12469.0849609375
Iteration 10300: Loss = -12469.08203125
Iteration 10400: Loss = -12469.0791015625
Iteration 10500: Loss = -12469.0771484375
Iteration 10600: Loss = -12469.0751953125
Iteration 10700: Loss = -12469.07421875
Iteration 10800: Loss = -12469.0712890625
Iteration 10900: Loss = -12469.0712890625
Iteration 11000: Loss = -12469.0693359375
Iteration 11100: Loss = -12469.068359375
Iteration 11200: Loss = -12469.06640625
Iteration 11300: Loss = -12469.06640625
Iteration 11400: Loss = -12469.064453125
Iteration 11500: Loss = -12469.0615234375
Iteration 11600: Loss = -12469.0625
1
Iteration 11700: Loss = -12469.0615234375
Iteration 11800: Loss = -12469.0595703125
Iteration 11900: Loss = -12469.05859375
Iteration 12000: Loss = -12469.05859375
Iteration 12100: Loss = -12469.0576171875
Iteration 12200: Loss = -12469.056640625
Iteration 12300: Loss = -12469.0556640625
Iteration 12400: Loss = -12469.0537109375
Iteration 12500: Loss = -12469.0537109375
Iteration 12600: Loss = -12469.0546875
1
Iteration 12700: Loss = -12469.052734375
Iteration 12800: Loss = -12469.052734375
Iteration 12900: Loss = -12469.0537109375
1
Iteration 13000: Loss = -12469.0517578125
Iteration 13100: Loss = -12469.05078125
Iteration 13200: Loss = -12469.052734375
1
Iteration 13300: Loss = -12469.0498046875
Iteration 13400: Loss = -12469.05078125
1
Iteration 13500: Loss = -12469.048828125
Iteration 13600: Loss = -12469.046875
Iteration 13700: Loss = -12469.0478515625
1
Iteration 13800: Loss = -12469.046875
Iteration 13900: Loss = -12469.0458984375
Iteration 14000: Loss = -12469.0458984375
Iteration 14100: Loss = -12469.046875
1
Iteration 14200: Loss = -12469.0458984375
Iteration 14300: Loss = -12469.0458984375
Iteration 14400: Loss = -12469.044921875
Iteration 14500: Loss = -12469.0439453125
Iteration 14600: Loss = -12469.044921875
1
Iteration 14700: Loss = -12469.0439453125
Iteration 14800: Loss = -12469.0439453125
Iteration 14900: Loss = -12469.041015625
Iteration 15000: Loss = -12469.04296875
1
Iteration 15100: Loss = -12469.041015625
Iteration 15200: Loss = -12469.0390625
Iteration 15300: Loss = -12469.0341796875
Iteration 15400: Loss = -12468.896484375
Iteration 15500: Loss = -12467.9541015625
Iteration 15600: Loss = -12467.59375
Iteration 15700: Loss = -12464.8251953125
Iteration 15800: Loss = -12463.6630859375
Iteration 15900: Loss = -12463.498046875
Iteration 16000: Loss = -12463.4365234375
Iteration 16100: Loss = -12463.390625
Iteration 16200: Loss = -12463.3798828125
Iteration 16300: Loss = -12463.3759765625
Iteration 16400: Loss = -12463.3701171875
Iteration 16500: Loss = -12463.369140625
Iteration 16600: Loss = -12463.3662109375
Iteration 16700: Loss = -12463.3662109375
Iteration 16800: Loss = -12463.3642578125
Iteration 16900: Loss = -12463.3642578125
Iteration 17000: Loss = -12463.3583984375
Iteration 17100: Loss = -12463.353515625
Iteration 17200: Loss = -12463.3515625
Iteration 17300: Loss = -12463.3515625
Iteration 17400: Loss = -12463.349609375
Iteration 17500: Loss = -12463.3505859375
1
Iteration 17600: Loss = -12463.349609375
Iteration 17700: Loss = -12463.34765625
Iteration 17800: Loss = -12463.3486328125
1
Iteration 17900: Loss = -12463.3466796875
Iteration 18000: Loss = -12463.3466796875
Iteration 18100: Loss = -12463.345703125
Iteration 18200: Loss = -12463.34765625
1
Iteration 18300: Loss = -12463.34765625
2
Iteration 18400: Loss = -12463.3447265625
Iteration 18500: Loss = -12463.345703125
1
Iteration 18600: Loss = -12463.3466796875
2
Iteration 18700: Loss = -12463.34375
Iteration 18800: Loss = -12463.34375
Iteration 18900: Loss = -12463.345703125
1
Iteration 19000: Loss = -12463.34375
Iteration 19100: Loss = -12463.34375
Iteration 19200: Loss = -12463.34375
Iteration 19300: Loss = -12463.34375
Iteration 19400: Loss = -12463.3427734375
Iteration 19500: Loss = -12463.3447265625
1
Iteration 19600: Loss = -12463.3447265625
2
Iteration 19700: Loss = -12463.3427734375
Iteration 19800: Loss = -12463.3447265625
1
Iteration 19900: Loss = -12463.34375
2
Iteration 20000: Loss = -12463.34375
3
Iteration 20100: Loss = -12463.3427734375
Iteration 20200: Loss = -12463.34375
1
Iteration 20300: Loss = -12463.3427734375
Iteration 20400: Loss = -12463.3427734375
Iteration 20500: Loss = -12463.34375
1
Iteration 20600: Loss = -12463.34375
2
Iteration 20700: Loss = -12463.341796875
Iteration 20800: Loss = -12463.341796875
Iteration 20900: Loss = -12463.3427734375
1
Iteration 21000: Loss = -12463.3427734375
2
Iteration 21100: Loss = -12463.34375
3
Iteration 21200: Loss = -12463.3427734375
4
Iteration 21300: Loss = -12463.3427734375
5
Iteration 21400: Loss = -12463.34375
6
Iteration 21500: Loss = -12463.3427734375
7
Iteration 21600: Loss = -12463.3427734375
8
Iteration 21700: Loss = -12463.341796875
Iteration 21800: Loss = -12463.341796875
Iteration 21900: Loss = -12463.341796875
Iteration 22000: Loss = -12463.341796875
Iteration 22100: Loss = -12463.341796875
Iteration 22200: Loss = -12463.34375
1
Iteration 22300: Loss = -12463.3427734375
2
Iteration 22400: Loss = -12463.3427734375
3
Iteration 22500: Loss = -12463.341796875
Iteration 22600: Loss = -12463.341796875
Iteration 22700: Loss = -12463.3427734375
1
Iteration 22800: Loss = -12463.34375
2
Iteration 22900: Loss = -12463.3427734375
3
Iteration 23000: Loss = -12463.341796875
Iteration 23100: Loss = -12463.3447265625
1
Iteration 23200: Loss = -12463.341796875
Iteration 23300: Loss = -12463.3408203125
Iteration 23400: Loss = -12463.341796875
1
Iteration 23500: Loss = -12463.341796875
2
Iteration 23600: Loss = -12463.341796875
3
Iteration 23700: Loss = -12463.3427734375
4
Iteration 23800: Loss = -12463.341796875
5
Iteration 23900: Loss = -12463.34375
6
Iteration 24000: Loss = -12463.3427734375
7
Iteration 24100: Loss = -12463.3408203125
Iteration 24200: Loss = -12463.3408203125
Iteration 24300: Loss = -12463.34375
1
Iteration 24400: Loss = -12463.341796875
2
Iteration 24500: Loss = -12463.341796875
3
Iteration 24600: Loss = -12463.341796875
4
Iteration 24700: Loss = -12463.341796875
5
Iteration 24800: Loss = -12463.3447265625
6
Iteration 24900: Loss = -12463.3408203125
Iteration 25000: Loss = -12463.341796875
1
Iteration 25100: Loss = -12463.3427734375
2
Iteration 25200: Loss = -12463.341796875
3
Iteration 25300: Loss = -12463.3427734375
4
Iteration 25400: Loss = -12463.34375
5
Iteration 25500: Loss = -12463.3408203125
Iteration 25600: Loss = -12463.3427734375
1
Iteration 25700: Loss = -12463.3427734375
2
Iteration 25800: Loss = -12463.34375
3
Iteration 25900: Loss = -12463.3408203125
Iteration 26000: Loss = -12463.341796875
1
Iteration 26100: Loss = -12463.34375
2
Iteration 26200: Loss = -12463.34375
3
Iteration 26300: Loss = -12463.33984375
Iteration 26400: Loss = -12463.341796875
1
Iteration 26500: Loss = -12463.341796875
2
Iteration 26600: Loss = -12463.341796875
3
Iteration 26700: Loss = -12463.341796875
4
Iteration 26800: Loss = -12463.34375
5
Iteration 26900: Loss = -12463.3427734375
6
Iteration 27000: Loss = -12463.3427734375
7
Iteration 27100: Loss = -12463.34375
8
Iteration 27200: Loss = -12463.3427734375
9
Iteration 27300: Loss = -12463.341796875
10
Iteration 27400: Loss = -12463.3427734375
11
Iteration 27500: Loss = -12463.341796875
12
Iteration 27600: Loss = -12463.3427734375
13
Iteration 27700: Loss = -12463.341796875
14
Iteration 27800: Loss = -12463.341796875
15
Stopping early at iteration 27800 due to no improvement.
pi: tensor([[1.0000e+00, 4.2309e-06],
        [3.7978e-01, 6.2022e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9562, 0.0438], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2051, 0.1073],
         [0.8638, 0.0463]],

        [[0.9412, 0.1353],
         [0.6964, 0.9776]],

        [[0.0275, 0.1703],
         [0.3034, 0.0216]],

        [[0.5446, 0.2189],
         [0.0262, 0.7585]],

        [[0.9891, 0.2016],
         [0.9576, 0.0145]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.003506908785360844
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.002428666622128136
Average Adjusted Rand Index: 0.0008629979186883304
[0.0021235046098361083, 0.002428666622128136] [0.0007013817570721688, 0.0008629979186883304] [12465.431640625, 12463.341796875]
-------------------------------------
This iteration is 37
True Objective function: Loss = -11867.980590644991
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28991.859375
Iteration 100: Loss = -18813.806640625
Iteration 200: Loss = -13682.021484375
Iteration 300: Loss = -12875.33203125
Iteration 400: Loss = -12662.6669921875
Iteration 500: Loss = -12593.1357421875
Iteration 600: Loss = -12554.3662109375
Iteration 700: Loss = -12522.7841796875
Iteration 800: Loss = -12497.380859375
Iteration 900: Loss = -12473.099609375
Iteration 1000: Loss = -12450.2900390625
Iteration 1100: Loss = -12432.58984375
Iteration 1200: Loss = -12418.3017578125
Iteration 1300: Loss = -12404.2060546875
Iteration 1400: Loss = -12387.3994140625
Iteration 1500: Loss = -12377.123046875
Iteration 1600: Loss = -12372.318359375
Iteration 1700: Loss = -12366.4453125
Iteration 1800: Loss = -12362.26953125
Iteration 1900: Loss = -12356.185546875
Iteration 2000: Loss = -12348.0673828125
Iteration 2100: Loss = -12339.49609375
Iteration 2200: Loss = -12331.6923828125
Iteration 2300: Loss = -12327.083984375
Iteration 2400: Loss = -12323.4697265625
Iteration 2500: Loss = -12321.0615234375
Iteration 2600: Loss = -12316.1572265625
Iteration 2700: Loss = -12313.7001953125
Iteration 2800: Loss = -12311.5048828125
Iteration 2900: Loss = -12308.3623046875
Iteration 3000: Loss = -12305.9287109375
Iteration 3100: Loss = -12304.044921875
Iteration 3200: Loss = -12300.37890625
Iteration 3300: Loss = -12298.4375
Iteration 3400: Loss = -12296.181640625
Iteration 3500: Loss = -12293.9697265625
Iteration 3600: Loss = -12291.982421875
Iteration 3700: Loss = -12290.3349609375
Iteration 3800: Loss = -12288.736328125
Iteration 3900: Loss = -12288.0546875
Iteration 4000: Loss = -12287.498046875
Iteration 4100: Loss = -12286.0908203125
Iteration 4200: Loss = -12285.51953125
Iteration 4300: Loss = -12285.240234375
Iteration 4400: Loss = -12285.025390625
Iteration 4500: Loss = -12284.8564453125
Iteration 4600: Loss = -12284.7177734375
Iteration 4700: Loss = -12284.59765625
Iteration 4800: Loss = -12284.4921875
Iteration 4900: Loss = -12284.396484375
Iteration 5000: Loss = -12284.30859375
Iteration 5100: Loss = -12284.2294921875
Iteration 5200: Loss = -12284.15625
Iteration 5300: Loss = -12284.0888671875
Iteration 5400: Loss = -12284.0234375
Iteration 5500: Loss = -12283.958984375
Iteration 5600: Loss = -12283.89453125
Iteration 5700: Loss = -12283.8212890625
Iteration 5800: Loss = -12283.7451171875
Iteration 5900: Loss = -12283.6630859375
Iteration 6000: Loss = -12283.4658203125
Iteration 6100: Loss = -12282.470703125
Iteration 6200: Loss = -12281.8994140625
Iteration 6300: Loss = -12281.49609375
Iteration 6400: Loss = -12277.17578125
Iteration 6500: Loss = -12276.53125
Iteration 6600: Loss = -12276.4091796875
Iteration 6700: Loss = -12276.3525390625
Iteration 6800: Loss = -12276.3134765625
Iteration 6900: Loss = -12276.2802734375
Iteration 7000: Loss = -12276.251953125
Iteration 7100: Loss = -12276.2255859375
Iteration 7200: Loss = -12276.203125
Iteration 7300: Loss = -12276.1826171875
Iteration 7400: Loss = -12276.162109375
Iteration 7500: Loss = -12276.14453125
Iteration 7600: Loss = -12276.12890625
Iteration 7700: Loss = -12276.115234375
Iteration 7800: Loss = -12276.1005859375
Iteration 7900: Loss = -12276.0859375
Iteration 8000: Loss = -12276.072265625
Iteration 8100: Loss = -12276.0625
Iteration 8200: Loss = -12276.05078125
Iteration 8300: Loss = -12276.0419921875
Iteration 8400: Loss = -12276.0302734375
Iteration 8500: Loss = -12276.021484375
Iteration 8600: Loss = -12276.013671875
Iteration 8700: Loss = -12276.0029296875
Iteration 8800: Loss = -12275.9951171875
Iteration 8900: Loss = -12275.98828125
Iteration 9000: Loss = -12275.98046875
Iteration 9100: Loss = -12275.970703125
Iteration 9200: Loss = -12275.9638671875
Iteration 9300: Loss = -12275.9580078125
Iteration 9400: Loss = -12275.9501953125
Iteration 9500: Loss = -12275.943359375
Iteration 9600: Loss = -12275.9365234375
Iteration 9700: Loss = -12275.9296875
Iteration 9800: Loss = -12275.921875
Iteration 9900: Loss = -12275.91015625
Iteration 10000: Loss = -12275.8076171875
Iteration 10100: Loss = -12275.77734375
Iteration 10200: Loss = -12275.7646484375
Iteration 10300: Loss = -12275.7568359375
Iteration 10400: Loss = -12275.75
Iteration 10500: Loss = -12275.744140625
Iteration 10600: Loss = -12275.7392578125
Iteration 10700: Loss = -12275.7333984375
Iteration 10800: Loss = -12275.7294921875
Iteration 10900: Loss = -12275.72265625
Iteration 11000: Loss = -12275.716796875
Iteration 11100: Loss = -12275.189453125
Iteration 11200: Loss = -12274.009765625
Iteration 11300: Loss = -12274.0
Iteration 11400: Loss = -12273.994140625
Iteration 11500: Loss = -12273.9921875
Iteration 11600: Loss = -12273.9873046875
Iteration 11700: Loss = -12273.9853515625
Iteration 11800: Loss = -12273.982421875
Iteration 11900: Loss = -12273.98046875
Iteration 12000: Loss = -12273.9794921875
Iteration 12100: Loss = -12273.9765625
Iteration 12200: Loss = -12273.9755859375
Iteration 12300: Loss = -12273.9736328125
Iteration 12400: Loss = -12273.970703125
Iteration 12500: Loss = -12273.96875
Iteration 12600: Loss = -12273.966796875
Iteration 12700: Loss = -12273.96484375
Iteration 12800: Loss = -12273.9599609375
Iteration 12900: Loss = -12273.951171875
Iteration 13000: Loss = -12273.943359375
Iteration 13100: Loss = -12273.94140625
Iteration 13200: Loss = -12273.9365234375
Iteration 13300: Loss = -12273.935546875
Iteration 13400: Loss = -12273.931640625
Iteration 13500: Loss = -12273.931640625
Iteration 13600: Loss = -12273.9296875
Iteration 13700: Loss = -12273.9287109375
Iteration 13800: Loss = -12273.9296875
1
Iteration 13900: Loss = -12273.9267578125
Iteration 14000: Loss = -12273.927734375
1
Iteration 14100: Loss = -12273.9267578125
Iteration 14200: Loss = -12273.9248046875
Iteration 14300: Loss = -12273.9248046875
Iteration 14400: Loss = -12273.9248046875
Iteration 14500: Loss = -12273.9140625
Iteration 14600: Loss = -12273.912109375
Iteration 14700: Loss = -12273.9111328125
Iteration 14800: Loss = -12273.9111328125
Iteration 14900: Loss = -12273.91015625
Iteration 15000: Loss = -12273.9111328125
1
Iteration 15100: Loss = -12273.91015625
Iteration 15200: Loss = -12273.9091796875
Iteration 15300: Loss = -12273.908203125
Iteration 15400: Loss = -12273.9091796875
1
Iteration 15500: Loss = -12273.9091796875
2
Iteration 15600: Loss = -12273.908203125
Iteration 15700: Loss = -12273.9033203125
Iteration 15800: Loss = -12271.74609375
Iteration 15900: Loss = -12270.0751953125
Iteration 16000: Loss = -12270.07421875
Iteration 16100: Loss = -12270.0751953125
1
Iteration 16200: Loss = -12270.0751953125
2
Iteration 16300: Loss = -12270.0732421875
Iteration 16400: Loss = -12270.0732421875
Iteration 16500: Loss = -12270.072265625
Iteration 16600: Loss = -12270.068359375
Iteration 16700: Loss = -12269.96484375
Iteration 16800: Loss = -12269.962890625
Iteration 16900: Loss = -12269.9638671875
1
Iteration 17000: Loss = -12269.962890625
Iteration 17100: Loss = -12269.9638671875
1
Iteration 17200: Loss = -12269.962890625
Iteration 17300: Loss = -12269.9619140625
Iteration 17400: Loss = -12269.9619140625
Iteration 17500: Loss = -12269.962890625
1
Iteration 17600: Loss = -12269.9619140625
Iteration 17700: Loss = -12269.9638671875
1
Iteration 17800: Loss = -12269.9619140625
Iteration 17900: Loss = -12269.962890625
1
Iteration 18000: Loss = -12269.9619140625
Iteration 18100: Loss = -12269.9619140625
Iteration 18200: Loss = -12269.9609375
Iteration 18300: Loss = -12269.9609375
Iteration 18400: Loss = -12269.9609375
Iteration 18500: Loss = -12269.9619140625
1
Iteration 18600: Loss = -12269.9609375
Iteration 18700: Loss = -12269.9609375
Iteration 18800: Loss = -12269.9619140625
1
Iteration 18900: Loss = -12269.9619140625
2
Iteration 19000: Loss = -12269.9609375
Iteration 19100: Loss = -12269.9609375
Iteration 19200: Loss = -12269.9609375
Iteration 19300: Loss = -12269.9619140625
1
Iteration 19400: Loss = -12269.9609375
Iteration 19500: Loss = -12269.9609375
Iteration 19600: Loss = -12269.9619140625
1
Iteration 19700: Loss = -12269.9619140625
2
Iteration 19800: Loss = -12269.9609375
Iteration 19900: Loss = -12269.9619140625
1
Iteration 20000: Loss = -12269.9609375
Iteration 20100: Loss = -12269.9609375
Iteration 20200: Loss = -12269.9609375
Iteration 20300: Loss = -12269.9599609375
Iteration 20400: Loss = -12269.9609375
1
Iteration 20500: Loss = -12269.9609375
2
Iteration 20600: Loss = -12269.9609375
3
Iteration 20700: Loss = -12269.9609375
4
Iteration 20800: Loss = -12269.958984375
Iteration 20900: Loss = -12269.9599609375
1
Iteration 21000: Loss = -12269.9609375
2
Iteration 21100: Loss = -12269.9599609375
3
Iteration 21200: Loss = -12269.85546875
Iteration 21300: Loss = -12269.625
Iteration 21400: Loss = -12269.625
Iteration 21500: Loss = -12269.625
Iteration 21600: Loss = -12269.626953125
1
Iteration 21700: Loss = -12269.6240234375
Iteration 21800: Loss = -12269.626953125
1
Iteration 21900: Loss = -12269.6279296875
2
Iteration 22000: Loss = -12269.625
3
Iteration 22100: Loss = -12269.5478515625
Iteration 22200: Loss = -12269.4921875
Iteration 22300: Loss = -12269.4931640625
1
Iteration 22400: Loss = -12269.4951171875
2
Iteration 22500: Loss = -12269.4931640625
3
Iteration 22600: Loss = -12269.4921875
Iteration 22700: Loss = -12269.4921875
Iteration 22800: Loss = -12269.494140625
1
Iteration 22900: Loss = -12269.4912109375
Iteration 23000: Loss = -12269.4912109375
Iteration 23100: Loss = -12269.4921875
1
Iteration 23200: Loss = -12269.4931640625
2
Iteration 23300: Loss = -12269.4921875
3
Iteration 23400: Loss = -12269.4912109375
Iteration 23500: Loss = -12269.4921875
1
Iteration 23600: Loss = -12269.4912109375
Iteration 23700: Loss = -12269.4912109375
Iteration 23800: Loss = -12269.4912109375
Iteration 23900: Loss = -12269.4931640625
1
Iteration 24000: Loss = -12269.4931640625
2
Iteration 24100: Loss = -12269.4921875
3
Iteration 24200: Loss = -12269.4921875
4
Iteration 24300: Loss = -12269.4931640625
5
Iteration 24400: Loss = -12269.4921875
6
Iteration 24500: Loss = -12269.4921875
7
Iteration 24600: Loss = -12269.4931640625
8
Iteration 24700: Loss = -12269.4921875
9
Iteration 24800: Loss = -12269.4892578125
Iteration 24900: Loss = -12269.4912109375
1
Iteration 25000: Loss = -12269.2177734375
Iteration 25100: Loss = -12269.2109375
Iteration 25200: Loss = -12269.212890625
1
Iteration 25300: Loss = -12269.2109375
Iteration 25400: Loss = -12269.2119140625
1
Iteration 25500: Loss = -12269.2119140625
2
Iteration 25600: Loss = -12269.212890625
3
Iteration 25700: Loss = -12269.2109375
Iteration 25800: Loss = -12269.2109375
Iteration 25900: Loss = -12269.2109375
Iteration 26000: Loss = -12269.2119140625
1
Iteration 26100: Loss = -12269.2109375
Iteration 26200: Loss = -12269.2099609375
Iteration 26300: Loss = -12269.2099609375
Iteration 26400: Loss = -12269.2099609375
Iteration 26500: Loss = -12269.2109375
1
Iteration 26600: Loss = -12269.2099609375
Iteration 26700: Loss = -12269.2080078125
Iteration 26800: Loss = -12269.2099609375
1
Iteration 26900: Loss = -12269.2099609375
2
Iteration 27000: Loss = -12269.208984375
3
Iteration 27100: Loss = -12269.2099609375
4
Iteration 27200: Loss = -12269.2099609375
5
Iteration 27300: Loss = -12269.2099609375
6
Iteration 27400: Loss = -12269.2080078125
Iteration 27500: Loss = -12269.2099609375
1
Iteration 27600: Loss = -12269.208984375
2
Iteration 27700: Loss = -12269.208984375
3
Iteration 27800: Loss = -12269.2080078125
Iteration 27900: Loss = -12269.2099609375
1
Iteration 28000: Loss = -12269.2099609375
2
Iteration 28100: Loss = -12269.208984375
3
Iteration 28200: Loss = -12269.2099609375
4
Iteration 28300: Loss = -12269.2080078125
Iteration 28400: Loss = -12269.2099609375
1
Iteration 28500: Loss = -12269.208984375
2
Iteration 28600: Loss = -12269.2080078125
Iteration 28700: Loss = -12269.2099609375
1
Iteration 28800: Loss = -12269.2099609375
2
Iteration 28900: Loss = -12269.2080078125
Iteration 29000: Loss = -12269.2099609375
1
Iteration 29100: Loss = -12269.2080078125
Iteration 29200: Loss = -12269.2099609375
1
Iteration 29300: Loss = -12269.208984375
2
Iteration 29400: Loss = -12269.078125
Iteration 29500: Loss = -12269.0771484375
Iteration 29600: Loss = -12268.873046875
Iteration 29700: Loss = -12268.7080078125
Iteration 29800: Loss = -12268.607421875
Iteration 29900: Loss = -12268.607421875
pi: tensor([[2.2065e-06, 1.0000e+00],
        [1.2306e-01, 8.7694e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5259, 0.4741], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3434, 0.1038],
         [0.9092, 0.2046]],

        [[0.7094, 0.1739],
         [0.0106, 0.1055]],

        [[0.9310, 0.1313],
         [0.0994, 0.0170]],

        [[0.9838, 0.2318],
         [0.9324, 0.9880]],

        [[0.8873, 0.2571],
         [0.0203, 0.7095]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 29
Adjusted Rand Index: 0.17008378432120727
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.007061766115298604
Average Adjusted Rand Index: 0.2181783730258576
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23537.376953125
Iteration 100: Loss = -16251.4755859375
Iteration 200: Loss = -13375.388671875
Iteration 300: Loss = -12645.4462890625
Iteration 400: Loss = -12505.056640625
Iteration 500: Loss = -12438.9462890625
Iteration 600: Loss = -12396.80078125
Iteration 700: Loss = -12368.22265625
Iteration 800: Loss = -12346.099609375
Iteration 900: Loss = -12329.611328125
Iteration 1000: Loss = -12319.3193359375
Iteration 1100: Loss = -12309.845703125
Iteration 1200: Loss = -12299.4775390625
Iteration 1300: Loss = -12291.095703125
Iteration 1400: Loss = -12284.15625
Iteration 1500: Loss = -12274.9228515625
Iteration 1600: Loss = -12270.8955078125
Iteration 1700: Loss = -12268.478515625
Iteration 1800: Loss = -12266.5078125
Iteration 1900: Loss = -12264.5068359375
Iteration 2000: Loss = -12262.1044921875
Iteration 2100: Loss = -12258.9228515625
Iteration 2200: Loss = -12255.25
Iteration 2300: Loss = -12253.11328125
Iteration 2400: Loss = -12251.4873046875
Iteration 2500: Loss = -12241.685546875
Iteration 2600: Loss = -12235.76953125
Iteration 2700: Loss = -12234.8134765625
Iteration 2800: Loss = -12233.9189453125
Iteration 2900: Loss = -12233.2177734375
Iteration 3000: Loss = -12232.1572265625
Iteration 3100: Loss = -12230.564453125
Iteration 3200: Loss = -12228.8125
Iteration 3300: Loss = -12226.0234375
Iteration 3400: Loss = -12221.62109375
Iteration 3500: Loss = -12215.974609375
Iteration 3600: Loss = -12204.01171875
Iteration 3700: Loss = -12181.6708984375
Iteration 3800: Loss = -12149.068359375
Iteration 3900: Loss = -12133.341796875
Iteration 4000: Loss = -12114.5693359375
Iteration 4100: Loss = -12092.1640625
Iteration 4200: Loss = -12070.5556640625
Iteration 4300: Loss = -12062.1044921875
Iteration 4400: Loss = -12027.48046875
Iteration 4500: Loss = -12008.068359375
Iteration 4600: Loss = -12000.3720703125
Iteration 4700: Loss = -11981.869140625
Iteration 4800: Loss = -11974.87890625
Iteration 4900: Loss = -11968.8369140625
Iteration 5000: Loss = -11962.755859375
Iteration 5100: Loss = -11958.0859375
Iteration 5200: Loss = -11955.111328125
Iteration 5300: Loss = -11951.5146484375
Iteration 5400: Loss = -11951.216796875
Iteration 5500: Loss = -11946.5830078125
Iteration 5600: Loss = -11946.3544921875
Iteration 5700: Loss = -11945.9306640625
Iteration 5800: Loss = -11938.8740234375
Iteration 5900: Loss = -11937.26953125
Iteration 6000: Loss = -11933.966796875
Iteration 6100: Loss = -11923.423828125
Iteration 6200: Loss = -11923.279296875
Iteration 6300: Loss = -11923.208984375
Iteration 6400: Loss = -11923.16015625
Iteration 6500: Loss = -11923.1220703125
Iteration 6600: Loss = -11923.0908203125
Iteration 6700: Loss = -11923.06640625
Iteration 6800: Loss = -11923.04296875
Iteration 6900: Loss = -11923.0224609375
Iteration 7000: Loss = -11923.005859375
Iteration 7100: Loss = -11922.9873046875
Iteration 7200: Loss = -11922.97265625
Iteration 7300: Loss = -11922.9599609375
Iteration 7400: Loss = -11922.9462890625
Iteration 7500: Loss = -11922.9384765625
Iteration 7600: Loss = -11922.92578125
Iteration 7700: Loss = -11922.91796875
Iteration 7800: Loss = -11922.91015625
Iteration 7900: Loss = -11922.9033203125
Iteration 8000: Loss = -11922.896484375
Iteration 8100: Loss = -11922.890625
Iteration 8200: Loss = -11922.8828125
Iteration 8300: Loss = -11922.876953125
Iteration 8400: Loss = -11922.869140625
Iteration 8500: Loss = -11922.857421875
Iteration 8600: Loss = -11920.828125
Iteration 8700: Loss = -11912.689453125
Iteration 8800: Loss = -11912.6201171875
Iteration 8900: Loss = -11912.5927734375
Iteration 9000: Loss = -11912.5791015625
Iteration 9100: Loss = -11912.5693359375
Iteration 9200: Loss = -11912.55859375
Iteration 9300: Loss = -11912.5537109375
Iteration 9400: Loss = -11912.5478515625
Iteration 9500: Loss = -11912.5439453125
Iteration 9600: Loss = -11912.5400390625
Iteration 9700: Loss = -11912.53515625
Iteration 9800: Loss = -11912.5283203125
Iteration 9900: Loss = -11904.8916015625
Iteration 10000: Loss = -11904.6435546875
Iteration 10100: Loss = -11904.6142578125
Iteration 10200: Loss = -11904.599609375
Iteration 10300: Loss = -11904.58984375
Iteration 10400: Loss = -11902.2099609375
Iteration 10500: Loss = -11900.71484375
Iteration 10600: Loss = -11900.7021484375
Iteration 10700: Loss = -11900.693359375
Iteration 10800: Loss = -11900.6904296875
Iteration 10900: Loss = -11900.6865234375
Iteration 11000: Loss = -11900.68359375
Iteration 11100: Loss = -11900.681640625
Iteration 11200: Loss = -11900.6796875
Iteration 11300: Loss = -11900.6787109375
Iteration 11400: Loss = -11900.6767578125
Iteration 11500: Loss = -11900.6748046875
Iteration 11600: Loss = -11900.6748046875
Iteration 11700: Loss = -11900.6728515625
Iteration 11800: Loss = -11900.6708984375
Iteration 11900: Loss = -11900.669921875
Iteration 12000: Loss = -11900.6689453125
Iteration 12100: Loss = -11900.6669921875
Iteration 12200: Loss = -11900.66796875
1
Iteration 12300: Loss = -11900.666015625
Iteration 12400: Loss = -11900.6669921875
1
Iteration 12500: Loss = -11900.6650390625
Iteration 12600: Loss = -11900.6640625
Iteration 12700: Loss = -11900.6640625
Iteration 12800: Loss = -11900.662109375
Iteration 12900: Loss = -11900.662109375
Iteration 13000: Loss = -11900.662109375
Iteration 13100: Loss = -11900.662109375
Iteration 13200: Loss = -11900.6611328125
Iteration 13300: Loss = -11900.662109375
1
Iteration 13400: Loss = -11900.66015625
Iteration 13500: Loss = -11900.66015625
Iteration 13600: Loss = -11900.66015625
Iteration 13700: Loss = -11900.66015625
Iteration 13800: Loss = -11900.6591796875
Iteration 13900: Loss = -11900.6591796875
Iteration 14000: Loss = -11900.6591796875
Iteration 14100: Loss = -11900.658203125
Iteration 14200: Loss = -11900.658203125
Iteration 14300: Loss = -11900.658203125
Iteration 14400: Loss = -11900.658203125
Iteration 14500: Loss = -11900.658203125
Iteration 14600: Loss = -11900.6591796875
1
Iteration 14700: Loss = -11900.658203125
Iteration 14800: Loss = -11900.6572265625
Iteration 14900: Loss = -11900.658203125
1
Iteration 15000: Loss = -11900.6572265625
Iteration 15100: Loss = -11900.65625
Iteration 15200: Loss = -11900.6572265625
1
Iteration 15300: Loss = -11900.65625
Iteration 15400: Loss = -11900.6572265625
1
Iteration 15500: Loss = -11900.65625
Iteration 15600: Loss = -11900.6572265625
1
Iteration 15700: Loss = -11900.65625
Iteration 15800: Loss = -11900.65625
Iteration 15900: Loss = -11900.65625
Iteration 16000: Loss = -11900.6552734375
Iteration 16100: Loss = -11900.654296875
Iteration 16200: Loss = -11900.6552734375
1
Iteration 16300: Loss = -11900.6552734375
2
Iteration 16400: Loss = -11900.65625
3
Iteration 16500: Loss = -11900.65625
4
Iteration 16600: Loss = -11900.65625
5
Iteration 16700: Loss = -11900.65625
6
Iteration 16800: Loss = -11900.6552734375
7
Iteration 16900: Loss = -11900.654296875
Iteration 17000: Loss = -11900.654296875
Iteration 17100: Loss = -11900.654296875
Iteration 17200: Loss = -11900.6533203125
Iteration 17300: Loss = -11900.654296875
1
Iteration 17400: Loss = -11900.654296875
2
Iteration 17500: Loss = -11900.65625
3
Iteration 17600: Loss = -11900.654296875
4
Iteration 17700: Loss = -11900.654296875
5
Iteration 17800: Loss = -11900.65625
6
Iteration 17900: Loss = -11900.6552734375
7
Iteration 18000: Loss = -11900.6552734375
8
Iteration 18100: Loss = -11900.654296875
9
Iteration 18200: Loss = -11900.654296875
10
Iteration 18300: Loss = -11900.6552734375
11
Iteration 18400: Loss = -11900.654296875
12
Iteration 18500: Loss = -11900.654296875
13
Iteration 18600: Loss = -11900.6552734375
14
Iteration 18700: Loss = -11900.6533203125
Iteration 18800: Loss = -11900.654296875
1
Iteration 18900: Loss = -11900.6552734375
2
Iteration 19000: Loss = -11900.6533203125
Iteration 19100: Loss = -11900.6552734375
1
Iteration 19200: Loss = -11900.654296875
2
Iteration 19300: Loss = -11900.6533203125
Iteration 19400: Loss = -11900.6533203125
Iteration 19500: Loss = -11900.6533203125
Iteration 19600: Loss = -11900.654296875
1
Iteration 19700: Loss = -11900.654296875
2
Iteration 19800: Loss = -11900.654296875
3
Iteration 19900: Loss = -11900.6552734375
4
Iteration 20000: Loss = -11900.654296875
5
Iteration 20100: Loss = -11900.6552734375
6
Iteration 20200: Loss = -11900.6494140625
Iteration 20300: Loss = -11900.6494140625
Iteration 20400: Loss = -11900.6494140625
Iteration 20500: Loss = -11900.650390625
1
Iteration 20600: Loss = -11900.6494140625
Iteration 20700: Loss = -11900.6494140625
Iteration 20800: Loss = -11900.6494140625
Iteration 20900: Loss = -11900.650390625
1
Iteration 21000: Loss = -11900.6455078125
Iteration 21100: Loss = -11900.646484375
1
Iteration 21200: Loss = -11900.646484375
2
Iteration 21300: Loss = -11900.6455078125
Iteration 21400: Loss = -11900.6455078125
Iteration 21500: Loss = -11900.6455078125
Iteration 21600: Loss = -11900.646484375
1
Iteration 21700: Loss = -11900.64453125
Iteration 21800: Loss = -11900.646484375
1
Iteration 21900: Loss = -11900.64453125
Iteration 22000: Loss = -11900.6455078125
1
Iteration 22100: Loss = -11900.6455078125
2
Iteration 22200: Loss = -11900.64453125
Iteration 22300: Loss = -11900.6455078125
1
Iteration 22400: Loss = -11900.64453125
Iteration 22500: Loss = -11900.6455078125
1
Iteration 22600: Loss = -11900.64453125
Iteration 22700: Loss = -11900.646484375
1
Iteration 22800: Loss = -11900.646484375
2
Iteration 22900: Loss = -11900.6455078125
3
Iteration 23000: Loss = -11900.6455078125
4
Iteration 23100: Loss = -11900.6455078125
5
Iteration 23200: Loss = -11900.6455078125
6
Iteration 23300: Loss = -11900.6455078125
7
Iteration 23400: Loss = -11900.6455078125
8
Iteration 23500: Loss = -11900.6455078125
9
Iteration 23600: Loss = -11900.6455078125
10
Iteration 23700: Loss = -11900.6416015625
Iteration 23800: Loss = -11900.638671875
Iteration 23900: Loss = -11900.6396484375
1
Iteration 24000: Loss = -11900.6396484375
2
Iteration 24100: Loss = -11900.640625
3
Iteration 24200: Loss = -11900.638671875
Iteration 24300: Loss = -11900.638671875
Iteration 24400: Loss = -11900.6396484375
1
Iteration 24500: Loss = -11900.6396484375
2
Iteration 24600: Loss = -11900.6396484375
3
Iteration 24700: Loss = -11900.6396484375
4
Iteration 24800: Loss = -11900.638671875
Iteration 24900: Loss = -11900.6396484375
1
Iteration 25000: Loss = -11900.640625
2
Iteration 25100: Loss = -11900.638671875
Iteration 25200: Loss = -11900.638671875
Iteration 25300: Loss = -11900.6396484375
1
Iteration 25400: Loss = -11900.6396484375
2
Iteration 25500: Loss = -11900.638671875
Iteration 25600: Loss = -11900.638671875
Iteration 25700: Loss = -11900.638671875
Iteration 25800: Loss = -11900.6396484375
1
Iteration 25900: Loss = -11900.6396484375
2
Iteration 26000: Loss = -11900.638671875
Iteration 26100: Loss = -11900.6396484375
1
Iteration 26200: Loss = -11900.6396484375
2
Iteration 26300: Loss = -11900.6396484375
3
Iteration 26400: Loss = -11900.6396484375
4
Iteration 26500: Loss = -11900.6396484375
5
Iteration 26600: Loss = -11900.6396484375
6
Iteration 26700: Loss = -11900.6396484375
7
Iteration 26800: Loss = -11900.6396484375
8
Iteration 26900: Loss = -11900.638671875
Iteration 27000: Loss = -11900.6376953125
Iteration 27100: Loss = -11900.638671875
1
Iteration 27200: Loss = -11900.6396484375
2
Iteration 27300: Loss = -11900.5927734375
Iteration 27400: Loss = -11900.59375
1
Iteration 27500: Loss = -11900.59375
2
Iteration 27600: Loss = -11900.5947265625
3
Iteration 27700: Loss = -11900.59375
4
Iteration 27800: Loss = -11900.5927734375
Iteration 27900: Loss = -11900.59375
1
Iteration 28000: Loss = -11900.5947265625
2
Iteration 28100: Loss = -11900.59375
3
Iteration 28200: Loss = -11900.59375
4
Iteration 28300: Loss = -11900.5947265625
5
Iteration 28400: Loss = -11900.59375
6
Iteration 28500: Loss = -11900.5947265625
7
Iteration 28600: Loss = -11900.59375
8
Iteration 28700: Loss = -11900.59375
9
Iteration 28800: Loss = -11900.5927734375
Iteration 28900: Loss = -11900.5927734375
Iteration 29000: Loss = -11900.59375
1
Iteration 29100: Loss = -11900.5927734375
Iteration 29200: Loss = -11900.5927734375
Iteration 29300: Loss = -11900.59375
1
Iteration 29400: Loss = -11900.59375
2
Iteration 29500: Loss = -11900.59375
3
Iteration 29600: Loss = -11900.5927734375
Iteration 29700: Loss = -11900.59375
1
Iteration 29800: Loss = -11900.5927734375
Iteration 29900: Loss = -11900.59375
1
pi: tensor([[0.2985, 0.7015],
        [0.6269, 0.3731]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5447, 0.4553], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3067, 0.1019],
         [0.5462, 0.2873]],

        [[0.8925, 0.0938],
         [0.9340, 0.7746]],

        [[0.2203, 0.1033],
         [0.0304, 0.6914]],

        [[0.1518, 0.1047],
         [0.2209, 0.9905]],

        [[0.8773, 0.1066],
         [0.9929, 0.0105]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.03807532553174031
Average Adjusted Rand Index: 0.9683227436307466
[0.007061766115298604, 0.03807532553174031] [0.2181783730258576, 0.9683227436307466] [12268.607421875, 11900.5927734375]
-------------------------------------
This iteration is 38
True Objective function: Loss = -12011.575698578867
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22266.212890625
Iteration 100: Loss = -15278.94140625
Iteration 200: Loss = -13128.2255859375
Iteration 300: Loss = -12727.900390625
Iteration 400: Loss = -12644.619140625
Iteration 500: Loss = -12608.2744140625
Iteration 600: Loss = -12576.0068359375
Iteration 700: Loss = -12562.8271484375
Iteration 800: Loss = -12551.5439453125
Iteration 900: Loss = -12534.3837890625
Iteration 1000: Loss = -12521.2724609375
Iteration 1100: Loss = -12506.3876953125
Iteration 1200: Loss = -12486.947265625
Iteration 1300: Loss = -12473.4599609375
Iteration 1400: Loss = -12461.3271484375
Iteration 1500: Loss = -12453.5068359375
Iteration 1600: Loss = -12449.8125
Iteration 1700: Loss = -12447.6328125
Iteration 1800: Loss = -12446.138671875
Iteration 1900: Loss = -12445.0322265625
Iteration 2000: Loss = -12444.171875
Iteration 2100: Loss = -12443.478515625
Iteration 2200: Loss = -12442.908203125
Iteration 2300: Loss = -12442.42578125
Iteration 2400: Loss = -12442.0166015625
Iteration 2500: Loss = -12441.6640625
Iteration 2600: Loss = -12441.35546875
Iteration 2700: Loss = -12441.0830078125
Iteration 2800: Loss = -12440.8427734375
Iteration 2900: Loss = -12440.6298828125
Iteration 3000: Loss = -12440.4384765625
Iteration 3100: Loss = -12440.267578125
Iteration 3200: Loss = -12440.111328125
Iteration 3300: Loss = -12439.9677734375
Iteration 3400: Loss = -12439.83984375
Iteration 3500: Loss = -12439.7236328125
Iteration 3600: Loss = -12439.6162109375
Iteration 3700: Loss = -12439.515625
Iteration 3800: Loss = -12439.427734375
Iteration 3900: Loss = -12439.3427734375
Iteration 4000: Loss = -12439.2646484375
Iteration 4100: Loss = -12439.1953125
Iteration 4200: Loss = -12439.1298828125
Iteration 4300: Loss = -12439.068359375
Iteration 4400: Loss = -12439.0107421875
Iteration 4500: Loss = -12438.9580078125
Iteration 4600: Loss = -12438.9091796875
Iteration 4700: Loss = -12438.8623046875
Iteration 4800: Loss = -12438.8193359375
Iteration 4900: Loss = -12438.779296875
Iteration 5000: Loss = -12438.7421875
Iteration 5100: Loss = -12438.7060546875
Iteration 5200: Loss = -12438.6748046875
Iteration 5300: Loss = -12438.64453125
Iteration 5400: Loss = -12438.615234375
Iteration 5500: Loss = -12438.5888671875
Iteration 5600: Loss = -12438.5625
Iteration 5700: Loss = -12438.541015625
Iteration 5800: Loss = -12438.51953125
Iteration 5900: Loss = -12438.4970703125
Iteration 6000: Loss = -12438.478515625
Iteration 6100: Loss = -12438.4599609375
Iteration 6200: Loss = -12438.443359375
Iteration 6300: Loss = -12438.427734375
Iteration 6400: Loss = -12438.4140625
Iteration 6500: Loss = -12438.3974609375
Iteration 6600: Loss = -12438.384765625
Iteration 6700: Loss = -12438.3720703125
Iteration 6800: Loss = -12438.361328125
Iteration 6900: Loss = -12438.349609375
Iteration 7000: Loss = -12438.3388671875
Iteration 7100: Loss = -12438.330078125
Iteration 7200: Loss = -12438.3212890625
Iteration 7300: Loss = -12438.3115234375
Iteration 7400: Loss = -12438.3046875
Iteration 7500: Loss = -12438.296875
Iteration 7600: Loss = -12438.2880859375
Iteration 7700: Loss = -12438.28125
Iteration 7800: Loss = -12438.2763671875
Iteration 7900: Loss = -12438.26953125
Iteration 8000: Loss = -12438.2646484375
Iteration 8100: Loss = -12438.2587890625
Iteration 8200: Loss = -12438.2529296875
Iteration 8300: Loss = -12438.248046875
Iteration 8400: Loss = -12438.244140625
Iteration 8500: Loss = -12438.2412109375
Iteration 8600: Loss = -12438.236328125
Iteration 8700: Loss = -12438.232421875
Iteration 8800: Loss = -12438.228515625
Iteration 8900: Loss = -12438.224609375
Iteration 9000: Loss = -12438.2216796875
Iteration 9100: Loss = -12438.2177734375
Iteration 9200: Loss = -12438.216796875
Iteration 9300: Loss = -12438.2138671875
Iteration 9400: Loss = -12438.2109375
Iteration 9500: Loss = -12438.208984375
Iteration 9600: Loss = -12438.2060546875
Iteration 9700: Loss = -12438.205078125
Iteration 9800: Loss = -12438.2021484375
Iteration 9900: Loss = -12438.19921875
Iteration 10000: Loss = -12438.1982421875
Iteration 10100: Loss = -12438.1953125
Iteration 10200: Loss = -12438.1953125
Iteration 10300: Loss = -12438.193359375
Iteration 10400: Loss = -12438.19140625
Iteration 10500: Loss = -12438.19140625
Iteration 10600: Loss = -12438.1904296875
Iteration 10700: Loss = -12438.1884765625
Iteration 10800: Loss = -12438.1875
Iteration 10900: Loss = -12438.1875
Iteration 11000: Loss = -12438.185546875
Iteration 11100: Loss = -12438.1845703125
Iteration 11200: Loss = -12438.1826171875
Iteration 11300: Loss = -12438.1826171875
Iteration 11400: Loss = -12438.1796875
Iteration 11500: Loss = -12438.181640625
1
Iteration 11600: Loss = -12438.1796875
Iteration 11700: Loss = -12438.1806640625
1
Iteration 11800: Loss = -12438.1787109375
Iteration 11900: Loss = -12438.177734375
Iteration 12000: Loss = -12438.1767578125
Iteration 12100: Loss = -12438.1767578125
Iteration 12200: Loss = -12438.17578125
Iteration 12300: Loss = -12438.1748046875
Iteration 12400: Loss = -12438.1748046875
Iteration 12500: Loss = -12438.1748046875
Iteration 12600: Loss = -12438.173828125
Iteration 12700: Loss = -12438.171875
Iteration 12800: Loss = -12438.1708984375
Iteration 12900: Loss = -12438.169921875
Iteration 13000: Loss = -12438.166015625
Iteration 13100: Loss = -12438.158203125
Iteration 13200: Loss = -12438.1416015625
Iteration 13300: Loss = -12438.1279296875
Iteration 13400: Loss = -12438.111328125
Iteration 13500: Loss = -12438.0927734375
Iteration 13600: Loss = -12438.0703125
Iteration 13700: Loss = -12438.0615234375
Iteration 13800: Loss = -12438.05859375
Iteration 13900: Loss = -12438.046875
Iteration 14000: Loss = -12438.03125
Iteration 14100: Loss = -12438.013671875
Iteration 14200: Loss = -12437.96875
Iteration 14300: Loss = -12437.931640625
Iteration 14400: Loss = -12437.9189453125
Iteration 14500: Loss = -12437.90625
Iteration 14600: Loss = -12437.9033203125
Iteration 14700: Loss = -12437.8994140625
Iteration 14800: Loss = -12437.88671875
Iteration 14900: Loss = -12437.8798828125
Iteration 15000: Loss = -12437.8701171875
Iteration 15100: Loss = -12437.859375
Iteration 15200: Loss = -12437.8447265625
Iteration 15300: Loss = -12437.8447265625
Iteration 15400: Loss = -12437.8310546875
Iteration 15500: Loss = -12437.830078125
Iteration 15600: Loss = -12437.830078125
Iteration 15700: Loss = -12437.8291015625
Iteration 15800: Loss = -12437.830078125
1
Iteration 15900: Loss = -12437.830078125
2
Iteration 16000: Loss = -12437.830078125
3
Iteration 16100: Loss = -12437.828125
Iteration 16200: Loss = -12437.8310546875
1
Iteration 16300: Loss = -12437.830078125
2
Iteration 16400: Loss = -12437.830078125
3
Iteration 16500: Loss = -12437.830078125
4
Iteration 16600: Loss = -12437.8291015625
5
Iteration 16700: Loss = -12437.8291015625
6
Iteration 16800: Loss = -12437.830078125
7
Iteration 16900: Loss = -12437.8291015625
8
Iteration 17000: Loss = -12437.828125
Iteration 17100: Loss = -12437.830078125
1
Iteration 17200: Loss = -12437.830078125
2
Iteration 17300: Loss = -12437.828125
Iteration 17400: Loss = -12437.8291015625
1
Iteration 17500: Loss = -12437.830078125
2
Iteration 17600: Loss = -12437.8291015625
3
Iteration 17700: Loss = -12437.8291015625
4
Iteration 17800: Loss = -12437.830078125
5
Iteration 17900: Loss = -12437.83203125
6
Iteration 18000: Loss = -12437.828125
Iteration 18100: Loss = -12437.8291015625
1
Iteration 18200: Loss = -12437.830078125
2
Iteration 18300: Loss = -12437.8291015625
3
Iteration 18400: Loss = -12437.830078125
4
Iteration 18500: Loss = -12437.828125
Iteration 18600: Loss = -12437.828125
Iteration 18700: Loss = -12437.828125
Iteration 18800: Loss = -12437.8291015625
1
Iteration 18900: Loss = -12437.830078125
2
Iteration 19000: Loss = -12437.828125
Iteration 19100: Loss = -12437.8291015625
1
Iteration 19200: Loss = -12437.830078125
2
Iteration 19300: Loss = -12437.830078125
3
Iteration 19400: Loss = -12437.8291015625
4
Iteration 19500: Loss = -12437.830078125
5
Iteration 19600: Loss = -12437.828125
Iteration 19700: Loss = -12437.828125
Iteration 19800: Loss = -12437.828125
Iteration 19900: Loss = -12437.828125
Iteration 20000: Loss = -12437.8291015625
1
Iteration 20100: Loss = -12437.8291015625
2
Iteration 20200: Loss = -12437.8291015625
3
Iteration 20300: Loss = -12437.828125
Iteration 20400: Loss = -12437.828125
Iteration 20500: Loss = -12437.8291015625
1
Iteration 20600: Loss = -12437.828125
Iteration 20700: Loss = -12437.8291015625
1
Iteration 20800: Loss = -12437.8291015625
2
Iteration 20900: Loss = -12437.828125
Iteration 21000: Loss = -12437.8291015625
1
Iteration 21100: Loss = -12437.8134765625
Iteration 21200: Loss = -12437.8125
Iteration 21300: Loss = -12437.8125
Iteration 21400: Loss = -12437.8115234375
Iteration 21500: Loss = -12437.79296875
Iteration 21600: Loss = -12437.7919921875
Iteration 21700: Loss = -12437.79296875
1
Iteration 21800: Loss = -12437.7939453125
2
Iteration 21900: Loss = -12437.79296875
3
Iteration 22000: Loss = -12437.7939453125
4
Iteration 22100: Loss = -12437.7939453125
5
Iteration 22200: Loss = -12437.79296875
6
Iteration 22300: Loss = -12437.7919921875
Iteration 22400: Loss = -12437.7939453125
1
Iteration 22500: Loss = -12437.79296875
2
Iteration 22600: Loss = -12437.79296875
3
Iteration 22700: Loss = -12437.79296875
4
Iteration 22800: Loss = -12437.79296875
5
Iteration 22900: Loss = -12437.7919921875
Iteration 23000: Loss = -12437.79296875
1
Iteration 23100: Loss = -12437.7939453125
2
Iteration 23200: Loss = -12437.779296875
Iteration 23300: Loss = -12437.779296875
Iteration 23400: Loss = -12437.7802734375
1
Iteration 23500: Loss = -12437.7802734375
2
Iteration 23600: Loss = -12437.78125
3
Iteration 23700: Loss = -12437.7802734375
4
Iteration 23800: Loss = -12437.78125
5
Iteration 23900: Loss = -12437.78125
6
Iteration 24000: Loss = -12437.7802734375
7
Iteration 24100: Loss = -12437.78125
8
Iteration 24200: Loss = -12437.78125
9
Iteration 24300: Loss = -12437.7626953125
Iteration 24400: Loss = -12437.765625
1
Iteration 24500: Loss = -12437.763671875
2
Iteration 24600: Loss = -12437.7626953125
Iteration 24700: Loss = -12437.7626953125
Iteration 24800: Loss = -12437.763671875
1
Iteration 24900: Loss = -12437.763671875
2
Iteration 25000: Loss = -12437.7626953125
Iteration 25100: Loss = -12437.763671875
1
Iteration 25200: Loss = -12437.7646484375
2
Iteration 25300: Loss = -12437.7626953125
Iteration 25400: Loss = -12437.7626953125
Iteration 25500: Loss = -12437.7626953125
Iteration 25600: Loss = -12437.7646484375
1
Iteration 25700: Loss = -12437.7626953125
Iteration 25800: Loss = -12437.76171875
Iteration 25900: Loss = -12437.7626953125
1
Iteration 26000: Loss = -12437.763671875
2
Iteration 26100: Loss = -12437.7626953125
3
Iteration 26200: Loss = -12437.7646484375
4
Iteration 26300: Loss = -12437.765625
5
Iteration 26400: Loss = -12437.7626953125
6
Iteration 26500: Loss = -12437.763671875
7
Iteration 26600: Loss = -12437.7626953125
8
Iteration 26700: Loss = -12437.7646484375
9
Iteration 26800: Loss = -12437.763671875
10
Iteration 26900: Loss = -12437.7626953125
11
Iteration 27000: Loss = -12437.7646484375
12
Iteration 27100: Loss = -12437.7666015625
13
Iteration 27200: Loss = -12437.763671875
14
Iteration 27300: Loss = -12437.7626953125
15
Stopping early at iteration 27300 due to no improvement.
pi: tensor([[9.8074e-01, 1.9262e-02],
        [1.0000e+00, 2.5318e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4523, 0.5477], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2089, 0.0990],
         [0.1138, 0.3120]],

        [[0.0123, 0.2144],
         [0.9890, 0.8580]],

        [[0.0632, 0.2278],
         [0.0071, 0.2928]],

        [[0.0233, 0.1434],
         [0.0085, 0.0294]],

        [[0.9355, 0.2389],
         [0.2081, 0.8386]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.06595893289709327
Average Adjusted Rand Index: 0.192
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -34183.296875
Iteration 100: Loss = -21235.189453125
Iteration 200: Loss = -14587.3779296875
Iteration 300: Loss = -12979.0595703125
Iteration 400: Loss = -12729.6259765625
Iteration 500: Loss = -12624.080078125
Iteration 600: Loss = -12578.2216796875
Iteration 700: Loss = -12559.962890625
Iteration 800: Loss = -12550.681640625
Iteration 900: Loss = -12544.6318359375
Iteration 1000: Loss = -12540.4228515625
Iteration 1100: Loss = -12537.318359375
Iteration 1200: Loss = -12534.9345703125
Iteration 1300: Loss = -12533.0517578125
Iteration 1400: Loss = -12531.521484375
Iteration 1500: Loss = -12530.2529296875
Iteration 1600: Loss = -12529.1806640625
Iteration 1700: Loss = -12528.263671875
Iteration 1800: Loss = -12527.470703125
Iteration 1900: Loss = -12526.77734375
Iteration 2000: Loss = -12526.1650390625
Iteration 2100: Loss = -12525.6240234375
Iteration 2200: Loss = -12525.1416015625
Iteration 2300: Loss = -12524.7109375
Iteration 2400: Loss = -12524.3203125
Iteration 2500: Loss = -12523.9677734375
Iteration 2600: Loss = -12523.646484375
Iteration 2700: Loss = -12523.3564453125
Iteration 2800: Loss = -12523.0947265625
Iteration 2900: Loss = -12522.853515625
Iteration 3000: Loss = -12522.634765625
Iteration 3100: Loss = -12522.4326171875
Iteration 3200: Loss = -12522.244140625
Iteration 3300: Loss = -12522.0751953125
Iteration 3400: Loss = -12521.9169921875
Iteration 3500: Loss = -12521.76953125
Iteration 3600: Loss = -12521.634765625
Iteration 3700: Loss = -12521.5078125
Iteration 3800: Loss = -12521.3916015625
Iteration 3900: Loss = -12521.2802734375
Iteration 4000: Loss = -12521.177734375
Iteration 4100: Loss = -12521.0830078125
Iteration 4200: Loss = -12520.990234375
Iteration 4300: Loss = -12520.904296875
Iteration 4400: Loss = -12520.828125
Iteration 4500: Loss = -12520.7548828125
Iteration 4600: Loss = -12520.6875
Iteration 4700: Loss = -12520.623046875
Iteration 4800: Loss = -12520.5625
Iteration 4900: Loss = -12520.5068359375
Iteration 5000: Loss = -12520.453125
Iteration 5100: Loss = -12520.4033203125
Iteration 5200: Loss = -12520.357421875
Iteration 5300: Loss = -12520.3125
Iteration 5400: Loss = -12520.2734375
Iteration 5500: Loss = -12520.2333984375
Iteration 5600: Loss = -12520.1982421875
Iteration 5700: Loss = -12520.1630859375
Iteration 5800: Loss = -12520.1298828125
Iteration 5900: Loss = -12520.1015625
Iteration 6000: Loss = -12520.0703125
Iteration 6100: Loss = -12520.04296875
Iteration 6200: Loss = -12520.0185546875
Iteration 6300: Loss = -12519.994140625
Iteration 6400: Loss = -12519.9697265625
Iteration 6500: Loss = -12519.94921875
Iteration 6600: Loss = -12519.927734375
Iteration 6700: Loss = -12519.908203125
Iteration 6800: Loss = -12519.890625
Iteration 6900: Loss = -12519.8720703125
Iteration 7000: Loss = -12519.85546875
Iteration 7100: Loss = -12519.8388671875
Iteration 7200: Loss = -12519.826171875
Iteration 7300: Loss = -12519.810546875
Iteration 7400: Loss = -12519.796875
Iteration 7500: Loss = -12519.783203125
Iteration 7600: Loss = -12519.771484375
Iteration 7700: Loss = -12519.7607421875
Iteration 7800: Loss = -12519.7470703125
Iteration 7900: Loss = -12519.73828125
Iteration 8000: Loss = -12519.7275390625
Iteration 8100: Loss = -12519.71875
Iteration 8200: Loss = -12519.7109375
Iteration 8300: Loss = -12519.69921875
Iteration 8400: Loss = -12519.6904296875
Iteration 8500: Loss = -12519.681640625
Iteration 8600: Loss = -12519.6748046875
Iteration 8700: Loss = -12519.666015625
Iteration 8800: Loss = -12519.658203125
Iteration 8900: Loss = -12519.6494140625
Iteration 9000: Loss = -12519.6416015625
Iteration 9100: Loss = -12519.6328125
Iteration 9200: Loss = -12519.623046875
Iteration 9300: Loss = -12519.61328125
Iteration 9400: Loss = -12519.6025390625
Iteration 9500: Loss = -12519.587890625
Iteration 9600: Loss = -12519.5712890625
Iteration 9700: Loss = -12519.5498046875
Iteration 9800: Loss = -12519.521484375
Iteration 9900: Loss = -12519.4833984375
Iteration 10000: Loss = -12519.4482421875
Iteration 10100: Loss = -12519.41796875
Iteration 10200: Loss = -12519.396484375
Iteration 10300: Loss = -12519.375
Iteration 10400: Loss = -12519.3583984375
Iteration 10500: Loss = -12519.34375
Iteration 10600: Loss = -12519.3291015625
Iteration 10700: Loss = -12519.3154296875
Iteration 10800: Loss = -12519.3046875
Iteration 10900: Loss = -12519.2919921875
Iteration 11000: Loss = -12519.28125
Iteration 11100: Loss = -12519.26953125
Iteration 11200: Loss = -12519.2568359375
Iteration 11300: Loss = -12519.2451171875
Iteration 11400: Loss = -12519.2314453125
Iteration 11500: Loss = -12519.2197265625
Iteration 11600: Loss = -12519.20703125
Iteration 11700: Loss = -12519.193359375
Iteration 11800: Loss = -12519.1796875
Iteration 11900: Loss = -12519.16015625
Iteration 12000: Loss = -12519.1435546875
Iteration 12100: Loss = -12519.123046875
Iteration 12200: Loss = -12519.0986328125
Iteration 12300: Loss = -12519.0712890625
Iteration 12400: Loss = -12519.0390625
Iteration 12500: Loss = -12518.9990234375
Iteration 12600: Loss = -12518.9501953125
Iteration 12700: Loss = -12518.8876953125
Iteration 12800: Loss = -12518.8046875
Iteration 12900: Loss = -12518.6875
Iteration 13000: Loss = -12518.5185546875
Iteration 13100: Loss = -12518.2568359375
Iteration 13200: Loss = -12517.8623046875
Iteration 13300: Loss = -12517.3857421875
Iteration 13400: Loss = -12517.1015625
Iteration 13500: Loss = -12516.9189453125
Iteration 13600: Loss = -12516.03515625
Iteration 13700: Loss = -12515.7216796875
Iteration 13800: Loss = -12515.6181640625
Iteration 13900: Loss = -12515.560546875
Iteration 14000: Loss = -12515.50390625
Iteration 14100: Loss = -12515.4326171875
Iteration 14200: Loss = -12515.361328125
Iteration 14300: Loss = -12515.275390625
Iteration 14400: Loss = -12488.9453125
Iteration 14500: Loss = -12395.5693359375
Iteration 14600: Loss = -12244.888671875
Iteration 14700: Loss = -12169.0400390625
Iteration 14800: Loss = -12105.1279296875
Iteration 14900: Loss = -12095.466796875
Iteration 15000: Loss = -12066.5869140625
Iteration 15100: Loss = -12050.6513671875
Iteration 15200: Loss = -12043.2119140625
Iteration 15300: Loss = -12042.796875
Iteration 15400: Loss = -12034.666015625
Iteration 15500: Loss = -12027.3740234375
Iteration 15600: Loss = -12012.697265625
Iteration 15700: Loss = -12012.2421875
Iteration 15800: Loss = -12012.1005859375
Iteration 15900: Loss = -12012.0126953125
Iteration 16000: Loss = -12011.947265625
Iteration 16100: Loss = -12011.896484375
Iteration 16200: Loss = -12011.8564453125
Iteration 16300: Loss = -12011.82421875
Iteration 16400: Loss = -12011.7978515625
Iteration 16500: Loss = -12011.7724609375
Iteration 16600: Loss = -12011.75
Iteration 16700: Loss = -12011.7333984375
Iteration 16800: Loss = -12011.71875
Iteration 16900: Loss = -12011.705078125
Iteration 17000: Loss = -12011.6904296875
Iteration 17100: Loss = -12011.6806640625
Iteration 17200: Loss = -12011.6689453125
Iteration 17300: Loss = -12011.66015625
Iteration 17400: Loss = -12011.650390625
Iteration 17500: Loss = -12011.642578125
Iteration 17600: Loss = -12011.6279296875
Iteration 17700: Loss = -12011.6201171875
Iteration 17800: Loss = -12011.6171875
Iteration 17900: Loss = -12011.611328125
Iteration 18000: Loss = -12011.603515625
Iteration 18100: Loss = -12011.4775390625
Iteration 18200: Loss = -12011.47265625
Iteration 18300: Loss = -12011.46875
Iteration 18400: Loss = -12011.46484375
Iteration 18500: Loss = -12011.4619140625
Iteration 18600: Loss = -12011.458984375
Iteration 18700: Loss = -12011.45703125
Iteration 18800: Loss = -12011.4541015625
Iteration 18900: Loss = -12011.451171875
Iteration 19000: Loss = -12011.4501953125
Iteration 19100: Loss = -12011.4462890625
Iteration 19200: Loss = -12011.4443359375
Iteration 19300: Loss = -12011.4423828125
Iteration 19400: Loss = -12011.439453125
Iteration 19500: Loss = -12011.4384765625
Iteration 19600: Loss = -12011.4365234375
Iteration 19700: Loss = -12011.435546875
Iteration 19800: Loss = -12011.4345703125
Iteration 19900: Loss = -12011.43359375
Iteration 20000: Loss = -12011.431640625
Iteration 20100: Loss = -12011.431640625
Iteration 20200: Loss = -12011.4287109375
Iteration 20300: Loss = -12011.4267578125
Iteration 20400: Loss = -12011.42578125
Iteration 20500: Loss = -12011.4248046875
Iteration 20600: Loss = -12011.423828125
Iteration 20700: Loss = -12011.4248046875
1
Iteration 20800: Loss = -12011.423828125
Iteration 20900: Loss = -12011.4228515625
Iteration 21000: Loss = -12011.421875
Iteration 21100: Loss = -12011.421875
Iteration 21200: Loss = -12011.421875
Iteration 21300: Loss = -12011.4208984375
Iteration 21400: Loss = -12011.41796875
Iteration 21500: Loss = -12011.4189453125
1
Iteration 21600: Loss = -12011.4189453125
2
Iteration 21700: Loss = -12011.41796875
Iteration 21800: Loss = -12011.4169921875
Iteration 21900: Loss = -12011.416015625
Iteration 22000: Loss = -12011.41796875
1
Iteration 22100: Loss = -12011.4130859375
Iteration 22200: Loss = -12011.412109375
Iteration 22300: Loss = -12011.4130859375
1
Iteration 22400: Loss = -12011.412109375
Iteration 22500: Loss = -12011.412109375
Iteration 22600: Loss = -12011.412109375
Iteration 22700: Loss = -12011.4111328125
Iteration 22800: Loss = -12011.41015625
Iteration 22900: Loss = -12011.4111328125
1
Iteration 23000: Loss = -12011.4111328125
2
Iteration 23100: Loss = -12011.41015625
Iteration 23200: Loss = -12011.4111328125
1
Iteration 23300: Loss = -12011.4111328125
2
Iteration 23400: Loss = -12011.41015625
Iteration 23500: Loss = -12011.4091796875
Iteration 23600: Loss = -12011.4091796875
Iteration 23700: Loss = -12011.4091796875
Iteration 23800: Loss = -12011.41015625
1
Iteration 23900: Loss = -12011.4091796875
Iteration 24000: Loss = -12011.408203125
Iteration 24100: Loss = -12011.408203125
Iteration 24200: Loss = -12011.4091796875
1
Iteration 24300: Loss = -12011.408203125
Iteration 24400: Loss = -12011.4072265625
Iteration 24500: Loss = -12011.4091796875
1
Iteration 24600: Loss = -12011.4072265625
Iteration 24700: Loss = -12011.408203125
1
Iteration 24800: Loss = -12011.408203125
2
Iteration 24900: Loss = -12011.4111328125
3
Iteration 25000: Loss = -12011.4091796875
4
Iteration 25100: Loss = -12011.408203125
5
Iteration 25200: Loss = -12011.4091796875
6
Iteration 25300: Loss = -12011.408203125
7
Iteration 25400: Loss = -12011.4091796875
8
Iteration 25500: Loss = -12011.40625
Iteration 25600: Loss = -12011.4072265625
1
Iteration 25700: Loss = -12011.408203125
2
Iteration 25800: Loss = -12011.408203125
3
Iteration 25900: Loss = -12011.408203125
4
Iteration 26000: Loss = -12011.408203125
5
Iteration 26100: Loss = -12011.408203125
6
Iteration 26200: Loss = -12011.4091796875
7
Iteration 26300: Loss = -12011.408203125
8
Iteration 26400: Loss = -12011.408203125
9
Iteration 26500: Loss = -12011.4072265625
10
Iteration 26600: Loss = -12011.4072265625
11
Iteration 26700: Loss = -12011.4072265625
12
Iteration 26800: Loss = -12011.4072265625
13
Iteration 26900: Loss = -12011.4072265625
14
Iteration 27000: Loss = -12011.4072265625
15
Stopping early at iteration 27000 due to no improvement.
pi: tensor([[0.3129, 0.6871],
        [0.6214, 0.3786]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4470, 0.5530], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2999, 0.0981],
         [0.0539, 0.3108]],

        [[0.9182, 0.1029],
         [0.0621, 0.9741]],

        [[0.5291, 0.1087],
         [0.0101, 0.9926]],

        [[0.1115, 0.0914],
         [0.7650, 0.0859]],

        [[0.9866, 0.1081],
         [0.0402, 0.9142]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 3
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.041347500345636876
Average Adjusted Rand Index: 0.9681606232309878
[0.06595893289709327, 0.041347500345636876] [0.192, 0.9681606232309878] [12437.7626953125, 12011.4072265625]
-------------------------------------
This iteration is 39
True Objective function: Loss = -11883.224546555504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -56397.18359375
Iteration 100: Loss = -36080.01953125
Iteration 200: Loss = -19379.099609375
Iteration 300: Loss = -13966.375
Iteration 400: Loss = -13009.302734375
Iteration 500: Loss = -12728.6318359375
Iteration 600: Loss = -12617.9736328125
Iteration 700: Loss = -12551.28515625
Iteration 800: Loss = -12500.5029296875
Iteration 900: Loss = -12477.1806640625
Iteration 1000: Loss = -12460.1015625
Iteration 1100: Loss = -12450.755859375
Iteration 1200: Loss = -12435.6044921875
Iteration 1300: Loss = -12430.5517578125
Iteration 1400: Loss = -12426.630859375
Iteration 1500: Loss = -12423.4658203125
Iteration 1600: Loss = -12420.8544921875
Iteration 1700: Loss = -12418.662109375
Iteration 1800: Loss = -12416.80078125
Iteration 1900: Loss = -12415.1884765625
Iteration 2000: Loss = -12409.845703125
Iteration 2100: Loss = -12407.6552734375
Iteration 2200: Loss = -12406.123046875
Iteration 2300: Loss = -12404.8310546875
Iteration 2400: Loss = -12403.7177734375
Iteration 2500: Loss = -12402.7421875
Iteration 2600: Loss = -12401.8837890625
Iteration 2700: Loss = -12401.12109375
Iteration 2800: Loss = -12400.4462890625
Iteration 2900: Loss = -12399.84375
Iteration 3000: Loss = -12399.2998046875
Iteration 3100: Loss = -12393.8779296875
Iteration 3200: Loss = -12393.2958984375
Iteration 3300: Loss = -12392.8427734375
Iteration 3400: Loss = -12392.453125
Iteration 3500: Loss = -12392.11328125
Iteration 3600: Loss = -12391.8056640625
Iteration 3700: Loss = -12391.529296875
Iteration 3800: Loss = -12391.279296875
Iteration 3900: Loss = -12391.05078125
Iteration 4000: Loss = -12390.8408203125
Iteration 4100: Loss = -12390.6484375
Iteration 4200: Loss = -12390.46875
Iteration 4300: Loss = -12390.3037109375
Iteration 4400: Loss = -12390.150390625
Iteration 4500: Loss = -12390.009765625
Iteration 4600: Loss = -12389.87890625
Iteration 4700: Loss = -12389.755859375
Iteration 4800: Loss = -12389.6416015625
Iteration 4900: Loss = -12389.5322265625
Iteration 5000: Loss = -12389.4345703125
Iteration 5100: Loss = -12389.3388671875
Iteration 5200: Loss = -12389.2509765625
Iteration 5300: Loss = -12389.1689453125
Iteration 5400: Loss = -12389.08984375
Iteration 5500: Loss = -12389.0185546875
Iteration 5600: Loss = -12388.9482421875
Iteration 5700: Loss = -12388.884765625
Iteration 5800: Loss = -12388.822265625
Iteration 5900: Loss = -12388.7666015625
Iteration 6000: Loss = -12388.7119140625
Iteration 6100: Loss = -12388.662109375
Iteration 6200: Loss = -12388.6123046875
Iteration 6300: Loss = -12388.5673828125
Iteration 6400: Loss = -12388.525390625
Iteration 6500: Loss = -12388.484375
Iteration 6600: Loss = -12388.4462890625
Iteration 6700: Loss = -12388.4111328125
Iteration 6800: Loss = -12388.375
Iteration 6900: Loss = -12388.3427734375
Iteration 7000: Loss = -12388.3125
Iteration 7100: Loss = -12388.2822265625
Iteration 7200: Loss = -12388.2568359375
Iteration 7300: Loss = -12388.2294921875
Iteration 7400: Loss = -12388.2041015625
Iteration 7500: Loss = -12388.181640625
Iteration 7600: Loss = -12388.1591796875
Iteration 7700: Loss = -12388.1376953125
Iteration 7800: Loss = -12388.1181640625
Iteration 7900: Loss = -12388.1005859375
Iteration 8000: Loss = -12388.0810546875
Iteration 8100: Loss = -12388.0654296875
Iteration 8200: Loss = -12388.048828125
Iteration 8300: Loss = -12388.0341796875
Iteration 8400: Loss = -12388.0166015625
Iteration 8500: Loss = -12388.00390625
Iteration 8600: Loss = -12387.9912109375
Iteration 8700: Loss = -12387.978515625
Iteration 8800: Loss = -12387.966796875
Iteration 8900: Loss = -12387.955078125
Iteration 9000: Loss = -12387.9443359375
Iteration 9100: Loss = -12387.9345703125
Iteration 9200: Loss = -12387.9228515625
Iteration 9300: Loss = -12387.9140625
Iteration 9400: Loss = -12387.90625
Iteration 9500: Loss = -12387.8984375
Iteration 9600: Loss = -12387.888671875
Iteration 9700: Loss = -12387.880859375
Iteration 9800: Loss = -12387.875
Iteration 9900: Loss = -12387.8671875
Iteration 10000: Loss = -12387.859375
Iteration 10100: Loss = -12387.8564453125
Iteration 10200: Loss = -12387.849609375
Iteration 10300: Loss = -12387.8427734375
Iteration 10400: Loss = -12387.837890625
Iteration 10500: Loss = -12387.8330078125
Iteration 10600: Loss = -12387.828125
Iteration 10700: Loss = -12387.82421875
Iteration 10800: Loss = -12387.818359375
Iteration 10900: Loss = -12387.814453125
Iteration 11000: Loss = -12387.810546875
Iteration 11100: Loss = -12387.8076171875
Iteration 11200: Loss = -12387.802734375
Iteration 11300: Loss = -12387.7998046875
Iteration 11400: Loss = -12387.794921875
Iteration 11500: Loss = -12387.7939453125
Iteration 11600: Loss = -12387.7890625
Iteration 11700: Loss = -12387.7880859375
Iteration 11800: Loss = -12387.783203125
Iteration 11900: Loss = -12387.78125
Iteration 12000: Loss = -12387.7783203125
Iteration 12100: Loss = -12387.77734375
Iteration 12200: Loss = -12387.775390625
Iteration 12300: Loss = -12387.7734375
Iteration 12400: Loss = -12387.771484375
Iteration 12500: Loss = -12387.7685546875
Iteration 12600: Loss = -12387.7685546875
Iteration 12700: Loss = -12387.7646484375
Iteration 12800: Loss = -12387.763671875
Iteration 12900: Loss = -12387.76171875
Iteration 13000: Loss = -12387.7607421875
Iteration 13100: Loss = -12387.7587890625
Iteration 13200: Loss = -12387.7578125
Iteration 13300: Loss = -12387.7568359375
Iteration 13400: Loss = -12387.75390625
Iteration 13500: Loss = -12387.7548828125
1
Iteration 13600: Loss = -12387.751953125
Iteration 13700: Loss = -12387.751953125
Iteration 13800: Loss = -12387.75
Iteration 13900: Loss = -12387.748046875
Iteration 14000: Loss = -12387.748046875
Iteration 14100: Loss = -12387.748046875
Iteration 14200: Loss = -12387.744140625
Iteration 14300: Loss = -12387.7451171875
1
Iteration 14400: Loss = -12387.7421875
Iteration 14500: Loss = -12387.7431640625
1
Iteration 14600: Loss = -12387.7431640625
2
Iteration 14700: Loss = -12387.7412109375
Iteration 14800: Loss = -12387.7412109375
Iteration 14900: Loss = -12387.740234375
Iteration 15000: Loss = -12387.7412109375
1
Iteration 15100: Loss = -12387.73828125
Iteration 15200: Loss = -12387.7392578125
1
Iteration 15300: Loss = -12387.7392578125
2
Iteration 15400: Loss = -12387.73828125
Iteration 15500: Loss = -12387.73828125
Iteration 15600: Loss = -12387.7373046875
Iteration 15700: Loss = -12387.736328125
Iteration 15800: Loss = -12387.736328125
Iteration 15900: Loss = -12387.736328125
Iteration 16000: Loss = -12387.7353515625
Iteration 16100: Loss = -12387.7373046875
1
Iteration 16200: Loss = -12387.7373046875
2
Iteration 16300: Loss = -12387.736328125
3
Iteration 16400: Loss = -12387.7353515625
Iteration 16500: Loss = -12387.734375
Iteration 16600: Loss = -12387.7333984375
Iteration 16700: Loss = -12387.7353515625
1
Iteration 16800: Loss = -12387.732421875
Iteration 16900: Loss = -12387.732421875
Iteration 17000: Loss = -12387.732421875
Iteration 17100: Loss = -12387.7333984375
1
Iteration 17200: Loss = -12387.7333984375
2
Iteration 17300: Loss = -12387.7314453125
Iteration 17400: Loss = -12387.7314453125
Iteration 17500: Loss = -12387.7314453125
Iteration 17600: Loss = -12387.732421875
1
Iteration 17700: Loss = -12387.7314453125
Iteration 17800: Loss = -12387.7314453125
Iteration 17900: Loss = -12387.73046875
Iteration 18000: Loss = -12387.732421875
1
Iteration 18100: Loss = -12387.7314453125
2
Iteration 18200: Loss = -12387.7333984375
3
Iteration 18300: Loss = -12387.7314453125
4
Iteration 18400: Loss = -12387.7314453125
5
Iteration 18500: Loss = -12387.73046875
Iteration 18600: Loss = -12387.73046875
Iteration 18700: Loss = -12387.7294921875
Iteration 18800: Loss = -12387.7294921875
Iteration 18900: Loss = -12387.7314453125
1
Iteration 19000: Loss = -12387.73046875
2
Iteration 19100: Loss = -12387.73046875
3
Iteration 19200: Loss = -12387.7294921875
Iteration 19300: Loss = -12387.7275390625
Iteration 19400: Loss = -12387.728515625
1
Iteration 19500: Loss = -12387.7314453125
2
Iteration 19600: Loss = -12387.7275390625
Iteration 19700: Loss = -12387.7275390625
Iteration 19800: Loss = -12387.728515625
1
Iteration 19900: Loss = -12387.7294921875
2
Iteration 20000: Loss = -12387.728515625
3
Iteration 20100: Loss = -12387.7294921875
4
Iteration 20200: Loss = -12387.728515625
5
Iteration 20300: Loss = -12387.7294921875
6
Iteration 20400: Loss = -12387.7275390625
Iteration 20500: Loss = -12387.728515625
1
Iteration 20600: Loss = -12387.7275390625
Iteration 20700: Loss = -12387.7294921875
1
Iteration 20800: Loss = -12387.728515625
2
Iteration 20900: Loss = -12387.728515625
3
Iteration 21000: Loss = -12387.7255859375
Iteration 21100: Loss = -12387.7255859375
Iteration 21200: Loss = -12387.7236328125
Iteration 21300: Loss = -12387.7236328125
Iteration 21400: Loss = -12387.7119140625
Iteration 21500: Loss = -12387.50390625
Iteration 21600: Loss = -12387.501953125
Iteration 21700: Loss = -12387.1142578125
Iteration 21800: Loss = -12384.7314453125
Iteration 21900: Loss = -12383.1630859375
Iteration 22000: Loss = -12383.162109375
Iteration 22100: Loss = -12383.1640625
1
Iteration 22200: Loss = -12383.162109375
Iteration 22300: Loss = -12383.162109375
Iteration 22400: Loss = -12383.162109375
Iteration 22500: Loss = -12383.1611328125
Iteration 22600: Loss = -12383.1630859375
1
Iteration 22700: Loss = -12383.162109375
2
Iteration 22800: Loss = -12383.162109375
3
Iteration 22900: Loss = -12383.162109375
4
Iteration 23000: Loss = -12383.1640625
5
Iteration 23100: Loss = -12383.162109375
6
Iteration 23200: Loss = -12383.162109375
7
Iteration 23300: Loss = -12383.1640625
8
Iteration 23400: Loss = -12383.162109375
9
Iteration 23500: Loss = -12383.162109375
10
Iteration 23600: Loss = -12383.162109375
11
Iteration 23700: Loss = -12383.1611328125
Iteration 23800: Loss = -12383.162109375
1
Iteration 23900: Loss = -12383.162109375
2
Iteration 24000: Loss = -12383.162109375
3
Iteration 24100: Loss = -12383.162109375
4
Iteration 24200: Loss = -12383.1630859375
5
Iteration 24300: Loss = -12383.1630859375
6
Iteration 24400: Loss = -12383.162109375
7
Iteration 24500: Loss = -12383.1630859375
8
Iteration 24600: Loss = -12383.162109375
9
Iteration 24700: Loss = -12383.1640625
10
Iteration 24800: Loss = -12383.1630859375
11
Iteration 24900: Loss = -12383.162109375
12
Iteration 25000: Loss = -12383.1611328125
Iteration 25100: Loss = -12383.162109375
1
Iteration 25200: Loss = -12383.162109375
2
Iteration 25300: Loss = -12383.162109375
3
Iteration 25400: Loss = -12383.1630859375
4
Iteration 25500: Loss = -12383.1630859375
5
Iteration 25600: Loss = -12383.1630859375
6
Iteration 25700: Loss = -12383.1630859375
7
Iteration 25800: Loss = -12383.1630859375
8
Iteration 25900: Loss = -12383.1650390625
9
Iteration 26000: Loss = -12383.1630859375
10
Iteration 26100: Loss = -12383.162109375
11
Iteration 26200: Loss = -12383.1630859375
12
Iteration 26300: Loss = -12383.1630859375
13
Iteration 26400: Loss = -12383.1630859375
14
Iteration 26500: Loss = -12383.1630859375
15
Stopping early at iteration 26500 due to no improvement.
pi: tensor([[1.0000e+00, 1.6588e-06],
        [1.0673e-01, 8.9327e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4898, 0.5102], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1851, 0.1994],
         [0.9878, 0.2208]],

        [[0.1604, 0.1978],
         [0.9586, 0.4301]],

        [[0.0293, 0.2178],
         [0.9895, 0.1229]],

        [[0.8735, 0.2189],
         [0.0784, 0.4331]],

        [[0.0176, 0.1829],
         [0.9565, 0.0429]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 37
Adjusted Rand Index: 0.058159677582870634
time is 1
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.008263629210462791
time is 2
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.009696969696969697
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.001744118975506078
time is 4
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 3.0741259601154114e-05
Global Adjusted Rand Index: 0.0004062231620568949
Average Adjusted Rand Index: 0.007697140191906644
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29109.0234375
Iteration 100: Loss = -19316.6640625
Iteration 200: Loss = -14398.6357421875
Iteration 300: Loss = -12708.2802734375
Iteration 400: Loss = -12502.041015625
Iteration 500: Loss = -12459.87109375
Iteration 600: Loss = -12433.6591796875
Iteration 700: Loss = -12416.6298828125
Iteration 800: Loss = -12409.326171875
Iteration 900: Loss = -12404.7265625
Iteration 1000: Loss = -12401.5537109375
Iteration 1100: Loss = -12399.23828125
Iteration 1200: Loss = -12397.4765625
Iteration 1300: Loss = -12396.1044921875
Iteration 1400: Loss = -12395.0234375
Iteration 1500: Loss = -12394.1435546875
Iteration 1600: Loss = -12393.4169921875
Iteration 1700: Loss = -12392.8046875
Iteration 1800: Loss = -12392.2861328125
Iteration 1900: Loss = -12391.8388671875
Iteration 2000: Loss = -12391.4541015625
Iteration 2100: Loss = -12391.1171875
Iteration 2200: Loss = -12390.818359375
Iteration 2300: Loss = -12390.5546875
Iteration 2400: Loss = -12390.318359375
Iteration 2500: Loss = -12390.10546875
Iteration 2600: Loss = -12389.9150390625
Iteration 2700: Loss = -12389.7412109375
Iteration 2800: Loss = -12389.5859375
Iteration 2900: Loss = -12389.443359375
Iteration 3000: Loss = -12389.3115234375
Iteration 3100: Loss = -12389.193359375
Iteration 3200: Loss = -12389.0830078125
Iteration 3300: Loss = -12388.982421875
Iteration 3400: Loss = -12388.8896484375
Iteration 3500: Loss = -12388.8056640625
Iteration 3600: Loss = -12388.7255859375
Iteration 3700: Loss = -12388.65234375
Iteration 3800: Loss = -12388.5849609375
Iteration 3900: Loss = -12388.5205078125
Iteration 4000: Loss = -12388.4638671875
Iteration 4100: Loss = -12388.41015625
Iteration 4200: Loss = -12388.357421875
Iteration 4300: Loss = -12388.310546875
Iteration 4400: Loss = -12388.267578125
Iteration 4500: Loss = -12388.2255859375
Iteration 4600: Loss = -12388.1865234375
Iteration 4700: Loss = -12388.1494140625
Iteration 4800: Loss = -12388.1171875
Iteration 4900: Loss = -12388.0849609375
Iteration 5000: Loss = -12388.0537109375
Iteration 5100: Loss = -12388.0244140625
Iteration 5200: Loss = -12387.998046875
Iteration 5300: Loss = -12387.970703125
Iteration 5400: Loss = -12387.9462890625
Iteration 5500: Loss = -12387.921875
Iteration 5600: Loss = -12387.9013671875
Iteration 5700: Loss = -12387.876953125
Iteration 5800: Loss = -12387.8564453125
Iteration 5900: Loss = -12387.8359375
Iteration 6000: Loss = -12387.8154296875
Iteration 6100: Loss = -12387.7939453125
Iteration 6200: Loss = -12387.7724609375
Iteration 6300: Loss = -12387.751953125
Iteration 6400: Loss = -12387.728515625
Iteration 6500: Loss = -12387.69921875
Iteration 6600: Loss = -12387.6650390625
Iteration 6700: Loss = -12387.62890625
Iteration 6800: Loss = -12387.587890625
Iteration 6900: Loss = -12387.552734375
Iteration 7000: Loss = -12387.521484375
Iteration 7100: Loss = -12387.49609375
Iteration 7200: Loss = -12387.47265625
Iteration 7300: Loss = -12387.44921875
Iteration 7400: Loss = -12387.4296875
Iteration 7500: Loss = -12387.4111328125
Iteration 7600: Loss = -12387.3896484375
Iteration 7700: Loss = -12387.373046875
Iteration 7800: Loss = -12387.3544921875
Iteration 7900: Loss = -12387.337890625
Iteration 8000: Loss = -12387.3203125
Iteration 8100: Loss = -12387.302734375
Iteration 8200: Loss = -12387.287109375
Iteration 8300: Loss = -12387.2685546875
Iteration 8400: Loss = -12387.25
Iteration 8500: Loss = -12387.2294921875
Iteration 8600: Loss = -12387.2060546875
Iteration 8700: Loss = -12387.1826171875
Iteration 8800: Loss = -12387.1572265625
Iteration 8900: Loss = -12387.1279296875
Iteration 9000: Loss = -12387.0966796875
Iteration 9100: Loss = -12387.060546875
Iteration 9200: Loss = -12387.0166015625
Iteration 9300: Loss = -12386.9697265625
Iteration 9400: Loss = -12386.9130859375
Iteration 9500: Loss = -12386.84375
Iteration 9600: Loss = -12386.7607421875
Iteration 9700: Loss = -12386.662109375
Iteration 9800: Loss = -12386.537109375
Iteration 9900: Loss = -12386.388671875
Iteration 10000: Loss = -12386.2099609375
Iteration 10100: Loss = -12385.998046875
Iteration 10200: Loss = -12385.7529296875
Iteration 10300: Loss = -12385.482421875
Iteration 10400: Loss = -12385.2080078125
Iteration 10500: Loss = -12384.9619140625
Iteration 10600: Loss = -12384.7880859375
Iteration 10700: Loss = -12384.6640625
Iteration 10800: Loss = -12384.5673828125
Iteration 10900: Loss = -12384.4599609375
Iteration 11000: Loss = -12384.3408203125
Iteration 11100: Loss = -12384.2470703125
Iteration 11200: Loss = -12384.177734375
Iteration 11300: Loss = -12384.09765625
Iteration 11400: Loss = -12384.03515625
Iteration 11500: Loss = -12383.9560546875
Iteration 11600: Loss = -12383.89453125
Iteration 11700: Loss = -12383.8134765625
Iteration 11800: Loss = -12383.7080078125
Iteration 11900: Loss = -12383.6171875
Iteration 12000: Loss = -12383.5732421875
Iteration 12100: Loss = -12383.5546875
Iteration 12200: Loss = -12383.5458984375
Iteration 12300: Loss = -12383.5400390625
Iteration 12400: Loss = -12383.53125
Iteration 12500: Loss = -12383.5244140625
Iteration 12600: Loss = -12383.515625
Iteration 12700: Loss = -12383.5068359375
Iteration 12800: Loss = -12383.4951171875
Iteration 12900: Loss = -12383.486328125
Iteration 13000: Loss = -12383.4794921875
Iteration 13100: Loss = -12383.4716796875
Iteration 13200: Loss = -12383.4638671875
Iteration 13300: Loss = -12383.4541015625
Iteration 13400: Loss = -12383.4287109375
Iteration 13500: Loss = -12383.0869140625
Iteration 13600: Loss = -12382.9228515625
Iteration 13700: Loss = -12382.8857421875
Iteration 13800: Loss = -12382.8701171875
Iteration 13900: Loss = -12382.8603515625
Iteration 14000: Loss = -12381.474609375
Iteration 14100: Loss = -11956.4541015625
Iteration 14200: Loss = -11920.974609375
Iteration 14300: Loss = -11919.33203125
Iteration 14400: Loss = -11918.658203125
Iteration 14500: Loss = -11918.2841796875
Iteration 14600: Loss = -11918.0419921875
Iteration 14700: Loss = -11917.876953125
Iteration 14800: Loss = -11917.7568359375
Iteration 14900: Loss = -11917.6650390625
Iteration 15000: Loss = -11917.59375
Iteration 15100: Loss = -11917.5341796875
Iteration 15200: Loss = -11917.4853515625
Iteration 15300: Loss = -11917.4482421875
Iteration 15400: Loss = -11917.4150390625
Iteration 15500: Loss = -11917.38671875
Iteration 15600: Loss = -11917.361328125
Iteration 15700: Loss = -11917.3408203125
Iteration 15800: Loss = -11917.322265625
Iteration 15900: Loss = -11917.3076171875
Iteration 16000: Loss = -11917.2919921875
Iteration 16100: Loss = -11917.2783203125
Iteration 16200: Loss = -11917.2685546875
Iteration 16300: Loss = -11917.2578125
Iteration 16400: Loss = -11917.2470703125
Iteration 16500: Loss = -11917.240234375
Iteration 16600: Loss = -11917.2333984375
Iteration 16700: Loss = -11917.2255859375
Iteration 16800: Loss = -11917.21875
Iteration 16900: Loss = -11917.2138671875
Iteration 17000: Loss = -11917.20703125
Iteration 17100: Loss = -11917.2021484375
Iteration 17200: Loss = -11917.1982421875
Iteration 17300: Loss = -11917.1943359375
Iteration 17400: Loss = -11917.189453125
Iteration 17500: Loss = -11917.1865234375
Iteration 17600: Loss = -11917.1826171875
Iteration 17700: Loss = -11917.1796875
Iteration 17800: Loss = -11917.177734375
Iteration 17900: Loss = -11917.17578125
Iteration 18000: Loss = -11917.1728515625
Iteration 18100: Loss = -11917.169921875
Iteration 18200: Loss = -11917.16796875
Iteration 18300: Loss = -11917.166015625
Iteration 18400: Loss = -11917.1630859375
Iteration 18500: Loss = -11917.1611328125
Iteration 18600: Loss = -11917.1591796875
Iteration 18700: Loss = -11917.1591796875
Iteration 18800: Loss = -11917.1572265625
Iteration 18900: Loss = -11917.1552734375
Iteration 19000: Loss = -11917.154296875
Iteration 19100: Loss = -11917.1533203125
Iteration 19200: Loss = -11917.1513671875
Iteration 19300: Loss = -11917.150390625
Iteration 19400: Loss = -11917.1494140625
Iteration 19500: Loss = -11917.1474609375
Iteration 19600: Loss = -11917.146484375
Iteration 19700: Loss = -11917.146484375
Iteration 19800: Loss = -11917.1455078125
Iteration 19900: Loss = -11917.14453125
Iteration 20000: Loss = -11917.14453125
Iteration 20100: Loss = -11917.1435546875
Iteration 20200: Loss = -11917.142578125
Iteration 20300: Loss = -11917.142578125
Iteration 20400: Loss = -11917.142578125
Iteration 20500: Loss = -11917.140625
Iteration 20600: Loss = -11917.140625
Iteration 20700: Loss = -11917.1396484375
Iteration 20800: Loss = -11917.138671875
Iteration 20900: Loss = -11917.1376953125
Iteration 21000: Loss = -11917.138671875
1
Iteration 21100: Loss = -11917.1376953125
Iteration 21200: Loss = -11917.1376953125
Iteration 21300: Loss = -11917.1376953125
Iteration 21400: Loss = -11917.1376953125
Iteration 21500: Loss = -11917.13671875
Iteration 21600: Loss = -11917.13671875
Iteration 21700: Loss = -11917.1357421875
Iteration 21800: Loss = -11917.1357421875
Iteration 21900: Loss = -11917.13671875
1
Iteration 22000: Loss = -11917.13671875
2
Iteration 22100: Loss = -11917.13671875
3
Iteration 22200: Loss = -11917.134765625
Iteration 22300: Loss = -11917.1357421875
1
Iteration 22400: Loss = -11917.134765625
Iteration 22500: Loss = -11917.1357421875
1
Iteration 22600: Loss = -11917.134765625
Iteration 22700: Loss = -11917.134765625
Iteration 22800: Loss = -11917.134765625
Iteration 22900: Loss = -11917.1337890625
Iteration 23000: Loss = -11917.134765625
1
Iteration 23100: Loss = -11917.1337890625
Iteration 23200: Loss = -11917.1337890625
Iteration 23300: Loss = -11917.134765625
1
Iteration 23400: Loss = -11917.134765625
2
Iteration 23500: Loss = -11917.1328125
Iteration 23600: Loss = -11917.134765625
1
Iteration 23700: Loss = -11917.1337890625
2
Iteration 23800: Loss = -11917.1328125
Iteration 23900: Loss = -11917.1337890625
1
Iteration 24000: Loss = -11917.1337890625
2
Iteration 24100: Loss = -11917.1337890625
3
Iteration 24200: Loss = -11917.1328125
Iteration 24300: Loss = -11917.1337890625
1
Iteration 24400: Loss = -11917.1318359375
Iteration 24500: Loss = -11917.1328125
1
Iteration 24600: Loss = -11917.1318359375
Iteration 24700: Loss = -11917.1318359375
Iteration 24800: Loss = -11917.1337890625
1
Iteration 24900: Loss = -11917.1328125
2
Iteration 25000: Loss = -11917.1328125
3
Iteration 25100: Loss = -11917.1328125
4
Iteration 25200: Loss = -11917.130859375
Iteration 25300: Loss = -11917.1328125
1
Iteration 25400: Loss = -11917.1318359375
2
Iteration 25500: Loss = -11917.1328125
3
Iteration 25600: Loss = -11917.1337890625
4
Iteration 25700: Loss = -11917.1318359375
5
Iteration 25800: Loss = -11917.1318359375
6
Iteration 25900: Loss = -11917.1318359375
7
Iteration 26000: Loss = -11917.1318359375
8
Iteration 26100: Loss = -11917.1328125
9
Iteration 26200: Loss = -11917.1328125
10
Iteration 26300: Loss = -11917.1328125
11
Iteration 26400: Loss = -11917.1328125
12
Iteration 26500: Loss = -11917.1328125
13
Iteration 26600: Loss = -11917.1318359375
14
Iteration 26700: Loss = -11917.1328125
15
Stopping early at iteration 26700 due to no improvement.
pi: tensor([[0.3673, 0.6327],
        [0.6484, 0.3516]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5397, 0.4603], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3076, 0.1082],
         [0.0585, 0.2909]],

        [[0.9768, 0.0971],
         [0.0718, 0.9005]],

        [[0.0106, 0.1019],
         [0.0762, 0.9932]],

        [[0.0175, 0.1157],
         [0.8565, 0.8644]],

        [[0.1726, 0.0854],
         [0.9841, 0.0933]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03648730285905462
Average Adjusted Rand Index: 0.9919995611635631
[0.0004062231620568949, 0.03648730285905462] [0.007697140191906644, 0.9919995611635631] [12383.1630859375, 11917.1328125]
-------------------------------------
This iteration is 40
True Objective function: Loss = -12044.145949507372
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21763.244140625
Iteration 100: Loss = -15227.83984375
Iteration 200: Loss = -13108.669921875
Iteration 300: Loss = -12786.697265625
Iteration 400: Loss = -12714.693359375
Iteration 500: Loss = -12680.0732421875
Iteration 600: Loss = -12658.5625
Iteration 700: Loss = -12638.296875
Iteration 800: Loss = -12611.7724609375
Iteration 900: Loss = -12593.6728515625
Iteration 1000: Loss = -12581.8349609375
Iteration 1100: Loss = -12573.0537109375
Iteration 1200: Loss = -12568.8681640625
Iteration 1300: Loss = -12565.0107421875
Iteration 1400: Loss = -12559.5166015625
Iteration 1500: Loss = -12556.1572265625
Iteration 1600: Loss = -12553.98046875
Iteration 1700: Loss = -12552.1279296875
Iteration 1800: Loss = -12550.748046875
Iteration 1900: Loss = -12549.5810546875
Iteration 2000: Loss = -12548.34765625
Iteration 2100: Loss = -12547.26953125
Iteration 2200: Loss = -12546.51171875
Iteration 2300: Loss = -12545.970703125
Iteration 2400: Loss = -12545.53125
Iteration 2500: Loss = -12545.158203125
Iteration 2600: Loss = -12544.8349609375
Iteration 2700: Loss = -12544.552734375
Iteration 2800: Loss = -12544.3017578125
Iteration 2900: Loss = -12544.080078125
Iteration 3000: Loss = -12543.87890625
Iteration 3100: Loss = -12543.69921875
Iteration 3200: Loss = -12543.5380859375
Iteration 3300: Loss = -12543.390625
Iteration 3400: Loss = -12543.25390625
Iteration 3500: Loss = -12543.1318359375
Iteration 3600: Loss = -12543.0185546875
Iteration 3700: Loss = -12542.9130859375
Iteration 3800: Loss = -12542.8193359375
Iteration 3900: Loss = -12542.7275390625
Iteration 4000: Loss = -12542.6455078125
Iteration 4100: Loss = -12542.5703125
Iteration 4200: Loss = -12542.498046875
Iteration 4300: Loss = -12542.4326171875
Iteration 4400: Loss = -12542.369140625
Iteration 4500: Loss = -12542.3125
Iteration 4600: Loss = -12542.2587890625
Iteration 4700: Loss = -12542.208984375
Iteration 4800: Loss = -12542.162109375
Iteration 4900: Loss = -12542.1171875
Iteration 5000: Loss = -12542.0751953125
Iteration 5100: Loss = -12542.0380859375
Iteration 5200: Loss = -12542.001953125
Iteration 5300: Loss = -12541.966796875
Iteration 5400: Loss = -12541.93359375
Iteration 5500: Loss = -12541.904296875
Iteration 5600: Loss = -12541.8740234375
Iteration 5700: Loss = -12541.84765625
Iteration 5800: Loss = -12541.8232421875
Iteration 5900: Loss = -12541.798828125
Iteration 6000: Loss = -12541.7763671875
Iteration 6100: Loss = -12541.7548828125
Iteration 6200: Loss = -12541.736328125
Iteration 6300: Loss = -12541.716796875
Iteration 6400: Loss = -12541.6982421875
Iteration 6500: Loss = -12541.6826171875
Iteration 6600: Loss = -12541.6650390625
Iteration 6700: Loss = -12541.650390625
Iteration 6800: Loss = -12541.6357421875
Iteration 6900: Loss = -12541.623046875
Iteration 7000: Loss = -12541.609375
Iteration 7100: Loss = -12541.595703125
Iteration 7200: Loss = -12541.5849609375
Iteration 7300: Loss = -12541.5751953125
Iteration 7400: Loss = -12541.5625
Iteration 7500: Loss = -12541.5546875
Iteration 7600: Loss = -12541.544921875
Iteration 7700: Loss = -12541.53515625
Iteration 7800: Loss = -12541.5263671875
Iteration 7900: Loss = -12541.51953125
Iteration 8000: Loss = -12541.5126953125
Iteration 8100: Loss = -12541.5048828125
Iteration 8200: Loss = -12541.49609375
Iteration 8300: Loss = -12541.4912109375
Iteration 8400: Loss = -12541.484375
Iteration 8500: Loss = -12541.4775390625
Iteration 8600: Loss = -12541.4716796875
Iteration 8700: Loss = -12541.46875
Iteration 8800: Loss = -12541.4619140625
Iteration 8900: Loss = -12541.45703125
Iteration 9000: Loss = -12541.4541015625
Iteration 9100: Loss = -12541.44921875
Iteration 9200: Loss = -12541.4443359375
Iteration 9300: Loss = -12541.4404296875
Iteration 9400: Loss = -12541.435546875
Iteration 9500: Loss = -12541.43359375
Iteration 9600: Loss = -12541.427734375
Iteration 9700: Loss = -12541.427734375
Iteration 9800: Loss = -12541.423828125
Iteration 9900: Loss = -12541.4208984375
Iteration 10000: Loss = -12541.4169921875
Iteration 10100: Loss = -12541.4150390625
Iteration 10200: Loss = -12541.4130859375
Iteration 10300: Loss = -12541.4091796875
Iteration 10400: Loss = -12541.40625
Iteration 10500: Loss = -12541.4072265625
1
Iteration 10600: Loss = -12541.4033203125
Iteration 10700: Loss = -12541.4013671875
Iteration 10800: Loss = -12541.3984375
Iteration 10900: Loss = -12541.3984375
Iteration 11000: Loss = -12541.3935546875
Iteration 11100: Loss = -12541.392578125
Iteration 11200: Loss = -12541.3916015625
Iteration 11300: Loss = -12541.3896484375
Iteration 11400: Loss = -12541.388671875
Iteration 11500: Loss = -12541.3876953125
Iteration 11600: Loss = -12541.384765625
Iteration 11700: Loss = -12541.3837890625
Iteration 11800: Loss = -12541.3828125
Iteration 11900: Loss = -12541.3818359375
Iteration 12000: Loss = -12541.380859375
Iteration 12100: Loss = -12541.3779296875
Iteration 12200: Loss = -12541.37890625
1
Iteration 12300: Loss = -12541.376953125
Iteration 12400: Loss = -12541.376953125
Iteration 12500: Loss = -12541.375
Iteration 12600: Loss = -12541.3759765625
1
Iteration 12700: Loss = -12541.3740234375
Iteration 12800: Loss = -12541.3740234375
Iteration 12900: Loss = -12541.3720703125
Iteration 13000: Loss = -12541.3720703125
Iteration 13100: Loss = -12541.3720703125
Iteration 13200: Loss = -12541.37109375
Iteration 13300: Loss = -12541.3701171875
Iteration 13400: Loss = -12541.3701171875
Iteration 13500: Loss = -12541.3681640625
Iteration 13600: Loss = -12541.369140625
1
Iteration 13700: Loss = -12541.3681640625
Iteration 13800: Loss = -12541.3681640625
Iteration 13900: Loss = -12541.369140625
1
Iteration 14000: Loss = -12541.3681640625
Iteration 14100: Loss = -12541.369140625
1
Iteration 14200: Loss = -12541.3671875
Iteration 14300: Loss = -12541.3671875
Iteration 14400: Loss = -12541.3671875
Iteration 14500: Loss = -12541.3681640625
1
Iteration 14600: Loss = -12541.3671875
Iteration 14700: Loss = -12541.3662109375
Iteration 14800: Loss = -12541.3671875
1
Iteration 14900: Loss = -12541.3671875
2
Iteration 15000: Loss = -12541.3662109375
Iteration 15100: Loss = -12541.3662109375
Iteration 15200: Loss = -12541.3642578125
Iteration 15300: Loss = -12541.365234375
1
Iteration 15400: Loss = -12541.3642578125
Iteration 15500: Loss = -12541.36328125
Iteration 15600: Loss = -12541.36328125
Iteration 15700: Loss = -12541.361328125
Iteration 15800: Loss = -12541.359375
Iteration 15900: Loss = -12541.357421875
Iteration 16000: Loss = -12541.3525390625
Iteration 16100: Loss = -12541.3466796875
Iteration 16200: Loss = -12541.3369140625
Iteration 16300: Loss = -12541.322265625
Iteration 16400: Loss = -12541.3125
Iteration 16500: Loss = -12541.279296875
Iteration 16600: Loss = -12541.244140625
Iteration 16700: Loss = -12541.21875
Iteration 16800: Loss = -12541.2080078125
Iteration 16900: Loss = -12541.2041015625
Iteration 17000: Loss = -12541.19921875
Iteration 17100: Loss = -12541.1875
Iteration 17200: Loss = -12541.0546875
Iteration 17300: Loss = -12540.90234375
Iteration 17400: Loss = -12540.8369140625
Iteration 17500: Loss = -12540.828125
Iteration 17600: Loss = -12540.818359375
Iteration 17700: Loss = -12540.8173828125
Iteration 17800: Loss = -12540.8154296875
Iteration 17900: Loss = -12540.814453125
Iteration 18000: Loss = -12540.8125
Iteration 18100: Loss = -12540.810546875
Iteration 18200: Loss = -12540.8095703125
Iteration 18300: Loss = -12540.8095703125
Iteration 18400: Loss = -12540.8056640625
Iteration 18500: Loss = -12540.8046875
Iteration 18600: Loss = -12540.8037109375
Iteration 18700: Loss = -12540.7998046875
Iteration 18800: Loss = -12540.80078125
1
Iteration 18900: Loss = -12540.7978515625
Iteration 19000: Loss = -12540.798828125
1
Iteration 19100: Loss = -12540.7958984375
Iteration 19200: Loss = -12540.7958984375
Iteration 19300: Loss = -12540.794921875
Iteration 19400: Loss = -12540.796875
1
Iteration 19500: Loss = -12540.7958984375
2
Iteration 19600: Loss = -12540.796875
3
Iteration 19700: Loss = -12540.7958984375
4
Iteration 19800: Loss = -12540.7958984375
5
Iteration 19900: Loss = -12540.7958984375
6
Iteration 20000: Loss = -12540.7958984375
7
Iteration 20100: Loss = -12540.7958984375
8
Iteration 20200: Loss = -12540.794921875
Iteration 20300: Loss = -12540.7861328125
Iteration 20400: Loss = -12540.7880859375
1
Iteration 20500: Loss = -12540.787109375
2
Iteration 20600: Loss = -12540.7861328125
Iteration 20700: Loss = -12540.787109375
1
Iteration 20800: Loss = -12540.78515625
Iteration 20900: Loss = -12540.78515625
Iteration 21000: Loss = -12540.7841796875
Iteration 21100: Loss = -12540.7861328125
1
Iteration 21200: Loss = -12540.78515625
2
Iteration 21300: Loss = -12540.78125
Iteration 21400: Loss = -12540.779296875
Iteration 21500: Loss = -12540.775390625
Iteration 21600: Loss = -12540.77734375
1
Iteration 21700: Loss = -12540.7783203125
2
Iteration 21800: Loss = -12540.77734375
3
Iteration 21900: Loss = -12540.7763671875
4
Iteration 22000: Loss = -12540.7734375
Iteration 22100: Loss = -12540.7705078125
Iteration 22200: Loss = -12540.7685546875
Iteration 22300: Loss = -12540.7705078125
1
Iteration 22400: Loss = -12540.7705078125
2
Iteration 22500: Loss = -12540.7685546875
Iteration 22600: Loss = -12540.7685546875
Iteration 22700: Loss = -12540.7705078125
1
Iteration 22800: Loss = -12540.7685546875
Iteration 22900: Loss = -12540.7685546875
Iteration 23000: Loss = -12540.7626953125
Iteration 23100: Loss = -12540.7607421875
Iteration 23200: Loss = -12540.7607421875
Iteration 23300: Loss = -12540.7607421875
Iteration 23400: Loss = -12540.7587890625
Iteration 23500: Loss = -12540.7607421875
1
Iteration 23600: Loss = -12540.7578125
Iteration 23700: Loss = -12540.7578125
Iteration 23800: Loss = -12540.755859375
Iteration 23900: Loss = -12540.755859375
Iteration 24000: Loss = -12540.755859375
Iteration 24100: Loss = -12540.7548828125
Iteration 24200: Loss = -12540.7529296875
Iteration 24300: Loss = -12540.7529296875
Iteration 24400: Loss = -12540.7529296875
Iteration 24500: Loss = -12540.75390625
1
Iteration 24600: Loss = -12540.751953125
Iteration 24700: Loss = -12540.751953125
Iteration 24800: Loss = -12540.75390625
1
Iteration 24900: Loss = -12540.75390625
2
Iteration 25000: Loss = -12540.751953125
Iteration 25100: Loss = -12540.7509765625
Iteration 25200: Loss = -12540.75
Iteration 25300: Loss = -12540.75
Iteration 25400: Loss = -12540.75
Iteration 25500: Loss = -12540.75
Iteration 25600: Loss = -12540.75
Iteration 25700: Loss = -12540.75
Iteration 25800: Loss = -12540.75
Iteration 25900: Loss = -12540.751953125
1
Iteration 26000: Loss = -12540.75
Iteration 26100: Loss = -12540.7490234375
Iteration 26200: Loss = -12540.7490234375
Iteration 26300: Loss = -12540.748046875
Iteration 26400: Loss = -12540.7490234375
1
Iteration 26500: Loss = -12540.748046875
Iteration 26600: Loss = -12540.748046875
Iteration 26700: Loss = -12540.748046875
Iteration 26800: Loss = -12540.748046875
Iteration 26900: Loss = -12540.74609375
Iteration 27000: Loss = -12540.7470703125
1
Iteration 27100: Loss = -12540.7470703125
2
Iteration 27200: Loss = -12540.74609375
Iteration 27300: Loss = -12540.7451171875
Iteration 27400: Loss = -12540.74609375
1
Iteration 27500: Loss = -12540.74609375
2
Iteration 27600: Loss = -12540.74609375
3
Iteration 27700: Loss = -12540.74609375
4
Iteration 27800: Loss = -12540.736328125
Iteration 27900: Loss = -12540.736328125
Iteration 28000: Loss = -12540.732421875
Iteration 28100: Loss = -12540.7333984375
1
Iteration 28200: Loss = -12540.7314453125
Iteration 28300: Loss = -12540.7314453125
Iteration 28400: Loss = -12540.7294921875
Iteration 28500: Loss = -12540.732421875
1
Iteration 28600: Loss = -12540.73046875
2
Iteration 28700: Loss = -12540.73046875
3
Iteration 28800: Loss = -12540.732421875
4
Iteration 28900: Loss = -12540.73046875
5
Iteration 29000: Loss = -12540.7314453125
6
Iteration 29100: Loss = -12540.732421875
7
Iteration 29200: Loss = -12540.73046875
8
Iteration 29300: Loss = -12540.7314453125
9
Iteration 29400: Loss = -12540.7265625
Iteration 29500: Loss = -12540.7275390625
1
Iteration 29600: Loss = -12540.7255859375
Iteration 29700: Loss = -12540.7265625
1
Iteration 29800: Loss = -12540.7265625
2
Iteration 29900: Loss = -12540.7265625
3
pi: tensor([[9.9999e-01, 6.4801e-06],
        [1.9301e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9650, 0.0350], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2046, 0.1858],
         [0.9862, 0.1948]],

        [[0.0124, 0.2612],
         [0.9894, 0.0131]],

        [[0.9740, 0.1648],
         [0.8985, 0.6016]],

        [[0.0101, 0.2052],
         [0.0585, 0.0487]],

        [[0.9408, 0.2061],
         [0.8497, 0.7452]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.005453571602473584
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
Global Adjusted Rand Index: -0.00021769226938953627
Average Adjusted Rand Index: -0.000828550054552973
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26548.021484375
Iteration 100: Loss = -18237.68359375
Iteration 200: Loss = -13893.15234375
Iteration 300: Loss = -12879.5322265625
Iteration 400: Loss = -12686.072265625
Iteration 500: Loss = -12607.591796875
Iteration 600: Loss = -12569.2890625
Iteration 700: Loss = -12549.958984375
Iteration 800: Loss = -12528.6259765625
Iteration 900: Loss = -12490.3408203125
Iteration 1000: Loss = -12429.6064453125
Iteration 1100: Loss = -12406.916015625
Iteration 1200: Loss = -12400.25
Iteration 1300: Loss = -12396.9326171875
Iteration 1400: Loss = -12394.7001953125
Iteration 1500: Loss = -12392.8525390625
Iteration 1600: Loss = -12388.9853515625
Iteration 1700: Loss = -12386.90234375
Iteration 1800: Loss = -12385.931640625
Iteration 1900: Loss = -12385.1865234375
Iteration 2000: Loss = -12384.5771484375
Iteration 2100: Loss = -12384.06640625
Iteration 2200: Loss = -12383.62890625
Iteration 2300: Loss = -12383.251953125
Iteration 2400: Loss = -12382.919921875
Iteration 2500: Loss = -12382.6328125
Iteration 2600: Loss = -12382.3759765625
Iteration 2700: Loss = -12382.1484375
Iteration 2800: Loss = -12381.9462890625
Iteration 2900: Loss = -12381.7626953125
Iteration 3000: Loss = -12381.59765625
Iteration 3100: Loss = -12381.451171875
Iteration 3200: Loss = -12381.31640625
Iteration 3300: Loss = -12381.1953125
Iteration 3400: Loss = -12381.083984375
Iteration 3500: Loss = -12380.9833984375
Iteration 3600: Loss = -12380.890625
Iteration 3700: Loss = -12380.8056640625
Iteration 3800: Loss = -12380.728515625
Iteration 3900: Loss = -12380.6552734375
Iteration 4000: Loss = -12380.58984375
Iteration 4100: Loss = -12380.529296875
Iteration 4200: Loss = -12380.47265625
Iteration 4300: Loss = -12380.4208984375
Iteration 4400: Loss = -12380.3720703125
Iteration 4500: Loss = -12380.3291015625
Iteration 4600: Loss = -12380.287109375
Iteration 4700: Loss = -12380.2490234375
Iteration 4800: Loss = -12380.2109375
Iteration 4900: Loss = -12380.1787109375
Iteration 5000: Loss = -12380.1474609375
Iteration 5100: Loss = -12380.1181640625
Iteration 5200: Loss = -12380.0888671875
Iteration 5300: Loss = -12380.0625
Iteration 5400: Loss = -12380.0361328125
Iteration 5500: Loss = -12380.0126953125
Iteration 5600: Loss = -12379.9873046875
Iteration 5700: Loss = -12379.9580078125
Iteration 5800: Loss = -12379.9169921875
Iteration 5900: Loss = -12379.85546875
Iteration 6000: Loss = -12379.7099609375
Iteration 6100: Loss = -12379.02734375
Iteration 6200: Loss = -12377.5185546875
Iteration 6300: Loss = -12376.9970703125
Iteration 6400: Loss = -12376.9013671875
Iteration 6500: Loss = -12376.87890625
Iteration 6600: Loss = -12376.859375
Iteration 6700: Loss = -12376.3193359375
Iteration 6800: Loss = -12376.2392578125
Iteration 6900: Loss = -12375.6123046875
Iteration 7000: Loss = -12375.5849609375
Iteration 7100: Loss = -12375.548828125
Iteration 7200: Loss = -12374.6201171875
Iteration 7300: Loss = -12372.4658203125
Iteration 7400: Loss = -12371.8701171875
Iteration 7500: Loss = -12371.177734375
Iteration 7600: Loss = -12369.7734375
Iteration 7700: Loss = -12369.46875
Iteration 7800: Loss = -12367.427734375
Iteration 7900: Loss = -12366.8662109375
Iteration 8000: Loss = -12365.5048828125
Iteration 8100: Loss = -12364.1943359375
Iteration 8200: Loss = -12362.72265625
Iteration 8300: Loss = -12361.5751953125
Iteration 8400: Loss = -12356.6025390625
Iteration 8500: Loss = -12341.5048828125
Iteration 8600: Loss = -12333.859375
Iteration 8700: Loss = -12331.248046875
Iteration 8800: Loss = -12316.83984375
Iteration 8900: Loss = -12301.2587890625
Iteration 9000: Loss = -12283.4970703125
Iteration 9100: Loss = -12265.14453125
Iteration 9200: Loss = -12252.5703125
Iteration 9300: Loss = -12252.0185546875
Iteration 9400: Loss = -12251.857421875
Iteration 9500: Loss = -12251.73046875
Iteration 9600: Loss = -12241.4443359375
Iteration 9700: Loss = -12239.927734375
Iteration 9800: Loss = -12239.8330078125
Iteration 9900: Loss = -12239.7890625
Iteration 10000: Loss = -12239.75390625
Iteration 10100: Loss = -12239.7197265625
Iteration 10200: Loss = -12239.4990234375
Iteration 10300: Loss = -12239.404296875
Iteration 10400: Loss = -12239.375
Iteration 10500: Loss = -12239.345703125
Iteration 10600: Loss = -12239.3115234375
Iteration 10700: Loss = -12239.2763671875
Iteration 10800: Loss = -12233.1640625
Iteration 10900: Loss = -12228.7255859375
Iteration 11000: Loss = -12228.6103515625
Iteration 11100: Loss = -12228.55859375
Iteration 11200: Loss = -12228.4990234375
Iteration 11300: Loss = -12227.9560546875
Iteration 11400: Loss = -12222.61328125
Iteration 11500: Loss = -12222.1884765625
Iteration 11600: Loss = -12219.671875
Iteration 11700: Loss = -12219.6025390625
Iteration 11800: Loss = -12216.7626953125
Iteration 11900: Loss = -12216.64453125
Iteration 12000: Loss = -12216.623046875
Iteration 12100: Loss = -12214.78515625
Iteration 12200: Loss = -12214.7392578125
Iteration 12300: Loss = -12214.732421875
Iteration 12400: Loss = -12214.7265625
Iteration 12500: Loss = -12213.859375
Iteration 12600: Loss = -12213.7216796875
Iteration 12700: Loss = -12213.568359375
Iteration 12800: Loss = -12213.56640625
Iteration 12900: Loss = -12213.5595703125
Iteration 13000: Loss = -12212.84375
Iteration 13100: Loss = -12199.134765625
Iteration 13200: Loss = -12198.87109375
Iteration 13300: Loss = -12198.833984375
Iteration 13400: Loss = -12198.8154296875
Iteration 13500: Loss = -12198.466796875
Iteration 13600: Loss = -12190.53125
Iteration 13700: Loss = -12190.4736328125
Iteration 13800: Loss = -12190.451171875
Iteration 13900: Loss = -12184.9033203125
Iteration 14000: Loss = -12176.8466796875
Iteration 14100: Loss = -12167.068359375
Iteration 14200: Loss = -12166.5068359375
Iteration 14300: Loss = -12163.318359375
Iteration 14400: Loss = -12159.12890625
Iteration 14500: Loss = -12151.4912109375
Iteration 14600: Loss = -12151.392578125
Iteration 14700: Loss = -12151.3564453125
Iteration 14800: Loss = -12151.3349609375
Iteration 14900: Loss = -12140.4482421875
Iteration 15000: Loss = -12137.697265625
Iteration 15100: Loss = -12133.18359375
Iteration 15200: Loss = -12133.119140625
Iteration 15300: Loss = -12133.046875
Iteration 15400: Loss = -12115.7685546875
Iteration 15500: Loss = -12112.03515625
Iteration 15600: Loss = -12102.021484375
Iteration 15700: Loss = -12093.1806640625
Iteration 15800: Loss = -12087.9404296875
Iteration 15900: Loss = -12067.5478515625
Iteration 16000: Loss = -12067.33984375
Iteration 16100: Loss = -12056.2666015625
Iteration 16200: Loss = -12055.853515625
Iteration 16300: Loss = -12055.7724609375
Iteration 16400: Loss = -12055.6416015625
Iteration 16500: Loss = -12040.603515625
Iteration 16600: Loss = -12040.4248046875
Iteration 16700: Loss = -12040.361328125
Iteration 16800: Loss = -12039.333984375
Iteration 16900: Loss = -12030.7158203125
Iteration 17000: Loss = -12030.634765625
Iteration 17100: Loss = -12030.5966796875
Iteration 17200: Loss = -12030.5712890625
Iteration 17300: Loss = -12030.5556640625
Iteration 17400: Loss = -12030.541015625
Iteration 17500: Loss = -12030.5302734375
Iteration 17600: Loss = -12030.5205078125
Iteration 17700: Loss = -12030.513671875
Iteration 17800: Loss = -12030.505859375
Iteration 17900: Loss = -12030.5009765625
Iteration 18000: Loss = -12030.49609375
Iteration 18100: Loss = -12030.490234375
Iteration 18200: Loss = -12030.4873046875
Iteration 18300: Loss = -12030.484375
Iteration 18400: Loss = -12030.4814453125
Iteration 18500: Loss = -12030.478515625
Iteration 18600: Loss = -12030.4755859375
Iteration 18700: Loss = -12030.47265625
Iteration 18800: Loss = -12030.470703125
Iteration 18900: Loss = -12030.4697265625
Iteration 19000: Loss = -12030.4677734375
Iteration 19100: Loss = -12030.4658203125
Iteration 19200: Loss = -12030.46484375
Iteration 19300: Loss = -12030.4638671875
Iteration 19400: Loss = -12030.4609375
Iteration 19500: Loss = -12030.4619140625
1
Iteration 19600: Loss = -12030.4580078125
Iteration 19700: Loss = -12030.4580078125
Iteration 19800: Loss = -12030.4580078125
Iteration 19900: Loss = -12030.45703125
Iteration 20000: Loss = -12030.4560546875
Iteration 20100: Loss = -12030.4541015625
Iteration 20200: Loss = -12030.4541015625
Iteration 20300: Loss = -12030.4541015625
Iteration 20400: Loss = -12030.451171875
Iteration 20500: Loss = -12030.4521484375
1
Iteration 20600: Loss = -12030.451171875
Iteration 20700: Loss = -12030.451171875
Iteration 20800: Loss = -12030.44921875
Iteration 20900: Loss = -12030.44921875
Iteration 21000: Loss = -12030.44921875
Iteration 21100: Loss = -12030.4482421875
Iteration 21200: Loss = -12030.44921875
1
Iteration 21300: Loss = -12030.4482421875
Iteration 21400: Loss = -12030.447265625
Iteration 21500: Loss = -12030.447265625
Iteration 21600: Loss = -12030.4462890625
Iteration 21700: Loss = -12030.4462890625
Iteration 21800: Loss = -12030.4453125
Iteration 21900: Loss = -12030.4462890625
1
Iteration 22000: Loss = -12030.4453125
Iteration 22100: Loss = -12030.4443359375
Iteration 22200: Loss = -12030.4443359375
Iteration 22300: Loss = -12030.4462890625
1
Iteration 22400: Loss = -12030.4443359375
Iteration 22500: Loss = -12030.4453125
1
Iteration 22600: Loss = -12030.4453125
2
Iteration 22700: Loss = -12030.443359375
Iteration 22800: Loss = -12030.443359375
Iteration 22900: Loss = -12030.4443359375
1
Iteration 23000: Loss = -12030.4443359375
2
Iteration 23100: Loss = -12030.4443359375
3
Iteration 23200: Loss = -12030.443359375
Iteration 23300: Loss = -12030.443359375
Iteration 23400: Loss = -12030.4423828125
Iteration 23500: Loss = -12030.4423828125
Iteration 23600: Loss = -12030.4443359375
1
Iteration 23700: Loss = -12030.443359375
2
Iteration 23800: Loss = -12030.443359375
3
Iteration 23900: Loss = -12030.4443359375
4
Iteration 24000: Loss = -12030.443359375
5
Iteration 24100: Loss = -12030.4423828125
Iteration 24200: Loss = -12030.4423828125
Iteration 24300: Loss = -12030.4443359375
1
Iteration 24400: Loss = -12030.443359375
2
Iteration 24500: Loss = -12030.44140625
Iteration 24600: Loss = -12030.4423828125
1
Iteration 24700: Loss = -12030.44140625
Iteration 24800: Loss = -12030.44140625
Iteration 24900: Loss = -12030.44140625
Iteration 25000: Loss = -12030.4423828125
1
Iteration 25100: Loss = -12030.44140625
Iteration 25200: Loss = -12030.44140625
Iteration 25300: Loss = -12030.443359375
1
Iteration 25400: Loss = -12030.44140625
Iteration 25500: Loss = -12030.44140625
Iteration 25600: Loss = -12030.44140625
Iteration 25700: Loss = -12030.443359375
1
Iteration 25800: Loss = -12030.4423828125
2
Iteration 25900: Loss = -12030.44140625
Iteration 26000: Loss = -12030.4423828125
1
Iteration 26100: Loss = -12030.44140625
Iteration 26200: Loss = -12030.44140625
Iteration 26300: Loss = -12030.44140625
Iteration 26400: Loss = -12030.4423828125
1
Iteration 26500: Loss = -12030.4423828125
2
Iteration 26600: Loss = -12030.44140625
Iteration 26700: Loss = -12030.4423828125
1
Iteration 26800: Loss = -12030.44140625
Iteration 26900: Loss = -12030.4423828125
1
Iteration 27000: Loss = -12030.44140625
Iteration 27100: Loss = -12030.4423828125
1
Iteration 27200: Loss = -12030.4423828125
2
Iteration 27300: Loss = -12030.443359375
3
Iteration 27400: Loss = -12030.44140625
Iteration 27500: Loss = -12030.44140625
Iteration 27600: Loss = -12030.4423828125
1
Iteration 27700: Loss = -12030.44140625
Iteration 27800: Loss = -12030.44140625
Iteration 27900: Loss = -12030.443359375
1
Iteration 28000: Loss = -12030.44140625
Iteration 28100: Loss = -12030.4423828125
1
Iteration 28200: Loss = -12030.4404296875
Iteration 28300: Loss = -12030.44140625
1
Iteration 28400: Loss = -12030.44140625
2
Iteration 28500: Loss = -12030.44140625
3
Iteration 28600: Loss = -12030.44140625
4
Iteration 28700: Loss = -12030.44140625
5
Iteration 28800: Loss = -12030.44140625
6
Iteration 28900: Loss = -12030.4423828125
7
Iteration 29000: Loss = -12030.443359375
8
Iteration 29100: Loss = -12030.4423828125
9
Iteration 29200: Loss = -12030.44140625
10
Iteration 29300: Loss = -12030.4423828125
11
Iteration 29400: Loss = -12030.44140625
12
Iteration 29500: Loss = -12030.44140625
13
Iteration 29600: Loss = -12030.4423828125
14
Iteration 29700: Loss = -12030.44140625
15
Stopping early at iteration 29700 due to no improvement.
pi: tensor([[0.7330, 0.2670],
        [0.3034, 0.6967]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4400, 0.5600], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3064, 0.0950],
         [0.9080, 0.3058]],

        [[0.3915, 0.0975],
         [0.9248, 0.1509]],

        [[0.9756, 0.1167],
         [0.6652, 0.9884]],

        [[0.0126, 0.1010],
         [0.3735, 0.6864]],

        [[0.0121, 0.1078],
         [0.0650, 0.2874]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9919999740011123
Average Adjusted Rand Index: 0.9919993417272899
[-0.00021769226938953627, 0.9919999740011123] [-0.000828550054552973, 0.9919993417272899] [12540.7255859375, 12030.44140625]
-------------------------------------
This iteration is 41
True Objective function: Loss = -11922.58733523354
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -44448.76953125
Iteration 100: Loss = -26528.24609375
Iteration 200: Loss = -15575.2978515625
Iteration 300: Loss = -13283.9921875
Iteration 400: Loss = -12773.8271484375
Iteration 500: Loss = -12627.966796875
Iteration 600: Loss = -12556.13671875
Iteration 700: Loss = -12516.474609375
Iteration 800: Loss = -12490.1787109375
Iteration 900: Loss = -12471.5771484375
Iteration 1000: Loss = -12457.8330078125
Iteration 1100: Loss = -12447.33984375
Iteration 1200: Loss = -12439.1162109375
Iteration 1300: Loss = -12432.53515625
Iteration 1400: Loss = -12427.1748046875
Iteration 1500: Loss = -12422.74609375
Iteration 1600: Loss = -12419.0400390625
Iteration 1700: Loss = -12415.904296875
Iteration 1800: Loss = -12413.2294921875
Iteration 1900: Loss = -12410.9287109375
Iteration 2000: Loss = -12408.93359375
Iteration 2100: Loss = -12407.1904296875
Iteration 2200: Loss = -12405.6630859375
Iteration 2300: Loss = -12404.3125
Iteration 2400: Loss = -12403.1162109375
Iteration 2500: Loss = -12402.046875
Iteration 2600: Loss = -12401.091796875
Iteration 2700: Loss = -12400.2236328125
Iteration 2800: Loss = -12397.525390625
Iteration 2900: Loss = -12395.265625
Iteration 3000: Loss = -12394.54296875
Iteration 3100: Loss = -12393.916015625
Iteration 3200: Loss = -12393.3564453125
Iteration 3300: Loss = -12392.85546875
Iteration 3400: Loss = -12392.3994140625
Iteration 3500: Loss = -12391.9892578125
Iteration 3600: Loss = -12391.6103515625
Iteration 3700: Loss = -12391.2626953125
Iteration 3800: Loss = -12390.943359375
Iteration 3900: Loss = -12390.6494140625
Iteration 4000: Loss = -12390.3798828125
Iteration 4100: Loss = -12390.12890625
Iteration 4200: Loss = -12389.89453125
Iteration 4300: Loss = -12389.677734375
Iteration 4400: Loss = -12389.4775390625
Iteration 4500: Loss = -12389.2900390625
Iteration 4600: Loss = -12389.115234375
Iteration 4700: Loss = -12388.951171875
Iteration 4800: Loss = -12388.798828125
Iteration 4900: Loss = -12388.65625
Iteration 5000: Loss = -12388.521484375
Iteration 5100: Loss = -12388.3974609375
Iteration 5200: Loss = -12388.279296875
Iteration 5300: Loss = -12388.16796875
Iteration 5400: Loss = -12388.0625
Iteration 5500: Loss = -12387.9677734375
Iteration 5600: Loss = -12387.875
Iteration 5700: Loss = -12387.7890625
Iteration 5800: Loss = -12387.70703125
Iteration 5900: Loss = -12387.630859375
Iteration 6000: Loss = -12387.5595703125
Iteration 6100: Loss = -12387.4892578125
Iteration 6200: Loss = -12387.4267578125
Iteration 6300: Loss = -12387.3671875
Iteration 6400: Loss = -12387.30859375
Iteration 6500: Loss = -12387.2548828125
Iteration 6600: Loss = -12387.2041015625
Iteration 6700: Loss = -12387.1552734375
Iteration 6800: Loss = -12387.1083984375
Iteration 6900: Loss = -12387.0673828125
Iteration 7000: Loss = -12387.0283203125
Iteration 7100: Loss = -12386.9873046875
Iteration 7200: Loss = -12386.951171875
Iteration 7300: Loss = -12386.91796875
Iteration 7400: Loss = -12386.8857421875
Iteration 7500: Loss = -12386.8544921875
Iteration 7600: Loss = -12386.8251953125
Iteration 7700: Loss = -12386.796875
Iteration 7800: Loss = -12386.7724609375
Iteration 7900: Loss = -12386.74609375
Iteration 8000: Loss = -12386.72265625
Iteration 8100: Loss = -12386.7001953125
Iteration 8200: Loss = -12386.677734375
Iteration 8300: Loss = -12386.6591796875
Iteration 8400: Loss = -12386.638671875
Iteration 8500: Loss = -12386.62109375
Iteration 8600: Loss = -12386.6025390625
Iteration 8700: Loss = -12386.5859375
Iteration 8800: Loss = -12386.5673828125
Iteration 8900: Loss = -12386.55078125
Iteration 9000: Loss = -12386.5341796875
Iteration 9100: Loss = -12386.51171875
Iteration 9200: Loss = -12386.484375
Iteration 9300: Loss = -12386.4462890625
Iteration 9400: Loss = -12386.3876953125
Iteration 9500: Loss = -12386.2470703125
Iteration 9600: Loss = -12386.1689453125
Iteration 9700: Loss = -12386.115234375
Iteration 9800: Loss = -12386.07421875
Iteration 9900: Loss = -12386.041015625
Iteration 10000: Loss = -12386.013671875
Iteration 10100: Loss = -12385.986328125
Iteration 10200: Loss = -12385.96484375
Iteration 10300: Loss = -12385.9423828125
Iteration 10400: Loss = -12385.92578125
Iteration 10500: Loss = -12385.9072265625
Iteration 10600: Loss = -12385.8916015625
Iteration 10700: Loss = -12385.8740234375
Iteration 10800: Loss = -12385.857421875
Iteration 10900: Loss = -12385.841796875
Iteration 11000: Loss = -12385.8271484375
Iteration 11100: Loss = -12385.810546875
Iteration 11200: Loss = -12385.7958984375
Iteration 11300: Loss = -12385.7783203125
Iteration 11400: Loss = -12385.7607421875
Iteration 11500: Loss = -12385.7421875
Iteration 11600: Loss = -12385.72265625
Iteration 11700: Loss = -12385.701171875
Iteration 11800: Loss = -12385.6767578125
Iteration 11900: Loss = -12385.650390625
Iteration 12000: Loss = -12385.6201171875
Iteration 12100: Loss = -12385.5830078125
Iteration 12200: Loss = -12385.5400390625
Iteration 12300: Loss = -12385.486328125
Iteration 12400: Loss = -12385.41796875
Iteration 12500: Loss = -12385.328125
Iteration 12600: Loss = -12385.19921875
Iteration 12700: Loss = -12385.0107421875
Iteration 12800: Loss = -12384.73046875
Iteration 12900: Loss = -12384.3583984375
Iteration 13000: Loss = -12384.01171875
Iteration 13100: Loss = -12383.7880859375
Iteration 13200: Loss = -12383.666015625
Iteration 13300: Loss = -12383.59765625
Iteration 13400: Loss = -12383.5546875
Iteration 13500: Loss = -12383.517578125
Iteration 13600: Loss = -12383.4951171875
Iteration 13700: Loss = -12383.4794921875
Iteration 13800: Loss = -12383.46484375
Iteration 13900: Loss = -12383.4580078125
Iteration 14000: Loss = -12383.4482421875
Iteration 14100: Loss = -12383.4453125
Iteration 14200: Loss = -12383.439453125
Iteration 14300: Loss = -12383.4365234375
Iteration 14400: Loss = -12383.4345703125
Iteration 14500: Loss = -12383.4306640625
Iteration 14600: Loss = -12383.4287109375
Iteration 14700: Loss = -12383.427734375
Iteration 14800: Loss = -12383.42578125
Iteration 14900: Loss = -12383.4248046875
Iteration 15000: Loss = -12383.423828125
Iteration 15100: Loss = -12383.4228515625
Iteration 15200: Loss = -12383.4228515625
Iteration 15300: Loss = -12383.4208984375
Iteration 15400: Loss = -12383.419921875
Iteration 15500: Loss = -12383.419921875
Iteration 15600: Loss = -12383.4208984375
1
Iteration 15700: Loss = -12383.419921875
Iteration 15800: Loss = -12383.4189453125
Iteration 15900: Loss = -12383.41796875
Iteration 16000: Loss = -12383.41796875
Iteration 16100: Loss = -12383.4189453125
1
Iteration 16200: Loss = -12383.4169921875
Iteration 16300: Loss = -12383.416015625
Iteration 16400: Loss = -12383.4189453125
1
Iteration 16500: Loss = -12383.416015625
Iteration 16600: Loss = -12383.416015625
Iteration 16700: Loss = -12383.4169921875
1
Iteration 16800: Loss = -12383.416015625
Iteration 16900: Loss = -12383.4150390625
Iteration 17000: Loss = -12383.416015625
1
Iteration 17100: Loss = -12383.416015625
2
Iteration 17200: Loss = -12383.4140625
Iteration 17300: Loss = -12383.4150390625
1
Iteration 17400: Loss = -12383.4150390625
2
Iteration 17500: Loss = -12383.4150390625
3
Iteration 17600: Loss = -12383.4150390625
4
Iteration 17700: Loss = -12383.416015625
5
Iteration 17800: Loss = -12383.4140625
Iteration 17900: Loss = -12383.4150390625
1
Iteration 18000: Loss = -12383.4150390625
2
Iteration 18100: Loss = -12383.4150390625
3
Iteration 18200: Loss = -12383.4140625
Iteration 18300: Loss = -12383.4140625
Iteration 18400: Loss = -12383.4130859375
Iteration 18500: Loss = -12383.4140625
1
Iteration 18600: Loss = -12383.4150390625
2
Iteration 18700: Loss = -12383.4130859375
Iteration 18800: Loss = -12383.4150390625
1
Iteration 18900: Loss = -12383.4130859375
Iteration 19000: Loss = -12383.4150390625
1
Iteration 19100: Loss = -12383.4130859375
Iteration 19200: Loss = -12383.4130859375
Iteration 19300: Loss = -12383.4130859375
Iteration 19400: Loss = -12383.4130859375
Iteration 19500: Loss = -12383.4130859375
Iteration 19600: Loss = -12383.4130859375
Iteration 19700: Loss = -12383.4130859375
Iteration 19800: Loss = -12383.4111328125
Iteration 19900: Loss = -12383.4130859375
1
Iteration 20000: Loss = -12383.4130859375
2
Iteration 20100: Loss = -12383.4130859375
3
Iteration 20200: Loss = -12383.4130859375
4
Iteration 20300: Loss = -12383.412109375
5
Iteration 20400: Loss = -12383.4130859375
6
Iteration 20500: Loss = -12383.4140625
7
Iteration 20600: Loss = -12383.412109375
8
Iteration 20700: Loss = -12383.4140625
9
Iteration 20800: Loss = -12383.4091796875
Iteration 20900: Loss = -12383.41015625
1
Iteration 21000: Loss = -12383.3994140625
Iteration 21100: Loss = -12383.353515625
Iteration 21200: Loss = -12383.220703125
Iteration 21300: Loss = -12382.9365234375
Iteration 21400: Loss = -12382.7822265625
Iteration 21500: Loss = -12382.703125
Iteration 21600: Loss = -12382.556640625
Iteration 21700: Loss = -12382.49609375
Iteration 21800: Loss = -12382.4013671875
Iteration 21900: Loss = -12382.357421875
Iteration 22000: Loss = -12382.3427734375
Iteration 22100: Loss = -12382.3359375
Iteration 22200: Loss = -12382.3369140625
1
Iteration 22300: Loss = -12382.3349609375
Iteration 22400: Loss = -12382.259765625
Iteration 22500: Loss = -12382.2587890625
Iteration 22600: Loss = -12382.2587890625
Iteration 22700: Loss = -12382.2587890625
Iteration 22800: Loss = -12382.2578125
Iteration 22900: Loss = -12382.26171875
1
Iteration 23000: Loss = -12382.2587890625
2
Iteration 23100: Loss = -12382.2587890625
3
Iteration 23200: Loss = -12382.2578125
Iteration 23300: Loss = -12382.2587890625
1
Iteration 23400: Loss = -12382.2607421875
2
Iteration 23500: Loss = -12382.259765625
3
Iteration 23600: Loss = -12382.2587890625
4
Iteration 23700: Loss = -12382.2587890625
5
Iteration 23800: Loss = -12382.259765625
6
Iteration 23900: Loss = -12382.259765625
7
Iteration 24000: Loss = -12382.2607421875
8
Iteration 24100: Loss = -12382.2607421875
9
Iteration 24200: Loss = -12382.259765625
10
Iteration 24300: Loss = -12382.2587890625
11
Iteration 24400: Loss = -12382.2587890625
12
Iteration 24500: Loss = -12382.2587890625
13
Iteration 24600: Loss = -12382.259765625
14
Iteration 24700: Loss = -12382.2587890625
15
Stopping early at iteration 24700 due to no improvement.
pi: tensor([[3.5554e-05, 9.9996e-01],
        [4.5393e-02, 9.5461e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1616, 0.8384], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2586, 0.2260],
         [0.9885, 0.1950]],

        [[0.4855, 0.2272],
         [0.1486, 0.9891]],

        [[0.6938, 0.1640],
         [0.4093, 0.9298]],

        [[0.5624, 0.2807],
         [0.8093, 0.8104]],

        [[0.9845, 0.2871],
         [0.9823, 0.8341]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0007766707522784365
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: -9.47328211527134e-05
Average Adjusted Rand Index: -0.0009049356661350077
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -38214.03515625
Iteration 100: Loss = -22633.0859375
Iteration 200: Loss = -14778.6787109375
Iteration 300: Loss = -13086.3916015625
Iteration 400: Loss = -12781.1435546875
Iteration 500: Loss = -12665.9833984375
Iteration 600: Loss = -12601.330078125
Iteration 700: Loss = -12551.3349609375
Iteration 800: Loss = -12521.7587890625
Iteration 900: Loss = -12501.94921875
Iteration 1000: Loss = -12484.4423828125
Iteration 1100: Loss = -12467.2724609375
Iteration 1200: Loss = -12457.6455078125
Iteration 1300: Loss = -12444.4541015625
Iteration 1400: Loss = -12437.6689453125
Iteration 1500: Loss = -12431.2138671875
Iteration 1600: Loss = -12426.662109375
Iteration 1700: Loss = -12423.2919921875
Iteration 1800: Loss = -12418.9462890625
Iteration 1900: Loss = -12415.5166015625
Iteration 2000: Loss = -12413.5185546875
Iteration 2100: Loss = -12411.7412109375
Iteration 2200: Loss = -12410.001953125
Iteration 2300: Loss = -12408.39453125
Iteration 2400: Loss = -12407.0908203125
Iteration 2500: Loss = -12406.0341796875
Iteration 2600: Loss = -12405.14453125
Iteration 2700: Loss = -12404.375
Iteration 2800: Loss = -12403.6953125
Iteration 2900: Loss = -12403.09375
Iteration 3000: Loss = -12402.548828125
Iteration 3100: Loss = -12402.056640625
Iteration 3200: Loss = -12401.6123046875
Iteration 3300: Loss = -12401.2080078125
Iteration 3400: Loss = -12400.8349609375
Iteration 3500: Loss = -12400.4951171875
Iteration 3600: Loss = -12400.1796875
Iteration 3700: Loss = -12399.8828125
Iteration 3800: Loss = -12397.1279296875
Iteration 3900: Loss = -12395.98828125
Iteration 4000: Loss = -12395.6328125
Iteration 4100: Loss = -12395.3642578125
Iteration 4200: Loss = -12395.1298828125
Iteration 4300: Loss = -12394.919921875
Iteration 4400: Loss = -12394.73046875
Iteration 4500: Loss = -12394.5537109375
Iteration 4600: Loss = -12394.384765625
Iteration 4700: Loss = -12389.44140625
Iteration 4800: Loss = -12389.03515625
Iteration 4900: Loss = -12388.8017578125
Iteration 5000: Loss = -12388.6240234375
Iteration 5100: Loss = -12388.4697265625
Iteration 5200: Loss = -12388.330078125
Iteration 5300: Loss = -12388.20703125
Iteration 5400: Loss = -12388.08984375
Iteration 5500: Loss = -12387.9853515625
Iteration 5600: Loss = -12387.8857421875
Iteration 5700: Loss = -12387.794921875
Iteration 5800: Loss = -12387.708984375
Iteration 5900: Loss = -12387.62890625
Iteration 6000: Loss = -12387.5546875
Iteration 6100: Loss = -12387.4853515625
Iteration 6200: Loss = -12387.4169921875
Iteration 6300: Loss = -12387.35546875
Iteration 6400: Loss = -12387.296875
Iteration 6500: Loss = -12387.2421875
Iteration 6600: Loss = -12387.189453125
Iteration 6700: Loss = -12387.1396484375
Iteration 6800: Loss = -12387.0947265625
Iteration 6900: Loss = -12387.0517578125
Iteration 7000: Loss = -12387.0107421875
Iteration 7100: Loss = -12386.970703125
Iteration 7200: Loss = -12386.935546875
Iteration 7300: Loss = -12386.900390625
Iteration 7400: Loss = -12386.8671875
Iteration 7500: Loss = -12386.8369140625
Iteration 7600: Loss = -12386.806640625
Iteration 7700: Loss = -12386.779296875
Iteration 7800: Loss = -12386.75390625
Iteration 7900: Loss = -12386.73046875
Iteration 8000: Loss = -12386.7041015625
Iteration 8100: Loss = -12386.685546875
Iteration 8200: Loss = -12386.6630859375
Iteration 8300: Loss = -12386.64453125
Iteration 8400: Loss = -12386.625
Iteration 8500: Loss = -12386.6083984375
Iteration 8600: Loss = -12386.5927734375
Iteration 8700: Loss = -12386.5751953125
Iteration 8800: Loss = -12386.5625
Iteration 8900: Loss = -12386.546875
Iteration 9000: Loss = -12386.5341796875
Iteration 9100: Loss = -12386.521484375
Iteration 9200: Loss = -12386.5107421875
Iteration 9300: Loss = -12386.4990234375
Iteration 9400: Loss = -12386.4873046875
Iteration 9500: Loss = -12386.478515625
Iteration 9600: Loss = -12386.46875
Iteration 9700: Loss = -12386.4599609375
Iteration 9800: Loss = -12386.451171875
Iteration 9900: Loss = -12386.4453125
Iteration 10000: Loss = -12386.4375
Iteration 10100: Loss = -12386.4306640625
Iteration 10200: Loss = -12386.423828125
Iteration 10300: Loss = -12386.41796875
Iteration 10400: Loss = -12386.412109375
Iteration 10500: Loss = -12386.4072265625
Iteration 10600: Loss = -12386.40234375
Iteration 10700: Loss = -12386.3984375
Iteration 10800: Loss = -12386.392578125
Iteration 10900: Loss = -12386.3876953125
Iteration 11000: Loss = -12386.384765625
Iteration 11100: Loss = -12386.37890625
Iteration 11200: Loss = -12386.375
Iteration 11300: Loss = -12386.37109375
Iteration 11400: Loss = -12386.3662109375
Iteration 11500: Loss = -12386.3623046875
Iteration 11600: Loss = -12386.3583984375
Iteration 11700: Loss = -12386.353515625
Iteration 11800: Loss = -12386.3466796875
Iteration 11900: Loss = -12386.33203125
Iteration 12000: Loss = -12386.28515625
Iteration 12100: Loss = -12386.2294921875
Iteration 12200: Loss = -12386.2021484375
Iteration 12300: Loss = -12386.177734375
Iteration 12400: Loss = -12386.0703125
Iteration 12500: Loss = -12386.015625
Iteration 12600: Loss = -12385.498046875
Iteration 12700: Loss = -12385.2978515625
Iteration 12800: Loss = -12385.1640625
Iteration 12900: Loss = -12385.044921875
Iteration 13000: Loss = -12384.9375
Iteration 13100: Loss = -12384.8212890625
Iteration 13200: Loss = -12384.619140625
Iteration 13300: Loss = -12384.5458984375
Iteration 13400: Loss = -12384.4873046875
Iteration 13500: Loss = -12384.431640625
Iteration 13600: Loss = -12384.3701171875
Iteration 13700: Loss = -12384.310546875
Iteration 13800: Loss = -12384.20703125
Iteration 13900: Loss = -12384.0791015625
Iteration 14000: Loss = -12383.8994140625
Iteration 14100: Loss = -12383.7099609375
Iteration 14200: Loss = -12383.5751953125
Iteration 14300: Loss = -12383.46484375
Iteration 14400: Loss = -12383.3095703125
Iteration 14500: Loss = -12383.2109375
Iteration 14600: Loss = -12382.9248046875
Iteration 14700: Loss = -12382.8994140625
Iteration 14800: Loss = -12382.8564453125
Iteration 14900: Loss = -12382.7939453125
Iteration 15000: Loss = -12382.775390625
Iteration 15100: Loss = -12382.759765625
Iteration 15200: Loss = -12382.748046875
Iteration 15300: Loss = -12382.736328125
Iteration 15400: Loss = -12382.7265625
Iteration 15500: Loss = -12382.71875
Iteration 15600: Loss = -12382.7109375
Iteration 15700: Loss = -12382.7060546875
Iteration 15800: Loss = -12382.701171875
Iteration 15900: Loss = -12382.6962890625
Iteration 16000: Loss = -12382.689453125
Iteration 16100: Loss = -12382.6865234375
Iteration 16200: Loss = -12382.681640625
Iteration 16300: Loss = -12382.677734375
Iteration 16400: Loss = -12382.6748046875
Iteration 16500: Loss = -12382.6728515625
Iteration 16600: Loss = -12382.6708984375
Iteration 16700: Loss = -12382.669921875
Iteration 16800: Loss = -12382.6689453125
Iteration 16900: Loss = -12382.6640625
Iteration 17000: Loss = -12382.623046875
Iteration 17100: Loss = -12382.6201171875
Iteration 17200: Loss = -12382.5947265625
Iteration 17300: Loss = -12382.5908203125
Iteration 17400: Loss = -12382.5732421875
Iteration 17500: Loss = -12382.458984375
Iteration 17600: Loss = -12382.408203125
Iteration 17700: Loss = -12382.1474609375
Iteration 17800: Loss = -12382.130859375
Iteration 17900: Loss = -12382.130859375
Iteration 18000: Loss = -12382.1298828125
Iteration 18100: Loss = -12382.130859375
1
Iteration 18200: Loss = -12382.12890625
Iteration 18300: Loss = -12382.1298828125
1
Iteration 18400: Loss = -12382.1279296875
Iteration 18500: Loss = -12382.1279296875
Iteration 18600: Loss = -12382.1298828125
1
Iteration 18700: Loss = -12382.1279296875
Iteration 18800: Loss = -12382.1279296875
Iteration 18900: Loss = -12382.12890625
1
Iteration 19000: Loss = -12382.1279296875
Iteration 19100: Loss = -12382.1279296875
Iteration 19200: Loss = -12382.12890625
1
Iteration 19300: Loss = -12382.1279296875
Iteration 19400: Loss = -12382.1279296875
Iteration 19500: Loss = -12382.078125
Iteration 19600: Loss = -12382.078125
Iteration 19700: Loss = -12382.078125
Iteration 19800: Loss = -12382.0263671875
Iteration 19900: Loss = -12381.9697265625
Iteration 20000: Loss = -12381.9697265625
Iteration 20100: Loss = -12381.966796875
Iteration 20200: Loss = -12381.966796875
Iteration 20300: Loss = -12381.8037109375
Iteration 20400: Loss = -12381.78125
Iteration 20500: Loss = -12381.78125
Iteration 20600: Loss = -12381.78125
Iteration 20700: Loss = -12381.7802734375
Iteration 20800: Loss = -12381.779296875
Iteration 20900: Loss = -12381.7724609375
Iteration 21000: Loss = -12381.7724609375
Iteration 21100: Loss = -12381.7724609375
Iteration 21200: Loss = -12381.7724609375
Iteration 21300: Loss = -12381.771484375
Iteration 21400: Loss = -12381.7724609375
1
Iteration 21500: Loss = -12381.771484375
Iteration 21600: Loss = -12381.7724609375
1
Iteration 21700: Loss = -12381.7724609375
2
Iteration 21800: Loss = -12381.7724609375
3
Iteration 21900: Loss = -12381.771484375
Iteration 22000: Loss = -12381.771484375
Iteration 22100: Loss = -12381.771484375
Iteration 22200: Loss = -12381.7734375
1
Iteration 22300: Loss = -12381.7734375
2
Iteration 22400: Loss = -12381.771484375
Iteration 22500: Loss = -12381.771484375
Iteration 22600: Loss = -12381.771484375
Iteration 22700: Loss = -12381.771484375
Iteration 22800: Loss = -12381.771484375
Iteration 22900: Loss = -12381.7734375
1
Iteration 23000: Loss = -12381.7724609375
2
Iteration 23100: Loss = -12381.7724609375
3
Iteration 23200: Loss = -12381.7734375
4
Iteration 23300: Loss = -12381.7724609375
5
Iteration 23400: Loss = -12381.7734375
6
Iteration 23500: Loss = -12381.7724609375
7
Iteration 23600: Loss = -12381.7724609375
8
Iteration 23700: Loss = -12381.7724609375
9
Iteration 23800: Loss = -12381.7724609375
10
Iteration 23900: Loss = -12381.7724609375
11
Iteration 24000: Loss = -12381.7724609375
12
Iteration 24100: Loss = -12381.771484375
Iteration 24200: Loss = -12381.7734375
1
Iteration 24300: Loss = -12381.771484375
Iteration 24400: Loss = -12381.7724609375
1
Iteration 24500: Loss = -12381.771484375
Iteration 24600: Loss = -12381.7734375
1
Iteration 24700: Loss = -12381.7724609375
2
Iteration 24800: Loss = -12381.771484375
Iteration 24900: Loss = -12381.7724609375
1
Iteration 25000: Loss = -12381.7724609375
2
Iteration 25100: Loss = -12381.7724609375
3
Iteration 25200: Loss = -12381.7705078125
Iteration 25300: Loss = -12381.7724609375
1
Iteration 25400: Loss = -12381.7744140625
2
Iteration 25500: Loss = -12381.771484375
3
Iteration 25600: Loss = -12381.771484375
4
Iteration 25700: Loss = -12381.7724609375
5
Iteration 25800: Loss = -12381.7724609375
6
Iteration 25900: Loss = -12381.7724609375
7
Iteration 26000: Loss = -12381.7724609375
8
Iteration 26100: Loss = -12381.7734375
9
Iteration 26200: Loss = -12381.771484375
10
Iteration 26300: Loss = -12381.7734375
11
Iteration 26400: Loss = -12381.7705078125
Iteration 26500: Loss = -12381.7724609375
1
Iteration 26600: Loss = -12381.771484375
2
Iteration 26700: Loss = -12381.7724609375
3
Iteration 26800: Loss = -12381.7724609375
4
Iteration 26900: Loss = -12381.771484375
5
Iteration 27000: Loss = -12381.771484375
6
Iteration 27100: Loss = -12381.7734375
7
Iteration 27200: Loss = -12381.7724609375
8
Iteration 27300: Loss = -12381.7734375
9
Iteration 27400: Loss = -12381.7724609375
10
Iteration 27500: Loss = -12381.7724609375
11
Iteration 27600: Loss = -12381.771484375
12
Iteration 27700: Loss = -12381.771484375
13
Iteration 27800: Loss = -12381.7724609375
14
Iteration 27900: Loss = -12381.771484375
15
Stopping early at iteration 27900 due to no improvement.
pi: tensor([[9.6448e-01, 3.5518e-02],
        [9.9964e-01, 3.5757e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4848, 0.5152], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1967, 0.2056],
         [0.2844, 0.2160]],

        [[0.9498, 0.1083],
         [0.2528, 0.0404]],

        [[0.7378, 0.1571],
         [0.0194, 0.9362]],

        [[0.4529, 0.2946],
         [0.5060, 0.6808]],

        [[0.9918, 0.2945],
         [0.9707, 0.6073]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.0025993316004456
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: -0.0005739679241052849
Average Adjusted Rand Index: -0.0012710541731068344
[-9.47328211527134e-05, -0.0005739679241052849] [-0.0009049356661350077, -0.0012710541731068344] [12382.2587890625, 12381.771484375]
-------------------------------------
This iteration is 42
True Objective function: Loss = -11853.776973073027
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25418.052734375
Iteration 100: Loss = -16138.0576171875
Iteration 200: Loss = -13100.216796875
Iteration 300: Loss = -12606.658203125
Iteration 400: Loss = -12511.9921875
Iteration 500: Loss = -12471.1708984375
Iteration 600: Loss = -12450.130859375
Iteration 700: Loss = -12433.3984375
Iteration 800: Loss = -12423.6953125
Iteration 900: Loss = -12419.236328125
Iteration 1000: Loss = -12416.302734375
Iteration 1100: Loss = -12414.09375
Iteration 1200: Loss = -12412.3525390625
Iteration 1300: Loss = -12410.9638671875
Iteration 1400: Loss = -12409.8349609375
Iteration 1500: Loss = -12408.90234375
Iteration 1600: Loss = -12408.1201171875
Iteration 1700: Loss = -12407.4541015625
Iteration 1800: Loss = -12406.8837890625
Iteration 1900: Loss = -12406.3896484375
Iteration 2000: Loss = -12405.9580078125
Iteration 2100: Loss = -12405.580078125
Iteration 2200: Loss = -12405.2451171875
Iteration 2300: Loss = -12404.9482421875
Iteration 2400: Loss = -12404.6806640625
Iteration 2500: Loss = -12404.44140625
Iteration 2600: Loss = -12404.228515625
Iteration 2700: Loss = -12404.0341796875
Iteration 2800: Loss = -12403.861328125
Iteration 2900: Loss = -12403.701171875
Iteration 3000: Loss = -12403.5556640625
Iteration 3100: Loss = -12403.423828125
Iteration 3200: Loss = -12403.3017578125
Iteration 3300: Loss = -12403.189453125
Iteration 3400: Loss = -12403.0869140625
Iteration 3500: Loss = -12402.9931640625
Iteration 3600: Loss = -12402.904296875
Iteration 3700: Loss = -12402.822265625
Iteration 3800: Loss = -12402.7470703125
Iteration 3900: Loss = -12402.67578125
Iteration 4000: Loss = -12402.611328125
Iteration 4100: Loss = -12402.548828125
Iteration 4200: Loss = -12402.4931640625
Iteration 4300: Loss = -12402.4384765625
Iteration 4400: Loss = -12402.3896484375
Iteration 4500: Loss = -12402.341796875
Iteration 4600: Loss = -12402.2998046875
Iteration 4700: Loss = -12402.2578125
Iteration 4800: Loss = -12402.216796875
Iteration 4900: Loss = -12402.1826171875
Iteration 5000: Loss = -12402.1474609375
Iteration 5100: Loss = -12402.1142578125
Iteration 5200: Loss = -12402.0849609375
Iteration 5300: Loss = -12402.0556640625
Iteration 5400: Loss = -12402.02734375
Iteration 5500: Loss = -12402.00390625
Iteration 5600: Loss = -12401.978515625
Iteration 5700: Loss = -12401.9541015625
Iteration 5800: Loss = -12401.9326171875
Iteration 5900: Loss = -12401.912109375
Iteration 6000: Loss = -12401.8916015625
Iteration 6100: Loss = -12401.8740234375
Iteration 6200: Loss = -12401.85546875
Iteration 6300: Loss = -12401.83984375
Iteration 6400: Loss = -12401.82421875
Iteration 6500: Loss = -12401.806640625
Iteration 6600: Loss = -12401.79296875
Iteration 6700: Loss = -12401.78125
Iteration 6800: Loss = -12401.767578125
Iteration 6900: Loss = -12401.7548828125
Iteration 7000: Loss = -12401.7412109375
Iteration 7100: Loss = -12401.73046875
Iteration 7200: Loss = -12401.7197265625
Iteration 7300: Loss = -12401.7109375
Iteration 7400: Loss = -12401.69921875
Iteration 7500: Loss = -12401.6904296875
Iteration 7600: Loss = -12401.681640625
Iteration 7700: Loss = -12401.6708984375
Iteration 7800: Loss = -12401.6591796875
Iteration 7900: Loss = -12401.6416015625
Iteration 8000: Loss = -12401.5703125
Iteration 8100: Loss = -12401.4521484375
Iteration 8200: Loss = -12401.4208984375
Iteration 8300: Loss = -12401.4052734375
Iteration 8400: Loss = -12401.392578125
Iteration 8500: Loss = -12401.3857421875
Iteration 8600: Loss = -12401.37890625
Iteration 8700: Loss = -12401.3740234375
Iteration 8800: Loss = -12401.3701171875
Iteration 8900: Loss = -12401.36328125
Iteration 9000: Loss = -12401.3603515625
Iteration 9100: Loss = -12401.3564453125
Iteration 9200: Loss = -12401.3544921875
Iteration 9300: Loss = -12401.3515625
Iteration 9400: Loss = -12401.3486328125
Iteration 9500: Loss = -12401.34765625
Iteration 9600: Loss = -12401.345703125
Iteration 9700: Loss = -12401.341796875
Iteration 9800: Loss = -12401.341796875
Iteration 9900: Loss = -12401.337890625
Iteration 10000: Loss = -12401.3359375
Iteration 10100: Loss = -12401.3349609375
Iteration 10200: Loss = -12401.333984375
Iteration 10300: Loss = -12401.3330078125
Iteration 10400: Loss = -12401.3310546875
Iteration 10500: Loss = -12401.3310546875
Iteration 10600: Loss = -12401.3310546875
Iteration 10700: Loss = -12401.328125
Iteration 10800: Loss = -12401.328125
Iteration 10900: Loss = -12401.3271484375
Iteration 11000: Loss = -12401.326171875
Iteration 11100: Loss = -12401.326171875
Iteration 11200: Loss = -12401.3251953125
Iteration 11300: Loss = -12401.3251953125
Iteration 11400: Loss = -12401.3251953125
Iteration 11500: Loss = -12401.3232421875
Iteration 11600: Loss = -12401.3232421875
Iteration 11700: Loss = -12401.3232421875
Iteration 11800: Loss = -12401.322265625
Iteration 11900: Loss = -12401.322265625
Iteration 12000: Loss = -12401.322265625
Iteration 12100: Loss = -12401.322265625
Iteration 12200: Loss = -12401.3203125
Iteration 12300: Loss = -12401.3203125
Iteration 12400: Loss = -12401.3212890625
1
Iteration 12500: Loss = -12401.3193359375
Iteration 12600: Loss = -12401.3232421875
1
Iteration 12700: Loss = -12401.3203125
2
Iteration 12800: Loss = -12401.3193359375
Iteration 12900: Loss = -12401.3193359375
Iteration 13000: Loss = -12401.318359375
Iteration 13100: Loss = -12401.3203125
1
Iteration 13200: Loss = -12401.3193359375
2
Iteration 13300: Loss = -12401.318359375
Iteration 13400: Loss = -12401.318359375
Iteration 13500: Loss = -12401.3193359375
1
Iteration 13600: Loss = -12401.3203125
2
Iteration 13700: Loss = -12401.318359375
Iteration 13800: Loss = -12401.318359375
Iteration 13900: Loss = -12401.31640625
Iteration 14000: Loss = -12401.3173828125
1
Iteration 14100: Loss = -12401.3173828125
2
Iteration 14200: Loss = -12401.31640625
Iteration 14300: Loss = -12401.3154296875
Iteration 14400: Loss = -12401.3134765625
Iteration 14500: Loss = -12401.3115234375
Iteration 14600: Loss = -12401.306640625
Iteration 14700: Loss = -12401.283203125
Iteration 14800: Loss = -12401.228515625
Iteration 14900: Loss = -12401.1806640625
Iteration 15000: Loss = -12401.13671875
Iteration 15100: Loss = -12401.037109375
Iteration 15200: Loss = -12400.77734375
Iteration 15300: Loss = -12400.748046875
Iteration 15400: Loss = -12400.7412109375
Iteration 15500: Loss = -12400.740234375
Iteration 15600: Loss = -12400.73828125
Iteration 15700: Loss = -12400.7373046875
Iteration 15800: Loss = -12400.7373046875
Iteration 15900: Loss = -12400.736328125
Iteration 16000: Loss = -12400.736328125
Iteration 16100: Loss = -12400.73828125
1
Iteration 16200: Loss = -12400.7373046875
2
Iteration 16300: Loss = -12400.736328125
Iteration 16400: Loss = -12400.7373046875
1
Iteration 16500: Loss = -12400.736328125
Iteration 16600: Loss = -12400.736328125
Iteration 16700: Loss = -12400.736328125
Iteration 16800: Loss = -12400.73828125
1
Iteration 16900: Loss = -12400.7353515625
Iteration 17000: Loss = -12400.7353515625
Iteration 17100: Loss = -12400.7353515625
Iteration 17200: Loss = -12400.736328125
1
Iteration 17300: Loss = -12400.736328125
2
Iteration 17400: Loss = -12400.7353515625
Iteration 17500: Loss = -12400.736328125
1
Iteration 17600: Loss = -12400.7373046875
2
Iteration 17700: Loss = -12400.736328125
3
Iteration 17800: Loss = -12400.73828125
4
Iteration 17900: Loss = -12400.7373046875
5
Iteration 18000: Loss = -12400.7373046875
6
Iteration 18100: Loss = -12400.7353515625
Iteration 18200: Loss = -12400.7353515625
Iteration 18300: Loss = -12400.7373046875
1
Iteration 18400: Loss = -12400.736328125
2
Iteration 18500: Loss = -12400.7353515625
Iteration 18600: Loss = -12400.7353515625
Iteration 18700: Loss = -12400.736328125
1
Iteration 18800: Loss = -12400.7353515625
Iteration 18900: Loss = -12400.736328125
1
Iteration 19000: Loss = -12400.7353515625
Iteration 19100: Loss = -12400.7353515625
Iteration 19200: Loss = -12400.736328125
1
Iteration 19300: Loss = -12400.7353515625
Iteration 19400: Loss = -12400.7353515625
Iteration 19500: Loss = -12400.7353515625
Iteration 19600: Loss = -12400.73828125
1
Iteration 19700: Loss = -12400.7373046875
2
Iteration 19800: Loss = -12400.736328125
3
Iteration 19900: Loss = -12400.7353515625
Iteration 20000: Loss = -12400.7373046875
1
Iteration 20100: Loss = -12400.736328125
2
Iteration 20200: Loss = -12400.7373046875
3
Iteration 20300: Loss = -12400.7353515625
Iteration 20400: Loss = -12400.7373046875
1
Iteration 20500: Loss = -12400.7373046875
2
Iteration 20600: Loss = -12400.7373046875
3
Iteration 20700: Loss = -12400.736328125
4
Iteration 20800: Loss = -12400.736328125
5
Iteration 20900: Loss = -12400.7373046875
6
Iteration 21000: Loss = -12400.736328125
7
Iteration 21100: Loss = -12400.7373046875
8
Iteration 21200: Loss = -12400.7373046875
9
Iteration 21300: Loss = -12400.736328125
10
Iteration 21400: Loss = -12400.736328125
11
Iteration 21500: Loss = -12400.7373046875
12
Iteration 21600: Loss = -12400.740234375
13
Iteration 21700: Loss = -12400.7373046875
14
Iteration 21800: Loss = -12400.736328125
15
Stopping early at iteration 21800 due to no improvement.
pi: tensor([[0.9691, 0.0309],
        [0.9271, 0.0729]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9926, 0.0074], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2012, 0.3013],
         [0.9929, 0.2069]],

        [[0.2323, 0.2028],
         [0.2221, 0.6330]],

        [[0.3504, 0.1179],
         [0.8376, 0.7588]],

        [[0.9381, 0.1942],
         [0.2376, 0.9726]],

        [[0.9210, 0.2153],
         [0.9602, 0.0271]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.01575757575757576
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0001884009551677587
Average Adjusted Rand Index: 0.0031515151515151517
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -45656.34765625
Iteration 100: Loss = -25637.005859375
Iteration 200: Loss = -15804.0283203125
Iteration 300: Loss = -13473.6826171875
Iteration 400: Loss = -12899.0771484375
Iteration 500: Loss = -12691.0703125
Iteration 600: Loss = -12604.1552734375
Iteration 700: Loss = -12554.525390625
Iteration 800: Loss = -12514.826171875
Iteration 900: Loss = -12491.7470703125
Iteration 1000: Loss = -12475.8369140625
Iteration 1100: Loss = -12463.9150390625
Iteration 1200: Loss = -12454.6689453125
Iteration 1300: Loss = -12447.326171875
Iteration 1400: Loss = -12441.380859375
Iteration 1500: Loss = -12436.4951171875
Iteration 1600: Loss = -12432.4248046875
Iteration 1700: Loss = -12428.9951171875
Iteration 1800: Loss = -12426.0732421875
Iteration 1900: Loss = -12423.5595703125
Iteration 2000: Loss = -12421.380859375
Iteration 2100: Loss = -12419.4921875
Iteration 2200: Loss = -12417.8408203125
Iteration 2300: Loss = -12416.3896484375
Iteration 2400: Loss = -12415.1044921875
Iteration 2500: Loss = -12413.9638671875
Iteration 2600: Loss = -12412.9462890625
Iteration 2700: Loss = -12412.0322265625
Iteration 2800: Loss = -12411.2177734375
Iteration 2900: Loss = -12410.4794921875
Iteration 3000: Loss = -12409.8134765625
Iteration 3100: Loss = -12409.208984375
Iteration 3200: Loss = -12408.6611328125
Iteration 3300: Loss = -12408.162109375
Iteration 3400: Loss = -12407.70703125
Iteration 3500: Loss = -12407.2900390625
Iteration 3600: Loss = -12406.908203125
Iteration 3700: Loss = -12406.556640625
Iteration 3800: Loss = -12406.2294921875
Iteration 3900: Loss = -12405.9326171875
Iteration 4000: Loss = -12405.65625
Iteration 4100: Loss = -12405.3974609375
Iteration 4200: Loss = -12405.1611328125
Iteration 4300: Loss = -12404.9404296875
Iteration 4400: Loss = -12404.734375
Iteration 4500: Loss = -12404.54296875
Iteration 4600: Loss = -12404.3642578125
Iteration 4700: Loss = -12404.1982421875
Iteration 4800: Loss = -12404.0400390625
Iteration 4900: Loss = -12403.8955078125
Iteration 5000: Loss = -12403.7578125
Iteration 5100: Loss = -12403.6298828125
Iteration 5200: Loss = -12403.509765625
Iteration 5300: Loss = -12403.396484375
Iteration 5400: Loss = -12403.2900390625
Iteration 5500: Loss = -12403.193359375
Iteration 5600: Loss = -12403.0966796875
Iteration 5700: Loss = -12403.009765625
Iteration 5800: Loss = -12402.9267578125
Iteration 5900: Loss = -12402.84765625
Iteration 6000: Loss = -12402.775390625
Iteration 6100: Loss = -12402.705078125
Iteration 6200: Loss = -12402.6416015625
Iteration 6300: Loss = -12402.5791015625
Iteration 6400: Loss = -12402.5224609375
Iteration 6500: Loss = -12402.466796875
Iteration 6600: Loss = -12402.4150390625
Iteration 6700: Loss = -12402.3662109375
Iteration 6800: Loss = -12402.3203125
Iteration 6900: Loss = -12402.2763671875
Iteration 7000: Loss = -12402.236328125
Iteration 7100: Loss = -12402.1982421875
Iteration 7200: Loss = -12402.1611328125
Iteration 7300: Loss = -12402.1259765625
Iteration 7400: Loss = -12402.0947265625
Iteration 7500: Loss = -12402.0634765625
Iteration 7600: Loss = -12402.03515625
Iteration 7700: Loss = -12402.0087890625
Iteration 7800: Loss = -12401.98046875
Iteration 7900: Loss = -12401.9580078125
Iteration 8000: Loss = -12401.9345703125
Iteration 8100: Loss = -12401.9130859375
Iteration 8200: Loss = -12401.892578125
Iteration 8300: Loss = -12401.873046875
Iteration 8400: Loss = -12401.8544921875
Iteration 8500: Loss = -12401.837890625
Iteration 8600: Loss = -12401.8212890625
Iteration 8700: Loss = -12401.8046875
Iteration 8800: Loss = -12401.7919921875
Iteration 8900: Loss = -12401.7763671875
Iteration 9000: Loss = -12401.765625
Iteration 9100: Loss = -12401.7529296875
Iteration 9200: Loss = -12401.7412109375
Iteration 9300: Loss = -12401.7294921875
Iteration 9400: Loss = -12401.720703125
Iteration 9500: Loss = -12401.7099609375
Iteration 9600: Loss = -12401.701171875
Iteration 9700: Loss = -12401.6904296875
Iteration 9800: Loss = -12401.681640625
Iteration 9900: Loss = -12401.671875
Iteration 10000: Loss = -12401.6640625
Iteration 10100: Loss = -12401.6572265625
Iteration 10200: Loss = -12401.6494140625
Iteration 10300: Loss = -12401.6416015625
Iteration 10400: Loss = -12401.6376953125
Iteration 10500: Loss = -12401.6328125
Iteration 10600: Loss = -12401.6298828125
Iteration 10700: Loss = -12401.6259765625
Iteration 10800: Loss = -12401.6240234375
Iteration 10900: Loss = -12401.6201171875
Iteration 11000: Loss = -12401.6181640625
Iteration 11100: Loss = -12401.615234375
Iteration 11200: Loss = -12401.611328125
Iteration 11300: Loss = -12401.609375
Iteration 11400: Loss = -12401.6044921875
Iteration 11500: Loss = -12401.6015625
Iteration 11600: Loss = -12401.5966796875
Iteration 11700: Loss = -12401.5908203125
Iteration 11800: Loss = -12401.5859375
Iteration 11900: Loss = -12401.5732421875
Iteration 12000: Loss = -12401.5576171875
Iteration 12100: Loss = -12401.5341796875
Iteration 12200: Loss = -12401.5078125
Iteration 12300: Loss = -12401.4931640625
Iteration 12400: Loss = -12401.4794921875
Iteration 12500: Loss = -12401.4765625
Iteration 12600: Loss = -12401.4716796875
Iteration 12700: Loss = -12401.466796875
Iteration 12800: Loss = -12401.4609375
Iteration 12900: Loss = -12401.4541015625
Iteration 13000: Loss = -12401.44921875
Iteration 13100: Loss = -12401.4443359375
Iteration 13200: Loss = -12401.44140625
Iteration 13300: Loss = -12401.4384765625
Iteration 13400: Loss = -12401.435546875
Iteration 13500: Loss = -12401.4306640625
Iteration 13600: Loss = -12401.4296875
Iteration 13700: Loss = -12401.42578125
Iteration 13800: Loss = -12401.4228515625
Iteration 13900: Loss = -12401.421875
Iteration 14000: Loss = -12401.421875
Iteration 14100: Loss = -12401.4189453125
Iteration 14200: Loss = -12401.4189453125
Iteration 14300: Loss = -12401.4150390625
Iteration 14400: Loss = -12401.4169921875
1
Iteration 14500: Loss = -12401.4130859375
Iteration 14600: Loss = -12401.4091796875
Iteration 14700: Loss = -12401.40625
Iteration 14800: Loss = -12401.4052734375
Iteration 14900: Loss = -12401.4013671875
Iteration 15000: Loss = -12401.3984375
Iteration 15100: Loss = -12401.3935546875
Iteration 15200: Loss = -12401.388671875
Iteration 15300: Loss = -12401.3828125
Iteration 15400: Loss = -12401.3779296875
Iteration 15500: Loss = -12401.3720703125
Iteration 15600: Loss = -12401.3662109375
Iteration 15700: Loss = -12401.359375
Iteration 15800: Loss = -12401.353515625
Iteration 15900: Loss = -12401.3466796875
Iteration 16000: Loss = -12401.337890625
Iteration 16100: Loss = -12401.328125
Iteration 16200: Loss = -12401.318359375
Iteration 16300: Loss = -12401.3056640625
Iteration 16400: Loss = -12401.29296875
Iteration 16500: Loss = -12401.2783203125
Iteration 16600: Loss = -12401.259765625
Iteration 16700: Loss = -12401.240234375
Iteration 16800: Loss = -12401.2119140625
Iteration 16900: Loss = -12401.1640625
Iteration 17000: Loss = -12401.0205078125
Iteration 17100: Loss = -12400.791015625
Iteration 17200: Loss = -12400.7705078125
Iteration 17300: Loss = -12400.7587890625
Iteration 17400: Loss = -12400.751953125
Iteration 17500: Loss = -12400.748046875
Iteration 17600: Loss = -12400.74609375
Iteration 17700: Loss = -12400.7451171875
Iteration 17800: Loss = -12400.744140625
Iteration 17900: Loss = -12400.744140625
Iteration 18000: Loss = -12400.744140625
Iteration 18100: Loss = -12400.7451171875
1
Iteration 18200: Loss = -12400.744140625
Iteration 18300: Loss = -12400.74609375
1
Iteration 18400: Loss = -12400.7431640625
Iteration 18500: Loss = -12400.7431640625
Iteration 18600: Loss = -12400.7431640625
Iteration 18700: Loss = -12400.7431640625
Iteration 18800: Loss = -12400.7421875
Iteration 18900: Loss = -12400.744140625
1
Iteration 19000: Loss = -12400.7431640625
2
Iteration 19100: Loss = -12400.7431640625
3
Iteration 19200: Loss = -12400.7431640625
4
Iteration 19300: Loss = -12400.7421875
Iteration 19400: Loss = -12400.7431640625
1
Iteration 19500: Loss = -12400.7451171875
2
Iteration 19600: Loss = -12400.744140625
3
Iteration 19700: Loss = -12400.7431640625
4
Iteration 19800: Loss = -12400.744140625
5
Iteration 19900: Loss = -12400.7431640625
6
Iteration 20000: Loss = -12400.7431640625
7
Iteration 20100: Loss = -12400.7431640625
8
Iteration 20200: Loss = -12400.7421875
Iteration 20300: Loss = -12400.744140625
1
Iteration 20400: Loss = -12400.7421875
Iteration 20500: Loss = -12400.7431640625
1
Iteration 20600: Loss = -12400.7431640625
2
Iteration 20700: Loss = -12400.744140625
3
Iteration 20800: Loss = -12400.7421875
Iteration 20900: Loss = -12400.7431640625
1
Iteration 21000: Loss = -12400.7421875
Iteration 21100: Loss = -12400.7431640625
1
Iteration 21200: Loss = -12400.7431640625
2
Iteration 21300: Loss = -12400.7421875
Iteration 21400: Loss = -12400.744140625
1
Iteration 21500: Loss = -12400.7431640625
2
Iteration 21600: Loss = -12400.7421875
Iteration 21700: Loss = -12400.7431640625
1
Iteration 21800: Loss = -12400.7421875
Iteration 21900: Loss = -12400.7431640625
1
Iteration 22000: Loss = -12400.744140625
2
Iteration 22100: Loss = -12400.7412109375
Iteration 22200: Loss = -12400.7431640625
1
Iteration 22300: Loss = -12400.7431640625
2
Iteration 22400: Loss = -12400.744140625
3
Iteration 22500: Loss = -12400.7431640625
4
Iteration 22600: Loss = -12400.7431640625
5
Iteration 22700: Loss = -12400.7431640625
6
Iteration 22800: Loss = -12400.7421875
7
Iteration 22900: Loss = -12400.744140625
8
Iteration 23000: Loss = -12400.7421875
9
Iteration 23100: Loss = -12400.7431640625
10
Iteration 23200: Loss = -12400.744140625
11
Iteration 23300: Loss = -12400.744140625
12
Iteration 23400: Loss = -12400.7421875
13
Iteration 23500: Loss = -12400.7431640625
14
Iteration 23600: Loss = -12400.7431640625
15
Stopping early at iteration 23600 due to no improvement.
pi: tensor([[0.0733, 0.9267],
        [0.0309, 0.9691]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0070, 0.9930], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2065, 0.3038],
         [0.0759, 0.2011]],

        [[0.0076, 0.2028],
         [0.1647, 0.9498]],

        [[0.9788, 0.1180],
         [0.6152, 0.2267]],

        [[0.9661, 0.1943],
         [0.6924, 0.9259]],

        [[0.9798, 0.2154],
         [0.0257, 0.0124]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.01575757575757576
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0001884009551677587
Average Adjusted Rand Index: 0.0031515151515151517
[-0.0001884009551677587, -0.0001884009551677587] [0.0031515151515151517, 0.0031515151515151517] [12400.736328125, 12400.7431640625]
-------------------------------------
This iteration is 43
True Objective function: Loss = -11790.315223201182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27174.87890625
Iteration 100: Loss = -18768.23046875
Iteration 200: Loss = -14110.5283203125
Iteration 300: Loss = -13026.669921875
Iteration 400: Loss = -12715.9306640625
Iteration 500: Loss = -12588.7138671875
Iteration 600: Loss = -12521.3349609375
Iteration 700: Loss = -12482.8828125
Iteration 800: Loss = -12454.404296875
Iteration 900: Loss = -12437.640625
Iteration 1000: Loss = -12423.9599609375
Iteration 1100: Loss = -12412.740234375
Iteration 1200: Loss = -12403.4775390625
Iteration 1300: Loss = -12395.04296875
Iteration 1400: Loss = -12388.0419921875
Iteration 1500: Loss = -12381.435546875
Iteration 1600: Loss = -12376.0595703125
Iteration 1700: Loss = -12371.1083984375
Iteration 1800: Loss = -12366.4033203125
Iteration 1900: Loss = -12359.7490234375
Iteration 2000: Loss = -12354.0537109375
Iteration 2100: Loss = -12346.9951171875
Iteration 2200: Loss = -12340.8330078125
Iteration 2300: Loss = -12335.794921875
Iteration 2400: Loss = -12330.740234375
Iteration 2500: Loss = -12327.2109375
Iteration 2600: Loss = -12323.478515625
Iteration 2700: Loss = -12318.6943359375
Iteration 2800: Loss = -12316.6884765625
Iteration 2900: Loss = -12314.349609375
Iteration 3000: Loss = -12311.4033203125
Iteration 3100: Loss = -12310.2509765625
Iteration 3200: Loss = -12309.552734375
Iteration 3300: Loss = -12309.0634765625
Iteration 3400: Loss = -12308.69140625
Iteration 3500: Loss = -12308.390625
Iteration 3600: Loss = -12308.14453125
Iteration 3700: Loss = -12307.9365234375
Iteration 3800: Loss = -12307.7607421875
Iteration 3900: Loss = -12307.609375
Iteration 4000: Loss = -12307.4765625
Iteration 4100: Loss = -12307.3623046875
Iteration 4200: Loss = -12307.259765625
Iteration 4300: Loss = -12307.1708984375
Iteration 4400: Loss = -12307.0908203125
Iteration 4500: Loss = -12307.017578125
Iteration 4600: Loss = -12306.94921875
Iteration 4700: Loss = -12306.8759765625
Iteration 4800: Loss = -12304.541015625
Iteration 4900: Loss = -12303.001953125
Iteration 5000: Loss = -12302.818359375
Iteration 5100: Loss = -12302.7109375
Iteration 5200: Loss = -12302.6357421875
Iteration 5300: Loss = -12302.578125
Iteration 5400: Loss = -12302.529296875
Iteration 5500: Loss = -12302.4912109375
Iteration 5600: Loss = -12302.4541015625
Iteration 5700: Loss = -12302.423828125
Iteration 5800: Loss = -12302.39453125
Iteration 5900: Loss = -12302.369140625
Iteration 6000: Loss = -12302.3466796875
Iteration 6100: Loss = -12302.3251953125
Iteration 6200: Loss = -12302.3056640625
Iteration 6300: Loss = -12302.2880859375
Iteration 6400: Loss = -12302.2744140625
Iteration 6500: Loss = -12302.2578125
Iteration 6600: Loss = -12302.244140625
Iteration 6700: Loss = -12302.228515625
Iteration 6800: Loss = -12302.2177734375
Iteration 6900: Loss = -12302.20703125
Iteration 7000: Loss = -12302.1953125
Iteration 7100: Loss = -12302.1845703125
Iteration 7200: Loss = -12302.1728515625
Iteration 7300: Loss = -12302.0986328125
Iteration 7400: Loss = -12297.5791015625
Iteration 7500: Loss = -12297.4169921875
Iteration 7600: Loss = -12297.3466796875
Iteration 7700: Loss = -12297.302734375
Iteration 7800: Loss = -12297.2734375
Iteration 7900: Loss = -12297.25
Iteration 8000: Loss = -12297.23046875
Iteration 8100: Loss = -12297.2158203125
Iteration 8200: Loss = -12297.203125
Iteration 8300: Loss = -12297.19140625
Iteration 8400: Loss = -12297.1806640625
Iteration 8500: Loss = -12297.1728515625
Iteration 8600: Loss = -12297.1650390625
Iteration 8700: Loss = -12297.1572265625
Iteration 8800: Loss = -12297.150390625
Iteration 8900: Loss = -12297.1435546875
Iteration 9000: Loss = -12297.1376953125
Iteration 9100: Loss = -12297.130859375
Iteration 9200: Loss = -12297.1259765625
Iteration 9300: Loss = -12297.1181640625
Iteration 9400: Loss = -12297.1142578125
Iteration 9500: Loss = -12297.109375
Iteration 9600: Loss = -12297.10546875
Iteration 9700: Loss = -12297.1025390625
Iteration 9800: Loss = -12297.099609375
Iteration 9900: Loss = -12297.0947265625
Iteration 10000: Loss = -12297.091796875
Iteration 10100: Loss = -12297.0908203125
Iteration 10200: Loss = -12297.087890625
Iteration 10300: Loss = -12297.0859375
Iteration 10400: Loss = -12297.0830078125
Iteration 10500: Loss = -12297.080078125
Iteration 10600: Loss = -12297.078125
Iteration 10700: Loss = -12297.0791015625
1
Iteration 10800: Loss = -12297.076171875
Iteration 10900: Loss = -12297.0732421875
Iteration 11000: Loss = -12297.072265625
Iteration 11100: Loss = -12297.0712890625
Iteration 11200: Loss = -12297.0693359375
Iteration 11300: Loss = -12297.068359375
Iteration 11400: Loss = -12297.0673828125
Iteration 11500: Loss = -12297.0654296875
Iteration 11600: Loss = -12297.064453125
Iteration 11700: Loss = -12297.0634765625
Iteration 11800: Loss = -12297.0634765625
Iteration 11900: Loss = -12297.0615234375
Iteration 12000: Loss = -12297.060546875
Iteration 12100: Loss = -12297.0595703125
Iteration 12200: Loss = -12297.05859375
Iteration 12300: Loss = -12297.0576171875
Iteration 12400: Loss = -12297.056640625
Iteration 12500: Loss = -12297.056640625
Iteration 12600: Loss = -12297.0546875
Iteration 12700: Loss = -12297.0517578125
Iteration 12800: Loss = -12297.048828125
Iteration 12900: Loss = -12297.0322265625
Iteration 13000: Loss = -12297.0244140625
Iteration 13100: Loss = -12297.0224609375
Iteration 13200: Loss = -12297.0244140625
1
Iteration 13300: Loss = -12297.021484375
Iteration 13400: Loss = -12297.021484375
Iteration 13500: Loss = -12297.0205078125
Iteration 13600: Loss = -12297.01953125
Iteration 13700: Loss = -12297.01953125
Iteration 13800: Loss = -12297.01953125
Iteration 13900: Loss = -12297.0185546875
Iteration 14000: Loss = -12297.017578125
Iteration 14100: Loss = -12297.017578125
Iteration 14200: Loss = -12297.0166015625
Iteration 14300: Loss = -12297.017578125
1
Iteration 14400: Loss = -12297.015625
Iteration 14500: Loss = -12297.013671875
Iteration 14600: Loss = -12297.0146484375
1
Iteration 14700: Loss = -12297.0146484375
2
Iteration 14800: Loss = -12297.015625
3
Iteration 14900: Loss = -12297.013671875
Iteration 15000: Loss = -12297.0146484375
1
Iteration 15100: Loss = -12297.013671875
Iteration 15200: Loss = -12297.013671875
Iteration 15300: Loss = -12297.013671875
Iteration 15400: Loss = -12297.013671875
Iteration 15500: Loss = -12297.0126953125
Iteration 15600: Loss = -12297.0146484375
1
Iteration 15700: Loss = -12297.0126953125
Iteration 15800: Loss = -12297.013671875
1
Iteration 15900: Loss = -12297.01171875
Iteration 16000: Loss = -12297.013671875
1
Iteration 16100: Loss = -12297.013671875
2
Iteration 16200: Loss = -12297.01171875
Iteration 16300: Loss = -12297.01171875
Iteration 16400: Loss = -12297.0185546875
1
Iteration 16500: Loss = -12297.0126953125
2
Iteration 16600: Loss = -12297.0126953125
3
Iteration 16700: Loss = -12297.0126953125
4
Iteration 16800: Loss = -12297.0126953125
5
Iteration 16900: Loss = -12297.0126953125
6
Iteration 17000: Loss = -12297.0126953125
7
Iteration 17100: Loss = -12297.0107421875
Iteration 17200: Loss = -12297.01171875
1
Iteration 17300: Loss = -12297.01171875
2
Iteration 17400: Loss = -12297.0107421875
Iteration 17500: Loss = -12297.0107421875
Iteration 17600: Loss = -12297.009765625
Iteration 17700: Loss = -12296.9853515625
Iteration 17800: Loss = -12296.9853515625
Iteration 17900: Loss = -12296.986328125
1
Iteration 18000: Loss = -12296.9833984375
Iteration 18100: Loss = -12296.984375
1
Iteration 18200: Loss = -12296.984375
2
Iteration 18300: Loss = -12296.984375
3
Iteration 18400: Loss = -12296.986328125
4
Iteration 18500: Loss = -12296.984375
5
Iteration 18600: Loss = -12296.9833984375
Iteration 18700: Loss = -12296.9833984375
Iteration 18800: Loss = -12296.982421875
Iteration 18900: Loss = -12296.9853515625
1
Iteration 19000: Loss = -12296.984375
2
Iteration 19100: Loss = -12296.984375
3
Iteration 19200: Loss = -12296.984375
4
Iteration 19300: Loss = -12296.984375
5
Iteration 19400: Loss = -12296.984375
6
Iteration 19500: Loss = -12296.984375
7
Iteration 19600: Loss = -12296.9833984375
8
Iteration 19700: Loss = -12296.9833984375
9
Iteration 19800: Loss = -12296.9833984375
10
Iteration 19900: Loss = -12296.9833984375
11
Iteration 20000: Loss = -12296.9833984375
12
Iteration 20100: Loss = -12296.9833984375
13
Iteration 20200: Loss = -12296.9853515625
14
Iteration 20300: Loss = -12296.9833984375
15
Stopping early at iteration 20300 due to no improvement.
pi: tensor([[1.0000e+00, 3.4153e-06],
        [9.9588e-01, 4.1192e-03]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0164, 0.9836], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1980, 0.2551],
         [0.9915, 0.1942]],

        [[0.0150, 0.1101],
         [0.9858, 0.9562]],

        [[0.9557, 0.2437],
         [0.0882, 0.7832]],

        [[0.0759, 0.2173],
         [0.3940, 0.1420]],

        [[0.0648, 0.8270],
         [0.8555, 0.9655]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0012739458395153792
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -63987.24609375
Iteration 100: Loss = -47003.64453125
Iteration 200: Loss = -31526.21875
Iteration 300: Loss = -20626.60546875
Iteration 400: Loss = -15511.048828125
Iteration 500: Loss = -13510.4404296875
Iteration 600: Loss = -12787.154296875
Iteration 700: Loss = -12520.8935546875
Iteration 800: Loss = -12424.8837890625
Iteration 900: Loss = -12383.5068359375
Iteration 1000: Loss = -12365.859375
Iteration 1100: Loss = -12342.7021484375
Iteration 1200: Loss = -12329.9892578125
Iteration 1300: Loss = -12321.3720703125
Iteration 1400: Loss = -12315.7578125
Iteration 1500: Loss = -12311.2646484375
Iteration 1600: Loss = -12307.6533203125
Iteration 1700: Loss = -12303.5791015625
Iteration 1800: Loss = -12300.8916015625
Iteration 1900: Loss = -12297.48828125
Iteration 2000: Loss = -12295.3466796875
Iteration 2100: Loss = -12294.3466796875
Iteration 2200: Loss = -12293.576171875
Iteration 2300: Loss = -12292.923828125
Iteration 2400: Loss = -12292.3359375
Iteration 2500: Loss = -12291.7578125
Iteration 2600: Loss = -12291.12890625
Iteration 2700: Loss = -12290.3857421875
Iteration 2800: Loss = -12289.5224609375
Iteration 2900: Loss = -12288.5244140625
Iteration 3000: Loss = -12287.037109375
Iteration 3100: Loss = -12285.01171875
Iteration 3200: Loss = -12283.6484375
Iteration 3300: Loss = -12282.63671875
Iteration 3400: Loss = -12281.8330078125
Iteration 3500: Loss = -12281.12109375
Iteration 3600: Loss = -12280.3115234375
Iteration 3700: Loss = -12278.9921875
Iteration 3800: Loss = -12276.03125
Iteration 3900: Loss = -12273.896484375
Iteration 4000: Loss = -12272.60546875
Iteration 4100: Loss = -12269.1416015625
Iteration 4200: Loss = -12267.8623046875
Iteration 4300: Loss = -12266.3515625
Iteration 4400: Loss = -12263.7607421875
Iteration 4500: Loss = -12262.6005859375
Iteration 4600: Loss = -12261.103515625
Iteration 4700: Loss = -12255.4970703125
Iteration 4800: Loss = -12249.474609375
Iteration 4900: Loss = -12243.1962890625
Iteration 5000: Loss = -12238.982421875
Iteration 5100: Loss = -12235.7607421875
Iteration 5200: Loss = -12229.189453125
Iteration 5300: Loss = -12209.236328125
Iteration 5400: Loss = -12192.3681640625
Iteration 5500: Loss = -12187.7744140625
Iteration 5600: Loss = -12176.6650390625
Iteration 5700: Loss = -12170.3798828125
Iteration 5800: Loss = -12166.3466796875
Iteration 5900: Loss = -12163.076171875
Iteration 6000: Loss = -12158.5
Iteration 6100: Loss = -12155.88671875
Iteration 6200: Loss = -12152.2724609375
Iteration 6300: Loss = -12150.1142578125
Iteration 6400: Loss = -12149.849609375
Iteration 6500: Loss = -12149.73828125
Iteration 6600: Loss = -12149.6650390625
Iteration 6700: Loss = -12149.611328125
Iteration 6800: Loss = -12149.5673828125
Iteration 6900: Loss = -12149.53125
Iteration 7000: Loss = -12149.5
Iteration 7100: Loss = -12149.470703125
Iteration 7200: Loss = -12149.4462890625
Iteration 7300: Loss = -12149.421875
Iteration 7400: Loss = -12149.40234375
Iteration 7500: Loss = -12149.3837890625
Iteration 7600: Loss = -12149.3681640625
Iteration 7700: Loss = -12149.353515625
Iteration 7800: Loss = -12149.33984375
Iteration 7900: Loss = -12149.3271484375
Iteration 8000: Loss = -12149.3154296875
Iteration 8100: Loss = -12149.28125
Iteration 8200: Loss = -12147.564453125
Iteration 8300: Loss = -12147.5341796875
Iteration 8400: Loss = -12147.5224609375
Iteration 8500: Loss = -12147.51171875
Iteration 8600: Loss = -12147.50390625
Iteration 8700: Loss = -12147.4970703125
Iteration 8800: Loss = -12147.4892578125
Iteration 8900: Loss = -12147.484375
Iteration 9000: Loss = -12147.478515625
Iteration 9100: Loss = -12147.4736328125
Iteration 9200: Loss = -12147.4697265625
Iteration 9300: Loss = -12147.46484375
Iteration 9400: Loss = -12147.4599609375
Iteration 9500: Loss = -12147.4580078125
Iteration 9600: Loss = -12147.453125
Iteration 9700: Loss = -12147.4501953125
Iteration 9800: Loss = -12147.4462890625
Iteration 9900: Loss = -12147.4443359375
Iteration 10000: Loss = -12147.4423828125
Iteration 10100: Loss = -12147.439453125
Iteration 10200: Loss = -12147.4375
Iteration 10300: Loss = -12147.435546875
Iteration 10400: Loss = -12147.4326171875
Iteration 10500: Loss = -12147.431640625
Iteration 10600: Loss = -12147.427734375
Iteration 10700: Loss = -12147.427734375
Iteration 10800: Loss = -12147.4248046875
Iteration 10900: Loss = -12147.4248046875
Iteration 11000: Loss = -12147.4208984375
Iteration 11100: Loss = -12147.419921875
Iteration 11200: Loss = -12147.3916015625
Iteration 11300: Loss = -12147.3623046875
Iteration 11400: Loss = -12147.361328125
Iteration 11500: Loss = -12147.3603515625
Iteration 11600: Loss = -12147.359375
Iteration 11700: Loss = -12147.3583984375
Iteration 11800: Loss = -12147.357421875
Iteration 11900: Loss = -12147.3564453125
Iteration 12000: Loss = -12147.3544921875
Iteration 12100: Loss = -12147.35546875
1
Iteration 12200: Loss = -12147.3544921875
Iteration 12300: Loss = -12147.3525390625
Iteration 12400: Loss = -12147.353515625
1
Iteration 12500: Loss = -12147.3525390625
Iteration 12600: Loss = -12147.3515625
Iteration 12700: Loss = -12147.3515625
Iteration 12800: Loss = -12147.3505859375
Iteration 12900: Loss = -12147.349609375
Iteration 13000: Loss = -12147.349609375
Iteration 13100: Loss = -12147.349609375
Iteration 13200: Loss = -12147.3486328125
Iteration 13300: Loss = -12147.34765625
Iteration 13400: Loss = -12147.34765625
Iteration 13500: Loss = -12147.3486328125
1
Iteration 13600: Loss = -12147.3466796875
Iteration 13700: Loss = -12147.3466796875
Iteration 13800: Loss = -12147.345703125
Iteration 13900: Loss = -12147.3466796875
1
Iteration 14000: Loss = -12147.33203125
Iteration 14100: Loss = -12147.3310546875
Iteration 14200: Loss = -12147.3310546875
Iteration 14300: Loss = -12147.33203125
1
Iteration 14400: Loss = -12147.3310546875
Iteration 14500: Loss = -12147.3310546875
Iteration 14600: Loss = -12147.3291015625
Iteration 14700: Loss = -12147.33203125
1
Iteration 14800: Loss = -12147.330078125
2
Iteration 14900: Loss = -12147.330078125
3
Iteration 15000: Loss = -12147.330078125
4
Iteration 15100: Loss = -12147.3291015625
Iteration 15200: Loss = -12147.3310546875
1
Iteration 15300: Loss = -12147.330078125
2
Iteration 15400: Loss = -12147.330078125
3
Iteration 15500: Loss = -12147.3291015625
Iteration 15600: Loss = -12147.330078125
1
Iteration 15700: Loss = -12147.330078125
2
Iteration 15800: Loss = -12147.3291015625
Iteration 15900: Loss = -12147.3291015625
Iteration 16000: Loss = -12147.3291015625
Iteration 16100: Loss = -12147.3291015625
Iteration 16200: Loss = -12147.330078125
1
Iteration 16300: Loss = -12147.3291015625
Iteration 16400: Loss = -12147.328125
Iteration 16500: Loss = -12147.3291015625
1
Iteration 16600: Loss = -12147.3291015625
2
Iteration 16700: Loss = -12147.326171875
Iteration 16800: Loss = -12147.3271484375
1
Iteration 16900: Loss = -12147.326171875
Iteration 17000: Loss = -12147.3251953125
Iteration 17100: Loss = -12147.32421875
Iteration 17200: Loss = -12147.32421875
Iteration 17300: Loss = -12147.32421875
Iteration 17400: Loss = -12147.3232421875
Iteration 17500: Loss = -12147.3232421875
Iteration 17600: Loss = -12147.32421875
1
Iteration 17700: Loss = -12147.322265625
Iteration 17800: Loss = -12147.3232421875
1
Iteration 17900: Loss = -12147.3232421875
2
Iteration 18000: Loss = -12147.3232421875
3
Iteration 18100: Loss = -12147.3203125
Iteration 18200: Loss = -12147.3203125
Iteration 18300: Loss = -12147.3212890625
1
Iteration 18400: Loss = -12147.3203125
Iteration 18500: Loss = -12147.3203125
Iteration 18600: Loss = -12147.3203125
Iteration 18700: Loss = -12147.3212890625
1
Iteration 18800: Loss = -12147.3193359375
Iteration 18900: Loss = -12147.3203125
1
Iteration 19000: Loss = -12147.3193359375
Iteration 19100: Loss = -12147.3173828125
Iteration 19200: Loss = -12147.3173828125
Iteration 19300: Loss = -12147.3173828125
Iteration 19400: Loss = -12147.318359375
1
Iteration 19500: Loss = -12147.3154296875
Iteration 19600: Loss = -12147.3173828125
1
Iteration 19700: Loss = -12147.31640625
2
Iteration 19800: Loss = -12147.3173828125
3
Iteration 19900: Loss = -12147.3193359375
4
Iteration 20000: Loss = -12147.3173828125
5
Iteration 20100: Loss = -12147.314453125
Iteration 20200: Loss = -12147.3154296875
1
Iteration 20300: Loss = -12147.314453125
Iteration 20400: Loss = -12147.314453125
Iteration 20500: Loss = -12147.314453125
Iteration 20600: Loss = -12147.314453125
Iteration 20700: Loss = -12147.3134765625
Iteration 20800: Loss = -12147.3134765625
Iteration 20900: Loss = -12147.314453125
1
Iteration 21000: Loss = -12147.3154296875
2
Iteration 21100: Loss = -12147.314453125
3
Iteration 21200: Loss = -12147.314453125
4
Iteration 21300: Loss = -12147.314453125
5
Iteration 21400: Loss = -12147.3154296875
6
Iteration 21500: Loss = -12147.314453125
7
Iteration 21600: Loss = -12147.314453125
8
Iteration 21700: Loss = -12147.314453125
9
Iteration 21800: Loss = -12147.3154296875
10
Iteration 21900: Loss = -12147.314453125
11
Iteration 22000: Loss = -12147.314453125
12
Iteration 22100: Loss = -12147.3154296875
13
Iteration 22200: Loss = -12147.314453125
14
Iteration 22300: Loss = -12147.314453125
15
Stopping early at iteration 22300 due to no improvement.
pi: tensor([[0.5105, 0.4895],
        [0.0717, 0.9283]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5026, 0.4974], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3046, 0.0960],
         [0.8769, 0.2222]],

        [[0.8808, 0.0998],
         [0.1532, 0.8059]],

        [[0.0073, 0.1167],
         [0.6143, 0.2813]],

        [[0.9561, 0.9422],
         [0.0818, 0.5989]],

        [[0.0080, 0.7961],
         [0.0070, 0.6511]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448543354594036
time is 2
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 6
Adjusted Rand Index: 0.7721116882917585
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.3169806485729374
Average Adjusted Rand Index: 0.5233932047502324
[-0.0012739458395153792, 0.3169806485729374] [0.0, 0.5233932047502324] [12296.9833984375, 12147.314453125]
-------------------------------------
This iteration is 44
True Objective function: Loss = -11834.095229828867
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30025.5234375
Iteration 100: Loss = -19419.6796875
Iteration 200: Loss = -13973.4970703125
Iteration 300: Loss = -12833.0546875
Iteration 400: Loss = -12640.3505859375
Iteration 500: Loss = -12559.5908203125
Iteration 600: Loss = -12516.462890625
Iteration 700: Loss = -12488.859375
Iteration 800: Loss = -12466.3544921875
Iteration 900: Loss = -12454.2841796875
Iteration 1000: Loss = -12440.763671875
Iteration 1100: Loss = -12429.5390625
Iteration 1200: Loss = -12423.16015625
Iteration 1300: Loss = -12418.0166015625
Iteration 1400: Loss = -12413.3056640625
Iteration 1500: Loss = -12408.666015625
Iteration 1600: Loss = -12403.904296875
Iteration 1700: Loss = -12400.169921875
Iteration 1800: Loss = -12396.9267578125
Iteration 1900: Loss = -12393.0224609375
Iteration 2000: Loss = -12390.62109375
Iteration 2100: Loss = -12389.0458984375
Iteration 2200: Loss = -12387.83203125
Iteration 2300: Loss = -12386.8349609375
Iteration 2400: Loss = -12386.0009765625
Iteration 2500: Loss = -12385.2880859375
Iteration 2600: Loss = -12384.669921875
Iteration 2700: Loss = -12384.126953125
Iteration 2800: Loss = -12383.6484375
Iteration 2900: Loss = -12383.220703125
Iteration 3000: Loss = -12382.8349609375
Iteration 3100: Loss = -12382.4873046875
Iteration 3200: Loss = -12382.171875
Iteration 3300: Loss = -12381.8837890625
Iteration 3400: Loss = -12381.623046875
Iteration 3500: Loss = -12381.3828125
Iteration 3600: Loss = -12381.162109375
Iteration 3700: Loss = -12380.958984375
Iteration 3800: Loss = -12380.771484375
Iteration 3900: Loss = -12380.599609375
Iteration 4000: Loss = -12380.44140625
Iteration 4100: Loss = -12380.2919921875
Iteration 4200: Loss = -12380.1533203125
Iteration 4300: Loss = -12380.02734375
Iteration 4400: Loss = -12379.9072265625
Iteration 4500: Loss = -12379.796875
Iteration 4600: Loss = -12379.6943359375
Iteration 4700: Loss = -12379.5966796875
Iteration 4800: Loss = -12379.505859375
Iteration 4900: Loss = -12379.421875
Iteration 5000: Loss = -12379.3427734375
Iteration 5100: Loss = -12379.267578125
Iteration 5200: Loss = -12379.197265625
Iteration 5300: Loss = -12379.1201171875
Iteration 5400: Loss = -12379.05859375
Iteration 5500: Loss = -12379.001953125
Iteration 5600: Loss = -12378.9482421875
Iteration 5700: Loss = -12378.8974609375
Iteration 5800: Loss = -12378.84765625
Iteration 5900: Loss = -12378.8056640625
Iteration 6000: Loss = -12378.7626953125
Iteration 6100: Loss = -12378.72265625
Iteration 6200: Loss = -12378.68359375
Iteration 6300: Loss = -12378.6484375
Iteration 6400: Loss = -12378.615234375
Iteration 6500: Loss = -12378.58203125
Iteration 6600: Loss = -12378.5537109375
Iteration 6700: Loss = -12378.525390625
Iteration 6800: Loss = -12378.498046875
Iteration 6900: Loss = -12378.4716796875
Iteration 7000: Loss = -12378.44921875
Iteration 7100: Loss = -12378.4267578125
Iteration 7200: Loss = -12378.4052734375
Iteration 7300: Loss = -12378.384765625
Iteration 7400: Loss = -12378.3642578125
Iteration 7500: Loss = -12378.34765625
Iteration 7600: Loss = -12378.328125
Iteration 7700: Loss = -12378.3134765625
Iteration 7800: Loss = -12378.2998046875
Iteration 7900: Loss = -12378.2841796875
Iteration 8000: Loss = -12378.26953125
Iteration 8100: Loss = -12378.2568359375
Iteration 8200: Loss = -12378.2431640625
Iteration 8300: Loss = -12378.2333984375
Iteration 8400: Loss = -12378.2236328125
Iteration 8500: Loss = -12378.2109375
Iteration 8600: Loss = -12378.201171875
Iteration 8700: Loss = -12378.19140625
Iteration 8800: Loss = -12378.1826171875
Iteration 8900: Loss = -12378.1728515625
Iteration 9000: Loss = -12378.166015625
Iteration 9100: Loss = -12378.1591796875
Iteration 9200: Loss = -12378.1494140625
Iteration 9300: Loss = -12378.14453125
Iteration 9400: Loss = -12378.138671875
Iteration 9500: Loss = -12378.130859375
Iteration 9600: Loss = -12378.1259765625
Iteration 9700: Loss = -12378.12109375
Iteration 9800: Loss = -12378.115234375
Iteration 9900: Loss = -12378.109375
Iteration 10000: Loss = -12378.1044921875
Iteration 10100: Loss = -12378.1005859375
Iteration 10200: Loss = -12378.0966796875
Iteration 10300: Loss = -12378.0927734375
Iteration 10400: Loss = -12378.08984375
Iteration 10500: Loss = -12378.0859375
Iteration 10600: Loss = -12378.08203125
Iteration 10700: Loss = -12378.0791015625
Iteration 10800: Loss = -12378.0751953125
Iteration 10900: Loss = -12378.072265625
Iteration 11000: Loss = -12378.0693359375
Iteration 11100: Loss = -12378.0673828125
Iteration 11200: Loss = -12378.0654296875
Iteration 11300: Loss = -12378.0625
Iteration 11400: Loss = -12378.05859375
Iteration 11500: Loss = -12378.0576171875
Iteration 11600: Loss = -12378.05859375
1
Iteration 11700: Loss = -12378.0537109375
Iteration 11800: Loss = -12378.052734375
Iteration 11900: Loss = -12378.05078125
Iteration 12000: Loss = -12378.048828125
Iteration 12100: Loss = -12378.0478515625
Iteration 12200: Loss = -12378.0458984375
Iteration 12300: Loss = -12378.046875
1
Iteration 12400: Loss = -12378.04296875
Iteration 12500: Loss = -12378.0439453125
1
Iteration 12600: Loss = -12378.041015625
Iteration 12700: Loss = -12378.041015625
Iteration 12800: Loss = -12378.0400390625
Iteration 12900: Loss = -12378.0380859375
Iteration 13000: Loss = -12378.0361328125
Iteration 13100: Loss = -12378.0361328125
Iteration 13200: Loss = -12378.0341796875
Iteration 13300: Loss = -12378.0341796875
Iteration 13400: Loss = -12378.033203125
Iteration 13500: Loss = -12378.033203125
Iteration 13600: Loss = -12378.03125
Iteration 13700: Loss = -12378.033203125
1
Iteration 13800: Loss = -12378.0302734375
Iteration 13900: Loss = -12378.0302734375
Iteration 14000: Loss = -12378.03125
1
Iteration 14100: Loss = -12378.0283203125
Iteration 14200: Loss = -12378.0283203125
Iteration 14300: Loss = -12378.029296875
1
Iteration 14400: Loss = -12378.0283203125
Iteration 14500: Loss = -12378.0263671875
Iteration 14600: Loss = -12378.0283203125
1
Iteration 14700: Loss = -12378.0244140625
Iteration 14800: Loss = -12378.025390625
1
Iteration 14900: Loss = -12378.025390625
2
Iteration 15000: Loss = -12378.0234375
Iteration 15100: Loss = -12378.0244140625
1
Iteration 15200: Loss = -12378.0234375
Iteration 15300: Loss = -12378.0224609375
Iteration 15400: Loss = -12378.021484375
Iteration 15500: Loss = -12378.0185546875
Iteration 15600: Loss = -12378.0166015625
Iteration 15700: Loss = -12378.01171875
Iteration 15800: Loss = -12378.00390625
Iteration 15900: Loss = -12377.9833984375
Iteration 16000: Loss = -12377.9462890625
Iteration 16100: Loss = -12377.916015625
Iteration 16200: Loss = -12377.8994140625
Iteration 16300: Loss = -12377.892578125
Iteration 16400: Loss = -12377.88671875
Iteration 16500: Loss = -12377.884765625
Iteration 16600: Loss = -12377.8818359375
Iteration 16700: Loss = -12377.8798828125
Iteration 16800: Loss = -12377.87890625
Iteration 16900: Loss = -12377.876953125
Iteration 17000: Loss = -12377.87890625
1
Iteration 17100: Loss = -12377.875
Iteration 17200: Loss = -12377.8740234375
Iteration 17300: Loss = -12377.8740234375
Iteration 17400: Loss = -12377.875
1
Iteration 17500: Loss = -12377.873046875
Iteration 17600: Loss = -12377.873046875
Iteration 17700: Loss = -12377.873046875
Iteration 17800: Loss = -12377.873046875
Iteration 17900: Loss = -12377.8720703125
Iteration 18000: Loss = -12377.873046875
1
Iteration 18100: Loss = -12377.873046875
2
Iteration 18200: Loss = -12377.8720703125
Iteration 18300: Loss = -12377.87109375
Iteration 18400: Loss = -12377.8720703125
1
Iteration 18500: Loss = -12377.87109375
Iteration 18600: Loss = -12377.8720703125
1
Iteration 18700: Loss = -12377.8720703125
2
Iteration 18800: Loss = -12377.8720703125
3
Iteration 18900: Loss = -12377.87109375
Iteration 19000: Loss = -12377.8720703125
1
Iteration 19100: Loss = -12377.8701171875
Iteration 19200: Loss = -12377.8740234375
1
Iteration 19300: Loss = -12377.8720703125
2
Iteration 19400: Loss = -12377.8720703125
3
Iteration 19500: Loss = -12377.8701171875
Iteration 19600: Loss = -12377.87109375
1
Iteration 19700: Loss = -12377.8720703125
2
Iteration 19800: Loss = -12377.8701171875
Iteration 19900: Loss = -12377.87109375
1
Iteration 20000: Loss = -12377.87109375
2
Iteration 20100: Loss = -12377.8720703125
3
Iteration 20200: Loss = -12377.8701171875
Iteration 20300: Loss = -12377.87109375
1
Iteration 20400: Loss = -12377.8720703125
2
Iteration 20500: Loss = -12377.87109375
3
Iteration 20600: Loss = -12377.869140625
Iteration 20700: Loss = -12377.87109375
1
Iteration 20800: Loss = -12377.87109375
2
Iteration 20900: Loss = -12377.87109375
3
Iteration 21000: Loss = -12377.87109375
4
Iteration 21100: Loss = -12377.87109375
5
Iteration 21200: Loss = -12377.87109375
6
Iteration 21300: Loss = -12377.87109375
7
Iteration 21400: Loss = -12377.87109375
8
Iteration 21500: Loss = -12377.87109375
9
Iteration 21600: Loss = -12377.87109375
10
Iteration 21700: Loss = -12377.8701171875
11
Iteration 21800: Loss = -12377.873046875
12
Iteration 21900: Loss = -12377.8701171875
13
Iteration 22000: Loss = -12377.8701171875
14
Iteration 22100: Loss = -12377.8720703125
15
Stopping early at iteration 22100 due to no improvement.
pi: tensor([[9.9999e-01, 8.3361e-06],
        [1.0000e+00, 1.7341e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0421, 0.9579], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2005, 0.1979],
         [0.5266, 0.1969]],

        [[0.4124, 0.2199],
         [0.1420, 0.1228]],

        [[0.9759, 0.2279],
         [0.0830, 0.0522]],

        [[0.9460, 0.2133],
         [0.9486, 0.0161]],

        [[0.9179, 0.2030],
         [0.8596, 0.1906]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -3.82715035040419e-05
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25675.818359375
Iteration 100: Loss = -19270.55859375
Iteration 200: Loss = -14152.064453125
Iteration 300: Loss = -12831.51171875
Iteration 400: Loss = -12605.78125
Iteration 500: Loss = -12525.916015625
Iteration 600: Loss = -12488.3125
Iteration 700: Loss = -12458.3955078125
Iteration 800: Loss = -12440.638671875
Iteration 900: Loss = -12429.9345703125
Iteration 1000: Loss = -12421.798828125
Iteration 1100: Loss = -12413.3583984375
Iteration 1200: Loss = -12406.9697265625
Iteration 1300: Loss = -12401.0390625
Iteration 1400: Loss = -12380.4267578125
Iteration 1500: Loss = -12339.02734375
Iteration 1600: Loss = -12328.1845703125
Iteration 1700: Loss = -12323.9033203125
Iteration 1800: Loss = -12318.8603515625
Iteration 1900: Loss = -12315.0908203125
Iteration 2000: Loss = -12305.7958984375
Iteration 2100: Loss = -12290.703125
Iteration 2200: Loss = -12280.6396484375
Iteration 2300: Loss = -12265.021484375
Iteration 2400: Loss = -12232.2392578125
Iteration 2500: Loss = -12202.8232421875
Iteration 2600: Loss = -12201.30078125
Iteration 2700: Loss = -12200.4619140625
Iteration 2800: Loss = -12199.853515625
Iteration 2900: Loss = -12199.365234375
Iteration 3000: Loss = -12198.9501953125
Iteration 3100: Loss = -12198.5625
Iteration 3200: Loss = -12197.8232421875
Iteration 3300: Loss = -12197.35546875
Iteration 3400: Loss = -12197.0888671875
Iteration 3500: Loss = -12196.861328125
Iteration 3600: Loss = -12196.66015625
Iteration 3700: Loss = -12196.478515625
Iteration 3800: Loss = -12196.3134765625
Iteration 3900: Loss = -12196.1640625
Iteration 4000: Loss = -12196.0234375
Iteration 4100: Loss = -12195.8994140625
Iteration 4200: Loss = -12195.78125
Iteration 4300: Loss = -12195.6728515625
Iteration 4400: Loss = -12195.5732421875
Iteration 4500: Loss = -12195.48046875
Iteration 4600: Loss = -12195.3955078125
Iteration 4700: Loss = -12195.3134765625
Iteration 4800: Loss = -12195.240234375
Iteration 4900: Loss = -12195.1708984375
Iteration 5000: Loss = -12195.1044921875
Iteration 5100: Loss = -12195.0439453125
Iteration 5200: Loss = -12194.9873046875
Iteration 5300: Loss = -12194.935546875
Iteration 5400: Loss = -12194.8837890625
Iteration 5500: Loss = -12194.8369140625
Iteration 5600: Loss = -12194.79296875
Iteration 5700: Loss = -12194.7509765625
Iteration 5800: Loss = -12194.7119140625
Iteration 5900: Loss = -12194.671875
Iteration 6000: Loss = -12194.6337890625
Iteration 6100: Loss = -12194.59765625
Iteration 6200: Loss = -12194.5615234375
Iteration 6300: Loss = -12194.5263671875
Iteration 6400: Loss = -12194.4892578125
Iteration 6500: Loss = -12194.4443359375
Iteration 6600: Loss = -12194.3876953125
Iteration 6700: Loss = -12194.28515625
Iteration 6800: Loss = -12193.95703125
Iteration 6900: Loss = -12193.056640625
Iteration 7000: Loss = -12192.6240234375
Iteration 7100: Loss = -12192.5673828125
Iteration 7200: Loss = -12192.2333984375
Iteration 7300: Loss = -12191.9794921875
Iteration 7400: Loss = -12191.7890625
Iteration 7500: Loss = -12189.966796875
Iteration 7600: Loss = -12187.7734375
Iteration 7700: Loss = -12185.763671875
Iteration 7800: Loss = -12184.7529296875
Iteration 7900: Loss = -12182.5625
Iteration 8000: Loss = -12181.515625
Iteration 8100: Loss = -12179.9267578125
Iteration 8200: Loss = -12178.9306640625
Iteration 8300: Loss = -12177.1376953125
Iteration 8400: Loss = -12172.3056640625
Iteration 8500: Loss = -12170.2568359375
Iteration 8600: Loss = -12160.6669921875
Iteration 8700: Loss = -12147.5849609375
Iteration 8800: Loss = -12110.4599609375
Iteration 8900: Loss = -12096.06640625
Iteration 9000: Loss = -12078.0302734375
Iteration 9100: Loss = -12077.126953125
Iteration 9200: Loss = -12076.8251953125
Iteration 9300: Loss = -12063.6767578125
Iteration 9400: Loss = -12054.4326171875
Iteration 9500: Loss = -12054.060546875
Iteration 9600: Loss = -12053.9619140625
Iteration 9700: Loss = -12053.9052734375
Iteration 9800: Loss = -12053.8662109375
Iteration 9900: Loss = -12046.3291015625
Iteration 10000: Loss = -12045.1376953125
Iteration 10100: Loss = -12045.078125
Iteration 10200: Loss = -12045.0458984375
Iteration 10300: Loss = -12045.0234375
Iteration 10400: Loss = -12045.0029296875
Iteration 10500: Loss = -12044.990234375
Iteration 10600: Loss = -12044.978515625
Iteration 10700: Loss = -12044.96875
Iteration 10800: Loss = -12044.9599609375
Iteration 10900: Loss = -12044.951171875
Iteration 11000: Loss = -12044.943359375
Iteration 11100: Loss = -12044.9375
Iteration 11200: Loss = -12044.931640625
Iteration 11300: Loss = -12044.9267578125
Iteration 11400: Loss = -12044.861328125
Iteration 11500: Loss = -12040.955078125
Iteration 11600: Loss = -12040.939453125
Iteration 11700: Loss = -12040.9306640625
Iteration 11800: Loss = -12040.92578125
Iteration 11900: Loss = -12040.9248046875
Iteration 12000: Loss = -12040.919921875
Iteration 12100: Loss = -12040.916015625
Iteration 12200: Loss = -12040.9140625
Iteration 12300: Loss = -12040.91015625
Iteration 12400: Loss = -12040.908203125
Iteration 12500: Loss = -12040.9052734375
Iteration 12600: Loss = -12040.9052734375
Iteration 12700: Loss = -12040.9033203125
Iteration 12800: Loss = -12040.90234375
Iteration 12900: Loss = -12040.8994140625
Iteration 13000: Loss = -12040.8984375
Iteration 13100: Loss = -12040.8974609375
Iteration 13200: Loss = -12040.8974609375
Iteration 13300: Loss = -12040.8955078125
Iteration 13400: Loss = -12040.8935546875
Iteration 13500: Loss = -12040.892578125
Iteration 13600: Loss = -12040.8935546875
1
Iteration 13700: Loss = -12040.890625
Iteration 13800: Loss = -12040.890625
Iteration 13900: Loss = -12040.8896484375
Iteration 14000: Loss = -12040.888671875
Iteration 14100: Loss = -12040.8896484375
1
Iteration 14200: Loss = -12040.88671875
Iteration 14300: Loss = -12040.88671875
Iteration 14400: Loss = -12032.6240234375
Iteration 14500: Loss = -12032.42578125
Iteration 14600: Loss = -12032.400390625
Iteration 14700: Loss = -12032.3876953125
Iteration 14800: Loss = -12032.380859375
Iteration 14900: Loss = -12032.376953125
Iteration 15000: Loss = -12032.3740234375
Iteration 15100: Loss = -12032.3701171875
Iteration 15200: Loss = -12032.3681640625
Iteration 15300: Loss = -12032.3681640625
Iteration 15400: Loss = -12032.3662109375
Iteration 15500: Loss = -12032.3642578125
Iteration 15600: Loss = -12032.3642578125
Iteration 15700: Loss = -12032.3623046875
Iteration 15800: Loss = -12032.3623046875
Iteration 15900: Loss = -12032.3623046875
Iteration 16000: Loss = -12032.361328125
Iteration 16100: Loss = -12032.361328125
Iteration 16200: Loss = -12032.361328125
Iteration 16300: Loss = -12032.3603515625
Iteration 16400: Loss = -12032.3603515625
Iteration 16500: Loss = -12032.359375
Iteration 16600: Loss = -12032.3583984375
Iteration 16700: Loss = -12032.3583984375
Iteration 16800: Loss = -12032.359375
1
Iteration 16900: Loss = -12032.357421875
Iteration 17000: Loss = -12032.3583984375
1
Iteration 17100: Loss = -12032.35546875
Iteration 17200: Loss = -12032.3564453125
1
Iteration 17300: Loss = -12032.357421875
2
Iteration 17400: Loss = -12032.3583984375
3
Iteration 17500: Loss = -12032.3564453125
4
Iteration 17600: Loss = -12032.3564453125
5
Iteration 17700: Loss = -12032.3564453125
6
Iteration 17800: Loss = -12032.35546875
Iteration 17900: Loss = -12032.35546875
Iteration 18000: Loss = -12032.35546875
Iteration 18100: Loss = -12032.3544921875
Iteration 18200: Loss = -12032.3564453125
1
Iteration 18300: Loss = -12032.35546875
2
Iteration 18400: Loss = -12032.35546875
3
Iteration 18500: Loss = -12032.3544921875
Iteration 18600: Loss = -12032.35546875
1
Iteration 18700: Loss = -12032.353515625
Iteration 18800: Loss = -12032.3544921875
1
Iteration 18900: Loss = -12032.3544921875
2
Iteration 19000: Loss = -12032.35546875
3
Iteration 19100: Loss = -12032.353515625
Iteration 19200: Loss = -12032.3544921875
1
Iteration 19300: Loss = -12032.318359375
Iteration 19400: Loss = -12027.5615234375
Iteration 19500: Loss = -12027.5380859375
Iteration 19600: Loss = -12027.5322265625
Iteration 19700: Loss = -12027.529296875
Iteration 19800: Loss = -12027.52734375
Iteration 19900: Loss = -12027.52734375
Iteration 20000: Loss = -12027.52734375
Iteration 20100: Loss = -12027.52734375
Iteration 20200: Loss = -12027.5263671875
Iteration 20300: Loss = -12027.525390625
Iteration 20400: Loss = -12027.525390625
Iteration 20500: Loss = -12027.5263671875
1
Iteration 20600: Loss = -12027.525390625
Iteration 20700: Loss = -12027.525390625
Iteration 20800: Loss = -12027.525390625
Iteration 20900: Loss = -12027.525390625
Iteration 21000: Loss = -12027.525390625
Iteration 21100: Loss = -12027.525390625
Iteration 21200: Loss = -12027.5263671875
1
Iteration 21300: Loss = -12027.525390625
Iteration 21400: Loss = -12027.525390625
Iteration 21500: Loss = -12027.5263671875
1
Iteration 21600: Loss = -12027.525390625
Iteration 21700: Loss = -12027.525390625
Iteration 21800: Loss = -12027.525390625
Iteration 21900: Loss = -12027.5263671875
1
Iteration 22000: Loss = -12027.5263671875
2
Iteration 22100: Loss = -12027.525390625
Iteration 22200: Loss = -12027.525390625
Iteration 22300: Loss = -12027.52734375
1
Iteration 22400: Loss = -12027.5263671875
2
Iteration 22500: Loss = -12027.5244140625
Iteration 22600: Loss = -12027.525390625
1
Iteration 22700: Loss = -12027.5263671875
2
Iteration 22800: Loss = -12027.525390625
3
Iteration 22900: Loss = -12027.525390625
4
Iteration 23000: Loss = -12027.5263671875
5
Iteration 23100: Loss = -12027.525390625
6
Iteration 23200: Loss = -12027.5244140625
Iteration 23300: Loss = -12027.5263671875
1
Iteration 23400: Loss = -12027.4619140625
Iteration 23500: Loss = -12027.462890625
1
Iteration 23600: Loss = -12027.4619140625
Iteration 23700: Loss = -12027.4619140625
Iteration 23800: Loss = -12027.462890625
1
Iteration 23900: Loss = -12027.4619140625
Iteration 24000: Loss = -12027.4638671875
1
Iteration 24100: Loss = -12027.462890625
2
Iteration 24200: Loss = -12027.462890625
3
Iteration 24300: Loss = -12027.462890625
4
Iteration 24400: Loss = -12027.4619140625
Iteration 24500: Loss = -12027.462890625
1
Iteration 24600: Loss = -12027.4619140625
Iteration 24700: Loss = -12027.4638671875
1
Iteration 24800: Loss = -12027.462890625
2
Iteration 24900: Loss = -12027.462890625
3
Iteration 25000: Loss = -12027.462890625
4
Iteration 25100: Loss = -12027.462890625
5
Iteration 25200: Loss = -12027.462890625
6
Iteration 25300: Loss = -12027.462890625
7
Iteration 25400: Loss = -12027.462890625
8
Iteration 25500: Loss = -12027.462890625
9
Iteration 25600: Loss = -12027.462890625
10
Iteration 25700: Loss = -12027.4638671875
11
Iteration 25800: Loss = -12027.462890625
12
Iteration 25900: Loss = -12027.4619140625
Iteration 26000: Loss = -12027.4619140625
Iteration 26100: Loss = -12027.4619140625
Iteration 26200: Loss = -12027.4619140625
Iteration 26300: Loss = -12027.462890625
1
Iteration 26400: Loss = -12027.4619140625
Iteration 26500: Loss = -12027.462890625
1
Iteration 26600: Loss = -12027.4638671875
2
Iteration 26700: Loss = -12027.4619140625
Iteration 26800: Loss = -12027.4638671875
1
Iteration 26900: Loss = -12027.4619140625
Iteration 27000: Loss = -12027.462890625
1
Iteration 27100: Loss = -12027.4619140625
Iteration 27200: Loss = -12027.4619140625
Iteration 27300: Loss = -12027.4619140625
Iteration 27400: Loss = -12027.462890625
1
Iteration 27500: Loss = -12027.4619140625
Iteration 27600: Loss = -12027.4638671875
1
Iteration 27700: Loss = -12027.4638671875
2
Iteration 27800: Loss = -12027.462890625
3
Iteration 27900: Loss = -12027.462890625
4
Iteration 28000: Loss = -12027.4619140625
Iteration 28100: Loss = -12027.462890625
1
Iteration 28200: Loss = -12027.4619140625
Iteration 28300: Loss = -12027.4619140625
Iteration 28400: Loss = -12027.4599609375
Iteration 28500: Loss = -12026.2705078125
Iteration 28600: Loss = -12002.955078125
Iteration 28700: Loss = -11935.72265625
Iteration 28800: Loss = -11917.4375
Iteration 28900: Loss = -11913.4765625
Iteration 29000: Loss = -11898.7158203125
Iteration 29100: Loss = -11898.3701171875
Iteration 29200: Loss = -11888.19921875
Iteration 29300: Loss = -11887.318359375
Iteration 29400: Loss = -11871.833984375
Iteration 29500: Loss = -11871.0126953125
Iteration 29600: Loss = -11870.896484375
Iteration 29700: Loss = -11870.8369140625
Iteration 29800: Loss = -11870.796875
Iteration 29900: Loss = -11870.7685546875
pi: tensor([[0.5088, 0.4912],
        [0.4796, 0.5204]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4698, 0.5302], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3037, 0.0998],
         [0.9167, 0.3023]],

        [[0.8817, 0.0965],
         [0.9746, 0.2261]],

        [[0.7391, 0.0953],
         [0.9279, 0.9833]],

        [[0.7716, 0.0907],
         [0.1127, 0.9555]],

        [[0.7849, 0.1056],
         [0.0584, 0.4537]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.0380732843065139
Average Adjusted Rand Index: 0.983998902890853
[-3.82715035040419e-05, 0.0380732843065139] [0.0, 0.983998902890853] [12377.8720703125, 11870.7451171875]
-------------------------------------
This iteration is 45
True Objective function: Loss = -11718.038105213333
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30679.015625
Iteration 100: Loss = -20191.43359375
Iteration 200: Loss = -14637.736328125
Iteration 300: Loss = -13077.9267578125
Iteration 400: Loss = -12684.9765625
Iteration 500: Loss = -12527.16796875
Iteration 600: Loss = -12445.6962890625
Iteration 700: Loss = -12401.224609375
Iteration 800: Loss = -12377.1015625
Iteration 900: Loss = -12358.2333984375
Iteration 1000: Loss = -12340.8828125
Iteration 1100: Loss = -12327.986328125
Iteration 1200: Loss = -12317.2919921875
Iteration 1300: Loss = -12310.56640625
Iteration 1400: Loss = -12305.0126953125
Iteration 1500: Loss = -12295.13671875
Iteration 1600: Loss = -12287.4169921875
Iteration 1700: Loss = -12282.2685546875
Iteration 1800: Loss = -12276.3388671875
Iteration 1900: Loss = -12270.951171875
Iteration 2000: Loss = -12264.203125
Iteration 2100: Loss = -12258.068359375
Iteration 2200: Loss = -12255.1220703125
Iteration 2300: Loss = -12251.75390625
Iteration 2400: Loss = -12248.66796875
Iteration 2500: Loss = -12245.81640625
Iteration 2600: Loss = -12243.9921875
Iteration 2700: Loss = -12242.486328125
Iteration 2800: Loss = -12240.5185546875
Iteration 2900: Loss = -12238.142578125
Iteration 3000: Loss = -12235.9453125
Iteration 3100: Loss = -12231.4189453125
Iteration 3200: Loss = -12229.8994140625
Iteration 3300: Loss = -12228.9072265625
Iteration 3400: Loss = -12228.09375
Iteration 3500: Loss = -12225.95703125
Iteration 3600: Loss = -12223.7841796875
Iteration 3700: Loss = -12223.2548828125
Iteration 3800: Loss = -12222.8212890625
Iteration 3900: Loss = -12222.4482421875
Iteration 4000: Loss = -12222.1220703125
Iteration 4100: Loss = -12221.7861328125
Iteration 4200: Loss = -12220.82421875
Iteration 4300: Loss = -12220.3837890625
Iteration 4400: Loss = -12220.16015625
Iteration 4500: Loss = -12219.96484375
Iteration 4600: Loss = -12219.7890625
Iteration 4700: Loss = -12219.626953125
Iteration 4800: Loss = -12219.4794921875
Iteration 4900: Loss = -12219.33984375
Iteration 5000: Loss = -12219.2080078125
Iteration 5100: Loss = -12219.0810546875
Iteration 5200: Loss = -12218.9609375
Iteration 5300: Loss = -12218.8466796875
Iteration 5400: Loss = -12218.7353515625
Iteration 5500: Loss = -12218.626953125
Iteration 5600: Loss = -12218.5302734375
Iteration 5700: Loss = -12218.4375
Iteration 5800: Loss = -12218.3564453125
Iteration 5900: Loss = -12218.2783203125
Iteration 6000: Loss = -12218.2119140625
Iteration 6100: Loss = -12218.1484375
Iteration 6200: Loss = -12218.091796875
Iteration 6300: Loss = -12218.0380859375
Iteration 6400: Loss = -12217.9892578125
Iteration 6500: Loss = -12217.943359375
Iteration 6600: Loss = -12217.896484375
Iteration 6700: Loss = -12217.8486328125
Iteration 6800: Loss = -12217.7822265625
Iteration 6900: Loss = -12217.69921875
Iteration 7000: Loss = -12217.65625
Iteration 7100: Loss = -12217.6201171875
Iteration 7200: Loss = -12217.587890625
Iteration 7300: Loss = -12217.5537109375
Iteration 7400: Loss = -12217.4775390625
Iteration 7500: Loss = -12217.3974609375
Iteration 7600: Loss = -12217.3681640625
Iteration 7700: Loss = -12217.337890625
Iteration 7800: Loss = -12217.310546875
Iteration 7900: Loss = -12217.287109375
Iteration 8000: Loss = -12217.265625
Iteration 8100: Loss = -12217.2431640625
Iteration 8200: Loss = -12217.22265625
Iteration 8300: Loss = -12217.2001953125
Iteration 8400: Loss = -12217.1728515625
Iteration 8500: Loss = -12217.1318359375
Iteration 8600: Loss = -12217.0908203125
Iteration 8700: Loss = -12217.0546875
Iteration 8800: Loss = -12216.998046875
Iteration 8900: Loss = -12216.9599609375
Iteration 9000: Loss = -12216.9365234375
Iteration 9100: Loss = -12216.9208984375
Iteration 9200: Loss = -12216.9052734375
Iteration 9300: Loss = -12216.89453125
Iteration 9400: Loss = -12216.8837890625
Iteration 9500: Loss = -12216.873046875
Iteration 9600: Loss = -12216.8662109375
Iteration 9700: Loss = -12216.8564453125
Iteration 9800: Loss = -12216.849609375
Iteration 9900: Loss = -12216.8427734375
Iteration 10000: Loss = -12216.8349609375
Iteration 10100: Loss = -12216.8291015625
Iteration 10200: Loss = -12216.82421875
Iteration 10300: Loss = -12216.818359375
Iteration 10400: Loss = -12216.8125
Iteration 10500: Loss = -12216.8056640625
Iteration 10600: Loss = -12216.7939453125
Iteration 10700: Loss = -12216.599609375
Iteration 10800: Loss = -12216.5185546875
Iteration 10900: Loss = -12216.4716796875
Iteration 11000: Loss = -12216.439453125
Iteration 11100: Loss = -12216.41796875
Iteration 11200: Loss = -12216.4013671875
Iteration 11300: Loss = -12216.3896484375
Iteration 11400: Loss = -12216.37890625
Iteration 11500: Loss = -12216.3720703125
Iteration 11600: Loss = -12216.365234375
Iteration 11700: Loss = -12216.359375
Iteration 11800: Loss = -12216.353515625
Iteration 11900: Loss = -12216.3486328125
Iteration 12000: Loss = -12216.3447265625
Iteration 12100: Loss = -12216.3408203125
Iteration 12200: Loss = -12216.337890625
Iteration 12300: Loss = -12216.333984375
Iteration 12400: Loss = -12216.33203125
Iteration 12500: Loss = -12216.328125
Iteration 12600: Loss = -12216.326171875
Iteration 12700: Loss = -12216.32421875
Iteration 12800: Loss = -12216.322265625
Iteration 12900: Loss = -12216.322265625
Iteration 13000: Loss = -12216.31640625
Iteration 13100: Loss = -12216.314453125
Iteration 13200: Loss = -12216.3134765625
Iteration 13300: Loss = -12216.3125
Iteration 13400: Loss = -12216.310546875
Iteration 13500: Loss = -12216.3095703125
Iteration 13600: Loss = -12216.3095703125
Iteration 13700: Loss = -12216.3076171875
Iteration 13800: Loss = -12216.3076171875
Iteration 13900: Loss = -12216.3056640625
Iteration 14000: Loss = -12216.3056640625
Iteration 14100: Loss = -12216.306640625
1
Iteration 14200: Loss = -12216.3046875
Iteration 14300: Loss = -12216.3046875
Iteration 14400: Loss = -12216.3076171875
1
Iteration 14500: Loss = -12216.3046875
Iteration 14600: Loss = -12216.302734375
Iteration 14700: Loss = -12216.302734375
Iteration 14800: Loss = -12216.3017578125
Iteration 14900: Loss = -12216.3037109375
1
Iteration 15000: Loss = -12216.30078125
Iteration 15100: Loss = -12216.30078125
Iteration 15200: Loss = -12216.3017578125
1
Iteration 15300: Loss = -12216.30078125
Iteration 15400: Loss = -12216.2998046875
Iteration 15500: Loss = -12216.30078125
1
Iteration 15600: Loss = -12216.30078125
2
Iteration 15700: Loss = -12216.2998046875
Iteration 15800: Loss = -12216.2998046875
Iteration 15900: Loss = -12216.2998046875
Iteration 16000: Loss = -12216.298828125
Iteration 16100: Loss = -12216.298828125
Iteration 16200: Loss = -12216.2978515625
Iteration 16300: Loss = -12216.2978515625
Iteration 16400: Loss = -12216.2978515625
Iteration 16500: Loss = -12216.2998046875
1
Iteration 16600: Loss = -12216.298828125
2
Iteration 16700: Loss = -12216.2978515625
Iteration 16800: Loss = -12216.2978515625
Iteration 16900: Loss = -12216.2978515625
Iteration 17000: Loss = -12216.298828125
1
Iteration 17100: Loss = -12216.298828125
2
Iteration 17200: Loss = -12216.2958984375
Iteration 17300: Loss = -12216.296875
1
Iteration 17400: Loss = -12216.296875
2
Iteration 17500: Loss = -12216.2978515625
3
Iteration 17600: Loss = -12216.2978515625
4
Iteration 17700: Loss = -12216.296875
5
Iteration 17800: Loss = -12216.2978515625
6
Iteration 17900: Loss = -12216.2958984375
Iteration 18000: Loss = -12216.2958984375
Iteration 18100: Loss = -12216.296875
1
Iteration 18200: Loss = -12216.2958984375
Iteration 18300: Loss = -12216.296875
1
Iteration 18400: Loss = -12216.2958984375
Iteration 18500: Loss = -12216.2958984375
Iteration 18600: Loss = -12216.296875
1
Iteration 18700: Loss = -12216.296875
2
Iteration 18800: Loss = -12216.2958984375
Iteration 18900: Loss = -12216.2958984375
Iteration 19000: Loss = -12216.296875
1
Iteration 19100: Loss = -12216.2958984375
Iteration 19200: Loss = -12216.2978515625
1
Iteration 19300: Loss = -12216.296875
2
Iteration 19400: Loss = -12216.294921875
Iteration 19500: Loss = -12216.2958984375
1
Iteration 19600: Loss = -12216.296875
2
Iteration 19700: Loss = -12216.296875
3
Iteration 19800: Loss = -12216.294921875
Iteration 19900: Loss = -12216.296875
1
Iteration 20000: Loss = -12216.2958984375
2
Iteration 20100: Loss = -12216.2958984375
3
Iteration 20200: Loss = -12216.2958984375
4
Iteration 20300: Loss = -12216.2958984375
5
Iteration 20400: Loss = -12216.2958984375
6
Iteration 20500: Loss = -12216.2958984375
7
Iteration 20600: Loss = -12216.2958984375
8
Iteration 20700: Loss = -12216.2958984375
9
Iteration 20800: Loss = -12216.296875
10
Iteration 20900: Loss = -12216.296875
11
Iteration 21000: Loss = -12216.296875
12
Iteration 21100: Loss = -12216.294921875
Iteration 21200: Loss = -12216.296875
1
Iteration 21300: Loss = -12216.2958984375
2
Iteration 21400: Loss = -12216.2958984375
3
Iteration 21500: Loss = -12216.296875
4
Iteration 21600: Loss = -12216.294921875
Iteration 21700: Loss = -12216.294921875
Iteration 21800: Loss = -12216.2958984375
1
Iteration 21900: Loss = -12216.296875
2
Iteration 22000: Loss = -12216.296875
3
Iteration 22100: Loss = -12216.294921875
Iteration 22200: Loss = -12216.296875
1
Iteration 22300: Loss = -12216.296875
2
Iteration 22400: Loss = -12216.294921875
Iteration 22500: Loss = -12216.296875
1
Iteration 22600: Loss = -12216.296875
2
Iteration 22700: Loss = -12216.296875
3
Iteration 22800: Loss = -12216.2939453125
Iteration 22900: Loss = -12216.2958984375
1
Iteration 23000: Loss = -12216.2958984375
2
Iteration 23100: Loss = -12216.2958984375
3
Iteration 23200: Loss = -12216.2958984375
4
Iteration 23300: Loss = -12216.2958984375
5
Iteration 23400: Loss = -12216.2958984375
6
Iteration 23500: Loss = -12216.294921875
7
Iteration 23600: Loss = -12216.2958984375
8
Iteration 23700: Loss = -12216.2958984375
9
Iteration 23800: Loss = -12216.2958984375
10
Iteration 23900: Loss = -12216.2958984375
11
Iteration 24000: Loss = -12216.2958984375
12
Iteration 24100: Loss = -12216.296875
13
Iteration 24200: Loss = -12216.2958984375
14
Iteration 24300: Loss = -12216.2958984375
15
Stopping early at iteration 24300 due to no improvement.
pi: tensor([[7.4540e-04, 9.9925e-01],
        [2.0662e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9985, 0.0015], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2051, 0.2049],
         [0.3924, 0.1927]],

        [[0.9886, 0.1992],
         [0.1011, 0.1982]],

        [[0.6537, 0.2247],
         [0.9293, 0.9920]],

        [[0.2237, 0.2207],
         [0.4271, 0.9884]],

        [[0.5406, 0.2128],
         [0.9315, 0.8910]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0012828947368421053
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -34539.50390625
Iteration 100: Loss = -22949.435546875
Iteration 200: Loss = -14357.798828125
Iteration 300: Loss = -12987.6923828125
Iteration 400: Loss = -12713.84375
Iteration 500: Loss = -12582.54296875
Iteration 600: Loss = -12498.484375
Iteration 700: Loss = -12448.3662109375
Iteration 800: Loss = -12410.1611328125
Iteration 900: Loss = -12386.2392578125
Iteration 1000: Loss = -12364.9931640625
Iteration 1100: Loss = -12350.0615234375
Iteration 1200: Loss = -12341.009765625
Iteration 1300: Loss = -12334.056640625
Iteration 1400: Loss = -12327.7666015625
Iteration 1500: Loss = -12319.4365234375
Iteration 1600: Loss = -12313.287109375
Iteration 1700: Loss = -12305.6669921875
Iteration 1800: Loss = -12299.0615234375
Iteration 1900: Loss = -12291.8447265625
Iteration 2000: Loss = -12284.3349609375
Iteration 2100: Loss = -12273.974609375
Iteration 2200: Loss = -12267.873046875
Iteration 2300: Loss = -12265.1611328125
Iteration 2400: Loss = -12262.1337890625
Iteration 2500: Loss = -12259.708984375
Iteration 2600: Loss = -12258.103515625
Iteration 2700: Loss = -12256.7470703125
Iteration 2800: Loss = -12255.5068359375
Iteration 2900: Loss = -12254.26171875
Iteration 3000: Loss = -12252.390625
Iteration 3100: Loss = -12247.4697265625
Iteration 3200: Loss = -12245.1318359375
Iteration 3300: Loss = -12243.892578125
Iteration 3400: Loss = -12242.96484375
Iteration 3500: Loss = -12242.248046875
Iteration 3600: Loss = -12241.66015625
Iteration 3700: Loss = -12241.162109375
Iteration 3800: Loss = -12240.7060546875
Iteration 3900: Loss = -12240.2177734375
Iteration 4000: Loss = -12238.3935546875
Iteration 4100: Loss = -12236.5009765625
Iteration 4200: Loss = -12236.0068359375
Iteration 4300: Loss = -12235.18359375
Iteration 4400: Loss = -12234.3681640625
Iteration 4500: Loss = -12234.0595703125
Iteration 4600: Loss = -12233.8232421875
Iteration 4700: Loss = -12233.6201171875
Iteration 4800: Loss = -12233.4423828125
Iteration 4900: Loss = -12233.27734375
Iteration 5000: Loss = -12233.1279296875
Iteration 5100: Loss = -12232.9921875
Iteration 5200: Loss = -12232.86328125
Iteration 5300: Loss = -12232.7431640625
Iteration 5400: Loss = -12232.6328125
Iteration 5500: Loss = -12232.5263671875
Iteration 5600: Loss = -12232.427734375
Iteration 5700: Loss = -12232.3330078125
Iteration 5800: Loss = -12232.2421875
Iteration 5900: Loss = -12232.1533203125
Iteration 6000: Loss = -12232.05078125
Iteration 6100: Loss = -12231.869140625
Iteration 6200: Loss = -12231.2861328125
Iteration 6300: Loss = -12228.06640625
Iteration 6400: Loss = -12225.1494140625
Iteration 6500: Loss = -12224.3828125
Iteration 6600: Loss = -12224.1767578125
Iteration 6700: Loss = -12224.046875
Iteration 6800: Loss = -12223.908203125
Iteration 6900: Loss = -12218.98828125
Iteration 7000: Loss = -12218.7685546875
Iteration 7100: Loss = -12218.658203125
Iteration 7200: Loss = -12218.578125
Iteration 7300: Loss = -12218.5166015625
Iteration 7400: Loss = -12218.4619140625
Iteration 7500: Loss = -12218.4150390625
Iteration 7600: Loss = -12218.3740234375
Iteration 7700: Loss = -12218.3349609375
Iteration 7800: Loss = -12218.30078125
Iteration 7900: Loss = -12218.267578125
Iteration 8000: Loss = -12218.2373046875
Iteration 8100: Loss = -12218.205078125
Iteration 8200: Loss = -12218.1787109375
Iteration 8300: Loss = -12218.1533203125
Iteration 8400: Loss = -12218.126953125
Iteration 8500: Loss = -12218.1044921875
Iteration 8600: Loss = -12218.0849609375
Iteration 8700: Loss = -12218.064453125
Iteration 8800: Loss = -12218.0478515625
Iteration 8900: Loss = -12218.033203125
Iteration 9000: Loss = -12218.017578125
Iteration 9100: Loss = -12218.005859375
Iteration 9200: Loss = -12217.994140625
Iteration 9300: Loss = -12217.982421875
Iteration 9400: Loss = -12217.970703125
Iteration 9500: Loss = -12217.9619140625
Iteration 9600: Loss = -12217.9521484375
Iteration 9700: Loss = -12217.94140625
Iteration 9800: Loss = -12217.93359375
Iteration 9900: Loss = -12217.92578125
Iteration 10000: Loss = -12217.9189453125
Iteration 10100: Loss = -12217.9111328125
Iteration 10200: Loss = -12217.9033203125
Iteration 10300: Loss = -12217.896484375
Iteration 10400: Loss = -12217.8916015625
Iteration 10500: Loss = -12217.8837890625
Iteration 10600: Loss = -12217.8779296875
Iteration 10700: Loss = -12217.8720703125
Iteration 10800: Loss = -12217.8662109375
Iteration 10900: Loss = -12217.8623046875
Iteration 11000: Loss = -12217.8583984375
Iteration 11100: Loss = -12217.853515625
Iteration 11200: Loss = -12217.84765625
Iteration 11300: Loss = -12217.8447265625
Iteration 11400: Loss = -12217.83984375
Iteration 11500: Loss = -12217.8349609375
Iteration 11600: Loss = -12217.8330078125
Iteration 11700: Loss = -12217.8291015625
Iteration 11800: Loss = -12217.826171875
Iteration 11900: Loss = -12217.8232421875
Iteration 12000: Loss = -12217.8193359375
Iteration 12100: Loss = -12217.8173828125
Iteration 12200: Loss = -12217.8154296875
Iteration 12300: Loss = -12217.814453125
Iteration 12400: Loss = -12217.810546875
Iteration 12500: Loss = -12217.8076171875
Iteration 12600: Loss = -12217.8115234375
1
Iteration 12700: Loss = -12217.8056640625
Iteration 12800: Loss = -12217.8037109375
Iteration 12900: Loss = -12217.80078125
Iteration 13000: Loss = -12217.798828125
Iteration 13100: Loss = -12217.796875
Iteration 13200: Loss = -12217.7958984375
Iteration 13300: Loss = -12217.7939453125
Iteration 13400: Loss = -12217.79296875
Iteration 13500: Loss = -12217.7919921875
Iteration 13600: Loss = -12217.7900390625
Iteration 13700: Loss = -12217.7900390625
Iteration 13800: Loss = -12217.7880859375
Iteration 13900: Loss = -12217.7861328125
Iteration 14000: Loss = -12217.78515625
Iteration 14100: Loss = -12217.78515625
Iteration 14200: Loss = -12217.78515625
Iteration 14300: Loss = -12217.783203125
Iteration 14400: Loss = -12217.7822265625
Iteration 14500: Loss = -12217.78125
Iteration 14600: Loss = -12217.78125
Iteration 14700: Loss = -12217.7802734375
Iteration 14800: Loss = -12217.78125
1
Iteration 14900: Loss = -12217.779296875
Iteration 15000: Loss = -12217.77734375
Iteration 15100: Loss = -12217.7783203125
1
Iteration 15200: Loss = -12217.7783203125
2
Iteration 15300: Loss = -12217.77734375
Iteration 15400: Loss = -12217.775390625
Iteration 15500: Loss = -12217.775390625
Iteration 15600: Loss = -12217.775390625
Iteration 15700: Loss = -12217.7763671875
1
Iteration 15800: Loss = -12217.7744140625
Iteration 15900: Loss = -12217.7744140625
Iteration 16000: Loss = -12217.7744140625
Iteration 16100: Loss = -12217.7734375
Iteration 16200: Loss = -12217.7734375
Iteration 16300: Loss = -12217.7705078125
Iteration 16400: Loss = -12217.76953125
Iteration 16500: Loss = -12217.7705078125
1
Iteration 16600: Loss = -12217.767578125
Iteration 16700: Loss = -12217.7685546875
1
Iteration 16800: Loss = -12217.7685546875
2
Iteration 16900: Loss = -12217.7685546875
3
Iteration 17000: Loss = -12217.767578125
Iteration 17100: Loss = -12217.767578125
Iteration 17200: Loss = -12217.767578125
Iteration 17300: Loss = -12217.7685546875
1
Iteration 17400: Loss = -12217.767578125
Iteration 17500: Loss = -12217.7666015625
Iteration 17600: Loss = -12217.765625
Iteration 17700: Loss = -12217.765625
Iteration 17800: Loss = -12217.767578125
1
Iteration 17900: Loss = -12217.765625
Iteration 18000: Loss = -12217.765625
Iteration 18100: Loss = -12217.7666015625
1
Iteration 18200: Loss = -12217.765625
Iteration 18300: Loss = -12217.7646484375
Iteration 18400: Loss = -12217.7666015625
1
Iteration 18500: Loss = -12217.7646484375
Iteration 18600: Loss = -12217.765625
1
Iteration 18700: Loss = -12217.765625
2
Iteration 18800: Loss = -12217.7646484375
Iteration 18900: Loss = -12217.765625
1
Iteration 19000: Loss = -12217.7646484375
Iteration 19100: Loss = -12217.7646484375
Iteration 19200: Loss = -12217.765625
1
Iteration 19300: Loss = -12217.765625
2
Iteration 19400: Loss = -12217.763671875
Iteration 19500: Loss = -12217.763671875
Iteration 19600: Loss = -12217.765625
1
Iteration 19700: Loss = -12217.7666015625
2
Iteration 19800: Loss = -12217.7646484375
3
Iteration 19900: Loss = -12217.7666015625
4
Iteration 20000: Loss = -12217.763671875
Iteration 20100: Loss = -12217.7646484375
1
Iteration 20200: Loss = -12217.763671875
Iteration 20300: Loss = -12217.7646484375
1
Iteration 20400: Loss = -12217.7646484375
2
Iteration 20500: Loss = -12217.7646484375
3
Iteration 20600: Loss = -12217.7646484375
4
Iteration 20700: Loss = -12217.7646484375
5
Iteration 20800: Loss = -12217.7646484375
6
Iteration 20900: Loss = -12217.763671875
Iteration 21000: Loss = -12217.765625
1
Iteration 21100: Loss = -12217.7646484375
2
Iteration 21200: Loss = -12217.7646484375
3
Iteration 21300: Loss = -12217.7666015625
4
Iteration 21400: Loss = -12217.7646484375
5
Iteration 21500: Loss = -12217.763671875
Iteration 21600: Loss = -12217.765625
1
Iteration 21700: Loss = -12217.7646484375
2
Iteration 21800: Loss = -12217.7646484375
3
Iteration 21900: Loss = -12217.765625
4
Iteration 22000: Loss = -12217.763671875
Iteration 22100: Loss = -12217.763671875
Iteration 22200: Loss = -12217.765625
1
Iteration 22300: Loss = -12217.7646484375
2
Iteration 22400: Loss = -12217.7626953125
Iteration 22500: Loss = -12217.7666015625
1
Iteration 22600: Loss = -12217.7646484375
2
Iteration 22700: Loss = -12217.7666015625
3
Iteration 22800: Loss = -12217.763671875
4
Iteration 22900: Loss = -12217.765625
5
Iteration 23000: Loss = -12217.765625
6
Iteration 23100: Loss = -12217.7646484375
7
Iteration 23200: Loss = -12217.765625
8
Iteration 23300: Loss = -12217.7646484375
9
Iteration 23400: Loss = -12217.7646484375
10
Iteration 23500: Loss = -12217.7646484375
11
Iteration 23600: Loss = -12217.7666015625
12
Iteration 23700: Loss = -12217.7646484375
13
Iteration 23800: Loss = -12217.765625
14
Iteration 23900: Loss = -12217.763671875
15
Stopping early at iteration 23900 due to no improvement.
pi: tensor([[0.9965, 0.0035],
        [0.8121, 0.1879]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 1.1325e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1956, 0.2117],
         [0.6561, 0.1699]],

        [[0.0073, 0.0738],
         [0.8897, 0.9917]],

        [[0.1043, 0.1417],
         [0.7755, 0.6626]],

        [[0.3029, 0.5233],
         [0.9858, 0.1037]],

        [[0.1413, 0.2300],
         [0.8584, 0.2052]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.000333334401712826
Average Adjusted Rand Index: 0.0006957530076822728
[-0.0012828947368421053, 0.000333334401712826] [0.0, 0.0006957530076822728] [12216.2958984375, 12217.763671875]
-------------------------------------
This iteration is 46
True Objective function: Loss = -11864.669612769521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -61516.625
Iteration 100: Loss = -45712.48046875
Iteration 200: Loss = -33002.47265625
Iteration 300: Loss = -23535.798828125
Iteration 400: Loss = -17355.498046875
Iteration 500: Loss = -14226.064453125
Iteration 600: Loss = -13018.8388671875
Iteration 700: Loss = -12589.8740234375
Iteration 800: Loss = -12474.5703125
Iteration 900: Loss = -12430.228515625
Iteration 1000: Loss = -12413.3828125
Iteration 1100: Loss = -12408.1220703125
Iteration 1200: Loss = -12402.5673828125
Iteration 1300: Loss = -12397.841796875
Iteration 1400: Loss = -12394.0693359375
Iteration 1500: Loss = -12390.150390625
Iteration 1600: Loss = -12387.9951171875
Iteration 1700: Loss = -12386.2900390625
Iteration 1800: Loss = -12384.4599609375
Iteration 1900: Loss = -12382.37890625
Iteration 2000: Loss = -12380.5546875
Iteration 2100: Loss = -12378.3046875
Iteration 2200: Loss = -12377.3544921875
Iteration 2300: Loss = -12376.779296875
Iteration 2400: Loss = -12376.3798828125
Iteration 2500: Loss = -12376.078125
Iteration 2600: Loss = -12375.833984375
Iteration 2700: Loss = -12375.6357421875
Iteration 2800: Loss = -12375.4677734375
Iteration 2900: Loss = -12375.3291015625
Iteration 3000: Loss = -12375.20703125
Iteration 3100: Loss = -12375.1015625
Iteration 3200: Loss = -12375.0078125
Iteration 3300: Loss = -12374.923828125
Iteration 3400: Loss = -12374.8466796875
Iteration 3500: Loss = -12374.77734375
Iteration 3600: Loss = -12374.716796875
Iteration 3700: Loss = -12374.6591796875
Iteration 3800: Loss = -12374.607421875
Iteration 3900: Loss = -12374.5615234375
Iteration 4000: Loss = -12374.517578125
Iteration 4100: Loss = -12374.474609375
Iteration 4200: Loss = -12374.4384765625
Iteration 4300: Loss = -12374.404296875
Iteration 4400: Loss = -12374.373046875
Iteration 4500: Loss = -12374.3447265625
Iteration 4600: Loss = -12374.318359375
Iteration 4700: Loss = -12374.29296875
Iteration 4800: Loss = -12374.26953125
Iteration 4900: Loss = -12374.248046875
Iteration 5000: Loss = -12374.2265625
Iteration 5100: Loss = -12374.2080078125
Iteration 5200: Loss = -12374.1884765625
Iteration 5300: Loss = -12374.171875
Iteration 5400: Loss = -12374.154296875
Iteration 5500: Loss = -12374.138671875
Iteration 5600: Loss = -12374.1259765625
Iteration 5700: Loss = -12374.109375
Iteration 5800: Loss = -12374.0986328125
Iteration 5900: Loss = -12374.0859375
Iteration 6000: Loss = -12374.0732421875
Iteration 6100: Loss = -12374.0654296875
Iteration 6200: Loss = -12374.0537109375
Iteration 6300: Loss = -12374.0439453125
Iteration 6400: Loss = -12374.0361328125
Iteration 6500: Loss = -12374.0263671875
Iteration 6600: Loss = -12374.0166015625
Iteration 6700: Loss = -12374.0087890625
Iteration 6800: Loss = -12374.0
Iteration 6900: Loss = -12373.994140625
Iteration 7000: Loss = -12373.9873046875
Iteration 7100: Loss = -12373.9794921875
Iteration 7200: Loss = -12373.974609375
Iteration 7300: Loss = -12373.9677734375
Iteration 7400: Loss = -12373.962890625
Iteration 7500: Loss = -12373.95703125
Iteration 7600: Loss = -12373.9521484375
Iteration 7700: Loss = -12373.9453125
Iteration 7800: Loss = -12373.939453125
Iteration 7900: Loss = -12373.9326171875
Iteration 8000: Loss = -12373.9267578125
Iteration 8100: Loss = -12373.919921875
Iteration 8200: Loss = -12373.9033203125
Iteration 8300: Loss = -12373.7021484375
Iteration 8400: Loss = -12372.53515625
Iteration 8500: Loss = -12372.279296875
Iteration 8600: Loss = -12371.974609375
Iteration 8700: Loss = -12371.798828125
Iteration 8800: Loss = -12371.7236328125
Iteration 8900: Loss = -12371.681640625
Iteration 9000: Loss = -12371.65625
Iteration 9100: Loss = -12371.63671875
Iteration 9200: Loss = -12371.6220703125
Iteration 9300: Loss = -12371.6103515625
Iteration 9400: Loss = -12371.6005859375
Iteration 9500: Loss = -12371.5908203125
Iteration 9600: Loss = -12371.5751953125
Iteration 9700: Loss = -12371.54296875
Iteration 9800: Loss = -12371.53515625
Iteration 9900: Loss = -12371.529296875
Iteration 10000: Loss = -12371.5224609375
Iteration 10100: Loss = -12371.5205078125
Iteration 10200: Loss = -12371.5126953125
Iteration 10300: Loss = -12371.5087890625
Iteration 10400: Loss = -12371.501953125
Iteration 10500: Loss = -12371.4921875
Iteration 10600: Loss = -12371.4833984375
Iteration 10700: Loss = -12371.4765625
Iteration 10800: Loss = -12371.4716796875
Iteration 10900: Loss = -12371.466796875
Iteration 11000: Loss = -12371.4619140625
Iteration 11100: Loss = -12371.45703125
Iteration 11200: Loss = -12371.44921875
Iteration 11300: Loss = -12371.44140625
Iteration 11400: Loss = -12371.3876953125
Iteration 11500: Loss = -12371.373046875
Iteration 11600: Loss = -12371.361328125
Iteration 11700: Loss = -12371.349609375
Iteration 11800: Loss = -12371.33984375
Iteration 11900: Loss = -12371.326171875
Iteration 12000: Loss = -12371.3076171875
Iteration 12100: Loss = -12371.2431640625
Iteration 12200: Loss = -12370.646484375
Iteration 12300: Loss = -12370.5498046875
Iteration 12400: Loss = -12370.5263671875
Iteration 12500: Loss = -12370.5087890625
Iteration 12600: Loss = -12370.4970703125
Iteration 12700: Loss = -12370.4853515625
Iteration 12800: Loss = -12370.4736328125
Iteration 12900: Loss = -12370.4609375
Iteration 13000: Loss = -12370.4482421875
Iteration 13100: Loss = -12370.435546875
Iteration 13200: Loss = -12370.4189453125
Iteration 13300: Loss = -12370.404296875
Iteration 13400: Loss = -12370.38671875
Iteration 13500: Loss = -12370.3681640625
Iteration 13600: Loss = -12370.349609375
Iteration 13700: Loss = -12370.328125
Iteration 13800: Loss = -12370.306640625
Iteration 13900: Loss = -12370.2861328125
Iteration 14000: Loss = -12370.263671875
Iteration 14100: Loss = -12370.23828125
Iteration 14200: Loss = -12370.2138671875
Iteration 14300: Loss = -12370.19140625
Iteration 14400: Loss = -12370.1669921875
Iteration 14500: Loss = -12370.1455078125
Iteration 14600: Loss = -12370.1220703125
Iteration 14700: Loss = -12370.1025390625
Iteration 14800: Loss = -12370.0849609375
Iteration 14900: Loss = -12370.0712890625
Iteration 15000: Loss = -12370.0595703125
Iteration 15100: Loss = -12370.05078125
Iteration 15200: Loss = -12370.0439453125
Iteration 15300: Loss = -12370.041015625
Iteration 15400: Loss = -12370.037109375
Iteration 15500: Loss = -12370.0341796875
Iteration 15600: Loss = -12370.033203125
Iteration 15700: Loss = -12370.0322265625
Iteration 15800: Loss = -12370.03125
Iteration 15900: Loss = -12370.0302734375
Iteration 16000: Loss = -12370.0302734375
Iteration 16100: Loss = -12370.03125
1
Iteration 16200: Loss = -12370.0302734375
Iteration 16300: Loss = -12370.029296875
Iteration 16400: Loss = -12370.0283203125
Iteration 16500: Loss = -12370.0283203125
Iteration 16600: Loss = -12370.029296875
1
Iteration 16700: Loss = -12370.0302734375
2
Iteration 16800: Loss = -12370.0283203125
Iteration 16900: Loss = -12370.02734375
Iteration 17000: Loss = -12370.02734375
Iteration 17100: Loss = -12370.0234375
Iteration 17200: Loss = -12370.0224609375
Iteration 17300: Loss = -12370.0205078125
Iteration 17400: Loss = -12369.822265625
Iteration 17500: Loss = -12369.8056640625
Iteration 17600: Loss = -12369.802734375
Iteration 17700: Loss = -12369.7978515625
Iteration 17800: Loss = -12369.7978515625
Iteration 17900: Loss = -12369.794921875
Iteration 18000: Loss = -12369.7978515625
1
Iteration 18100: Loss = -12369.7939453125
Iteration 18200: Loss = -12369.7939453125
Iteration 18300: Loss = -12369.7939453125
Iteration 18400: Loss = -12369.7939453125
Iteration 18500: Loss = -12369.7939453125
Iteration 18600: Loss = -12369.7939453125
Iteration 18700: Loss = -12369.7890625
Iteration 18800: Loss = -12369.7880859375
Iteration 18900: Loss = -12369.7900390625
1
Iteration 19000: Loss = -12369.7890625
2
Iteration 19100: Loss = -12369.7890625
3
Iteration 19200: Loss = -12369.7890625
4
Iteration 19300: Loss = -12369.7880859375
Iteration 19400: Loss = -12369.7890625
1
Iteration 19500: Loss = -12369.7880859375
Iteration 19600: Loss = -12369.7861328125
Iteration 19700: Loss = -12369.787109375
1
Iteration 19800: Loss = -12369.7880859375
2
Iteration 19900: Loss = -12369.787109375
3
Iteration 20000: Loss = -12369.787109375
4
Iteration 20100: Loss = -12369.7861328125
Iteration 20200: Loss = -12369.7880859375
1
Iteration 20300: Loss = -12369.7861328125
Iteration 20400: Loss = -12369.787109375
1
Iteration 20500: Loss = -12369.7861328125
Iteration 20600: Loss = -12369.787109375
1
Iteration 20700: Loss = -12369.78515625
Iteration 20800: Loss = -12369.7861328125
1
Iteration 20900: Loss = -12369.7861328125
2
Iteration 21000: Loss = -12369.7861328125
3
Iteration 21100: Loss = -12369.78515625
Iteration 21200: Loss = -12369.7861328125
1
Iteration 21300: Loss = -12369.7841796875
Iteration 21400: Loss = -12369.78515625
1
Iteration 21500: Loss = -12369.787109375
2
Iteration 21600: Loss = -12369.7861328125
3
Iteration 21700: Loss = -12369.7861328125
4
Iteration 21800: Loss = -12369.7861328125
5
Iteration 21900: Loss = -12369.7861328125
6
Iteration 22000: Loss = -12369.78515625
7
Iteration 22100: Loss = -12369.78515625
8
Iteration 22200: Loss = -12369.78515625
9
Iteration 22300: Loss = -12369.787109375
10
Iteration 22400: Loss = -12369.78515625
11
Iteration 22500: Loss = -12369.7861328125
12
Iteration 22600: Loss = -12369.78515625
13
Iteration 22700: Loss = -12369.787109375
14
Iteration 22800: Loss = -12369.7861328125
15
Stopping early at iteration 22800 due to no improvement.
pi: tensor([[9.9999e-01, 1.0018e-05],
        [3.7820e-01, 6.2180e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9644, 0.0356], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1996, 0.2133],
         [0.9884, 0.0885]],

        [[0.0184, 0.1049],
         [0.9690, 0.1133]],

        [[0.7448, 0.3386],
         [0.0199, 0.2135]],

        [[0.8341, 0.2707],
         [0.9844, 0.9162]],

        [[0.0106, 0.2030],
         [0.6342, 0.9908]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
Global Adjusted Rand Index: 0.0007493607032682356
Average Adjusted Rand Index: 0.0004925310074841425
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -54594.1953125
Iteration 100: Loss = -37138.12890625
Iteration 200: Loss = -22850.33203125
Iteration 300: Loss = -15902.1025390625
Iteration 400: Loss = -13498.982421875
Iteration 500: Loss = -12787.751953125
Iteration 600: Loss = -12588.5712890625
Iteration 700: Loss = -12508.2138671875
Iteration 800: Loss = -12476.10546875
Iteration 900: Loss = -12452.716796875
Iteration 1000: Loss = -12430.423828125
Iteration 1100: Loss = -12417.2451171875
Iteration 1200: Loss = -12410.0537109375
Iteration 1300: Loss = -12406.064453125
Iteration 1400: Loss = -12403.162109375
Iteration 1500: Loss = -12399.830078125
Iteration 1600: Loss = -12395.4921875
Iteration 1700: Loss = -12393.8759765625
Iteration 1800: Loss = -12392.5869140625
Iteration 1900: Loss = -12391.517578125
Iteration 2000: Loss = -12390.599609375
Iteration 2100: Loss = -12389.791015625
Iteration 2200: Loss = -12389.11328125
Iteration 2300: Loss = -12388.5166015625
Iteration 2400: Loss = -12387.978515625
Iteration 2500: Loss = -12387.5048828125
Iteration 2600: Loss = -12387.0830078125
Iteration 2700: Loss = -12386.701171875
Iteration 2800: Loss = -12386.337890625
Iteration 2900: Loss = -12385.87890625
Iteration 3000: Loss = -12385.5859375
Iteration 3100: Loss = -12385.330078125
Iteration 3200: Loss = -12385.0966796875
Iteration 3300: Loss = -12384.87890625
Iteration 3400: Loss = -12384.6806640625
Iteration 3500: Loss = -12384.4951171875
Iteration 3600: Loss = -12384.326171875
Iteration 3700: Loss = -12384.169921875
Iteration 3800: Loss = -12384.0244140625
Iteration 3900: Loss = -12383.8876953125
Iteration 4000: Loss = -12383.76171875
Iteration 4100: Loss = -12383.64453125
Iteration 4200: Loss = -12383.533203125
Iteration 4300: Loss = -12383.4287109375
Iteration 4400: Loss = -12383.330078125
Iteration 4500: Loss = -12383.2314453125
Iteration 4600: Loss = -12383.1435546875
Iteration 4700: Loss = -12383.060546875
Iteration 4800: Loss = -12382.98046875
Iteration 4900: Loss = -12382.908203125
Iteration 5000: Loss = -12382.8369140625
Iteration 5100: Loss = -12382.76953125
Iteration 5200: Loss = -12382.70703125
Iteration 5300: Loss = -12382.64453125
Iteration 5400: Loss = -12382.587890625
Iteration 5500: Loss = -12382.5302734375
Iteration 5600: Loss = -12382.478515625
Iteration 5700: Loss = -12382.423828125
Iteration 5800: Loss = -12382.376953125
Iteration 5900: Loss = -12382.3310546875
Iteration 6000: Loss = -12382.2880859375
Iteration 6100: Loss = -12382.2470703125
Iteration 6200: Loss = -12382.20703125
Iteration 6300: Loss = -12382.1669921875
Iteration 6400: Loss = -12382.12890625
Iteration 6500: Loss = -12382.09375
Iteration 6600: Loss = -12382.0595703125
Iteration 6700: Loss = -12382.029296875
Iteration 6800: Loss = -12381.9990234375
Iteration 6900: Loss = -12381.9716796875
Iteration 7000: Loss = -12381.9443359375
Iteration 7100: Loss = -12381.91796875
Iteration 7200: Loss = -12381.8935546875
Iteration 7300: Loss = -12381.8681640625
Iteration 7400: Loss = -12381.8359375
Iteration 7500: Loss = -12381.8125
Iteration 7600: Loss = -12381.791015625
Iteration 7700: Loss = -12381.767578125
Iteration 7800: Loss = -12381.7470703125
Iteration 7900: Loss = -12381.7275390625
Iteration 8000: Loss = -12381.703125
Iteration 8100: Loss = -12381.6748046875
Iteration 8200: Loss = -12381.6337890625
Iteration 8300: Loss = -12381.52734375
Iteration 8400: Loss = -12380.7265625
Iteration 8500: Loss = -12379.6982421875
Iteration 8600: Loss = -12379.3603515625
Iteration 8700: Loss = -12379.189453125
Iteration 8800: Loss = -12379.0927734375
Iteration 8900: Loss = -12379.0302734375
Iteration 9000: Loss = -12378.9853515625
Iteration 9100: Loss = -12378.953125
Iteration 9200: Loss = -12378.927734375
Iteration 9300: Loss = -12378.9072265625
Iteration 9400: Loss = -12378.890625
Iteration 9500: Loss = -12378.8759765625
Iteration 9600: Loss = -12378.865234375
Iteration 9700: Loss = -12378.85546875
Iteration 9800: Loss = -12378.8466796875
Iteration 9900: Loss = -12378.8369140625
Iteration 10000: Loss = -12378.830078125
Iteration 10100: Loss = -12378.822265625
Iteration 10200: Loss = -12378.8173828125
Iteration 10300: Loss = -12378.8125
Iteration 10400: Loss = -12378.8056640625
Iteration 10500: Loss = -12378.80078125
Iteration 10600: Loss = -12378.7978515625
Iteration 10700: Loss = -12378.791015625
Iteration 10800: Loss = -12378.787109375
Iteration 10900: Loss = -12378.7861328125
Iteration 11000: Loss = -12378.783203125
Iteration 11100: Loss = -12378.779296875
Iteration 11200: Loss = -12378.775390625
Iteration 11300: Loss = -12378.775390625
Iteration 11400: Loss = -12378.771484375
Iteration 11500: Loss = -12378.7685546875
Iteration 11600: Loss = -12378.765625
Iteration 11700: Loss = -12378.765625
Iteration 11800: Loss = -12378.763671875
Iteration 11900: Loss = -12378.7607421875
Iteration 12000: Loss = -12378.759765625
Iteration 12100: Loss = -12378.755859375
Iteration 12200: Loss = -12378.755859375
Iteration 12300: Loss = -12378.75390625
Iteration 12400: Loss = -12378.7509765625
Iteration 12500: Loss = -12378.7509765625
Iteration 12600: Loss = -12378.75
Iteration 12700: Loss = -12378.7490234375
Iteration 12800: Loss = -12378.748046875
Iteration 12900: Loss = -12378.7470703125
Iteration 13000: Loss = -12378.74609375
Iteration 13100: Loss = -12378.7451171875
Iteration 13200: Loss = -12378.744140625
Iteration 13300: Loss = -12378.744140625
Iteration 13400: Loss = -12378.7421875
Iteration 13500: Loss = -12378.7412109375
Iteration 13600: Loss = -12378.7412109375
Iteration 13700: Loss = -12378.740234375
Iteration 13800: Loss = -12378.740234375
Iteration 13900: Loss = -12378.7392578125
Iteration 14000: Loss = -12378.7392578125
Iteration 14100: Loss = -12378.7392578125
Iteration 14200: Loss = -12378.736328125
Iteration 14300: Loss = -12378.736328125
Iteration 14400: Loss = -12378.736328125
Iteration 14500: Loss = -12378.736328125
Iteration 14600: Loss = -12378.7333984375
Iteration 14700: Loss = -12378.734375
1
Iteration 14800: Loss = -12378.7333984375
Iteration 14900: Loss = -12378.7353515625
1
Iteration 15000: Loss = -12378.7333984375
Iteration 15100: Loss = -12378.7314453125
Iteration 15200: Loss = -12378.732421875
1
Iteration 15300: Loss = -12378.732421875
2
Iteration 15400: Loss = -12378.732421875
3
Iteration 15500: Loss = -12378.7314453125
Iteration 15600: Loss = -12378.732421875
1
Iteration 15700: Loss = -12378.732421875
2
Iteration 15800: Loss = -12378.73046875
Iteration 15900: Loss = -12378.7314453125
1
Iteration 16000: Loss = -12378.73046875
Iteration 16100: Loss = -12378.73046875
Iteration 16200: Loss = -12378.73046875
Iteration 16300: Loss = -12378.7314453125
1
Iteration 16400: Loss = -12378.73046875
Iteration 16500: Loss = -12378.7294921875
Iteration 16600: Loss = -12378.7294921875
Iteration 16700: Loss = -12378.7294921875
Iteration 16800: Loss = -12378.73046875
1
Iteration 16900: Loss = -12378.73046875
2
Iteration 17000: Loss = -12378.7294921875
Iteration 17100: Loss = -12378.7294921875
Iteration 17200: Loss = -12378.7294921875
Iteration 17300: Loss = -12378.73046875
1
Iteration 17400: Loss = -12378.7275390625
Iteration 17500: Loss = -12378.7294921875
1
Iteration 17600: Loss = -12378.7275390625
Iteration 17700: Loss = -12378.7275390625
Iteration 17800: Loss = -12378.728515625
1
Iteration 17900: Loss = -12378.728515625
2
Iteration 18000: Loss = -12378.7275390625
Iteration 18100: Loss = -12378.728515625
1
Iteration 18200: Loss = -12378.728515625
2
Iteration 18300: Loss = -12378.7265625
Iteration 18400: Loss = -12378.7275390625
1
Iteration 18500: Loss = -12378.7265625
Iteration 18600: Loss = -12378.728515625
1
Iteration 18700: Loss = -12378.728515625
2
Iteration 18800: Loss = -12378.7265625
Iteration 18900: Loss = -12378.7275390625
1
Iteration 19000: Loss = -12378.7255859375
Iteration 19100: Loss = -12378.7275390625
1
Iteration 19200: Loss = -12378.7265625
2
Iteration 19300: Loss = -12378.7255859375
Iteration 19400: Loss = -12378.7255859375
Iteration 19500: Loss = -12378.7275390625
1
Iteration 19600: Loss = -12378.7265625
2
Iteration 19700: Loss = -12378.7265625
3
Iteration 19800: Loss = -12378.7255859375
Iteration 19900: Loss = -12378.7265625
1
Iteration 20000: Loss = -12378.728515625
2
Iteration 20100: Loss = -12378.7265625
3
Iteration 20200: Loss = -12378.7255859375
Iteration 20300: Loss = -12378.7265625
1
Iteration 20400: Loss = -12378.7265625
2
Iteration 20500: Loss = -12378.7275390625
3
Iteration 20600: Loss = -12378.7265625
4
Iteration 20700: Loss = -12378.7275390625
5
Iteration 20800: Loss = -12378.7265625
6
Iteration 20900: Loss = -12378.7275390625
7
Iteration 21000: Loss = -12378.7275390625
8
Iteration 21100: Loss = -12378.7265625
9
Iteration 21200: Loss = -12378.7275390625
10
Iteration 21300: Loss = -12378.7265625
11
Iteration 21400: Loss = -12378.7275390625
12
Iteration 21500: Loss = -12378.7265625
13
Iteration 21600: Loss = -12378.7265625
14
Iteration 21700: Loss = -12378.7265625
15
Stopping early at iteration 21700 due to no improvement.
pi: tensor([[9.7428e-01, 2.5723e-02],
        [9.9999e-01, 5.4229e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9986e-01, 1.4096e-04], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2002, 0.1767],
         [0.9784, 0.8006]],

        [[0.9623, 0.0656],
         [0.8583, 0.8426]],

        [[0.8829, 0.8165],
         [0.0245, 0.5010]],

        [[0.0975, 0.1682],
         [0.0091, 0.7284]],

        [[0.7114, 0.2550],
         [0.0597, 0.0116]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.029240383366564728
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.000870906178304837
Average Adjusted Rand Index: 0.006543829680995218
[0.0007493607032682356, -0.000870906178304837] [0.0004925310074841425, 0.006543829680995218] [12369.7861328125, 12378.7265625]
-------------------------------------
This iteration is 47
True Objective function: Loss = -12000.855364882842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35143.0
Iteration 100: Loss = -22321.876953125
Iteration 200: Loss = -14866.3037109375
Iteration 300: Loss = -13074.060546875
Iteration 400: Loss = -12801.005859375
Iteration 500: Loss = -12679.423828125
Iteration 600: Loss = -12621.6015625
Iteration 700: Loss = -12597.259765625
Iteration 800: Loss = -12577.53125
Iteration 900: Loss = -12566.5947265625
Iteration 1000: Loss = -12557.0224609375
Iteration 1100: Loss = -12551.986328125
Iteration 1200: Loss = -12548.24609375
Iteration 1300: Loss = -12545.2392578125
Iteration 1400: Loss = -12542.787109375
Iteration 1500: Loss = -12540.7724609375
Iteration 1600: Loss = -12539.126953125
Iteration 1700: Loss = -12537.77734375
Iteration 1800: Loss = -12536.658203125
Iteration 1900: Loss = -12535.7177734375
Iteration 2000: Loss = -12534.919921875
Iteration 2100: Loss = -12534.23828125
Iteration 2200: Loss = -12533.6474609375
Iteration 2300: Loss = -12533.1328125
Iteration 2400: Loss = -12532.681640625
Iteration 2500: Loss = -12532.2783203125
Iteration 2600: Loss = -12531.9208984375
Iteration 2700: Loss = -12531.6005859375
Iteration 2800: Loss = -12531.310546875
Iteration 2900: Loss = -12531.0498046875
Iteration 3000: Loss = -12530.8134765625
Iteration 3100: Loss = -12530.5986328125
Iteration 3200: Loss = -12530.3994140625
Iteration 3300: Loss = -12530.2177734375
Iteration 3400: Loss = -12530.052734375
Iteration 3500: Loss = -12529.8974609375
Iteration 3600: Loss = -12529.7548828125
Iteration 3700: Loss = -12529.62109375
Iteration 3800: Loss = -12529.4951171875
Iteration 3900: Loss = -12529.3818359375
Iteration 4000: Loss = -12529.2724609375
Iteration 4100: Loss = -12529.169921875
Iteration 4200: Loss = -12529.0732421875
Iteration 4300: Loss = -12528.982421875
Iteration 4400: Loss = -12528.896484375
Iteration 4500: Loss = -12528.814453125
Iteration 4600: Loss = -12528.73828125
Iteration 4700: Loss = -12528.6650390625
Iteration 4800: Loss = -12528.5966796875
Iteration 4900: Loss = -12528.5322265625
Iteration 5000: Loss = -12528.470703125
Iteration 5100: Loss = -12528.4130859375
Iteration 5200: Loss = -12528.359375
Iteration 5300: Loss = -12528.3076171875
Iteration 5400: Loss = -12528.2587890625
Iteration 5500: Loss = -12528.212890625
Iteration 5600: Loss = -12528.171875
Iteration 5700: Loss = -12528.130859375
Iteration 5800: Loss = -12528.0947265625
Iteration 5900: Loss = -12528.05859375
Iteration 6000: Loss = -12528.025390625
Iteration 6100: Loss = -12527.994140625
Iteration 6200: Loss = -12527.9658203125
Iteration 6300: Loss = -12527.9404296875
Iteration 6400: Loss = -12527.9140625
Iteration 6500: Loss = -12527.888671875
Iteration 6600: Loss = -12527.8671875
Iteration 6700: Loss = -12527.84765625
Iteration 6800: Loss = -12527.8291015625
Iteration 6900: Loss = -12527.8115234375
Iteration 7000: Loss = -12527.7919921875
Iteration 7100: Loss = -12527.7763671875
Iteration 7200: Loss = -12527.76171875
Iteration 7300: Loss = -12527.748046875
Iteration 7400: Loss = -12527.7333984375
Iteration 7500: Loss = -12527.7216796875
Iteration 7600: Loss = -12527.7099609375
Iteration 7700: Loss = -12527.701171875
Iteration 7800: Loss = -12527.689453125
Iteration 7900: Loss = -12527.6806640625
Iteration 8000: Loss = -12527.6708984375
Iteration 8100: Loss = -12527.662109375
Iteration 8200: Loss = -12527.654296875
Iteration 8300: Loss = -12527.646484375
Iteration 8400: Loss = -12527.640625
Iteration 8500: Loss = -12527.6337890625
Iteration 8600: Loss = -12527.6259765625
Iteration 8700: Loss = -12527.619140625
Iteration 8800: Loss = -12527.615234375
Iteration 8900: Loss = -12527.6083984375
Iteration 9000: Loss = -12527.603515625
Iteration 9100: Loss = -12527.5986328125
Iteration 9200: Loss = -12527.5947265625
Iteration 9300: Loss = -12527.5888671875
Iteration 9400: Loss = -12527.583984375
Iteration 9500: Loss = -12527.5791015625
Iteration 9600: Loss = -12527.576171875
Iteration 9700: Loss = -12527.572265625
Iteration 9800: Loss = -12527.5673828125
Iteration 9900: Loss = -12527.5634765625
Iteration 10000: Loss = -12527.55859375
Iteration 10100: Loss = -12527.5556640625
Iteration 10200: Loss = -12527.5498046875
Iteration 10300: Loss = -12527.5458984375
Iteration 10400: Loss = -12527.541015625
Iteration 10500: Loss = -12527.533203125
Iteration 10600: Loss = -12527.5263671875
Iteration 10700: Loss = -12527.5166015625
Iteration 10800: Loss = -12527.505859375
Iteration 10900: Loss = -12527.490234375
Iteration 11000: Loss = -12527.4716796875
Iteration 11100: Loss = -12527.453125
Iteration 11200: Loss = -12527.439453125
Iteration 11300: Loss = -12527.4267578125
Iteration 11400: Loss = -12527.421875
Iteration 11500: Loss = -12527.412109375
Iteration 11600: Loss = -12527.40625
Iteration 11700: Loss = -12527.3974609375
Iteration 11800: Loss = -12527.388671875
Iteration 11900: Loss = -12527.380859375
Iteration 12000: Loss = -12527.37109375
Iteration 12100: Loss = -12527.3623046875
Iteration 12200: Loss = -12527.3525390625
Iteration 12300: Loss = -12527.333984375
Iteration 12400: Loss = -12527.3134765625
Iteration 12500: Loss = -12527.2841796875
Iteration 12600: Loss = -12527.2490234375
Iteration 12700: Loss = -12527.2216796875
Iteration 12800: Loss = -12527.193359375
Iteration 12900: Loss = -12527.1650390625
Iteration 13000: Loss = -12527.123046875
Iteration 13100: Loss = -12527.0595703125
Iteration 13200: Loss = -12526.9638671875
Iteration 13300: Loss = -12526.82421875
Iteration 13400: Loss = -12526.5830078125
Iteration 13500: Loss = -12526.37890625
Iteration 13600: Loss = -12526.341796875
Iteration 13700: Loss = -12526.3232421875
Iteration 13800: Loss = -12526.3017578125
Iteration 13900: Loss = -12526.2861328125
Iteration 14000: Loss = -12526.2763671875
Iteration 14100: Loss = -12526.2529296875
Iteration 14200: Loss = -12526.23828125
Iteration 14300: Loss = -12526.2275390625
Iteration 14400: Loss = -12526.2138671875
Iteration 14500: Loss = -12526.1904296875
Iteration 14600: Loss = -12526.1787109375
Iteration 14700: Loss = -12526.1708984375
Iteration 14800: Loss = -12526.1572265625
Iteration 14900: Loss = -12526.1455078125
Iteration 15000: Loss = -12526.125
Iteration 15100: Loss = -12526.107421875
Iteration 15200: Loss = -12526.0947265625
Iteration 15300: Loss = -12526.08203125
Iteration 15400: Loss = -12526.0654296875
Iteration 15500: Loss = -12526.0546875
Iteration 15600: Loss = -12526.03515625
Iteration 15700: Loss = -12526.0302734375
Iteration 15800: Loss = -12526.0146484375
Iteration 15900: Loss = -12526.0
Iteration 16000: Loss = -12525.9833984375
Iteration 16100: Loss = -12525.9736328125
Iteration 16200: Loss = -12525.970703125
Iteration 16300: Loss = -12525.970703125
Iteration 16400: Loss = -12525.9716796875
1
Iteration 16500: Loss = -12525.9697265625
Iteration 16600: Loss = -12525.9384765625
Iteration 16700: Loss = -12525.9365234375
Iteration 16800: Loss = -12525.9326171875
Iteration 16900: Loss = -12525.9306640625
Iteration 17000: Loss = -12525.923828125
Iteration 17100: Loss = -12525.923828125
Iteration 17200: Loss = -12525.9267578125
1
Iteration 17300: Loss = -12525.9228515625
Iteration 17400: Loss = -12525.923828125
1
Iteration 17500: Loss = -12525.923828125
2
Iteration 17600: Loss = -12525.9228515625
Iteration 17700: Loss = -12525.923828125
1
Iteration 17800: Loss = -12525.9228515625
Iteration 17900: Loss = -12525.9228515625
Iteration 18000: Loss = -12525.9228515625
Iteration 18100: Loss = -12525.9228515625
Iteration 18200: Loss = -12525.923828125
1
Iteration 18300: Loss = -12525.923828125
2
Iteration 18400: Loss = -12525.921875
Iteration 18500: Loss = -12525.923828125
1
Iteration 18600: Loss = -12525.9228515625
2
Iteration 18700: Loss = -12525.923828125
3
Iteration 18800: Loss = -12525.9228515625
4
Iteration 18900: Loss = -12525.923828125
5
Iteration 19000: Loss = -12525.921875
Iteration 19100: Loss = -12525.9228515625
1
Iteration 19200: Loss = -12525.9228515625
2
Iteration 19300: Loss = -12525.923828125
3
Iteration 19400: Loss = -12525.9228515625
4
Iteration 19500: Loss = -12525.921875
Iteration 19600: Loss = -12525.921875
Iteration 19700: Loss = -12525.9228515625
1
Iteration 19800: Loss = -12525.9228515625
2
Iteration 19900: Loss = -12525.921875
Iteration 20000: Loss = -12525.9228515625
1
Iteration 20100: Loss = -12525.921875
Iteration 20200: Loss = -12525.921875
Iteration 20300: Loss = -12525.921875
Iteration 20400: Loss = -12525.9228515625
1
Iteration 20500: Loss = -12525.9248046875
2
Iteration 20600: Loss = -12525.9228515625
3
Iteration 20700: Loss = -12525.921875
Iteration 20800: Loss = -12525.9228515625
1
Iteration 20900: Loss = -12525.9248046875
2
Iteration 21000: Loss = -12525.923828125
3
Iteration 21100: Loss = -12525.921875
Iteration 21200: Loss = -12525.9228515625
1
Iteration 21300: Loss = -12525.9228515625
2
Iteration 21400: Loss = -12525.9228515625
3
Iteration 21500: Loss = -12525.9228515625
4
Iteration 21600: Loss = -12525.92578125
5
Iteration 21700: Loss = -12525.921875
Iteration 21800: Loss = -12525.92578125
1
Iteration 21900: Loss = -12525.9228515625
2
Iteration 22000: Loss = -12525.9228515625
3
Iteration 22100: Loss = -12525.9228515625
4
Iteration 22200: Loss = -12525.923828125
5
Iteration 22300: Loss = -12525.9228515625
6
Iteration 22400: Loss = -12525.923828125
7
Iteration 22500: Loss = -12525.921875
Iteration 22600: Loss = -12525.9228515625
1
Iteration 22700: Loss = -12525.921875
Iteration 22800: Loss = -12525.9228515625
1
Iteration 22900: Loss = -12525.921875
Iteration 23000: Loss = -12525.9228515625
1
Iteration 23100: Loss = -12525.9228515625
2
Iteration 23200: Loss = -12525.9228515625
3
Iteration 23300: Loss = -12525.923828125
4
Iteration 23400: Loss = -12525.9228515625
5
Iteration 23500: Loss = -12525.9228515625
6
Iteration 23600: Loss = -12525.9228515625
7
Iteration 23700: Loss = -12525.9228515625
8
Iteration 23800: Loss = -12525.9228515625
9
Iteration 23900: Loss = -12525.9228515625
10
Iteration 24000: Loss = -12525.9228515625
11
Iteration 24100: Loss = -12525.9228515625
12
Iteration 24200: Loss = -12525.921875
Iteration 24300: Loss = -12525.921875
Iteration 24400: Loss = -12525.923828125
1
Iteration 24500: Loss = -12525.921875
Iteration 24600: Loss = -12525.9228515625
1
Iteration 24700: Loss = -12525.9228515625
2
Iteration 24800: Loss = -12525.923828125
3
Iteration 24900: Loss = -12525.9228515625
4
Iteration 25000: Loss = -12525.9228515625
5
Iteration 25100: Loss = -12525.9228515625
6
Iteration 25200: Loss = -12525.923828125
7
Iteration 25300: Loss = -12525.923828125
8
Iteration 25400: Loss = -12525.923828125
9
Iteration 25500: Loss = -12525.9228515625
10
Iteration 25600: Loss = -12525.9228515625
11
Iteration 25700: Loss = -12525.9228515625
12
Iteration 25800: Loss = -12525.921875
Iteration 25900: Loss = -12525.9228515625
1
Iteration 26000: Loss = -12525.923828125
2
Iteration 26100: Loss = -12525.9150390625
Iteration 26200: Loss = -12525.9150390625
Iteration 26300: Loss = -12525.9140625
Iteration 26400: Loss = -12525.908203125
Iteration 26500: Loss = -12525.9033203125
Iteration 26600: Loss = -12525.9033203125
Iteration 26700: Loss = -12525.8916015625
Iteration 26800: Loss = -12525.8916015625
Iteration 26900: Loss = -12525.884765625
Iteration 27000: Loss = -12525.880859375
Iteration 27100: Loss = -12525.8818359375
1
Iteration 27200: Loss = -12525.8818359375
2
Iteration 27300: Loss = -12525.8798828125
Iteration 27400: Loss = -12525.8671875
Iteration 27500: Loss = -12525.85546875
Iteration 27600: Loss = -12525.85546875
Iteration 27700: Loss = -12525.853515625
Iteration 27800: Loss = -12525.8544921875
1
Iteration 27900: Loss = -12525.833984375
Iteration 28000: Loss = -12525.8359375
1
Iteration 28100: Loss = -12525.83203125
Iteration 28200: Loss = -12525.8330078125
1
Iteration 28300: Loss = -12525.8330078125
2
Iteration 28400: Loss = -12525.83203125
Iteration 28500: Loss = -12525.8330078125
1
Iteration 28600: Loss = -12525.83203125
Iteration 28700: Loss = -12525.8359375
1
Iteration 28800: Loss = -12525.8154296875
Iteration 28900: Loss = -12525.814453125
Iteration 29000: Loss = -12525.818359375
1
Iteration 29100: Loss = -12525.8154296875
2
Iteration 29200: Loss = -12525.8154296875
3
Iteration 29300: Loss = -12525.8173828125
4
Iteration 29400: Loss = -12525.8154296875
5
Iteration 29500: Loss = -12525.81640625
6
Iteration 29600: Loss = -12525.8154296875
7
Iteration 29700: Loss = -12525.81640625
8
Iteration 29800: Loss = -12525.81640625
9
Iteration 29900: Loss = -12525.81640625
10
pi: tensor([[2.7307e-05, 9.9997e-01],
        [1.0338e-02, 9.8966e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0815, 0.9185], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1201, 0.1557],
         [0.2581, 0.2053]],

        [[0.0074, 0.1773],
         [0.2153, 0.0716]],

        [[0.9807, 0.2503],
         [0.0093, 0.2231]],

        [[0.7882, 0.3367],
         [0.0656, 0.9472]],

        [[0.1817, 0.1937],
         [0.0703, 0.2273]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00012438889750662795
Average Adjusted Rand Index: -0.0003091481305245508
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17364.00390625
Iteration 100: Loss = -13684.3681640625
Iteration 200: Loss = -12732.9072265625
Iteration 300: Loss = -12579.748046875
Iteration 400: Loss = -12549.7861328125
Iteration 500: Loss = -12537.427734375
Iteration 600: Loss = -12532.6669921875
Iteration 700: Loss = -12530.4931640625
Iteration 800: Loss = -12529.2919921875
Iteration 900: Loss = -12528.5634765625
Iteration 1000: Loss = -12528.1044921875
Iteration 1100: Loss = -12527.7783203125
Iteration 1200: Loss = -12527.5458984375
Iteration 1300: Loss = -12527.3681640625
Iteration 1400: Loss = -12527.228515625
Iteration 1500: Loss = -12527.115234375
Iteration 1600: Loss = -12527.021484375
Iteration 1700: Loss = -12526.9375
Iteration 1800: Loss = -12526.869140625
Iteration 1900: Loss = -12526.8134765625
Iteration 2000: Loss = -12526.7685546875
Iteration 2100: Loss = -12526.7314453125
Iteration 2200: Loss = -12526.7001953125
Iteration 2300: Loss = -12526.673828125
Iteration 2400: Loss = -12526.6513671875
Iteration 2500: Loss = -12526.630859375
Iteration 2600: Loss = -12526.61328125
Iteration 2700: Loss = -12526.59765625
Iteration 2800: Loss = -12526.5830078125
Iteration 2900: Loss = -12526.5703125
Iteration 3000: Loss = -12526.5576171875
Iteration 3100: Loss = -12526.5439453125
Iteration 3200: Loss = -12526.5322265625
Iteration 3300: Loss = -12526.521484375
Iteration 3400: Loss = -12526.5107421875
Iteration 3500: Loss = -12526.5
Iteration 3600: Loss = -12526.48828125
Iteration 3700: Loss = -12526.478515625
Iteration 3800: Loss = -12526.466796875
Iteration 3900: Loss = -12526.455078125
Iteration 4000: Loss = -12526.4423828125
Iteration 4100: Loss = -12526.4296875
Iteration 4200: Loss = -12526.4150390625
Iteration 4300: Loss = -12526.3994140625
Iteration 4400: Loss = -12526.3818359375
Iteration 4500: Loss = -12526.3623046875
Iteration 4600: Loss = -12526.33984375
Iteration 4700: Loss = -12526.3154296875
Iteration 4800: Loss = -12526.2900390625
Iteration 4900: Loss = -12526.2607421875
Iteration 5000: Loss = -12526.232421875
Iteration 5100: Loss = -12526.205078125
Iteration 5200: Loss = -12526.181640625
Iteration 5300: Loss = -12526.1630859375
Iteration 5400: Loss = -12526.146484375
Iteration 5500: Loss = -12526.1337890625
Iteration 5600: Loss = -12526.123046875
Iteration 5700: Loss = -12526.1123046875
Iteration 5800: Loss = -12526.103515625
Iteration 5900: Loss = -12526.099609375
Iteration 6000: Loss = -12526.0908203125
Iteration 6100: Loss = -12526.087890625
Iteration 6200: Loss = -12526.0849609375
Iteration 6300: Loss = -12526.0791015625
Iteration 6400: Loss = -12526.0751953125
Iteration 6500: Loss = -12526.0712890625
Iteration 6600: Loss = -12526.0703125
Iteration 6700: Loss = -12526.0673828125
Iteration 6800: Loss = -12526.064453125
Iteration 6900: Loss = -12526.0615234375
Iteration 7000: Loss = -12526.0615234375
Iteration 7100: Loss = -12526.0595703125
Iteration 7200: Loss = -12526.0556640625
Iteration 7300: Loss = -12526.0546875
Iteration 7400: Loss = -12526.0556640625
1
Iteration 7500: Loss = -12526.0537109375
Iteration 7600: Loss = -12526.052734375
Iteration 7700: Loss = -12526.0517578125
Iteration 7800: Loss = -12526.048828125
Iteration 7900: Loss = -12526.048828125
Iteration 8000: Loss = -12526.048828125
Iteration 8100: Loss = -12526.046875
Iteration 8200: Loss = -12526.046875
Iteration 8300: Loss = -12526.046875
Iteration 8400: Loss = -12526.0458984375
Iteration 8500: Loss = -12526.044921875
Iteration 8600: Loss = -12526.044921875
Iteration 8700: Loss = -12526.0439453125
Iteration 8800: Loss = -12526.0419921875
Iteration 8900: Loss = -12526.04296875
1
Iteration 9000: Loss = -12526.0419921875
Iteration 9100: Loss = -12526.0400390625
Iteration 9200: Loss = -12526.0400390625
Iteration 9300: Loss = -12526.0390625
Iteration 9400: Loss = -12526.037109375
Iteration 9500: Loss = -12526.037109375
Iteration 9600: Loss = -12526.03515625
Iteration 9700: Loss = -12526.037109375
1
Iteration 9800: Loss = -12526.03125
Iteration 9900: Loss = -12526.029296875
Iteration 10000: Loss = -12526.0263671875
Iteration 10100: Loss = -12526.02734375
1
Iteration 10200: Loss = -12526.0205078125
Iteration 10300: Loss = -12526.0166015625
Iteration 10400: Loss = -12526.0126953125
Iteration 10500: Loss = -12526.0068359375
Iteration 10600: Loss = -12526.001953125
Iteration 10700: Loss = -12525.998046875
Iteration 10800: Loss = -12525.9921875
Iteration 10900: Loss = -12525.98828125
Iteration 11000: Loss = -12525.984375
Iteration 11100: Loss = -12525.9794921875
Iteration 11200: Loss = -12525.974609375
Iteration 11300: Loss = -12525.9697265625
Iteration 11400: Loss = -12525.9677734375
Iteration 11500: Loss = -12525.96484375
Iteration 11600: Loss = -12525.9609375
Iteration 11700: Loss = -12525.958984375
Iteration 11800: Loss = -12525.95703125
Iteration 11900: Loss = -12525.955078125
Iteration 12000: Loss = -12525.953125
Iteration 12100: Loss = -12525.951171875
Iteration 12200: Loss = -12525.951171875
Iteration 12300: Loss = -12525.94921875
Iteration 12400: Loss = -12525.9482421875
Iteration 12500: Loss = -12525.947265625
Iteration 12600: Loss = -12525.947265625
Iteration 12700: Loss = -12525.9462890625
Iteration 12800: Loss = -12525.9443359375
Iteration 12900: Loss = -12525.9462890625
1
Iteration 13000: Loss = -12525.9443359375
Iteration 13100: Loss = -12525.943359375
Iteration 13200: Loss = -12525.943359375
Iteration 13300: Loss = -12525.943359375
Iteration 13400: Loss = -12525.9423828125
Iteration 13500: Loss = -12525.9423828125
Iteration 13600: Loss = -12525.9423828125
Iteration 13700: Loss = -12525.943359375
1
Iteration 13800: Loss = -12525.943359375
2
Iteration 13900: Loss = -12525.9423828125
Iteration 14000: Loss = -12525.943359375
1
Iteration 14100: Loss = -12525.9423828125
Iteration 14200: Loss = -12525.943359375
1
Iteration 14300: Loss = -12525.9423828125
Iteration 14400: Loss = -12525.9423828125
Iteration 14500: Loss = -12525.9423828125
Iteration 14600: Loss = -12525.94140625
Iteration 14700: Loss = -12525.94140625
Iteration 14800: Loss = -12525.94140625
Iteration 14900: Loss = -12525.943359375
1
Iteration 15000: Loss = -12525.9423828125
2
Iteration 15100: Loss = -12525.9423828125
3
Iteration 15200: Loss = -12525.9423828125
4
Iteration 15300: Loss = -12525.9423828125
5
Iteration 15400: Loss = -12525.9423828125
6
Iteration 15500: Loss = -12525.9423828125
7
Iteration 15600: Loss = -12525.9443359375
8
Iteration 15700: Loss = -12525.9423828125
9
Iteration 15800: Loss = -12525.9423828125
10
Iteration 15900: Loss = -12525.94140625
Iteration 16000: Loss = -12525.94140625
Iteration 16100: Loss = -12525.94140625
Iteration 16200: Loss = -12525.9423828125
1
Iteration 16300: Loss = -12525.94140625
Iteration 16400: Loss = -12525.9423828125
1
Iteration 16500: Loss = -12525.9423828125
2
Iteration 16600: Loss = -12525.9423828125
3
Iteration 16700: Loss = -12525.9423828125
4
Iteration 16800: Loss = -12525.9423828125
5
Iteration 16900: Loss = -12525.9443359375
6
Iteration 17000: Loss = -12525.9423828125
7
Iteration 17100: Loss = -12525.9423828125
8
Iteration 17200: Loss = -12525.9423828125
9
Iteration 17300: Loss = -12525.94140625
Iteration 17400: Loss = -12525.94140625
Iteration 17500: Loss = -12525.9423828125
1
Iteration 17600: Loss = -12525.9423828125
2
Iteration 17700: Loss = -12525.9423828125
3
Iteration 17800: Loss = -12525.943359375
4
Iteration 17900: Loss = -12525.9423828125
5
Iteration 18000: Loss = -12525.9423828125
6
Iteration 18100: Loss = -12525.9423828125
7
Iteration 18200: Loss = -12525.9443359375
8
Iteration 18300: Loss = -12525.9423828125
9
Iteration 18400: Loss = -12525.9423828125
10
Iteration 18500: Loss = -12525.9423828125
11
Iteration 18600: Loss = -12525.94140625
Iteration 18700: Loss = -12525.9423828125
1
Iteration 18800: Loss = -12525.94140625
Iteration 18900: Loss = -12525.9423828125
1
Iteration 19000: Loss = -12525.9423828125
2
Iteration 19100: Loss = -12525.9423828125
3
Iteration 19200: Loss = -12525.9423828125
4
Iteration 19300: Loss = -12525.9423828125
5
Iteration 19400: Loss = -12525.9443359375
6
Iteration 19500: Loss = -12525.9423828125
7
Iteration 19600: Loss = -12525.9423828125
8
Iteration 19700: Loss = -12525.9423828125
9
Iteration 19800: Loss = -12525.94140625
Iteration 19900: Loss = -12525.943359375
1
Iteration 20000: Loss = -12525.94140625
Iteration 20100: Loss = -12525.9423828125
1
Iteration 20200: Loss = -12525.9423828125
2
Iteration 20300: Loss = -12525.9423828125
3
Iteration 20400: Loss = -12525.9423828125
4
Iteration 20500: Loss = -12525.9423828125
5
Iteration 20600: Loss = -12525.9423828125
6
Iteration 20700: Loss = -12525.943359375
7
Iteration 20800: Loss = -12525.9423828125
8
Iteration 20900: Loss = -12525.943359375
9
Iteration 21000: Loss = -12525.9423828125
10
Iteration 21100: Loss = -12525.943359375
11
Iteration 21200: Loss = -12525.9423828125
12
Iteration 21300: Loss = -12525.943359375
13
Iteration 21400: Loss = -12525.943359375
14
Iteration 21500: Loss = -12525.9423828125
15
Stopping early at iteration 21500 due to no improvement.
pi: tensor([[0.9385, 0.0615],
        [0.5317, 0.4683]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0243, 0.9757], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2067, 0.1417],
         [0.8557, 0.2013]],

        [[0.0116, 0.1980],
         [0.8463, 0.0393]],

        [[0.0860, 0.2152],
         [0.8783, 0.7355]],

        [[0.9917, 0.2081],
         [0.9746, 0.5474]],

        [[0.6184, 0.1989],
         [0.3997, 0.9565]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.004166919759460609
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0013800116272139585
Average Adjusted Rand Index: 0.0008333839518921218
[-0.00012438889750662795, -0.0013800116272139585] [-0.0003091481305245508, 0.0008333839518921218] [12525.8154296875, 12525.9423828125]
-------------------------------------
This iteration is 48
True Objective function: Loss = -11937.839452549195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -57771.65234375
Iteration 100: Loss = -33737.88671875
Iteration 200: Loss = -18013.591796875
Iteration 300: Loss = -13922.8388671875
Iteration 400: Loss = -13067.3857421875
Iteration 500: Loss = -12790.9775390625
Iteration 600: Loss = -12670.34765625
Iteration 700: Loss = -12616.26953125
Iteration 800: Loss = -12570.4423828125
Iteration 900: Loss = -12541.4443359375
Iteration 1000: Loss = -12523.4951171875
Iteration 1100: Loss = -12508.677734375
Iteration 1200: Loss = -12497.076171875
Iteration 1300: Loss = -12489.6826171875
Iteration 1400: Loss = -12484.05078125
Iteration 1500: Loss = -12479.5498046875
Iteration 1600: Loss = -12475.8642578125
Iteration 1700: Loss = -12472.744140625
Iteration 1800: Loss = -12468.705078125
Iteration 1900: Loss = -12465.3271484375
Iteration 2000: Loss = -12463.1796875
Iteration 2100: Loss = -12461.4150390625
Iteration 2200: Loss = -12459.9130859375
Iteration 2300: Loss = -12458.619140625
Iteration 2400: Loss = -12457.484375
Iteration 2500: Loss = -12456.4814453125
Iteration 2600: Loss = -12455.5859375
Iteration 2700: Loss = -12454.78515625
Iteration 2800: Loss = -12454.064453125
Iteration 2900: Loss = -12453.4111328125
Iteration 3000: Loss = -12452.822265625
Iteration 3100: Loss = -12452.28125
Iteration 3200: Loss = -12451.78515625
Iteration 3300: Loss = -12451.3349609375
Iteration 3400: Loss = -12450.9228515625
Iteration 3500: Loss = -12450.5439453125
Iteration 3600: Loss = -12450.1953125
Iteration 3700: Loss = -12449.8720703125
Iteration 3800: Loss = -12449.5751953125
Iteration 3900: Loss = -12449.3017578125
Iteration 4000: Loss = -12449.048828125
Iteration 4100: Loss = -12448.8125
Iteration 4200: Loss = -12448.5947265625
Iteration 4300: Loss = -12448.390625
Iteration 4400: Loss = -12448.203125
Iteration 4500: Loss = -12448.0244140625
Iteration 4600: Loss = -12447.861328125
Iteration 4700: Loss = -12447.708984375
Iteration 4800: Loss = -12447.564453125
Iteration 4900: Loss = -12447.431640625
Iteration 5000: Loss = -12447.3056640625
Iteration 5100: Loss = -12447.1884765625
Iteration 5200: Loss = -12447.078125
Iteration 5300: Loss = -12446.9736328125
Iteration 5400: Loss = -12446.8779296875
Iteration 5500: Loss = -12446.7861328125
Iteration 5600: Loss = -12446.69921875
Iteration 5700: Loss = -12446.6162109375
Iteration 5800: Loss = -12446.541015625
Iteration 5900: Loss = -12446.46875
Iteration 6000: Loss = -12446.3994140625
Iteration 6100: Loss = -12446.3330078125
Iteration 6200: Loss = -12446.271484375
Iteration 6300: Loss = -12446.2119140625
Iteration 6400: Loss = -12446.1513671875
Iteration 6500: Loss = -12446.0791015625
Iteration 6600: Loss = -12440.7568359375
Iteration 6700: Loss = -12440.201171875
Iteration 6800: Loss = -12439.994140625
Iteration 6900: Loss = -12439.845703125
Iteration 7000: Loss = -12439.7373046875
Iteration 7100: Loss = -12439.65234375
Iteration 7200: Loss = -12439.5771484375
Iteration 7300: Loss = -12439.5126953125
Iteration 7400: Loss = -12439.4560546875
Iteration 7500: Loss = -12439.40625
Iteration 7600: Loss = -12439.365234375
Iteration 7700: Loss = -12439.328125
Iteration 7800: Loss = -12439.2978515625
Iteration 7900: Loss = -12439.26953125
Iteration 8000: Loss = -12439.2421875
Iteration 8100: Loss = -12439.2158203125
Iteration 8200: Loss = -12439.19140625
Iteration 8300: Loss = -12439.166015625
Iteration 8400: Loss = -12439.1474609375
Iteration 8500: Loss = -12439.130859375
Iteration 8600: Loss = -12439.1162109375
Iteration 8700: Loss = -12439.1025390625
Iteration 8800: Loss = -12439.087890625
Iteration 8900: Loss = -12439.0771484375
Iteration 9000: Loss = -12439.064453125
Iteration 9100: Loss = -12439.052734375
Iteration 9200: Loss = -12439.0439453125
Iteration 9300: Loss = -12439.033203125
Iteration 9400: Loss = -12439.0224609375
Iteration 9500: Loss = -12439.0146484375
Iteration 9600: Loss = -12439.0068359375
Iteration 9700: Loss = -12438.998046875
Iteration 9800: Loss = -12438.990234375
Iteration 9900: Loss = -12438.9833984375
Iteration 10000: Loss = -12438.9775390625
Iteration 10100: Loss = -12438.96875
Iteration 10200: Loss = -12438.962890625
Iteration 10300: Loss = -12438.9580078125
Iteration 10400: Loss = -12438.9521484375
Iteration 10500: Loss = -12438.9462890625
Iteration 10600: Loss = -12438.94140625
Iteration 10700: Loss = -12438.9365234375
Iteration 10800: Loss = -12438.9326171875
Iteration 10900: Loss = -12438.9287109375
Iteration 11000: Loss = -12438.921875
Iteration 11100: Loss = -12438.919921875
Iteration 11200: Loss = -12438.9150390625
Iteration 11300: Loss = -12438.9111328125
Iteration 11400: Loss = -12438.9072265625
Iteration 11500: Loss = -12438.90234375
Iteration 11600: Loss = -12438.9013671875
Iteration 11700: Loss = -12438.8974609375
Iteration 11800: Loss = -12438.8935546875
Iteration 11900: Loss = -12438.890625
Iteration 12000: Loss = -12438.8876953125
Iteration 12100: Loss = -12438.8876953125
Iteration 12200: Loss = -12438.8828125
Iteration 12300: Loss = -12438.880859375
Iteration 12400: Loss = -12438.87890625
Iteration 12500: Loss = -12438.87890625
Iteration 12600: Loss = -12438.875
Iteration 12700: Loss = -12438.873046875
Iteration 12800: Loss = -12438.8701171875
Iteration 12900: Loss = -12438.869140625
Iteration 13000: Loss = -12438.8681640625
Iteration 13100: Loss = -12438.8662109375
Iteration 13200: Loss = -12438.8642578125
Iteration 13300: Loss = -12438.861328125
Iteration 13400: Loss = -12438.8603515625
Iteration 13500: Loss = -12438.8564453125
Iteration 13600: Loss = -12438.8525390625
Iteration 13700: Loss = -12438.8515625
Iteration 13800: Loss = -12438.8486328125
Iteration 13900: Loss = -12438.8466796875
Iteration 14000: Loss = -12438.8447265625
Iteration 14100: Loss = -12438.8408203125
Iteration 14200: Loss = -12438.837890625
Iteration 14300: Loss = -12438.8369140625
Iteration 14400: Loss = -12438.8349609375
Iteration 14500: Loss = -12438.8330078125
Iteration 14600: Loss = -12438.83203125
Iteration 14700: Loss = -12438.8310546875
Iteration 14800: Loss = -12438.8291015625
Iteration 14900: Loss = -12438.8251953125
Iteration 15000: Loss = -12438.82421875
Iteration 15100: Loss = -12438.8232421875
Iteration 15200: Loss = -12438.8203125
Iteration 15300: Loss = -12438.818359375
Iteration 15400: Loss = -12438.8173828125
Iteration 15500: Loss = -12438.81640625
Iteration 15600: Loss = -12438.8154296875
Iteration 15700: Loss = -12438.814453125
Iteration 15800: Loss = -12438.8125
Iteration 15900: Loss = -12438.8134765625
1
Iteration 16000: Loss = -12438.8115234375
Iteration 16100: Loss = -12438.8095703125
Iteration 16200: Loss = -12438.80859375
Iteration 16300: Loss = -12438.806640625
Iteration 16400: Loss = -12438.8056640625
Iteration 16500: Loss = -12438.802734375
Iteration 16600: Loss = -12438.802734375
Iteration 16700: Loss = -12438.7998046875
Iteration 16800: Loss = -12438.798828125
Iteration 16900: Loss = -12438.7978515625
Iteration 17000: Loss = -12438.7958984375
Iteration 17100: Loss = -12438.791015625
Iteration 17200: Loss = -12438.7900390625
Iteration 17300: Loss = -12438.7841796875
Iteration 17400: Loss = -12438.7783203125
Iteration 17500: Loss = -12438.7724609375
Iteration 17600: Loss = -12438.759765625
Iteration 17700: Loss = -12438.73828125
Iteration 17800: Loss = -12438.6962890625
Iteration 17900: Loss = -12438.55859375
Iteration 18000: Loss = -12438.24609375
Iteration 18100: Loss = -12438.1474609375
Iteration 18200: Loss = -12438.1201171875
Iteration 18300: Loss = -12438.1162109375
Iteration 18400: Loss = -12438.1142578125
Iteration 18500: Loss = -12438.111328125
Iteration 18600: Loss = -12438.111328125
Iteration 18700: Loss = -12438.1103515625
Iteration 18800: Loss = -12438.1103515625
Iteration 18900: Loss = -12438.109375
Iteration 19000: Loss = -12438.1083984375
Iteration 19100: Loss = -12438.109375
1
Iteration 19200: Loss = -12438.109375
2
Iteration 19300: Loss = -12438.1083984375
Iteration 19400: Loss = -12438.1064453125
Iteration 19500: Loss = -12438.1083984375
1
Iteration 19600: Loss = -12438.1083984375
2
Iteration 19700: Loss = -12438.109375
3
Iteration 19800: Loss = -12438.1064453125
Iteration 19900: Loss = -12438.1083984375
1
Iteration 20000: Loss = -12438.107421875
2
Iteration 20100: Loss = -12438.107421875
3
Iteration 20200: Loss = -12438.107421875
4
Iteration 20300: Loss = -12438.1123046875
5
Iteration 20400: Loss = -12438.107421875
6
Iteration 20500: Loss = -12438.1083984375
7
Iteration 20600: Loss = -12438.1083984375
8
Iteration 20700: Loss = -12438.107421875
9
Iteration 20800: Loss = -12438.1083984375
10
Iteration 20900: Loss = -12438.1064453125
Iteration 21000: Loss = -12438.107421875
1
Iteration 21100: Loss = -12438.107421875
2
Iteration 21200: Loss = -12438.107421875
3
Iteration 21300: Loss = -12438.1083984375
4
Iteration 21400: Loss = -12438.1064453125
Iteration 21500: Loss = -12438.107421875
1
Iteration 21600: Loss = -12438.1083984375
2
Iteration 21700: Loss = -12438.1083984375
3
Iteration 21800: Loss = -12438.107421875
4
Iteration 21900: Loss = -12438.1064453125
Iteration 22000: Loss = -12438.1083984375
1
Iteration 22100: Loss = -12438.1064453125
Iteration 22200: Loss = -12438.10546875
Iteration 22300: Loss = -12438.10546875
Iteration 22400: Loss = -12438.103515625
Iteration 22500: Loss = -12438.1064453125
1
Iteration 22600: Loss = -12438.10546875
2
Iteration 22700: Loss = -12438.1044921875
3
Iteration 22800: Loss = -12438.10546875
4
Iteration 22900: Loss = -12438.10546875
5
Iteration 23000: Loss = -12438.1025390625
Iteration 23100: Loss = -12438.103515625
1
Iteration 23200: Loss = -12438.1044921875
2
Iteration 23300: Loss = -12438.1044921875
3
Iteration 23400: Loss = -12438.1015625
Iteration 23500: Loss = -12438.103515625
1
Iteration 23600: Loss = -12438.1015625
Iteration 23700: Loss = -12438.1005859375
Iteration 23800: Loss = -12438.1005859375
Iteration 23900: Loss = -12438.0986328125
Iteration 24000: Loss = -12438.095703125
Iteration 24100: Loss = -12438.0693359375
Iteration 24200: Loss = -12437.9072265625
Iteration 24300: Loss = -12432.3984375
Iteration 24400: Loss = -12427.4228515625
Iteration 24500: Loss = -12305.8544921875
Iteration 24600: Loss = -12255.529296875
Iteration 24700: Loss = -12059.9794921875
Iteration 24800: Loss = -12033.5849609375
Iteration 24900: Loss = -12017.271484375
Iteration 25000: Loss = -12011.9892578125
Iteration 25100: Loss = -12002.5888671875
Iteration 25200: Loss = -12000.8544921875
Iteration 25300: Loss = -11996.591796875
Iteration 25400: Loss = -11996.44140625
Iteration 25500: Loss = -11996.3525390625
Iteration 25600: Loss = -11996.2802734375
Iteration 25700: Loss = -11996.220703125
Iteration 25800: Loss = -11996.181640625
Iteration 25900: Loss = -11996.146484375
Iteration 26000: Loss = -11996.1171875
Iteration 26100: Loss = -11996.091796875
Iteration 26200: Loss = -11996.072265625
Iteration 26300: Loss = -11996.0556640625
Iteration 26400: Loss = -11996.0419921875
Iteration 26500: Loss = -11996.02734375
Iteration 26600: Loss = -11996.0146484375
Iteration 26700: Loss = -11996.0048828125
Iteration 26800: Loss = -11995.9931640625
Iteration 26900: Loss = -11995.984375
Iteration 27000: Loss = -11995.978515625
Iteration 27100: Loss = -11995.970703125
Iteration 27200: Loss = -11995.9638671875
Iteration 27300: Loss = -11995.9580078125
Iteration 27400: Loss = -11995.9541015625
Iteration 27500: Loss = -11995.94921875
Iteration 27600: Loss = -11995.9453125
Iteration 27700: Loss = -11995.9404296875
Iteration 27800: Loss = -11995.9365234375
Iteration 27900: Loss = -11995.93359375
Iteration 28000: Loss = -11995.9296875
Iteration 28100: Loss = -11995.92578125
Iteration 28200: Loss = -11995.9248046875
Iteration 28300: Loss = -11995.921875
Iteration 28400: Loss = -11995.9189453125
Iteration 28500: Loss = -11995.916015625
Iteration 28600: Loss = -11995.9140625
Iteration 28700: Loss = -11995.9130859375
Iteration 28800: Loss = -11995.91015625
Iteration 28900: Loss = -11995.908203125
Iteration 29000: Loss = -11995.90625
Iteration 29100: Loss = -11995.9052734375
Iteration 29200: Loss = -11995.9033203125
Iteration 29300: Loss = -11995.90234375
Iteration 29400: Loss = -11995.90234375
Iteration 29500: Loss = -11995.900390625
Iteration 29600: Loss = -11995.8994140625
Iteration 29700: Loss = -11995.8984375
Iteration 29800: Loss = -11995.8974609375
Iteration 29900: Loss = -11995.896484375
pi: tensor([[0.7461, 0.2539],
        [0.2363, 0.7637]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3974, 0.6026], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3073, 0.1273],
         [0.2027, 0.2881]],

        [[0.3335, 0.1007],
         [0.9566, 0.8589]],

        [[0.0485, 0.1069],
         [0.5091, 0.7313]],

        [[0.0178, 0.1002],
         [0.8821, 0.0149]],

        [[0.0105, 0.0957],
         [0.0199, 0.7830]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 6
Adjusted Rand Index: 0.7721463199647421
time is 1
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9137618876028301
Average Adjusted Rand Index: 0.9150743710858196
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33878.4296875
Iteration 100: Loss = -20173.287109375
Iteration 200: Loss = -14598.7841796875
Iteration 300: Loss = -13230.0107421875
Iteration 400: Loss = -12846.2470703125
Iteration 500: Loss = -12720.599609375
Iteration 600: Loss = -12653.33984375
Iteration 700: Loss = -12600.8623046875
Iteration 800: Loss = -12570.5283203125
Iteration 900: Loss = -12536.61328125
Iteration 1000: Loss = -12524.953125
Iteration 1100: Loss = -12514.5263671875
Iteration 1200: Loss = -12504.2734375
Iteration 1300: Loss = -12498.82421875
Iteration 1400: Loss = -12493.537109375
Iteration 1500: Loss = -12487.330078125
Iteration 1600: Loss = -12481.5458984375
Iteration 1700: Loss = -12479.2919921875
Iteration 1800: Loss = -12477.419921875
Iteration 1900: Loss = -12475.7451171875
Iteration 2000: Loss = -12474.1435546875
Iteration 2100: Loss = -12472.5068359375
Iteration 2200: Loss = -12470.7109375
Iteration 2300: Loss = -12468.8017578125
Iteration 2400: Loss = -12467.3251953125
Iteration 2500: Loss = -12466.3544921875
Iteration 2600: Loss = -12465.5966796875
Iteration 2700: Loss = -12464.931640625
Iteration 2800: Loss = -12464.3056640625
Iteration 2900: Loss = -12463.6904296875
Iteration 3000: Loss = -12463.0361328125
Iteration 3100: Loss = -12462.130859375
Iteration 3200: Loss = -12461.1806640625
Iteration 3300: Loss = -12460.470703125
Iteration 3400: Loss = -12459.861328125
Iteration 3500: Loss = -12459.2685546875
Iteration 3600: Loss = -12458.646484375
Iteration 3700: Loss = -12458.0029296875
Iteration 3800: Loss = -12457.3193359375
Iteration 3900: Loss = -12456.58203125
Iteration 4000: Loss = -12455.8681640625
Iteration 4100: Loss = -12455.2431640625
Iteration 4200: Loss = -12447.4638671875
Iteration 4300: Loss = -12445.1884765625
Iteration 4400: Loss = -12444.748046875
Iteration 4500: Loss = -12444.4541015625
Iteration 4600: Loss = -12443.998046875
Iteration 4700: Loss = -12443.5166015625
Iteration 4800: Loss = -12442.7021484375
Iteration 4900: Loss = -12440.78125
Iteration 5000: Loss = -12435.8671875
Iteration 5100: Loss = -12432.79296875
Iteration 5200: Loss = -12431.0244140625
Iteration 5300: Loss = -12425.669921875
Iteration 5400: Loss = -12418.576171875
Iteration 5500: Loss = -12414.751953125
Iteration 5600: Loss = -12406.998046875
Iteration 5700: Loss = -12399.5947265625
Iteration 5800: Loss = -12382.6279296875
Iteration 5900: Loss = -12370.6845703125
Iteration 6000: Loss = -12350.3056640625
Iteration 6100: Loss = -12345.4091796875
Iteration 6200: Loss = -12339.6328125
Iteration 6300: Loss = -12332.06640625
Iteration 6400: Loss = -12328.8544921875
Iteration 6500: Loss = -12328.576171875
Iteration 6600: Loss = -12328.3984375
Iteration 6700: Loss = -12328.203125
Iteration 6800: Loss = -12324.56640625
Iteration 6900: Loss = -12320.2333984375
Iteration 7000: Loss = -12315.453125
Iteration 7100: Loss = -12315.2978515625
Iteration 7200: Loss = -12315.197265625
Iteration 7300: Loss = -12315.0390625
Iteration 7400: Loss = -12314.9423828125
Iteration 7500: Loss = -12314.794921875
Iteration 7600: Loss = -12314.45703125
Iteration 7700: Loss = -12313.9365234375
Iteration 7800: Loss = -12312.89453125
Iteration 7900: Loss = -12312.736328125
Iteration 8000: Loss = -12312.658203125
Iteration 8100: Loss = -12312.607421875
Iteration 8200: Loss = -12312.5654296875
Iteration 8300: Loss = -12312.5283203125
Iteration 8400: Loss = -12312.4990234375
Iteration 8500: Loss = -12312.4697265625
Iteration 8600: Loss = -12311.76953125
Iteration 8700: Loss = -12307.4150390625
Iteration 8800: Loss = -12307.369140625
Iteration 8900: Loss = -12307.3447265625
Iteration 9000: Loss = -12307.3193359375
Iteration 9100: Loss = -12307.2490234375
Iteration 9200: Loss = -12301.6591796875
Iteration 9300: Loss = -12301.5703125
Iteration 9400: Loss = -12299.5546875
Iteration 9500: Loss = -12297.755859375
Iteration 9600: Loss = -12297.7333984375
Iteration 9700: Loss = -12297.720703125
Iteration 9800: Loss = -12297.7099609375
Iteration 9900: Loss = -12297.7041015625
Iteration 10000: Loss = -12297.69921875
Iteration 10100: Loss = -12297.6923828125
Iteration 10200: Loss = -12297.6865234375
Iteration 10300: Loss = -12297.681640625
Iteration 10400: Loss = -12297.6767578125
Iteration 10500: Loss = -12297.6728515625
Iteration 10600: Loss = -12297.6689453125
Iteration 10700: Loss = -12297.6630859375
Iteration 10800: Loss = -12297.6591796875
Iteration 10900: Loss = -12297.6533203125
Iteration 11000: Loss = -12297.642578125
Iteration 11100: Loss = -12297.6318359375
Iteration 11200: Loss = -12297.6279296875
Iteration 11300: Loss = -12297.626953125
Iteration 11400: Loss = -12297.6201171875
Iteration 11500: Loss = -12297.6142578125
Iteration 11600: Loss = -12297.599609375
Iteration 11700: Loss = -12297.5947265625
Iteration 11800: Loss = -12297.5908203125
Iteration 11900: Loss = -12297.5888671875
Iteration 12000: Loss = -12297.5869140625
Iteration 12100: Loss = -12297.5869140625
Iteration 12200: Loss = -12297.5849609375
Iteration 12300: Loss = -12297.58203125
Iteration 12400: Loss = -12297.58203125
Iteration 12500: Loss = -12297.580078125
Iteration 12600: Loss = -12297.5791015625
Iteration 12700: Loss = -12297.578125
Iteration 12800: Loss = -12297.578125
Iteration 12900: Loss = -12297.576171875
Iteration 13000: Loss = -12297.576171875
Iteration 13100: Loss = -12297.576171875
Iteration 13200: Loss = -12297.5732421875
Iteration 13300: Loss = -12297.5732421875
Iteration 13400: Loss = -12297.572265625
Iteration 13500: Loss = -12297.572265625
Iteration 13600: Loss = -12297.5703125
Iteration 13700: Loss = -12297.568359375
Iteration 13800: Loss = -12292.654296875
Iteration 13900: Loss = -12292.0087890625
Iteration 14000: Loss = -12291.9921875
Iteration 14100: Loss = -12291.984375
Iteration 14200: Loss = -12291.982421875
Iteration 14300: Loss = -12291.9794921875
Iteration 14400: Loss = -12291.9765625
Iteration 14500: Loss = -12291.9765625
Iteration 14600: Loss = -12291.9765625
Iteration 14700: Loss = -12291.974609375
Iteration 14800: Loss = -12291.9736328125
Iteration 14900: Loss = -12291.97265625
Iteration 15000: Loss = -12291.970703125
Iteration 15100: Loss = -12290.5634765625
Iteration 15200: Loss = -12289.44921875
Iteration 15300: Loss = -12288.4658203125
Iteration 15400: Loss = -12286.6357421875
Iteration 15500: Loss = -12284.9892578125
Iteration 15600: Loss = -12284.421875
Iteration 15700: Loss = -12282.2294921875
Iteration 15800: Loss = -12281.6005859375
Iteration 15900: Loss = -12279.1181640625
Iteration 16000: Loss = -12276.2666015625
Iteration 16100: Loss = -12272.0478515625
Iteration 16200: Loss = -12265.408203125
Iteration 16300: Loss = -12248.70703125
Iteration 16400: Loss = -12216.162109375
Iteration 16500: Loss = -12183.9619140625
Iteration 16600: Loss = -12114.3017578125
Iteration 16700: Loss = -12075.75
Iteration 16800: Loss = -12054.2880859375
Iteration 16900: Loss = -12031.361328125
Iteration 17000: Loss = -12018.0009765625
Iteration 17100: Loss = -12016.2685546875
Iteration 17200: Loss = -12000.1103515625
Iteration 17300: Loss = -11998.232421875
Iteration 17400: Loss = -11998.0966796875
Iteration 17500: Loss = -11998.013671875
Iteration 17600: Loss = -11997.9541015625
Iteration 17700: Loss = -11997.91015625
Iteration 17800: Loss = -11997.876953125
Iteration 17900: Loss = -11997.8486328125
Iteration 18000: Loss = -11997.826171875
Iteration 18100: Loss = -11997.8076171875
Iteration 18200: Loss = -11997.7919921875
Iteration 18300: Loss = -11997.7783203125
Iteration 18400: Loss = -11997.7666015625
Iteration 18500: Loss = -11997.755859375
Iteration 18600: Loss = -11997.74609375
Iteration 18700: Loss = -11997.73828125
Iteration 18800: Loss = -11997.73046875
Iteration 18900: Loss = -11997.724609375
Iteration 19000: Loss = -11997.71875
Iteration 19100: Loss = -11997.7119140625
Iteration 19200: Loss = -11997.7080078125
Iteration 19300: Loss = -11997.705078125
Iteration 19400: Loss = -11997.7001953125
Iteration 19500: Loss = -11997.697265625
Iteration 19600: Loss = -11997.693359375
Iteration 19700: Loss = -11997.689453125
Iteration 19800: Loss = -11997.6865234375
Iteration 19900: Loss = -11997.685546875
Iteration 20000: Loss = -11997.6826171875
Iteration 20100: Loss = -11997.6796875
Iteration 20200: Loss = -11997.677734375
Iteration 20300: Loss = -11997.6748046875
Iteration 20400: Loss = -11997.6728515625
Iteration 20500: Loss = -11997.5947265625
Iteration 20600: Loss = -11986.7099609375
Iteration 20700: Loss = -11986.6064453125
Iteration 20800: Loss = -11986.5771484375
Iteration 20900: Loss = -11986.564453125
Iteration 21000: Loss = -11986.5556640625
Iteration 21100: Loss = -11986.5478515625
Iteration 21200: Loss = -11986.54296875
Iteration 21300: Loss = -11986.5400390625
Iteration 21400: Loss = -11986.537109375
Iteration 21500: Loss = -11986.533203125
Iteration 21600: Loss = -11986.4833984375
Iteration 21700: Loss = -11986.4833984375
Iteration 21800: Loss = -11986.4814453125
Iteration 21900: Loss = -11986.48046875
Iteration 22000: Loss = -11986.4658203125
Iteration 22100: Loss = -11986.4638671875
Iteration 22200: Loss = -11986.4638671875
Iteration 22300: Loss = -11986.4619140625
Iteration 22400: Loss = -11986.4619140625
Iteration 22500: Loss = -11986.4599609375
Iteration 22600: Loss = -11986.4609375
1
Iteration 22700: Loss = -11986.458984375
Iteration 22800: Loss = -11986.45703125
Iteration 22900: Loss = -11986.4580078125
1
Iteration 23000: Loss = -11986.45703125
Iteration 23100: Loss = -11986.4560546875
Iteration 23200: Loss = -11986.45703125
1
Iteration 23300: Loss = -11986.45703125
2
Iteration 23400: Loss = -11986.4560546875
Iteration 23500: Loss = -11986.455078125
Iteration 23600: Loss = -11986.453125
Iteration 23700: Loss = -11986.4541015625
1
Iteration 23800: Loss = -11986.4541015625
2
Iteration 23900: Loss = -11986.4541015625
3
Iteration 24000: Loss = -11986.4521484375
Iteration 24100: Loss = -11986.453125
1
Iteration 24200: Loss = -11986.453125
2
Iteration 24300: Loss = -11986.4541015625
3
Iteration 24400: Loss = -11986.453125
4
Iteration 24500: Loss = -11986.4521484375
Iteration 24600: Loss = -11986.4521484375
Iteration 24700: Loss = -11986.451171875
Iteration 24800: Loss = -11986.4521484375
1
Iteration 24900: Loss = -11986.4521484375
2
Iteration 25000: Loss = -11986.4521484375
3
Iteration 25100: Loss = -11986.451171875
Iteration 25200: Loss = -11986.453125
1
Iteration 25300: Loss = -11986.4521484375
2
Iteration 25400: Loss = -11986.451171875
Iteration 25500: Loss = -11986.4501953125
Iteration 25600: Loss = -11975.84765625
Iteration 25700: Loss = -11975.5703125
Iteration 25800: Loss = -11975.5341796875
Iteration 25900: Loss = -11975.5166015625
Iteration 26000: Loss = -11975.5078125
Iteration 26100: Loss = -11975.501953125
Iteration 26200: Loss = -11975.4990234375
Iteration 26300: Loss = -11975.4951171875
Iteration 26400: Loss = -11975.4921875
Iteration 26500: Loss = -11975.4931640625
1
Iteration 26600: Loss = -11975.4892578125
Iteration 26700: Loss = -11975.4892578125
Iteration 26800: Loss = -11975.4873046875
Iteration 26900: Loss = -11975.4873046875
Iteration 27000: Loss = -11975.486328125
Iteration 27100: Loss = -11975.4853515625
Iteration 27200: Loss = -11975.486328125
1
Iteration 27300: Loss = -11975.4833984375
Iteration 27400: Loss = -11975.4853515625
1
Iteration 27500: Loss = -11975.4833984375
Iteration 27600: Loss = -11975.4833984375
Iteration 27700: Loss = -11975.484375
1
Iteration 27800: Loss = -11975.4814453125
Iteration 27900: Loss = -11975.4833984375
1
Iteration 28000: Loss = -11975.4833984375
2
Iteration 28100: Loss = -11975.48046875
Iteration 28200: Loss = -11975.48046875
Iteration 28300: Loss = -11975.4794921875
Iteration 28400: Loss = -11975.48046875
1
Iteration 28500: Loss = -11975.4794921875
Iteration 28600: Loss = -11975.48046875
1
Iteration 28700: Loss = -11975.4794921875
Iteration 28800: Loss = -11975.4794921875
Iteration 28900: Loss = -11975.478515625
Iteration 29000: Loss = -11975.478515625
Iteration 29100: Loss = -11975.478515625
Iteration 29200: Loss = -11975.478515625
Iteration 29300: Loss = -11975.4775390625
Iteration 29400: Loss = -11975.478515625
1
Iteration 29500: Loss = -11975.4794921875
2
Iteration 29600: Loss = -11975.4775390625
Iteration 29700: Loss = -11975.4775390625
Iteration 29800: Loss = -11975.4775390625
Iteration 29900: Loss = -11975.4775390625
pi: tensor([[0.5696, 0.4304],
        [0.3806, 0.6194]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5396, 0.4604], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3036, 0.1094],
         [0.2944, 0.3004]],

        [[0.0082, 0.1037],
         [0.9785, 0.1613]],

        [[0.8646, 0.1019],
         [0.1307, 0.3174]],

        [[0.4463, 0.0998],
         [0.9933, 0.7889]],

        [[0.9363, 0.0957],
         [0.0197, 0.9814]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.34916440965677675
Average Adjusted Rand Index: 0.9839992163297293
[0.9137618876028301, 0.34916440965677675] [0.9150743710858196, 0.9839992163297293] [11995.8955078125, 11975.4775390625]
-------------------------------------
This iteration is 49
True Objective function: Loss = -11806.428627594172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29320.05859375
Iteration 100: Loss = -18501.94921875
Iteration 200: Loss = -13601.6220703125
Iteration 300: Loss = -12700.998046875
Iteration 400: Loss = -12527.3486328125
Iteration 500: Loss = -12465.8857421875
Iteration 600: Loss = -12426.47265625
Iteration 700: Loss = -12402.349609375
Iteration 800: Loss = -12378.5166015625
Iteration 900: Loss = -12365.0205078125
Iteration 1000: Loss = -12354.7138671875
Iteration 1100: Loss = -12342.8212890625
Iteration 1200: Loss = -12335.4833984375
Iteration 1300: Loss = -12330.7333984375
Iteration 1400: Loss = -12328.251953125
Iteration 1500: Loss = -12326.416015625
Iteration 1600: Loss = -12324.6796875
Iteration 1700: Loss = -12320.330078125
Iteration 1800: Loss = -12318.6337890625
Iteration 1900: Loss = -12317.51171875
Iteration 2000: Loss = -12316.6142578125
Iteration 2100: Loss = -12315.859375
Iteration 2200: Loss = -12315.208984375
Iteration 2300: Loss = -12314.6376953125
Iteration 2400: Loss = -12314.1337890625
Iteration 2500: Loss = -12313.685546875
Iteration 2600: Loss = -12313.283203125
Iteration 2700: Loss = -12312.9228515625
Iteration 2800: Loss = -12312.59765625
Iteration 2900: Loss = -12312.2998046875
Iteration 3000: Loss = -12312.029296875
Iteration 3100: Loss = -12311.7841796875
Iteration 3200: Loss = -12311.5576171875
Iteration 3300: Loss = -12311.3505859375
Iteration 3400: Loss = -12311.16015625
Iteration 3500: Loss = -12310.982421875
Iteration 3600: Loss = -12310.8203125
Iteration 3700: Loss = -12310.6708984375
Iteration 3800: Loss = -12310.5283203125
Iteration 3900: Loss = -12310.3974609375
Iteration 4000: Loss = -12310.27734375
Iteration 4100: Loss = -12310.162109375
Iteration 4200: Loss = -12310.0576171875
Iteration 4300: Loss = -12309.96484375
Iteration 4400: Loss = -12309.8681640625
Iteration 4500: Loss = -12309.7822265625
Iteration 4600: Loss = -12309.7021484375
Iteration 4700: Loss = -12309.626953125
Iteration 4800: Loss = -12309.5556640625
Iteration 4900: Loss = -12309.490234375
Iteration 5000: Loss = -12309.42578125
Iteration 5100: Loss = -12309.3671875
Iteration 5200: Loss = -12309.3115234375
Iteration 5300: Loss = -12309.259765625
Iteration 5400: Loss = -12309.2109375
Iteration 5500: Loss = -12309.162109375
Iteration 5600: Loss = -12309.119140625
Iteration 5700: Loss = -12309.078125
Iteration 5800: Loss = -12309.0400390625
Iteration 5900: Loss = -12309.001953125
Iteration 6000: Loss = -12308.966796875
Iteration 6100: Loss = -12308.935546875
Iteration 6200: Loss = -12308.90234375
Iteration 6300: Loss = -12308.8720703125
Iteration 6400: Loss = -12308.845703125
Iteration 6500: Loss = -12308.8193359375
Iteration 6600: Loss = -12308.79296875
Iteration 6700: Loss = -12308.7705078125
Iteration 6800: Loss = -12308.74609375
Iteration 6900: Loss = -12308.7275390625
Iteration 7000: Loss = -12308.7041015625
Iteration 7100: Loss = -12308.685546875
Iteration 7200: Loss = -12308.66796875
Iteration 7300: Loss = -12308.6484375
Iteration 7400: Loss = -12308.630859375
Iteration 7500: Loss = -12308.6162109375
Iteration 7600: Loss = -12308.6005859375
Iteration 7700: Loss = -12308.5869140625
Iteration 7800: Loss = -12308.5732421875
Iteration 7900: Loss = -12308.55859375
Iteration 8000: Loss = -12308.5458984375
Iteration 8100: Loss = -12308.5341796875
Iteration 8200: Loss = -12308.5234375
Iteration 8300: Loss = -12308.5107421875
Iteration 8400: Loss = -12308.5
Iteration 8500: Loss = -12308.4892578125
Iteration 8600: Loss = -12308.4794921875
Iteration 8700: Loss = -12308.46875
Iteration 8800: Loss = -12308.458984375
Iteration 8900: Loss = -12308.44921875
Iteration 9000: Loss = -12308.4423828125
Iteration 9100: Loss = -12308.43359375
Iteration 9200: Loss = -12308.4248046875
Iteration 9300: Loss = -12308.4169921875
Iteration 9400: Loss = -12308.4072265625
Iteration 9500: Loss = -12308.400390625
Iteration 9600: Loss = -12308.390625
Iteration 9700: Loss = -12308.3837890625
Iteration 9800: Loss = -12308.3759765625
Iteration 9900: Loss = -12308.369140625
Iteration 10000: Loss = -12308.36328125
Iteration 10100: Loss = -12308.357421875
Iteration 10200: Loss = -12308.3515625
Iteration 10300: Loss = -12308.34765625
Iteration 10400: Loss = -12308.33984375
Iteration 10500: Loss = -12308.3310546875
Iteration 10600: Loss = -12308.314453125
Iteration 10700: Loss = -12308.30078125
Iteration 10800: Loss = -12308.283203125
Iteration 10900: Loss = -12308.2255859375
Iteration 11000: Loss = -12308.1650390625
Iteration 11100: Loss = -12307.8046875
Iteration 11200: Loss = -12307.3076171875
Iteration 11300: Loss = -12307.1982421875
Iteration 11400: Loss = -12307.078125
Iteration 11500: Loss = -12307.0068359375
Iteration 11600: Loss = -12304.4072265625
Iteration 11700: Loss = -12303.73828125
Iteration 11800: Loss = -12303.30859375
Iteration 11900: Loss = -12303.0166015625
Iteration 12000: Loss = -12302.8095703125
Iteration 12100: Loss = -12302.65625
Iteration 12200: Loss = -12302.5380859375
Iteration 12300: Loss = -12302.447265625
Iteration 12400: Loss = -12302.3701171875
Iteration 12500: Loss = -12302.30859375
Iteration 12600: Loss = -12302.2578125
Iteration 12700: Loss = -12302.2158203125
Iteration 12800: Loss = -12302.1767578125
Iteration 12900: Loss = -12302.1455078125
Iteration 13000: Loss = -12302.1181640625
Iteration 13100: Loss = -12302.0927734375
Iteration 13200: Loss = -12302.072265625
Iteration 13300: Loss = -12302.0517578125
Iteration 13400: Loss = -12302.0361328125
Iteration 13500: Loss = -12302.021484375
Iteration 13600: Loss = -12302.0068359375
Iteration 13700: Loss = -12301.9951171875
Iteration 13800: Loss = -12301.982421875
Iteration 13900: Loss = -12301.97265625
Iteration 14000: Loss = -12301.9638671875
Iteration 14100: Loss = -12301.9560546875
Iteration 14200: Loss = -12301.947265625
Iteration 14300: Loss = -12301.939453125
Iteration 14400: Loss = -12301.9345703125
Iteration 14500: Loss = -12301.9267578125
Iteration 14600: Loss = -12301.921875
Iteration 14700: Loss = -12301.91796875
Iteration 14800: Loss = -12301.9130859375
Iteration 14900: Loss = -12301.908203125
Iteration 15000: Loss = -12301.904296875
Iteration 15100: Loss = -12301.8994140625
Iteration 15200: Loss = -12301.896484375
Iteration 15300: Loss = -12301.8955078125
Iteration 15400: Loss = -12301.888671875
Iteration 15500: Loss = -12301.88671875
Iteration 15600: Loss = -12301.8837890625
Iteration 15700: Loss = -12301.880859375
Iteration 15800: Loss = -12301.880859375
Iteration 15900: Loss = -12301.8759765625
Iteration 16000: Loss = -12301.876953125
1
Iteration 16100: Loss = -12301.87109375
Iteration 16200: Loss = -12301.87109375
Iteration 16300: Loss = -12301.8701171875
Iteration 16400: Loss = -12301.8720703125
1
Iteration 16500: Loss = -12301.8662109375
Iteration 16600: Loss = -12301.8642578125
Iteration 16700: Loss = -12301.86328125
Iteration 16800: Loss = -12301.8623046875
Iteration 16900: Loss = -12301.8603515625
Iteration 17000: Loss = -12301.859375
Iteration 17100: Loss = -12301.857421875
Iteration 17200: Loss = -12301.857421875
Iteration 17300: Loss = -12301.85546875
Iteration 17400: Loss = -12301.85546875
Iteration 17500: Loss = -12301.8525390625
Iteration 17600: Loss = -12301.8525390625
Iteration 17700: Loss = -12301.8525390625
Iteration 17800: Loss = -12301.8525390625
Iteration 17900: Loss = -12301.8515625
Iteration 18000: Loss = -12301.8525390625
1
Iteration 18100: Loss = -12301.849609375
Iteration 18200: Loss = -12301.8505859375
1
Iteration 18300: Loss = -12301.84765625
Iteration 18400: Loss = -12301.84765625
Iteration 18500: Loss = -12301.84765625
Iteration 18600: Loss = -12301.84765625
Iteration 18700: Loss = -12301.84765625
Iteration 18800: Loss = -12301.84765625
Iteration 18900: Loss = -12301.845703125
Iteration 19000: Loss = -12301.845703125
Iteration 19100: Loss = -12301.8447265625
Iteration 19200: Loss = -12301.8447265625
Iteration 19300: Loss = -12301.845703125
1
Iteration 19400: Loss = -12301.8447265625
Iteration 19500: Loss = -12301.8427734375
Iteration 19600: Loss = -12301.84375
1
Iteration 19700: Loss = -12301.84375
2
Iteration 19800: Loss = -12301.841796875
Iteration 19900: Loss = -12301.8408203125
Iteration 20000: Loss = -12301.8427734375
1
Iteration 20100: Loss = -12301.8408203125
Iteration 20200: Loss = -12301.8427734375
1
Iteration 20300: Loss = -12301.8427734375
2
Iteration 20400: Loss = -12301.8427734375
3
Iteration 20500: Loss = -12301.8427734375
4
Iteration 20600: Loss = -12301.841796875
5
Iteration 20700: Loss = -12301.841796875
6
Iteration 20800: Loss = -12301.8427734375
7
Iteration 20900: Loss = -12301.8408203125
Iteration 21000: Loss = -12301.8408203125
Iteration 21100: Loss = -12301.8408203125
Iteration 21200: Loss = -12301.8408203125
Iteration 21300: Loss = -12301.841796875
1
Iteration 21400: Loss = -12301.841796875
2
Iteration 21500: Loss = -12301.8408203125
Iteration 21600: Loss = -12301.841796875
1
Iteration 21700: Loss = -12301.8427734375
2
Iteration 21800: Loss = -12301.8408203125
Iteration 21900: Loss = -12301.83984375
Iteration 22000: Loss = -12301.8408203125
1
Iteration 22100: Loss = -12301.8408203125
2
Iteration 22200: Loss = -12301.8388671875
Iteration 22300: Loss = -12301.837890625
Iteration 22400: Loss = -12301.83984375
1
Iteration 22500: Loss = -12301.8388671875
2
Iteration 22600: Loss = -12301.83984375
3
Iteration 22700: Loss = -12301.83984375
4
Iteration 22800: Loss = -12301.8408203125
5
Iteration 22900: Loss = -12301.83984375
6
Iteration 23000: Loss = -12301.8408203125
7
Iteration 23100: Loss = -12301.83984375
8
Iteration 23200: Loss = -12301.83984375
9
Iteration 23300: Loss = -12301.83984375
10
Iteration 23400: Loss = -12301.83984375
11
Iteration 23500: Loss = -12301.837890625
Iteration 23600: Loss = -12301.83984375
1
Iteration 23700: Loss = -12301.8388671875
2
Iteration 23800: Loss = -12301.83984375
3
Iteration 23900: Loss = -12301.83984375
4
Iteration 24000: Loss = -12301.8388671875
5
Iteration 24100: Loss = -12301.837890625
Iteration 24200: Loss = -12301.83984375
1
Iteration 24300: Loss = -12301.83984375
2
Iteration 24400: Loss = -12301.83984375
3
Iteration 24500: Loss = -12301.83984375
4
Iteration 24600: Loss = -12301.8388671875
5
Iteration 24700: Loss = -12301.83984375
6
Iteration 24800: Loss = -12301.837890625
Iteration 24900: Loss = -12301.83984375
1
Iteration 25000: Loss = -12301.837890625
Iteration 25100: Loss = -12301.83984375
1
Iteration 25200: Loss = -12301.837890625
Iteration 25300: Loss = -12301.8388671875
1
Iteration 25400: Loss = -12301.837890625
Iteration 25500: Loss = -12301.83984375
1
Iteration 25600: Loss = -12301.83984375
2
Iteration 25700: Loss = -12301.83984375
3
Iteration 25800: Loss = -12301.83984375
4
Iteration 25900: Loss = -12301.8388671875
5
Iteration 26000: Loss = -12301.8388671875
6
Iteration 26100: Loss = -12301.83984375
7
Iteration 26200: Loss = -12301.8388671875
8
Iteration 26300: Loss = -12301.8388671875
9
Iteration 26400: Loss = -12301.83984375
10
Iteration 26500: Loss = -12301.837890625
Iteration 26600: Loss = -12301.8388671875
1
Iteration 26700: Loss = -12301.837890625
Iteration 26800: Loss = -12301.837890625
Iteration 26900: Loss = -12301.8388671875
1
Iteration 27000: Loss = -12301.8369140625
Iteration 27100: Loss = -12301.8408203125
1
Iteration 27200: Loss = -12301.8388671875
2
Iteration 27300: Loss = -12301.8408203125
3
Iteration 27400: Loss = -12301.837890625
4
Iteration 27500: Loss = -12301.837890625
5
Iteration 27600: Loss = -12301.837890625
6
Iteration 27700: Loss = -12301.83984375
7
Iteration 27800: Loss = -12301.83984375
8
Iteration 27900: Loss = -12301.8388671875
9
Iteration 28000: Loss = -12301.8388671875
10
Iteration 28100: Loss = -12301.83984375
11
Iteration 28200: Loss = -12301.8388671875
12
Iteration 28300: Loss = -12301.837890625
13
Iteration 28400: Loss = -12301.8388671875
14
Iteration 28500: Loss = -12301.837890625
15
Stopping early at iteration 28500 due to no improvement.
pi: tensor([[1.0000e+00, 5.9666e-08],
        [1.9613e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9800, 0.0200], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1971, 0.2653],
         [0.8204, 1.0000]],

        [[0.9758, 0.2296],
         [0.6351, 0.9479]],

        [[0.7283, 0.2551],
         [0.8258, 0.0225]],

        [[0.4990, 0.1581],
         [0.0673, 0.6393]],

        [[0.0476, 0.1429],
         [0.8136, 0.0077]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: 0.0014110503942673831
Average Adjusted Rand Index: -0.0009301675800314144
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35801.89453125
Iteration 100: Loss = -23071.494140625
Iteration 200: Loss = -15284.7421875
Iteration 300: Loss = -13273.703125
Iteration 400: Loss = -12774.0546875
Iteration 500: Loss = -12595.4765625
Iteration 600: Loss = -12512.5107421875
Iteration 700: Loss = -12471.6787109375
Iteration 800: Loss = -12445.2001953125
Iteration 900: Loss = -12428.4921875
Iteration 1000: Loss = -12413.4033203125
Iteration 1100: Loss = -12402.9169921875
Iteration 1200: Loss = -12394.1884765625
Iteration 1300: Loss = -12386.1806640625
Iteration 1400: Loss = -12380.0
Iteration 1500: Loss = -12373.583984375
Iteration 1600: Loss = -12369.005859375
Iteration 1700: Loss = -12365.927734375
Iteration 1800: Loss = -12360.7392578125
Iteration 1900: Loss = -12354.994140625
Iteration 2000: Loss = -12351.1748046875
Iteration 2100: Loss = -12348.88671875
Iteration 2200: Loss = -12346.650390625
Iteration 2300: Loss = -12344.7470703125
Iteration 2400: Loss = -12342.9140625
Iteration 2500: Loss = -12341.1181640625
Iteration 2600: Loss = -12339.375
Iteration 2700: Loss = -12337.3291015625
Iteration 2800: Loss = -12335.8212890625
Iteration 2900: Loss = -12334.4072265625
Iteration 3000: Loss = -12332.77734375
Iteration 3100: Loss = -12331.3515625
Iteration 3200: Loss = -12329.94140625
Iteration 3300: Loss = -12328.5478515625
Iteration 3400: Loss = -12327.390625
Iteration 3500: Loss = -12326.349609375
Iteration 3600: Loss = -12325.416015625
Iteration 3700: Loss = -12324.578125
Iteration 3800: Loss = -12323.71875
Iteration 3900: Loss = -12322.794921875
Iteration 4000: Loss = -12321.9404296875
Iteration 4100: Loss = -12321.294921875
Iteration 4200: Loss = -12320.8046875
Iteration 4300: Loss = -12320.271484375
Iteration 4400: Loss = -12319.875
Iteration 4500: Loss = -12319.607421875
Iteration 4600: Loss = -12319.388671875
Iteration 4700: Loss = -12319.20703125
Iteration 4800: Loss = -12319.048828125
Iteration 4900: Loss = -12318.908203125
Iteration 5000: Loss = -12318.77734375
Iteration 5100: Loss = -12318.6552734375
Iteration 5200: Loss = -12318.5234375
Iteration 5300: Loss = -12318.279296875
Iteration 5400: Loss = -12318.0234375
Iteration 5500: Loss = -12317.9072265625
Iteration 5600: Loss = -12317.802734375
Iteration 5700: Loss = -12317.693359375
Iteration 5800: Loss = -12317.4599609375
Iteration 5900: Loss = -12316.2255859375
Iteration 6000: Loss = -12315.9189453125
Iteration 6100: Loss = -12315.7451171875
Iteration 6200: Loss = -12315.56640625
Iteration 6300: Loss = -12313.015625
Iteration 6400: Loss = -12312.3330078125
Iteration 6500: Loss = -12312.0498046875
Iteration 6600: Loss = -12311.71875
Iteration 6700: Loss = -12305.6953125
Iteration 6800: Loss = -12288.2783203125
Iteration 6900: Loss = -12204.3740234375
Iteration 7000: Loss = -12202.091796875
Iteration 7100: Loss = -12201.40625
Iteration 7200: Loss = -12201.056640625
Iteration 7300: Loss = -12200.58203125
Iteration 7400: Loss = -12195.3583984375
Iteration 7500: Loss = -12191.046875
Iteration 7600: Loss = -12169.8251953125
Iteration 7700: Loss = -12166.88671875
Iteration 7800: Loss = -12166.6220703125
Iteration 7900: Loss = -12166.4814453125
Iteration 8000: Loss = -12166.3603515625
Iteration 8100: Loss = -12154.208984375
Iteration 8200: Loss = -12153.9921875
Iteration 8300: Loss = -12153.904296875
Iteration 8400: Loss = -12153.845703125
Iteration 8500: Loss = -12153.802734375
Iteration 8600: Loss = -12153.765625
Iteration 8700: Loss = -12153.736328125
Iteration 8800: Loss = -12153.7109375
Iteration 8900: Loss = -12153.6884765625
Iteration 9000: Loss = -12153.6689453125
Iteration 9100: Loss = -12153.6513671875
Iteration 9200: Loss = -12153.6357421875
Iteration 9300: Loss = -12153.62109375
Iteration 9400: Loss = -12153.6083984375
Iteration 9500: Loss = -12153.595703125
Iteration 9600: Loss = -12153.5859375
Iteration 9700: Loss = -12153.57421875
Iteration 9800: Loss = -12153.5634765625
Iteration 9900: Loss = -12153.548828125
Iteration 10000: Loss = -12153.28125
Iteration 10100: Loss = -12142.4423828125
Iteration 10200: Loss = -12142.3427734375
Iteration 10300: Loss = -12142.30859375
Iteration 10400: Loss = -12142.287109375
Iteration 10500: Loss = -12142.2744140625
Iteration 10600: Loss = -12142.26171875
Iteration 10700: Loss = -12142.25390625
Iteration 10800: Loss = -12142.2451171875
Iteration 10900: Loss = -12142.23828125
Iteration 11000: Loss = -12142.23046875
Iteration 11100: Loss = -12142.2275390625
Iteration 11200: Loss = -12142.22265625
Iteration 11300: Loss = -12142.2177734375
Iteration 11400: Loss = -12142.2138671875
Iteration 11500: Loss = -12142.2109375
Iteration 11600: Loss = -12142.20703125
Iteration 11700: Loss = -12142.2041015625
Iteration 11800: Loss = -12142.2001953125
Iteration 11900: Loss = -12142.1982421875
Iteration 12000: Loss = -12142.1953125
Iteration 12100: Loss = -12142.1923828125
Iteration 12200: Loss = -12142.19140625
Iteration 12300: Loss = -12142.1875
Iteration 12400: Loss = -12142.185546875
Iteration 12500: Loss = -12142.1845703125
Iteration 12600: Loss = -12142.1826171875
Iteration 12700: Loss = -12142.1806640625
Iteration 12800: Loss = -12142.1787109375
Iteration 12900: Loss = -12142.177734375
Iteration 13000: Loss = -12142.177734375
Iteration 13100: Loss = -12142.17578125
Iteration 13200: Loss = -12142.1728515625
Iteration 13300: Loss = -12142.1728515625
Iteration 13400: Loss = -12142.068359375
Iteration 13500: Loss = -12142.0263671875
Iteration 13600: Loss = -12142.0234375
Iteration 13700: Loss = -12142.021484375
Iteration 13800: Loss = -12142.021484375
Iteration 13900: Loss = -12142.0185546875
Iteration 14000: Loss = -12142.0166015625
Iteration 14100: Loss = -12142.017578125
1
Iteration 14200: Loss = -12142.0166015625
Iteration 14300: Loss = -12142.015625
Iteration 14400: Loss = -12142.015625
Iteration 14500: Loss = -12142.013671875
Iteration 14600: Loss = -12142.0126953125
Iteration 14700: Loss = -12142.0126953125
Iteration 14800: Loss = -12142.0126953125
Iteration 14900: Loss = -12142.01171875
Iteration 15000: Loss = -12142.0126953125
1
Iteration 15100: Loss = -12142.01171875
Iteration 15200: Loss = -12142.009765625
Iteration 15300: Loss = -12142.009765625
Iteration 15400: Loss = -12142.0087890625
Iteration 15500: Loss = -12142.009765625
1
Iteration 15600: Loss = -12142.0087890625
Iteration 15700: Loss = -12141.9619140625
Iteration 15800: Loss = -12138.1728515625
Iteration 15900: Loss = -12138.158203125
Iteration 16000: Loss = -12138.1533203125
Iteration 16100: Loss = -12138.150390625
Iteration 16200: Loss = -12138.1474609375
Iteration 16300: Loss = -12138.1484375
1
Iteration 16400: Loss = -12138.1474609375
Iteration 16500: Loss = -12138.146484375
Iteration 16600: Loss = -12138.146484375
Iteration 16700: Loss = -12138.146484375
Iteration 16800: Loss = -12138.1455078125
Iteration 16900: Loss = -12138.1455078125
Iteration 17000: Loss = -12138.14453125
Iteration 17100: Loss = -12138.1396484375
Iteration 17200: Loss = -12138.1396484375
Iteration 17300: Loss = -12138.1396484375
Iteration 17400: Loss = -12138.138671875
Iteration 17500: Loss = -12138.1396484375
1
Iteration 17600: Loss = -12138.138671875
Iteration 17700: Loss = -12138.138671875
Iteration 17800: Loss = -12138.138671875
Iteration 17900: Loss = -12138.140625
1
Iteration 18000: Loss = -12138.138671875
Iteration 18100: Loss = -12138.1376953125
Iteration 18200: Loss = -12138.138671875
1
Iteration 18300: Loss = -12138.1376953125
Iteration 18400: Loss = -12138.138671875
1
Iteration 18500: Loss = -12138.1376953125
Iteration 18600: Loss = -12138.13671875
Iteration 18700: Loss = -12138.138671875
1
Iteration 18800: Loss = -12138.13671875
Iteration 18900: Loss = -12138.13671875
Iteration 19000: Loss = -12138.1376953125
1
Iteration 19100: Loss = -12138.1376953125
2
Iteration 19200: Loss = -12138.13671875
Iteration 19300: Loss = -12138.13671875
Iteration 19400: Loss = -12138.13671875
Iteration 19500: Loss = -12137.6943359375
Iteration 19600: Loss = -12135.5673828125
Iteration 19700: Loss = -12134.677734375
Iteration 19800: Loss = -12133.376953125
Iteration 19900: Loss = -12130.0654296875
Iteration 20000: Loss = -12124.708984375
Iteration 20100: Loss = -12119.48046875
Iteration 20200: Loss = -12113.0888671875
Iteration 20300: Loss = -12105.8564453125
Iteration 20400: Loss = -12102.46875
Iteration 20500: Loss = -12100.2734375
Iteration 20600: Loss = -12100.2548828125
Iteration 20700: Loss = -12100.2451171875
Iteration 20800: Loss = -12100.0869140625
Iteration 20900: Loss = -12100.0830078125
Iteration 21000: Loss = -12100.0810546875
Iteration 21100: Loss = -12100.078125
Iteration 21200: Loss = -12100.0771484375
Iteration 21300: Loss = -12100.076171875
Iteration 21400: Loss = -12100.076171875
Iteration 21500: Loss = -12100.076171875
Iteration 21600: Loss = -12100.0751953125
Iteration 21700: Loss = -12100.0732421875
Iteration 21800: Loss = -12100.07421875
1
Iteration 21900: Loss = -12100.07421875
2
Iteration 22000: Loss = -12100.0732421875
Iteration 22100: Loss = -12100.0732421875
Iteration 22200: Loss = -12100.072265625
Iteration 22300: Loss = -12100.064453125
Iteration 22400: Loss = -12098.9970703125
Iteration 22500: Loss = -12098.99609375
Iteration 22600: Loss = -12098.9931640625
Iteration 22700: Loss = -12098.9970703125
1
Iteration 22800: Loss = -12098.9931640625
Iteration 22900: Loss = -12098.9931640625
Iteration 23000: Loss = -12098.9921875
Iteration 23100: Loss = -12098.9931640625
1
Iteration 23200: Loss = -12098.994140625
2
Iteration 23300: Loss = -12098.9931640625
3
Iteration 23400: Loss = -12098.994140625
4
Iteration 23500: Loss = -12098.9921875
Iteration 23600: Loss = -12098.9931640625
1
Iteration 23700: Loss = -12098.9951171875
2
Iteration 23800: Loss = -12098.9931640625
3
Iteration 23900: Loss = -12098.9931640625
4
Iteration 24000: Loss = -12098.994140625
5
Iteration 24100: Loss = -12098.9931640625
6
Iteration 24200: Loss = -12098.994140625
7
Iteration 24300: Loss = -12098.9931640625
8
Iteration 24400: Loss = -12098.9931640625
9
Iteration 24500: Loss = -12098.9921875
Iteration 24600: Loss = -12098.9931640625
1
Iteration 24700: Loss = -12098.9921875
Iteration 24800: Loss = -12098.9912109375
Iteration 24900: Loss = -12098.044921875
Iteration 25000: Loss = -12097.7939453125
Iteration 25100: Loss = -12096.9833984375
Iteration 25200: Loss = -12096.26171875
Iteration 25300: Loss = -12096.2607421875
Iteration 25400: Loss = -12095.640625
Iteration 25500: Loss = -12095.60546875
Iteration 25600: Loss = -12094.6484375
Iteration 25700: Loss = -12094.64453125
Iteration 25800: Loss = -12094.6455078125
1
Iteration 25900: Loss = -12094.6455078125
2
Iteration 26000: Loss = -12094.640625
Iteration 26100: Loss = -12094.6416015625
1
Iteration 26200: Loss = -12094.642578125
2
Iteration 26300: Loss = -12094.642578125
3
Iteration 26400: Loss = -12094.6357421875
Iteration 26500: Loss = -12094.0517578125
Iteration 26600: Loss = -12094.0517578125
Iteration 26700: Loss = -12094.0283203125
Iteration 26800: Loss = -12093.724609375
Iteration 26900: Loss = -12093.71875
Iteration 27000: Loss = -12093.71484375
Iteration 27100: Loss = -12093.712890625
Iteration 27200: Loss = -12093.18359375
Iteration 27300: Loss = -12093.1015625
Iteration 27400: Loss = -12091.7421875
Iteration 27500: Loss = -12091.736328125
Iteration 27600: Loss = -12088.173828125
Iteration 27700: Loss = -12087.568359375
Iteration 27800: Loss = -12087.5556640625
Iteration 27900: Loss = -12087.5537109375
Iteration 28000: Loss = -12086.7314453125
Iteration 28100: Loss = -12086.7294921875
Iteration 28200: Loss = -12085.873046875
Iteration 28300: Loss = -12085.720703125
Iteration 28400: Loss = -12085.7197265625
Iteration 28500: Loss = -12085.71875
Iteration 28600: Loss = -12085.7197265625
1
Iteration 28700: Loss = -12085.71875
Iteration 28800: Loss = -12085.71875
Iteration 28900: Loss = -12085.7197265625
1
Iteration 29000: Loss = -12085.7197265625
2
Iteration 29100: Loss = -12085.7177734375
Iteration 29200: Loss = -12085.71875
1
Iteration 29300: Loss = -12085.7197265625
2
Iteration 29400: Loss = -12085.7197265625
3
Iteration 29500: Loss = -12085.7197265625
4
Iteration 29600: Loss = -12085.720703125
5
Iteration 29700: Loss = -12085.7197265625
6
Iteration 29800: Loss = -12085.7197265625
7
Iteration 29900: Loss = -12085.7197265625
8
pi: tensor([[6.0260e-01, 3.9740e-01],
        [1.0000e+00, 1.7883e-07]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4401, 0.5599], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2338, 0.0938],
         [0.0377, 0.3155]],

        [[0.9441, 0.1465],
         [0.0736, 0.9739]],

        [[0.3369, 0.1113],
         [0.5075, 0.9773]],

        [[0.9919, 0.1530],
         [0.2959, 0.5740]],

        [[0.9925, 0.0935],
         [0.1058, 0.0075]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 25
Adjusted Rand Index: 0.24392315839843434
time is 2
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 89
Adjusted Rand Index: 0.6042568328689156
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.010344451541541386
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.005928333479845426
Average Adjusted Rand Index: 0.555865147169801
[0.0014110503942673831, 0.005928333479845426] [-0.0009301675800314144, 0.555865147169801] [12301.837890625, 12085.71875]
-------------------------------------
This iteration is 50
True Objective function: Loss = -11767.55348392735
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28303.71875
Iteration 100: Loss = -19384.365234375
Iteration 200: Loss = -14283.142578125
Iteration 300: Loss = -12987.7041015625
Iteration 400: Loss = -12603.126953125
Iteration 500: Loss = -12509.2685546875
Iteration 600: Loss = -12474.181640625
Iteration 700: Loss = -12456.6376953125
Iteration 800: Loss = -12442.4248046875
Iteration 900: Loss = -12430.2333984375
Iteration 1000: Loss = -12419.5126953125
Iteration 1100: Loss = -12409.2646484375
Iteration 1200: Loss = -12401.763671875
Iteration 1300: Loss = -12394.095703125
Iteration 1400: Loss = -12387.8203125
Iteration 1500: Loss = -12381.7197265625
Iteration 1600: Loss = -12377.01953125
Iteration 1700: Loss = -12372.009765625
Iteration 1800: Loss = -12368.0009765625
Iteration 1900: Loss = -12364.0205078125
Iteration 2000: Loss = -12359.392578125
Iteration 2100: Loss = -12355.63671875
Iteration 2200: Loss = -12352.3408203125
Iteration 2300: Loss = -12349.5263671875
Iteration 2400: Loss = -12347.0693359375
Iteration 2500: Loss = -12344.7529296875
Iteration 2600: Loss = -12343.5546875
Iteration 2700: Loss = -12342.6220703125
Iteration 2800: Loss = -12339.796875
Iteration 2900: Loss = -12338.212890625
Iteration 3000: Loss = -12335.544921875
Iteration 3100: Loss = -12333.220703125
Iteration 3200: Loss = -12332.505859375
Iteration 3300: Loss = -12332.177734375
Iteration 3400: Loss = -12331.912109375
Iteration 3500: Loss = -12331.6123046875
Iteration 3600: Loss = -12326.8076171875
Iteration 3700: Loss = -12326.2900390625
Iteration 3800: Loss = -12325.9736328125
Iteration 3900: Loss = -12325.728515625
Iteration 4000: Loss = -12324.3701171875
Iteration 4100: Loss = -12323.099609375
Iteration 4200: Loss = -12322.76953125
Iteration 4300: Loss = -12322.55078125
Iteration 4400: Loss = -12322.373046875
Iteration 4500: Loss = -12322.2275390625
Iteration 4600: Loss = -12322.1044921875
Iteration 4700: Loss = -12321.9931640625
Iteration 4800: Loss = -12321.8759765625
Iteration 4900: Loss = -12320.189453125
Iteration 5000: Loss = -12319.041015625
Iteration 5100: Loss = -12318.8359375
Iteration 5200: Loss = -12316.0498046875
Iteration 5300: Loss = -12315.4453125
Iteration 5400: Loss = -12315.3251953125
Iteration 5500: Loss = -12315.2236328125
Iteration 5600: Loss = -12315.119140625
Iteration 5700: Loss = -12315.03125
Iteration 5800: Loss = -12312.2578125
Iteration 5900: Loss = -12311.4921875
Iteration 6000: Loss = -12311.388671875
Iteration 6100: Loss = -12308.09375
Iteration 6200: Loss = -12307.6865234375
Iteration 6300: Loss = -12307.5927734375
Iteration 6400: Loss = -12307.517578125
Iteration 6500: Loss = -12307.455078125
Iteration 6600: Loss = -12307.396484375
Iteration 6700: Loss = -12302.5751953125
Iteration 6800: Loss = -12299.0830078125
Iteration 6900: Loss = -12298.96484375
Iteration 7000: Loss = -12298.8837890625
Iteration 7100: Loss = -12298.81640625
Iteration 7200: Loss = -12298.7587890625
Iteration 7300: Loss = -12298.7021484375
Iteration 7400: Loss = -12298.650390625
Iteration 7500: Loss = -12298.5966796875
Iteration 7600: Loss = -12298.5390625
Iteration 7700: Loss = -12298.4794921875
Iteration 7800: Loss = -12298.4365234375
Iteration 7900: Loss = -12298.3916015625
Iteration 8000: Loss = -12298.3466796875
Iteration 8100: Loss = -12297.716796875
Iteration 8200: Loss = -12293.84375
Iteration 8300: Loss = -12293.78515625
Iteration 8400: Loss = -12293.75390625
Iteration 8500: Loss = -12293.7353515625
Iteration 8600: Loss = -12293.72265625
Iteration 8700: Loss = -12293.708984375
Iteration 8800: Loss = -12293.69921875
Iteration 8900: Loss = -12293.689453125
Iteration 9000: Loss = -12288.189453125
Iteration 9100: Loss = -12287.6728515625
Iteration 9200: Loss = -12287.5771484375
Iteration 9300: Loss = -12287.52734375
Iteration 9400: Loss = -12287.4921875
Iteration 9500: Loss = -12287.466796875
Iteration 9600: Loss = -12287.447265625
Iteration 9700: Loss = -12287.43359375
Iteration 9800: Loss = -12287.421875
Iteration 9900: Loss = -12287.4091796875
Iteration 10000: Loss = -12287.4013671875
Iteration 10100: Loss = -12287.3935546875
Iteration 10200: Loss = -12287.38671875
Iteration 10300: Loss = -12287.3798828125
Iteration 10400: Loss = -12287.3740234375
Iteration 10500: Loss = -12287.3701171875
Iteration 10600: Loss = -12287.3642578125
Iteration 10700: Loss = -12287.361328125
Iteration 10800: Loss = -12287.357421875
Iteration 10900: Loss = -12287.3525390625
Iteration 11000: Loss = -12287.3505859375
Iteration 11100: Loss = -12287.34765625
Iteration 11200: Loss = -12287.3427734375
Iteration 11300: Loss = -12287.3408203125
Iteration 11400: Loss = -12287.3388671875
Iteration 11500: Loss = -12287.3359375
Iteration 11600: Loss = -12287.3330078125
Iteration 11700: Loss = -12287.33203125
Iteration 11800: Loss = -12287.330078125
Iteration 11900: Loss = -12287.328125
Iteration 12000: Loss = -12287.3271484375
Iteration 12100: Loss = -12287.326171875
Iteration 12200: Loss = -12287.32421875
Iteration 12300: Loss = -12287.322265625
Iteration 12400: Loss = -12287.3203125
Iteration 12500: Loss = -12287.3212890625
1
Iteration 12600: Loss = -12287.3193359375
Iteration 12700: Loss = -12287.3173828125
Iteration 12800: Loss = -12287.31640625
Iteration 12900: Loss = -12287.31640625
Iteration 13000: Loss = -12287.314453125
Iteration 13100: Loss = -12287.314453125
Iteration 13200: Loss = -12287.314453125
Iteration 13300: Loss = -12287.3125
Iteration 13400: Loss = -12287.3115234375
Iteration 13500: Loss = -12287.3095703125
Iteration 13600: Loss = -12287.3115234375
1
Iteration 13700: Loss = -12287.3095703125
Iteration 13800: Loss = -12287.30859375
Iteration 13900: Loss = -12287.30859375
Iteration 14000: Loss = -12287.30859375
Iteration 14100: Loss = -12287.3076171875
Iteration 14200: Loss = -12287.3056640625
Iteration 14300: Loss = -12287.306640625
1
Iteration 14400: Loss = -12287.3056640625
Iteration 14500: Loss = -12287.3056640625
Iteration 14600: Loss = -12287.302734375
Iteration 14700: Loss = -12287.3046875
1
Iteration 14800: Loss = -12287.3046875
2
Iteration 14900: Loss = -12287.3046875
3
Iteration 15000: Loss = -12287.3037109375
4
Iteration 15100: Loss = -12287.3046875
5
Iteration 15200: Loss = -12287.3017578125
Iteration 15300: Loss = -12287.3017578125
Iteration 15400: Loss = -12287.3017578125
Iteration 15500: Loss = -12287.3017578125
Iteration 15600: Loss = -12287.302734375
1
Iteration 15700: Loss = -12287.3017578125
Iteration 15800: Loss = -12287.3017578125
Iteration 15900: Loss = -12287.3017578125
Iteration 16000: Loss = -12287.3017578125
Iteration 16100: Loss = -12287.30078125
Iteration 16200: Loss = -12287.3017578125
1
Iteration 16300: Loss = -12287.3017578125
2
Iteration 16400: Loss = -12287.3017578125
3
Iteration 16500: Loss = -12287.2998046875
Iteration 16600: Loss = -12287.2998046875
Iteration 16700: Loss = -12287.298828125
Iteration 16800: Loss = -12287.3017578125
1
Iteration 16900: Loss = -12287.2998046875
2
Iteration 17000: Loss = -12287.30078125
3
Iteration 17100: Loss = -12287.298828125
Iteration 17200: Loss = -12287.30078125
1
Iteration 17300: Loss = -12287.2998046875
2
Iteration 17400: Loss = -12287.2998046875
3
Iteration 17500: Loss = -12287.298828125
Iteration 17600: Loss = -12287.298828125
Iteration 17700: Loss = -12287.298828125
Iteration 17800: Loss = -12287.2978515625
Iteration 17900: Loss = -12287.298828125
1
Iteration 18000: Loss = -12287.298828125
2
Iteration 18100: Loss = -12287.298828125
3
Iteration 18200: Loss = -12287.298828125
4
Iteration 18300: Loss = -12287.298828125
5
Iteration 18400: Loss = -12287.298828125
6
Iteration 18500: Loss = -12287.2978515625
Iteration 18600: Loss = -12287.298828125
1
Iteration 18700: Loss = -12287.298828125
2
Iteration 18800: Loss = -12287.2978515625
Iteration 18900: Loss = -12287.2998046875
1
Iteration 19000: Loss = -12287.2998046875
2
Iteration 19100: Loss = -12287.298828125
3
Iteration 19200: Loss = -12287.298828125
4
Iteration 19300: Loss = -12287.298828125
5
Iteration 19400: Loss = -12287.2978515625
Iteration 19500: Loss = -12287.2978515625
Iteration 19600: Loss = -12287.2978515625
Iteration 19700: Loss = -12287.2978515625
Iteration 19800: Loss = -12287.298828125
1
Iteration 19900: Loss = -12287.2978515625
Iteration 20000: Loss = -12287.2978515625
Iteration 20100: Loss = -12287.2978515625
Iteration 20200: Loss = -12287.2978515625
Iteration 20300: Loss = -12287.2978515625
Iteration 20400: Loss = -12287.2978515625
Iteration 20500: Loss = -12287.2978515625
Iteration 20600: Loss = -12287.2978515625
Iteration 20700: Loss = -12287.298828125
1
Iteration 20800: Loss = -12287.2978515625
Iteration 20900: Loss = -12287.2978515625
Iteration 21000: Loss = -12287.298828125
1
Iteration 21100: Loss = -12287.2978515625
Iteration 21200: Loss = -12287.2998046875
1
Iteration 21300: Loss = -12287.2978515625
Iteration 21400: Loss = -12287.2978515625
Iteration 21500: Loss = -12287.2978515625
Iteration 21600: Loss = -12287.2978515625
Iteration 21700: Loss = -12287.2998046875
1
Iteration 21800: Loss = -12287.2998046875
2
Iteration 21900: Loss = -12287.2978515625
Iteration 22000: Loss = -12287.2978515625
Iteration 22100: Loss = -12287.298828125
1
Iteration 22200: Loss = -12287.2998046875
2
Iteration 22300: Loss = -12287.298828125
3
Iteration 22400: Loss = -12287.298828125
4
Iteration 22500: Loss = -12287.298828125
5
Iteration 22600: Loss = -12287.3017578125
6
Iteration 22700: Loss = -12287.2978515625
Iteration 22800: Loss = -12287.2978515625
Iteration 22900: Loss = -12287.2978515625
Iteration 23000: Loss = -12287.298828125
1
Iteration 23100: Loss = -12287.2998046875
2
Iteration 23200: Loss = -12287.298828125
3
Iteration 23300: Loss = -12287.2978515625
Iteration 23400: Loss = -12287.298828125
1
Iteration 23500: Loss = -12287.2978515625
Iteration 23600: Loss = -12287.298828125
1
Iteration 23700: Loss = -12287.2978515625
Iteration 23800: Loss = -12287.2978515625
Iteration 23900: Loss = -12287.2978515625
Iteration 24000: Loss = -12287.298828125
1
Iteration 24100: Loss = -12287.2998046875
2
Iteration 24200: Loss = -12287.2998046875
3
Iteration 24300: Loss = -12287.298828125
4
Iteration 24400: Loss = -12287.2978515625
Iteration 24500: Loss = -12287.298828125
1
Iteration 24600: Loss = -12287.2998046875
2
Iteration 24700: Loss = -12287.2998046875
3
Iteration 24800: Loss = -12287.298828125
4
Iteration 24900: Loss = -12287.2978515625
Iteration 25000: Loss = -12287.2978515625
Iteration 25100: Loss = -12287.2998046875
1
Iteration 25200: Loss = -12287.2998046875
2
Iteration 25300: Loss = -12287.298828125
3
Iteration 25400: Loss = -12287.2998046875
4
Iteration 25500: Loss = -12287.2998046875
5
Iteration 25600: Loss = -12287.298828125
6
Iteration 25700: Loss = -12287.2998046875
7
Iteration 25800: Loss = -12287.2998046875
8
Iteration 25900: Loss = -12287.2978515625
Iteration 26000: Loss = -12287.2998046875
1
Iteration 26100: Loss = -12287.296875
Iteration 26200: Loss = -12287.296875
Iteration 26300: Loss = -12287.296875
Iteration 26400: Loss = -12287.296875
Iteration 26500: Loss = -12287.298828125
1
Iteration 26600: Loss = -12287.298828125
2
Iteration 26700: Loss = -12287.298828125
3
Iteration 26800: Loss = -12287.298828125
4
Iteration 26900: Loss = -12287.296875
Iteration 27000: Loss = -12287.298828125
1
Iteration 27100: Loss = -12287.298828125
2
Iteration 27200: Loss = -12287.2978515625
3
Iteration 27300: Loss = -12287.2978515625
4
Iteration 27400: Loss = -12287.298828125
5
Iteration 27500: Loss = -12287.2978515625
6
Iteration 27600: Loss = -12287.298828125
7
Iteration 27700: Loss = -12287.298828125
8
Iteration 27800: Loss = -12287.298828125
9
Iteration 27900: Loss = -12287.298828125
10
Iteration 28000: Loss = -12287.298828125
11
Iteration 28100: Loss = -12287.298828125
12
Iteration 28200: Loss = -12287.296875
Iteration 28300: Loss = -12287.298828125
1
Iteration 28400: Loss = -12287.298828125
2
Iteration 28500: Loss = -12287.296875
Iteration 28600: Loss = -12287.296875
Iteration 28700: Loss = -12287.298828125
1
Iteration 28800: Loss = -12287.298828125
2
Iteration 28900: Loss = -12287.2998046875
3
Iteration 29000: Loss = -12287.298828125
4
Iteration 29100: Loss = -12287.296875
Iteration 29200: Loss = -12287.298828125
1
Iteration 29300: Loss = -12287.2978515625
2
Iteration 29400: Loss = -12287.298828125
3
Iteration 29500: Loss = -12287.296875
Iteration 29600: Loss = -12287.30078125
1
Iteration 29700: Loss = -12287.2978515625
2
Iteration 29800: Loss = -12287.2978515625
3
Iteration 29900: Loss = -12287.298828125
4
pi: tensor([[4.8858e-04, 9.9951e-01],
        [1.7251e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.1668e-05, 9.9999e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2482, 0.1945],
         [0.6930, 0.1972]],

        [[0.2186, 0.2362],
         [0.6527, 0.0147]],

        [[0.1182, 0.2041],
         [0.0125, 0.8462]],

        [[0.4094, 0.2048],
         [0.0293, 0.0276]],

        [[0.0739, 0.1802],
         [0.0566, 0.8073]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -41713.4921875
Iteration 100: Loss = -24537.7890625
Iteration 200: Loss = -15299.86328125
Iteration 300: Loss = -13238.49609375
Iteration 400: Loss = -12765.91015625
Iteration 500: Loss = -12584.8955078125
Iteration 600: Loss = -12494.958984375
Iteration 700: Loss = -12429.4609375
Iteration 800: Loss = -12399.6220703125
Iteration 900: Loss = -12371.890625
Iteration 1000: Loss = -12358.43359375
Iteration 1100: Loss = -12348.486328125
Iteration 1200: Loss = -12339.9326171875
Iteration 1300: Loss = -12325.9130859375
Iteration 1400: Loss = -12317.8203125
Iteration 1500: Loss = -12313.833984375
Iteration 1600: Loss = -12310.80859375
Iteration 1700: Loss = -12308.326171875
Iteration 1800: Loss = -12306.1953125
Iteration 1900: Loss = -12304.3916015625
Iteration 2000: Loss = -12302.8486328125
Iteration 2100: Loss = -12301.505859375
Iteration 2200: Loss = -12300.3251953125
Iteration 2300: Loss = -12299.2763671875
Iteration 2400: Loss = -12298.341796875
Iteration 2500: Loss = -12297.50390625
Iteration 2600: Loss = -12296.75
Iteration 2700: Loss = -12296.06640625
Iteration 2800: Loss = -12295.4482421875
Iteration 2900: Loss = -12294.8837890625
Iteration 3000: Loss = -12294.365234375
Iteration 3100: Loss = -12293.892578125
Iteration 3200: Loss = -12293.4599609375
Iteration 3300: Loss = -12293.060546875
Iteration 3400: Loss = -12292.6943359375
Iteration 3500: Loss = -12292.3544921875
Iteration 3600: Loss = -12292.0439453125
Iteration 3700: Loss = -12291.7548828125
Iteration 3800: Loss = -12291.4873046875
Iteration 3900: Loss = -12291.23828125
Iteration 4000: Loss = -12291.005859375
Iteration 4100: Loss = -12290.791015625
Iteration 4200: Loss = -12290.587890625
Iteration 4300: Loss = -12290.400390625
Iteration 4400: Loss = -12290.224609375
Iteration 4500: Loss = -12290.05859375
Iteration 4600: Loss = -12289.9052734375
Iteration 4700: Loss = -12289.7607421875
Iteration 4800: Loss = -12289.625
Iteration 4900: Loss = -12289.49609375
Iteration 5000: Loss = -12289.376953125
Iteration 5100: Loss = -12289.263671875
Iteration 5200: Loss = -12289.1591796875
Iteration 5300: Loss = -12289.05859375
Iteration 5400: Loss = -12288.966796875
Iteration 5500: Loss = -12288.875
Iteration 5600: Loss = -12288.7919921875
Iteration 5700: Loss = -12288.7138671875
Iteration 5800: Loss = -12288.638671875
Iteration 5900: Loss = -12288.5693359375
Iteration 6000: Loss = -12288.5029296875
Iteration 6100: Loss = -12288.4404296875
Iteration 6200: Loss = -12288.3798828125
Iteration 6300: Loss = -12288.3232421875
Iteration 6400: Loss = -12288.2705078125
Iteration 6500: Loss = -12288.2216796875
Iteration 6600: Loss = -12288.173828125
Iteration 6700: Loss = -12288.12890625
Iteration 6800: Loss = -12288.0849609375
Iteration 6900: Loss = -12288.0439453125
Iteration 7000: Loss = -12288.005859375
Iteration 7100: Loss = -12287.9697265625
Iteration 7200: Loss = -12287.9345703125
Iteration 7300: Loss = -12287.9033203125
Iteration 7400: Loss = -12287.87109375
Iteration 7500: Loss = -12287.84375
Iteration 7600: Loss = -12287.8173828125
Iteration 7700: Loss = -12287.7890625
Iteration 7800: Loss = -12287.763671875
Iteration 7900: Loss = -12287.7412109375
Iteration 8000: Loss = -12287.7177734375
Iteration 8100: Loss = -12287.6982421875
Iteration 8200: Loss = -12287.6767578125
Iteration 8300: Loss = -12287.658203125
Iteration 8400: Loss = -12287.638671875
Iteration 8500: Loss = -12287.623046875
Iteration 8600: Loss = -12287.60546875
Iteration 8700: Loss = -12287.5908203125
Iteration 8800: Loss = -12287.57421875
Iteration 8900: Loss = -12287.5615234375
Iteration 9000: Loss = -12287.546875
Iteration 9100: Loss = -12287.5341796875
Iteration 9200: Loss = -12287.5234375
Iteration 9300: Loss = -12287.5107421875
Iteration 9400: Loss = -12287.4990234375
Iteration 9500: Loss = -12287.4892578125
Iteration 9600: Loss = -12287.4794921875
Iteration 9700: Loss = -12287.470703125
Iteration 9800: Loss = -12287.4609375
Iteration 9900: Loss = -12287.4541015625
Iteration 10000: Loss = -12287.4443359375
Iteration 10100: Loss = -12287.4365234375
Iteration 10200: Loss = -12287.427734375
Iteration 10300: Loss = -12287.4228515625
Iteration 10400: Loss = -12287.416015625
Iteration 10500: Loss = -12287.4091796875
Iteration 10600: Loss = -12287.404296875
Iteration 10700: Loss = -12287.400390625
Iteration 10800: Loss = -12287.392578125
Iteration 10900: Loss = -12287.38671875
Iteration 11000: Loss = -12287.3818359375
Iteration 11100: Loss = -12287.3779296875
Iteration 11200: Loss = -12287.373046875
Iteration 11300: Loss = -12287.3701171875
Iteration 11400: Loss = -12287.3671875
Iteration 11500: Loss = -12287.361328125
Iteration 11600: Loss = -12287.357421875
Iteration 11700: Loss = -12287.3525390625
Iteration 11800: Loss = -12287.3515625
Iteration 11900: Loss = -12287.34765625
Iteration 12000: Loss = -12287.3447265625
Iteration 12100: Loss = -12287.341796875
Iteration 12200: Loss = -12287.337890625
Iteration 12300: Loss = -12287.333984375
Iteration 12400: Loss = -12287.33203125
Iteration 12500: Loss = -12287.3310546875
Iteration 12600: Loss = -12287.326171875
Iteration 12700: Loss = -12287.322265625
Iteration 12800: Loss = -12287.322265625
Iteration 12900: Loss = -12287.3193359375
Iteration 13000: Loss = -12287.3193359375
Iteration 13100: Loss = -12287.31640625
Iteration 13200: Loss = -12287.31640625
Iteration 13300: Loss = -12287.31640625
Iteration 13400: Loss = -12287.3154296875
Iteration 13500: Loss = -12287.3125
Iteration 13600: Loss = -12287.3115234375
Iteration 13700: Loss = -12287.3095703125
Iteration 13800: Loss = -12287.3095703125
Iteration 13900: Loss = -12287.30859375
Iteration 14000: Loss = -12287.306640625
Iteration 14100: Loss = -12286.6279296875
Iteration 14200: Loss = -12284.7431640625
Iteration 14300: Loss = -12284.5751953125
Iteration 14400: Loss = -12284.4765625
Iteration 14500: Loss = -12284.4111328125
Iteration 14600: Loss = -12284.3671875
Iteration 14700: Loss = -12284.3349609375
Iteration 14800: Loss = -12284.3056640625
Iteration 14900: Loss = -12284.279296875
Iteration 15000: Loss = -12284.248046875
Iteration 15100: Loss = -12284.2119140625
Iteration 15200: Loss = -12284.171875
Iteration 15300: Loss = -12284.1181640625
Iteration 15400: Loss = -12284.05859375
Iteration 15500: Loss = -12283.9892578125
Iteration 15600: Loss = -12283.9150390625
Iteration 15700: Loss = -12283.84765625
Iteration 15800: Loss = -12283.796875
Iteration 15900: Loss = -12283.75390625
Iteration 16000: Loss = -12283.70703125
Iteration 16100: Loss = -12283.6875
Iteration 16200: Loss = -12283.66796875
Iteration 16300: Loss = -12283.6552734375
Iteration 16400: Loss = -12283.63671875
Iteration 16500: Loss = -12283.6171875
Iteration 16600: Loss = -12283.5966796875
Iteration 16700: Loss = -12283.583984375
Iteration 16800: Loss = -12283.5771484375
Iteration 16900: Loss = -12283.572265625
Iteration 17000: Loss = -12283.5673828125
Iteration 17100: Loss = -12283.5615234375
Iteration 17200: Loss = -12283.5478515625
Iteration 17300: Loss = -12283.5380859375
Iteration 17400: Loss = -12283.5205078125
Iteration 17500: Loss = -12283.5078125
Iteration 17600: Loss = -12283.5078125
Iteration 17700: Loss = -12283.4990234375
Iteration 17800: Loss = -12283.4951171875
Iteration 17900: Loss = -12283.49609375
1
Iteration 18000: Loss = -12283.4970703125
2
Iteration 18100: Loss = -12283.4970703125
3
Iteration 18200: Loss = -12283.49609375
4
Iteration 18300: Loss = -12283.4970703125
5
Iteration 18400: Loss = -12283.4970703125
6
Iteration 18500: Loss = -12283.494140625
Iteration 18600: Loss = -12283.4951171875
1
Iteration 18700: Loss = -12283.4951171875
2
Iteration 18800: Loss = -12283.4951171875
3
Iteration 18900: Loss = -12283.4970703125
4
Iteration 19000: Loss = -12283.49609375
5
Iteration 19100: Loss = -12283.4951171875
6
Iteration 19200: Loss = -12283.4951171875
7
Iteration 19300: Loss = -12283.4970703125
8
Iteration 19400: Loss = -12283.4951171875
9
Iteration 19500: Loss = -12283.49609375
10
Iteration 19600: Loss = -12283.494140625
Iteration 19700: Loss = -12283.494140625
Iteration 19800: Loss = -12283.494140625
Iteration 19900: Loss = -12283.4951171875
1
Iteration 20000: Loss = -12283.49609375
2
Iteration 20100: Loss = -12283.4951171875
3
Iteration 20200: Loss = -12283.494140625
Iteration 20300: Loss = -12283.498046875
1
Iteration 20400: Loss = -12283.4951171875
2
Iteration 20500: Loss = -12283.49609375
3
Iteration 20600: Loss = -12283.494140625
Iteration 20700: Loss = -12283.498046875
1
Iteration 20800: Loss = -12283.494140625
Iteration 20900: Loss = -12283.4951171875
1
Iteration 21000: Loss = -12283.49609375
2
Iteration 21100: Loss = -12283.4951171875
3
Iteration 21200: Loss = -12283.4951171875
4
Iteration 21300: Loss = -12283.4921875
Iteration 21400: Loss = -12283.4931640625
1
Iteration 21500: Loss = -12283.494140625
2
Iteration 21600: Loss = -12283.4970703125
3
Iteration 21700: Loss = -12283.4951171875
4
Iteration 21800: Loss = -12283.494140625
5
Iteration 21900: Loss = -12283.4951171875
6
Iteration 22000: Loss = -12283.4931640625
7
Iteration 22100: Loss = -12283.494140625
8
Iteration 22200: Loss = -12283.49609375
9
Iteration 22300: Loss = -12283.4951171875
10
Iteration 22400: Loss = -12283.4931640625
11
Iteration 22500: Loss = -12283.4931640625
12
Iteration 22600: Loss = -12283.4931640625
13
Iteration 22700: Loss = -12283.4951171875
14
Iteration 22800: Loss = -12283.4951171875
15
Stopping early at iteration 22800 due to no improvement.
pi: tensor([[9.8519e-01, 1.4812e-02],
        [9.9977e-01, 2.3314e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9967e-01, 3.2568e-04], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1978, 0.1874],
         [0.7817, 0.2071]],

        [[0.0575, 0.2436],
         [0.9932, 0.0693]],

        [[0.2867, 0.2035],
         [0.0070, 0.9893]],

        [[0.0349, 0.0630],
         [0.2990, 0.6373]],

        [[0.9807, 0.1695],
         [0.1859, 0.0142]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0009285052269176978
Average Adjusted Rand Index: 0.0008888888888888889
[0.0, 0.0009285052269176978] [0.0, 0.0008888888888888889] [12287.2978515625, 12283.4951171875]
-------------------------------------
This iteration is 51
True Objective function: Loss = -11865.818419698497
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -41356.66015625
Iteration 100: Loss = -29795.1171875
Iteration 200: Loss = -21093.66796875
Iteration 300: Loss = -15751.41796875
Iteration 400: Loss = -13495.1943359375
Iteration 500: Loss = -12764.5439453125
Iteration 600: Loss = -12544.4326171875
Iteration 700: Loss = -12465.07421875
Iteration 800: Loss = -12441.017578125
Iteration 900: Loss = -12424.4677734375
Iteration 1000: Loss = -12411.6689453125
Iteration 1100: Loss = -12403.1953125
Iteration 1200: Loss = -12397.38671875
Iteration 1300: Loss = -12393.3173828125
Iteration 1400: Loss = -12389.4677734375
Iteration 1500: Loss = -12387.08984375
Iteration 1600: Loss = -12384.01171875
Iteration 1700: Loss = -12383.2744140625
Iteration 1800: Loss = -12382.7041015625
Iteration 1900: Loss = -12382.0947265625
Iteration 2000: Loss = -12381.548828125
Iteration 2100: Loss = -12381.0810546875
Iteration 2200: Loss = -12380.373046875
Iteration 2300: Loss = -12379.451171875
Iteration 2400: Loss = -12377.634765625
Iteration 2500: Loss = -12376.15625
Iteration 2600: Loss = -12373.095703125
Iteration 2700: Loss = -12370.4404296875
Iteration 2800: Loss = -12365.51171875
Iteration 2900: Loss = -12364.515625
Iteration 3000: Loss = -12363.94140625
Iteration 3100: Loss = -12363.623046875
Iteration 3200: Loss = -12363.412109375
Iteration 3300: Loss = -12363.2421875
Iteration 3400: Loss = -12363.09375
Iteration 3500: Loss = -12362.9638671875
Iteration 3600: Loss = -12362.853515625
Iteration 3700: Loss = -12362.76171875
Iteration 3800: Loss = -12362.6806640625
Iteration 3900: Loss = -12362.61328125
Iteration 4000: Loss = -12362.55078125
Iteration 4100: Loss = -12362.4951171875
Iteration 4200: Loss = -12362.4443359375
Iteration 4300: Loss = -12362.4013671875
Iteration 4400: Loss = -12362.3623046875
Iteration 4500: Loss = -12362.328125
Iteration 4600: Loss = -12362.2958984375
Iteration 4700: Loss = -12362.265625
Iteration 4800: Loss = -12362.23828125
Iteration 4900: Loss = -12362.2138671875
Iteration 5000: Loss = -12362.189453125
Iteration 5100: Loss = -12362.169921875
Iteration 5200: Loss = -12362.150390625
Iteration 5300: Loss = -12362.1318359375
Iteration 5400: Loss = -12362.115234375
Iteration 5500: Loss = -12362.099609375
Iteration 5600: Loss = -12362.0830078125
Iteration 5700: Loss = -12362.0712890625
Iteration 5800: Loss = -12362.0576171875
Iteration 5900: Loss = -12362.046875
Iteration 6000: Loss = -12362.0341796875
Iteration 6100: Loss = -12362.0234375
Iteration 6200: Loss = -12362.0146484375
Iteration 6300: Loss = -12362.00390625
Iteration 6400: Loss = -12361.99609375
Iteration 6500: Loss = -12361.9873046875
Iteration 6600: Loss = -12361.98046875
Iteration 6700: Loss = -12361.9736328125
Iteration 6800: Loss = -12361.9658203125
Iteration 6900: Loss = -12361.958984375
Iteration 7000: Loss = -12361.953125
Iteration 7100: Loss = -12361.9482421875
Iteration 7200: Loss = -12361.9423828125
Iteration 7300: Loss = -12361.9384765625
Iteration 7400: Loss = -12361.9345703125
Iteration 7500: Loss = -12361.9287109375
Iteration 7600: Loss = -12361.92578125
Iteration 7700: Loss = -12361.9208984375
Iteration 7800: Loss = -12361.9189453125
Iteration 7900: Loss = -12361.9140625
Iteration 8000: Loss = -12361.912109375
Iteration 8100: Loss = -12361.908203125
Iteration 8200: Loss = -12361.9052734375
Iteration 8300: Loss = -12361.904296875
Iteration 8400: Loss = -12361.900390625
Iteration 8500: Loss = -12361.8974609375
Iteration 8600: Loss = -12361.896484375
Iteration 8700: Loss = -12361.8935546875
Iteration 8800: Loss = -12361.892578125
Iteration 8900: Loss = -12361.8916015625
Iteration 9000: Loss = -12361.888671875
Iteration 9100: Loss = -12361.8876953125
Iteration 9200: Loss = -12361.88671875
Iteration 9300: Loss = -12361.884765625
Iteration 9400: Loss = -12361.8828125
Iteration 9500: Loss = -12361.8818359375
Iteration 9600: Loss = -12361.8798828125
Iteration 9700: Loss = -12361.87890625
Iteration 9800: Loss = -12361.87890625
Iteration 9900: Loss = -12361.8779296875
Iteration 10000: Loss = -12361.8759765625
Iteration 10100: Loss = -12361.875
Iteration 10200: Loss = -12361.8740234375
Iteration 10300: Loss = -12361.87109375
Iteration 10400: Loss = -12361.8720703125
1
Iteration 10500: Loss = -12361.869140625
Iteration 10600: Loss = -12361.8681640625
Iteration 10700: Loss = -12361.8671875
Iteration 10800: Loss = -12361.865234375
Iteration 10900: Loss = -12361.8642578125
Iteration 11000: Loss = -12361.865234375
1
Iteration 11100: Loss = -12361.8642578125
Iteration 11200: Loss = -12361.86328125
Iteration 11300: Loss = -12361.8642578125
1
Iteration 11400: Loss = -12361.8623046875
Iteration 11500: Loss = -12361.86328125
1
Iteration 11600: Loss = -12361.861328125
Iteration 11700: Loss = -12361.8623046875
1
Iteration 11800: Loss = -12361.861328125
Iteration 11900: Loss = -12361.8623046875
1
Iteration 12000: Loss = -12361.861328125
Iteration 12100: Loss = -12361.8603515625
Iteration 12200: Loss = -12361.857421875
Iteration 12300: Loss = -12361.859375
1
Iteration 12400: Loss = -12361.859375
2
Iteration 12500: Loss = -12361.859375
3
Iteration 12600: Loss = -12361.8583984375
4
Iteration 12700: Loss = -12361.8564453125
Iteration 12800: Loss = -12361.8564453125
Iteration 12900: Loss = -12361.85546875
Iteration 13000: Loss = -12361.8564453125
1
Iteration 13100: Loss = -12361.8564453125
2
Iteration 13200: Loss = -12361.85546875
Iteration 13300: Loss = -12361.8544921875
Iteration 13400: Loss = -12361.8544921875
Iteration 13500: Loss = -12361.8544921875
Iteration 13600: Loss = -12361.8544921875
Iteration 13700: Loss = -12361.8564453125
1
Iteration 13800: Loss = -12361.8544921875
Iteration 13900: Loss = -12361.8525390625
Iteration 14000: Loss = -12361.853515625
1
Iteration 14100: Loss = -12361.8525390625
Iteration 14200: Loss = -12361.853515625
1
Iteration 14300: Loss = -12361.853515625
2
Iteration 14400: Loss = -12361.8515625
Iteration 14500: Loss = -12361.8525390625
1
Iteration 14600: Loss = -12361.8525390625
2
Iteration 14700: Loss = -12361.8525390625
3
Iteration 14800: Loss = -12361.8525390625
4
Iteration 14900: Loss = -12361.853515625
5
Iteration 15000: Loss = -12361.8525390625
6
Iteration 15100: Loss = -12361.8525390625
7
Iteration 15200: Loss = -12361.8515625
Iteration 15300: Loss = -12361.8515625
Iteration 15400: Loss = -12361.8515625
Iteration 15500: Loss = -12361.849609375
Iteration 15600: Loss = -12361.8486328125
Iteration 15700: Loss = -12361.849609375
1
Iteration 15800: Loss = -12361.849609375
2
Iteration 15900: Loss = -12361.84765625
Iteration 16000: Loss = -12361.84765625
Iteration 16100: Loss = -12361.849609375
1
Iteration 16200: Loss = -12361.84765625
Iteration 16300: Loss = -12361.8486328125
1
Iteration 16400: Loss = -12361.845703125
Iteration 16500: Loss = -12361.84765625
1
Iteration 16600: Loss = -12361.8466796875
2
Iteration 16700: Loss = -12361.8486328125
3
Iteration 16800: Loss = -12361.845703125
Iteration 16900: Loss = -12361.845703125
Iteration 17000: Loss = -12361.8466796875
1
Iteration 17100: Loss = -12361.8466796875
2
Iteration 17200: Loss = -12361.8466796875
3
Iteration 17300: Loss = -12361.8466796875
4
Iteration 17400: Loss = -12361.845703125
Iteration 17500: Loss = -12361.84375
Iteration 17600: Loss = -12361.8447265625
1
Iteration 17700: Loss = -12361.84375
Iteration 17800: Loss = -12361.8427734375
Iteration 17900: Loss = -12361.8427734375
Iteration 18000: Loss = -12361.8408203125
Iteration 18100: Loss = -12361.8408203125
Iteration 18200: Loss = -12361.83984375
Iteration 18300: Loss = -12361.8408203125
1
Iteration 18400: Loss = -12361.8388671875
Iteration 18500: Loss = -12361.8408203125
1
Iteration 18600: Loss = -12361.83984375
2
Iteration 18700: Loss = -12361.8408203125
3
Iteration 18800: Loss = -12361.8388671875
Iteration 18900: Loss = -12361.8388671875
Iteration 19000: Loss = -12361.83984375
1
Iteration 19100: Loss = -12361.83984375
2
Iteration 19200: Loss = -12361.841796875
3
Iteration 19300: Loss = -12361.8388671875
Iteration 19400: Loss = -12361.83984375
1
Iteration 19500: Loss = -12361.83984375
2
Iteration 19600: Loss = -12361.837890625
Iteration 19700: Loss = -12361.83984375
1
Iteration 19800: Loss = -12361.83984375
2
Iteration 19900: Loss = -12361.8388671875
3
Iteration 20000: Loss = -12361.8369140625
Iteration 20100: Loss = -12361.83984375
1
Iteration 20200: Loss = -12361.837890625
2
Iteration 20300: Loss = -12361.8388671875
3
Iteration 20400: Loss = -12361.8369140625
Iteration 20500: Loss = -12361.837890625
1
Iteration 20600: Loss = -12361.8359375
Iteration 20700: Loss = -12361.8369140625
1
Iteration 20800: Loss = -12361.8359375
Iteration 20900: Loss = -12361.837890625
1
Iteration 21000: Loss = -12361.8359375
Iteration 21100: Loss = -12361.8369140625
1
Iteration 21200: Loss = -12361.8359375
Iteration 21300: Loss = -12361.8369140625
1
Iteration 21400: Loss = -12361.8408203125
2
Iteration 21500: Loss = -12361.8359375
Iteration 21600: Loss = -12361.837890625
1
Iteration 21700: Loss = -12361.8369140625
2
Iteration 21800: Loss = -12361.8369140625
3
Iteration 21900: Loss = -12361.837890625
4
Iteration 22000: Loss = -12361.8359375
Iteration 22100: Loss = -12361.8359375
Iteration 22200: Loss = -12361.8359375
Iteration 22300: Loss = -12361.837890625
1
Iteration 22400: Loss = -12361.8359375
Iteration 22500: Loss = -12361.8369140625
1
Iteration 22600: Loss = -12361.8359375
Iteration 22700: Loss = -12361.8359375
Iteration 22800: Loss = -12361.8359375
Iteration 22900: Loss = -12361.8369140625
1
Iteration 23000: Loss = -12361.8369140625
2
Iteration 23100: Loss = -12361.8359375
Iteration 23200: Loss = -12361.8388671875
1
Iteration 23300: Loss = -12361.8359375
Iteration 23400: Loss = -12361.8359375
Iteration 23500: Loss = -12361.8359375
Iteration 23600: Loss = -12361.8359375
Iteration 23700: Loss = -12361.8369140625
1
Iteration 23800: Loss = -12361.8369140625
2
Iteration 23900: Loss = -12361.8369140625
3
Iteration 24000: Loss = -12361.8369140625
4
Iteration 24100: Loss = -12361.8359375
Iteration 24200: Loss = -12361.8359375
Iteration 24300: Loss = -12361.8369140625
1
Iteration 24400: Loss = -12361.8359375
Iteration 24500: Loss = -12361.8369140625
1
Iteration 24600: Loss = -12361.8369140625
2
Iteration 24700: Loss = -12361.8359375
Iteration 24800: Loss = -12361.8359375
Iteration 24900: Loss = -12361.8369140625
1
Iteration 25000: Loss = -12361.8359375
Iteration 25100: Loss = -12361.8369140625
1
Iteration 25200: Loss = -12361.8369140625
2
Iteration 25300: Loss = -12361.8359375
Iteration 25400: Loss = -12361.8349609375
Iteration 25500: Loss = -12361.8359375
1
Iteration 25600: Loss = -12361.8369140625
2
Iteration 25700: Loss = -12361.8359375
3
Iteration 25800: Loss = -12361.8369140625
4
Iteration 25900: Loss = -12361.8349609375
Iteration 26000: Loss = -12361.837890625
1
Iteration 26100: Loss = -12361.8359375
2
Iteration 26200: Loss = -12361.8359375
3
Iteration 26300: Loss = -12361.8349609375
Iteration 26400: Loss = -12361.8359375
1
Iteration 26500: Loss = -12361.8359375
2
Iteration 26600: Loss = -12361.8359375
3
Iteration 26700: Loss = -12361.8359375
4
Iteration 26800: Loss = -12361.8349609375
Iteration 26900: Loss = -12361.8369140625
1
Iteration 27000: Loss = -12361.8359375
2
Iteration 27100: Loss = -12361.8369140625
3
Iteration 27200: Loss = -12361.837890625
4
Iteration 27300: Loss = -12361.8359375
5
Iteration 27400: Loss = -12361.8359375
6
Iteration 27500: Loss = -12361.8359375
7
Iteration 27600: Loss = -12361.8369140625
8
Iteration 27700: Loss = -12361.8369140625
9
Iteration 27800: Loss = -12361.8359375
10
Iteration 27900: Loss = -12361.8349609375
Iteration 28000: Loss = -12361.8359375
1
Iteration 28100: Loss = -12361.837890625
2
Iteration 28200: Loss = -12361.8359375
3
Iteration 28300: Loss = -12361.8359375
4
Iteration 28400: Loss = -12361.8369140625
5
Iteration 28500: Loss = -12361.8369140625
6
Iteration 28600: Loss = -12361.8369140625
7
Iteration 28700: Loss = -12361.8369140625
8
Iteration 28800: Loss = -12361.8359375
9
Iteration 28900: Loss = -12361.8369140625
10
Iteration 29000: Loss = -12361.8359375
11
Iteration 29100: Loss = -12361.8369140625
12
Iteration 29200: Loss = -12361.8349609375
Iteration 29300: Loss = -12361.8359375
1
Iteration 29400: Loss = -12361.8349609375
Iteration 29500: Loss = -12361.8369140625
1
Iteration 29600: Loss = -12361.8369140625
2
Iteration 29700: Loss = -12361.8359375
3
Iteration 29800: Loss = -12361.8369140625
4
Iteration 29900: Loss = -12361.8369140625
5
pi: tensor([[9.9840e-01, 1.6045e-03],
        [9.9999e-01, 5.7236e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9067, 0.0933], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2006, 0.1432],
         [0.8709, 0.6837]],

        [[0.9169, 0.1868],
         [0.9829, 0.6068]],

        [[0.3887, 0.1602],
         [0.0430, 0.9931]],

        [[0.0558, 0.2686],
         [0.1044, 0.6639]],

        [[0.2763, 0.1992],
         [0.9469, 0.9324]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.02918749315918129
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.007074483026103074
Average Adjusted Rand Index: 0.005837498631836259
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31932.912109375
Iteration 100: Loss = -20906.8515625
Iteration 200: Loss = -15136.8671875
Iteration 300: Loss = -13532.6083984375
Iteration 400: Loss = -12983.0048828125
Iteration 500: Loss = -12726.708984375
Iteration 600: Loss = -12607.546875
Iteration 700: Loss = -12538.3974609375
Iteration 800: Loss = -12500.5576171875
Iteration 900: Loss = -12480.83984375
Iteration 1000: Loss = -12467.06640625
Iteration 1100: Loss = -12454.5830078125
Iteration 1200: Loss = -12446.8212890625
Iteration 1300: Loss = -12440.982421875
Iteration 1400: Loss = -12436.1552734375
Iteration 1500: Loss = -12431.0
Iteration 1600: Loss = -12427.1240234375
Iteration 1700: Loss = -12424.533203125
Iteration 1800: Loss = -12422.306640625
Iteration 1900: Loss = -12419.7744140625
Iteration 2000: Loss = -12413.58984375
Iteration 2100: Loss = -12406.6162109375
Iteration 2200: Loss = -12403.734375
Iteration 2300: Loss = -12400.2490234375
Iteration 2400: Loss = -12398.2392578125
Iteration 2500: Loss = -12396.1669921875
Iteration 2600: Loss = -12393.3125
Iteration 2700: Loss = -12391.8330078125
Iteration 2800: Loss = -12390.6123046875
Iteration 2900: Loss = -12389.8369140625
Iteration 3000: Loss = -12389.3046875
Iteration 3100: Loss = -12387.9794921875
Iteration 3200: Loss = -12382.8525390625
Iteration 3300: Loss = -12382.185546875
Iteration 3400: Loss = -12381.697265625
Iteration 3500: Loss = -12380.8203125
Iteration 3600: Loss = -12378.38671875
Iteration 3700: Loss = -12377.5185546875
Iteration 3800: Loss = -12377.1767578125
Iteration 3900: Loss = -12376.943359375
Iteration 4000: Loss = -12376.73046875
Iteration 4100: Loss = -12376.423828125
Iteration 4200: Loss = -12373.748046875
Iteration 4300: Loss = -12372.4345703125
Iteration 4400: Loss = -12372.16015625
Iteration 4500: Loss = -12371.7705078125
Iteration 4600: Loss = -12371.6181640625
Iteration 4700: Loss = -12370.814453125
Iteration 4800: Loss = -12369.966796875
Iteration 4900: Loss = -12369.8505859375
Iteration 5000: Loss = -12369.7373046875
Iteration 5100: Loss = -12369.5068359375
Iteration 5200: Loss = -12369.4150390625
Iteration 5300: Loss = -12369.3388671875
Iteration 5400: Loss = -12369.2685546875
Iteration 5500: Loss = -12369.205078125
Iteration 5600: Loss = -12369.150390625
Iteration 5700: Loss = -12369.1015625
Iteration 5800: Loss = -12369.0556640625
Iteration 5900: Loss = -12369.0146484375
Iteration 6000: Loss = -12368.9775390625
Iteration 6100: Loss = -12368.9423828125
Iteration 6200: Loss = -12368.9072265625
Iteration 6300: Loss = -12368.8779296875
Iteration 6400: Loss = -12368.845703125
Iteration 6500: Loss = -12368.8173828125
Iteration 6600: Loss = -12368.7919921875
Iteration 6700: Loss = -12368.7646484375
Iteration 6800: Loss = -12368.7392578125
Iteration 6900: Loss = -12368.7138671875
Iteration 7000: Loss = -12368.69140625
Iteration 7100: Loss = -12368.671875
Iteration 7200: Loss = -12368.6484375
Iteration 7300: Loss = -12368.6318359375
Iteration 7400: Loss = -12368.611328125
Iteration 7500: Loss = -12368.59375
Iteration 7600: Loss = -12368.576171875
Iteration 7700: Loss = -12368.55859375
Iteration 7800: Loss = -12368.515625
Iteration 7900: Loss = -12368.3076171875
Iteration 8000: Loss = -12368.2890625
Iteration 8100: Loss = -12368.2666015625
Iteration 8200: Loss = -12368.2529296875
Iteration 8300: Loss = -12368.23828125
Iteration 8400: Loss = -12368.2255859375
Iteration 8500: Loss = -12368.208984375
Iteration 8600: Loss = -12368.1923828125
Iteration 8700: Loss = -12368.177734375
Iteration 8800: Loss = -12368.166015625
Iteration 8900: Loss = -12368.1533203125
Iteration 9000: Loss = -12368.1435546875
Iteration 9100: Loss = -12368.1328125
Iteration 9200: Loss = -12368.0556640625
Iteration 9300: Loss = -12368.0478515625
Iteration 9400: Loss = -12368.0419921875
Iteration 9500: Loss = -12368.033203125
Iteration 9600: Loss = -12368.01171875
Iteration 9700: Loss = -12368.0048828125
Iteration 9800: Loss = -12367.998046875
Iteration 9900: Loss = -12367.9765625
Iteration 10000: Loss = -12367.9599609375
Iteration 10100: Loss = -12367.943359375
Iteration 10200: Loss = -12367.84375
Iteration 10300: Loss = -12367.8359375
Iteration 10400: Loss = -12367.8310546875
Iteration 10500: Loss = -12367.8251953125
Iteration 10600: Loss = -12367.8212890625
Iteration 10700: Loss = -12367.814453125
Iteration 10800: Loss = -12367.8095703125
Iteration 10900: Loss = -12367.8037109375
Iteration 11000: Loss = -12367.7978515625
Iteration 11100: Loss = -12367.7919921875
Iteration 11200: Loss = -12367.75
Iteration 11300: Loss = -12367.73828125
Iteration 11400: Loss = -12367.7314453125
Iteration 11500: Loss = -12367.724609375
Iteration 11600: Loss = -12367.7197265625
Iteration 11700: Loss = -12367.7138671875
Iteration 11800: Loss = -12367.705078125
Iteration 11900: Loss = -12367.6923828125
Iteration 12000: Loss = -12367.685546875
Iteration 12100: Loss = -12367.6796875
Iteration 12200: Loss = -12367.6708984375
Iteration 12300: Loss = -12367.6650390625
Iteration 12400: Loss = -12367.6611328125
Iteration 12500: Loss = -12367.658203125
Iteration 12600: Loss = -12367.65625
Iteration 12700: Loss = -12367.6533203125
Iteration 12800: Loss = -12367.65234375
Iteration 12900: Loss = -12367.650390625
Iteration 13000: Loss = -12367.6484375
Iteration 13100: Loss = -12367.6474609375
Iteration 13200: Loss = -12367.646484375
Iteration 13300: Loss = -12367.6455078125
Iteration 13400: Loss = -12367.64453125
Iteration 13500: Loss = -12367.6455078125
1
Iteration 13600: Loss = -12367.6416015625
Iteration 13700: Loss = -12367.6396484375
Iteration 13800: Loss = -12366.2099609375
Iteration 13900: Loss = -12366.169921875
Iteration 14000: Loss = -12366.1689453125
Iteration 14100: Loss = -12366.1669921875
Iteration 14200: Loss = -12366.1640625
Iteration 14300: Loss = -12366.16015625
Iteration 14400: Loss = -12366.154296875
Iteration 14500: Loss = -12366.1513671875
Iteration 14600: Loss = -12366.1533203125
1
Iteration 14700: Loss = -12366.150390625
Iteration 14800: Loss = -12366.1494140625
Iteration 14900: Loss = -12366.1494140625
Iteration 15000: Loss = -12366.1474609375
Iteration 15100: Loss = -12366.1474609375
Iteration 15200: Loss = -12366.1455078125
Iteration 15300: Loss = -12366.146484375
1
Iteration 15400: Loss = -12366.146484375
2
Iteration 15500: Loss = -12366.14453125
Iteration 15600: Loss = -12366.1455078125
1
Iteration 15700: Loss = -12366.1455078125
2
Iteration 15800: Loss = -12366.078125
Iteration 15900: Loss = -12363.6142578125
Iteration 16000: Loss = -12363.6103515625
Iteration 16100: Loss = -12363.609375
Iteration 16200: Loss = -12363.607421875
Iteration 16300: Loss = -12363.6083984375
1
Iteration 16400: Loss = -12363.607421875
Iteration 16500: Loss = -12363.6083984375
1
Iteration 16600: Loss = -12363.607421875
Iteration 16700: Loss = -12363.6064453125
Iteration 16800: Loss = -12363.6044921875
Iteration 16900: Loss = -12363.6064453125
1
Iteration 17000: Loss = -12363.60546875
2
Iteration 17100: Loss = -12363.6064453125
3
Iteration 17200: Loss = -12363.60546875
4
Iteration 17300: Loss = -12363.6044921875
Iteration 17400: Loss = -12363.6064453125
1
Iteration 17500: Loss = -12363.6064453125
2
Iteration 17600: Loss = -12363.60546875
3
Iteration 17700: Loss = -12363.6044921875
Iteration 17800: Loss = -12363.60546875
1
Iteration 17900: Loss = -12363.60546875
2
Iteration 18000: Loss = -12363.60546875
3
Iteration 18100: Loss = -12363.60546875
4
Iteration 18200: Loss = -12363.60546875
5
Iteration 18300: Loss = -12363.60546875
6
Iteration 18400: Loss = -12363.60546875
7
Iteration 18500: Loss = -12363.60546875
8
Iteration 18600: Loss = -12363.6044921875
Iteration 18700: Loss = -12363.60546875
1
Iteration 18800: Loss = -12363.60546875
2
Iteration 18900: Loss = -12363.603515625
Iteration 19000: Loss = -12363.6044921875
1
Iteration 19100: Loss = -12363.6044921875
2
Iteration 19200: Loss = -12363.6044921875
3
Iteration 19300: Loss = -12363.603515625
Iteration 19400: Loss = -12363.6044921875
1
Iteration 19500: Loss = -12363.603515625
Iteration 19600: Loss = -12363.6044921875
1
Iteration 19700: Loss = -12363.6064453125
2
Iteration 19800: Loss = -12363.6044921875
3
Iteration 19900: Loss = -12363.60546875
4
Iteration 20000: Loss = -12363.6044921875
5
Iteration 20100: Loss = -12363.6044921875
6
Iteration 20200: Loss = -12363.603515625
Iteration 20300: Loss = -12363.603515625
Iteration 20400: Loss = -12363.6044921875
1
Iteration 20500: Loss = -12363.603515625
Iteration 20600: Loss = -12363.60546875
1
Iteration 20700: Loss = -12363.603515625
Iteration 20800: Loss = -12363.603515625
Iteration 20900: Loss = -12363.603515625
Iteration 21000: Loss = -12363.603515625
Iteration 21100: Loss = -12363.603515625
Iteration 21200: Loss = -12363.6025390625
Iteration 21300: Loss = -12363.6044921875
1
Iteration 21400: Loss = -12363.603515625
2
Iteration 21500: Loss = -12363.6044921875
3
Iteration 21600: Loss = -12363.603515625
4
Iteration 21700: Loss = -12363.603515625
5
Iteration 21800: Loss = -12363.6025390625
Iteration 21900: Loss = -12363.6044921875
1
Iteration 22000: Loss = -12363.6025390625
Iteration 22100: Loss = -12363.603515625
1
Iteration 22200: Loss = -12363.6025390625
Iteration 22300: Loss = -12363.603515625
1
Iteration 22400: Loss = -12363.603515625
2
Iteration 22500: Loss = -12363.603515625
3
Iteration 22600: Loss = -12363.6044921875
4
Iteration 22700: Loss = -12363.603515625
5
Iteration 22800: Loss = -12363.603515625
6
Iteration 22900: Loss = -12363.603515625
7
Iteration 23000: Loss = -12363.603515625
8
Iteration 23100: Loss = -12363.6015625
Iteration 23200: Loss = -12363.603515625
1
Iteration 23300: Loss = -12363.6025390625
2
Iteration 23400: Loss = -12363.6025390625
3
Iteration 23500: Loss = -12363.6044921875
4
Iteration 23600: Loss = -12363.6025390625
5
Iteration 23700: Loss = -12363.60546875
6
Iteration 23800: Loss = -12363.603515625
7
Iteration 23900: Loss = -12363.6044921875
8
Iteration 24000: Loss = -12363.6025390625
9
Iteration 24100: Loss = -12363.603515625
10
Iteration 24200: Loss = -12363.603515625
11
Iteration 24300: Loss = -12363.6025390625
12
Iteration 24400: Loss = -12363.603515625
13
Iteration 24500: Loss = -12363.603515625
14
Iteration 24600: Loss = -12363.6044921875
15
Stopping early at iteration 24600 due to no improvement.
pi: tensor([[1.0000e+00, 1.9665e-06],
        [8.3104e-01, 1.6896e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([2.1032e-05, 9.9998e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2012, 0.1977],
         [0.6616, 0.1950]],

        [[0.1566, 0.1960],
         [0.0129, 0.2562]],

        [[0.0263, 0.1661],
         [0.9858, 0.0809]],

        [[0.9565, 0.2772],
         [0.8563, 0.9338]],

        [[0.9912, 0.7974],
         [0.5803, 0.9907]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0024296332119278135
Average Adjusted Rand Index: 0.0
[0.007074483026103074, 0.0024296332119278135] [0.005837498631836259, 0.0] [12361.837890625, 12363.6044921875]
-------------------------------------
This iteration is 52
True Objective function: Loss = -11732.017535829336
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -44964.69140625
Iteration 100: Loss = -28092.8125
Iteration 200: Loss = -16515.794921875
Iteration 300: Loss = -13229.0849609375
Iteration 400: Loss = -12709.330078125
Iteration 500: Loss = -12550.154296875
Iteration 600: Loss = -12464.4560546875
Iteration 700: Loss = -12407.841796875
Iteration 800: Loss = -12368.21484375
Iteration 900: Loss = -12338.2333984375
Iteration 1000: Loss = -12316.09375
Iteration 1100: Loss = -12293.6953125
Iteration 1200: Loss = -12284.50390625
Iteration 1300: Loss = -12277.2236328125
Iteration 1400: Loss = -12267.00390625
Iteration 1500: Loss = -12261.75390625
Iteration 1600: Loss = -12249.97265625
Iteration 1700: Loss = -12242.5849609375
Iteration 1800: Loss = -12238.802734375
Iteration 1900: Loss = -12234.1044921875
Iteration 2000: Loss = -12231.208984375
Iteration 2100: Loss = -12228.9208984375
Iteration 2200: Loss = -12227.0361328125
Iteration 2300: Loss = -12225.451171875
Iteration 2400: Loss = -12224.0927734375
Iteration 2500: Loss = -12222.91796875
Iteration 2600: Loss = -12221.8876953125
Iteration 2700: Loss = -12220.9794921875
Iteration 2800: Loss = -12220.171875
Iteration 2900: Loss = -12219.4501953125
Iteration 3000: Loss = -12218.802734375
Iteration 3100: Loss = -12218.21875
Iteration 3200: Loss = -12217.689453125
Iteration 3300: Loss = -12217.208984375
Iteration 3400: Loss = -12216.7724609375
Iteration 3500: Loss = -12216.37109375
Iteration 3600: Loss = -12216.00390625
Iteration 3700: Loss = -12215.66796875
Iteration 3800: Loss = -12215.359375
Iteration 3900: Loss = -12215.07421875
Iteration 4000: Loss = -12214.80859375
Iteration 4100: Loss = -12214.5654296875
Iteration 4200: Loss = -12214.3388671875
Iteration 4300: Loss = -12214.1279296875
Iteration 4400: Loss = -12213.9326171875
Iteration 4500: Loss = -12213.75
Iteration 4600: Loss = -12213.5791015625
Iteration 4700: Loss = -12213.419921875
Iteration 4800: Loss = -12208.962890625
Iteration 4900: Loss = -12208.513671875
Iteration 5000: Loss = -12208.2255859375
Iteration 5100: Loss = -12207.9951171875
Iteration 5200: Loss = -12207.8017578125
Iteration 5300: Loss = -12207.634765625
Iteration 5400: Loss = -12207.484375
Iteration 5500: Loss = -12207.3515625
Iteration 5600: Loss = -12207.2314453125
Iteration 5700: Loss = -12207.1201171875
Iteration 5800: Loss = -12207.0185546875
Iteration 5900: Loss = -12206.9248046875
Iteration 6000: Loss = -12206.8369140625
Iteration 6100: Loss = -12206.755859375
Iteration 6200: Loss = -12206.6806640625
Iteration 6300: Loss = -12206.6123046875
Iteration 6400: Loss = -12206.5439453125
Iteration 6500: Loss = -12206.4814453125
Iteration 6600: Loss = -12206.42578125
Iteration 6700: Loss = -12206.3720703125
Iteration 6800: Loss = -12206.3203125
Iteration 6900: Loss = -12206.2705078125
Iteration 7000: Loss = -12206.2255859375
Iteration 7100: Loss = -12206.1845703125
Iteration 7200: Loss = -12206.1435546875
Iteration 7300: Loss = -12206.1044921875
Iteration 7400: Loss = -12206.068359375
Iteration 7500: Loss = -12206.03515625
Iteration 7600: Loss = -12206.0029296875
Iteration 7700: Loss = -12205.9716796875
Iteration 7800: Loss = -12205.9443359375
Iteration 7900: Loss = -12205.9189453125
Iteration 8000: Loss = -12205.8916015625
Iteration 8100: Loss = -12205.8681640625
Iteration 8200: Loss = -12205.84375
Iteration 8300: Loss = -12205.822265625
Iteration 8400: Loss = -12205.8037109375
Iteration 8500: Loss = -12205.7822265625
Iteration 8600: Loss = -12205.765625
Iteration 8700: Loss = -12205.74609375
Iteration 8800: Loss = -12205.7314453125
Iteration 8900: Loss = -12205.7158203125
Iteration 9000: Loss = -12205.69921875
Iteration 9100: Loss = -12205.6845703125
Iteration 9200: Loss = -12205.673828125
Iteration 9300: Loss = -12205.6611328125
Iteration 9400: Loss = -12205.646484375
Iteration 9500: Loss = -12205.63671875
Iteration 9600: Loss = -12205.6259765625
Iteration 9700: Loss = -12205.615234375
Iteration 9800: Loss = -12205.607421875
Iteration 9900: Loss = -12205.59765625
Iteration 10000: Loss = -12205.5888671875
Iteration 10100: Loss = -12205.5810546875
Iteration 10200: Loss = -12205.572265625
Iteration 10300: Loss = -12205.5654296875
Iteration 10400: Loss = -12205.55859375
Iteration 10500: Loss = -12205.5517578125
Iteration 10600: Loss = -12205.544921875
Iteration 10700: Loss = -12205.5390625
Iteration 10800: Loss = -12205.5341796875
Iteration 10900: Loss = -12205.52734375
Iteration 11000: Loss = -12205.5224609375
Iteration 11100: Loss = -12205.5185546875
Iteration 11200: Loss = -12205.513671875
Iteration 11300: Loss = -12205.5078125
Iteration 11400: Loss = -12205.5048828125
Iteration 11500: Loss = -12205.501953125
Iteration 11600: Loss = -12205.49609375
Iteration 11700: Loss = -12205.494140625
Iteration 11800: Loss = -12205.490234375
Iteration 11900: Loss = -12205.48828125
Iteration 12000: Loss = -12205.484375
Iteration 12100: Loss = -12205.4814453125
Iteration 12200: Loss = -12205.478515625
Iteration 12300: Loss = -12205.4755859375
Iteration 12400: Loss = -12205.474609375
Iteration 12500: Loss = -12205.4716796875
Iteration 12600: Loss = -12205.46875
Iteration 12700: Loss = -12205.4677734375
Iteration 12800: Loss = -12205.46484375
Iteration 12900: Loss = -12205.4638671875
Iteration 13000: Loss = -12205.4638671875
Iteration 13100: Loss = -12205.4619140625
Iteration 13200: Loss = -12205.4599609375
Iteration 13300: Loss = -12205.4580078125
Iteration 13400: Loss = -12205.45703125
Iteration 13500: Loss = -12205.4560546875
Iteration 13600: Loss = -12205.4541015625
Iteration 13700: Loss = -12205.451171875
Iteration 13800: Loss = -12205.451171875
Iteration 13900: Loss = -12205.4501953125
Iteration 14000: Loss = -12205.4501953125
Iteration 14100: Loss = -12205.44921875
Iteration 14200: Loss = -12205.44921875
Iteration 14300: Loss = -12205.447265625
Iteration 14400: Loss = -12205.4462890625
Iteration 14500: Loss = -12205.4462890625
Iteration 14600: Loss = -12205.4462890625
Iteration 14700: Loss = -12205.4453125
Iteration 14800: Loss = -12205.4453125
Iteration 14900: Loss = -12205.4423828125
Iteration 15000: Loss = -12205.443359375
1
Iteration 15100: Loss = -12205.4423828125
Iteration 15200: Loss = -12205.443359375
1
Iteration 15300: Loss = -12205.44140625
Iteration 15400: Loss = -12205.4423828125
1
Iteration 15500: Loss = -12205.4404296875
Iteration 15600: Loss = -12205.44140625
1
Iteration 15700: Loss = -12205.4404296875
Iteration 15800: Loss = -12205.4384765625
Iteration 15900: Loss = -12205.4384765625
Iteration 16000: Loss = -12205.4384765625
Iteration 16100: Loss = -12205.4384765625
Iteration 16200: Loss = -12205.4365234375
Iteration 16300: Loss = -12205.4375
1
Iteration 16400: Loss = -12205.435546875
Iteration 16500: Loss = -12205.4375
1
Iteration 16600: Loss = -12205.4375
2
Iteration 16700: Loss = -12205.4375
3
Iteration 16800: Loss = -12205.4375
4
Iteration 16900: Loss = -12205.4365234375
5
Iteration 17000: Loss = -12205.435546875
Iteration 17100: Loss = -12205.435546875
Iteration 17200: Loss = -12205.4365234375
1
Iteration 17300: Loss = -12205.43359375
Iteration 17400: Loss = -12205.4345703125
1
Iteration 17500: Loss = -12205.4345703125
2
Iteration 17600: Loss = -12205.4345703125
3
Iteration 17700: Loss = -12205.43359375
Iteration 17800: Loss = -12205.4345703125
1
Iteration 17900: Loss = -12205.43359375
Iteration 18000: Loss = -12205.4326171875
Iteration 18100: Loss = -12205.4345703125
1
Iteration 18200: Loss = -12205.4326171875
Iteration 18300: Loss = -12205.43359375
1
Iteration 18400: Loss = -12205.4326171875
Iteration 18500: Loss = -12205.4326171875
Iteration 18600: Loss = -12205.43359375
1
Iteration 18700: Loss = -12205.43359375
2
Iteration 18800: Loss = -12205.4326171875
Iteration 18900: Loss = -12205.4306640625
Iteration 19000: Loss = -12205.431640625
1
Iteration 19100: Loss = -12205.4326171875
2
Iteration 19200: Loss = -12205.43359375
3
Iteration 19300: Loss = -12205.431640625
4
Iteration 19400: Loss = -12205.4326171875
5
Iteration 19500: Loss = -12205.43359375
6
Iteration 19600: Loss = -12205.431640625
7
Iteration 19700: Loss = -12205.431640625
8
Iteration 19800: Loss = -12205.4306640625
Iteration 19900: Loss = -12205.43359375
1
Iteration 20000: Loss = -12205.4326171875
2
Iteration 20100: Loss = -12205.43359375
3
Iteration 20200: Loss = -12205.431640625
4
Iteration 20300: Loss = -12205.4326171875
5
Iteration 20400: Loss = -12205.4326171875
6
Iteration 20500: Loss = -12205.4326171875
7
Iteration 20600: Loss = -12205.431640625
8
Iteration 20700: Loss = -12205.431640625
9
Iteration 20800: Loss = -12205.431640625
10
Iteration 20900: Loss = -12205.4306640625
Iteration 21000: Loss = -12205.4326171875
1
Iteration 21100: Loss = -12205.431640625
2
Iteration 21200: Loss = -12205.4326171875
3
Iteration 21300: Loss = -12205.431640625
4
Iteration 21400: Loss = -12205.431640625
5
Iteration 21500: Loss = -12205.4326171875
6
Iteration 21600: Loss = -12205.4326171875
7
Iteration 21700: Loss = -12205.4306640625
Iteration 21800: Loss = -12205.4326171875
1
Iteration 21900: Loss = -12205.431640625
2
Iteration 22000: Loss = -12205.4326171875
3
Iteration 22100: Loss = -12205.4306640625
Iteration 22200: Loss = -12205.4306640625
Iteration 22300: Loss = -12205.4306640625
Iteration 22400: Loss = -12205.4306640625
Iteration 22500: Loss = -12205.4326171875
1
Iteration 22600: Loss = -12205.431640625
2
Iteration 22700: Loss = -12205.4326171875
3
Iteration 22800: Loss = -12205.4326171875
4
Iteration 22900: Loss = -12205.4326171875
5
Iteration 23000: Loss = -12205.4306640625
Iteration 23100: Loss = -12205.4326171875
1
Iteration 23200: Loss = -12205.4326171875
2
Iteration 23300: Loss = -12205.431640625
3
Iteration 23400: Loss = -12205.431640625
4
Iteration 23500: Loss = -12205.431640625
5
Iteration 23600: Loss = -12205.431640625
6
Iteration 23700: Loss = -12205.4306640625
Iteration 23800: Loss = -12205.4326171875
1
Iteration 23900: Loss = -12205.4326171875
2
Iteration 24000: Loss = -12205.4326171875
3
Iteration 24100: Loss = -12205.431640625
4
Iteration 24200: Loss = -12205.4326171875
5
Iteration 24300: Loss = -12205.4326171875
6
Iteration 24400: Loss = -12205.4326171875
7
Iteration 24500: Loss = -12205.4326171875
8
Iteration 24600: Loss = -12205.4326171875
9
Iteration 24700: Loss = -12205.431640625
10
Iteration 24800: Loss = -12205.43359375
11
Iteration 24900: Loss = -12205.4326171875
12
Iteration 25000: Loss = -12205.4326171875
13
Iteration 25100: Loss = -12205.4326171875
14
Iteration 25200: Loss = -12205.4326171875
15
Stopping early at iteration 25200 due to no improvement.
pi: tensor([[1.0000e+00, 1.7567e-06],
        [9.9020e-01, 9.7979e-03]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9998e-01, 1.7552e-05], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1948, 0.2128],
         [0.9925, 0.5299]],

        [[0.3987, 0.1970],
         [0.1800, 0.9785]],

        [[0.9872, 0.2004],
         [0.9045, 0.9489]],

        [[0.9389, 0.3818],
         [0.9515, 0.0399]],

        [[0.1190, 0.2244],
         [0.8171, 0.1594]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37574.28515625
Iteration 100: Loss = -23668.759765625
Iteration 200: Loss = -14806.6904296875
Iteration 300: Loss = -12929.875
Iteration 400: Loss = -12639.4541015625
Iteration 500: Loss = -12505.1337890625
Iteration 600: Loss = -12435.8046875
Iteration 700: Loss = -12385.482421875
Iteration 800: Loss = -12346.330078125
Iteration 900: Loss = -12323.1171875
Iteration 1000: Loss = -12305.3017578125
Iteration 1100: Loss = -12290.1962890625
Iteration 1200: Loss = -12279.755859375
Iteration 1300: Loss = -12272.1259765625
Iteration 1400: Loss = -12266.072265625
Iteration 1500: Loss = -12261.4345703125
Iteration 1600: Loss = -12257.61328125
Iteration 1700: Loss = -12254.681640625
Iteration 1800: Loss = -12252.0400390625
Iteration 1900: Loss = -12245.9345703125
Iteration 2000: Loss = -12243.9453125
Iteration 2100: Loss = -12242.3056640625
Iteration 2200: Loss = -12240.857421875
Iteration 2300: Loss = -12239.55078125
Iteration 2400: Loss = -12238.3408203125
Iteration 2500: Loss = -12237.1953125
Iteration 2600: Loss = -12236.1240234375
Iteration 2700: Loss = -12235.154296875
Iteration 2800: Loss = -12230.6201171875
Iteration 2900: Loss = -12228.556640625
Iteration 3000: Loss = -12227.8291015625
Iteration 3100: Loss = -12227.23046875
Iteration 3200: Loss = -12226.7060546875
Iteration 3300: Loss = -12226.234375
Iteration 3400: Loss = -12225.80078125
Iteration 3500: Loss = -12225.3095703125
Iteration 3600: Loss = -12220.4267578125
Iteration 3700: Loss = -12219.9599609375
Iteration 3800: Loss = -12219.60546875
Iteration 3900: Loss = -12215.4912109375
Iteration 4000: Loss = -12213.9951171875
Iteration 4100: Loss = -12213.5546875
Iteration 4200: Loss = -12213.232421875
Iteration 4300: Loss = -12212.9658203125
Iteration 4400: Loss = -12212.732421875
Iteration 4500: Loss = -12212.330078125
Iteration 4600: Loss = -12208.8798828125
Iteration 4700: Loss = -12208.5849609375
Iteration 4800: Loss = -12208.3486328125
Iteration 4900: Loss = -12208.1494140625
Iteration 5000: Loss = -12207.9697265625
Iteration 5100: Loss = -12207.80859375
Iteration 5200: Loss = -12207.662109375
Iteration 5300: Loss = -12207.52734375
Iteration 5400: Loss = -12207.404296875
Iteration 5500: Loss = -12207.291015625
Iteration 5600: Loss = -12207.18359375
Iteration 5700: Loss = -12207.0830078125
Iteration 5800: Loss = -12206.9921875
Iteration 5900: Loss = -12206.9052734375
Iteration 6000: Loss = -12206.82421875
Iteration 6100: Loss = -12206.75
Iteration 6200: Loss = -12206.677734375
Iteration 6300: Loss = -12206.611328125
Iteration 6400: Loss = -12206.5478515625
Iteration 6500: Loss = -12206.48828125
Iteration 6600: Loss = -12206.43359375
Iteration 6700: Loss = -12206.380859375
Iteration 6800: Loss = -12206.3310546875
Iteration 6900: Loss = -12206.283203125
Iteration 7000: Loss = -12206.23828125
Iteration 7100: Loss = -12206.1982421875
Iteration 7200: Loss = -12206.158203125
Iteration 7300: Loss = -12206.12109375
Iteration 7400: Loss = -12206.0849609375
Iteration 7500: Loss = -12206.0517578125
Iteration 7600: Loss = -12206.01953125
Iteration 7700: Loss = -12205.9892578125
Iteration 7800: Loss = -12205.9619140625
Iteration 7900: Loss = -12205.9345703125
Iteration 8000: Loss = -12205.9091796875
Iteration 8100: Loss = -12205.8857421875
Iteration 8200: Loss = -12205.8623046875
Iteration 8300: Loss = -12205.841796875
Iteration 8400: Loss = -12205.8212890625
Iteration 8500: Loss = -12205.802734375
Iteration 8600: Loss = -12205.7841796875
Iteration 8700: Loss = -12205.765625
Iteration 8800: Loss = -12205.7470703125
Iteration 8900: Loss = -12205.7333984375
Iteration 9000: Loss = -12205.7177734375
Iteration 9100: Loss = -12205.703125
Iteration 9200: Loss = -12205.689453125
Iteration 9300: Loss = -12205.67578125
Iteration 9400: Loss = -12205.6650390625
Iteration 9500: Loss = -12205.65234375
Iteration 9600: Loss = -12205.640625
Iteration 9700: Loss = -12205.6298828125
Iteration 9800: Loss = -12205.62109375
Iteration 9900: Loss = -12205.6103515625
Iteration 10000: Loss = -12205.6025390625
Iteration 10100: Loss = -12205.5947265625
Iteration 10200: Loss = -12205.5869140625
Iteration 10300: Loss = -12205.578125
Iteration 10400: Loss = -12205.5693359375
Iteration 10500: Loss = -12205.5634765625
Iteration 10600: Loss = -12205.556640625
Iteration 10700: Loss = -12205.5517578125
Iteration 10800: Loss = -12205.544921875
Iteration 10900: Loss = -12205.5390625
Iteration 11000: Loss = -12205.533203125
Iteration 11100: Loss = -12205.52734375
Iteration 11200: Loss = -12205.5234375
Iteration 11300: Loss = -12205.51953125
Iteration 11400: Loss = -12205.5146484375
Iteration 11500: Loss = -12205.5087890625
Iteration 11600: Loss = -12205.505859375
Iteration 11700: Loss = -12205.5009765625
Iteration 11800: Loss = -12205.4990234375
Iteration 11900: Loss = -12205.49609375
Iteration 12000: Loss = -12205.4931640625
Iteration 12100: Loss = -12205.4892578125
Iteration 12200: Loss = -12205.4853515625
Iteration 12300: Loss = -12205.4833984375
Iteration 12400: Loss = -12205.48046875
Iteration 12500: Loss = -12205.478515625
Iteration 12600: Loss = -12205.4755859375
Iteration 12700: Loss = -12205.4736328125
Iteration 12800: Loss = -12205.4716796875
Iteration 12900: Loss = -12205.4697265625
Iteration 13000: Loss = -12205.46875
Iteration 13100: Loss = -12205.46484375
Iteration 13200: Loss = -12205.46484375
Iteration 13300: Loss = -12205.4619140625
Iteration 13400: Loss = -12205.4619140625
Iteration 13500: Loss = -12205.458984375
Iteration 13600: Loss = -12205.4560546875
Iteration 13700: Loss = -12205.455078125
Iteration 13800: Loss = -12205.4541015625
Iteration 13900: Loss = -12205.4560546875
1
Iteration 14000: Loss = -12205.453125
Iteration 14100: Loss = -12205.4521484375
Iteration 14200: Loss = -12205.451171875
Iteration 14300: Loss = -12205.451171875
Iteration 14400: Loss = -12205.44921875
Iteration 14500: Loss = -12205.4462890625
Iteration 14600: Loss = -12205.447265625
1
Iteration 14700: Loss = -12205.4453125
Iteration 14800: Loss = -12205.4462890625
1
Iteration 14900: Loss = -12205.447265625
2
Iteration 15000: Loss = -12205.443359375
Iteration 15100: Loss = -12205.4423828125
Iteration 15200: Loss = -12205.4423828125
Iteration 15300: Loss = -12205.44140625
Iteration 15400: Loss = -12205.443359375
1
Iteration 15500: Loss = -12205.4404296875
Iteration 15600: Loss = -12205.4404296875
Iteration 15700: Loss = -12205.44140625
1
Iteration 15800: Loss = -12205.439453125
Iteration 15900: Loss = -12205.4404296875
1
Iteration 16000: Loss = -12205.4375
Iteration 16100: Loss = -12205.4384765625
1
Iteration 16200: Loss = -12205.4384765625
2
Iteration 16300: Loss = -12205.4375
Iteration 16400: Loss = -12205.4365234375
Iteration 16500: Loss = -12205.4375
1
Iteration 16600: Loss = -12205.435546875
Iteration 16700: Loss = -12205.4375
1
Iteration 16800: Loss = -12205.435546875
Iteration 16900: Loss = -12205.4365234375
1
Iteration 17000: Loss = -12205.4365234375
2
Iteration 17100: Loss = -12205.4345703125
Iteration 17200: Loss = -12205.4345703125
Iteration 17300: Loss = -12205.4365234375
1
Iteration 17400: Loss = -12205.4345703125
Iteration 17500: Loss = -12205.43359375
Iteration 17600: Loss = -12205.43359375
Iteration 17700: Loss = -12205.43359375
Iteration 17800: Loss = -12205.43359375
Iteration 17900: Loss = -12205.431640625
Iteration 18000: Loss = -12205.43359375
1
Iteration 18100: Loss = -12205.4326171875
2
Iteration 18200: Loss = -12205.4326171875
3
Iteration 18300: Loss = -12205.431640625
Iteration 18400: Loss = -12205.4326171875
1
Iteration 18500: Loss = -12205.431640625
Iteration 18600: Loss = -12205.4326171875
1
Iteration 18700: Loss = -12205.431640625
Iteration 18800: Loss = -12205.431640625
Iteration 18900: Loss = -12205.4326171875
1
Iteration 19000: Loss = -12205.4326171875
2
Iteration 19100: Loss = -12205.43359375
3
Iteration 19200: Loss = -12205.4326171875
4
Iteration 19300: Loss = -12205.4326171875
5
Iteration 19400: Loss = -12205.4306640625
Iteration 19500: Loss = -12205.4345703125
1
Iteration 19600: Loss = -12205.4326171875
2
Iteration 19700: Loss = -12205.4326171875
3
Iteration 19800: Loss = -12205.431640625
4
Iteration 19900: Loss = -12205.4326171875
5
Iteration 20000: Loss = -12205.431640625
6
Iteration 20100: Loss = -12205.4326171875
7
Iteration 20200: Loss = -12205.431640625
8
Iteration 20300: Loss = -12205.4326171875
9
Iteration 20400: Loss = -12205.431640625
10
Iteration 20500: Loss = -12205.4326171875
11
Iteration 20600: Loss = -12205.4326171875
12
Iteration 20700: Loss = -12205.4326171875
13
Iteration 20800: Loss = -12205.4326171875
14
Iteration 20900: Loss = -12205.4326171875
15
Stopping early at iteration 20900 due to no improvement.
pi: tensor([[2.0754e-02, 9.7925e-01],
        [1.7465e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([6.8004e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4433, 0.2926],
         [0.9008, 0.1948]],

        [[0.5645, 0.2031],
         [0.0640, 0.9913]],

        [[0.3994, 0.2427],
         [0.7670, 0.0088]],

        [[0.0836, 0.1832],
         [0.4064, 0.0528]],

        [[0.8112, 0.2634],
         [0.9303, 0.0645]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.0, 0.0] [0.0, 0.0] [12205.4326171875, 12205.4326171875]
-------------------------------------
This iteration is 53
True Objective function: Loss = -12046.580035829336
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -40192.546875
Iteration 100: Loss = -23523.11328125
Iteration 200: Loss = -15015.298828125
Iteration 300: Loss = -13317.38671875
Iteration 400: Loss = -13004.873046875
Iteration 500: Loss = -12873.701171875
Iteration 600: Loss = -12774.810546875
Iteration 700: Loss = -12718.8515625
Iteration 800: Loss = -12682.2919921875
Iteration 900: Loss = -12653.302734375
Iteration 1000: Loss = -12639.48046875
Iteration 1100: Loss = -12621.53515625
Iteration 1200: Loss = -12606.6025390625
Iteration 1300: Loss = -12600.9794921875
Iteration 1400: Loss = -12597.1591796875
Iteration 1500: Loss = -12594.1142578125
Iteration 1600: Loss = -12591.59375
Iteration 1700: Loss = -12589.466796875
Iteration 1800: Loss = -12587.6328125
Iteration 1900: Loss = -12586.0048828125
Iteration 2000: Loss = -12584.271484375
Iteration 2100: Loss = -12579.439453125
Iteration 2200: Loss = -12577.9560546875
Iteration 2300: Loss = -12576.9228515625
Iteration 2400: Loss = -12576.04296875
Iteration 2500: Loss = -12575.267578125
Iteration 2600: Loss = -12574.5732421875
Iteration 2700: Loss = -12573.9501953125
Iteration 2800: Loss = -12573.3857421875
Iteration 2900: Loss = -12572.87109375
Iteration 3000: Loss = -12572.4013671875
Iteration 3100: Loss = -12571.970703125
Iteration 3200: Loss = -12571.5771484375
Iteration 3300: Loss = -12571.2138671875
Iteration 3400: Loss = -12570.87890625
Iteration 3500: Loss = -12570.5673828125
Iteration 3600: Loss = -12570.283203125
Iteration 3700: Loss = -12570.0146484375
Iteration 3800: Loss = -12569.765625
Iteration 3900: Loss = -12569.5341796875
Iteration 4000: Loss = -12569.3193359375
Iteration 4100: Loss = -12569.119140625
Iteration 4200: Loss = -12568.9306640625
Iteration 4300: Loss = -12568.755859375
Iteration 4400: Loss = -12568.58984375
Iteration 4500: Loss = -12568.4375
Iteration 4600: Loss = -12568.2919921875
Iteration 4700: Loss = -12568.15625
Iteration 4800: Loss = -12568.0283203125
Iteration 4900: Loss = -12567.908203125
Iteration 5000: Loss = -12567.7958984375
Iteration 5100: Loss = -12567.689453125
Iteration 5200: Loss = -12567.587890625
Iteration 5300: Loss = -12567.4931640625
Iteration 5400: Loss = -12567.4052734375
Iteration 5500: Loss = -12567.3203125
Iteration 5600: Loss = -12567.2392578125
Iteration 5700: Loss = -12567.1640625
Iteration 5800: Loss = -12567.0927734375
Iteration 5900: Loss = -12567.0263671875
Iteration 6000: Loss = -12566.9619140625
Iteration 6100: Loss = -12566.900390625
Iteration 6200: Loss = -12566.8447265625
Iteration 6300: Loss = -12566.7900390625
Iteration 6400: Loss = -12566.740234375
Iteration 6500: Loss = -12566.6904296875
Iteration 6600: Loss = -12566.6435546875
Iteration 6700: Loss = -12566.5986328125
Iteration 6800: Loss = -12566.5595703125
Iteration 6900: Loss = -12566.5185546875
Iteration 7000: Loss = -12566.48046875
Iteration 7100: Loss = -12566.4443359375
Iteration 7200: Loss = -12566.412109375
Iteration 7300: Loss = -12566.37890625
Iteration 7400: Loss = -12566.3486328125
Iteration 7500: Loss = -12566.3193359375
Iteration 7600: Loss = -12566.2919921875
Iteration 7700: Loss = -12566.2646484375
Iteration 7800: Loss = -12566.23828125
Iteration 7900: Loss = -12566.21484375
Iteration 8000: Loss = -12566.19140625
Iteration 8100: Loss = -12566.169921875
Iteration 8200: Loss = -12566.1474609375
Iteration 8300: Loss = -12566.1279296875
Iteration 8400: Loss = -12566.109375
Iteration 8500: Loss = -12566.08984375
Iteration 8600: Loss = -12566.0703125
Iteration 8700: Loss = -12566.0537109375
Iteration 8800: Loss = -12566.03515625
Iteration 8900: Loss = -12566.0166015625
Iteration 9000: Loss = -12565.9990234375
Iteration 9100: Loss = -12565.9794921875
Iteration 9200: Loss = -12565.95703125
Iteration 9300: Loss = -12565.927734375
Iteration 9400: Loss = -12565.8740234375
Iteration 9500: Loss = -12565.6796875
Iteration 9600: Loss = -12563.458984375
Iteration 9700: Loss = -12563.1376953125
Iteration 9800: Loss = -12563.0126953125
Iteration 9900: Loss = -12562.943359375
Iteration 10000: Loss = -12562.8955078125
Iteration 10100: Loss = -12562.8583984375
Iteration 10200: Loss = -12562.8310546875
Iteration 10300: Loss = -12562.8076171875
Iteration 10400: Loss = -12562.7890625
Iteration 10500: Loss = -12562.771484375
Iteration 10600: Loss = -12562.755859375
Iteration 10700: Loss = -12562.7421875
Iteration 10800: Loss = -12562.73046875
Iteration 10900: Loss = -12562.716796875
Iteration 11000: Loss = -12562.7080078125
Iteration 11100: Loss = -12562.6982421875
Iteration 11200: Loss = -12562.6884765625
Iteration 11300: Loss = -12562.6796875
Iteration 11400: Loss = -12562.669921875
Iteration 11500: Loss = -12562.662109375
Iteration 11600: Loss = -12562.6533203125
Iteration 11700: Loss = -12562.646484375
Iteration 11800: Loss = -12562.6357421875
Iteration 11900: Loss = -12562.6279296875
Iteration 12000: Loss = -12562.6181640625
Iteration 12100: Loss = -12562.609375
Iteration 12200: Loss = -12562.5986328125
Iteration 12300: Loss = -12562.5869140625
Iteration 12400: Loss = -12562.572265625
Iteration 12500: Loss = -12562.5546875
Iteration 12600: Loss = -12562.5341796875
Iteration 12700: Loss = -12562.5087890625
Iteration 12800: Loss = -12562.4736328125
Iteration 12900: Loss = -12562.4228515625
Iteration 13000: Loss = -12562.3505859375
Iteration 13100: Loss = -12562.23046875
Iteration 13200: Loss = -12562.025390625
Iteration 13300: Loss = -12561.6962890625
Iteration 13400: Loss = -12561.50390625
Iteration 13500: Loss = -12561.4228515625
Iteration 13600: Loss = -12561.3984375
Iteration 13700: Loss = -12561.3779296875
Iteration 13800: Loss = -12561.3681640625
Iteration 13900: Loss = -12561.3486328125
Iteration 14000: Loss = -12561.3330078125
Iteration 14100: Loss = -12561.3271484375
Iteration 14200: Loss = -12561.3203125
Iteration 14300: Loss = -12561.314453125
Iteration 14400: Loss = -12561.3076171875
Iteration 14500: Loss = -12561.29296875
Iteration 14600: Loss = -12561.26953125
Iteration 14700: Loss = -12543.900390625
Iteration 14800: Loss = -12504.3798828125
Iteration 14900: Loss = -12445.80078125
Iteration 15000: Loss = -12364.951171875
Iteration 15100: Loss = -12324.9501953125
Iteration 15200: Loss = -12291.5283203125
Iteration 15300: Loss = -12277.4638671875
Iteration 15400: Loss = -12253.171875
Iteration 15500: Loss = -12236.5634765625
Iteration 15600: Loss = -12197.470703125
Iteration 15700: Loss = -12166.3134765625
Iteration 15800: Loss = -12144.123046875
Iteration 15900: Loss = -12143.634765625
Iteration 16000: Loss = -12143.4462890625
Iteration 16100: Loss = -12143.333984375
Iteration 16200: Loss = -12143.2568359375
Iteration 16300: Loss = -12143.2001953125
Iteration 16400: Loss = -12142.9794921875
Iteration 16500: Loss = -12131.337890625
Iteration 16600: Loss = -12131.111328125
Iteration 16700: Loss = -12131.0458984375
Iteration 16800: Loss = -12131.0
Iteration 16900: Loss = -12126.443359375
Iteration 17000: Loss = -12126.1279296875
Iteration 17100: Loss = -12120.56640625
Iteration 17200: Loss = -12120.36328125
Iteration 17300: Loss = -12096.0693359375
Iteration 17400: Loss = -12095.62109375
Iteration 17500: Loss = -12095.517578125
Iteration 17600: Loss = -12095.43359375
Iteration 17700: Loss = -12083.8388671875
Iteration 17800: Loss = -12080.806640625
Iteration 17900: Loss = -12080.22265625
Iteration 18000: Loss = -12059.275390625
Iteration 18100: Loss = -12050.9462890625
Iteration 18200: Loss = -12050.6806640625
Iteration 18300: Loss = -12050.6064453125
Iteration 18400: Loss = -12050.5615234375
Iteration 18500: Loss = -12050.5322265625
Iteration 18600: Loss = -12050.5107421875
Iteration 18700: Loss = -12050.4921875
Iteration 18800: Loss = -12050.4775390625
Iteration 18900: Loss = -12050.4638671875
Iteration 19000: Loss = -12050.447265625
Iteration 19100: Loss = -12050.4384765625
Iteration 19200: Loss = -12050.4306640625
Iteration 19300: Loss = -12050.423828125
Iteration 19400: Loss = -12050.41796875
Iteration 19500: Loss = -12050.4130859375
Iteration 19600: Loss = -12050.4091796875
Iteration 19700: Loss = -12050.4033203125
Iteration 19800: Loss = -12050.40234375
Iteration 19900: Loss = -12050.3955078125
Iteration 20000: Loss = -12050.392578125
Iteration 20100: Loss = -12050.3896484375
Iteration 20200: Loss = -12050.3857421875
Iteration 20300: Loss = -12050.384765625
Iteration 20400: Loss = -12050.3818359375
Iteration 20500: Loss = -12050.3798828125
Iteration 20600: Loss = -12050.376953125
Iteration 20700: Loss = -12050.3759765625
Iteration 20800: Loss = -12050.373046875
Iteration 20900: Loss = -12050.373046875
Iteration 21000: Loss = -12050.37109375
Iteration 21100: Loss = -12050.369140625
Iteration 21200: Loss = -12050.3681640625
Iteration 21300: Loss = -12050.3681640625
Iteration 21400: Loss = -12050.3671875
Iteration 21500: Loss = -12050.365234375
Iteration 21600: Loss = -12050.36328125
Iteration 21700: Loss = -12050.36328125
Iteration 21800: Loss = -12050.3623046875
Iteration 21900: Loss = -12050.3623046875
Iteration 22000: Loss = -12050.361328125
Iteration 22100: Loss = -12050.3603515625
Iteration 22200: Loss = -12050.359375
Iteration 22300: Loss = -12050.3603515625
1
Iteration 22400: Loss = -12050.357421875
Iteration 22500: Loss = -12050.3583984375
1
Iteration 22600: Loss = -12050.357421875
Iteration 22700: Loss = -12050.357421875
Iteration 22800: Loss = -12050.3564453125
Iteration 22900: Loss = -12050.3564453125
Iteration 23000: Loss = -12050.35546875
Iteration 23100: Loss = -12050.3544921875
Iteration 23200: Loss = -12050.35546875
1
Iteration 23300: Loss = -12050.3544921875
Iteration 23400: Loss = -12050.353515625
Iteration 23500: Loss = -12050.3544921875
1
Iteration 23600: Loss = -12050.353515625
Iteration 23700: Loss = -12050.353515625
Iteration 23800: Loss = -12050.3544921875
1
Iteration 23900: Loss = -12050.353515625
Iteration 24000: Loss = -12050.3525390625
Iteration 24100: Loss = -12050.3525390625
Iteration 24200: Loss = -12050.3525390625
Iteration 24300: Loss = -12050.3515625
Iteration 24400: Loss = -12050.3515625
Iteration 24500: Loss = -12050.3515625
Iteration 24600: Loss = -12050.3525390625
1
Iteration 24700: Loss = -12050.3515625
Iteration 24800: Loss = -12050.349609375
Iteration 24900: Loss = -12050.3505859375
1
Iteration 25000: Loss = -12050.3505859375
2
Iteration 25100: Loss = -12050.3515625
3
Iteration 25200: Loss = -12050.3505859375
4
Iteration 25300: Loss = -12050.3515625
5
Iteration 25400: Loss = -12050.3505859375
6
Iteration 25500: Loss = -12050.3505859375
7
Iteration 25600: Loss = -12050.3515625
8
Iteration 25700: Loss = -12050.3505859375
9
Iteration 25800: Loss = -12050.3505859375
10
Iteration 25900: Loss = -12050.349609375
Iteration 26000: Loss = -12050.349609375
Iteration 26100: Loss = -12050.3505859375
1
Iteration 26200: Loss = -12050.3515625
2
Iteration 26300: Loss = -12050.3486328125
Iteration 26400: Loss = -12050.3505859375
1
Iteration 26500: Loss = -12050.349609375
2
Iteration 26600: Loss = -12050.349609375
3
Iteration 26700: Loss = -12050.349609375
4
Iteration 26800: Loss = -12050.349609375
5
Iteration 26900: Loss = -12050.349609375
6
Iteration 27000: Loss = -12050.3505859375
7
Iteration 27100: Loss = -12050.349609375
8
Iteration 27200: Loss = -12050.3505859375
9
Iteration 27300: Loss = -12050.3505859375
10
Iteration 27400: Loss = -12050.349609375
11
Iteration 27500: Loss = -12050.349609375
12
Iteration 27600: Loss = -12050.349609375
13
Iteration 27700: Loss = -12050.349609375
14
Iteration 27800: Loss = -12050.3486328125
Iteration 27900: Loss = -12050.3505859375
1
Iteration 28000: Loss = -12050.349609375
2
Iteration 28100: Loss = -12050.349609375
3
Iteration 28200: Loss = -12050.349609375
4
Iteration 28300: Loss = -12050.349609375
5
Iteration 28400: Loss = -12050.349609375
6
Iteration 28500: Loss = -12050.349609375
7
Iteration 28600: Loss = -12050.349609375
8
Iteration 28700: Loss = -12050.3486328125
Iteration 28800: Loss = -12050.3486328125
Iteration 28900: Loss = -12050.3486328125
Iteration 29000: Loss = -12050.3486328125
Iteration 29100: Loss = -12050.3486328125
Iteration 29200: Loss = -12050.3486328125
Iteration 29300: Loss = -12050.34765625
Iteration 29400: Loss = -12050.3486328125
1
Iteration 29500: Loss = -12050.3486328125
2
Iteration 29600: Loss = -12050.34765625
Iteration 29700: Loss = -12050.34765625
Iteration 29800: Loss = -12050.3486328125
1
Iteration 29900: Loss = -12050.3486328125
2
pi: tensor([[0.8167, 0.1833],
        [0.2614, 0.7386]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5727, 0.4273], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3060, 0.0931],
         [0.0267, 0.2979]],

        [[0.0221, 0.1168],
         [0.9725, 0.0212]],

        [[0.0412, 0.0963],
         [0.5491, 0.7801]],

        [[0.0108, 0.1068],
         [0.6305, 0.0131]],

        [[0.1211, 0.1072],
         [0.9046, 0.1341]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 2
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207702484198148
time is 4
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
Global Adjusted Rand Index: 0.9602918814578075
Average Adjusted Rand Index: 0.9601487512009026
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -43235.12890625
Iteration 100: Loss = -28691.1640625
Iteration 200: Loss = -17593.298828125
Iteration 300: Loss = -13861.7294921875
Iteration 400: Loss = -13135.7060546875
Iteration 500: Loss = -12903.9619140625
Iteration 600: Loss = -12822.01171875
Iteration 700: Loss = -12768.7353515625
Iteration 800: Loss = -12741.04296875
Iteration 900: Loss = -12720.3486328125
Iteration 1000: Loss = -12702.1748046875
Iteration 1100: Loss = -12685.8603515625
Iteration 1200: Loss = -12673.791015625
Iteration 1300: Loss = -12663.0966796875
Iteration 1400: Loss = -12651.109375
Iteration 1500: Loss = -12640.421875
Iteration 1600: Loss = -12632.49609375
Iteration 1700: Loss = -12623.7392578125
Iteration 1800: Loss = -12616.021484375
Iteration 1900: Loss = -12610.2685546875
Iteration 2000: Loss = -12604.0908203125
Iteration 2100: Loss = -12597.7333984375
Iteration 2200: Loss = -12591.6748046875
Iteration 2300: Loss = -12585.3076171875
Iteration 2400: Loss = -12577.3408203125
Iteration 2500: Loss = -12567.19921875
Iteration 2600: Loss = -12554.3642578125
Iteration 2700: Loss = -12537.5791015625
Iteration 2800: Loss = -12529.509765625
Iteration 2900: Loss = -12523.638671875
Iteration 3000: Loss = -12518.134765625
Iteration 3100: Loss = -12513.58984375
Iteration 3200: Loss = -12510.4765625
Iteration 3300: Loss = -12508.4521484375
Iteration 3400: Loss = -12506.7646484375
Iteration 3500: Loss = -12499.4970703125
Iteration 3600: Loss = -12497.4482421875
Iteration 3700: Loss = -12494.5087890625
Iteration 3800: Loss = -12492.267578125
Iteration 3900: Loss = -12490.669921875
Iteration 4000: Loss = -12489.4677734375
Iteration 4100: Loss = -12488.6796875
Iteration 4200: Loss = -12487.248046875
Iteration 4300: Loss = -12478.662109375
Iteration 4400: Loss = -12478.1103515625
Iteration 4500: Loss = -12477.7978515625
Iteration 4600: Loss = -12477.361328125
Iteration 4700: Loss = -12473.4267578125
Iteration 4800: Loss = -12473.1162109375
Iteration 4900: Loss = -12472.9208984375
Iteration 5000: Loss = -12472.765625
Iteration 5100: Loss = -12472.6357421875
Iteration 5200: Loss = -12472.5205078125
Iteration 5300: Loss = -12472.419921875
Iteration 5400: Loss = -12472.3271484375
Iteration 5500: Loss = -12472.2421875
Iteration 5600: Loss = -12472.166015625
Iteration 5700: Loss = -12472.0947265625
Iteration 5800: Loss = -12472.02734375
Iteration 5900: Loss = -12471.943359375
Iteration 6000: Loss = -12468.2041015625
Iteration 6100: Loss = -12467.5703125
Iteration 6200: Loss = -12467.3994140625
Iteration 6300: Loss = -12467.2919921875
Iteration 6400: Loss = -12467.2109375
Iteration 6500: Loss = -12467.14453125
Iteration 6600: Loss = -12467.0869140625
Iteration 6700: Loss = -12467.0380859375
Iteration 6800: Loss = -12466.994140625
Iteration 6900: Loss = -12466.951171875
Iteration 7000: Loss = -12466.916015625
Iteration 7100: Loss = -12466.8837890625
Iteration 7200: Loss = -12466.8515625
Iteration 7300: Loss = -12466.8212890625
Iteration 7400: Loss = -12466.794921875
Iteration 7500: Loss = -12466.7705078125
Iteration 7600: Loss = -12466.7451171875
Iteration 7700: Loss = -12466.72265625
Iteration 7800: Loss = -12466.69921875
Iteration 7900: Loss = -12466.6796875
Iteration 8000: Loss = -12466.6611328125
Iteration 8100: Loss = -12466.64453125
Iteration 8200: Loss = -12466.6259765625
Iteration 8300: Loss = -12466.611328125
Iteration 8400: Loss = -12466.59765625
Iteration 8500: Loss = -12466.5859375
Iteration 8600: Loss = -12466.572265625
Iteration 8700: Loss = -12466.5615234375
Iteration 8800: Loss = -12466.5498046875
Iteration 8900: Loss = -12466.541015625
Iteration 9000: Loss = -12466.529296875
Iteration 9100: Loss = -12466.51953125
Iteration 9200: Loss = -12466.5107421875
Iteration 9300: Loss = -12466.5029296875
Iteration 9400: Loss = -12466.494140625
Iteration 9500: Loss = -12466.4853515625
Iteration 9600: Loss = -12466.4794921875
Iteration 9700: Loss = -12466.4697265625
Iteration 9800: Loss = -12466.4609375
Iteration 9900: Loss = -12466.4541015625
Iteration 10000: Loss = -12466.4453125
Iteration 10100: Loss = -12466.4375
Iteration 10200: Loss = -12466.4296875
Iteration 10300: Loss = -12466.4248046875
Iteration 10400: Loss = -12466.4189453125
Iteration 10500: Loss = -12466.4130859375
Iteration 10600: Loss = -12465.7900390625
Iteration 10700: Loss = -12461.3623046875
Iteration 10800: Loss = -12461.2529296875
Iteration 10900: Loss = -12461.205078125
Iteration 11000: Loss = -12461.1748046875
Iteration 11100: Loss = -12461.154296875
Iteration 11200: Loss = -12461.140625
Iteration 11300: Loss = -12461.12890625
Iteration 11400: Loss = -12461.119140625
Iteration 11500: Loss = -12461.1103515625
Iteration 11600: Loss = -12461.103515625
Iteration 11700: Loss = -12461.09765625
Iteration 11800: Loss = -12461.0908203125
Iteration 11900: Loss = -12461.0859375
Iteration 12000: Loss = -12461.08203125
Iteration 12100: Loss = -12461.078125
Iteration 12200: Loss = -12461.0732421875
Iteration 12300: Loss = -12461.0703125
Iteration 12400: Loss = -12461.0673828125
Iteration 12500: Loss = -12461.0654296875
Iteration 12600: Loss = -12461.0625
Iteration 12700: Loss = -12461.0595703125
Iteration 12800: Loss = -12461.056640625
Iteration 12900: Loss = -12461.0546875
Iteration 13000: Loss = -12461.052734375
Iteration 13100: Loss = -12461.05078125
Iteration 13200: Loss = -12461.048828125
Iteration 13300: Loss = -12461.0478515625
Iteration 13400: Loss = -12461.044921875
Iteration 13500: Loss = -12461.0458984375
1
Iteration 13600: Loss = -12461.04296875
Iteration 13700: Loss = -12461.041015625
Iteration 13800: Loss = -12461.0400390625
Iteration 13900: Loss = -12461.041015625
1
Iteration 14000: Loss = -12461.0380859375
Iteration 14100: Loss = -12461.0361328125
Iteration 14200: Loss = -12461.0361328125
Iteration 14300: Loss = -12461.03515625
Iteration 14400: Loss = -12461.0341796875
Iteration 14500: Loss = -12461.0341796875
Iteration 14600: Loss = -12461.0322265625
Iteration 14700: Loss = -12461.0322265625
Iteration 14800: Loss = -12461.03125
Iteration 14900: Loss = -12461.0302734375
Iteration 15000: Loss = -12461.0302734375
Iteration 15100: Loss = -12461.029296875
Iteration 15200: Loss = -12461.029296875
Iteration 15300: Loss = -12461.0302734375
1
Iteration 15400: Loss = -12461.0283203125
Iteration 15500: Loss = -12461.029296875
1
Iteration 15600: Loss = -12461.02734375
Iteration 15700: Loss = -12461.0263671875
Iteration 15800: Loss = -12461.0322265625
1
Iteration 15900: Loss = -12461.0263671875
Iteration 16000: Loss = -12461.0244140625
Iteration 16100: Loss = -12461.025390625
1
Iteration 16200: Loss = -12461.025390625
2
Iteration 16300: Loss = -12461.025390625
3
Iteration 16400: Loss = -12461.0244140625
Iteration 16500: Loss = -12461.0234375
Iteration 16600: Loss = -12461.0244140625
1
Iteration 16700: Loss = -12461.0234375
Iteration 16800: Loss = -12461.0224609375
Iteration 16900: Loss = -12461.0224609375
Iteration 17000: Loss = -12461.021484375
Iteration 17100: Loss = -12461.0224609375
1
Iteration 17200: Loss = -12461.021484375
Iteration 17300: Loss = -12461.021484375
Iteration 17400: Loss = -12461.0185546875
Iteration 17500: Loss = -12461.0185546875
Iteration 17600: Loss = -12461.015625
Iteration 17700: Loss = -12461.013671875
Iteration 17800: Loss = -12461.0009765625
Iteration 17900: Loss = -12460.970703125
Iteration 18000: Loss = -12460.9560546875
Iteration 18100: Loss = -12460.951171875
Iteration 18200: Loss = -12460.9384765625
Iteration 18300: Loss = -12460.935546875
Iteration 18400: Loss = -12460.935546875
Iteration 18500: Loss = -12460.93359375
Iteration 18600: Loss = -12460.93359375
Iteration 18700: Loss = -12460.93359375
Iteration 18800: Loss = -12460.9326171875
Iteration 18900: Loss = -12460.93359375
1
Iteration 19000: Loss = -12460.93359375
2
Iteration 19100: Loss = -12460.931640625
Iteration 19200: Loss = -12460.9326171875
1
Iteration 19300: Loss = -12460.9326171875
2
Iteration 19400: Loss = -12460.9306640625
Iteration 19500: Loss = -12460.9306640625
Iteration 19600: Loss = -12460.9306640625
Iteration 19700: Loss = -12460.9296875
Iteration 19800: Loss = -12460.9296875
Iteration 19900: Loss = -12460.9287109375
Iteration 20000: Loss = -12460.9296875
1
Iteration 20100: Loss = -12460.9287109375
Iteration 20200: Loss = -12460.927734375
Iteration 20300: Loss = -12460.9296875
1
Iteration 20400: Loss = -12460.9296875
2
Iteration 20500: Loss = -12460.9287109375
3
Iteration 20600: Loss = -12460.9267578125
Iteration 20700: Loss = -12460.9267578125
Iteration 20800: Loss = -12460.927734375
1
Iteration 20900: Loss = -12460.9267578125
Iteration 21000: Loss = -12460.92578125
Iteration 21100: Loss = -12460.9267578125
1
Iteration 21200: Loss = -12460.92578125
Iteration 21300: Loss = -12460.92578125
Iteration 21400: Loss = -12460.92578125
Iteration 21500: Loss = -12460.927734375
1
Iteration 21600: Loss = -12460.92578125
Iteration 21700: Loss = -12460.9248046875
Iteration 21800: Loss = -12460.92578125
1
Iteration 21900: Loss = -12460.923828125
Iteration 22000: Loss = -12460.923828125
Iteration 22100: Loss = -12460.923828125
Iteration 22200: Loss = -12460.9248046875
1
Iteration 22300: Loss = -12460.923828125
Iteration 22400: Loss = -12460.9248046875
1
Iteration 22500: Loss = -12460.92578125
2
Iteration 22600: Loss = -12460.9248046875
3
Iteration 22700: Loss = -12460.923828125
Iteration 22800: Loss = -12460.9248046875
1
Iteration 22900: Loss = -12460.923828125
Iteration 23000: Loss = -12460.919921875
Iteration 23100: Loss = -12460.919921875
Iteration 23200: Loss = -12460.9189453125
Iteration 23300: Loss = -12460.919921875
1
Iteration 23400: Loss = -12460.919921875
2
Iteration 23500: Loss = -12460.919921875
3
Iteration 23600: Loss = -12460.919921875
4
Iteration 23700: Loss = -12460.919921875
5
Iteration 23800: Loss = -12460.919921875
6
Iteration 23900: Loss = -12460.919921875
7
Iteration 24000: Loss = -12460.9189453125
Iteration 24100: Loss = -12460.919921875
1
Iteration 24200: Loss = -12460.919921875
2
Iteration 24300: Loss = -12460.919921875
3
Iteration 24400: Loss = -12460.9189453125
Iteration 24500: Loss = -12460.91796875
Iteration 24600: Loss = -12460.916015625
Iteration 24700: Loss = -12460.9169921875
1
Iteration 24800: Loss = -12460.916015625
Iteration 24900: Loss = -12460.9169921875
1
Iteration 25000: Loss = -12460.916015625
Iteration 25100: Loss = -12460.9150390625
Iteration 25200: Loss = -12460.9130859375
Iteration 25300: Loss = -12460.9140625
1
Iteration 25400: Loss = -12460.912109375
Iteration 25500: Loss = -12460.916015625
1
Iteration 25600: Loss = -12460.9111328125
Iteration 25700: Loss = -12460.9140625
1
Iteration 25800: Loss = -12460.9091796875
Iteration 25900: Loss = -12460.9091796875
Iteration 26000: Loss = -12460.908203125
Iteration 26100: Loss = -12460.908203125
Iteration 26200: Loss = -12460.908203125
Iteration 26300: Loss = -12460.9072265625
Iteration 26400: Loss = -12460.9052734375
Iteration 26500: Loss = -12460.9033203125
Iteration 26600: Loss = -12460.9013671875
Iteration 26700: Loss = -12460.90234375
1
Iteration 26800: Loss = -12460.9033203125
2
Iteration 26900: Loss = -12460.8994140625
Iteration 27000: Loss = -12460.888671875
Iteration 27100: Loss = -12460.875
Iteration 27200: Loss = -12460.861328125
Iteration 27300: Loss = -12460.861328125
Iteration 27400: Loss = -12460.85546875
Iteration 27500: Loss = -12460.84375
Iteration 27600: Loss = -12460.8349609375
Iteration 27700: Loss = -12460.8330078125
Iteration 27800: Loss = -12460.8330078125
Iteration 27900: Loss = -12460.833984375
1
Iteration 28000: Loss = -12460.833984375
2
Iteration 28100: Loss = -12460.833984375
3
Iteration 28200: Loss = -12460.833984375
4
Iteration 28300: Loss = -12460.8349609375
5
Iteration 28400: Loss = -12460.8330078125
Iteration 28500: Loss = -12460.833984375
1
Iteration 28600: Loss = -12460.8330078125
Iteration 28700: Loss = -12460.8349609375
1
Iteration 28800: Loss = -12460.8330078125
Iteration 28900: Loss = -12460.8349609375
1
Iteration 29000: Loss = -12460.833984375
2
Iteration 29100: Loss = -12460.8349609375
3
Iteration 29200: Loss = -12460.8330078125
Iteration 29300: Loss = -12460.833984375
1
Iteration 29400: Loss = -12460.8330078125
Iteration 29500: Loss = -12460.8330078125
Iteration 29600: Loss = -12460.8330078125
Iteration 29700: Loss = -12460.83203125
Iteration 29800: Loss = -12460.83203125
Iteration 29900: Loss = -12460.8349609375
1
pi: tensor([[1.1330e-06, 1.0000e+00],
        [6.6759e-03, 9.9332e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5726, 0.4274], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3146, 0.0912],
         [0.8464, 0.2098]],

        [[0.8637, 0.2938],
         [0.7651, 0.3480]],

        [[0.9146, 0.1579],
         [0.5169, 0.7736]],

        [[0.5267, 0.1813],
         [0.0137, 0.9901]],

        [[0.9843, 0.3133],
         [0.0133, 0.9896]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.016866378399786924
Average Adjusted Rand Index: 0.19199877740731
[0.9602918814578075, -0.016866378399786924] [0.9601487512009026, 0.19199877740731] [12050.3486328125, 12460.8330078125]
-------------------------------------
This iteration is 54
True Objective function: Loss = -11835.668533587863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28573.599609375
Iteration 100: Loss = -19130.419921875
Iteration 200: Loss = -14229.123046875
Iteration 300: Loss = -12794.828125
Iteration 400: Loss = -12549.595703125
Iteration 500: Loss = -12475.0009765625
Iteration 600: Loss = -12429.171875
Iteration 700: Loss = -12394.6806640625
Iteration 800: Loss = -12357.517578125
Iteration 900: Loss = -12317.7626953125
Iteration 1000: Loss = -12272.4755859375
Iteration 1100: Loss = -12238.3291015625
Iteration 1200: Loss = -12199.4375
Iteration 1300: Loss = -12183.9130859375
Iteration 1400: Loss = -12151.8271484375
Iteration 1500: Loss = -12133.712890625
Iteration 1600: Loss = -12122.634765625
Iteration 1700: Loss = -12118.0634765625
Iteration 1800: Loss = -12116.9189453125
Iteration 1900: Loss = -12116.138671875
Iteration 2000: Loss = -12114.7099609375
Iteration 2100: Loss = -12113.560546875
Iteration 2200: Loss = -12113.171875
Iteration 2300: Loss = -12112.8623046875
Iteration 2400: Loss = -12112.60546875
Iteration 2500: Loss = -12112.3916015625
Iteration 2600: Loss = -12112.2060546875
Iteration 2700: Loss = -12112.044921875
Iteration 2800: Loss = -12111.904296875
Iteration 2900: Loss = -12111.7802734375
Iteration 3000: Loss = -12111.669921875
Iteration 3100: Loss = -12111.5732421875
Iteration 3200: Loss = -12111.486328125
Iteration 3300: Loss = -12111.41015625
Iteration 3400: Loss = -12111.33984375
Iteration 3500: Loss = -12111.2763671875
Iteration 3600: Loss = -12111.2197265625
Iteration 3700: Loss = -12111.16796875
Iteration 3800: Loss = -12111.12109375
Iteration 3900: Loss = -12111.076171875
Iteration 4000: Loss = -12111.0361328125
Iteration 4100: Loss = -12111.0009765625
Iteration 4200: Loss = -12110.966796875
Iteration 4300: Loss = -12110.935546875
Iteration 4400: Loss = -12110.9072265625
Iteration 4500: Loss = -12110.880859375
Iteration 4600: Loss = -12110.8564453125
Iteration 4700: Loss = -12110.8330078125
Iteration 4800: Loss = -12110.8115234375
Iteration 4900: Loss = -12110.79296875
Iteration 5000: Loss = -12110.775390625
Iteration 5100: Loss = -12110.7568359375
Iteration 5200: Loss = -12110.7421875
Iteration 5300: Loss = -12110.7275390625
Iteration 5400: Loss = -12110.712890625
Iteration 5500: Loss = -12110.7001953125
Iteration 5600: Loss = -12110.6884765625
Iteration 5700: Loss = -12110.67578125
Iteration 5800: Loss = -12110.666015625
Iteration 5900: Loss = -12110.654296875
Iteration 6000: Loss = -12110.6455078125
Iteration 6100: Loss = -12110.63671875
Iteration 6200: Loss = -12110.6279296875
Iteration 6300: Loss = -12110.619140625
Iteration 6400: Loss = -12110.6103515625
Iteration 6500: Loss = -12110.603515625
Iteration 6600: Loss = -12110.5966796875
Iteration 6700: Loss = -12110.5888671875
Iteration 6800: Loss = -12110.5849609375
Iteration 6900: Loss = -12110.578125
Iteration 7000: Loss = -12110.5712890625
Iteration 7100: Loss = -12110.564453125
Iteration 7200: Loss = -12110.5576171875
Iteration 7300: Loss = -12110.5498046875
Iteration 7400: Loss = -12110.5419921875
Iteration 7500: Loss = -12110.53125
Iteration 7600: Loss = -12110.5166015625
Iteration 7700: Loss = -12110.4892578125
Iteration 7800: Loss = -12110.41796875
Iteration 7900: Loss = -12109.78515625
Iteration 8000: Loss = -12105.578125
Iteration 8100: Loss = -12103.2255859375
Iteration 8200: Loss = -12101.26171875
Iteration 8300: Loss = -12092.2197265625
Iteration 8400: Loss = -12087.4599609375
Iteration 8500: Loss = -12076.7548828125
Iteration 8600: Loss = -12074.2255859375
Iteration 8700: Loss = -12066.267578125
Iteration 8800: Loss = -12032.400390625
Iteration 8900: Loss = -12024.341796875
Iteration 9000: Loss = -12020.6591796875
Iteration 9100: Loss = -12010.2392578125
Iteration 9200: Loss = -12008.9052734375
Iteration 9300: Loss = -11999.3369140625
Iteration 9400: Loss = -11994.4130859375
Iteration 9500: Loss = -11983.43359375
Iteration 9600: Loss = -11972.2060546875
Iteration 9700: Loss = -11970.302734375
Iteration 9800: Loss = -11951.61328125
Iteration 9900: Loss = -11947.7939453125
Iteration 10000: Loss = -11936.1904296875
Iteration 10100: Loss = -11935.7998046875
Iteration 10200: Loss = -11923.9208984375
Iteration 10300: Loss = -11923.478515625
Iteration 10400: Loss = -11914.4072265625
Iteration 10500: Loss = -11903.6728515625
Iteration 10600: Loss = -11903.509765625
Iteration 10700: Loss = -11903.435546875
Iteration 10800: Loss = -11903.3896484375
Iteration 10900: Loss = -11903.357421875
Iteration 11000: Loss = -11903.33203125
Iteration 11100: Loss = -11903.306640625
Iteration 11200: Loss = -11899.1904296875
Iteration 11300: Loss = -11891.6689453125
Iteration 11400: Loss = -11891.59765625
Iteration 11500: Loss = -11891.5712890625
Iteration 11600: Loss = -11891.5546875
Iteration 11700: Loss = -11891.541015625
Iteration 11800: Loss = -11891.53125
Iteration 11900: Loss = -11891.5205078125
Iteration 12000: Loss = -11891.509765625
Iteration 12100: Loss = -11891.5048828125
Iteration 12200: Loss = -11891.4990234375
Iteration 12300: Loss = -11891.494140625
Iteration 12400: Loss = -11891.4892578125
Iteration 12500: Loss = -11891.486328125
Iteration 12600: Loss = -11891.482421875
Iteration 12700: Loss = -11883.87890625
Iteration 12800: Loss = -11875.2197265625
Iteration 12900: Loss = -11875.119140625
Iteration 13000: Loss = -11875.080078125
Iteration 13100: Loss = -11875.05859375
Iteration 13200: Loss = -11875.0439453125
Iteration 13300: Loss = -11875.03515625
Iteration 13400: Loss = -11875.0263671875
Iteration 13500: Loss = -11875.0205078125
Iteration 13600: Loss = -11875.015625
Iteration 13700: Loss = -11875.0126953125
Iteration 13800: Loss = -11875.0087890625
Iteration 13900: Loss = -11875.005859375
Iteration 14000: Loss = -11875.001953125
Iteration 14100: Loss = -11875.0
Iteration 14200: Loss = -11874.998046875
Iteration 14300: Loss = -11874.99609375
Iteration 14400: Loss = -11874.994140625
Iteration 14500: Loss = -11874.9931640625
Iteration 14600: Loss = -11874.9921875
Iteration 14700: Loss = -11874.9912109375
Iteration 14800: Loss = -11874.98828125
Iteration 14900: Loss = -11874.9873046875
Iteration 15000: Loss = -11874.986328125
Iteration 15100: Loss = -11874.9853515625
Iteration 15200: Loss = -11874.984375
Iteration 15300: Loss = -11874.9833984375
Iteration 15400: Loss = -11874.9833984375
Iteration 15500: Loss = -11874.982421875
Iteration 15600: Loss = -11874.9833984375
1
Iteration 15700: Loss = -11874.9814453125
Iteration 15800: Loss = -11874.982421875
1
Iteration 15900: Loss = -11874.98046875
Iteration 16000: Loss = -11874.9794921875
Iteration 16100: Loss = -11874.9794921875
Iteration 16200: Loss = -11874.978515625
Iteration 16300: Loss = -11870.8173828125
Iteration 16400: Loss = -11870.7080078125
Iteration 16500: Loss = -11870.701171875
Iteration 16600: Loss = -11870.6953125
Iteration 16700: Loss = -11870.6953125
Iteration 16800: Loss = -11870.693359375
Iteration 16900: Loss = -11870.6923828125
Iteration 17000: Loss = -11870.693359375
1
Iteration 17100: Loss = -11870.69140625
Iteration 17200: Loss = -11870.6923828125
1
Iteration 17300: Loss = -11870.6904296875
Iteration 17400: Loss = -11870.69140625
1
Iteration 17500: Loss = -11870.6884765625
Iteration 17600: Loss = -11870.689453125
1
Iteration 17700: Loss = -11870.6884765625
Iteration 17800: Loss = -11870.6875
Iteration 17900: Loss = -11870.689453125
1
Iteration 18000: Loss = -11870.6884765625
2
Iteration 18100: Loss = -11870.689453125
3
Iteration 18200: Loss = -11870.6884765625
4
Iteration 18300: Loss = -11870.6875
Iteration 18400: Loss = -11870.6875
Iteration 18500: Loss = -11870.6884765625
1
Iteration 18600: Loss = -11870.6875
Iteration 18700: Loss = -11870.6884765625
1
Iteration 18800: Loss = -11870.6875
Iteration 18900: Loss = -11870.6875
Iteration 19000: Loss = -11870.6875
Iteration 19100: Loss = -11870.6875
Iteration 19200: Loss = -11870.6875
Iteration 19300: Loss = -11870.6875
Iteration 19400: Loss = -11870.6865234375
Iteration 19500: Loss = -11870.6875
1
Iteration 19600: Loss = -11870.6865234375
Iteration 19700: Loss = -11870.6865234375
Iteration 19800: Loss = -11870.6875
1
Iteration 19900: Loss = -11870.6533203125
Iteration 20000: Loss = -11860.4765625
Iteration 20100: Loss = -11860.328125
Iteration 20200: Loss = -11860.296875
Iteration 20300: Loss = -11860.2822265625
Iteration 20400: Loss = -11860.275390625
Iteration 20500: Loss = -11860.2685546875
Iteration 20600: Loss = -11860.2666015625
Iteration 20700: Loss = -11860.263671875
Iteration 20800: Loss = -11860.259765625
Iteration 20900: Loss = -11860.259765625
Iteration 21000: Loss = -11860.2578125
Iteration 21100: Loss = -11860.255859375
Iteration 21200: Loss = -11860.255859375
Iteration 21300: Loss = -11860.25390625
Iteration 21400: Loss = -11860.2548828125
1
Iteration 21500: Loss = -11860.25390625
Iteration 21600: Loss = -11860.251953125
Iteration 21700: Loss = -11860.2509765625
Iteration 21800: Loss = -11860.251953125
1
Iteration 21900: Loss = -11860.251953125
2
Iteration 22000: Loss = -11860.2509765625
Iteration 22100: Loss = -11860.25
Iteration 22200: Loss = -11860.2509765625
1
Iteration 22300: Loss = -11860.2509765625
2
Iteration 22400: Loss = -11860.25
Iteration 22500: Loss = -11860.25
Iteration 22600: Loss = -11860.248046875
Iteration 22700: Loss = -11860.248046875
Iteration 22800: Loss = -11860.248046875
Iteration 22900: Loss = -11860.2470703125
Iteration 23000: Loss = -11860.248046875
1
Iteration 23100: Loss = -11860.2470703125
Iteration 23200: Loss = -11860.2470703125
Iteration 23300: Loss = -11860.2470703125
Iteration 23400: Loss = -11860.24609375
Iteration 23500: Loss = -11860.248046875
1
Iteration 23600: Loss = -11860.24609375
Iteration 23700: Loss = -11860.24609375
Iteration 23800: Loss = -11860.24609375
Iteration 23900: Loss = -11860.24609375
Iteration 24000: Loss = -11860.24609375
Iteration 24100: Loss = -11860.24609375
Iteration 24200: Loss = -11860.24609375
Iteration 24300: Loss = -11860.248046875
1
Iteration 24400: Loss = -11860.24609375
Iteration 24500: Loss = -11860.24609375
Iteration 24600: Loss = -11860.24609375
Iteration 24700: Loss = -11860.2451171875
Iteration 24800: Loss = -11860.2470703125
1
Iteration 24900: Loss = -11860.24609375
2
Iteration 25000: Loss = -11860.15625
Iteration 25100: Loss = -11860.1572265625
1
Iteration 25200: Loss = -11860.1572265625
2
Iteration 25300: Loss = -11860.1572265625
3
Iteration 25400: Loss = -11860.15625
Iteration 25500: Loss = -11860.1572265625
1
Iteration 25600: Loss = -11860.1572265625
2
Iteration 25700: Loss = -11860.15625
Iteration 25800: Loss = -11860.15625
Iteration 25900: Loss = -11860.15625
Iteration 26000: Loss = -11860.15625
Iteration 26100: Loss = -11860.15625
Iteration 26200: Loss = -11860.15625
Iteration 26300: Loss = -11860.15625
Iteration 26400: Loss = -11860.154296875
Iteration 26500: Loss = -11860.1552734375
1
Iteration 26600: Loss = -11860.154296875
Iteration 26700: Loss = -11860.154296875
Iteration 26800: Loss = -11860.154296875
Iteration 26900: Loss = -11860.1552734375
1
Iteration 27000: Loss = -11860.1552734375
2
Iteration 27100: Loss = -11860.1552734375
3
Iteration 27200: Loss = -11860.154296875
Iteration 27300: Loss = -11860.154296875
Iteration 27400: Loss = -11860.1533203125
Iteration 27500: Loss = -11860.1552734375
1
Iteration 27600: Loss = -11860.154296875
2
Iteration 27700: Loss = -11860.154296875
3
Iteration 27800: Loss = -11860.1552734375
4
Iteration 27900: Loss = -11860.154296875
5
Iteration 28000: Loss = -11860.1533203125
Iteration 28100: Loss = -11860.154296875
1
Iteration 28200: Loss = -11860.15625
2
Iteration 28300: Loss = -11860.154296875
3
Iteration 28400: Loss = -11860.1533203125
Iteration 28500: Loss = -11860.1552734375
1
Iteration 28600: Loss = -11860.1533203125
Iteration 28700: Loss = -11860.150390625
Iteration 28800: Loss = -11860.15234375
1
Iteration 28900: Loss = -11860.1513671875
2
Iteration 29000: Loss = -11860.15234375
3
Iteration 29100: Loss = -11860.15234375
4
Iteration 29200: Loss = -11860.15234375
5
Iteration 29300: Loss = -11860.1435546875
Iteration 29400: Loss = -11860.1416015625
Iteration 29500: Loss = -11860.142578125
1
Iteration 29600: Loss = -11860.1416015625
Iteration 29700: Loss = -11860.142578125
1
Iteration 29800: Loss = -11860.142578125
2
Iteration 29900: Loss = -11860.1416015625
pi: tensor([[0.5823, 0.4177],
        [0.3801, 0.6199]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4935, 0.5065], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3043, 0.0942],
         [0.2644, 0.3073]],

        [[0.0146, 0.0936],
         [0.9563, 0.1477]],

        [[0.0507, 0.0921],
         [0.2181, 0.6698]],

        [[0.9604, 0.1014],
         [0.9867, 0.0103]],

        [[0.9776, 0.0978],
         [0.1823, 0.2312]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 2
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.03493403045501691
Average Adjusted Rand Index: 0.9681591812880072
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30504.5546875
Iteration 100: Loss = -18295.1484375
Iteration 200: Loss = -13385.5810546875
Iteration 300: Loss = -12654.08984375
Iteration 400: Loss = -12550.7900390625
Iteration 500: Loss = -12491.0810546875
Iteration 600: Loss = -12462.40234375
Iteration 700: Loss = -12445.8740234375
Iteration 800: Loss = -12437.1982421875
Iteration 900: Loss = -12432.75390625
Iteration 1000: Loss = -12429.802734375
Iteration 1100: Loss = -12427.6611328125
Iteration 1200: Loss = -12426.02734375
Iteration 1300: Loss = -12424.7255859375
Iteration 1400: Loss = -12423.658203125
Iteration 1500: Loss = -12422.7724609375
Iteration 1600: Loss = -12422.01953125
Iteration 1700: Loss = -12421.3740234375
Iteration 1800: Loss = -12420.8125
Iteration 1900: Loss = -12420.3212890625
Iteration 2000: Loss = -12419.8876953125
Iteration 2100: Loss = -12419.5009765625
Iteration 2200: Loss = -12419.158203125
Iteration 2300: Loss = -12418.84765625
Iteration 2400: Loss = -12418.5654296875
Iteration 2500: Loss = -12418.3125
Iteration 2600: Loss = -12418.0888671875
Iteration 2700: Loss = -12417.884765625
Iteration 2800: Loss = -12417.7021484375
Iteration 2900: Loss = -12417.533203125
Iteration 3000: Loss = -12417.3779296875
Iteration 3100: Loss = -12417.236328125
Iteration 3200: Loss = -12417.1064453125
Iteration 3300: Loss = -12416.9833984375
Iteration 3400: Loss = -12416.8701171875
Iteration 3500: Loss = -12416.7666015625
Iteration 3600: Loss = -12416.669921875
Iteration 3700: Loss = -12416.580078125
Iteration 3800: Loss = -12416.4951171875
Iteration 3900: Loss = -12416.4169921875
Iteration 4000: Loss = -12416.34375
Iteration 4100: Loss = -12416.2734375
Iteration 4200: Loss = -12416.20703125
Iteration 4300: Loss = -12416.1416015625
Iteration 4400: Loss = -12416.078125
Iteration 4500: Loss = -12416.01171875
Iteration 4600: Loss = -12415.9404296875
Iteration 4700: Loss = -12415.8525390625
Iteration 4800: Loss = -12415.74609375
Iteration 4900: Loss = -12415.6240234375
Iteration 5000: Loss = -12415.509765625
Iteration 5100: Loss = -12415.4140625
Iteration 5200: Loss = -12415.3271484375
Iteration 5300: Loss = -12415.2490234375
Iteration 5400: Loss = -12415.1748046875
Iteration 5500: Loss = -12415.1015625
Iteration 5600: Loss = -12415.029296875
Iteration 5700: Loss = -12414.95703125
Iteration 5800: Loss = -12414.8798828125
Iteration 5900: Loss = -12414.796875
Iteration 6000: Loss = -12414.69921875
Iteration 6100: Loss = -12414.5908203125
Iteration 6200: Loss = -12414.4755859375
Iteration 6300: Loss = -12414.3681640625
Iteration 6400: Loss = -12414.2802734375
Iteration 6500: Loss = -12414.21484375
Iteration 6600: Loss = -12414.16015625
Iteration 6700: Loss = -12414.1142578125
Iteration 6800: Loss = -12414.0712890625
Iteration 6900: Loss = -12414.033203125
Iteration 7000: Loss = -12413.998046875
Iteration 7100: Loss = -12413.96484375
Iteration 7200: Loss = -12413.9345703125
Iteration 7300: Loss = -12413.9052734375
Iteration 7400: Loss = -12413.87890625
Iteration 7500: Loss = -12413.849609375
Iteration 7600: Loss = -12413.8173828125
Iteration 7700: Loss = -12413.7802734375
Iteration 7800: Loss = -12413.740234375
Iteration 7900: Loss = -12413.6953125
Iteration 8000: Loss = -12413.6689453125
Iteration 8100: Loss = -12413.6474609375
Iteration 8200: Loss = -12413.630859375
Iteration 8300: Loss = -12413.6123046875
Iteration 8400: Loss = -12413.5986328125
Iteration 8500: Loss = -12413.583984375
Iteration 8600: Loss = -12413.5693359375
Iteration 8700: Loss = -12413.5576171875
Iteration 8800: Loss = -12413.544921875
Iteration 8900: Loss = -12413.533203125
Iteration 9000: Loss = -12413.521484375
Iteration 9100: Loss = -12413.513671875
Iteration 9200: Loss = -12413.5029296875
Iteration 9300: Loss = -12413.494140625
Iteration 9400: Loss = -12413.4833984375
Iteration 9500: Loss = -12413.4697265625
Iteration 9600: Loss = -12413.4169921875
Iteration 9700: Loss = -12413.2392578125
Iteration 9800: Loss = -12413.208984375
Iteration 9900: Loss = -12413.1884765625
Iteration 10000: Loss = -12413.169921875
Iteration 10100: Loss = -12413.154296875
Iteration 10200: Loss = -12413.142578125
Iteration 10300: Loss = -12413.1298828125
Iteration 10400: Loss = -12413.1171875
Iteration 10500: Loss = -12413.107421875
Iteration 10600: Loss = -12413.0966796875
Iteration 10700: Loss = -12413.087890625
Iteration 10800: Loss = -12413.078125
Iteration 10900: Loss = -12413.0693359375
Iteration 11000: Loss = -12413.060546875
Iteration 11100: Loss = -12413.0546875
Iteration 11200: Loss = -12413.046875
Iteration 11300: Loss = -12413.0400390625
Iteration 11400: Loss = -12413.0341796875
Iteration 11500: Loss = -12413.0283203125
Iteration 11600: Loss = -12413.0234375
Iteration 11700: Loss = -12413.017578125
Iteration 11800: Loss = -12413.0126953125
Iteration 11900: Loss = -12413.0087890625
Iteration 12000: Loss = -12413.00390625
Iteration 12100: Loss = -12413.0009765625
Iteration 12200: Loss = -12412.9970703125
Iteration 12300: Loss = -12412.994140625
Iteration 12400: Loss = -12412.990234375
Iteration 12500: Loss = -12412.98828125
Iteration 12600: Loss = -12412.9853515625
Iteration 12700: Loss = -12412.9814453125
Iteration 12800: Loss = -12412.98046875
Iteration 12900: Loss = -12412.9794921875
Iteration 13000: Loss = -12412.9765625
Iteration 13100: Loss = -12412.9765625
Iteration 13200: Loss = -12412.974609375
Iteration 13300: Loss = -12412.9716796875
Iteration 13400: Loss = -12412.9697265625
Iteration 13500: Loss = -12412.9697265625
Iteration 13600: Loss = -12412.9677734375
Iteration 13700: Loss = -12412.9677734375
Iteration 13800: Loss = -12412.96484375
Iteration 13900: Loss = -12412.9638671875
Iteration 14000: Loss = -12412.9638671875
Iteration 14100: Loss = -12412.962890625
Iteration 14200: Loss = -12412.9599609375
Iteration 14300: Loss = -12412.9599609375
Iteration 14400: Loss = -12412.9599609375
Iteration 14500: Loss = -12412.9580078125
Iteration 14600: Loss = -12412.95703125
Iteration 14700: Loss = -12412.95703125
Iteration 14800: Loss = -12412.95703125
Iteration 14900: Loss = -12412.9541015625
Iteration 15000: Loss = -12412.9560546875
1
Iteration 15100: Loss = -12412.9521484375
Iteration 15200: Loss = -12412.9501953125
Iteration 15300: Loss = -12412.9482421875
Iteration 15400: Loss = -12412.94921875
1
Iteration 15500: Loss = -12412.951171875
2
Iteration 15600: Loss = -12412.9462890625
Iteration 15700: Loss = -12412.947265625
1
Iteration 15800: Loss = -12412.947265625
2
Iteration 15900: Loss = -12412.9453125
Iteration 16000: Loss = -12412.9443359375
Iteration 16100: Loss = -12412.9453125
1
Iteration 16200: Loss = -12412.9453125
2
Iteration 16300: Loss = -12412.9453125
3
Iteration 16400: Loss = -12412.9453125
4
Iteration 16500: Loss = -12412.943359375
Iteration 16600: Loss = -12412.9453125
1
Iteration 16700: Loss = -12412.943359375
Iteration 16800: Loss = -12412.9423828125
Iteration 16900: Loss = -12412.943359375
1
Iteration 17000: Loss = -12412.9423828125
Iteration 17100: Loss = -12412.9423828125
Iteration 17200: Loss = -12412.9423828125
Iteration 17300: Loss = -12412.9423828125
Iteration 17400: Loss = -12412.943359375
1
Iteration 17500: Loss = -12412.943359375
2
Iteration 17600: Loss = -12412.9404296875
Iteration 17700: Loss = -12412.943359375
1
Iteration 17800: Loss = -12412.94140625
2
Iteration 17900: Loss = -12412.9404296875
Iteration 18000: Loss = -12412.94140625
1
Iteration 18100: Loss = -12412.94140625
2
Iteration 18200: Loss = -12412.939453125
Iteration 18300: Loss = -12412.9404296875
1
Iteration 18400: Loss = -12412.939453125
Iteration 18500: Loss = -12412.939453125
Iteration 18600: Loss = -12412.9404296875
1
Iteration 18700: Loss = -12412.939453125
Iteration 18800: Loss = -12412.939453125
Iteration 18900: Loss = -12412.9384765625
Iteration 19000: Loss = -12412.9404296875
1
Iteration 19100: Loss = -12412.939453125
2
Iteration 19200: Loss = -12412.9384765625
Iteration 19300: Loss = -12412.9384765625
Iteration 19400: Loss = -12412.939453125
1
Iteration 19500: Loss = -12412.9375
Iteration 19600: Loss = -12412.939453125
1
Iteration 19700: Loss = -12412.9384765625
2
Iteration 19800: Loss = -12412.939453125
3
Iteration 19900: Loss = -12412.9384765625
4
Iteration 20000: Loss = -12412.943359375
5
Iteration 20100: Loss = -12412.9384765625
6
Iteration 20200: Loss = -12412.9375
Iteration 20300: Loss = -12412.939453125
1
Iteration 20400: Loss = -12412.9384765625
2
Iteration 20500: Loss = -12412.9375
Iteration 20600: Loss = -12412.9375
Iteration 20700: Loss = -12412.9375
Iteration 20800: Loss = -12412.939453125
1
Iteration 20900: Loss = -12412.9365234375
Iteration 21000: Loss = -12412.9365234375
Iteration 21100: Loss = -12412.9375
1
Iteration 21200: Loss = -12412.935546875
Iteration 21300: Loss = -12412.138671875
Iteration 21400: Loss = -12412.02734375
Iteration 21500: Loss = -12412.0166015625
Iteration 21600: Loss = -12412.01171875
Iteration 21700: Loss = -12412.0107421875
Iteration 21800: Loss = -12412.0107421875
Iteration 21900: Loss = -12412.0078125
Iteration 22000: Loss = -12412.0078125
Iteration 22100: Loss = -12412.0048828125
Iteration 22200: Loss = -12412.005859375
1
Iteration 22300: Loss = -12412.0068359375
2
Iteration 22400: Loss = -12412.005859375
3
Iteration 22500: Loss = -12412.005859375
4
Iteration 22600: Loss = -12412.00390625
Iteration 22700: Loss = -12412.0068359375
1
Iteration 22800: Loss = -12412.0048828125
2
Iteration 22900: Loss = -12412.0029296875
Iteration 23000: Loss = -12412.0029296875
Iteration 23100: Loss = -12412.005859375
1
Iteration 23200: Loss = -12412.00390625
2
Iteration 23300: Loss = -12412.0048828125
3
Iteration 23400: Loss = -12412.00390625
4
Iteration 23500: Loss = -12412.00390625
5
Iteration 23600: Loss = -12412.00390625
6
Iteration 23700: Loss = -12412.0029296875
Iteration 23800: Loss = -12412.0029296875
Iteration 23900: Loss = -12412.00390625
1
Iteration 24000: Loss = -12412.0029296875
Iteration 24100: Loss = -12412.00390625
1
Iteration 24200: Loss = -12412.00390625
2
Iteration 24300: Loss = -12412.0048828125
3
Iteration 24400: Loss = -12412.0009765625
Iteration 24500: Loss = -12412.0029296875
1
Iteration 24600: Loss = -12412.0029296875
2
Iteration 24700: Loss = -12412.0029296875
3
Iteration 24800: Loss = -12412.00390625
4
Iteration 24900: Loss = -12412.00390625
5
Iteration 25000: Loss = -12412.0029296875
6
Iteration 25100: Loss = -12412.00390625
7
Iteration 25200: Loss = -12412.001953125
8
Iteration 25300: Loss = -12412.00390625
9
Iteration 25400: Loss = -12412.0048828125
10
Iteration 25500: Loss = -12412.0009765625
Iteration 25600: Loss = -12412.0029296875
1
Iteration 25700: Loss = -12412.001953125
2
Iteration 25800: Loss = -12412.0029296875
3
Iteration 25900: Loss = -12412.001953125
4
Iteration 26000: Loss = -12412.00390625
5
Iteration 26100: Loss = -12412.0029296875
6
Iteration 26200: Loss = -12412.00390625
7
Iteration 26300: Loss = -12412.00390625
8
Iteration 26400: Loss = -12412.00390625
9
Iteration 26500: Loss = -12412.001953125
10
Iteration 26600: Loss = -12412.00390625
11
Iteration 26700: Loss = -12412.0009765625
Iteration 26800: Loss = -12412.0048828125
1
Iteration 26900: Loss = -12412.001953125
2
Iteration 27000: Loss = -12412.0029296875
3
Iteration 27100: Loss = -12412.0029296875
4
Iteration 27200: Loss = -12412.0029296875
5
Iteration 27300: Loss = -12412.0029296875
6
Iteration 27400: Loss = -12412.0029296875
7
Iteration 27500: Loss = -12412.0029296875
8
Iteration 27600: Loss = -12412.001953125
9
Iteration 27700: Loss = -12412.001953125
10
Iteration 27800: Loss = -12412.0029296875
11
Iteration 27900: Loss = -12412.0029296875
12
Iteration 28000: Loss = -12412.001953125
13
Iteration 28100: Loss = -12412.0029296875
14
Iteration 28200: Loss = -12412.0029296875
15
Stopping early at iteration 28200 due to no improvement.
pi: tensor([[1.0000e+00, 1.8408e-06],
        [1.8180e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0309, 0.9691], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1325, 0.2468],
         [0.0075, 0.1983]],

        [[0.9539, 0.2442],
         [0.0515, 0.2563]],

        [[0.9766, 0.1800],
         [0.5882, 0.9678]],

        [[0.0744, 0.2346],
         [0.9897, 0.9862]],

        [[0.7508, 0.3021],
         [0.0765, 0.0153]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0007766707522784365
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 60
Adjusted Rand Index: -0.004063414211148451
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.002193311857154732
Global Adjusted Rand Index: -0.0020226881837753564
Average Adjusted Rand Index: -0.002265796409388813
[0.03493403045501691, -0.0020226881837753564] [0.9681591812880072, -0.002265796409388813] [11860.1435546875, 12412.0029296875]
-------------------------------------
This iteration is 55
True Objective function: Loss = -11894.311378522678
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -52913.6796875
Iteration 100: Loss = -30702.076171875
Iteration 200: Loss = -17031.693359375
Iteration 300: Loss = -13908.1279296875
Iteration 400: Loss = -13181.4599609375
Iteration 500: Loss = -12917.0224609375
Iteration 600: Loss = -12753.73046875
Iteration 700: Loss = -12624.4189453125
Iteration 800: Loss = -12562.2861328125
Iteration 900: Loss = -12526.708984375
Iteration 1000: Loss = -12489.521484375
Iteration 1100: Loss = -12462.19921875
Iteration 1200: Loss = -12452.2666015625
Iteration 1300: Loss = -12444.9375
Iteration 1400: Loss = -12439.0517578125
Iteration 1500: Loss = -12433.9306640625
Iteration 1600: Loss = -12423.361328125
Iteration 1700: Loss = -12418.818359375
Iteration 1800: Loss = -12415.5966796875
Iteration 1900: Loss = -12413.107421875
Iteration 2000: Loss = -12411.0498046875
Iteration 2100: Loss = -12409.294921875
Iteration 2200: Loss = -12407.7744140625
Iteration 2300: Loss = -12406.4384765625
Iteration 2400: Loss = -12405.25390625
Iteration 2500: Loss = -12401.8671875
Iteration 2600: Loss = -12395.94921875
Iteration 2700: Loss = -12394.966796875
Iteration 2800: Loss = -12394.1494140625
Iteration 2900: Loss = -12393.43359375
Iteration 3000: Loss = -12392.7939453125
Iteration 3100: Loss = -12392.212890625
Iteration 3200: Loss = -12391.6875
Iteration 3300: Loss = -12391.2109375
Iteration 3400: Loss = -12390.7744140625
Iteration 3500: Loss = -12390.3740234375
Iteration 3600: Loss = -12390.005859375
Iteration 3700: Loss = -12389.6669921875
Iteration 3800: Loss = -12389.3544921875
Iteration 3900: Loss = -12389.0634765625
Iteration 4000: Loss = -12388.7978515625
Iteration 4100: Loss = -12388.546875
Iteration 4200: Loss = -12388.31640625
Iteration 4300: Loss = -12388.1015625
Iteration 4400: Loss = -12387.9013671875
Iteration 4500: Loss = -12387.71484375
Iteration 4600: Loss = -12387.5400390625
Iteration 4700: Loss = -12387.3759765625
Iteration 4800: Loss = -12387.2236328125
Iteration 4900: Loss = -12387.0810546875
Iteration 5000: Loss = -12386.94921875
Iteration 5100: Loss = -12386.82421875
Iteration 5200: Loss = -12386.7060546875
Iteration 5300: Loss = -12386.5947265625
Iteration 5400: Loss = -12386.4912109375
Iteration 5500: Loss = -12386.3935546875
Iteration 5600: Loss = -12386.30078125
Iteration 5700: Loss = -12386.21484375
Iteration 5800: Loss = -12386.134765625
Iteration 5900: Loss = -12386.0576171875
Iteration 6000: Loss = -12385.9873046875
Iteration 6100: Loss = -12385.9189453125
Iteration 6200: Loss = -12385.8544921875
Iteration 6300: Loss = -12385.79296875
Iteration 6400: Loss = -12385.7353515625
Iteration 6500: Loss = -12385.6826171875
Iteration 6600: Loss = -12385.6298828125
Iteration 6700: Loss = -12385.580078125
Iteration 6800: Loss = -12385.5341796875
Iteration 6900: Loss = -12385.4912109375
Iteration 7000: Loss = -12385.44921875
Iteration 7100: Loss = -12385.41015625
Iteration 7200: Loss = -12385.3720703125
Iteration 7300: Loss = -12385.3359375
Iteration 7400: Loss = -12385.302734375
Iteration 7500: Loss = -12385.2685546875
Iteration 7600: Loss = -12385.240234375
Iteration 7700: Loss = -12385.20703125
Iteration 7800: Loss = -12385.1796875
Iteration 7900: Loss = -12385.1533203125
Iteration 8000: Loss = -12385.1259765625
Iteration 8100: Loss = -12385.1015625
Iteration 8200: Loss = -12385.0771484375
Iteration 8300: Loss = -12385.0546875
Iteration 8400: Loss = -12385.0322265625
Iteration 8500: Loss = -12385.0107421875
Iteration 8600: Loss = -12384.9912109375
Iteration 8700: Loss = -12384.9716796875
Iteration 8800: Loss = -12384.9541015625
Iteration 8900: Loss = -12384.935546875
Iteration 9000: Loss = -12384.919921875
Iteration 9100: Loss = -12384.90234375
Iteration 9200: Loss = -12384.88671875
Iteration 9300: Loss = -12384.87109375
Iteration 9400: Loss = -12384.85546875
Iteration 9500: Loss = -12384.83984375
Iteration 9600: Loss = -12384.8251953125
Iteration 9700: Loss = -12384.810546875
Iteration 9800: Loss = -12384.7939453125
Iteration 9900: Loss = -12384.7802734375
Iteration 10000: Loss = -12384.76171875
Iteration 10100: Loss = -12384.7431640625
Iteration 10200: Loss = -12384.7216796875
Iteration 10300: Loss = -12384.6962890625
Iteration 10400: Loss = -12384.6591796875
Iteration 10500: Loss = -12384.6015625
Iteration 10600: Loss = -12384.498046875
Iteration 10700: Loss = -12384.171875
Iteration 10800: Loss = -12383.42578125
Iteration 10900: Loss = -12383.3095703125
Iteration 11000: Loss = -12383.2607421875
Iteration 11100: Loss = -12383.228515625
Iteration 11200: Loss = -12383.20703125
Iteration 11300: Loss = -12383.1884765625
Iteration 11400: Loss = -12383.171875
Iteration 11500: Loss = -12383.1591796875
Iteration 11600: Loss = -12383.1455078125
Iteration 11700: Loss = -12383.130859375
Iteration 11800: Loss = -12383.119140625
Iteration 11900: Loss = -12383.107421875
Iteration 12000: Loss = -12383.091796875
Iteration 12100: Loss = -12383.0732421875
Iteration 12200: Loss = -12383.05078125
Iteration 12300: Loss = -12383.0234375
Iteration 12400: Loss = -12382.9013671875
Iteration 12500: Loss = -12382.8251953125
Iteration 12600: Loss = -12382.7958984375
Iteration 12700: Loss = -12382.7548828125
Iteration 12800: Loss = -12382.6455078125
Iteration 12900: Loss = -12382.478515625
Iteration 13000: Loss = -12382.3623046875
Iteration 13100: Loss = -12382.1953125
Iteration 13200: Loss = -12381.9404296875
Iteration 13300: Loss = -12381.6552734375
Iteration 13400: Loss = -12381.3994140625
Iteration 13500: Loss = -12381.0947265625
Iteration 13600: Loss = -12380.9189453125
Iteration 13700: Loss = -12380.794921875
Iteration 13800: Loss = -12379.8056640625
Iteration 13900: Loss = -12379.521484375
Iteration 14000: Loss = -12379.4375
Iteration 14100: Loss = -12379.3818359375
Iteration 14200: Loss = -12379.3466796875
Iteration 14300: Loss = -12379.330078125
Iteration 14400: Loss = -12379.3134765625
Iteration 14500: Loss = -12379.2919921875
Iteration 14600: Loss = -12379.2197265625
Iteration 14700: Loss = -12378.90625
Iteration 14800: Loss = -12374.6318359375
Iteration 14900: Loss = -12374.4228515625
Iteration 15000: Loss = -12374.37890625
Iteration 15100: Loss = -12374.3564453125
Iteration 15200: Loss = -12374.34375
Iteration 15300: Loss = -12374.3359375
Iteration 15400: Loss = -12374.3271484375
Iteration 15500: Loss = -12374.32421875
Iteration 15600: Loss = -12374.318359375
Iteration 15700: Loss = -12374.31640625
Iteration 15800: Loss = -12374.3115234375
Iteration 15900: Loss = -12374.3095703125
Iteration 16000: Loss = -12374.3076171875
Iteration 16100: Loss = -12374.3056640625
Iteration 16200: Loss = -12374.3056640625
Iteration 16300: Loss = -12374.3037109375
Iteration 16400: Loss = -12374.3017578125
Iteration 16500: Loss = -12374.30078125
Iteration 16600: Loss = -12374.2998046875
Iteration 16700: Loss = -12374.2978515625
Iteration 16800: Loss = -12374.2978515625
Iteration 16900: Loss = -12374.2958984375
Iteration 17000: Loss = -12374.296875
1
Iteration 17100: Loss = -12374.2958984375
Iteration 17200: Loss = -12374.294921875
Iteration 17300: Loss = -12374.2939453125
Iteration 17400: Loss = -12374.2958984375
1
Iteration 17500: Loss = -12374.29296875
Iteration 17600: Loss = -12374.291015625
Iteration 17700: Loss = -12374.291015625
Iteration 17800: Loss = -12374.2900390625
Iteration 17900: Loss = -12374.2900390625
Iteration 18000: Loss = -12374.291015625
1
Iteration 18100: Loss = -12374.2900390625
Iteration 18200: Loss = -12374.2890625
Iteration 18300: Loss = -12374.2900390625
1
Iteration 18400: Loss = -12374.2880859375
Iteration 18500: Loss = -12374.2880859375
Iteration 18600: Loss = -12374.2890625
1
Iteration 18700: Loss = -12374.2861328125
Iteration 18800: Loss = -12374.2861328125
Iteration 18900: Loss = -12374.2861328125
Iteration 19000: Loss = -12374.2880859375
1
Iteration 19100: Loss = -12374.287109375
2
Iteration 19200: Loss = -12374.2861328125
Iteration 19300: Loss = -12374.2861328125
Iteration 19400: Loss = -12374.2861328125
Iteration 19500: Loss = -12374.2861328125
Iteration 19600: Loss = -12374.2861328125
Iteration 19700: Loss = -12374.2861328125
Iteration 19800: Loss = -12374.2861328125
Iteration 19900: Loss = -12374.287109375
1
Iteration 20000: Loss = -12374.2861328125
Iteration 20100: Loss = -12374.28515625
Iteration 20200: Loss = -12374.2861328125
1
Iteration 20300: Loss = -12374.2861328125
2
Iteration 20400: Loss = -12374.2841796875
Iteration 20500: Loss = -12374.2841796875
Iteration 20600: Loss = -12374.28515625
1
Iteration 20700: Loss = -12374.28515625
2
Iteration 20800: Loss = -12374.2841796875
Iteration 20900: Loss = -12374.2841796875
Iteration 21000: Loss = -12374.2861328125
1
Iteration 21100: Loss = -12374.283203125
Iteration 21200: Loss = -12374.2841796875
1
Iteration 21300: Loss = -12374.2841796875
2
Iteration 21400: Loss = -12374.28515625
3
Iteration 21500: Loss = -12374.2841796875
4
Iteration 21600: Loss = -12374.28515625
5
Iteration 21700: Loss = -12374.2841796875
6
Iteration 21800: Loss = -12374.283203125
Iteration 21900: Loss = -12374.28515625
1
Iteration 22000: Loss = -12374.2841796875
2
Iteration 22100: Loss = -12374.28515625
3
Iteration 22200: Loss = -12374.283203125
Iteration 22300: Loss = -12374.2841796875
1
Iteration 22400: Loss = -12374.283203125
Iteration 22500: Loss = -12374.283203125
Iteration 22600: Loss = -12374.28125
Iteration 22700: Loss = -12374.142578125
Iteration 22800: Loss = -12373.9599609375
Iteration 22900: Loss = -12373.8642578125
Iteration 23000: Loss = -12373.82421875
Iteration 23100: Loss = -12373.6455078125
Iteration 23200: Loss = -12373.6279296875
Iteration 23300: Loss = -12373.5048828125
Iteration 23400: Loss = -12373.4990234375
Iteration 23500: Loss = -12373.4716796875
Iteration 23600: Loss = -12373.47265625
1
Iteration 23700: Loss = -12373.458984375
Iteration 23800: Loss = -12373.4482421875
Iteration 23900: Loss = -12373.4443359375
Iteration 24000: Loss = -12373.4365234375
Iteration 24100: Loss = -12373.42578125
Iteration 24200: Loss = -12373.419921875
Iteration 24300: Loss = -12373.412109375
Iteration 24400: Loss = -12373.4052734375
Iteration 24500: Loss = -12373.4052734375
Iteration 24600: Loss = -12373.3935546875
Iteration 24700: Loss = -12373.392578125
Iteration 24800: Loss = -12373.388671875
Iteration 24900: Loss = -12373.384765625
Iteration 25000: Loss = -12373.3828125
Iteration 25100: Loss = -12373.37890625
Iteration 25200: Loss = -12373.37890625
Iteration 25300: Loss = -12373.3779296875
Iteration 25400: Loss = -12373.3779296875
Iteration 25500: Loss = -12373.3779296875
Iteration 25600: Loss = -12373.376953125
Iteration 25700: Loss = -12373.3779296875
1
Iteration 25800: Loss = -12373.380859375
2
Iteration 25900: Loss = -12373.37890625
3
Iteration 26000: Loss = -12373.376953125
Iteration 26100: Loss = -12373.3759765625
Iteration 26200: Loss = -12373.3779296875
1
Iteration 26300: Loss = -12373.37890625
2
Iteration 26400: Loss = -12373.376953125
3
Iteration 26500: Loss = -12373.376953125
4
Iteration 26600: Loss = -12373.3779296875
5
Iteration 26700: Loss = -12373.3798828125
6
Iteration 26800: Loss = -12373.3759765625
Iteration 26900: Loss = -12373.3779296875
1
Iteration 27000: Loss = -12373.3779296875
2
Iteration 27100: Loss = -12373.376953125
3
Iteration 27200: Loss = -12373.3779296875
4
Iteration 27300: Loss = -12373.3779296875
5
Iteration 27400: Loss = -12373.3779296875
6
Iteration 27500: Loss = -12373.3779296875
7
Iteration 27600: Loss = -12373.3779296875
8
Iteration 27700: Loss = -12373.3779296875
9
Iteration 27800: Loss = -12373.376953125
10
Iteration 27900: Loss = -12373.3759765625
Iteration 28000: Loss = -12373.37890625
1
Iteration 28100: Loss = -12373.376953125
2
Iteration 28200: Loss = -12373.37890625
3
Iteration 28300: Loss = -12373.3779296875
4
Iteration 28400: Loss = -12373.3759765625
Iteration 28500: Loss = -12373.376953125
1
Iteration 28600: Loss = -12373.3798828125
2
Iteration 28700: Loss = -12373.376953125
3
Iteration 28800: Loss = -12373.3779296875
4
Iteration 28900: Loss = -12373.376953125
5
Iteration 29000: Loss = -12373.3779296875
6
Iteration 29100: Loss = -12373.37890625
7
Iteration 29200: Loss = -12373.37890625
8
Iteration 29300: Loss = -12373.376953125
9
Iteration 29400: Loss = -12373.37890625
10
Iteration 29500: Loss = -12373.3759765625
Iteration 29600: Loss = -12373.3779296875
1
Iteration 29700: Loss = -12373.3779296875
2
Iteration 29800: Loss = -12373.37890625
3
Iteration 29900: Loss = -12373.376953125
4
pi: tensor([[0.9789, 0.0211],
        [0.9610, 0.0390]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.7053e-04, 9.9983e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1971, 0.2129],
         [0.0253, 0.2128]],

        [[0.0124, 0.1053],
         [0.2966, 0.4824]],

        [[0.0686, 0.2359],
         [0.0533, 0.1229]],

        [[0.0144, 0.1727],
         [0.6169, 0.9759]],

        [[0.9741, 0.3059],
         [0.6696, 0.9865]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
Global Adjusted Rand Index: -0.0018433012433402263
Average Adjusted Rand Index: 0.0018060494616241745
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27611.6796875
Iteration 100: Loss = -19776.90234375
Iteration 200: Loss = -14229.7958984375
Iteration 300: Loss = -13005.3671875
Iteration 400: Loss = -12757.541015625
Iteration 500: Loss = -12655.421875
Iteration 600: Loss = -12601.35546875
Iteration 700: Loss = -12565.36328125
Iteration 800: Loss = -12532.0390625
Iteration 900: Loss = -12511.1728515625
Iteration 1000: Loss = -12497.296875
Iteration 1100: Loss = -12483.1064453125
Iteration 1200: Loss = -12471.4931640625
Iteration 1300: Loss = -12461.5341796875
Iteration 1400: Loss = -12450.90234375
Iteration 1500: Loss = -12441.140625
Iteration 1600: Loss = -12433.892578125
Iteration 1700: Loss = -12429.3154296875
Iteration 1800: Loss = -12424.671875
Iteration 1900: Loss = -12421.9716796875
Iteration 2000: Loss = -12419.8115234375
Iteration 2100: Loss = -12417.8740234375
Iteration 2200: Loss = -12413.8935546875
Iteration 2300: Loss = -12409.8154296875
Iteration 2400: Loss = -12406.0458984375
Iteration 2500: Loss = -12403.3359375
Iteration 2600: Loss = -12401.0478515625
Iteration 2700: Loss = -12399.3486328125
Iteration 2800: Loss = -12397.1962890625
Iteration 2900: Loss = -12395.5478515625
Iteration 3000: Loss = -12394.611328125
Iteration 3100: Loss = -12393.8994140625
Iteration 3200: Loss = -12393.08203125
Iteration 3300: Loss = -12389.8828125
Iteration 3400: Loss = -12388.619140625
Iteration 3500: Loss = -12388.0888671875
Iteration 3600: Loss = -12387.693359375
Iteration 3700: Loss = -12387.365234375
Iteration 3800: Loss = -12387.080078125
Iteration 3900: Loss = -12386.8271484375
Iteration 4000: Loss = -12386.5986328125
Iteration 4100: Loss = -12386.392578125
Iteration 4200: Loss = -12386.2001953125
Iteration 4300: Loss = -12386.0234375
Iteration 4400: Loss = -12385.861328125
Iteration 4500: Loss = -12385.6865234375
Iteration 4600: Loss = -12381.19140625
Iteration 4700: Loss = -12380.8984375
Iteration 4800: Loss = -12380.63671875
Iteration 4900: Loss = -12378.921875
Iteration 5000: Loss = -12377.240234375
Iteration 5100: Loss = -12376.84375
Iteration 5200: Loss = -12376.60546875
Iteration 5300: Loss = -12376.427734375
Iteration 5400: Loss = -12376.2841796875
Iteration 5500: Loss = -12376.162109375
Iteration 5600: Loss = -12376.0556640625
Iteration 5700: Loss = -12375.9580078125
Iteration 5800: Loss = -12375.873046875
Iteration 5900: Loss = -12375.794921875
Iteration 6000: Loss = -12375.7236328125
Iteration 6100: Loss = -12375.6572265625
Iteration 6200: Loss = -12375.5966796875
Iteration 6300: Loss = -12375.541015625
Iteration 6400: Loss = -12375.4873046875
Iteration 6500: Loss = -12375.439453125
Iteration 6600: Loss = -12375.3935546875
Iteration 6700: Loss = -12375.3505859375
Iteration 6800: Loss = -12375.310546875
Iteration 6900: Loss = -12375.2734375
Iteration 7000: Loss = -12375.2392578125
Iteration 7100: Loss = -12375.2060546875
Iteration 7200: Loss = -12375.17578125
Iteration 7300: Loss = -12375.146484375
Iteration 7400: Loss = -12375.12109375
Iteration 7500: Loss = -12375.0947265625
Iteration 7600: Loss = -12375.0703125
Iteration 7700: Loss = -12375.0478515625
Iteration 7800: Loss = -12375.0263671875
Iteration 7900: Loss = -12375.0068359375
Iteration 8000: Loss = -12374.98828125
Iteration 8100: Loss = -12374.9677734375
Iteration 8200: Loss = -12374.94921875
Iteration 8300: Loss = -12374.93359375
Iteration 8400: Loss = -12374.9169921875
Iteration 8500: Loss = -12374.8955078125
Iteration 8600: Loss = -12374.7724609375
Iteration 8700: Loss = -12374.578125
Iteration 8800: Loss = -12374.5546875
Iteration 8900: Loss = -12374.5380859375
Iteration 9000: Loss = -12374.5234375
Iteration 9100: Loss = -12374.5078125
Iteration 9200: Loss = -12374.482421875
Iteration 9300: Loss = -12374.46875
Iteration 9400: Loss = -12374.458984375
Iteration 9500: Loss = -12374.4501953125
Iteration 9600: Loss = -12374.44140625
Iteration 9700: Loss = -12374.4345703125
Iteration 9800: Loss = -12374.4267578125
Iteration 9900: Loss = -12374.419921875
Iteration 10000: Loss = -12374.4130859375
Iteration 10100: Loss = -12374.4072265625
Iteration 10200: Loss = -12374.4013671875
Iteration 10300: Loss = -12374.396484375
Iteration 10400: Loss = -12374.390625
Iteration 10500: Loss = -12374.384765625
Iteration 10600: Loss = -12374.380859375
Iteration 10700: Loss = -12374.375
Iteration 10800: Loss = -12374.3720703125
Iteration 10900: Loss = -12374.3662109375
Iteration 11000: Loss = -12374.365234375
Iteration 11100: Loss = -12374.3583984375
Iteration 11200: Loss = -12374.35546875
Iteration 11300: Loss = -12374.353515625
Iteration 11400: Loss = -12374.3505859375
Iteration 11500: Loss = -12374.345703125
Iteration 11600: Loss = -12374.34375
Iteration 11700: Loss = -12374.341796875
Iteration 11800: Loss = -12374.33984375
Iteration 11900: Loss = -12374.3369140625
Iteration 12000: Loss = -12374.3349609375
Iteration 12100: Loss = -12374.33203125
Iteration 12200: Loss = -12374.330078125
Iteration 12300: Loss = -12374.326171875
Iteration 12400: Loss = -12374.32421875
Iteration 12500: Loss = -12374.32421875
Iteration 12600: Loss = -12374.3212890625
Iteration 12700: Loss = -12374.3212890625
Iteration 12800: Loss = -12374.318359375
Iteration 12900: Loss = -12374.3193359375
1
Iteration 13000: Loss = -12374.31640625
Iteration 13100: Loss = -12374.314453125
Iteration 13200: Loss = -12374.31640625
1
Iteration 13300: Loss = -12374.3115234375
Iteration 13400: Loss = -12374.3115234375
Iteration 13500: Loss = -12374.3095703125
Iteration 13600: Loss = -12374.30859375
Iteration 13700: Loss = -12374.3076171875
Iteration 13800: Loss = -12374.3076171875
Iteration 13900: Loss = -12374.3076171875
Iteration 14000: Loss = -12374.3046875
Iteration 14100: Loss = -12374.3056640625
1
Iteration 14200: Loss = -12374.302734375
Iteration 14300: Loss = -12374.302734375
Iteration 14400: Loss = -12374.302734375
Iteration 14500: Loss = -12374.3017578125
Iteration 14600: Loss = -12374.30078125
Iteration 14700: Loss = -12374.2998046875
Iteration 14800: Loss = -12374.2998046875
Iteration 14900: Loss = -12374.2978515625
Iteration 15000: Loss = -12374.298828125
1
Iteration 15100: Loss = -12374.298828125
2
Iteration 15200: Loss = -12374.2978515625
Iteration 15300: Loss = -12374.2958984375
Iteration 15400: Loss = -12374.296875
1
Iteration 15500: Loss = -12374.2958984375
Iteration 15600: Loss = -12374.2939453125
Iteration 15700: Loss = -12374.29296875
Iteration 15800: Loss = -12374.29296875
Iteration 15900: Loss = -12374.29296875
Iteration 16000: Loss = -12374.2919921875
Iteration 16100: Loss = -12374.291015625
Iteration 16200: Loss = -12374.291015625
Iteration 16300: Loss = -12374.291015625
Iteration 16400: Loss = -12374.2890625
Iteration 16500: Loss = -12374.2919921875
1
Iteration 16600: Loss = -12374.291015625
2
Iteration 16700: Loss = -12374.2890625
Iteration 16800: Loss = -12374.2900390625
1
Iteration 16900: Loss = -12374.2890625
Iteration 17000: Loss = -12374.2890625
Iteration 17100: Loss = -12374.2890625
Iteration 17200: Loss = -12374.2880859375
Iteration 17300: Loss = -12374.2880859375
Iteration 17400: Loss = -12374.2890625
1
Iteration 17500: Loss = -12374.2890625
2
Iteration 17600: Loss = -12374.2880859375
Iteration 17700: Loss = -12374.2880859375
Iteration 17800: Loss = -12374.287109375
Iteration 17900: Loss = -12374.287109375
Iteration 18000: Loss = -12374.2880859375
1
Iteration 18100: Loss = -12374.2880859375
2
Iteration 18200: Loss = -12374.287109375
Iteration 18300: Loss = -12374.2890625
1
Iteration 18400: Loss = -12374.287109375
Iteration 18500: Loss = -12374.2861328125
Iteration 18600: Loss = -12374.2880859375
1
Iteration 18700: Loss = -12374.2861328125
Iteration 18800: Loss = -12374.2861328125
Iteration 18900: Loss = -12374.2861328125
Iteration 19000: Loss = -12374.28515625
Iteration 19100: Loss = -12374.2861328125
1
Iteration 19200: Loss = -12374.2861328125
2
Iteration 19300: Loss = -12374.2861328125
3
Iteration 19400: Loss = -12374.2861328125
4
Iteration 19500: Loss = -12374.28515625
Iteration 19600: Loss = -12374.287109375
1
Iteration 19700: Loss = -12374.283203125
Iteration 19800: Loss = -12374.28515625
1
Iteration 19900: Loss = -12374.28515625
2
Iteration 20000: Loss = -12374.2841796875
3
Iteration 20100: Loss = -12374.2841796875
4
Iteration 20200: Loss = -12374.283203125
Iteration 20300: Loss = -12374.2841796875
1
Iteration 20400: Loss = -12374.2841796875
2
Iteration 20500: Loss = -12374.28515625
3
Iteration 20600: Loss = -12374.2841796875
4
Iteration 20700: Loss = -12374.2841796875
5
Iteration 20800: Loss = -12374.28515625
6
Iteration 20900: Loss = -12374.28515625
7
Iteration 21000: Loss = -12374.283203125
Iteration 21100: Loss = -12374.283203125
Iteration 21200: Loss = -12374.283203125
Iteration 21300: Loss = -12374.2841796875
1
Iteration 21400: Loss = -12374.2841796875
2
Iteration 21500: Loss = -12374.283203125
Iteration 21600: Loss = -12374.2841796875
1
Iteration 21700: Loss = -12374.283203125
Iteration 21800: Loss = -12374.283203125
Iteration 21900: Loss = -12374.2841796875
1
Iteration 22000: Loss = -12374.2841796875
2
Iteration 22100: Loss = -12374.28515625
3
Iteration 22200: Loss = -12374.2841796875
4
Iteration 22300: Loss = -12374.28515625
5
Iteration 22400: Loss = -12374.283203125
Iteration 22500: Loss = -12374.2841796875
1
Iteration 22600: Loss = -12374.283203125
Iteration 22700: Loss = -12374.283203125
Iteration 22800: Loss = -12374.283203125
Iteration 22900: Loss = -12374.2841796875
1
Iteration 23000: Loss = -12374.2841796875
2
Iteration 23100: Loss = -12374.283203125
Iteration 23200: Loss = -12374.283203125
Iteration 23300: Loss = -12374.2841796875
1
Iteration 23400: Loss = -12374.28515625
2
Iteration 23500: Loss = -12374.2841796875
3
Iteration 23600: Loss = -12374.28515625
4
Iteration 23700: Loss = -12374.283203125
Iteration 23800: Loss = -12374.2841796875
1
Iteration 23900: Loss = -12374.28515625
2
Iteration 24000: Loss = -12374.2841796875
3
Iteration 24100: Loss = -12374.283203125
Iteration 24200: Loss = -12374.2841796875
1
Iteration 24300: Loss = -12374.28515625
2
Iteration 24400: Loss = -12374.2841796875
3
Iteration 24500: Loss = -12374.2841796875
4
Iteration 24600: Loss = -12374.2841796875
5
Iteration 24700: Loss = -12374.28515625
6
Iteration 24800: Loss = -12374.2841796875
7
Iteration 24900: Loss = -12374.28515625
8
Iteration 25000: Loss = -12374.283203125
Iteration 25100: Loss = -12374.2841796875
1
Iteration 25200: Loss = -12374.2841796875
2
Iteration 25300: Loss = -12374.2841796875
3
Iteration 25400: Loss = -12374.2841796875
4
Iteration 25500: Loss = -12374.2841796875
5
Iteration 25600: Loss = -12374.2841796875
6
Iteration 25700: Loss = -12374.283203125
Iteration 25800: Loss = -12374.2841796875
1
Iteration 25900: Loss = -12374.283203125
Iteration 26000: Loss = -12374.28515625
1
Iteration 26100: Loss = -12374.2841796875
2
Iteration 26200: Loss = -12374.2841796875
3
Iteration 26300: Loss = -12374.2841796875
4
Iteration 26400: Loss = -12374.283203125
Iteration 26500: Loss = -12374.283203125
Iteration 26600: Loss = -12374.2841796875
1
Iteration 26700: Loss = -12374.2841796875
2
Iteration 26800: Loss = -12374.2841796875
3
Iteration 26900: Loss = -12374.283203125
Iteration 27000: Loss = -12374.283203125
Iteration 27100: Loss = -12374.283203125
Iteration 27200: Loss = -12374.2841796875
1
Iteration 27300: Loss = -12374.2841796875
2
Iteration 27400: Loss = -12374.283203125
Iteration 27500: Loss = -12374.2841796875
1
Iteration 27600: Loss = -12374.28515625
2
Iteration 27700: Loss = -12374.28515625
3
Iteration 27800: Loss = -12374.2841796875
4
Iteration 27900: Loss = -12374.28515625
5
Iteration 28000: Loss = -12374.283203125
Iteration 28100: Loss = -12374.2841796875
1
Iteration 28200: Loss = -12374.283203125
Iteration 28300: Loss = -12374.2822265625
Iteration 28400: Loss = -12374.1796875
Iteration 28500: Loss = -12374.0810546875
Iteration 28600: Loss = -12373.849609375
Iteration 28700: Loss = -12373.84765625
Iteration 28800: Loss = -12373.845703125
Iteration 28900: Loss = -12373.845703125
Iteration 29000: Loss = -12373.8447265625
Iteration 29100: Loss = -12373.8466796875
1
Iteration 29200: Loss = -12373.84375
Iteration 29300: Loss = -12373.845703125
1
Iteration 29400: Loss = -12373.845703125
2
Iteration 29500: Loss = -12373.8486328125
3
Iteration 29600: Loss = -12373.845703125
4
Iteration 29700: Loss = -12373.845703125
5
Iteration 29800: Loss = -12373.845703125
6
Iteration 29900: Loss = -12373.845703125
7
pi: tensor([[0.0409, 0.9591],
        [0.0114, 0.9886]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 2.9827e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2128, 0.2146],
         [0.1297, 0.1976]],

        [[0.0069, 0.1061],
         [0.1190, 0.1589]],

        [[0.9098, 0.2398],
         [0.0497, 0.0518]],

        [[0.9743, 0.1690],
         [0.0823, 0.9641]],

        [[0.9832, 0.3266],
         [0.8439, 0.3043]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
Global Adjusted Rand Index: -0.001990945394170742
Average Adjusted Rand Index: 0.0016315980465453847
[-0.0018433012433402263, -0.001990945394170742] [0.0018060494616241745, 0.0016315980465453847] [12373.3779296875, 12373.8447265625]
-------------------------------------
This iteration is 56
True Objective function: Loss = -11752.281064037514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -38944.4609375
Iteration 100: Loss = -22419.396484375
Iteration 200: Loss = -14302.298828125
Iteration 300: Loss = -12673.6953125
Iteration 400: Loss = -12468.4140625
Iteration 500: Loss = -12393.9296875
Iteration 600: Loss = -12358.283203125
Iteration 700: Loss = -12339.46484375
Iteration 800: Loss = -12323.3330078125
Iteration 900: Loss = -12312.0712890625
Iteration 1000: Loss = -12306.482421875
Iteration 1100: Loss = -12302.853515625
Iteration 1200: Loss = -12300.0390625
Iteration 1300: Loss = -12297.763671875
Iteration 1400: Loss = -12295.8427734375
Iteration 1500: Loss = -12294.275390625
Iteration 1600: Loss = -12292.9501953125
Iteration 1700: Loss = -12291.8193359375
Iteration 1800: Loss = -12290.8466796875
Iteration 1900: Loss = -12290.00390625
Iteration 2000: Loss = -12289.265625
Iteration 2100: Loss = -12288.6220703125
Iteration 2200: Loss = -12288.0517578125
Iteration 2300: Loss = -12287.5439453125
Iteration 2400: Loss = -12287.0986328125
Iteration 2500: Loss = -12286.71484375
Iteration 2600: Loss = -12286.380859375
Iteration 2700: Loss = -12286.0869140625
Iteration 2800: Loss = -12285.8291015625
Iteration 2900: Loss = -12285.603515625
Iteration 3000: Loss = -12285.3984375
Iteration 3100: Loss = -12285.216796875
Iteration 3200: Loss = -12285.0556640625
Iteration 3300: Loss = -12284.91015625
Iteration 3400: Loss = -12284.779296875
Iteration 3500: Loss = -12284.66015625
Iteration 3600: Loss = -12284.5517578125
Iteration 3700: Loss = -12284.4521484375
Iteration 3800: Loss = -12284.36328125
Iteration 3900: Loss = -12284.28125
Iteration 4000: Loss = -12284.203125
Iteration 4100: Loss = -12284.1337890625
Iteration 4200: Loss = -12284.0673828125
Iteration 4300: Loss = -12284.0048828125
Iteration 4400: Loss = -12283.9501953125
Iteration 4500: Loss = -12283.896484375
Iteration 4600: Loss = -12283.84765625
Iteration 4700: Loss = -12283.8017578125
Iteration 4800: Loss = -12283.7587890625
Iteration 4900: Loss = -12283.71875
Iteration 5000: Loss = -12283.677734375
Iteration 5100: Loss = -12283.6435546875
Iteration 5200: Loss = -12283.607421875
Iteration 5300: Loss = -12283.5771484375
Iteration 5400: Loss = -12283.544921875
Iteration 5500: Loss = -12283.5185546875
Iteration 5600: Loss = -12283.4912109375
Iteration 5700: Loss = -12283.4638671875
Iteration 5800: Loss = -12283.439453125
Iteration 5900: Loss = -12283.416015625
Iteration 6000: Loss = -12283.392578125
Iteration 6100: Loss = -12283.373046875
Iteration 6200: Loss = -12283.353515625
Iteration 6300: Loss = -12283.3369140625
Iteration 6400: Loss = -12283.318359375
Iteration 6500: Loss = -12283.30078125
Iteration 6600: Loss = -12283.28515625
Iteration 6700: Loss = -12283.2705078125
Iteration 6800: Loss = -12283.2548828125
Iteration 6900: Loss = -12283.240234375
Iteration 7000: Loss = -12283.228515625
Iteration 7100: Loss = -12283.2158203125
Iteration 7200: Loss = -12283.2041015625
Iteration 7300: Loss = -12283.193359375
Iteration 7400: Loss = -12283.1826171875
Iteration 7500: Loss = -12283.169921875
Iteration 7600: Loss = -12283.162109375
Iteration 7700: Loss = -12283.15234375
Iteration 7800: Loss = -12283.142578125
Iteration 7900: Loss = -12283.134765625
Iteration 8000: Loss = -12283.1259765625
Iteration 8100: Loss = -12283.1181640625
Iteration 8200: Loss = -12283.1103515625
Iteration 8300: Loss = -12283.1025390625
Iteration 8400: Loss = -12283.0966796875
Iteration 8500: Loss = -12283.087890625
Iteration 8600: Loss = -12283.0791015625
Iteration 8700: Loss = -12283.07421875
Iteration 8800: Loss = -12283.0673828125
Iteration 8900: Loss = -12283.060546875
Iteration 9000: Loss = -12283.0537109375
Iteration 9100: Loss = -12283.0478515625
Iteration 9200: Loss = -12283.0380859375
Iteration 9300: Loss = -12283.033203125
Iteration 9400: Loss = -12283.0244140625
Iteration 9500: Loss = -12283.017578125
Iteration 9600: Loss = -12283.009765625
Iteration 9700: Loss = -12283.001953125
Iteration 9800: Loss = -12282.994140625
Iteration 9900: Loss = -12282.986328125
Iteration 10000: Loss = -12282.9775390625
Iteration 10100: Loss = -12282.9697265625
Iteration 10200: Loss = -12282.9609375
Iteration 10300: Loss = -12282.953125
Iteration 10400: Loss = -12282.9462890625
Iteration 10500: Loss = -12282.935546875
Iteration 10600: Loss = -12282.9287109375
Iteration 10700: Loss = -12282.9189453125
Iteration 10800: Loss = -12282.91015625
Iteration 10900: Loss = -12282.900390625
Iteration 11000: Loss = -12282.890625
Iteration 11100: Loss = -12282.87890625
Iteration 11200: Loss = -12282.8681640625
Iteration 11300: Loss = -12282.85546875
Iteration 11400: Loss = -12282.8369140625
Iteration 11500: Loss = -12282.80859375
Iteration 11600: Loss = -12282.7568359375
Iteration 11700: Loss = -12282.6865234375
Iteration 11800: Loss = -12282.6416015625
Iteration 11900: Loss = -12282.6005859375
Iteration 12000: Loss = -12282.5625
Iteration 12100: Loss = -12282.5224609375
Iteration 12200: Loss = -12282.478515625
Iteration 12300: Loss = -12282.423828125
Iteration 12400: Loss = -12282.3544921875
Iteration 12500: Loss = -12282.2626953125
Iteration 12600: Loss = -12282.126953125
Iteration 12700: Loss = -12281.9765625
Iteration 12800: Loss = -12281.8369140625
Iteration 12900: Loss = -12281.71875
Iteration 13000: Loss = -12281.6259765625
Iteration 13100: Loss = -12281.55859375
Iteration 13200: Loss = -12281.5107421875
Iteration 13300: Loss = -12281.4755859375
Iteration 13400: Loss = -12281.44921875
Iteration 13500: Loss = -12281.423828125
Iteration 13600: Loss = -12281.41015625
Iteration 13700: Loss = -12281.3984375
Iteration 13800: Loss = -12281.3935546875
Iteration 13900: Loss = -12281.38671875
Iteration 14000: Loss = -12281.380859375
Iteration 14100: Loss = -12281.376953125
Iteration 14200: Loss = -12281.3740234375
Iteration 14300: Loss = -12281.37109375
Iteration 14400: Loss = -12281.3681640625
Iteration 14500: Loss = -12281.3671875
Iteration 14600: Loss = -12281.3642578125
Iteration 14700: Loss = -12281.3623046875
Iteration 14800: Loss = -12281.3623046875
Iteration 14900: Loss = -12281.361328125
Iteration 15000: Loss = -12281.361328125
Iteration 15100: Loss = -12281.359375
Iteration 15200: Loss = -12281.3583984375
Iteration 15300: Loss = -12281.3603515625
1
Iteration 15400: Loss = -12281.357421875
Iteration 15500: Loss = -12281.3583984375
1
Iteration 15600: Loss = -12281.3583984375
2
Iteration 15700: Loss = -12281.357421875
Iteration 15800: Loss = -12281.3583984375
1
Iteration 15900: Loss = -12281.3583984375
2
Iteration 16000: Loss = -12281.3564453125
Iteration 16100: Loss = -12281.3564453125
Iteration 16200: Loss = -12281.35546875
Iteration 16300: Loss = -12281.3564453125
1
Iteration 16400: Loss = -12281.3583984375
2
Iteration 16500: Loss = -12281.35546875
Iteration 16600: Loss = -12281.3544921875
Iteration 16700: Loss = -12281.35546875
1
Iteration 16800: Loss = -12281.3564453125
2
Iteration 16900: Loss = -12281.35546875
3
Iteration 17000: Loss = -12281.35546875
4
Iteration 17100: Loss = -12281.35546875
5
Iteration 17200: Loss = -12281.3544921875
Iteration 17300: Loss = -12281.35546875
1
Iteration 17400: Loss = -12281.3544921875
Iteration 17500: Loss = -12281.35546875
1
Iteration 17600: Loss = -12281.353515625
Iteration 17700: Loss = -12281.35546875
1
Iteration 17800: Loss = -12281.35546875
2
Iteration 17900: Loss = -12281.353515625
Iteration 18000: Loss = -12281.353515625
Iteration 18100: Loss = -12281.3544921875
1
Iteration 18200: Loss = -12281.3544921875
2
Iteration 18300: Loss = -12281.3544921875
3
Iteration 18400: Loss = -12281.353515625
Iteration 18500: Loss = -12281.353515625
Iteration 18600: Loss = -12281.353515625
Iteration 18700: Loss = -12281.353515625
Iteration 18800: Loss = -12281.3525390625
Iteration 18900: Loss = -12281.35546875
1
Iteration 19000: Loss = -12281.353515625
2
Iteration 19100: Loss = -12281.353515625
3
Iteration 19200: Loss = -12281.353515625
4
Iteration 19300: Loss = -12281.353515625
5
Iteration 19400: Loss = -12281.3525390625
Iteration 19500: Loss = -12281.353515625
1
Iteration 19600: Loss = -12281.3544921875
2
Iteration 19700: Loss = -12281.35546875
3
Iteration 19800: Loss = -12281.3525390625
Iteration 19900: Loss = -12281.353515625
1
Iteration 20000: Loss = -12281.353515625
2
Iteration 20100: Loss = -12281.35546875
3
Iteration 20200: Loss = -12281.3544921875
4
Iteration 20300: Loss = -12281.3525390625
Iteration 20400: Loss = -12281.3544921875
1
Iteration 20500: Loss = -12281.3525390625
Iteration 20600: Loss = -12281.3525390625
Iteration 20700: Loss = -12281.3525390625
Iteration 20800: Loss = -12281.353515625
1
Iteration 20900: Loss = -12281.3515625
Iteration 21000: Loss = -12281.3525390625
1
Iteration 21100: Loss = -12281.3515625
Iteration 21200: Loss = -12281.3525390625
1
Iteration 21300: Loss = -12281.3525390625
2
Iteration 21400: Loss = -12281.353515625
3
Iteration 21500: Loss = -12281.3525390625
4
Iteration 21600: Loss = -12281.353515625
5
Iteration 21700: Loss = -12281.353515625
6
Iteration 21800: Loss = -12281.353515625
7
Iteration 21900: Loss = -12281.3515625
Iteration 22000: Loss = -12281.353515625
1
Iteration 22100: Loss = -12281.353515625
2
Iteration 22200: Loss = -12281.35546875
3
Iteration 22300: Loss = -12281.3525390625
4
Iteration 22400: Loss = -12281.353515625
5
Iteration 22500: Loss = -12281.353515625
6
Iteration 22600: Loss = -12281.353515625
7
Iteration 22700: Loss = -12281.353515625
8
Iteration 22800: Loss = -12281.3525390625
9
Iteration 22900: Loss = -12281.353515625
10
Iteration 23000: Loss = -12281.3525390625
11
Iteration 23100: Loss = -12281.353515625
12
Iteration 23200: Loss = -12281.353515625
13
Iteration 23300: Loss = -12281.3515625
Iteration 23400: Loss = -12281.353515625
1
Iteration 23500: Loss = -12281.3525390625
2
Iteration 23600: Loss = -12281.3515625
Iteration 23700: Loss = -12281.353515625
1
Iteration 23800: Loss = -12281.3525390625
2
Iteration 23900: Loss = -12281.353515625
3
Iteration 24000: Loss = -12281.353515625
4
Iteration 24100: Loss = -12281.3515625
Iteration 24200: Loss = -12281.3515625
Iteration 24300: Loss = -12281.3525390625
1
Iteration 24400: Loss = -12281.353515625
2
Iteration 24500: Loss = -12281.353515625
3
Iteration 24600: Loss = -12281.3525390625
4
Iteration 24700: Loss = -12281.3525390625
5
Iteration 24800: Loss = -12281.3515625
Iteration 24900: Loss = -12281.3525390625
1
Iteration 25000: Loss = -12281.3515625
Iteration 25100: Loss = -12281.353515625
1
Iteration 25200: Loss = -12281.3515625
Iteration 25300: Loss = -12281.3515625
Iteration 25400: Loss = -12281.3515625
Iteration 25500: Loss = -12281.353515625
1
Iteration 25600: Loss = -12281.3515625
Iteration 25700: Loss = -12281.3525390625
1
Iteration 25800: Loss = -12281.3515625
Iteration 25900: Loss = -12281.3525390625
1
Iteration 26000: Loss = -12281.353515625
2
Iteration 26100: Loss = -12281.3525390625
3
Iteration 26200: Loss = -12281.3515625
Iteration 26300: Loss = -12281.353515625
1
Iteration 26400: Loss = -12281.353515625
2
Iteration 26500: Loss = -12281.3525390625
3
Iteration 26600: Loss = -12281.353515625
4
Iteration 26700: Loss = -12281.353515625
5
Iteration 26800: Loss = -12281.353515625
6
Iteration 26900: Loss = -12281.3515625
Iteration 27000: Loss = -12281.3525390625
1
Iteration 27100: Loss = -12281.353515625
2
Iteration 27200: Loss = -12281.3525390625
3
Iteration 27300: Loss = -12281.3525390625
4
Iteration 27400: Loss = -12281.3515625
Iteration 27500: Loss = -12281.353515625
1
Iteration 27600: Loss = -12281.3515625
Iteration 27700: Loss = -12281.353515625
1
Iteration 27800: Loss = -12281.353515625
2
Iteration 27900: Loss = -12281.3515625
Iteration 28000: Loss = -12281.3515625
Iteration 28100: Loss = -12281.3505859375
Iteration 28200: Loss = -12281.3515625
1
Iteration 28300: Loss = -12281.353515625
2
Iteration 28400: Loss = -12281.3525390625
3
Iteration 28500: Loss = -12281.3525390625
4
Iteration 28600: Loss = -12281.3525390625
5
Iteration 28700: Loss = -12281.3525390625
6
Iteration 28800: Loss = -12281.3525390625
7
Iteration 28900: Loss = -12281.353515625
8
Iteration 29000: Loss = -12281.3515625
9
Iteration 29100: Loss = -12281.3544921875
10
Iteration 29200: Loss = -12281.353515625
11
Iteration 29300: Loss = -12281.3525390625
12
Iteration 29400: Loss = -12281.3515625
13
Iteration 29500: Loss = -12281.3525390625
14
Iteration 29600: Loss = -12281.353515625
15
Stopping early at iteration 29600 due to no improvement.
pi: tensor([[5.0820e-06, 9.9999e-01],
        [4.0554e-02, 9.5945e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0128, 0.9872], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2575, 0.1102],
         [0.0084, 0.1960]],

        [[0.5926, 0.2409],
         [0.7689, 0.3572]],

        [[0.0084, 0.2536],
         [0.9167, 0.0533]],

        [[0.8685, 0.2317],
         [0.8821, 0.6853]],

        [[0.0092, 0.1388],
         [0.0118, 0.8377]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0002779938456853471
Average Adjusted Rand Index: -0.00015692302765368048
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -57898.34375
Iteration 100: Loss = -34911.11328125
Iteration 200: Loss = -18232.708984375
Iteration 300: Loss = -14042.037109375
Iteration 400: Loss = -13009.5009765625
Iteration 500: Loss = -12697.07421875
Iteration 600: Loss = -12579.8701171875
Iteration 700: Loss = -12508.830078125
Iteration 800: Loss = -12458.669921875
Iteration 900: Loss = -12426.2587890625
Iteration 1000: Loss = -12402.353515625
Iteration 1100: Loss = -12383.111328125
Iteration 1200: Loss = -12370.2841796875
Iteration 1300: Loss = -12360.3984375
Iteration 1400: Loss = -12352.5625
Iteration 1500: Loss = -12346.236328125
Iteration 1600: Loss = -12341.044921875
Iteration 1700: Loss = -12336.71484375
Iteration 1800: Loss = -12333.0546875
Iteration 1900: Loss = -12329.9404296875
Iteration 2000: Loss = -12327.2568359375
Iteration 2100: Loss = -12324.9296875
Iteration 2200: Loss = -12322.8955078125
Iteration 2300: Loss = -12321.1044921875
Iteration 2400: Loss = -12319.5224609375
Iteration 2500: Loss = -12318.11328125
Iteration 2600: Loss = -12316.8525390625
Iteration 2700: Loss = -12315.72265625
Iteration 2800: Loss = -12314.7041015625
Iteration 2900: Loss = -12313.7822265625
Iteration 3000: Loss = -12312.9462890625
Iteration 3100: Loss = -12312.1826171875
Iteration 3200: Loss = -12311.4853515625
Iteration 3300: Loss = -12310.84765625
Iteration 3400: Loss = -12310.26171875
Iteration 3500: Loss = -12309.7216796875
Iteration 3600: Loss = -12309.224609375
Iteration 3700: Loss = -12308.7626953125
Iteration 3800: Loss = -12308.333984375
Iteration 3900: Loss = -12307.935546875
Iteration 4000: Loss = -12307.5654296875
Iteration 4100: Loss = -12307.21875
Iteration 4200: Loss = -12306.8935546875
Iteration 4300: Loss = -12306.5888671875
Iteration 4400: Loss = -12306.302734375
Iteration 4500: Loss = -12306.033203125
Iteration 4600: Loss = -12305.7783203125
Iteration 4700: Loss = -12305.5380859375
Iteration 4800: Loss = -12305.310546875
Iteration 4900: Loss = -12305.0927734375
Iteration 5000: Loss = -12304.88671875
Iteration 5100: Loss = -12304.6904296875
Iteration 5200: Loss = -12304.5009765625
Iteration 5300: Loss = -12304.3212890625
Iteration 5400: Loss = -12304.14453125
Iteration 5500: Loss = -12303.9765625
Iteration 5600: Loss = -12303.5791015625
Iteration 5700: Loss = -12298.0625
Iteration 5800: Loss = -12297.806640625
Iteration 5900: Loss = -12297.6650390625
Iteration 6000: Loss = -12297.546875
Iteration 6100: Loss = -12297.44140625
Iteration 6200: Loss = -12297.3291015625
Iteration 6300: Loss = -12297.236328125
Iteration 6400: Loss = -12297.1474609375
Iteration 6500: Loss = -12297.0595703125
Iteration 6600: Loss = -12296.982421875
Iteration 6700: Loss = -12296.90234375
Iteration 6800: Loss = -12296.8232421875
Iteration 6900: Loss = -12296.748046875
Iteration 7000: Loss = -12296.67578125
Iteration 7100: Loss = -12296.6015625
Iteration 7200: Loss = -12296.529296875
Iteration 7300: Loss = -12296.458984375
Iteration 7400: Loss = -12296.392578125
Iteration 7500: Loss = -12296.322265625
Iteration 7600: Loss = -12296.255859375
Iteration 7700: Loss = -12296.1865234375
Iteration 7800: Loss = -12296.119140625
Iteration 7900: Loss = -12296.0517578125
Iteration 8000: Loss = -12295.9833984375
Iteration 8100: Loss = -12295.9140625
Iteration 8200: Loss = -12295.84765625
Iteration 8300: Loss = -12295.7783203125
Iteration 8400: Loss = -12295.6962890625
Iteration 8500: Loss = -12289.7763671875
Iteration 8600: Loss = -12289.6455078125
Iteration 8700: Loss = -12289.6025390625
Iteration 8800: Loss = -12289.5703125
Iteration 8900: Loss = -12289.546875
Iteration 9000: Loss = -12289.5244140625
Iteration 9100: Loss = -12289.50390625
Iteration 9200: Loss = -12289.482421875
Iteration 9300: Loss = -12289.451171875
Iteration 9400: Loss = -12289.42578125
Iteration 9500: Loss = -12289.408203125
Iteration 9600: Loss = -12289.392578125
Iteration 9700: Loss = -12289.37890625
Iteration 9800: Loss = -12289.3671875
Iteration 9900: Loss = -12289.35546875
Iteration 10000: Loss = -12289.34375
Iteration 10100: Loss = -12283.8818359375
Iteration 10200: Loss = -12283.51171875
Iteration 10300: Loss = -12283.41796875
Iteration 10400: Loss = -12283.3681640625
Iteration 10500: Loss = -12283.333984375
Iteration 10600: Loss = -12283.310546875
Iteration 10700: Loss = -12283.291015625
Iteration 10800: Loss = -12283.2724609375
Iteration 10900: Loss = -12283.259765625
Iteration 11000: Loss = -12283.24609375
Iteration 11100: Loss = -12283.236328125
Iteration 11200: Loss = -12283.2255859375
Iteration 11300: Loss = -12283.216796875
Iteration 11400: Loss = -12283.20703125
Iteration 11500: Loss = -12283.2001953125
Iteration 11600: Loss = -12283.193359375
Iteration 11700: Loss = -12283.1875
Iteration 11800: Loss = -12283.1796875
Iteration 11900: Loss = -12283.17578125
Iteration 12000: Loss = -12283.1708984375
Iteration 12100: Loss = -12283.1650390625
Iteration 12200: Loss = -12283.1611328125
Iteration 12300: Loss = -12283.158203125
Iteration 12400: Loss = -12283.1533203125
Iteration 12500: Loss = -12283.1494140625
Iteration 12600: Loss = -12283.146484375
Iteration 12700: Loss = -12283.1435546875
Iteration 12800: Loss = -12283.1396484375
Iteration 12900: Loss = -12283.1376953125
Iteration 13000: Loss = -12283.1328125
Iteration 13100: Loss = -12283.130859375
Iteration 13200: Loss = -12283.1298828125
Iteration 13300: Loss = -12283.126953125
Iteration 13400: Loss = -12283.1240234375
Iteration 13500: Loss = -12283.1220703125
Iteration 13600: Loss = -12283.119140625
Iteration 13700: Loss = -12283.119140625
Iteration 13800: Loss = -12283.1162109375
Iteration 13900: Loss = -12283.115234375
Iteration 14000: Loss = -12283.1142578125
Iteration 14100: Loss = -12283.111328125
Iteration 14200: Loss = -12283.1103515625
Iteration 14300: Loss = -12283.109375
Iteration 14400: Loss = -12283.1064453125
Iteration 14500: Loss = -12283.10546875
Iteration 14600: Loss = -12283.107421875
1
Iteration 14700: Loss = -12283.1044921875
Iteration 14800: Loss = -12283.103515625
Iteration 14900: Loss = -12283.103515625
Iteration 15000: Loss = -12283.1015625
Iteration 15100: Loss = -12283.1015625
Iteration 15200: Loss = -12283.099609375
Iteration 15300: Loss = -12283.0986328125
Iteration 15400: Loss = -12283.09765625
Iteration 15500: Loss = -12283.1005859375
1
Iteration 15600: Loss = -12283.095703125
Iteration 15700: Loss = -12283.095703125
Iteration 15800: Loss = -12283.095703125
Iteration 15900: Loss = -12283.09375
Iteration 16000: Loss = -12283.09375
Iteration 16100: Loss = -12283.0927734375
Iteration 16200: Loss = -12283.0927734375
Iteration 16300: Loss = -12283.091796875
Iteration 16400: Loss = -12283.091796875
Iteration 16500: Loss = -12283.0908203125
Iteration 16600: Loss = -12283.087890625
Iteration 16700: Loss = -12283.0888671875
1
Iteration 16800: Loss = -12283.087890625
Iteration 16900: Loss = -12283.0859375
Iteration 17000: Loss = -12283.0654296875
Iteration 17100: Loss = -12282.8349609375
Iteration 17200: Loss = -12282.8076171875
Iteration 17300: Loss = -12282.7763671875
Iteration 17400: Loss = -12282.7294921875
Iteration 17500: Loss = -12282.65625
Iteration 17600: Loss = -12282.5244140625
Iteration 17700: Loss = -12282.2822265625
Iteration 17800: Loss = -12282.19140625
Iteration 17900: Loss = -12282.16015625
Iteration 18000: Loss = -12282.123046875
Iteration 18100: Loss = -12282.09375
Iteration 18200: Loss = -12282.0703125
Iteration 18300: Loss = -12282.048828125
Iteration 18400: Loss = -12282.0400390625
Iteration 18500: Loss = -12282.03125
Iteration 18600: Loss = -12282.0244140625
Iteration 18700: Loss = -12282.0234375
Iteration 18800: Loss = -12282.01953125
Iteration 18900: Loss = -12282.017578125
Iteration 19000: Loss = -12282.017578125
Iteration 19100: Loss = -12282.01953125
1
Iteration 19200: Loss = -12282.017578125
Iteration 19300: Loss = -12282.017578125
Iteration 19400: Loss = -12282.0185546875
1
Iteration 19500: Loss = -12282.0166015625
Iteration 19600: Loss = -12282.017578125
1
Iteration 19700: Loss = -12282.015625
Iteration 19800: Loss = -12282.0166015625
1
Iteration 19900: Loss = -12282.017578125
2
Iteration 20000: Loss = -12282.015625
Iteration 20100: Loss = -12282.0185546875
1
Iteration 20200: Loss = -12282.0166015625
2
Iteration 20300: Loss = -12282.017578125
3
Iteration 20400: Loss = -12282.0166015625
4
Iteration 20500: Loss = -12282.01953125
5
Iteration 20600: Loss = -12282.017578125
6
Iteration 20700: Loss = -12282.0166015625
7
Iteration 20800: Loss = -12282.0166015625
8
Iteration 20900: Loss = -12282.015625
Iteration 21000: Loss = -12282.017578125
1
Iteration 21100: Loss = -12282.015625
Iteration 21200: Loss = -12282.015625
Iteration 21300: Loss = -12282.01171875
Iteration 21400: Loss = -12282.005859375
Iteration 21500: Loss = -12281.998046875
Iteration 21600: Loss = -12281.9951171875
Iteration 21700: Loss = -12281.9892578125
Iteration 21800: Loss = -12281.9873046875
Iteration 21900: Loss = -12281.984375
Iteration 22000: Loss = -12281.9755859375
Iteration 22100: Loss = -12281.97265625
Iteration 22200: Loss = -12281.96875
Iteration 22300: Loss = -12281.9560546875
Iteration 22400: Loss = -12281.953125
Iteration 22500: Loss = -12281.9462890625
Iteration 22600: Loss = -12281.9453125
Iteration 22700: Loss = -12281.9423828125
Iteration 22800: Loss = -12281.94140625
Iteration 22900: Loss = -12281.9384765625
Iteration 23000: Loss = -12281.9375
Iteration 23100: Loss = -12281.9384765625
1
Iteration 23200: Loss = -12281.9365234375
Iteration 23300: Loss = -12281.935546875
Iteration 23400: Loss = -12281.9345703125
Iteration 23500: Loss = -12281.935546875
1
Iteration 23600: Loss = -12281.935546875
2
Iteration 23700: Loss = -12281.93359375
Iteration 23800: Loss = -12281.9365234375
1
Iteration 23900: Loss = -12281.935546875
2
Iteration 24000: Loss = -12281.935546875
3
Iteration 24100: Loss = -12281.9345703125
4
Iteration 24200: Loss = -12281.9345703125
5
Iteration 24300: Loss = -12281.9345703125
6
Iteration 24400: Loss = -12281.9345703125
7
Iteration 24500: Loss = -12281.9365234375
8
Iteration 24600: Loss = -12281.935546875
9
Iteration 24700: Loss = -12281.935546875
10
Iteration 24800: Loss = -12281.935546875
11
Iteration 24900: Loss = -12281.935546875
12
Iteration 25000: Loss = -12281.9345703125
13
Iteration 25100: Loss = -12281.9365234375
14
Iteration 25200: Loss = -12281.935546875
15
Stopping early at iteration 25200 due to no improvement.
pi: tensor([[1.0000e+00, 1.7282e-06],
        [8.0375e-01, 1.9625e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6398, 0.3602], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1980, 0.1872],
         [0.9043, 0.1781]],

        [[0.9777, 0.2043],
         [0.9463, 0.9897]],

        [[0.9557, 0.2750],
         [0.0073, 0.9612]],

        [[0.2632, 0.1666],
         [0.3368, 0.3639]],

        [[0.9471, 0.1954],
         [0.4756, 0.0774]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -1.1256141347984021e-05
Average Adjusted Rand Index: -0.0001522251028708703
[-0.0002779938456853471, -1.1256141347984021e-05] [-0.00015692302765368048, -0.0001522251028708703] [12281.353515625, 12281.935546875]
-------------------------------------
This iteration is 57
True Objective function: Loss = -11850.273128394521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18769.08984375
Iteration 100: Loss = -14441.8896484375
Iteration 200: Loss = -12924.234375
Iteration 300: Loss = -12608.1357421875
Iteration 400: Loss = -12538.87109375
Iteration 500: Loss = -12512.4853515625
Iteration 600: Loss = -12498.53125
Iteration 700: Loss = -12490.2822265625
Iteration 800: Loss = -12486.5419921875
Iteration 900: Loss = -12484.19140625
Iteration 1000: Loss = -12482.5205078125
Iteration 1100: Loss = -12481.3046875
Iteration 1200: Loss = -12480.3408203125
Iteration 1300: Loss = -12479.0244140625
Iteration 1400: Loss = -12477.6552734375
Iteration 1500: Loss = -12477.1611328125
Iteration 1600: Loss = -12476.7861328125
Iteration 1700: Loss = -12476.439453125
Iteration 1800: Loss = -12476.166015625
Iteration 1900: Loss = -12475.9130859375
Iteration 2000: Loss = -12475.6826171875
Iteration 2100: Loss = -12475.46875
Iteration 2200: Loss = -12475.2705078125
Iteration 2300: Loss = -12475.08203125
Iteration 2400: Loss = -12474.91015625
Iteration 2500: Loss = -12474.74609375
Iteration 2600: Loss = -12474.5927734375
Iteration 2700: Loss = -12474.443359375
Iteration 2800: Loss = -12474.306640625
Iteration 2900: Loss = -12474.173828125
Iteration 3000: Loss = -12474.048828125
Iteration 3100: Loss = -12473.9306640625
Iteration 3200: Loss = -12473.8193359375
Iteration 3300: Loss = -12473.7099609375
Iteration 3400: Loss = -12473.6083984375
Iteration 3500: Loss = -12473.51171875
Iteration 3600: Loss = -12473.4169921875
Iteration 3700: Loss = -12473.328125
Iteration 3800: Loss = -12473.2431640625
Iteration 3900: Loss = -12473.1611328125
Iteration 4000: Loss = -12473.083984375
Iteration 4100: Loss = -12473.009765625
Iteration 4200: Loss = -12472.9365234375
Iteration 4300: Loss = -12472.8671875
Iteration 4400: Loss = -12472.8017578125
Iteration 4500: Loss = -12472.740234375
Iteration 4600: Loss = -12472.6796875
Iteration 4700: Loss = -12472.623046875
Iteration 4800: Loss = -12472.5693359375
Iteration 4900: Loss = -12472.517578125
Iteration 5000: Loss = -12472.466796875
Iteration 5100: Loss = -12472.4208984375
Iteration 5200: Loss = -12472.3740234375
Iteration 5300: Loss = -12472.3330078125
Iteration 5400: Loss = -12472.2919921875
Iteration 5500: Loss = -12472.251953125
Iteration 5600: Loss = -12472.2138671875
Iteration 5700: Loss = -12472.1796875
Iteration 5800: Loss = -12472.146484375
Iteration 5900: Loss = -12472.115234375
Iteration 6000: Loss = -12472.0849609375
Iteration 6100: Loss = -12472.05859375
Iteration 6200: Loss = -12472.0322265625
Iteration 6300: Loss = -12472.0087890625
Iteration 6400: Loss = -12471.986328125
Iteration 6500: Loss = -12471.96484375
Iteration 6600: Loss = -12471.9453125
Iteration 6700: Loss = -12471.9267578125
Iteration 6800: Loss = -12471.91015625
Iteration 6900: Loss = -12471.8935546875
Iteration 7000: Loss = -12471.876953125
Iteration 7100: Loss = -12471.8642578125
Iteration 7200: Loss = -12471.849609375
Iteration 7300: Loss = -12471.8388671875
Iteration 7400: Loss = -12471.8271484375
Iteration 7500: Loss = -12471.81640625
Iteration 7600: Loss = -12471.8056640625
Iteration 7700: Loss = -12471.796875
Iteration 7800: Loss = -12471.787109375
Iteration 7900: Loss = -12471.779296875
Iteration 8000: Loss = -12471.771484375
Iteration 8100: Loss = -12471.7646484375
Iteration 8200: Loss = -12471.7578125
Iteration 8300: Loss = -12471.7529296875
Iteration 8400: Loss = -12471.7451171875
Iteration 8500: Loss = -12471.7412109375
Iteration 8600: Loss = -12471.736328125
Iteration 8700: Loss = -12471.7314453125
Iteration 8800: Loss = -12471.7275390625
Iteration 8900: Loss = -12471.720703125
Iteration 9000: Loss = -12471.7177734375
Iteration 9100: Loss = -12471.712890625
Iteration 9200: Loss = -12471.7109375
Iteration 9300: Loss = -12471.7060546875
Iteration 9400: Loss = -12471.703125
Iteration 9500: Loss = -12471.7001953125
Iteration 9600: Loss = -12471.697265625
Iteration 9700: Loss = -12471.6943359375
Iteration 9800: Loss = -12471.693359375
Iteration 9900: Loss = -12471.69140625
Iteration 10000: Loss = -12471.6884765625
Iteration 10100: Loss = -12471.6865234375
Iteration 10200: Loss = -12471.6845703125
Iteration 10300: Loss = -12471.681640625
Iteration 10400: Loss = -12471.6806640625
Iteration 10500: Loss = -12471.6796875
Iteration 10600: Loss = -12471.6767578125
Iteration 10700: Loss = -12471.677734375
1
Iteration 10800: Loss = -12471.6748046875
Iteration 10900: Loss = -12471.6728515625
Iteration 11000: Loss = -12471.6708984375
Iteration 11100: Loss = -12471.669921875
Iteration 11200: Loss = -12471.669921875
Iteration 11300: Loss = -12471.6669921875
Iteration 11400: Loss = -12471.6669921875
Iteration 11500: Loss = -12471.666015625
Iteration 11600: Loss = -12471.6640625
Iteration 11700: Loss = -12471.6650390625
1
Iteration 11800: Loss = -12471.6630859375
Iteration 11900: Loss = -12471.6630859375
Iteration 12000: Loss = -12471.66015625
Iteration 12100: Loss = -12471.662109375
1
Iteration 12200: Loss = -12471.66015625
Iteration 12300: Loss = -12471.6591796875
Iteration 12400: Loss = -12471.66015625
1
Iteration 12500: Loss = -12471.6591796875
Iteration 12600: Loss = -12471.6591796875
Iteration 12700: Loss = -12471.658203125
Iteration 12800: Loss = -12471.65625
Iteration 12900: Loss = -12471.6572265625
1
Iteration 13000: Loss = -12471.654296875
Iteration 13100: Loss = -12471.65625
1
Iteration 13200: Loss = -12471.6552734375
2
Iteration 13300: Loss = -12471.654296875
Iteration 13400: Loss = -12471.654296875
Iteration 13500: Loss = -12471.654296875
Iteration 13600: Loss = -12471.6533203125
Iteration 13700: Loss = -12471.65234375
Iteration 13800: Loss = -12471.65234375
Iteration 13900: Loss = -12471.6533203125
1
Iteration 14000: Loss = -12471.6513671875
Iteration 14100: Loss = -12471.6533203125
1
Iteration 14200: Loss = -12471.6513671875
Iteration 14300: Loss = -12471.65234375
1
Iteration 14400: Loss = -12471.6513671875
Iteration 14500: Loss = -12471.6494140625
Iteration 14600: Loss = -12471.6513671875
1
Iteration 14700: Loss = -12471.6513671875
2
Iteration 14800: Loss = -12471.6513671875
3
Iteration 14900: Loss = -12471.6513671875
4
Iteration 15000: Loss = -12471.6494140625
Iteration 15100: Loss = -12471.6494140625
Iteration 15200: Loss = -12471.6513671875
1
Iteration 15300: Loss = -12471.650390625
2
Iteration 15400: Loss = -12471.6494140625
Iteration 15500: Loss = -12471.6484375
Iteration 15600: Loss = -12471.6494140625
1
Iteration 15700: Loss = -12471.6484375
Iteration 15800: Loss = -12471.6494140625
1
Iteration 15900: Loss = -12471.6484375
Iteration 16000: Loss = -12471.6484375
Iteration 16100: Loss = -12471.6494140625
1
Iteration 16200: Loss = -12471.6484375
Iteration 16300: Loss = -12471.6484375
Iteration 16400: Loss = -12471.6494140625
1
Iteration 16500: Loss = -12471.6484375
Iteration 16600: Loss = -12471.6484375
Iteration 16700: Loss = -12471.6474609375
Iteration 16800: Loss = -12471.6484375
1
Iteration 16900: Loss = -12471.6474609375
Iteration 17000: Loss = -12471.6494140625
1
Iteration 17100: Loss = -12471.6484375
2
Iteration 17200: Loss = -12471.6484375
3
Iteration 17300: Loss = -12471.6484375
4
Iteration 17400: Loss = -12471.6474609375
Iteration 17500: Loss = -12471.6484375
1
Iteration 17600: Loss = -12471.6474609375
Iteration 17700: Loss = -12471.6474609375
Iteration 17800: Loss = -12471.6484375
1
Iteration 17900: Loss = -12471.6474609375
Iteration 18000: Loss = -12471.6484375
1
Iteration 18100: Loss = -12471.6484375
2
Iteration 18200: Loss = -12471.6474609375
Iteration 18300: Loss = -12471.6484375
1
Iteration 18400: Loss = -12471.6474609375
Iteration 18500: Loss = -12471.6474609375
Iteration 18600: Loss = -12471.6484375
1
Iteration 18700: Loss = -12471.6474609375
Iteration 18800: Loss = -12471.6484375
1
Iteration 18900: Loss = -12471.6474609375
Iteration 19000: Loss = -12471.6484375
1
Iteration 19100: Loss = -12471.646484375
Iteration 19200: Loss = -12471.6484375
1
Iteration 19300: Loss = -12471.6484375
2
Iteration 19400: Loss = -12471.6474609375
3
Iteration 19500: Loss = -12471.646484375
Iteration 19600: Loss = -12471.6474609375
1
Iteration 19700: Loss = -12471.6474609375
2
Iteration 19800: Loss = -12471.6484375
3
Iteration 19900: Loss = -12471.6484375
4
Iteration 20000: Loss = -12471.6494140625
5
Iteration 20100: Loss = -12471.6484375
6
Iteration 20200: Loss = -12471.6474609375
7
Iteration 20300: Loss = -12471.6484375
8
Iteration 20400: Loss = -12471.6474609375
9
Iteration 20500: Loss = -12471.6484375
10
Iteration 20600: Loss = -12471.646484375
Iteration 20700: Loss = -12471.6474609375
1
Iteration 20800: Loss = -12471.6484375
2
Iteration 20900: Loss = -12471.6474609375
3
Iteration 21000: Loss = -12471.6474609375
4
Iteration 21100: Loss = -12471.6494140625
5
Iteration 21200: Loss = -12471.6474609375
6
Iteration 21300: Loss = -12471.6474609375
7
Iteration 21400: Loss = -12471.6494140625
8
Iteration 21500: Loss = -12471.6474609375
9
Iteration 21600: Loss = -12471.6484375
10
Iteration 21700: Loss = -12471.6474609375
11
Iteration 21800: Loss = -12471.650390625
12
Iteration 21900: Loss = -12471.646484375
Iteration 22000: Loss = -12471.6474609375
1
Iteration 22100: Loss = -12471.6494140625
2
Iteration 22200: Loss = -12471.6494140625
3
Iteration 22300: Loss = -12471.6494140625
4
Iteration 22400: Loss = -12471.6474609375
5
Iteration 22500: Loss = -12471.6474609375
6
Iteration 22600: Loss = -12471.6474609375
7
Iteration 22700: Loss = -12471.6494140625
8
Iteration 22800: Loss = -12471.6494140625
9
Iteration 22900: Loss = -12471.6484375
10
Iteration 23000: Loss = -12471.6494140625
11
Iteration 23100: Loss = -12471.646484375
Iteration 23200: Loss = -12471.6494140625
1
Iteration 23300: Loss = -12471.6494140625
2
Iteration 23400: Loss = -12471.6474609375
3
Iteration 23500: Loss = -12471.6494140625
4
Iteration 23600: Loss = -12471.6484375
5
Iteration 23700: Loss = -12471.6494140625
6
Iteration 23800: Loss = -12471.646484375
Iteration 23900: Loss = -12471.6474609375
1
Iteration 24000: Loss = -12471.6474609375
2
Iteration 24100: Loss = -12471.6494140625
3
Iteration 24200: Loss = -12471.6484375
4
Iteration 24300: Loss = -12471.6455078125
Iteration 24400: Loss = -12471.646484375
1
Iteration 24500: Loss = -12471.6484375
2
Iteration 24600: Loss = -12471.646484375
3
Iteration 24700: Loss = -12471.646484375
4
Iteration 24800: Loss = -12471.646484375
5
Iteration 24900: Loss = -12471.6484375
6
Iteration 25000: Loss = -12471.6484375
7
Iteration 25100: Loss = -12471.6474609375
8
Iteration 25200: Loss = -12471.6484375
9
Iteration 25300: Loss = -12471.646484375
10
Iteration 25400: Loss = -12471.646484375
11
Iteration 25500: Loss = -12471.646484375
12
Iteration 25600: Loss = -12471.646484375
13
Iteration 25700: Loss = -12471.6474609375
14
Iteration 25800: Loss = -12471.646484375
15
Stopping early at iteration 25800 due to no improvement.
pi: tensor([[1.0000e+00, 1.4475e-06],
        [2.3059e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9896, 0.0104], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2021, 0.3104],
         [0.0542, 0.2799]],

        [[0.7417, 0.1912],
         [0.0225, 0.0684]],

        [[0.0087, 0.3104],
         [0.0789, 0.9747]],

        [[0.9775, 0.2522],
         [0.0107, 0.0757]],

        [[0.9847, 0.1025],
         [0.8347, 0.4362]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: 0.0006639165226487879
Average Adjusted Rand Index: 0.0005695075040550283
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -39777.54296875
Iteration 100: Loss = -23704.837890625
Iteration 200: Loss = -14887.7021484375
Iteration 300: Loss = -13193.99609375
Iteration 400: Loss = -12928.419921875
Iteration 500: Loss = -12796.765625
Iteration 600: Loss = -12701.6982421875
Iteration 700: Loss = -12655.044921875
Iteration 800: Loss = -12619.9755859375
Iteration 900: Loss = -12582.3876953125
Iteration 1000: Loss = -12548.2353515625
Iteration 1100: Loss = -12536.4306640625
Iteration 1200: Loss = -12528.5517578125
Iteration 1300: Loss = -12522.16796875
Iteration 1400: Loss = -12513.888671875
Iteration 1500: Loss = -12509.208984375
Iteration 1600: Loss = -12505.5341796875
Iteration 1700: Loss = -12502.75
Iteration 1800: Loss = -12500.5849609375
Iteration 1900: Loss = -12498.818359375
Iteration 2000: Loss = -12497.3359375
Iteration 2100: Loss = -12496.060546875
Iteration 2200: Loss = -12494.9560546875
Iteration 2300: Loss = -12493.978515625
Iteration 2400: Loss = -12493.1142578125
Iteration 2500: Loss = -12492.3408203125
Iteration 2600: Loss = -12491.6484375
Iteration 2700: Loss = -12491.025390625
Iteration 2800: Loss = -12490.44921875
Iteration 2900: Loss = -12489.9462890625
Iteration 3000: Loss = -12489.4921875
Iteration 3100: Loss = -12489.0771484375
Iteration 3200: Loss = -12488.7001953125
Iteration 3300: Loss = -12488.353515625
Iteration 3400: Loss = -12488.0341796875
Iteration 3500: Loss = -12487.7177734375
Iteration 3600: Loss = -12487.4501953125
Iteration 3700: Loss = -12487.203125
Iteration 3800: Loss = -12486.9736328125
Iteration 3900: Loss = -12486.7626953125
Iteration 4000: Loss = -12486.5673828125
Iteration 4100: Loss = -12486.384765625
Iteration 4200: Loss = -12486.212890625
Iteration 4300: Loss = -12480.7158203125
Iteration 4400: Loss = -12480.1806640625
Iteration 4500: Loss = -12479.96484375
Iteration 4600: Loss = -12479.787109375
Iteration 4700: Loss = -12479.62890625
Iteration 4800: Loss = -12479.484375
Iteration 4900: Loss = -12479.3544921875
Iteration 5000: Loss = -12479.2353515625
Iteration 5100: Loss = -12479.123046875
Iteration 5200: Loss = -12479.0185546875
Iteration 5300: Loss = -12478.9228515625
Iteration 5400: Loss = -12478.8310546875
Iteration 5500: Loss = -12478.744140625
Iteration 5600: Loss = -12478.666015625
Iteration 5700: Loss = -12478.591796875
Iteration 5800: Loss = -12478.51953125
Iteration 5900: Loss = -12478.4541015625
Iteration 6000: Loss = -12478.3916015625
Iteration 6100: Loss = -12478.330078125
Iteration 6200: Loss = -12478.2763671875
Iteration 6300: Loss = -12478.2236328125
Iteration 6400: Loss = -12478.1748046875
Iteration 6500: Loss = -12478.126953125
Iteration 6600: Loss = -12478.0810546875
Iteration 6700: Loss = -12478.0400390625
Iteration 6800: Loss = -12478.0
Iteration 6900: Loss = -12477.9638671875
Iteration 7000: Loss = -12477.927734375
Iteration 7100: Loss = -12477.8935546875
Iteration 7200: Loss = -12477.8603515625
Iteration 7300: Loss = -12477.830078125
Iteration 7400: Loss = -12477.8017578125
Iteration 7500: Loss = -12477.7734375
Iteration 7600: Loss = -12477.7470703125
Iteration 7700: Loss = -12477.7236328125
Iteration 7800: Loss = -12477.697265625
Iteration 7900: Loss = -12477.6767578125
Iteration 8000: Loss = -12477.654296875
Iteration 8100: Loss = -12477.6337890625
Iteration 8200: Loss = -12477.615234375
Iteration 8300: Loss = -12477.5947265625
Iteration 8400: Loss = -12477.5771484375
Iteration 8500: Loss = -12477.5615234375
Iteration 8600: Loss = -12477.544921875
Iteration 8700: Loss = -12477.529296875
Iteration 8800: Loss = -12477.515625
Iteration 8900: Loss = -12477.5029296875
Iteration 9000: Loss = -12477.490234375
Iteration 9100: Loss = -12477.4775390625
Iteration 9200: Loss = -12477.46484375
Iteration 9300: Loss = -12477.455078125
Iteration 9400: Loss = -12477.4453125
Iteration 9500: Loss = -12477.435546875
Iteration 9600: Loss = -12477.427734375
Iteration 9700: Loss = -12477.4189453125
Iteration 9800: Loss = -12477.412109375
Iteration 9900: Loss = -12477.4052734375
Iteration 10000: Loss = -12477.3974609375
Iteration 10100: Loss = -12477.392578125
Iteration 10200: Loss = -12477.3837890625
Iteration 10300: Loss = -12477.37890625
Iteration 10400: Loss = -12477.373046875
Iteration 10500: Loss = -12477.3701171875
Iteration 10600: Loss = -12477.36328125
Iteration 10700: Loss = -12477.359375
Iteration 10800: Loss = -12477.353515625
Iteration 10900: Loss = -12477.349609375
Iteration 11000: Loss = -12477.3466796875
Iteration 11100: Loss = -12477.3427734375
Iteration 11200: Loss = -12477.3408203125
Iteration 11300: Loss = -12477.3369140625
Iteration 11400: Loss = -12477.3330078125
Iteration 11500: Loss = -12477.3310546875
Iteration 11600: Loss = -12477.328125
Iteration 11700: Loss = -12477.32421875
Iteration 11800: Loss = -12477.3232421875
Iteration 11900: Loss = -12477.3212890625
Iteration 12000: Loss = -12477.3193359375
Iteration 12100: Loss = -12477.31640625
Iteration 12200: Loss = -12477.314453125
Iteration 12300: Loss = -12477.3125
Iteration 12400: Loss = -12477.3115234375
Iteration 12500: Loss = -12477.310546875
Iteration 12600: Loss = -12477.30859375
Iteration 12700: Loss = -12477.306640625
Iteration 12800: Loss = -12477.3037109375
Iteration 12900: Loss = -12477.3037109375
Iteration 13000: Loss = -12477.3037109375
Iteration 13100: Loss = -12477.30078125
Iteration 13200: Loss = -12477.30078125
Iteration 13300: Loss = -12477.2978515625
Iteration 13400: Loss = -12477.296875
Iteration 13500: Loss = -12477.294921875
Iteration 13600: Loss = -12477.2958984375
1
Iteration 13700: Loss = -12477.2939453125
Iteration 13800: Loss = -12477.29296875
Iteration 13900: Loss = -12477.294921875
1
Iteration 14000: Loss = -12477.2919921875
Iteration 14100: Loss = -12477.2919921875
Iteration 14200: Loss = -12477.291015625
Iteration 14300: Loss = -12477.2900390625
Iteration 14400: Loss = -12477.2900390625
Iteration 14500: Loss = -12477.2890625
Iteration 14600: Loss = -12477.2880859375
Iteration 14700: Loss = -12477.2880859375
Iteration 14800: Loss = -12477.2880859375
Iteration 14900: Loss = -12477.2861328125
Iteration 15000: Loss = -12477.2880859375
1
Iteration 15100: Loss = -12477.28515625
Iteration 15200: Loss = -12477.283203125
Iteration 15300: Loss = -12477.2841796875
1
Iteration 15400: Loss = -12477.2841796875
2
Iteration 15500: Loss = -12477.2822265625
Iteration 15600: Loss = -12477.2822265625
Iteration 15700: Loss = -12477.28125
Iteration 15800: Loss = -12477.28125
Iteration 15900: Loss = -12477.2822265625
1
Iteration 16000: Loss = -12477.2822265625
2
Iteration 16100: Loss = -12477.28125
Iteration 16200: Loss = -12477.28125
Iteration 16300: Loss = -12477.2822265625
1
Iteration 16400: Loss = -12477.28125
Iteration 16500: Loss = -12477.2822265625
1
Iteration 16600: Loss = -12477.28125
Iteration 16700: Loss = -12477.28125
Iteration 16800: Loss = -12477.2783203125
Iteration 16900: Loss = -12477.2783203125
Iteration 17000: Loss = -12477.2783203125
Iteration 17100: Loss = -12477.2783203125
Iteration 17200: Loss = -12477.279296875
1
Iteration 17300: Loss = -12477.279296875
2
Iteration 17400: Loss = -12477.2783203125
Iteration 17500: Loss = -12477.27734375
Iteration 17600: Loss = -12477.2783203125
1
Iteration 17700: Loss = -12477.2763671875
Iteration 17800: Loss = -12477.2783203125
1
Iteration 17900: Loss = -12477.27734375
2
Iteration 18000: Loss = -12477.27734375
3
Iteration 18100: Loss = -12477.27734375
4
Iteration 18200: Loss = -12477.2763671875
Iteration 18300: Loss = -12477.27734375
1
Iteration 18400: Loss = -12477.27734375
2
Iteration 18500: Loss = -12477.27734375
3
Iteration 18600: Loss = -12477.27734375
4
Iteration 18700: Loss = -12477.27734375
5
Iteration 18800: Loss = -12477.27734375
6
Iteration 18900: Loss = -12477.27734375
7
Iteration 19000: Loss = -12477.275390625
Iteration 19100: Loss = -12477.2763671875
1
Iteration 19200: Loss = -12477.275390625
Iteration 19300: Loss = -12477.275390625
Iteration 19400: Loss = -12477.2763671875
1
Iteration 19500: Loss = -12477.275390625
Iteration 19600: Loss = -12477.2744140625
Iteration 19700: Loss = -12477.2763671875
1
Iteration 19800: Loss = -12477.27734375
2
Iteration 19900: Loss = -12477.2744140625
Iteration 20000: Loss = -12477.2734375
Iteration 20100: Loss = -12477.2763671875
1
Iteration 20200: Loss = -12477.2744140625
2
Iteration 20300: Loss = -12477.2744140625
3
Iteration 20400: Loss = -12477.27734375
4
Iteration 20500: Loss = -12477.27734375
5
Iteration 20600: Loss = -12477.275390625
6
Iteration 20700: Loss = -12477.275390625
7
Iteration 20800: Loss = -12477.2763671875
8
Iteration 20900: Loss = -12477.2744140625
9
Iteration 21000: Loss = -12477.275390625
10
Iteration 21100: Loss = -12477.275390625
11
Iteration 21200: Loss = -12477.275390625
12
Iteration 21300: Loss = -12477.275390625
13
Iteration 21400: Loss = -12477.275390625
14
Iteration 21500: Loss = -12477.2744140625
15
Stopping early at iteration 21500 due to no improvement.
pi: tensor([[9.9999e-01, 1.1736e-05],
        [9.4740e-01, 5.2601e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9987, 0.0013], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2027, 0.2350],
         [0.2166, 0.5502]],

        [[0.9918, 0.1839],
         [0.9823, 0.9338]],

        [[0.0104, 0.2991],
         [0.5128, 0.9904]],

        [[0.2961, 0.2202],
         [0.9156, 0.1562]],

        [[0.9451, 0.1956],
         [0.0178, 0.9713]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.0006639165226487879, 0.0] [0.0005695075040550283, 0.0] [12471.646484375, 12477.2744140625]
-------------------------------------
This iteration is 58
True Objective function: Loss = -11780.665952805504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -40133.46484375
Iteration 100: Loss = -24897.3125
Iteration 200: Loss = -15967.0224609375
Iteration 300: Loss = -13078.560546875
Iteration 400: Loss = -12543.24609375
Iteration 500: Loss = -12421.748046875
Iteration 600: Loss = -12386.53515625
Iteration 700: Loss = -12374.056640625
Iteration 800: Loss = -12361.806640625
Iteration 900: Loss = -12356.4697265625
Iteration 1000: Loss = -12352.8427734375
Iteration 1100: Loss = -12350.1513671875
Iteration 1200: Loss = -12348.0478515625
Iteration 1300: Loss = -12346.3818359375
Iteration 1400: Loss = -12345.037109375
Iteration 1500: Loss = -12343.923828125
Iteration 1600: Loss = -12342.9921875
Iteration 1700: Loss = -12342.205078125
Iteration 1800: Loss = -12341.5302734375
Iteration 1900: Loss = -12340.9453125
Iteration 2000: Loss = -12340.4345703125
Iteration 2100: Loss = -12339.9892578125
Iteration 2200: Loss = -12339.599609375
Iteration 2300: Loss = -12339.2509765625
Iteration 2400: Loss = -12338.9189453125
Iteration 2500: Loss = -12338.6376953125
Iteration 2600: Loss = -12338.3955078125
Iteration 2700: Loss = -12338.1796875
Iteration 2800: Loss = -12337.982421875
Iteration 2900: Loss = -12337.8076171875
Iteration 3000: Loss = -12337.64453125
Iteration 3100: Loss = -12337.494140625
Iteration 3200: Loss = -12337.35546875
Iteration 3300: Loss = -12337.2236328125
Iteration 3400: Loss = -12337.1025390625
Iteration 3500: Loss = -12336.984375
Iteration 3600: Loss = -12336.8671875
Iteration 3700: Loss = -12336.748046875
Iteration 3800: Loss = -12336.6025390625
Iteration 3900: Loss = -12336.294921875
Iteration 4000: Loss = -12334.3916015625
Iteration 4100: Loss = -12332.931640625
Iteration 4200: Loss = -12332.5517578125
Iteration 4300: Loss = -12332.2705078125
Iteration 4400: Loss = -12332.021484375
Iteration 4500: Loss = -12331.798828125
Iteration 4600: Loss = -12331.5927734375
Iteration 4700: Loss = -12331.4091796875
Iteration 4800: Loss = -12331.2265625
Iteration 4900: Loss = -12331.072265625
Iteration 5000: Loss = -12330.9375
Iteration 5100: Loss = -12330.8203125
Iteration 5200: Loss = -12330.720703125
Iteration 5300: Loss = -12330.62890625
Iteration 5400: Loss = -12330.5478515625
Iteration 5500: Loss = -12330.470703125
Iteration 5600: Loss = -12330.3935546875
Iteration 5700: Loss = -12330.314453125
Iteration 5800: Loss = -12330.2314453125
Iteration 5900: Loss = -12330.1630859375
Iteration 6000: Loss = -12330.08984375
Iteration 6100: Loss = -12330.01171875
Iteration 6200: Loss = -12329.935546875
Iteration 6300: Loss = -12329.8681640625
Iteration 6400: Loss = -12329.8046875
Iteration 6500: Loss = -12329.75
Iteration 6600: Loss = -12329.69921875
Iteration 6700: Loss = -12329.6552734375
Iteration 6800: Loss = -12329.615234375
Iteration 6900: Loss = -12329.58203125
Iteration 7000: Loss = -12329.548828125
Iteration 7100: Loss = -12329.521484375
Iteration 7200: Loss = -12329.49609375
Iteration 7300: Loss = -12329.47265625
Iteration 7400: Loss = -12329.451171875
Iteration 7500: Loss = -12329.4306640625
Iteration 7600: Loss = -12329.4111328125
Iteration 7700: Loss = -12329.3955078125
Iteration 7800: Loss = -12329.3779296875
Iteration 7900: Loss = -12329.361328125
Iteration 8000: Loss = -12329.3486328125
Iteration 8100: Loss = -12329.3349609375
Iteration 8200: Loss = -12329.3212890625
Iteration 8300: Loss = -12329.30859375
Iteration 8400: Loss = -12329.294921875
Iteration 8500: Loss = -12329.2841796875
Iteration 8600: Loss = -12329.271484375
Iteration 8700: Loss = -12329.2607421875
Iteration 8800: Loss = -12329.248046875
Iteration 8900: Loss = -12329.2373046875
Iteration 9000: Loss = -12329.2255859375
Iteration 9100: Loss = -12329.212890625
Iteration 9200: Loss = -12329.203125
Iteration 9300: Loss = -12329.189453125
Iteration 9400: Loss = -12329.1767578125
Iteration 9500: Loss = -12329.162109375
Iteration 9600: Loss = -12329.1455078125
Iteration 9700: Loss = -12329.12890625
Iteration 9800: Loss = -12329.1103515625
Iteration 9900: Loss = -12329.0908203125
Iteration 10000: Loss = -12329.06640625
Iteration 10100: Loss = -12329.0400390625
Iteration 10200: Loss = -12329.005859375
Iteration 10300: Loss = -12328.96875
Iteration 10400: Loss = -12328.923828125
Iteration 10500: Loss = -12328.8662109375
Iteration 10600: Loss = -12328.7978515625
Iteration 10700: Loss = -12328.7158203125
Iteration 10800: Loss = -12328.6201171875
Iteration 10900: Loss = -12328.51171875
Iteration 11000: Loss = -12328.404296875
Iteration 11100: Loss = -12328.3056640625
Iteration 11200: Loss = -12328.22265625
Iteration 11300: Loss = -12328.068359375
Iteration 11400: Loss = -12327.857421875
Iteration 11500: Loss = -12327.8427734375
Iteration 11600: Loss = -12327.8349609375
Iteration 11700: Loss = -12327.8271484375
Iteration 11800: Loss = -12327.8212890625
Iteration 11900: Loss = -12327.8173828125
Iteration 12000: Loss = -12327.8154296875
Iteration 12100: Loss = -12327.810546875
Iteration 12200: Loss = -12327.80859375
Iteration 12300: Loss = -12327.8046875
Iteration 12400: Loss = -12327.8017578125
Iteration 12500: Loss = -12327.7978515625
Iteration 12600: Loss = -12327.7783203125
Iteration 12700: Loss = -12327.4765625
Iteration 12800: Loss = -12326.84765625
Iteration 12900: Loss = -12326.8046875
Iteration 13000: Loss = -12326.7900390625
Iteration 13100: Loss = -12326.7802734375
Iteration 13200: Loss = -12326.77734375
Iteration 13300: Loss = -12326.7744140625
Iteration 13400: Loss = -12326.771484375
Iteration 13500: Loss = -12326.7705078125
Iteration 13600: Loss = -12326.76953125
Iteration 13700: Loss = -12326.7705078125
1
Iteration 13800: Loss = -12326.7685546875
Iteration 13900: Loss = -12326.7685546875
Iteration 14000: Loss = -12326.7685546875
Iteration 14100: Loss = -12326.7666015625
Iteration 14200: Loss = -12326.7666015625
Iteration 14300: Loss = -12326.765625
Iteration 14400: Loss = -12326.7646484375
Iteration 14500: Loss = -12326.7646484375
Iteration 14600: Loss = -12326.7646484375
Iteration 14700: Loss = -12326.7646484375
Iteration 14800: Loss = -12326.763671875
Iteration 14900: Loss = -12326.763671875
Iteration 15000: Loss = -12326.7646484375
1
Iteration 15100: Loss = -12326.763671875
Iteration 15200: Loss = -12326.763671875
Iteration 15300: Loss = -12326.763671875
Iteration 15400: Loss = -12326.7646484375
1
Iteration 15500: Loss = -12326.7626953125
Iteration 15600: Loss = -12326.763671875
1
Iteration 15700: Loss = -12326.7626953125
Iteration 15800: Loss = -12326.7626953125
Iteration 15900: Loss = -12326.76171875
Iteration 16000: Loss = -12326.76171875
Iteration 16100: Loss = -12326.763671875
1
Iteration 16200: Loss = -12326.76171875
Iteration 16300: Loss = -12326.7626953125
1
Iteration 16400: Loss = -12326.7607421875
Iteration 16500: Loss = -12326.7607421875
Iteration 16600: Loss = -12326.759765625
Iteration 16700: Loss = -12326.7607421875
1
Iteration 16800: Loss = -12326.7587890625
Iteration 16900: Loss = -12326.7607421875
1
Iteration 17000: Loss = -12326.7607421875
2
Iteration 17100: Loss = -12326.7607421875
3
Iteration 17200: Loss = -12326.7607421875
4
Iteration 17300: Loss = -12326.759765625
5
Iteration 17400: Loss = -12326.759765625
6
Iteration 17500: Loss = -12326.76171875
7
Iteration 17600: Loss = -12326.7587890625
Iteration 17700: Loss = -12326.759765625
1
Iteration 17800: Loss = -12326.7587890625
Iteration 17900: Loss = -12326.7587890625
Iteration 18000: Loss = -12326.7578125
Iteration 18100: Loss = -12326.7587890625
1
Iteration 18200: Loss = -12326.7587890625
2
Iteration 18300: Loss = -12326.7587890625
3
Iteration 18400: Loss = -12326.7607421875
4
Iteration 18500: Loss = -12326.7587890625
5
Iteration 18600: Loss = -12326.7587890625
6
Iteration 18700: Loss = -12326.7578125
Iteration 18800: Loss = -12326.763671875
1
Iteration 18900: Loss = -12326.759765625
2
Iteration 19000: Loss = -12326.7587890625
3
Iteration 19100: Loss = -12326.7587890625
4
Iteration 19200: Loss = -12326.7578125
Iteration 19300: Loss = -12326.7578125
Iteration 19400: Loss = -12326.7587890625
1
Iteration 19500: Loss = -12326.7578125
Iteration 19600: Loss = -12326.759765625
1
Iteration 19700: Loss = -12326.7587890625
2
Iteration 19800: Loss = -12326.7607421875
3
Iteration 19900: Loss = -12326.7587890625
4
Iteration 20000: Loss = -12326.7626953125
5
Iteration 20100: Loss = -12326.7587890625
6
Iteration 20200: Loss = -12326.7578125
Iteration 20300: Loss = -12326.7587890625
1
Iteration 20400: Loss = -12326.7587890625
2
Iteration 20500: Loss = -12326.7587890625
3
Iteration 20600: Loss = -12326.7587890625
4
Iteration 20700: Loss = -12326.7587890625
5
Iteration 20800: Loss = -12326.7587890625
6
Iteration 20900: Loss = -12326.7587890625
7
Iteration 21000: Loss = -12326.759765625
8
Iteration 21100: Loss = -12326.7578125
Iteration 21200: Loss = -12326.7578125
Iteration 21300: Loss = -12326.7587890625
1
Iteration 21400: Loss = -12326.759765625
2
Iteration 21500: Loss = -12326.7587890625
3
Iteration 21600: Loss = -12326.7587890625
4
Iteration 21700: Loss = -12326.7587890625
5
Iteration 21800: Loss = -12326.759765625
6
Iteration 21900: Loss = -12326.7587890625
7
Iteration 22000: Loss = -12326.7607421875
8
Iteration 22100: Loss = -12326.7587890625
9
Iteration 22200: Loss = -12326.7587890625
10
Iteration 22300: Loss = -12326.7578125
Iteration 22400: Loss = -12326.7578125
Iteration 22500: Loss = -12326.7578125
Iteration 22600: Loss = -12326.7607421875
1
Iteration 22700: Loss = -12326.759765625
2
Iteration 22800: Loss = -12326.7587890625
3
Iteration 22900: Loss = -12326.7587890625
4
Iteration 23000: Loss = -12326.7587890625
5
Iteration 23100: Loss = -12326.7587890625
6
Iteration 23200: Loss = -12326.7587890625
7
Iteration 23300: Loss = -12326.7587890625
8
Iteration 23400: Loss = -12326.7587890625
9
Iteration 23500: Loss = -12326.7587890625
10
Iteration 23600: Loss = -12326.7578125
Iteration 23700: Loss = -12326.759765625
1
Iteration 23800: Loss = -12326.759765625
2
Iteration 23900: Loss = -12326.7587890625
3
Iteration 24000: Loss = -12326.7587890625
4
Iteration 24100: Loss = -12326.7587890625
5
Iteration 24200: Loss = -12326.7578125
Iteration 24300: Loss = -12326.7578125
Iteration 24400: Loss = -12326.759765625
1
Iteration 24500: Loss = -12326.7607421875
2
Iteration 24600: Loss = -12326.7587890625
3
Iteration 24700: Loss = -12326.7587890625
4
Iteration 24800: Loss = -12326.7587890625
5
Iteration 24900: Loss = -12326.7578125
Iteration 25000: Loss = -12326.7587890625
1
Iteration 25100: Loss = -12326.7587890625
2
Iteration 25200: Loss = -12326.7587890625
3
Iteration 25300: Loss = -12326.7578125
Iteration 25400: Loss = -12326.7587890625
1
Iteration 25500: Loss = -12326.759765625
2
Iteration 25600: Loss = -12326.7578125
Iteration 25700: Loss = -12326.7587890625
1
Iteration 25800: Loss = -12326.759765625
2
Iteration 25900: Loss = -12326.7578125
Iteration 26000: Loss = -12326.7587890625
1
Iteration 26100: Loss = -12326.7587890625
2
Iteration 26200: Loss = -12326.7587890625
3
Iteration 26300: Loss = -12326.7578125
Iteration 26400: Loss = -12326.759765625
1
Iteration 26500: Loss = -12326.7587890625
2
Iteration 26600: Loss = -12326.7587890625
3
Iteration 26700: Loss = -12326.759765625
4
Iteration 26800: Loss = -12326.7587890625
5
Iteration 26900: Loss = -12326.7587890625
6
Iteration 27000: Loss = -12326.7578125
Iteration 27100: Loss = -12326.7587890625
1
Iteration 27200: Loss = -12326.759765625
2
Iteration 27300: Loss = -12326.7587890625
3
Iteration 27400: Loss = -12326.7587890625
4
Iteration 27500: Loss = -12326.7578125
Iteration 27600: Loss = -12326.759765625
1
Iteration 27700: Loss = -12326.759765625
2
Iteration 27800: Loss = -12326.7587890625
3
Iteration 27900: Loss = -12326.7587890625
4
Iteration 28000: Loss = -12326.759765625
5
Iteration 28100: Loss = -12326.7587890625
6
Iteration 28200: Loss = -12326.7587890625
7
Iteration 28300: Loss = -12326.7587890625
8
Iteration 28400: Loss = -12326.759765625
9
Iteration 28500: Loss = -12326.759765625
10
Iteration 28600: Loss = -12326.7587890625
11
Iteration 28700: Loss = -12326.7587890625
12
Iteration 28800: Loss = -12326.759765625
13
Iteration 28900: Loss = -12326.7578125
Iteration 29000: Loss = -12326.7587890625
1
Iteration 29100: Loss = -12326.7587890625
2
Iteration 29200: Loss = -12326.7587890625
3
Iteration 29300: Loss = -12326.7587890625
4
Iteration 29400: Loss = -12326.7568359375
Iteration 29500: Loss = -12326.7578125
1
Iteration 29600: Loss = -12326.7578125
2
Iteration 29700: Loss = -12326.7587890625
3
Iteration 29800: Loss = -12326.759765625
4
Iteration 29900: Loss = -12326.759765625
5
pi: tensor([[1.2960e-06, 1.0000e+00],
        [2.3181e-02, 9.7682e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0275, 0.9725], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3665, 0.3343],
         [0.9475, 0.1974]],

        [[0.9738, 0.2765],
         [0.7922, 0.0096]],

        [[0.9129, 0.1010],
         [0.9754, 0.9455]],

        [[0.2421, 0.2551],
         [0.9926, 0.0080]],

        [[0.2408, 0.0922],
         [0.4412, 0.2852]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.00035341041046094813
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 60
Adjusted Rand Index: 0.00877236457352431
Global Adjusted Rand Index: -6.771069336677129e-05
Average Adjusted Rand Index: 0.0023964497236561196
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -52157.74609375
Iteration 100: Loss = -31516.197265625
Iteration 200: Loss = -17657.580078125
Iteration 300: Loss = -13836.958984375
Iteration 400: Loss = -12983.59375
Iteration 500: Loss = -12698.3251953125
Iteration 600: Loss = -12555.408203125
Iteration 700: Loss = -12501.1728515625
Iteration 800: Loss = -12452.3173828125
Iteration 900: Loss = -12426.7958984375
Iteration 1000: Loss = -12409.330078125
Iteration 1100: Loss = -12397.8125
Iteration 1200: Loss = -12390.736328125
Iteration 1300: Loss = -12385.0859375
Iteration 1400: Loss = -12379.947265625
Iteration 1500: Loss = -12373.578125
Iteration 1600: Loss = -12369.201171875
Iteration 1700: Loss = -12366.4140625
Iteration 1800: Loss = -12364.1669921875
Iteration 1900: Loss = -12362.265625
Iteration 2000: Loss = -12360.6240234375
Iteration 2100: Loss = -12359.1953125
Iteration 2200: Loss = -12357.9384765625
Iteration 2300: Loss = -12356.822265625
Iteration 2400: Loss = -12355.830078125
Iteration 2500: Loss = -12354.94140625
Iteration 2600: Loss = -12354.140625
Iteration 2700: Loss = -12353.4111328125
Iteration 2800: Loss = -12352.7392578125
Iteration 2900: Loss = -12352.08203125
Iteration 3000: Loss = -12351.1015625
Iteration 3100: Loss = -12347.5263671875
Iteration 3200: Loss = -12346.55078125
Iteration 3300: Loss = -12346.01953125
Iteration 3400: Loss = -12340.8935546875
Iteration 3500: Loss = -12340.171875
Iteration 3600: Loss = -12339.7509765625
Iteration 3700: Loss = -12339.41015625
Iteration 3800: Loss = -12339.1123046875
Iteration 3900: Loss = -12338.8408203125
Iteration 4000: Loss = -12338.5927734375
Iteration 4100: Loss = -12338.3662109375
Iteration 4200: Loss = -12338.1572265625
Iteration 4300: Loss = -12337.962890625
Iteration 4400: Loss = -12337.7841796875
Iteration 4500: Loss = -12337.6162109375
Iteration 4600: Loss = -12337.458984375
Iteration 4700: Loss = -12337.3154296875
Iteration 4800: Loss = -12337.1787109375
Iteration 4900: Loss = -12337.052734375
Iteration 5000: Loss = -12336.931640625
Iteration 5100: Loss = -12336.8193359375
Iteration 5200: Loss = -12336.7138671875
Iteration 5300: Loss = -12336.6142578125
Iteration 5400: Loss = -12336.521484375
Iteration 5500: Loss = -12336.43359375
Iteration 5600: Loss = -12336.3505859375
Iteration 5700: Loss = -12336.271484375
Iteration 5800: Loss = -12336.2001953125
Iteration 5900: Loss = -12336.1298828125
Iteration 6000: Loss = -12336.064453125
Iteration 6100: Loss = -12336.001953125
Iteration 6200: Loss = -12335.9453125
Iteration 6300: Loss = -12335.890625
Iteration 6400: Loss = -12335.8359375
Iteration 6500: Loss = -12335.7880859375
Iteration 6600: Loss = -12335.7392578125
Iteration 6700: Loss = -12335.6953125
Iteration 6800: Loss = -12335.6533203125
Iteration 6900: Loss = -12335.6142578125
Iteration 7000: Loss = -12335.5771484375
Iteration 7100: Loss = -12335.5380859375
Iteration 7200: Loss = -12335.50390625
Iteration 7300: Loss = -12335.47265625
Iteration 7400: Loss = -12335.439453125
Iteration 7500: Loss = -12335.40625
Iteration 7600: Loss = -12334.7509765625
Iteration 7700: Loss = -12334.580078125
Iteration 7800: Loss = -12333.5810546875
Iteration 7900: Loss = -12330.5185546875
Iteration 8000: Loss = -12330.35546875
Iteration 8100: Loss = -12330.263671875
Iteration 8200: Loss = -12330.193359375
Iteration 8300: Loss = -12330.138671875
Iteration 8400: Loss = -12330.0908203125
Iteration 8500: Loss = -12330.0537109375
Iteration 8600: Loss = -12330.017578125
Iteration 8700: Loss = -12329.9873046875
Iteration 8800: Loss = -12329.9599609375
Iteration 8900: Loss = -12329.935546875
Iteration 9000: Loss = -12329.9130859375
Iteration 9100: Loss = -12329.8916015625
Iteration 9200: Loss = -12329.8720703125
Iteration 9300: Loss = -12329.853515625
Iteration 9400: Loss = -12329.8388671875
Iteration 9500: Loss = -12329.818359375
Iteration 9600: Loss = -12329.802734375
Iteration 9700: Loss = -12329.7890625
Iteration 9800: Loss = -12329.7763671875
Iteration 9900: Loss = -12329.765625
Iteration 10000: Loss = -12329.7529296875
Iteration 10100: Loss = -12329.7431640625
Iteration 10200: Loss = -12329.732421875
Iteration 10300: Loss = -12329.724609375
Iteration 10400: Loss = -12329.7138671875
Iteration 10500: Loss = -12329.70703125
Iteration 10600: Loss = -12329.7001953125
Iteration 10700: Loss = -12329.693359375
Iteration 10800: Loss = -12329.6884765625
Iteration 10900: Loss = -12329.681640625
Iteration 11000: Loss = -12329.6767578125
Iteration 11100: Loss = -12329.671875
Iteration 11200: Loss = -12329.666015625
Iteration 11300: Loss = -12329.662109375
Iteration 11400: Loss = -12329.6572265625
Iteration 11500: Loss = -12329.65234375
Iteration 11600: Loss = -12329.6513671875
Iteration 11700: Loss = -12329.646484375
Iteration 11800: Loss = -12329.6435546875
Iteration 11900: Loss = -12329.640625
Iteration 12000: Loss = -12329.638671875
Iteration 12100: Loss = -12329.634765625
Iteration 12200: Loss = -12329.6337890625
Iteration 12300: Loss = -12329.630859375
Iteration 12400: Loss = -12329.6279296875
Iteration 12500: Loss = -12329.6279296875
Iteration 12600: Loss = -12329.626953125
Iteration 12700: Loss = -12329.623046875
Iteration 12800: Loss = -12329.6201171875
Iteration 12900: Loss = -12329.6201171875
Iteration 13000: Loss = -12329.619140625
Iteration 13100: Loss = -12329.6171875
Iteration 13200: Loss = -12329.6142578125
Iteration 13300: Loss = -12329.6142578125
Iteration 13400: Loss = -12329.6123046875
Iteration 13500: Loss = -12329.609375
Iteration 13600: Loss = -12329.609375
Iteration 13700: Loss = -12329.609375
Iteration 13800: Loss = -12329.607421875
Iteration 13900: Loss = -12329.60546875
Iteration 14000: Loss = -12329.6044921875
Iteration 14100: Loss = -12329.6025390625
Iteration 14200: Loss = -12329.6025390625
Iteration 14300: Loss = -12329.6005859375
Iteration 14400: Loss = -12329.599609375
Iteration 14500: Loss = -12329.5986328125
Iteration 14600: Loss = -12329.595703125
Iteration 14700: Loss = -12329.5947265625
Iteration 14800: Loss = -12329.5859375
Iteration 14900: Loss = -12329.5712890625
Iteration 15000: Loss = -12329.560546875
Iteration 15100: Loss = -12329.5537109375
Iteration 15200: Loss = -12329.548828125
Iteration 15300: Loss = -12329.5458984375
Iteration 15400: Loss = -12329.5439453125
Iteration 15500: Loss = -12329.5439453125
Iteration 15600: Loss = -12329.541015625
Iteration 15700: Loss = -12329.5390625
Iteration 15800: Loss = -12329.5390625
Iteration 15900: Loss = -12329.5361328125
Iteration 16000: Loss = -12329.5361328125
Iteration 16100: Loss = -12329.5361328125
Iteration 16200: Loss = -12329.5341796875
Iteration 16300: Loss = -12329.533203125
Iteration 16400: Loss = -12329.5322265625
Iteration 16500: Loss = -12329.5322265625
Iteration 16600: Loss = -12329.5322265625
Iteration 16700: Loss = -12329.53125
Iteration 16800: Loss = -12329.5322265625
1
Iteration 16900: Loss = -12329.5302734375
Iteration 17000: Loss = -12329.5302734375
Iteration 17100: Loss = -12329.5302734375
Iteration 17200: Loss = -12329.529296875
Iteration 17300: Loss = -12329.5302734375
1
Iteration 17400: Loss = -12329.53125
2
Iteration 17500: Loss = -12329.5302734375
3
Iteration 17600: Loss = -12329.5302734375
4
Iteration 17700: Loss = -12329.52734375
Iteration 17800: Loss = -12329.529296875
1
Iteration 17900: Loss = -12329.5283203125
2
Iteration 18000: Loss = -12329.5283203125
3
Iteration 18100: Loss = -12329.5283203125
4
Iteration 18200: Loss = -12329.52734375
Iteration 18300: Loss = -12329.5244140625
Iteration 18400: Loss = -12329.0283203125
Iteration 18500: Loss = -12328.744140625
Iteration 18600: Loss = -12328.4287109375
Iteration 18700: Loss = -12328.232421875
Iteration 18800: Loss = -12328.189453125
Iteration 18900: Loss = -12328.0224609375
Iteration 19000: Loss = -12327.935546875
Iteration 19100: Loss = -12327.9267578125
Iteration 19200: Loss = -12327.8642578125
Iteration 19300: Loss = -12327.830078125
Iteration 19400: Loss = -12327.818359375
Iteration 19500: Loss = -12327.7919921875
Iteration 19600: Loss = -12327.7900390625
Iteration 19700: Loss = -12327.7685546875
Iteration 19800: Loss = -12327.7275390625
Iteration 19900: Loss = -12327.708984375
Iteration 20000: Loss = -12327.6953125
Iteration 20100: Loss = -12327.6728515625
Iteration 20200: Loss = -12327.6689453125
Iteration 20300: Loss = -12327.6689453125
Iteration 20400: Loss = -12327.6669921875
Iteration 20500: Loss = -12327.658203125
Iteration 20600: Loss = -12327.658203125
Iteration 20700: Loss = -12327.603515625
Iteration 20800: Loss = -12327.572265625
Iteration 20900: Loss = -12327.5703125
Iteration 21000: Loss = -12327.5712890625
1
Iteration 21100: Loss = -12327.5712890625
2
Iteration 21200: Loss = -12327.5703125
Iteration 21300: Loss = -12327.55078125
Iteration 21400: Loss = -12327.548828125
Iteration 21500: Loss = -12327.5478515625
Iteration 21600: Loss = -12327.546875
Iteration 21700: Loss = -12327.54296875
Iteration 21800: Loss = -12327.5419921875
Iteration 21900: Loss = -12327.52734375
Iteration 22000: Loss = -12327.525390625
Iteration 22100: Loss = -12327.52734375
1
Iteration 22200: Loss = -12327.5263671875
2
Iteration 22300: Loss = -12327.5224609375
Iteration 22400: Loss = -12327.5205078125
Iteration 22500: Loss = -12327.5185546875
Iteration 22600: Loss = -12327.5185546875
Iteration 22700: Loss = -12327.5126953125
Iteration 22800: Loss = -12327.51171875
Iteration 22900: Loss = -12327.51171875
Iteration 23000: Loss = -12327.5126953125
1
Iteration 23100: Loss = -12327.5107421875
Iteration 23200: Loss = -12327.5126953125
1
Iteration 23300: Loss = -12327.51171875
2
Iteration 23400: Loss = -12327.51171875
3
Iteration 23500: Loss = -12327.5126953125
4
Iteration 23600: Loss = -12327.5107421875
Iteration 23700: Loss = -12327.51171875
1
Iteration 23800: Loss = -12327.5126953125
2
Iteration 23900: Loss = -12327.5126953125
3
Iteration 24000: Loss = -12327.51171875
4
Iteration 24100: Loss = -12327.51171875
5
Iteration 24200: Loss = -12327.5126953125
6
Iteration 24300: Loss = -12327.5107421875
Iteration 24400: Loss = -12327.5126953125
1
Iteration 24500: Loss = -12327.513671875
2
Iteration 24600: Loss = -12327.5126953125
3
Iteration 24700: Loss = -12327.51171875
4
Iteration 24800: Loss = -12327.5107421875
Iteration 24900: Loss = -12327.51171875
1
Iteration 25000: Loss = -12327.51171875
2
Iteration 25100: Loss = -12327.509765625
Iteration 25200: Loss = -12327.51171875
1
Iteration 25300: Loss = -12327.5107421875
2
Iteration 25400: Loss = -12327.5126953125
3
Iteration 25500: Loss = -12327.513671875
4
Iteration 25600: Loss = -12327.51171875
5
Iteration 25700: Loss = -12327.5107421875
6
Iteration 25800: Loss = -12327.51171875
7
Iteration 25900: Loss = -12327.5107421875
8
Iteration 26000: Loss = -12327.5107421875
9
Iteration 26100: Loss = -12327.5126953125
10
Iteration 26200: Loss = -12327.51171875
11
Iteration 26300: Loss = -12327.5107421875
12
Iteration 26400: Loss = -12327.51171875
13
Iteration 26500: Loss = -12327.51171875
14
Iteration 26600: Loss = -12327.51171875
15
Stopping early at iteration 26600 due to no improvement.
pi: tensor([[2.5062e-06, 1.0000e+00],
        [2.7563e-02, 9.7244e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0286, 0.9714], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4270, 0.3322],
         [0.0938, 0.1952]],

        [[0.0114, 0.2871],
         [0.0388, 0.7764]],

        [[0.8425, 0.0996],
         [0.2067, 0.1337]],

        [[0.0133, 0.2587],
         [0.8002, 0.1355]],

        [[0.9763, 0.2699],
         [0.1040, 0.3509]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.00035341041046094813
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: -0.006096049957872825
Global Adjusted Rand Index: 0.0003805778552278475
Average Adjusted Rand Index: -0.0005772331826233073
[-6.771069336677129e-05, 0.0003805778552278475] [0.0023964497236561196, -0.0005772331826233073] [12326.7578125, 12327.51171875]
-------------------------------------
This iteration is 59
True Objective function: Loss = -11844.443173412514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -42496.5859375
Iteration 100: Loss = -24651.494140625
Iteration 200: Loss = -14988.60546875
Iteration 300: Loss = -13396.080078125
Iteration 400: Loss = -13017.4560546875
Iteration 500: Loss = -12821.099609375
Iteration 600: Loss = -12721.5498046875
Iteration 700: Loss = -12662.7685546875
Iteration 800: Loss = -12613.490234375
Iteration 900: Loss = -12582.2958984375
Iteration 1000: Loss = -12552.5703125
Iteration 1100: Loss = -12524.5146484375
Iteration 1200: Loss = -12510.287109375
Iteration 1300: Loss = -12500.3837890625
Iteration 1400: Loss = -12490.818359375
Iteration 1500: Loss = -12483.4580078125
Iteration 1600: Loss = -12478.572265625
Iteration 1700: Loss = -12475.12109375
Iteration 1800: Loss = -12472.501953125
Iteration 1900: Loss = -12470.400390625
Iteration 2000: Loss = -12468.6552734375
Iteration 2100: Loss = -12467.1630859375
Iteration 2200: Loss = -12465.8671875
Iteration 2300: Loss = -12464.705078125
Iteration 2400: Loss = -12463.65234375
Iteration 2500: Loss = -12453.068359375
Iteration 2600: Loss = -12451.9033203125
Iteration 2700: Loss = -12451.111328125
Iteration 2800: Loss = -12446.1279296875
Iteration 2900: Loss = -12444.2373046875
Iteration 3000: Loss = -12437.4912109375
Iteration 3100: Loss = -12435.638671875
Iteration 3200: Loss = -12434.763671875
Iteration 3300: Loss = -12434.1005859375
Iteration 3400: Loss = -12433.4208984375
Iteration 3500: Loss = -12432.599609375
Iteration 3600: Loss = -12431.791015625
Iteration 3700: Loss = -12431.201171875
Iteration 3800: Loss = -12430.7626953125
Iteration 3900: Loss = -12430.41015625
Iteration 4000: Loss = -12430.109375
Iteration 4100: Loss = -12429.85546875
Iteration 4200: Loss = -12429.62890625
Iteration 4300: Loss = -12429.4267578125
Iteration 4400: Loss = -12429.2431640625
Iteration 4500: Loss = -12429.076171875
Iteration 4600: Loss = -12428.919921875
Iteration 4700: Loss = -12428.77734375
Iteration 4800: Loss = -12428.646484375
Iteration 4900: Loss = -12428.5244140625
Iteration 5000: Loss = -12428.4111328125
Iteration 5100: Loss = -12428.3037109375
Iteration 5200: Loss = -12428.203125
Iteration 5300: Loss = -12428.1123046875
Iteration 5400: Loss = -12428.0244140625
Iteration 5500: Loss = -12427.9423828125
Iteration 5600: Loss = -12427.8642578125
Iteration 5700: Loss = -12427.7919921875
Iteration 5800: Loss = -12427.7236328125
Iteration 5900: Loss = -12427.658203125
Iteration 6000: Loss = -12427.5966796875
Iteration 6100: Loss = -12427.5380859375
Iteration 6200: Loss = -12427.4833984375
Iteration 6300: Loss = -12427.431640625
Iteration 6400: Loss = -12427.3837890625
Iteration 6500: Loss = -12427.3359375
Iteration 6600: Loss = -12427.2919921875
Iteration 6700: Loss = -12427.251953125
Iteration 6800: Loss = -12427.2119140625
Iteration 6900: Loss = -12427.17578125
Iteration 7000: Loss = -12427.138671875
Iteration 7100: Loss = -12427.10546875
Iteration 7200: Loss = -12427.072265625
Iteration 7300: Loss = -12427.04296875
Iteration 7400: Loss = -12427.013671875
Iteration 7500: Loss = -12426.98828125
Iteration 7600: Loss = -12426.9599609375
Iteration 7700: Loss = -12426.93359375
Iteration 7800: Loss = -12426.9111328125
Iteration 7900: Loss = -12426.888671875
Iteration 8000: Loss = -12426.8671875
Iteration 8100: Loss = -12426.845703125
Iteration 8200: Loss = -12426.8271484375
Iteration 8300: Loss = -12426.8095703125
Iteration 8400: Loss = -12426.79296875
Iteration 8500: Loss = -12426.7763671875
Iteration 8600: Loss = -12426.7578125
Iteration 8700: Loss = -12426.744140625
Iteration 8800: Loss = -12426.728515625
Iteration 8900: Loss = -12426.71484375
Iteration 9000: Loss = -12426.7041015625
Iteration 9100: Loss = -12426.6904296875
Iteration 9200: Loss = -12426.677734375
Iteration 9300: Loss = -12426.6650390625
Iteration 9400: Loss = -12426.654296875
Iteration 9500: Loss = -12426.6435546875
Iteration 9600: Loss = -12426.634765625
Iteration 9700: Loss = -12426.625
Iteration 9800: Loss = -12426.615234375
Iteration 9900: Loss = -12426.607421875
Iteration 10000: Loss = -12426.6005859375
Iteration 10100: Loss = -12426.591796875
Iteration 10200: Loss = -12426.5859375
Iteration 10300: Loss = -12426.578125
Iteration 10400: Loss = -12426.5703125
Iteration 10500: Loss = -12426.564453125
Iteration 10600: Loss = -12426.55859375
Iteration 10700: Loss = -12426.552734375
Iteration 10800: Loss = -12426.546875
Iteration 10900: Loss = -12426.541015625
Iteration 11000: Loss = -12426.537109375
Iteration 11100: Loss = -12426.5322265625
Iteration 11200: Loss = -12426.5263671875
Iteration 11300: Loss = -12426.5224609375
Iteration 11400: Loss = -12426.5185546875
Iteration 11500: Loss = -12426.5166015625
Iteration 11600: Loss = -12426.5107421875
Iteration 11700: Loss = -12426.5068359375
Iteration 11800: Loss = -12426.5048828125
Iteration 11900: Loss = -12426.5
Iteration 12000: Loss = -12426.498046875
Iteration 12100: Loss = -12426.49609375
Iteration 12200: Loss = -12426.4931640625
Iteration 12300: Loss = -12426.4892578125
Iteration 12400: Loss = -12426.48828125
Iteration 12500: Loss = -12426.4853515625
Iteration 12600: Loss = -12426.482421875
Iteration 12700: Loss = -12426.4794921875
Iteration 12800: Loss = -12426.4765625
Iteration 12900: Loss = -12426.4736328125
Iteration 13000: Loss = -12426.47265625
Iteration 13100: Loss = -12426.4716796875
Iteration 13200: Loss = -12426.470703125
Iteration 13300: Loss = -12426.4697265625
Iteration 13400: Loss = -12426.466796875
Iteration 13500: Loss = -12426.4658203125
Iteration 13600: Loss = -12426.46484375
Iteration 13700: Loss = -12426.4619140625
Iteration 13800: Loss = -12426.4619140625
Iteration 13900: Loss = -12426.4599609375
Iteration 14000: Loss = -12426.4580078125
Iteration 14100: Loss = -12426.45703125
Iteration 14200: Loss = -12426.455078125
Iteration 14300: Loss = -12426.4541015625
Iteration 14400: Loss = -12426.4541015625
Iteration 14500: Loss = -12426.453125
Iteration 14600: Loss = -12426.4521484375
Iteration 14700: Loss = -12426.4501953125
Iteration 14800: Loss = -12426.451171875
1
Iteration 14900: Loss = -12426.44921875
Iteration 15000: Loss = -12426.4482421875
Iteration 15100: Loss = -12426.4462890625
Iteration 15200: Loss = -12426.4482421875
1
Iteration 15300: Loss = -12426.447265625
2
Iteration 15400: Loss = -12426.447265625
3
Iteration 15500: Loss = -12426.4462890625
Iteration 15600: Loss = -12426.4462890625
Iteration 15700: Loss = -12426.4443359375
Iteration 15800: Loss = -12426.443359375
Iteration 15900: Loss = -12426.443359375
Iteration 16000: Loss = -12426.443359375
Iteration 16100: Loss = -12426.44140625
Iteration 16200: Loss = -12426.4423828125
1
Iteration 16300: Loss = -12426.44140625
Iteration 16400: Loss = -12426.4423828125
1
Iteration 16500: Loss = -12426.4404296875
Iteration 16600: Loss = -12426.4423828125
1
Iteration 16700: Loss = -12426.4404296875
Iteration 16800: Loss = -12426.439453125
Iteration 16900: Loss = -12426.4384765625
Iteration 17000: Loss = -12426.3359375
Iteration 17100: Loss = -12426.2978515625
Iteration 17200: Loss = -12426.2529296875
Iteration 17300: Loss = -12426.166015625
Iteration 17400: Loss = -12425.912109375
Iteration 17500: Loss = -12425.5927734375
Iteration 17600: Loss = -12425.5625
Iteration 17700: Loss = -12425.55078125
Iteration 17800: Loss = -12425.5439453125
Iteration 17900: Loss = -12425.537109375
Iteration 18000: Loss = -12425.5361328125
Iteration 18100: Loss = -12425.53515625
Iteration 18200: Loss = -12425.53515625
Iteration 18300: Loss = -12425.53515625
Iteration 18400: Loss = -12425.5341796875
Iteration 18500: Loss = -12425.5341796875
Iteration 18600: Loss = -12425.53515625
1
Iteration 18700: Loss = -12425.53515625
2
Iteration 18800: Loss = -12425.5341796875
Iteration 18900: Loss = -12425.533203125
Iteration 19000: Loss = -12425.53125
Iteration 19100: Loss = -12425.5322265625
1
Iteration 19200: Loss = -12425.533203125
2
Iteration 19300: Loss = -12425.533203125
3
Iteration 19400: Loss = -12425.5302734375
Iteration 19500: Loss = -12425.5302734375
Iteration 19600: Loss = -12425.5341796875
1
Iteration 19700: Loss = -12425.5322265625
2
Iteration 19800: Loss = -12425.53125
3
Iteration 19900: Loss = -12425.529296875
Iteration 20000: Loss = -12425.53125
1
Iteration 20100: Loss = -12425.5302734375
2
Iteration 20200: Loss = -12425.5302734375
3
Iteration 20300: Loss = -12425.5322265625
4
Iteration 20400: Loss = -12425.53125
5
Iteration 20500: Loss = -12425.529296875
Iteration 20600: Loss = -12425.5322265625
1
Iteration 20700: Loss = -12425.53125
2
Iteration 20800: Loss = -12425.5302734375
3
Iteration 20900: Loss = -12425.5283203125
Iteration 21000: Loss = -12425.529296875
1
Iteration 21100: Loss = -12425.5322265625
2
Iteration 21200: Loss = -12425.53125
3
Iteration 21300: Loss = -12425.5302734375
4
Iteration 21400: Loss = -12425.5302734375
5
Iteration 21500: Loss = -12425.529296875
6
Iteration 21600: Loss = -12425.3837890625
Iteration 21700: Loss = -12425.2353515625
Iteration 21800: Loss = -12425.095703125
Iteration 21900: Loss = -12424.8740234375
Iteration 22000: Loss = -12424.59375
Iteration 22100: Loss = -12423.2607421875
Iteration 22200: Loss = -12423.2373046875
Iteration 22300: Loss = -12423.2255859375
Iteration 22400: Loss = -12423.220703125
Iteration 22500: Loss = -12423.21875
Iteration 22600: Loss = -12423.216796875
Iteration 22700: Loss = -12423.2158203125
Iteration 22800: Loss = -12423.21484375
Iteration 22900: Loss = -12423.2138671875
Iteration 23000: Loss = -12423.2138671875
Iteration 23100: Loss = -12423.21484375
1
Iteration 23200: Loss = -12423.2138671875
Iteration 23300: Loss = -12423.2119140625
Iteration 23400: Loss = -12423.212890625
1
Iteration 23500: Loss = -12423.212890625
2
Iteration 23600: Loss = -12423.21484375
3
Iteration 23700: Loss = -12423.212890625
4
Iteration 23800: Loss = -12423.2119140625
Iteration 23900: Loss = -12423.2109375
Iteration 24000: Loss = -12423.2109375
Iteration 24100: Loss = -12423.2119140625
1
Iteration 24200: Loss = -12423.2109375
Iteration 24300: Loss = -12423.2119140625
1
Iteration 24400: Loss = -12423.2109375
Iteration 24500: Loss = -12423.208984375
Iteration 24600: Loss = -12423.2099609375
1
Iteration 24700: Loss = -12423.2099609375
2
Iteration 24800: Loss = -12423.2099609375
3
Iteration 24900: Loss = -12423.208984375
Iteration 25000: Loss = -12423.2099609375
1
Iteration 25100: Loss = -12423.2109375
2
Iteration 25200: Loss = -12423.2109375
3
Iteration 25300: Loss = -12423.2099609375
4
Iteration 25400: Loss = -12423.2099609375
5
Iteration 25500: Loss = -12423.2109375
6
Iteration 25600: Loss = -12423.2099609375
7
Iteration 25700: Loss = -12423.2109375
8
Iteration 25800: Loss = -12423.2099609375
9
Iteration 25900: Loss = -12423.2099609375
10
Iteration 26000: Loss = -12423.2099609375
11
Iteration 26100: Loss = -12423.2099609375
12
Iteration 26200: Loss = -12423.2099609375
13
Iteration 26300: Loss = -12423.2099609375
14
Iteration 26400: Loss = -12423.2099609375
15
Stopping early at iteration 26400 due to no improvement.
pi: tensor([[1.0000e+00, 7.1057e-07],
        [2.3760e-04, 9.9976e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9751, 0.0249], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2016, 0.1392],
         [0.0159, 0.1301]],

        [[0.9855, 0.1163],
         [0.0202, 0.0085]],

        [[0.8993, 0.2359],
         [0.0312, 0.8286]],

        [[0.8767, 0.2264],
         [0.8875, 0.9516]],

        [[0.2111, 0.2527],
         [0.9892, 0.4144]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0007766707522784365
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0010149900615556472
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.006467401572035518
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
Global Adjusted Rand Index: -0.0011231119832131045
Average Adjusted Rand Index: -0.0022793027514080026
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22585.34375
Iteration 100: Loss = -16328.6865234375
Iteration 200: Loss = -13738.240234375
Iteration 300: Loss = -12850.49609375
Iteration 400: Loss = -12588.1845703125
Iteration 500: Loss = -12526.3564453125
Iteration 600: Loss = -12496.6796875
Iteration 700: Loss = -12473.708984375
Iteration 800: Loss = -12457.8759765625
Iteration 900: Loss = -12448.9541015625
Iteration 1000: Loss = -12443.8662109375
Iteration 1100: Loss = -12440.2021484375
Iteration 1200: Loss = -12437.4423828125
Iteration 1300: Loss = -12435.5107421875
Iteration 1400: Loss = -12434.0126953125
Iteration 1500: Loss = -12432.8193359375
Iteration 1600: Loss = -12431.849609375
Iteration 1700: Loss = -12431.05859375
Iteration 1800: Loss = -12430.3984375
Iteration 1900: Loss = -12429.8564453125
Iteration 2000: Loss = -12429.40234375
Iteration 2100: Loss = -12429.0224609375
Iteration 2200: Loss = -12428.7021484375
Iteration 2300: Loss = -12428.4287109375
Iteration 2400: Loss = -12428.1962890625
Iteration 2500: Loss = -12427.9970703125
Iteration 2600: Loss = -12427.8232421875
Iteration 2700: Loss = -12427.6748046875
Iteration 2800: Loss = -12427.5439453125
Iteration 2900: Loss = -12427.4326171875
Iteration 3000: Loss = -12427.3291015625
Iteration 3100: Loss = -12427.2412109375
Iteration 3200: Loss = -12427.162109375
Iteration 3300: Loss = -12427.09375
Iteration 3400: Loss = -12427.033203125
Iteration 3500: Loss = -12426.9775390625
Iteration 3600: Loss = -12426.927734375
Iteration 3700: Loss = -12426.8837890625
Iteration 3800: Loss = -12426.84375
Iteration 3900: Loss = -12426.8076171875
Iteration 4000: Loss = -12426.7734375
Iteration 4100: Loss = -12426.744140625
Iteration 4200: Loss = -12426.71484375
Iteration 4300: Loss = -12426.6904296875
Iteration 4400: Loss = -12426.6669921875
Iteration 4500: Loss = -12426.646484375
Iteration 4600: Loss = -12426.6259765625
Iteration 4700: Loss = -12426.607421875
Iteration 4800: Loss = -12426.5888671875
Iteration 4900: Loss = -12426.572265625
Iteration 5000: Loss = -12426.5556640625
Iteration 5100: Loss = -12426.5380859375
Iteration 5200: Loss = -12426.521484375
Iteration 5300: Loss = -12426.5029296875
Iteration 5400: Loss = -12426.486328125
Iteration 5500: Loss = -12426.4677734375
Iteration 5600: Loss = -12426.4482421875
Iteration 5700: Loss = -12426.427734375
Iteration 5800: Loss = -12426.4072265625
Iteration 5900: Loss = -12426.3857421875
Iteration 6000: Loss = -12426.3662109375
Iteration 6100: Loss = -12426.345703125
Iteration 6200: Loss = -12426.328125
Iteration 6300: Loss = -12426.30859375
Iteration 6400: Loss = -12426.2900390625
Iteration 6500: Loss = -12426.267578125
Iteration 6600: Loss = -12426.2470703125
Iteration 6700: Loss = -12426.220703125
Iteration 6800: Loss = -12426.1953125
Iteration 6900: Loss = -12426.1650390625
Iteration 7000: Loss = -12426.1328125
Iteration 7100: Loss = -12426.1044921875
Iteration 7200: Loss = -12426.076171875
Iteration 7300: Loss = -12426.0537109375
Iteration 7400: Loss = -12426.0341796875
Iteration 7500: Loss = -12426.0185546875
Iteration 7600: Loss = -12426.005859375
Iteration 7700: Loss = -12425.99609375
Iteration 7800: Loss = -12425.9892578125
Iteration 7900: Loss = -12425.984375
Iteration 8000: Loss = -12425.9775390625
Iteration 8100: Loss = -12425.9736328125
Iteration 8200: Loss = -12425.96875
Iteration 8300: Loss = -12425.9658203125
Iteration 8400: Loss = -12425.962890625
Iteration 8500: Loss = -12425.958984375
Iteration 8600: Loss = -12425.955078125
Iteration 8700: Loss = -12425.9521484375
Iteration 8800: Loss = -12425.9462890625
Iteration 8900: Loss = -12425.9453125
Iteration 9000: Loss = -12425.94140625
Iteration 9100: Loss = -12425.9375
Iteration 9200: Loss = -12425.931640625
Iteration 9300: Loss = -12425.9287109375
Iteration 9400: Loss = -12425.9228515625
Iteration 9500: Loss = -12425.9169921875
Iteration 9600: Loss = -12425.9111328125
Iteration 9700: Loss = -12425.904296875
Iteration 9800: Loss = -12425.89453125
Iteration 9900: Loss = -12425.8876953125
Iteration 10000: Loss = -12425.8701171875
Iteration 10100: Loss = -12425.8515625
Iteration 10200: Loss = -12425.826171875
Iteration 10300: Loss = -12425.796875
Iteration 10400: Loss = -12425.7666015625
Iteration 10500: Loss = -12425.740234375
Iteration 10600: Loss = -12425.7158203125
Iteration 10700: Loss = -12425.6943359375
Iteration 10800: Loss = -12425.671875
Iteration 10900: Loss = -12425.646484375
Iteration 11000: Loss = -12425.619140625
Iteration 11100: Loss = -12425.5966796875
Iteration 11200: Loss = -12425.5771484375
Iteration 11300: Loss = -12425.564453125
Iteration 11400: Loss = -12425.5546875
Iteration 11500: Loss = -12425.546875
Iteration 11600: Loss = -12425.5419921875
Iteration 11700: Loss = -12425.5380859375
Iteration 11800: Loss = -12425.53515625
Iteration 11900: Loss = -12425.5322265625
Iteration 12000: Loss = -12425.5283203125
Iteration 12100: Loss = -12425.52734375
Iteration 12200: Loss = -12425.5244140625
Iteration 12300: Loss = -12425.5234375
Iteration 12400: Loss = -12425.521484375
Iteration 12500: Loss = -12425.5205078125
Iteration 12600: Loss = -12425.5205078125
Iteration 12700: Loss = -12425.5185546875
Iteration 12800: Loss = -12425.5185546875
Iteration 12900: Loss = -12425.517578125
Iteration 13000: Loss = -12425.515625
Iteration 13100: Loss = -12425.515625
Iteration 13200: Loss = -12425.515625
Iteration 13300: Loss = -12425.513671875
Iteration 13400: Loss = -12425.513671875
Iteration 13500: Loss = -12425.513671875
Iteration 13600: Loss = -12425.5126953125
Iteration 13700: Loss = -12425.51171875
Iteration 13800: Loss = -12425.5126953125
1
Iteration 13900: Loss = -12425.51171875
Iteration 14000: Loss = -12425.51171875
Iteration 14100: Loss = -12425.5107421875
Iteration 14200: Loss = -12425.51171875
1
Iteration 14300: Loss = -12425.5107421875
Iteration 14400: Loss = -12425.5107421875
Iteration 14500: Loss = -12425.5107421875
Iteration 14600: Loss = -12425.5087890625
Iteration 14700: Loss = -12425.5087890625
Iteration 14800: Loss = -12425.5087890625
Iteration 14900: Loss = -12425.509765625
1
Iteration 15000: Loss = -12425.5087890625
Iteration 15100: Loss = -12425.5078125
Iteration 15200: Loss = -12425.5087890625
1
Iteration 15300: Loss = -12425.5087890625
2
Iteration 15400: Loss = -12425.509765625
3
Iteration 15500: Loss = -12425.5078125
Iteration 15600: Loss = -12425.5087890625
1
Iteration 15700: Loss = -12425.5068359375
Iteration 15800: Loss = -12425.5068359375
Iteration 15900: Loss = -12425.5078125
1
Iteration 16000: Loss = -12425.5068359375
Iteration 16100: Loss = -12425.505859375
Iteration 16200: Loss = -12425.505859375
Iteration 16300: Loss = -12425.505859375
Iteration 16400: Loss = -12425.505859375
Iteration 16500: Loss = -12425.5048828125
Iteration 16600: Loss = -12425.5048828125
Iteration 16700: Loss = -12425.5068359375
1
Iteration 16800: Loss = -12425.505859375
2
Iteration 16900: Loss = -12425.5068359375
3
Iteration 17000: Loss = -12425.5048828125
Iteration 17100: Loss = -12425.50390625
Iteration 17200: Loss = -12425.5048828125
1
Iteration 17300: Loss = -12425.5048828125
2
Iteration 17400: Loss = -12425.5048828125
3
Iteration 17500: Loss = -12425.50390625
Iteration 17600: Loss = -12425.505859375
1
Iteration 17700: Loss = -12425.5048828125
2
Iteration 17800: Loss = -12425.5048828125
3
Iteration 17900: Loss = -12425.505859375
4
Iteration 18000: Loss = -12425.5048828125
5
Iteration 18100: Loss = -12425.50390625
Iteration 18200: Loss = -12425.5048828125
1
Iteration 18300: Loss = -12425.5048828125
2
Iteration 18400: Loss = -12425.5078125
3
Iteration 18500: Loss = -12425.50390625
Iteration 18600: Loss = -12425.505859375
1
Iteration 18700: Loss = -12425.5048828125
2
Iteration 18800: Loss = -12425.50390625
Iteration 18900: Loss = -12425.5048828125
1
Iteration 19000: Loss = -12425.505859375
2
Iteration 19100: Loss = -12425.5048828125
3
Iteration 19200: Loss = -12425.5068359375
4
Iteration 19300: Loss = -12425.5048828125
5
Iteration 19400: Loss = -12425.505859375
6
Iteration 19500: Loss = -12425.505859375
7
Iteration 19600: Loss = -12425.5048828125
8
Iteration 19700: Loss = -12425.5048828125
9
Iteration 19800: Loss = -12425.5048828125
10
Iteration 19900: Loss = -12425.505859375
11
Iteration 20000: Loss = -12425.5048828125
12
Iteration 20100: Loss = -12425.5048828125
13
Iteration 20200: Loss = -12425.5048828125
14
Iteration 20300: Loss = -12425.505859375
15
Stopping early at iteration 20300 due to no improvement.
pi: tensor([[4.8256e-01, 5.1744e-01],
        [2.9196e-04, 9.9971e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9922e-01, 7.7710e-04], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1971, 0.1996],
         [0.2070, 0.2035]],

        [[0.0480, 0.1959],
         [0.1127, 0.6033]],

        [[0.4801, 0.2034],
         [0.0879, 0.6891]],

        [[0.0145, 0.2121],
         [0.9906, 0.0448]],

        [[0.0094, 0.1934],
         [0.0404, 0.1549]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.010101010101010102
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0016083947137397405
Average Adjusted Rand Index: -0.00202020202020202
[-0.0011231119832131045, -0.0016083947137397405] [-0.0022793027514080026, -0.00202020202020202] [12423.2099609375, 12425.505859375]
-------------------------------------
This iteration is 60
True Objective function: Loss = -11862.774084433191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -40865.59375
Iteration 100: Loss = -23441.10546875
Iteration 200: Loss = -14835.2431640625
Iteration 300: Loss = -13216.615234375
Iteration 400: Loss = -12858.5078125
Iteration 500: Loss = -12674.099609375
Iteration 600: Loss = -12577.52734375
Iteration 700: Loss = -12520.0341796875
Iteration 800: Loss = -12478.728515625
Iteration 900: Loss = -12458.7236328125
Iteration 1000: Loss = -12434.8232421875
Iteration 1100: Loss = -12416.9619140625
Iteration 1200: Loss = -12405.240234375
Iteration 1300: Loss = -12398.345703125
Iteration 1400: Loss = -12388.1630859375
Iteration 1500: Loss = -12380.240234375
Iteration 1600: Loss = -12375.6923828125
Iteration 1700: Loss = -12371.130859375
Iteration 1800: Loss = -12367.515625
Iteration 1900: Loss = -12365.166015625
Iteration 2000: Loss = -12363.0908203125
Iteration 2100: Loss = -12360.068359375
Iteration 2200: Loss = -12346.9111328125
Iteration 2300: Loss = -12345.23046875
Iteration 2400: Loss = -12344.1142578125
Iteration 2500: Loss = -12341.623046875
Iteration 2600: Loss = -12340.5029296875
Iteration 2700: Loss = -12339.7451171875
Iteration 2800: Loss = -12339.099609375
Iteration 2900: Loss = -12338.5244140625
Iteration 3000: Loss = -12338.0048828125
Iteration 3100: Loss = -12337.533203125
Iteration 3200: Loss = -12337.10546875
Iteration 3300: Loss = -12336.7158203125
Iteration 3400: Loss = -12336.3564453125
Iteration 3500: Loss = -12336.0244140625
Iteration 3600: Loss = -12335.716796875
Iteration 3700: Loss = -12335.4345703125
Iteration 3800: Loss = -12335.169921875
Iteration 3900: Loss = -12334.92578125
Iteration 4000: Loss = -12334.697265625
Iteration 4100: Loss = -12334.484375
Iteration 4200: Loss = -12334.2841796875
Iteration 4300: Loss = -12334.095703125
Iteration 4400: Loss = -12333.919921875
Iteration 4500: Loss = -12333.7529296875
Iteration 4600: Loss = -12333.5966796875
Iteration 4700: Loss = -12333.4482421875
Iteration 4800: Loss = -12333.30859375
Iteration 4900: Loss = -12333.173828125
Iteration 5000: Loss = -12327.8134765625
Iteration 5100: Loss = -12326.9677734375
Iteration 5200: Loss = -12326.763671875
Iteration 5300: Loss = -12326.6259765625
Iteration 5400: Loss = -12326.5185546875
Iteration 5500: Loss = -12326.4248046875
Iteration 5600: Loss = -12326.341796875
Iteration 5700: Loss = -12326.267578125
Iteration 5800: Loss = -12326.201171875
Iteration 5900: Loss = -12326.1416015625
Iteration 6000: Loss = -12326.0849609375
Iteration 6100: Loss = -12326.033203125
Iteration 6200: Loss = -12325.9853515625
Iteration 6300: Loss = -12325.94140625
Iteration 6400: Loss = -12325.8994140625
Iteration 6500: Loss = -12325.8603515625
Iteration 6600: Loss = -12325.82421875
Iteration 6700: Loss = -12325.7890625
Iteration 6800: Loss = -12325.7587890625
Iteration 6900: Loss = -12325.7265625
Iteration 7000: Loss = -12325.6982421875
Iteration 7100: Loss = -12325.6708984375
Iteration 7200: Loss = -12325.6435546875
Iteration 7300: Loss = -12325.6201171875
Iteration 7400: Loss = -12325.5947265625
Iteration 7500: Loss = -12325.572265625
Iteration 7600: Loss = -12325.5498046875
Iteration 7700: Loss = -12325.236328125
Iteration 7800: Loss = -12320.375
Iteration 7900: Loss = -12320.2109375
Iteration 8000: Loss = -12320.12890625
Iteration 8100: Loss = -12320.0693359375
Iteration 8200: Loss = -12320.025390625
Iteration 8300: Loss = -12319.986328125
Iteration 8400: Loss = -12319.9521484375
Iteration 8500: Loss = -12319.9248046875
Iteration 8600: Loss = -12319.900390625
Iteration 8700: Loss = -12319.876953125
Iteration 8800: Loss = -12319.85546875
Iteration 8900: Loss = -12319.8349609375
Iteration 9000: Loss = -12319.818359375
Iteration 9100: Loss = -12319.8017578125
Iteration 9200: Loss = -12319.7880859375
Iteration 9300: Loss = -12319.771484375
Iteration 9400: Loss = -12319.7587890625
Iteration 9500: Loss = -12319.7470703125
Iteration 9600: Loss = -12319.7333984375
Iteration 9700: Loss = -12319.7236328125
Iteration 9800: Loss = -12319.7119140625
Iteration 9900: Loss = -12319.703125
Iteration 10000: Loss = -12319.6943359375
Iteration 10100: Loss = -12319.685546875
Iteration 10200: Loss = -12319.677734375
Iteration 10300: Loss = -12319.6689453125
Iteration 10400: Loss = -12319.662109375
Iteration 10500: Loss = -12319.6552734375
Iteration 10600: Loss = -12319.6494140625
Iteration 10700: Loss = -12319.6416015625
Iteration 10800: Loss = -12319.6357421875
Iteration 10900: Loss = -12319.630859375
Iteration 11000: Loss = -12319.623046875
Iteration 11100: Loss = -12319.619140625
Iteration 11200: Loss = -12319.615234375
Iteration 11300: Loss = -12319.609375
Iteration 11400: Loss = -12319.603515625
Iteration 11500: Loss = -12319.6005859375
Iteration 11600: Loss = -12319.59765625
Iteration 11700: Loss = -12319.59375
Iteration 11800: Loss = -12319.58984375
Iteration 11900: Loss = -12319.5859375
Iteration 12000: Loss = -12319.5830078125
Iteration 12100: Loss = -12319.5791015625
Iteration 12200: Loss = -12319.576171875
Iteration 12300: Loss = -12319.5732421875
Iteration 12400: Loss = -12319.5712890625
Iteration 12500: Loss = -12319.568359375
Iteration 12600: Loss = -12319.56640625
Iteration 12700: Loss = -12319.5634765625
Iteration 12800: Loss = -12319.5615234375
Iteration 12900: Loss = -12319.5595703125
Iteration 13000: Loss = -12319.55859375
Iteration 13100: Loss = -12319.556640625
Iteration 13200: Loss = -12319.5537109375
Iteration 13300: Loss = -12319.5546875
1
Iteration 13400: Loss = -12319.55078125
Iteration 13500: Loss = -12319.5498046875
Iteration 13600: Loss = -12319.5478515625
Iteration 13700: Loss = -12319.5478515625
Iteration 13800: Loss = -12319.5458984375
Iteration 13900: Loss = -12319.5439453125
Iteration 14000: Loss = -12319.5439453125
Iteration 14100: Loss = -12319.5419921875
Iteration 14200: Loss = -12319.541015625
Iteration 14300: Loss = -12319.5400390625
Iteration 14400: Loss = -12319.5380859375
Iteration 14500: Loss = -12319.5380859375
Iteration 14600: Loss = -12319.5361328125
Iteration 14700: Loss = -12319.537109375
1
Iteration 14800: Loss = -12319.537109375
2
Iteration 14900: Loss = -12319.53515625
Iteration 15000: Loss = -12319.5341796875
Iteration 15100: Loss = -12319.53515625
1
Iteration 15200: Loss = -12319.533203125
Iteration 15300: Loss = -12319.5322265625
Iteration 15400: Loss = -12319.5322265625
Iteration 15500: Loss = -12319.53125
Iteration 15600: Loss = -12319.529296875
Iteration 15700: Loss = -12319.529296875
Iteration 15800: Loss = -12319.529296875
Iteration 15900: Loss = -12319.5283203125
Iteration 16000: Loss = -12319.5283203125
Iteration 16100: Loss = -12319.5283203125
Iteration 16200: Loss = -12319.5263671875
Iteration 16300: Loss = -12319.5263671875
Iteration 16400: Loss = -12319.5244140625
Iteration 16500: Loss = -12319.5234375
Iteration 16600: Loss = -12319.5224609375
Iteration 16700: Loss = -12319.5224609375
Iteration 16800: Loss = -12319.5185546875
Iteration 16900: Loss = -12319.517578125
Iteration 17000: Loss = -12318.91796875
Iteration 17100: Loss = -12318.6484375
Iteration 17200: Loss = -12318.353515625
Iteration 17300: Loss = -12318.1513671875
Iteration 17400: Loss = -12318.11328125
Iteration 17500: Loss = -12318.0625
Iteration 17600: Loss = -12318.0458984375
Iteration 17700: Loss = -12318.0439453125
Iteration 17800: Loss = -12318.0439453125
Iteration 17900: Loss = -12318.044921875
1
Iteration 18000: Loss = -12318.0419921875
Iteration 18100: Loss = -12318.0419921875
Iteration 18200: Loss = -12318.0419921875
Iteration 18300: Loss = -12318.0419921875
Iteration 18400: Loss = -12318.0419921875
Iteration 18500: Loss = -12318.0400390625
Iteration 18600: Loss = -12318.041015625
1
Iteration 18700: Loss = -12318.041015625
2
Iteration 18800: Loss = -12318.0400390625
Iteration 18900: Loss = -12318.0400390625
Iteration 19000: Loss = -12318.0390625
Iteration 19100: Loss = -12318.0341796875
Iteration 19200: Loss = -12318.0341796875
Iteration 19300: Loss = -12318.033203125
Iteration 19400: Loss = -12318.0341796875
1
Iteration 19500: Loss = -12318.033203125
Iteration 19600: Loss = -12318.0322265625
Iteration 19700: Loss = -12318.0322265625
Iteration 19800: Loss = -12318.0322265625
Iteration 19900: Loss = -12318.0322265625
Iteration 20000: Loss = -12318.0322265625
Iteration 20100: Loss = -12318.0322265625
Iteration 20200: Loss = -12318.0322265625
Iteration 20300: Loss = -12318.0322265625
Iteration 20400: Loss = -12318.033203125
1
Iteration 20500: Loss = -12318.0322265625
Iteration 20600: Loss = -12318.0322265625
Iteration 20700: Loss = -12318.0341796875
1
Iteration 20800: Loss = -12318.033203125
2
Iteration 20900: Loss = -12318.033203125
3
Iteration 21000: Loss = -12318.03125
Iteration 21100: Loss = -12318.0302734375
Iteration 21200: Loss = -12318.03125
1
Iteration 21300: Loss = -12318.03125
2
Iteration 21400: Loss = -12318.0302734375
Iteration 21500: Loss = -12318.0322265625
1
Iteration 21600: Loss = -12318.0322265625
2
Iteration 21700: Loss = -12318.03125
3
Iteration 21800: Loss = -12318.03125
4
Iteration 21900: Loss = -12318.03125
5
Iteration 22000: Loss = -12318.03125
6
Iteration 22100: Loss = -12318.0322265625
7
Iteration 22200: Loss = -12318.0302734375
Iteration 22300: Loss = -12318.03515625
1
Iteration 22400: Loss = -12318.033203125
2
Iteration 22500: Loss = -12318.03125
3
Iteration 22600: Loss = -12318.03125
4
Iteration 22700: Loss = -12318.0283203125
Iteration 22800: Loss = -12318.0263671875
Iteration 22900: Loss = -12318.02734375
1
Iteration 23000: Loss = -12318.0263671875
Iteration 23100: Loss = -12318.025390625
Iteration 23200: Loss = -12317.8076171875
Iteration 23300: Loss = -12317.3134765625
Iteration 23400: Loss = -12317.0224609375
Iteration 23500: Loss = -12316.982421875
Iteration 23600: Loss = -12316.96484375
Iteration 23700: Loss = -12316.4873046875
Iteration 23800: Loss = -12316.4658203125
Iteration 23900: Loss = -12316.4580078125
Iteration 24000: Loss = -12316.4638671875
1
Iteration 24100: Loss = -12316.451171875
Iteration 24200: Loss = -12316.44921875
Iteration 24300: Loss = -12316.447265625
Iteration 24400: Loss = -12316.4453125
Iteration 24500: Loss = -12316.4453125
Iteration 24600: Loss = -12315.6640625
Iteration 24700: Loss = -12315.6572265625
Iteration 24800: Loss = -12315.658203125
1
Iteration 24900: Loss = -12315.6552734375
Iteration 25000: Loss = -12315.6533203125
Iteration 25100: Loss = -12315.654296875
1
Iteration 25200: Loss = -12315.65234375
Iteration 25300: Loss = -12315.6533203125
1
Iteration 25400: Loss = -12315.6533203125
2
Iteration 25500: Loss = -12315.6513671875
Iteration 25600: Loss = -12315.6533203125
1
Iteration 25700: Loss = -12315.65234375
2
Iteration 25800: Loss = -12315.650390625
Iteration 25900: Loss = -12315.6513671875
1
Iteration 26000: Loss = -12315.65234375
2
Iteration 26100: Loss = -12315.65234375
3
Iteration 26200: Loss = -12315.650390625
Iteration 26300: Loss = -12315.65234375
1
Iteration 26400: Loss = -12315.6513671875
2
Iteration 26500: Loss = -12315.6494140625
Iteration 26600: Loss = -12315.6513671875
1
Iteration 26700: Loss = -12315.6513671875
2
Iteration 26800: Loss = -12315.6513671875
3
Iteration 26900: Loss = -12315.65234375
4
Iteration 27000: Loss = -12315.6494140625
Iteration 27100: Loss = -12315.6513671875
1
Iteration 27200: Loss = -12315.650390625
2
Iteration 27300: Loss = -12315.6494140625
Iteration 27400: Loss = -12315.650390625
1
Iteration 27500: Loss = -12315.6494140625
Iteration 27600: Loss = -12315.6513671875
1
Iteration 27700: Loss = -12315.6513671875
2
Iteration 27800: Loss = -12315.65234375
3
Iteration 27900: Loss = -12315.6494140625
Iteration 28000: Loss = -12315.6494140625
Iteration 28100: Loss = -12315.650390625
1
Iteration 28200: Loss = -12315.6494140625
Iteration 28300: Loss = -12315.6513671875
1
Iteration 28400: Loss = -12315.650390625
2
Iteration 28500: Loss = -12315.6494140625
Iteration 28600: Loss = -12315.6494140625
Iteration 28700: Loss = -12315.6484375
Iteration 28800: Loss = -12315.6513671875
1
Iteration 28900: Loss = -12315.6513671875
2
Iteration 29000: Loss = -12315.6513671875
3
Iteration 29100: Loss = -12315.650390625
4
Iteration 29200: Loss = -12315.6494140625
5
Iteration 29300: Loss = -12315.650390625
6
Iteration 29400: Loss = -12315.6494140625
7
Iteration 29500: Loss = -12315.6494140625
8
Iteration 29600: Loss = -12315.650390625
9
Iteration 29700: Loss = -12315.6513671875
10
Iteration 29800: Loss = -12315.6513671875
11
Iteration 29900: Loss = -12315.6494140625
12
pi: tensor([[9.9997e-01, 3.2024e-05],
        [2.9837e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0356, 0.9644], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[4.7956e-06, 1.5058e-01],
         [2.0672e-01, 1.9876e-01]],

        [[9.8020e-01, 1.4260e-01],
         [8.4167e-01, 9.7784e-01]],

        [[9.6915e-01, 2.6334e-01],
         [9.6750e-01, 9.0209e-01]],

        [[9.3099e-01, 2.3565e-01],
         [2.1128e-01, 9.8402e-01]],

        [[8.8247e-01, 1.6797e-01],
         [6.7891e-03, 1.4298e-01]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 4.85601903559462e-05
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.007419103651150718
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.003506908785360844
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.0009975514204148314
Global Adjusted Rand Index: -5.183158583483814e-05
Average Adjusted Rand Index: 0.0009671567202241873
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29804.916015625
Iteration 100: Loss = -19873.732421875
Iteration 200: Loss = -14196.7958984375
Iteration 300: Loss = -13048.6083984375
Iteration 400: Loss = -12707.5
Iteration 500: Loss = -12548.7001953125
Iteration 600: Loss = -12472.802734375
Iteration 700: Loss = -12435.0166015625
Iteration 800: Loss = -12414.8671875
Iteration 900: Loss = -12402.6083984375
Iteration 1000: Loss = -12393.900390625
Iteration 1100: Loss = -12386.48828125
Iteration 1200: Loss = -12381.8994140625
Iteration 1300: Loss = -12377.8935546875
Iteration 1400: Loss = -12373.6328125
Iteration 1500: Loss = -12370.46875
Iteration 1600: Loss = -12366.8046875
Iteration 1700: Loss = -12362.6630859375
Iteration 1800: Loss = -12358.638671875
Iteration 1900: Loss = -12355.9287109375
Iteration 2000: Loss = -12353.51171875
Iteration 2100: Loss = -12350.1787109375
Iteration 2200: Loss = -12347.4267578125
Iteration 2300: Loss = -12345.4931640625
Iteration 2400: Loss = -12343.541015625
Iteration 2500: Loss = -12341.4873046875
Iteration 2600: Loss = -12339.0263671875
Iteration 2700: Loss = -12336.65234375
Iteration 2800: Loss = -12334.8974609375
Iteration 2900: Loss = -12333.3232421875
Iteration 3000: Loss = -12331.3720703125
Iteration 3100: Loss = -12329.349609375
Iteration 3200: Loss = -12326.4267578125
Iteration 3300: Loss = -12325.353515625
Iteration 3400: Loss = -12324.3642578125
Iteration 3500: Loss = -12323.439453125
Iteration 3600: Loss = -12322.7734375
Iteration 3700: Loss = -12322.2939453125
Iteration 3800: Loss = -12321.923828125
Iteration 3900: Loss = -12321.626953125
Iteration 4000: Loss = -12321.3779296875
Iteration 4100: Loss = -12321.162109375
Iteration 4200: Loss = -12320.9755859375
Iteration 4300: Loss = -12320.8125
Iteration 4400: Loss = -12320.6669921875
Iteration 4500: Loss = -12320.5341796875
Iteration 4600: Loss = -12320.4140625
Iteration 4700: Loss = -12320.3056640625
Iteration 4800: Loss = -12320.2060546875
Iteration 4900: Loss = -12320.1123046875
Iteration 5000: Loss = -12320.0283203125
Iteration 5100: Loss = -12319.9462890625
Iteration 5200: Loss = -12319.8720703125
Iteration 5300: Loss = -12319.8046875
Iteration 5400: Loss = -12319.73828125
Iteration 5500: Loss = -12319.677734375
Iteration 5600: Loss = -12319.619140625
Iteration 5700: Loss = -12319.56640625
Iteration 5800: Loss = -12319.517578125
Iteration 5900: Loss = -12319.46875
Iteration 6000: Loss = -12319.423828125
Iteration 6100: Loss = -12319.3798828125
Iteration 6200: Loss = -12319.337890625
Iteration 6300: Loss = -12319.296875
Iteration 6400: Loss = -12319.2548828125
Iteration 6500: Loss = -12319.2177734375
Iteration 6600: Loss = -12319.1826171875
Iteration 6700: Loss = -12319.1494140625
Iteration 6800: Loss = -12319.1201171875
Iteration 6900: Loss = -12319.091796875
Iteration 7000: Loss = -12319.0654296875
Iteration 7100: Loss = -12319.0400390625
Iteration 7200: Loss = -12319.0166015625
Iteration 7300: Loss = -12318.994140625
Iteration 7400: Loss = -12318.9736328125
Iteration 7500: Loss = -12318.953125
Iteration 7600: Loss = -12318.93359375
Iteration 7700: Loss = -12318.9150390625
Iteration 7800: Loss = -12318.8974609375
Iteration 7900: Loss = -12318.8818359375
Iteration 8000: Loss = -12318.8671875
Iteration 8100: Loss = -12318.8544921875
Iteration 8200: Loss = -12318.841796875
Iteration 8300: Loss = -12318.830078125
Iteration 8400: Loss = -12318.8193359375
Iteration 8500: Loss = -12318.810546875
Iteration 8600: Loss = -12318.80078125
Iteration 8700: Loss = -12318.79296875
Iteration 8800: Loss = -12318.7841796875
Iteration 8900: Loss = -12318.7763671875
Iteration 9000: Loss = -12318.76953125
Iteration 9100: Loss = -12318.763671875
Iteration 9200: Loss = -12318.755859375
Iteration 9300: Loss = -12318.7509765625
Iteration 9400: Loss = -12318.744140625
Iteration 9500: Loss = -12318.740234375
Iteration 9600: Loss = -12318.732421875
Iteration 9700: Loss = -12318.728515625
Iteration 9800: Loss = -12318.7236328125
Iteration 9900: Loss = -12318.7197265625
Iteration 10000: Loss = -12318.7158203125
Iteration 10100: Loss = -12318.7109375
Iteration 10200: Loss = -12318.70703125
Iteration 10300: Loss = -12318.703125
Iteration 10400: Loss = -12318.701171875
Iteration 10500: Loss = -12318.6953125
Iteration 10600: Loss = -12318.6943359375
Iteration 10700: Loss = -12318.689453125
Iteration 10800: Loss = -12318.6875
Iteration 10900: Loss = -12318.6845703125
Iteration 11000: Loss = -12318.6826171875
Iteration 11100: Loss = -12318.6806640625
Iteration 11200: Loss = -12318.677734375
Iteration 11300: Loss = -12318.671875
Iteration 11400: Loss = -12318.6708984375
Iteration 11500: Loss = -12318.6689453125
Iteration 11600: Loss = -12318.6689453125
Iteration 11700: Loss = -12318.6650390625
Iteration 11800: Loss = -12318.6640625
Iteration 11900: Loss = -12318.662109375
Iteration 12000: Loss = -12318.6611328125
Iteration 12100: Loss = -12318.6591796875
Iteration 12200: Loss = -12318.658203125
Iteration 12300: Loss = -12318.6552734375
Iteration 12400: Loss = -12318.654296875
Iteration 12500: Loss = -12318.654296875
Iteration 12600: Loss = -12318.6513671875
Iteration 12700: Loss = -12318.650390625
Iteration 12800: Loss = -12318.6484375
Iteration 12900: Loss = -12318.6494140625
1
Iteration 13000: Loss = -12318.646484375
Iteration 13100: Loss = -12318.6474609375
1
Iteration 13200: Loss = -12318.6455078125
Iteration 13300: Loss = -12318.64453125
Iteration 13400: Loss = -12318.64453125
Iteration 13500: Loss = -12318.642578125
Iteration 13600: Loss = -12318.642578125
Iteration 13700: Loss = -12318.640625
Iteration 13800: Loss = -12318.642578125
1
Iteration 13900: Loss = -12318.640625
Iteration 14000: Loss = -12318.6396484375
Iteration 14100: Loss = -12318.6376953125
Iteration 14200: Loss = -12318.638671875
1
Iteration 14300: Loss = -12318.638671875
2
Iteration 14400: Loss = -12318.63671875
Iteration 14500: Loss = -12318.63671875
Iteration 14600: Loss = -12318.6357421875
Iteration 14700: Loss = -12318.634765625
Iteration 14800: Loss = -12318.6357421875
1
Iteration 14900: Loss = -12318.634765625
Iteration 15000: Loss = -12318.634765625
Iteration 15100: Loss = -12318.634765625
Iteration 15200: Loss = -12318.6337890625
Iteration 15300: Loss = -12318.6318359375
Iteration 15400: Loss = -12318.6337890625
1
Iteration 15500: Loss = -12318.6328125
2
Iteration 15600: Loss = -12318.6328125
3
Iteration 15700: Loss = -12318.6318359375
Iteration 15800: Loss = -12318.6318359375
Iteration 15900: Loss = -12318.6328125
1
Iteration 16000: Loss = -12318.630859375
Iteration 16100: Loss = -12318.630859375
Iteration 16200: Loss = -12318.6318359375
1
Iteration 16300: Loss = -12318.630859375
Iteration 16400: Loss = -12318.630859375
Iteration 16500: Loss = -12318.6298828125
Iteration 16600: Loss = -12318.630859375
1
Iteration 16700: Loss = -12318.6298828125
Iteration 16800: Loss = -12318.6298828125
Iteration 16900: Loss = -12318.6298828125
Iteration 17000: Loss = -12318.62890625
Iteration 17100: Loss = -12318.6298828125
1
Iteration 17200: Loss = -12318.62890625
Iteration 17300: Loss = -12318.62890625
Iteration 17400: Loss = -12318.6298828125
1
Iteration 17500: Loss = -12318.6298828125
2
Iteration 17600: Loss = -12318.630859375
3
Iteration 17700: Loss = -12318.6298828125
4
Iteration 17800: Loss = -12318.6298828125
5
Iteration 17900: Loss = -12318.62890625
Iteration 18000: Loss = -12318.6279296875
Iteration 18100: Loss = -12318.6298828125
1
Iteration 18200: Loss = -12318.62890625
2
Iteration 18300: Loss = -12318.62890625
3
Iteration 18400: Loss = -12318.6259765625
Iteration 18500: Loss = -12318.626953125
1
Iteration 18600: Loss = -12318.626953125
2
Iteration 18700: Loss = -12318.6279296875
3
Iteration 18800: Loss = -12318.626953125
4
Iteration 18900: Loss = -12318.62890625
5
Iteration 19000: Loss = -12318.626953125
6
Iteration 19100: Loss = -12318.6279296875
7
Iteration 19200: Loss = -12318.6279296875
8
Iteration 19300: Loss = -12318.626953125
9
Iteration 19400: Loss = -12318.6298828125
10
Iteration 19500: Loss = -12318.626953125
11
Iteration 19600: Loss = -12318.62890625
12
Iteration 19700: Loss = -12318.626953125
13
Iteration 19800: Loss = -12318.626953125
14
Iteration 19900: Loss = -12318.626953125
15
Stopping early at iteration 19900 due to no improvement.
pi: tensor([[9.8808e-01, 1.1917e-02],
        [1.0000e+00, 3.7648e-07]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([2.9561e-04, 9.9970e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1967, 0.2009],
         [0.3638, 0.2010]],

        [[0.0650, 0.7682],
         [0.9889, 0.0555]],

        [[0.6552, 0.3179],
         [0.9922, 0.4049]],

        [[0.0078, 0.2157],
         [0.9907, 0.6407]],

        [[0.1258, 0.1760],
         [0.3159, 0.0374]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 3.852673631893922e-06
Average Adjusted Rand Index: -0.0008569898232458489
[-5.183158583483814e-05, 3.852673631893922e-06] [0.0009671567202241873, -0.0008569898232458489] [12315.6513671875, 12318.626953125]
-------------------------------------
This iteration is 61
True Objective function: Loss = -12046.164832576182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27732.01953125
Iteration 100: Loss = -18117.572265625
Iteration 200: Loss = -13709.75390625
Iteration 300: Loss = -12832.1123046875
Iteration 400: Loss = -12658.7265625
Iteration 500: Loss = -12600.181640625
Iteration 600: Loss = -12570.849609375
Iteration 700: Loss = -12544.384765625
Iteration 800: Loss = -12523.740234375
Iteration 900: Loss = -12493.4384765625
Iteration 1000: Loss = -12464.9267578125
Iteration 1100: Loss = -12433.7880859375
Iteration 1200: Loss = -12376.138671875
Iteration 1300: Loss = -12327.318359375
Iteration 1400: Loss = -12300.8740234375
Iteration 1500: Loss = -12282.5810546875
Iteration 1600: Loss = -12267.7216796875
Iteration 1700: Loss = -12251.28515625
Iteration 1800: Loss = -12243.849609375
Iteration 1900: Loss = -12241.3642578125
Iteration 2000: Loss = -12239.9375
Iteration 2100: Loss = -12238.7568359375
Iteration 2200: Loss = -12224.1181640625
Iteration 2300: Loss = -12222.76953125
Iteration 2400: Loss = -12222.1181640625
Iteration 2500: Loss = -12221.64453125
Iteration 2600: Loss = -12221.2666015625
Iteration 2700: Loss = -12220.95703125
Iteration 2800: Loss = -12220.6923828125
Iteration 2900: Loss = -12220.46484375
Iteration 3000: Loss = -12220.2666015625
Iteration 3100: Loss = -12220.0888671875
Iteration 3200: Loss = -12219.9189453125
Iteration 3300: Loss = -12219.71484375
Iteration 3400: Loss = -12213.873046875
Iteration 3500: Loss = -12213.306640625
Iteration 3600: Loss = -12213.0283203125
Iteration 3700: Loss = -12212.8583984375
Iteration 3800: Loss = -12212.7353515625
Iteration 3900: Loss = -12212.6357421875
Iteration 4000: Loss = -12212.55078125
Iteration 4100: Loss = -12212.4775390625
Iteration 4200: Loss = -12212.4130859375
Iteration 4300: Loss = -12212.35546875
Iteration 4400: Loss = -12212.2998046875
Iteration 4500: Loss = -12212.251953125
Iteration 4600: Loss = -12212.2080078125
Iteration 4700: Loss = -12212.1669921875
Iteration 4800: Loss = -12212.130859375
Iteration 4900: Loss = -12212.0947265625
Iteration 5000: Loss = -12212.0634765625
Iteration 5100: Loss = -12212.033203125
Iteration 5200: Loss = -12212.0048828125
Iteration 5300: Loss = -12211.98046875
Iteration 5400: Loss = -12211.9560546875
Iteration 5500: Loss = -12211.931640625
Iteration 5600: Loss = -12211.8935546875
Iteration 5700: Loss = -12211.75390625
Iteration 5800: Loss = -12211.7275390625
Iteration 5900: Loss = -12211.708984375
Iteration 6000: Loss = -12211.69140625
Iteration 6100: Loss = -12211.6748046875
Iteration 6200: Loss = -12211.6591796875
Iteration 6300: Loss = -12211.6416015625
Iteration 6400: Loss = -12211.3232421875
Iteration 6500: Loss = -12210.90625
Iteration 6600: Loss = -12210.8857421875
Iteration 6700: Loss = -12210.873046875
Iteration 6800: Loss = -12210.8603515625
Iteration 6900: Loss = -12210.8515625
Iteration 7000: Loss = -12210.83984375
Iteration 7100: Loss = -12210.8310546875
Iteration 7200: Loss = -12210.8212890625
Iteration 7300: Loss = -12210.8125
Iteration 7400: Loss = -12210.8037109375
Iteration 7500: Loss = -12210.7958984375
Iteration 7600: Loss = -12210.7880859375
Iteration 7700: Loss = -12210.7822265625
Iteration 7800: Loss = -12210.7763671875
Iteration 7900: Loss = -12210.771484375
Iteration 8000: Loss = -12210.767578125
Iteration 8100: Loss = -12210.7607421875
Iteration 8200: Loss = -12210.7578125
Iteration 8300: Loss = -12210.7529296875
Iteration 8400: Loss = -12210.7490234375
Iteration 8500: Loss = -12210.74609375
Iteration 8600: Loss = -12210.7421875
Iteration 8700: Loss = -12210.73828125
Iteration 8800: Loss = -12210.736328125
Iteration 8900: Loss = -12210.7314453125
Iteration 9000: Loss = -12210.728515625
Iteration 9100: Loss = -12210.7265625
Iteration 9200: Loss = -12210.7236328125
Iteration 9300: Loss = -12210.7216796875
Iteration 9400: Loss = -12210.71875
Iteration 9500: Loss = -12210.716796875
Iteration 9600: Loss = -12210.71484375
Iteration 9700: Loss = -12210.712890625
Iteration 9800: Loss = -12210.7109375
Iteration 9900: Loss = -12210.708984375
Iteration 10000: Loss = -12210.7080078125
Iteration 10100: Loss = -12210.70703125
Iteration 10200: Loss = -12210.705078125
Iteration 10300: Loss = -12210.7021484375
Iteration 10400: Loss = -12210.701171875
Iteration 10500: Loss = -12210.701171875
Iteration 10600: Loss = -12210.69921875
Iteration 10700: Loss = -12210.6982421875
Iteration 10800: Loss = -12210.6982421875
Iteration 10900: Loss = -12210.6962890625
Iteration 11000: Loss = -12210.6943359375
Iteration 11100: Loss = -12210.6943359375
Iteration 11200: Loss = -12210.693359375
Iteration 11300: Loss = -12210.69140625
Iteration 11400: Loss = -12210.6904296875
Iteration 11500: Loss = -12210.6904296875
Iteration 11600: Loss = -12210.689453125
Iteration 11700: Loss = -12210.6884765625
Iteration 11800: Loss = -12210.6875
Iteration 11900: Loss = -12210.6875
Iteration 12000: Loss = -12210.6875
Iteration 12100: Loss = -12210.6865234375
Iteration 12200: Loss = -12210.6865234375
Iteration 12300: Loss = -12210.685546875
Iteration 12400: Loss = -12210.6845703125
Iteration 12500: Loss = -12210.6845703125
Iteration 12600: Loss = -12210.68359375
Iteration 12700: Loss = -12210.6845703125
1
Iteration 12800: Loss = -12210.68359375
Iteration 12900: Loss = -12210.68359375
Iteration 13000: Loss = -12210.681640625
Iteration 13100: Loss = -12210.681640625
Iteration 13200: Loss = -12210.6806640625
Iteration 13300: Loss = -12210.6806640625
Iteration 13400: Loss = -12210.6806640625
Iteration 13500: Loss = -12210.6796875
Iteration 13600: Loss = -12210.6796875
Iteration 13700: Loss = -12210.6787109375
Iteration 13800: Loss = -12210.6787109375
Iteration 13900: Loss = -12210.6796875
1
Iteration 14000: Loss = -12210.6787109375
Iteration 14100: Loss = -12210.677734375
Iteration 14200: Loss = -12210.677734375
Iteration 14300: Loss = -12210.6767578125
Iteration 14400: Loss = -12210.677734375
1
Iteration 14500: Loss = -12210.677734375
2
Iteration 14600: Loss = -12210.677734375
3
Iteration 14700: Loss = -12210.6767578125
Iteration 14800: Loss = -12210.6494140625
Iteration 14900: Loss = -12210.6474609375
Iteration 15000: Loss = -12210.6494140625
1
Iteration 15100: Loss = -12210.6484375
2
Iteration 15200: Loss = -12210.6484375
3
Iteration 15300: Loss = -12210.6484375
4
Iteration 15400: Loss = -12210.6474609375
Iteration 15500: Loss = -12210.646484375
Iteration 15600: Loss = -12210.6474609375
1
Iteration 15700: Loss = -12210.6474609375
2
Iteration 15800: Loss = -12210.646484375
Iteration 15900: Loss = -12210.646484375
Iteration 16000: Loss = -12210.6484375
1
Iteration 16100: Loss = -12210.6474609375
2
Iteration 16200: Loss = -12210.6474609375
3
Iteration 16300: Loss = -12210.6484375
4
Iteration 16400: Loss = -12210.646484375
Iteration 16500: Loss = -12210.64453125
Iteration 16600: Loss = -12210.64453125
Iteration 16700: Loss = -12210.64453125
Iteration 16800: Loss = -12210.6435546875
Iteration 16900: Loss = -12210.64453125
1
Iteration 17000: Loss = -12210.6455078125
2
Iteration 17100: Loss = -12210.64453125
3
Iteration 17200: Loss = -12210.6435546875
Iteration 17300: Loss = -12210.64453125
1
Iteration 17400: Loss = -12210.6435546875
Iteration 17500: Loss = -12210.646484375
1
Iteration 17600: Loss = -12210.6435546875
Iteration 17700: Loss = -12210.6435546875
Iteration 17800: Loss = -12210.6435546875
Iteration 17900: Loss = -12210.6455078125
1
Iteration 18000: Loss = -12210.64453125
2
Iteration 18100: Loss = -12210.64453125
3
Iteration 18200: Loss = -12210.6455078125
4
Iteration 18300: Loss = -12210.6435546875
Iteration 18400: Loss = -12210.6435546875
Iteration 18500: Loss = -12210.6416015625
Iteration 18600: Loss = -12210.6435546875
1
Iteration 18700: Loss = -12210.6435546875
2
Iteration 18800: Loss = -12210.6455078125
3
Iteration 18900: Loss = -12210.642578125
4
Iteration 19000: Loss = -12210.6435546875
5
Iteration 19100: Loss = -12210.64453125
6
Iteration 19200: Loss = -12210.642578125
7
Iteration 19300: Loss = -12210.6435546875
8
Iteration 19400: Loss = -12210.6435546875
9
Iteration 19500: Loss = -12210.6435546875
10
Iteration 19600: Loss = -12210.6435546875
11
Iteration 19700: Loss = -12210.6435546875
12
Iteration 19800: Loss = -12210.642578125
13
Iteration 19900: Loss = -12210.6435546875
14
Iteration 20000: Loss = -12210.64453125
15
Stopping early at iteration 20000 due to no improvement.
pi: tensor([[0.4912, 0.5088],
        [0.8578, 0.1422]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4957, 0.5043], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2519, 0.1058],
         [0.0305, 0.3060]],

        [[0.2208, 0.1071],
         [0.3384, 0.1334]],

        [[0.9429, 0.1023],
         [0.1842, 0.2399]],

        [[0.9598, 0.5013],
         [0.7839, 0.3314]],

        [[0.9704, 0.0996],
         [0.0711, 0.2588]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.1396453319292638
Average Adjusted Rand Index: 0.7526429841031932
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26165.564453125
Iteration 100: Loss = -19269.349609375
Iteration 200: Loss = -14181.177734375
Iteration 300: Loss = -12934.095703125
Iteration 400: Loss = -12707.0732421875
Iteration 500: Loss = -12623.5712890625
Iteration 600: Loss = -12581.9658203125
Iteration 700: Loss = -12556.2998046875
Iteration 800: Loss = -12540.5341796875
Iteration 900: Loss = -12528.9560546875
Iteration 1000: Loss = -12520.939453125
Iteration 1100: Loss = -12514.974609375
Iteration 1200: Loss = -12510.3544921875
Iteration 1300: Loss = -12506.689453125
Iteration 1400: Loss = -12503.72265625
Iteration 1500: Loss = -12501.28515625
Iteration 1600: Loss = -12499.2529296875
Iteration 1700: Loss = -12497.533203125
Iteration 1800: Loss = -12496.0625
Iteration 1900: Loss = -12494.791015625
Iteration 2000: Loss = -12493.685546875
Iteration 2100: Loss = -12492.7197265625
Iteration 2200: Loss = -12491.86328125
Iteration 2300: Loss = -12491.1005859375
Iteration 2400: Loss = -12490.4150390625
Iteration 2500: Loss = -12489.7958984375
Iteration 2600: Loss = -12489.2275390625
Iteration 2700: Loss = -12488.7041015625
Iteration 2800: Loss = -12488.21875
Iteration 2900: Loss = -12487.7744140625
Iteration 3000: Loss = -12487.36328125
Iteration 3100: Loss = -12486.9853515625
Iteration 3200: Loss = -12486.634765625
Iteration 3300: Loss = -12486.3095703125
Iteration 3400: Loss = -12486.005859375
Iteration 3500: Loss = -12485.72265625
Iteration 3600: Loss = -12485.4580078125
Iteration 3700: Loss = -12485.2109375
Iteration 3800: Loss = -12484.978515625
Iteration 3900: Loss = -12484.759765625
Iteration 4000: Loss = -12484.556640625
Iteration 4100: Loss = -12484.365234375
Iteration 4200: Loss = -12484.185546875
Iteration 4300: Loss = -12484.015625
Iteration 4400: Loss = -12483.8583984375
Iteration 4500: Loss = -12483.7109375
Iteration 4600: Loss = -12483.572265625
Iteration 4700: Loss = -12483.44140625
Iteration 4800: Loss = -12483.3203125
Iteration 4900: Loss = -12483.205078125
Iteration 5000: Loss = -12483.0986328125
Iteration 5100: Loss = -12482.9970703125
Iteration 5200: Loss = -12482.90234375
Iteration 5300: Loss = -12482.814453125
Iteration 5400: Loss = -12482.7294921875
Iteration 5500: Loss = -12482.6513671875
Iteration 5600: Loss = -12482.580078125
Iteration 5700: Loss = -12482.5107421875
Iteration 5800: Loss = -12482.447265625
Iteration 5900: Loss = -12482.3857421875
Iteration 6000: Loss = -12482.330078125
Iteration 6100: Loss = -12482.2783203125
Iteration 6200: Loss = -12482.228515625
Iteration 6300: Loss = -12482.181640625
Iteration 6400: Loss = -12482.138671875
Iteration 6500: Loss = -12482.09765625
Iteration 6600: Loss = -12482.060546875
Iteration 6700: Loss = -12482.0244140625
Iteration 6800: Loss = -12481.9892578125
Iteration 6900: Loss = -12481.95703125
Iteration 7000: Loss = -12481.9287109375
Iteration 7100: Loss = -12481.9013671875
Iteration 7200: Loss = -12481.8740234375
Iteration 7300: Loss = -12481.8486328125
Iteration 7400: Loss = -12481.8251953125
Iteration 7500: Loss = -12481.8046875
Iteration 7600: Loss = -12481.7822265625
Iteration 7700: Loss = -12481.7626953125
Iteration 7800: Loss = -12481.7451171875
Iteration 7900: Loss = -12481.7275390625
Iteration 8000: Loss = -12481.7099609375
Iteration 8100: Loss = -12481.6943359375
Iteration 8200: Loss = -12481.6806640625
Iteration 8300: Loss = -12481.66796875
Iteration 8400: Loss = -12481.6533203125
Iteration 8500: Loss = -12481.6416015625
Iteration 8600: Loss = -12481.6318359375
Iteration 8700: Loss = -12481.6171875
Iteration 8800: Loss = -12481.607421875
Iteration 8900: Loss = -12481.5986328125
Iteration 9000: Loss = -12481.5888671875
Iteration 9100: Loss = -12481.578125
Iteration 9200: Loss = -12481.5732421875
Iteration 9300: Loss = -12481.5634765625
Iteration 9400: Loss = -12481.5546875
Iteration 9500: Loss = -12481.5498046875
Iteration 9600: Loss = -12481.54296875
Iteration 9700: Loss = -12481.5361328125
Iteration 9800: Loss = -12481.5302734375
Iteration 9900: Loss = -12481.5244140625
Iteration 10000: Loss = -12481.5185546875
Iteration 10100: Loss = -12481.513671875
Iteration 10200: Loss = -12481.5087890625
Iteration 10300: Loss = -12481.5029296875
Iteration 10400: Loss = -12481.5009765625
Iteration 10500: Loss = -12481.49609375
Iteration 10600: Loss = -12481.4921875
Iteration 10700: Loss = -12481.490234375
Iteration 10800: Loss = -12481.484375
Iteration 10900: Loss = -12481.48046875
Iteration 11000: Loss = -12481.478515625
Iteration 11100: Loss = -12481.474609375
Iteration 11200: Loss = -12481.4716796875
Iteration 11300: Loss = -12481.470703125
Iteration 11400: Loss = -12481.466796875
Iteration 11500: Loss = -12481.46484375
Iteration 11600: Loss = -12481.4619140625
Iteration 11700: Loss = -12481.4599609375
Iteration 11800: Loss = -12481.4580078125
Iteration 11900: Loss = -12481.4560546875
Iteration 12000: Loss = -12481.4521484375
Iteration 12100: Loss = -12481.453125
1
Iteration 12200: Loss = -12481.4501953125
Iteration 12300: Loss = -12481.44921875
Iteration 12400: Loss = -12481.447265625
Iteration 12500: Loss = -12481.4453125
Iteration 12600: Loss = -12481.4423828125
Iteration 12700: Loss = -12481.4423828125
Iteration 12800: Loss = -12481.44140625
Iteration 12900: Loss = -12481.4404296875
Iteration 13000: Loss = -12481.4384765625
Iteration 13100: Loss = -12481.4384765625
Iteration 13200: Loss = -12481.4365234375
Iteration 13300: Loss = -12481.4375
1
Iteration 13400: Loss = -12481.435546875
Iteration 13500: Loss = -12481.43359375
Iteration 13600: Loss = -12481.43359375
Iteration 13700: Loss = -12481.4326171875
Iteration 13800: Loss = -12481.431640625
Iteration 13900: Loss = -12481.431640625
Iteration 14000: Loss = -12481.4306640625
Iteration 14100: Loss = -12481.431640625
1
Iteration 14200: Loss = -12481.4287109375
Iteration 14300: Loss = -12481.4296875
1
Iteration 14400: Loss = -12481.427734375
Iteration 14500: Loss = -12481.4267578125
Iteration 14600: Loss = -12481.427734375
1
Iteration 14700: Loss = -12481.4267578125
Iteration 14800: Loss = -12481.4267578125
Iteration 14900: Loss = -12481.42578125
Iteration 15000: Loss = -12481.4248046875
Iteration 15100: Loss = -12481.42578125
1
Iteration 15200: Loss = -12481.4248046875
Iteration 15300: Loss = -12481.4248046875
Iteration 15400: Loss = -12481.4248046875
Iteration 15500: Loss = -12481.4228515625
Iteration 15600: Loss = -12481.423828125
1
Iteration 15700: Loss = -12481.4228515625
Iteration 15800: Loss = -12481.4228515625
Iteration 15900: Loss = -12481.4228515625
Iteration 16000: Loss = -12481.421875
Iteration 16100: Loss = -12481.4228515625
1
Iteration 16200: Loss = -12481.4228515625
2
Iteration 16300: Loss = -12481.4228515625
3
Iteration 16400: Loss = -12481.4208984375
Iteration 16500: Loss = -12481.421875
1
Iteration 16600: Loss = -12481.421875
2
Iteration 16700: Loss = -12481.4208984375
Iteration 16800: Loss = -12481.419921875
Iteration 16900: Loss = -12481.419921875
Iteration 17000: Loss = -12481.4189453125
Iteration 17100: Loss = -12481.4189453125
Iteration 17200: Loss = -12481.4189453125
Iteration 17300: Loss = -12481.421875
1
Iteration 17400: Loss = -12481.4189453125
Iteration 17500: Loss = -12481.419921875
1
Iteration 17600: Loss = -12481.419921875
2
Iteration 17700: Loss = -12481.4189453125
Iteration 17800: Loss = -12481.4189453125
Iteration 17900: Loss = -12481.4189453125
Iteration 18000: Loss = -12481.419921875
1
Iteration 18100: Loss = -12481.4189453125
Iteration 18200: Loss = -12481.41796875
Iteration 18300: Loss = -12481.41796875
Iteration 18400: Loss = -12481.41796875
Iteration 18500: Loss = -12481.4189453125
1
Iteration 18600: Loss = -12481.4189453125
2
Iteration 18700: Loss = -12481.41796875
Iteration 18800: Loss = -12481.419921875
1
Iteration 18900: Loss = -12481.4189453125
2
Iteration 19000: Loss = -12481.4189453125
3
Iteration 19100: Loss = -12481.419921875
4
Iteration 19200: Loss = -12481.4189453125
5
Iteration 19300: Loss = -12481.423828125
6
Iteration 19400: Loss = -12481.4189453125
7
Iteration 19500: Loss = -12481.41796875
Iteration 19600: Loss = -12481.41796875
Iteration 19700: Loss = -12481.4189453125
1
Iteration 19800: Loss = -12481.4189453125
2
Iteration 19900: Loss = -12481.41796875
Iteration 20000: Loss = -12481.4189453125
1
Iteration 20100: Loss = -12481.4189453125
2
Iteration 20200: Loss = -12481.41796875
Iteration 20300: Loss = -12481.4189453125
1
Iteration 20400: Loss = -12481.4189453125
2
Iteration 20500: Loss = -12481.4189453125
3
Iteration 20600: Loss = -12481.4189453125
4
Iteration 20700: Loss = -12481.4189453125
5
Iteration 20800: Loss = -12481.4189453125
6
Iteration 20900: Loss = -12481.4189453125
7
Iteration 21000: Loss = -12481.4189453125
8
Iteration 21100: Loss = -12481.4189453125
9
Iteration 21200: Loss = -12481.4189453125
10
Iteration 21300: Loss = -12481.41796875
Iteration 21400: Loss = -12481.41796875
Iteration 21500: Loss = -12481.41796875
Iteration 21600: Loss = -12481.4189453125
1
Iteration 21700: Loss = -12481.41796875
Iteration 21800: Loss = -12481.4189453125
1
Iteration 21900: Loss = -12481.41796875
Iteration 22000: Loss = -12481.41796875
Iteration 22100: Loss = -12481.4169921875
Iteration 22200: Loss = -12481.4189453125
1
Iteration 22300: Loss = -12481.41796875
2
Iteration 22400: Loss = -12481.4189453125
3
Iteration 22500: Loss = -12481.41796875
4
Iteration 22600: Loss = -12481.41796875
5
Iteration 22700: Loss = -12481.4189453125
6
Iteration 22800: Loss = -12481.41796875
7
Iteration 22900: Loss = -12481.41796875
8
Iteration 23000: Loss = -12481.41796875
9
Iteration 23100: Loss = -12481.4189453125
10
Iteration 23200: Loss = -12481.41796875
11
Iteration 23300: Loss = -12481.41796875
12
Iteration 23400: Loss = -12481.41796875
13
Iteration 23500: Loss = -12481.4169921875
Iteration 23600: Loss = -12481.4169921875
Iteration 23700: Loss = -12481.41796875
1
Iteration 23800: Loss = -12481.4189453125
2
Iteration 23900: Loss = -12481.41796875
3
Iteration 24000: Loss = -12481.4189453125
4
Iteration 24100: Loss = -12481.4189453125
5
Iteration 24200: Loss = -12481.41796875
6
Iteration 24300: Loss = -12481.4169921875
Iteration 24400: Loss = -12481.4189453125
1
Iteration 24500: Loss = -12481.41796875
2
Iteration 24600: Loss = -12481.4189453125
3
Iteration 24700: Loss = -12481.419921875
4
Iteration 24800: Loss = -12481.4189453125
5
Iteration 24900: Loss = -12481.4169921875
Iteration 25000: Loss = -12481.41796875
1
Iteration 25100: Loss = -12481.4169921875
Iteration 25200: Loss = -12481.41796875
1
Iteration 25300: Loss = -12481.41796875
2
Iteration 25400: Loss = -12481.4189453125
3
Iteration 25500: Loss = -12481.41796875
4
Iteration 25600: Loss = -12481.4169921875
Iteration 25700: Loss = -12481.419921875
1
Iteration 25800: Loss = -12481.4189453125
2
Iteration 25900: Loss = -12481.4189453125
3
Iteration 26000: Loss = -12481.41796875
4
Iteration 26100: Loss = -12481.4169921875
Iteration 26200: Loss = -12481.4169921875
Iteration 26300: Loss = -12481.41796875
1
Iteration 26400: Loss = -12481.4189453125
2
Iteration 26500: Loss = -12481.4189453125
3
Iteration 26600: Loss = -12481.4189453125
4
Iteration 26700: Loss = -12481.41796875
5
Iteration 26800: Loss = -12481.41796875
6
Iteration 26900: Loss = -12481.4189453125
7
Iteration 27000: Loss = -12481.4189453125
8
Iteration 27100: Loss = -12481.4189453125
9
Iteration 27200: Loss = -12481.41796875
10
Iteration 27300: Loss = -12481.41796875
11
Iteration 27400: Loss = -12481.419921875
12
Iteration 27500: Loss = -12481.41796875
13
Iteration 27600: Loss = -12481.41796875
14
Iteration 27700: Loss = -12481.4189453125
15
Stopping early at iteration 27700 due to no improvement.
pi: tensor([[2.3233e-06, 1.0000e+00],
        [9.9294e-01, 7.0639e-03]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.9226e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2034, 0.2045],
         [0.0280, 0.2019]],

        [[0.6027, 0.1065],
         [0.0904, 0.0989]],

        [[0.9902, 0.1809],
         [0.1936, 0.0114]],

        [[0.0285, 0.3049],
         [0.8463, 0.9102]],

        [[0.6297, 0.2611],
         [0.1935, 0.9438]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0019515007406963793
Average Adjusted Rand Index: -0.0008467737386296445
[0.1396453319292638, -0.0019515007406963793] [0.7526429841031932, -0.0008467737386296445] [12210.64453125, 12481.4189453125]
-------------------------------------
This iteration is 62
True Objective function: Loss = -12033.031064037514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23749.271484375
Iteration 100: Loss = -16932.263671875
Iteration 200: Loss = -13334.5869140625
Iteration 300: Loss = -12900.48828125
Iteration 400: Loss = -12817.283203125
Iteration 500: Loss = -12778.396484375
Iteration 600: Loss = -12754.0595703125
Iteration 700: Loss = -12735.060546875
Iteration 800: Loss = -12720.017578125
Iteration 900: Loss = -12707.1474609375
Iteration 1000: Loss = -12695.1640625
Iteration 1100: Loss = -12684.7412109375
Iteration 1200: Loss = -12669.9716796875
Iteration 1300: Loss = -12652.146484375
Iteration 1400: Loss = -12637.8505859375
Iteration 1500: Loss = -12622.03515625
Iteration 1600: Loss = -12609.67578125
Iteration 1700: Loss = -12598.8564453125
Iteration 1800: Loss = -12591.3955078125
Iteration 1900: Loss = -12587.1181640625
Iteration 2000: Loss = -12583.6328125
Iteration 2100: Loss = -12577.9189453125
Iteration 2200: Loss = -12572.505859375
Iteration 2300: Loss = -12566.6767578125
Iteration 2400: Loss = -12564.0087890625
Iteration 2500: Loss = -12561.7724609375
Iteration 2600: Loss = -12559.4931640625
Iteration 2700: Loss = -12555.5537109375
Iteration 2800: Loss = -12554.1396484375
Iteration 2900: Loss = -12553.1650390625
Iteration 3000: Loss = -12552.3916015625
Iteration 3100: Loss = -12551.7490234375
Iteration 3200: Loss = -12551.1982421875
Iteration 3300: Loss = -12550.732421875
Iteration 3400: Loss = -12550.341796875
Iteration 3500: Loss = -12550.0107421875
Iteration 3600: Loss = -12549.7216796875
Iteration 3700: Loss = -12549.466796875
Iteration 3800: Loss = -12549.2373046875
Iteration 3900: Loss = -12549.033203125
Iteration 4000: Loss = -12548.8466796875
Iteration 4100: Loss = -12548.6787109375
Iteration 4200: Loss = -12548.52734375
Iteration 4300: Loss = -12548.3876953125
Iteration 4400: Loss = -12548.259765625
Iteration 4500: Loss = -12548.14453125
Iteration 4600: Loss = -12548.037109375
Iteration 4700: Loss = -12547.9404296875
Iteration 4800: Loss = -12547.84765625
Iteration 4900: Loss = -12547.7646484375
Iteration 5000: Loss = -12547.6884765625
Iteration 5100: Loss = -12547.615234375
Iteration 5200: Loss = -12547.548828125
Iteration 5300: Loss = -12547.4873046875
Iteration 5400: Loss = -12547.4296875
Iteration 5500: Loss = -12547.3759765625
Iteration 5600: Loss = -12547.3271484375
Iteration 5700: Loss = -12547.2802734375
Iteration 5800: Loss = -12547.2353515625
Iteration 5900: Loss = -12547.1953125
Iteration 6000: Loss = -12547.1572265625
Iteration 6100: Loss = -12547.1220703125
Iteration 6200: Loss = -12547.087890625
Iteration 6300: Loss = -12547.0576171875
Iteration 6400: Loss = -12547.0263671875
Iteration 6500: Loss = -12546.9990234375
Iteration 6600: Loss = -12546.9736328125
Iteration 6700: Loss = -12546.951171875
Iteration 6800: Loss = -12546.92578125
Iteration 6900: Loss = -12546.9033203125
Iteration 7000: Loss = -12546.8828125
Iteration 7100: Loss = -12546.865234375
Iteration 7200: Loss = -12546.8466796875
Iteration 7300: Loss = -12546.8291015625
Iteration 7400: Loss = -12546.8125
Iteration 7500: Loss = -12546.7978515625
Iteration 7600: Loss = -12546.783203125
Iteration 7700: Loss = -12546.7685546875
Iteration 7800: Loss = -12546.7578125
Iteration 7900: Loss = -12546.74609375
Iteration 8000: Loss = -12546.7353515625
Iteration 8100: Loss = -12546.7255859375
Iteration 8200: Loss = -12546.71484375
Iteration 8300: Loss = -12546.705078125
Iteration 8400: Loss = -12546.697265625
Iteration 8500: Loss = -12546.689453125
Iteration 8600: Loss = -12546.681640625
Iteration 8700: Loss = -12546.67578125
Iteration 8800: Loss = -12546.66796875
Iteration 8900: Loss = -12546.6591796875
Iteration 9000: Loss = -12546.6533203125
Iteration 9100: Loss = -12546.6494140625
Iteration 9200: Loss = -12546.6416015625
Iteration 9300: Loss = -12546.63671875
Iteration 9400: Loss = -12546.6328125
Iteration 9500: Loss = -12546.6279296875
Iteration 9600: Loss = -12546.6220703125
Iteration 9700: Loss = -12546.6171875
Iteration 9800: Loss = -12546.6142578125
Iteration 9900: Loss = -12546.609375
Iteration 10000: Loss = -12546.6064453125
Iteration 10100: Loss = -12546.6025390625
Iteration 10200: Loss = -12546.5986328125
Iteration 10300: Loss = -12546.59765625
Iteration 10400: Loss = -12546.59375
Iteration 10500: Loss = -12546.58984375
Iteration 10600: Loss = -12546.5869140625
Iteration 10700: Loss = -12546.5830078125
Iteration 10800: Loss = -12546.58203125
Iteration 10900: Loss = -12546.5791015625
Iteration 11000: Loss = -12546.576171875
Iteration 11100: Loss = -12546.5751953125
Iteration 11200: Loss = -12546.5732421875
Iteration 11300: Loss = -12546.5703125
Iteration 11400: Loss = -12546.568359375
Iteration 11500: Loss = -12546.5673828125
Iteration 11600: Loss = -12546.5654296875
Iteration 11700: Loss = -12546.5615234375
Iteration 11800: Loss = -12546.5625
1
Iteration 11900: Loss = -12546.55859375
Iteration 12000: Loss = -12546.55859375
Iteration 12100: Loss = -12546.5576171875
Iteration 12200: Loss = -12546.5546875
Iteration 12300: Loss = -12546.5546875
Iteration 12400: Loss = -12546.5537109375
Iteration 12500: Loss = -12546.552734375
Iteration 12600: Loss = -12546.55078125
Iteration 12700: Loss = -12546.5498046875
Iteration 12800: Loss = -12546.5498046875
Iteration 12900: Loss = -12546.548828125
Iteration 13000: Loss = -12546.546875
Iteration 13100: Loss = -12546.544921875
Iteration 13200: Loss = -12546.5458984375
1
Iteration 13300: Loss = -12546.544921875
Iteration 13400: Loss = -12546.544921875
Iteration 13500: Loss = -12546.54296875
Iteration 13600: Loss = -12546.54296875
Iteration 13700: Loss = -12546.541015625
Iteration 13800: Loss = -12546.541015625
Iteration 13900: Loss = -12546.541015625
Iteration 14000: Loss = -12546.5400390625
Iteration 14100: Loss = -12546.5390625
Iteration 14200: Loss = -12546.5390625
Iteration 14300: Loss = -12546.5380859375
Iteration 14400: Loss = -12546.5390625
1
Iteration 14500: Loss = -12546.5390625
2
Iteration 14600: Loss = -12546.537109375
Iteration 14700: Loss = -12546.5380859375
1
Iteration 14800: Loss = -12546.5361328125
Iteration 14900: Loss = -12546.5341796875
Iteration 15000: Loss = -12546.5361328125
1
Iteration 15100: Loss = -12546.5361328125
2
Iteration 15200: Loss = -12546.53515625
3
Iteration 15300: Loss = -12546.5361328125
4
Iteration 15400: Loss = -12546.5361328125
5
Iteration 15500: Loss = -12546.53515625
6
Iteration 15600: Loss = -12546.53515625
7
Iteration 15700: Loss = -12546.5341796875
Iteration 15800: Loss = -12546.5341796875
Iteration 15900: Loss = -12546.5341796875
Iteration 16000: Loss = -12546.5322265625
Iteration 16100: Loss = -12546.533203125
1
Iteration 16200: Loss = -12546.5322265625
Iteration 16300: Loss = -12546.5322265625
Iteration 16400: Loss = -12546.533203125
1
Iteration 16500: Loss = -12546.5322265625
Iteration 16600: Loss = -12546.5322265625
Iteration 16700: Loss = -12546.5322265625
Iteration 16800: Loss = -12546.533203125
1
Iteration 16900: Loss = -12546.5322265625
Iteration 17000: Loss = -12546.53125
Iteration 17100: Loss = -12546.533203125
1
Iteration 17200: Loss = -12546.5322265625
2
Iteration 17300: Loss = -12546.5302734375
Iteration 17400: Loss = -12546.52734375
Iteration 17500: Loss = -12546.5244140625
Iteration 17600: Loss = -12546.525390625
1
Iteration 17700: Loss = -12546.5244140625
Iteration 17800: Loss = -12546.525390625
1
Iteration 17900: Loss = -12546.5244140625
Iteration 18000: Loss = -12546.5244140625
Iteration 18100: Loss = -12546.5244140625
Iteration 18200: Loss = -12546.5224609375
Iteration 18300: Loss = -12546.5234375
1
Iteration 18400: Loss = -12546.5205078125
Iteration 18500: Loss = -12546.5205078125
Iteration 18600: Loss = -12546.5205078125
Iteration 18700: Loss = -12546.521484375
1
Iteration 18800: Loss = -12546.5205078125
Iteration 18900: Loss = -12546.5205078125
Iteration 19000: Loss = -12546.5185546875
Iteration 19100: Loss = -12546.5185546875
Iteration 19200: Loss = -12546.5205078125
1
Iteration 19300: Loss = -12546.5185546875
Iteration 19400: Loss = -12546.5185546875
Iteration 19500: Loss = -12546.5166015625
Iteration 19600: Loss = -12546.515625
Iteration 19700: Loss = -12546.5146484375
Iteration 19800: Loss = -12546.513671875
Iteration 19900: Loss = -12546.509765625
Iteration 20000: Loss = -12546.5078125
Iteration 20100: Loss = -12546.5048828125
Iteration 20200: Loss = -12546.49609375
Iteration 20300: Loss = -12546.486328125
Iteration 20400: Loss = -12546.46875
Iteration 20500: Loss = -12546.40234375
Iteration 20600: Loss = -12546.2529296875
Iteration 20700: Loss = -12545.666015625
Iteration 20800: Loss = -12545.28515625
Iteration 20900: Loss = -12545.1240234375
Iteration 21000: Loss = -12545.1240234375
Iteration 21100: Loss = -12545.1240234375
Iteration 21200: Loss = -12545.1240234375
Iteration 21300: Loss = -12545.1240234375
Iteration 21400: Loss = -12545.1240234375
Iteration 21500: Loss = -12545.1220703125
Iteration 21600: Loss = -12545.1240234375
1
Iteration 21700: Loss = -12545.125
2
Iteration 21800: Loss = -12545.125
3
Iteration 21900: Loss = -12545.123046875
4
Iteration 22000: Loss = -12545.1240234375
5
Iteration 22100: Loss = -12545.1240234375
6
Iteration 22200: Loss = -12545.1240234375
7
Iteration 22300: Loss = -12545.1240234375
8
Iteration 22400: Loss = -12545.125
9
Iteration 22500: Loss = -12545.1240234375
10
Iteration 22600: Loss = -12545.1240234375
11
Iteration 22700: Loss = -12545.123046875
12
Iteration 22800: Loss = -12545.123046875
13
Iteration 22900: Loss = -12545.1240234375
14
Iteration 23000: Loss = -12545.1240234375
15
Stopping early at iteration 23000 due to no improvement.
pi: tensor([[1.0000e+00, 6.5578e-07],
        [5.6263e-01, 4.3737e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([3.1924e-05, 9.9997e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2008, 0.1977],
         [0.8629, 0.2031]],

        [[0.0260, 0.2114],
         [0.9142, 0.9453]],

        [[0.9920, 0.1945],
         [0.0247, 0.0158]],

        [[0.4717, 0.2494],
         [0.0351, 0.0107]],

        [[0.9887, 0.2820],
         [0.2808, 0.9845]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.007246281224304942
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.005431979218977636
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.0005624593535232805
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
Global Adjusted Rand Index: 0.002079493666496847
Average Adjusted Rand Index: -0.0009294232631260517
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37313.36328125
Iteration 100: Loss = -24296.546875
Iteration 200: Loss = -15348.232421875
Iteration 300: Loss = -13214.5546875
Iteration 400: Loss = -12923.83203125
Iteration 500: Loss = -12827.0654296875
Iteration 600: Loss = -12773.833984375
Iteration 700: Loss = -12735.3125
Iteration 800: Loss = -12710.4580078125
Iteration 900: Loss = -12684.275390625
Iteration 1000: Loss = -12664.888671875
Iteration 1100: Loss = -12654.2568359375
Iteration 1200: Loss = -12644.640625
Iteration 1300: Loss = -12635.9775390625
Iteration 1400: Loss = -12627.296875
Iteration 1500: Loss = -12620.72265625
Iteration 1600: Loss = -12607.841796875
Iteration 1700: Loss = -12595.5703125
Iteration 1800: Loss = -12588.3857421875
Iteration 1900: Loss = -12579.2919921875
Iteration 2000: Loss = -12572.638671875
Iteration 2100: Loss = -12568.5537109375
Iteration 2200: Loss = -12565.8955078125
Iteration 2300: Loss = -12563.9130859375
Iteration 2400: Loss = -12562.173828125
Iteration 2500: Loss = -12560.5107421875
Iteration 2600: Loss = -12558.912109375
Iteration 2700: Loss = -12557.06640625
Iteration 2800: Loss = -12555.4404296875
Iteration 2900: Loss = -12554.435546875
Iteration 3000: Loss = -12553.6904296875
Iteration 3100: Loss = -12553.0810546875
Iteration 3200: Loss = -12552.5595703125
Iteration 3300: Loss = -12552.103515625
Iteration 3400: Loss = -12551.7001953125
Iteration 3500: Loss = -12551.3388671875
Iteration 3600: Loss = -12551.0146484375
Iteration 3700: Loss = -12550.720703125
Iteration 3800: Loss = -12550.4541015625
Iteration 3900: Loss = -12550.2060546875
Iteration 4000: Loss = -12549.978515625
Iteration 4100: Loss = -12549.771484375
Iteration 4200: Loss = -12549.578125
Iteration 4300: Loss = -12549.4033203125
Iteration 4400: Loss = -12549.244140625
Iteration 4500: Loss = -12549.0947265625
Iteration 4600: Loss = -12548.9580078125
Iteration 4700: Loss = -12548.830078125
Iteration 4800: Loss = -12548.708984375
Iteration 4900: Loss = -12548.599609375
Iteration 5000: Loss = -12548.494140625
Iteration 5100: Loss = -12548.3974609375
Iteration 5200: Loss = -12548.306640625
Iteration 5300: Loss = -12548.220703125
Iteration 5400: Loss = -12548.142578125
Iteration 5500: Loss = -12548.0673828125
Iteration 5600: Loss = -12547.99609375
Iteration 5700: Loss = -12547.9296875
Iteration 5800: Loss = -12547.8671875
Iteration 5900: Loss = -12547.80859375
Iteration 6000: Loss = -12547.75390625
Iteration 6100: Loss = -12547.703125
Iteration 6200: Loss = -12547.654296875
Iteration 6300: Loss = -12547.6083984375
Iteration 6400: Loss = -12547.564453125
Iteration 6500: Loss = -12547.5234375
Iteration 6600: Loss = -12547.4873046875
Iteration 6700: Loss = -12547.44921875
Iteration 6800: Loss = -12547.4150390625
Iteration 6900: Loss = -12547.3837890625
Iteration 7000: Loss = -12547.3525390625
Iteration 7100: Loss = -12547.3232421875
Iteration 7200: Loss = -12547.296875
Iteration 7300: Loss = -12547.271484375
Iteration 7400: Loss = -12547.2470703125
Iteration 7500: Loss = -12547.2216796875
Iteration 7600: Loss = -12547.2021484375
Iteration 7700: Loss = -12547.1806640625
Iteration 7800: Loss = -12547.16015625
Iteration 7900: Loss = -12547.140625
Iteration 8000: Loss = -12547.123046875
Iteration 8100: Loss = -12547.1064453125
Iteration 8200: Loss = -12547.08984375
Iteration 8300: Loss = -12547.0751953125
Iteration 8400: Loss = -12547.060546875
Iteration 8500: Loss = -12547.048828125
Iteration 8600: Loss = -12547.03515625
Iteration 8700: Loss = -12547.0224609375
Iteration 8800: Loss = -12547.01171875
Iteration 8900: Loss = -12547.0009765625
Iteration 9000: Loss = -12546.990234375
Iteration 9100: Loss = -12546.98046875
Iteration 9200: Loss = -12546.9716796875
Iteration 9300: Loss = -12546.9609375
Iteration 9400: Loss = -12546.9521484375
Iteration 9500: Loss = -12546.9453125
Iteration 9600: Loss = -12546.9375
Iteration 9700: Loss = -12546.9296875
Iteration 9800: Loss = -12546.9248046875
Iteration 9900: Loss = -12546.91796875
Iteration 10000: Loss = -12546.9111328125
Iteration 10100: Loss = -12546.9052734375
Iteration 10200: Loss = -12546.9013671875
Iteration 10300: Loss = -12546.896484375
Iteration 10400: Loss = -12546.8896484375
Iteration 10500: Loss = -12546.884765625
Iteration 10600: Loss = -12546.8798828125
Iteration 10700: Loss = -12546.875
Iteration 10800: Loss = -12546.87109375
Iteration 10900: Loss = -12546.8681640625
Iteration 11000: Loss = -12546.8642578125
Iteration 11100: Loss = -12546.8603515625
Iteration 11200: Loss = -12546.8583984375
Iteration 11300: Loss = -12546.8544921875
Iteration 11400: Loss = -12546.8515625
Iteration 11500: Loss = -12546.84765625
Iteration 11600: Loss = -12546.8447265625
Iteration 11700: Loss = -12546.84375
Iteration 11800: Loss = -12546.8408203125
Iteration 11900: Loss = -12546.837890625
Iteration 12000: Loss = -12546.837890625
Iteration 12100: Loss = -12546.8349609375
Iteration 12200: Loss = -12546.833984375
Iteration 12300: Loss = -12546.8310546875
Iteration 12400: Loss = -12546.830078125
Iteration 12500: Loss = -12546.8271484375
Iteration 12600: Loss = -12546.8271484375
Iteration 12700: Loss = -12546.82421875
Iteration 12800: Loss = -12546.822265625
Iteration 12900: Loss = -12546.8212890625
Iteration 13000: Loss = -12546.8212890625
Iteration 13100: Loss = -12546.8193359375
Iteration 13200: Loss = -12546.818359375
Iteration 13300: Loss = -12546.8173828125
Iteration 13400: Loss = -12546.814453125
Iteration 13500: Loss = -12546.81640625
1
Iteration 13600: Loss = -12546.8154296875
2
Iteration 13700: Loss = -12546.8134765625
Iteration 13800: Loss = -12546.8115234375
Iteration 13900: Loss = -12546.810546875
Iteration 14000: Loss = -12546.8095703125
Iteration 14100: Loss = -12546.8095703125
Iteration 14200: Loss = -12546.80859375
Iteration 14300: Loss = -12546.8076171875
Iteration 14400: Loss = -12546.8076171875
Iteration 14500: Loss = -12546.8056640625
Iteration 14600: Loss = -12546.8056640625
Iteration 14700: Loss = -12546.8037109375
Iteration 14800: Loss = -12546.8056640625
1
Iteration 14900: Loss = -12546.8037109375
Iteration 15000: Loss = -12546.8046875
1
Iteration 15100: Loss = -12546.8037109375
Iteration 15200: Loss = -12546.802734375
Iteration 15300: Loss = -12546.802734375
Iteration 15400: Loss = -12546.8017578125
Iteration 15500: Loss = -12546.8017578125
Iteration 15600: Loss = -12546.8017578125
Iteration 15700: Loss = -12546.8017578125
Iteration 15800: Loss = -12546.7998046875
Iteration 15900: Loss = -12546.7998046875
Iteration 16000: Loss = -12546.798828125
Iteration 16100: Loss = -12546.798828125
Iteration 16200: Loss = -12546.7998046875
1
Iteration 16300: Loss = -12546.7978515625
Iteration 16400: Loss = -12546.7998046875
1
Iteration 16500: Loss = -12546.7998046875
2
Iteration 16600: Loss = -12546.80078125
3
Iteration 16700: Loss = -12546.798828125
4
Iteration 16800: Loss = -12546.796875
Iteration 16900: Loss = -12546.7978515625
1
Iteration 17000: Loss = -12546.7978515625
2
Iteration 17100: Loss = -12546.7978515625
3
Iteration 17200: Loss = -12546.7958984375
Iteration 17300: Loss = -12546.7958984375
Iteration 17400: Loss = -12546.7978515625
1
Iteration 17500: Loss = -12546.7978515625
2
Iteration 17600: Loss = -12546.7978515625
3
Iteration 17700: Loss = -12546.7978515625
4
Iteration 17800: Loss = -12546.796875
5
Iteration 17900: Loss = -12546.7958984375
Iteration 18000: Loss = -12546.794921875
Iteration 18100: Loss = -12546.796875
1
Iteration 18200: Loss = -12546.794921875
Iteration 18300: Loss = -12546.796875
1
Iteration 18400: Loss = -12546.794921875
Iteration 18500: Loss = -12546.7958984375
1
Iteration 18600: Loss = -12546.7958984375
2
Iteration 18700: Loss = -12546.794921875
Iteration 18800: Loss = -12546.794921875
Iteration 18900: Loss = -12546.7958984375
1
Iteration 19000: Loss = -12546.794921875
Iteration 19100: Loss = -12546.796875
1
Iteration 19200: Loss = -12546.7958984375
2
Iteration 19300: Loss = -12546.7939453125
Iteration 19400: Loss = -12546.794921875
1
Iteration 19500: Loss = -12546.794921875
2
Iteration 19600: Loss = -12546.794921875
3
Iteration 19700: Loss = -12546.7958984375
4
Iteration 19800: Loss = -12546.794921875
5
Iteration 19900: Loss = -12546.794921875
6
Iteration 20000: Loss = -12546.794921875
7
Iteration 20100: Loss = -12546.794921875
8
Iteration 20200: Loss = -12546.7939453125
Iteration 20300: Loss = -12546.796875
1
Iteration 20400: Loss = -12546.7958984375
2
Iteration 20500: Loss = -12546.7958984375
3
Iteration 20600: Loss = -12546.794921875
4
Iteration 20700: Loss = -12546.794921875
5
Iteration 20800: Loss = -12546.7939453125
Iteration 20900: Loss = -12546.79296875
Iteration 21000: Loss = -12546.7939453125
1
Iteration 21100: Loss = -12546.7958984375
2
Iteration 21200: Loss = -12546.7939453125
3
Iteration 21300: Loss = -12546.79296875
Iteration 21400: Loss = -12546.6943359375
Iteration 21500: Loss = -12546.5693359375
Iteration 21600: Loss = -12546.546875
Iteration 21700: Loss = -12546.541015625
Iteration 21800: Loss = -12546.5380859375
Iteration 21900: Loss = -12546.5361328125
Iteration 22000: Loss = -12546.53515625
Iteration 22100: Loss = -12546.533203125
Iteration 22200: Loss = -12546.5341796875
1
Iteration 22300: Loss = -12546.5322265625
Iteration 22400: Loss = -12546.5341796875
1
Iteration 22500: Loss = -12546.5322265625
Iteration 22600: Loss = -12546.53125
Iteration 22700: Loss = -12546.53125
Iteration 22800: Loss = -12546.5302734375
Iteration 22900: Loss = -12546.533203125
1
Iteration 23000: Loss = -12546.5322265625
2
Iteration 23100: Loss = -12546.533203125
3
Iteration 23200: Loss = -12546.5322265625
4
Iteration 23300: Loss = -12546.53125
5
Iteration 23400: Loss = -12546.52734375
Iteration 23500: Loss = -12546.5234375
Iteration 23600: Loss = -12546.525390625
1
Iteration 23700: Loss = -12546.5244140625
2
Iteration 23800: Loss = -12546.525390625
3
Iteration 23900: Loss = -12546.5263671875
4
Iteration 24000: Loss = -12546.525390625
5
Iteration 24100: Loss = -12546.5244140625
6
Iteration 24200: Loss = -12546.5244140625
7
Iteration 24300: Loss = -12546.5244140625
8
Iteration 24400: Loss = -12546.5224609375
Iteration 24500: Loss = -12546.5224609375
Iteration 24600: Loss = -12546.5205078125
Iteration 24700: Loss = -12546.5224609375
1
Iteration 24800: Loss = -12546.5224609375
2
Iteration 24900: Loss = -12546.521484375
3
Iteration 25000: Loss = -12546.5205078125
Iteration 25100: Loss = -12546.5185546875
Iteration 25200: Loss = -12546.515625
Iteration 25300: Loss = -12546.51171875
Iteration 25400: Loss = -12546.505859375
Iteration 25500: Loss = -12546.4990234375
Iteration 25600: Loss = -12546.48828125
Iteration 25700: Loss = -12546.4716796875
Iteration 25800: Loss = -12546.4609375
Iteration 25900: Loss = -12546.4599609375
Iteration 26000: Loss = -12546.333984375
Iteration 26100: Loss = -12546.224609375
Iteration 26200: Loss = -12544.498046875
Iteration 26300: Loss = -12542.0068359375
Iteration 26400: Loss = -12541.9716796875
Iteration 26500: Loss = -12541.9609375
Iteration 26600: Loss = -12541.953125
Iteration 26700: Loss = -12541.94921875
Iteration 26800: Loss = -12541.947265625
Iteration 26900: Loss = -12541.9453125
Iteration 27000: Loss = -12541.9443359375
Iteration 27100: Loss = -12541.943359375
Iteration 27200: Loss = -12541.9443359375
1
Iteration 27300: Loss = -12541.94140625
Iteration 27400: Loss = -12541.94140625
Iteration 27500: Loss = -12541.9404296875
Iteration 27600: Loss = -12541.9443359375
1
Iteration 27700: Loss = -12541.94140625
2
Iteration 27800: Loss = -12541.939453125
Iteration 27900: Loss = -12541.94140625
1
Iteration 28000: Loss = -12541.9404296875
2
Iteration 28100: Loss = -12541.939453125
Iteration 28200: Loss = -12541.939453125
Iteration 28300: Loss = -12541.939453125
Iteration 28400: Loss = -12541.9375
Iteration 28500: Loss = -12541.9404296875
1
Iteration 28600: Loss = -12541.9384765625
2
Iteration 28700: Loss = -12541.939453125
3
Iteration 28800: Loss = -12541.939453125
4
Iteration 28900: Loss = -12541.9375
Iteration 29000: Loss = -12541.9384765625
1
Iteration 29100: Loss = -12541.9384765625
2
Iteration 29200: Loss = -12541.939453125
3
Iteration 29300: Loss = -12541.9375
Iteration 29400: Loss = -12541.939453125
1
Iteration 29500: Loss = -12541.9384765625
2
Iteration 29600: Loss = -12541.9384765625
3
Iteration 29700: Loss = -12541.9375
Iteration 29800: Loss = -12541.939453125
1
Iteration 29900: Loss = -12541.9384765625
2
pi: tensor([[9.9988e-01, 1.1615e-04],
        [4.1722e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0633, 0.9367], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2698, 0.1966],
         [0.0097, 0.2008]],

        [[0.0090, 0.2578],
         [0.0250, 0.7592]],

        [[0.1365, 0.1803],
         [0.4051, 0.9300]],

        [[0.7666, 0.2649],
         [0.0195, 0.0619]],

        [[0.7886, 0.2608],
         [0.3218, 0.6027]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0005624593535232805
time is 2
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.005431979218977636
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0005624593535232805
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.002911726262720568
Global Adjusted Rand Index: 0.0004998468968646926
Average Adjusted Rand Index: 0.0002790668498421015
[0.002079493666496847, 0.0004998468968646926] [-0.0009294232631260517, 0.0002790668498421015] [12545.1240234375, 12541.9375]
-------------------------------------
This iteration is 63
True Objective function: Loss = -11826.907935067185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22490.310546875
Iteration 100: Loss = -16379.35546875
Iteration 200: Loss = -13480.5107421875
Iteration 300: Loss = -12626.1123046875
Iteration 400: Loss = -12478.25390625
Iteration 500: Loss = -12438.2783203125
Iteration 600: Loss = -12393.8056640625
Iteration 700: Loss = -12328.2734375
Iteration 800: Loss = -12197.0126953125
Iteration 900: Loss = -12113.912109375
Iteration 1000: Loss = -12085.576171875
Iteration 1100: Loss = -12073.0322265625
Iteration 1200: Loss = -12063.2255859375
Iteration 1300: Loss = -12048.7158203125
Iteration 1400: Loss = -12045.921875
Iteration 1500: Loss = -12043.7080078125
Iteration 1600: Loss = -12042.3232421875
Iteration 1700: Loss = -12041.419921875
Iteration 1800: Loss = -12040.732421875
Iteration 1900: Loss = -12040.173828125
Iteration 2000: Loss = -12039.697265625
Iteration 2100: Loss = -12039.177734375
Iteration 2200: Loss = -12035.0341796875
Iteration 2300: Loss = -12034.626953125
Iteration 2400: Loss = -12034.357421875
Iteration 2500: Loss = -12034.1328125
Iteration 2600: Loss = -12033.9375
Iteration 2700: Loss = -12033.765625
Iteration 2800: Loss = -12033.611328125
Iteration 2900: Loss = -12033.4755859375
Iteration 3000: Loss = -12033.353515625
Iteration 3100: Loss = -12033.2431640625
Iteration 3200: Loss = -12033.1435546875
Iteration 3300: Loss = -12033.052734375
Iteration 3400: Loss = -12032.96875
Iteration 3500: Loss = -12032.8935546875
Iteration 3600: Loss = -12032.8251953125
Iteration 3700: Loss = -12032.7607421875
Iteration 3800: Loss = -12032.7021484375
Iteration 3900: Loss = -12032.6494140625
Iteration 4000: Loss = -12032.599609375
Iteration 4100: Loss = -12032.5517578125
Iteration 4200: Loss = -12032.509765625
Iteration 4300: Loss = -12032.4697265625
Iteration 4400: Loss = -12032.43359375
Iteration 4500: Loss = -12032.3974609375
Iteration 4600: Loss = -12032.3427734375
Iteration 4700: Loss = -12022.513671875
Iteration 4800: Loss = -12022.31640625
Iteration 4900: Loss = -12022.251953125
Iteration 5000: Loss = -12022.208984375
Iteration 5100: Loss = -12022.17578125
Iteration 5200: Loss = -12022.146484375
Iteration 5300: Loss = -12022.123046875
Iteration 5400: Loss = -12022.1015625
Iteration 5500: Loss = -12022.08203125
Iteration 5600: Loss = -12022.0634765625
Iteration 5700: Loss = -12022.0458984375
Iteration 5800: Loss = -12022.029296875
Iteration 5900: Loss = -12022.015625
Iteration 6000: Loss = -12022.0009765625
Iteration 6100: Loss = -12021.990234375
Iteration 6200: Loss = -12021.9765625
Iteration 6300: Loss = -12021.9658203125
Iteration 6400: Loss = -12021.95703125
Iteration 6500: Loss = -12021.9453125
Iteration 6600: Loss = -12021.9375
Iteration 6700: Loss = -12021.927734375
Iteration 6800: Loss = -12021.9189453125
Iteration 6900: Loss = -12021.9111328125
Iteration 7000: Loss = -12021.9033203125
Iteration 7100: Loss = -12021.8974609375
Iteration 7200: Loss = -12021.8916015625
Iteration 7300: Loss = -12021.884765625
Iteration 7400: Loss = -12021.87890625
Iteration 7500: Loss = -12021.873046875
Iteration 7600: Loss = -12021.8681640625
Iteration 7700: Loss = -12021.861328125
Iteration 7800: Loss = -12021.8583984375
Iteration 7900: Loss = -12021.8525390625
Iteration 8000: Loss = -12021.84765625
Iteration 8100: Loss = -12021.8447265625
Iteration 8200: Loss = -12021.841796875
Iteration 8300: Loss = -12021.8369140625
Iteration 8400: Loss = -12021.833984375
Iteration 8500: Loss = -12021.83203125
Iteration 8600: Loss = -12021.8271484375
Iteration 8700: Loss = -12021.82421875
Iteration 8800: Loss = -12021.822265625
Iteration 8900: Loss = -12021.8193359375
Iteration 9000: Loss = -12021.81640625
Iteration 9100: Loss = -12021.8125
Iteration 9200: Loss = -12021.80859375
Iteration 9300: Loss = -12021.8076171875
Iteration 9400: Loss = -12021.806640625
Iteration 9500: Loss = -12021.8046875
Iteration 9600: Loss = -12021.802734375
Iteration 9700: Loss = -12021.8017578125
Iteration 9800: Loss = -12021.7998046875
Iteration 9900: Loss = -12021.7978515625
Iteration 10000: Loss = -12021.7958984375
Iteration 10100: Loss = -12021.796875
1
Iteration 10200: Loss = -12021.7939453125
Iteration 10300: Loss = -12021.79296875
Iteration 10400: Loss = -12021.791015625
Iteration 10500: Loss = -12021.791015625
Iteration 10600: Loss = -12021.7900390625
Iteration 10700: Loss = -12021.7900390625
Iteration 10800: Loss = -12021.7880859375
Iteration 10900: Loss = -12021.7880859375
Iteration 11000: Loss = -12021.7861328125
Iteration 11100: Loss = -12021.7861328125
Iteration 11200: Loss = -12021.7841796875
Iteration 11300: Loss = -12021.78515625
1
Iteration 11400: Loss = -12021.7841796875
Iteration 11500: Loss = -12021.78515625
1
Iteration 11600: Loss = -12021.7841796875
Iteration 11700: Loss = -12021.783203125
Iteration 11800: Loss = -12021.78125
Iteration 11900: Loss = -12021.77734375
Iteration 12000: Loss = -12009.216796875
Iteration 12100: Loss = -12008.609375
Iteration 12200: Loss = -12007.9619140625
Iteration 12300: Loss = -12007.94140625
Iteration 12400: Loss = -12007.9296875
Iteration 12500: Loss = -12007.921875
Iteration 12600: Loss = -12007.91796875
Iteration 12700: Loss = -12007.9130859375
Iteration 12800: Loss = -12007.91015625
Iteration 12900: Loss = -12007.9072265625
Iteration 13000: Loss = -12007.9052734375
Iteration 13100: Loss = -12007.904296875
Iteration 13200: Loss = -12007.9033203125
Iteration 13300: Loss = -12007.900390625
Iteration 13400: Loss = -12007.8994140625
Iteration 13500: Loss = -12007.8984375
Iteration 13600: Loss = -12007.8984375
Iteration 13700: Loss = -12007.8974609375
Iteration 13800: Loss = -12007.8955078125
Iteration 13900: Loss = -12007.8955078125
Iteration 14000: Loss = -12007.896484375
1
Iteration 14100: Loss = -12007.89453125
Iteration 14200: Loss = -12007.8935546875
Iteration 14300: Loss = -12007.892578125
Iteration 14400: Loss = -12007.890625
Iteration 14500: Loss = -12007.8896484375
Iteration 14600: Loss = -12007.8876953125
Iteration 14700: Loss = -12007.8837890625
Iteration 14800: Loss = -12007.8564453125
Iteration 14900: Loss = -12000.86328125
Iteration 15000: Loss = -11984.748046875
Iteration 15100: Loss = -11972.056640625
Iteration 15200: Loss = -11884.8896484375
Iteration 15300: Loss = -11853.677734375
Iteration 15400: Loss = -11852.970703125
Iteration 15500: Loss = -11852.7109375
Iteration 15600: Loss = -11852.57421875
Iteration 15700: Loss = -11852.4912109375
Iteration 15800: Loss = -11852.43359375
Iteration 15900: Loss = -11852.3935546875
Iteration 16000: Loss = -11852.3623046875
Iteration 16100: Loss = -11852.3388671875
Iteration 16200: Loss = -11852.3193359375
Iteration 16300: Loss = -11852.3037109375
Iteration 16400: Loss = -11852.2900390625
Iteration 16500: Loss = -11852.2802734375
Iteration 16600: Loss = -11852.271484375
Iteration 16700: Loss = -11852.263671875
Iteration 16800: Loss = -11852.255859375
Iteration 16900: Loss = -11852.25
Iteration 17000: Loss = -11852.2421875
Iteration 17100: Loss = -11852.2392578125
Iteration 17200: Loss = -11852.236328125
Iteration 17300: Loss = -11852.2314453125
Iteration 17400: Loss = -11852.2275390625
Iteration 17500: Loss = -11852.2265625
Iteration 17600: Loss = -11852.22265625
Iteration 17700: Loss = -11852.2197265625
Iteration 17800: Loss = -11852.216796875
Iteration 17900: Loss = -11852.2158203125
Iteration 18000: Loss = -11852.212890625
Iteration 18100: Loss = -11852.2119140625
Iteration 18200: Loss = -11852.2099609375
Iteration 18300: Loss = -11852.2080078125
Iteration 18400: Loss = -11852.2080078125
Iteration 18500: Loss = -11852.2060546875
Iteration 18600: Loss = -11852.2060546875
Iteration 18700: Loss = -11852.205078125
Iteration 18800: Loss = -11852.203125
Iteration 18900: Loss = -11852.2021484375
Iteration 19000: Loss = -11852.201171875
Iteration 19100: Loss = -11852.2001953125
Iteration 19200: Loss = -11852.2001953125
Iteration 19300: Loss = -11852.1982421875
Iteration 19400: Loss = -11852.197265625
Iteration 19500: Loss = -11852.197265625
Iteration 19600: Loss = -11852.1953125
Iteration 19700: Loss = -11852.197265625
1
Iteration 19800: Loss = -11852.1953125
Iteration 19900: Loss = -11852.1962890625
1
Iteration 20000: Loss = -11852.1943359375
Iteration 20100: Loss = -11852.193359375
Iteration 20200: Loss = -11852.193359375
Iteration 20300: Loss = -11852.193359375
Iteration 20400: Loss = -11852.193359375
Iteration 20500: Loss = -11852.193359375
Iteration 20600: Loss = -11852.1923828125
Iteration 20700: Loss = -11852.1923828125
Iteration 20800: Loss = -11852.19140625
Iteration 20900: Loss = -11852.19140625
Iteration 21000: Loss = -11852.1923828125
1
Iteration 21100: Loss = -11852.1923828125
2
Iteration 21200: Loss = -11852.1923828125
3
Iteration 21300: Loss = -11852.1904296875
Iteration 21400: Loss = -11852.1904296875
Iteration 21500: Loss = -11852.19140625
1
Iteration 21600: Loss = -11852.19140625
2
Iteration 21700: Loss = -11852.19140625
3
Iteration 21800: Loss = -11852.1884765625
Iteration 21900: Loss = -11852.1904296875
1
Iteration 22000: Loss = -11852.189453125
2
Iteration 22100: Loss = -11852.189453125
3
Iteration 22200: Loss = -11852.1904296875
4
Iteration 22300: Loss = -11852.189453125
5
Iteration 22400: Loss = -11852.189453125
6
Iteration 22500: Loss = -11852.189453125
7
Iteration 22600: Loss = -11852.1884765625
Iteration 22700: Loss = -11852.189453125
1
Iteration 22800: Loss = -11852.1884765625
Iteration 22900: Loss = -11852.1904296875
1
Iteration 23000: Loss = -11852.189453125
2
Iteration 23100: Loss = -11852.189453125
3
Iteration 23200: Loss = -11852.189453125
4
Iteration 23300: Loss = -11852.1884765625
Iteration 23400: Loss = -11852.1884765625
Iteration 23500: Loss = -11852.189453125
1
Iteration 23600: Loss = -11852.189453125
2
Iteration 23700: Loss = -11852.189453125
3
Iteration 23800: Loss = -11852.1884765625
Iteration 23900: Loss = -11852.1884765625
Iteration 24000: Loss = -11852.189453125
1
Iteration 24100: Loss = -11852.1884765625
Iteration 24200: Loss = -11852.189453125
1
Iteration 24300: Loss = -11852.1875
Iteration 24400: Loss = -11852.1884765625
1
Iteration 24500: Loss = -11852.1884765625
2
Iteration 24600: Loss = -11852.1884765625
3
Iteration 24700: Loss = -11852.1884765625
4
Iteration 24800: Loss = -11852.1875
Iteration 24900: Loss = -11852.1875
Iteration 25000: Loss = -11852.1884765625
1
Iteration 25100: Loss = -11852.1875
Iteration 25200: Loss = -11852.1884765625
1
Iteration 25300: Loss = -11852.1884765625
2
Iteration 25400: Loss = -11852.19140625
3
Iteration 25500: Loss = -11852.1875
Iteration 25600: Loss = -11852.189453125
1
Iteration 25700: Loss = -11852.1875
Iteration 25800: Loss = -11852.1884765625
1
Iteration 25900: Loss = -11852.1884765625
2
Iteration 26000: Loss = -11852.1875
Iteration 26100: Loss = -11852.1875
Iteration 26200: Loss = -11852.1884765625
1
Iteration 26300: Loss = -11852.1884765625
2
Iteration 26400: Loss = -11852.1884765625
3
Iteration 26500: Loss = -11852.1884765625
4
Iteration 26600: Loss = -11852.1884765625
5
Iteration 26700: Loss = -11852.1884765625
6
Iteration 26800: Loss = -11852.1875
Iteration 26900: Loss = -11852.1884765625
1
Iteration 27000: Loss = -11852.1875
Iteration 27100: Loss = -11852.1884765625
1
Iteration 27200: Loss = -11852.1884765625
2
Iteration 27300: Loss = -11852.1884765625
3
Iteration 27400: Loss = -11852.1884765625
4
Iteration 27500: Loss = -11852.1884765625
5
Iteration 27600: Loss = -11852.1875
Iteration 27700: Loss = -11852.1875
Iteration 27800: Loss = -11852.1875
Iteration 27900: Loss = -11852.1884765625
1
Iteration 28000: Loss = -11852.189453125
2
Iteration 28100: Loss = -11852.1884765625
3
Iteration 28200: Loss = -11852.1884765625
4
Iteration 28300: Loss = -11852.1875
Iteration 28400: Loss = -11852.1875
Iteration 28500: Loss = -11852.1884765625
1
Iteration 28600: Loss = -11852.1884765625
2
Iteration 28700: Loss = -11852.1875
Iteration 28800: Loss = -11852.1875
Iteration 28900: Loss = -11852.1884765625
1
Iteration 29000: Loss = -11852.1884765625
2
Iteration 29100: Loss = -11852.1884765625
3
Iteration 29200: Loss = -11852.1884765625
4
Iteration 29300: Loss = -11852.1875
Iteration 29400: Loss = -11852.1884765625
1
Iteration 29500: Loss = -11852.1875
Iteration 29600: Loss = -11852.1875
Iteration 29700: Loss = -11852.1875
Iteration 29800: Loss = -11852.1875
Iteration 29900: Loss = -11852.1875
pi: tensor([[0.6344, 0.3656],
        [0.3705, 0.6295]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5858, 0.4142], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3003, 0.0959],
         [0.4388, 0.3085]],

        [[0.0194, 0.0967],
         [0.0118, 0.9764]],

        [[0.0343, 0.1034],
         [0.8692, 0.2418]],

        [[0.3484, 0.0903],
         [0.0539, 0.9914]],

        [[0.9312, 0.0970],
         [0.9847, 0.0400]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9207907017983009
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.35871777622624434
Average Adjusted Rand Index: 0.9681575134563596
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -32399.02734375
Iteration 100: Loss = -19093.259765625
Iteration 200: Loss = -13661.9453125
Iteration 300: Loss = -12712.896484375
Iteration 400: Loss = -12540.345703125
Iteration 500: Loss = -12490.15625
Iteration 600: Loss = -12458.3515625
Iteration 700: Loss = -12433.833984375
Iteration 800: Loss = -12424.3076171875
Iteration 900: Loss = -12418.2744140625
Iteration 1000: Loss = -12413.923828125
Iteration 1100: Loss = -12410.6533203125
Iteration 1200: Loss = -12408.125
Iteration 1300: Loss = -12406.119140625
Iteration 1400: Loss = -12404.4970703125
Iteration 1500: Loss = -12403.16015625
Iteration 1600: Loss = -12402.037109375
Iteration 1700: Loss = -12401.080078125
Iteration 1800: Loss = -12400.2568359375
Iteration 1900: Loss = -12399.537109375
Iteration 2000: Loss = -12398.9052734375
Iteration 2100: Loss = -12398.3408203125
Iteration 2200: Loss = -12397.83984375
Iteration 2300: Loss = -12397.3916015625
Iteration 2400: Loss = -12396.986328125
Iteration 2500: Loss = -12396.6181640625
Iteration 2600: Loss = -12396.28515625
Iteration 2700: Loss = -12395.9794921875
Iteration 2800: Loss = -12395.69921875
Iteration 2900: Loss = -12395.443359375
Iteration 3000: Loss = -12395.2099609375
Iteration 3100: Loss = -12394.994140625
Iteration 3200: Loss = -12394.7939453125
Iteration 3300: Loss = -12394.6103515625
Iteration 3400: Loss = -12394.4404296875
Iteration 3500: Loss = -12394.283203125
Iteration 3600: Loss = -12394.134765625
Iteration 3700: Loss = -12394.0009765625
Iteration 3800: Loss = -12393.8759765625
Iteration 3900: Loss = -12393.7587890625
Iteration 4000: Loss = -12393.6484375
Iteration 4100: Loss = -12393.544921875
Iteration 4200: Loss = -12393.4501953125
Iteration 4300: Loss = -12393.3603515625
Iteration 4400: Loss = -12393.275390625
Iteration 4500: Loss = -12393.1962890625
Iteration 4600: Loss = -12393.123046875
Iteration 4700: Loss = -12393.0546875
Iteration 4800: Loss = -12392.9892578125
Iteration 4900: Loss = -12392.9287109375
Iteration 5000: Loss = -12392.8720703125
Iteration 5100: Loss = -12392.8173828125
Iteration 5200: Loss = -12392.765625
Iteration 5300: Loss = -12392.71875
Iteration 5400: Loss = -12392.6728515625
Iteration 5500: Loss = -12392.630859375
Iteration 5600: Loss = -12392.5888671875
Iteration 5700: Loss = -12392.552734375
Iteration 5800: Loss = -12392.5166015625
Iteration 5900: Loss = -12392.4814453125
Iteration 6000: Loss = -12392.451171875
Iteration 6100: Loss = -12392.419921875
Iteration 6200: Loss = -12392.392578125
Iteration 6300: Loss = -12392.365234375
Iteration 6400: Loss = -12392.33984375
Iteration 6500: Loss = -12392.314453125
Iteration 6600: Loss = -12392.29296875
Iteration 6700: Loss = -12392.2724609375
Iteration 6800: Loss = -12392.251953125
Iteration 6900: Loss = -12392.232421875
Iteration 7000: Loss = -12392.2138671875
Iteration 7100: Loss = -12392.197265625
Iteration 7200: Loss = -12392.1806640625
Iteration 7300: Loss = -12392.1630859375
Iteration 7400: Loss = -12392.1494140625
Iteration 7500: Loss = -12392.13671875
Iteration 7600: Loss = -12392.1220703125
Iteration 7700: Loss = -12392.111328125
Iteration 7800: Loss = -12392.1005859375
Iteration 7900: Loss = -12392.087890625
Iteration 8000: Loss = -12392.078125
Iteration 8100: Loss = -12392.06640625
Iteration 8200: Loss = -12392.05859375
Iteration 8300: Loss = -12392.0498046875
Iteration 8400: Loss = -12392.0419921875
Iteration 8500: Loss = -12392.0322265625
Iteration 8600: Loss = -12392.025390625
Iteration 8700: Loss = -12392.017578125
Iteration 8800: Loss = -12392.0107421875
Iteration 8900: Loss = -12392.00390625
Iteration 9000: Loss = -12391.9970703125
Iteration 9100: Loss = -12391.9921875
Iteration 9200: Loss = -12391.986328125
Iteration 9300: Loss = -12391.9814453125
Iteration 9400: Loss = -12391.9765625
Iteration 9500: Loss = -12391.9716796875
Iteration 9600: Loss = -12391.966796875
Iteration 9700: Loss = -12391.962890625
Iteration 9800: Loss = -12391.958984375
Iteration 9900: Loss = -12391.955078125
Iteration 10000: Loss = -12391.9521484375
Iteration 10100: Loss = -12391.947265625
Iteration 10200: Loss = -12391.9443359375
Iteration 10300: Loss = -12391.9404296875
Iteration 10400: Loss = -12391.9384765625
Iteration 10500: Loss = -12391.9345703125
Iteration 10600: Loss = -12391.93359375
Iteration 10700: Loss = -12391.9296875
Iteration 10800: Loss = -12391.92578125
Iteration 10900: Loss = -12391.9248046875
Iteration 11000: Loss = -12391.923828125
Iteration 11100: Loss = -12391.9208984375
Iteration 11200: Loss = -12391.9189453125
Iteration 11300: Loss = -12391.9169921875
Iteration 11400: Loss = -12391.916015625
Iteration 11500: Loss = -12391.9130859375
Iteration 11600: Loss = -12391.912109375
Iteration 11700: Loss = -12391.91015625
Iteration 11800: Loss = -12391.9091796875
Iteration 11900: Loss = -12391.908203125
Iteration 12000: Loss = -12391.9052734375
Iteration 12100: Loss = -12391.9052734375
Iteration 12200: Loss = -12391.904296875
Iteration 12300: Loss = -12391.9033203125
Iteration 12400: Loss = -12391.90234375
Iteration 12500: Loss = -12391.900390625
Iteration 12600: Loss = -12391.8974609375
Iteration 12700: Loss = -12391.896484375
Iteration 12800: Loss = -12391.8974609375
1
Iteration 12900: Loss = -12391.896484375
Iteration 13000: Loss = -12391.8955078125
Iteration 13100: Loss = -12391.892578125
Iteration 13200: Loss = -12391.8935546875
1
Iteration 13300: Loss = -12391.892578125
Iteration 13400: Loss = -12391.892578125
Iteration 13500: Loss = -12391.890625
Iteration 13600: Loss = -12391.890625
Iteration 13700: Loss = -12391.8896484375
Iteration 13800: Loss = -12391.888671875
Iteration 13900: Loss = -12391.888671875
Iteration 14000: Loss = -12391.88671875
Iteration 14100: Loss = -12391.88671875
Iteration 14200: Loss = -12391.8857421875
Iteration 14300: Loss = -12391.8857421875
Iteration 14400: Loss = -12391.884765625
Iteration 14500: Loss = -12391.8857421875
1
Iteration 14600: Loss = -12391.8857421875
2
Iteration 14700: Loss = -12391.8828125
Iteration 14800: Loss = -12391.8828125
Iteration 14900: Loss = -12391.8828125
Iteration 15000: Loss = -12391.8837890625
1
Iteration 15100: Loss = -12391.8828125
Iteration 15200: Loss = -12391.8828125
Iteration 15300: Loss = -12391.8818359375
Iteration 15400: Loss = -12391.880859375
Iteration 15500: Loss = -12391.8818359375
1
Iteration 15600: Loss = -12391.880859375
Iteration 15700: Loss = -12391.880859375
Iteration 15800: Loss = -12391.8818359375
1
Iteration 15900: Loss = -12391.8798828125
Iteration 16000: Loss = -12391.8828125
1
Iteration 16100: Loss = -12391.880859375
2
Iteration 16200: Loss = -12391.8818359375
3
Iteration 16300: Loss = -12391.8798828125
Iteration 16400: Loss = -12391.880859375
1
Iteration 16500: Loss = -12391.8798828125
Iteration 16600: Loss = -12391.8818359375
1
Iteration 16700: Loss = -12391.8798828125
Iteration 16800: Loss = -12391.880859375
1
Iteration 16900: Loss = -12391.87890625
Iteration 17000: Loss = -12391.87890625
Iteration 17100: Loss = -12391.876953125
Iteration 17200: Loss = -12391.8779296875
1
Iteration 17300: Loss = -12391.87890625
2
Iteration 17400: Loss = -12391.8779296875
3
Iteration 17500: Loss = -12391.876953125
Iteration 17600: Loss = -12391.8779296875
1
Iteration 17700: Loss = -12391.8759765625
Iteration 17800: Loss = -12391.87890625
1
Iteration 17900: Loss = -12391.8759765625
Iteration 18000: Loss = -12391.8759765625
Iteration 18100: Loss = -12391.8759765625
Iteration 18200: Loss = -12391.8740234375
Iteration 18300: Loss = -12391.873046875
Iteration 18400: Loss = -12391.8720703125
Iteration 18500: Loss = -12391.8701171875
Iteration 18600: Loss = -12391.8681640625
Iteration 18700: Loss = -12391.86328125
Iteration 18800: Loss = -12391.8525390625
Iteration 18900: Loss = -12391.802734375
Iteration 19000: Loss = -12391.509765625
Iteration 19100: Loss = -12391.4501953125
Iteration 19200: Loss = -12391.4208984375
Iteration 19300: Loss = -12391.4111328125
Iteration 19400: Loss = -12391.4091796875
Iteration 19500: Loss = -12391.404296875
Iteration 19600: Loss = -12391.4013671875
Iteration 19700: Loss = -12391.400390625
Iteration 19800: Loss = -12391.3994140625
Iteration 19900: Loss = -12391.392578125
Iteration 20000: Loss = -12391.3896484375
Iteration 20100: Loss = -12391.392578125
1
Iteration 20200: Loss = -12391.3857421875
Iteration 20300: Loss = -12391.3837890625
Iteration 20400: Loss = -12391.3818359375
Iteration 20500: Loss = -12391.3828125
1
Iteration 20600: Loss = -12391.380859375
Iteration 20700: Loss = -12391.376953125
Iteration 20800: Loss = -12391.376953125
Iteration 20900: Loss = -12391.3779296875
1
Iteration 21000: Loss = -12391.3779296875
2
Iteration 21100: Loss = -12391.3759765625
Iteration 21200: Loss = -12391.3759765625
Iteration 21300: Loss = -12391.3759765625
Iteration 21400: Loss = -12391.376953125
1
Iteration 21500: Loss = -12391.3740234375
Iteration 21600: Loss = -12391.3720703125
Iteration 21700: Loss = -12391.373046875
1
Iteration 21800: Loss = -12391.3720703125
Iteration 21900: Loss = -12391.37109375
Iteration 22000: Loss = -12391.3720703125
1
Iteration 22100: Loss = -12391.3701171875
Iteration 22200: Loss = -12391.3701171875
Iteration 22300: Loss = -12391.3681640625
Iteration 22400: Loss = -12391.3662109375
Iteration 22500: Loss = -12391.2236328125
Iteration 22600: Loss = -12391.1533203125
Iteration 22700: Loss = -12391.1357421875
Iteration 22800: Loss = -12391.12109375
Iteration 22900: Loss = -12391.1181640625
Iteration 23000: Loss = -12391.1162109375
Iteration 23100: Loss = -12391.1162109375
Iteration 23200: Loss = -12391.1123046875
Iteration 23300: Loss = -12391.107421875
Iteration 23400: Loss = -12391.109375
1
Iteration 23500: Loss = -12391.107421875
Iteration 23600: Loss = -12391.107421875
Iteration 23700: Loss = -12391.1064453125
Iteration 23800: Loss = -12391.1044921875
Iteration 23900: Loss = -12391.1044921875
Iteration 24000: Loss = -12391.1025390625
Iteration 24100: Loss = -12391.1015625
Iteration 24200: Loss = -12391.1025390625
1
Iteration 24300: Loss = -12391.1044921875
2
Iteration 24400: Loss = -12391.103515625
3
Iteration 24500: Loss = -12391.103515625
4
Iteration 24600: Loss = -12391.103515625
5
Iteration 24700: Loss = -12391.103515625
6
Iteration 24800: Loss = -12391.1025390625
7
Iteration 24900: Loss = -12391.1025390625
8
Iteration 25000: Loss = -12391.1025390625
9
Iteration 25100: Loss = -12391.1025390625
10
Iteration 25200: Loss = -12391.1025390625
11
Iteration 25300: Loss = -12391.1015625
Iteration 25400: Loss = -12391.1025390625
1
Iteration 25500: Loss = -12391.1015625
Iteration 25600: Loss = -12391.1005859375
Iteration 25700: Loss = -12391.1005859375
Iteration 25800: Loss = -12391.1015625
1
Iteration 25900: Loss = -12391.103515625
2
Iteration 26000: Loss = -12391.1015625
3
Iteration 26100: Loss = -12391.1025390625
4
Iteration 26200: Loss = -12391.1025390625
5
Iteration 26300: Loss = -12391.1025390625
6
Iteration 26400: Loss = -12391.1025390625
7
Iteration 26500: Loss = -12391.1025390625
8
Iteration 26600: Loss = -12391.103515625
9
Iteration 26700: Loss = -12391.1005859375
Iteration 26800: Loss = -12391.1015625
1
Iteration 26900: Loss = -12391.1015625
2
Iteration 27000: Loss = -12391.1025390625
3
Iteration 27100: Loss = -12391.1025390625
4
Iteration 27200: Loss = -12391.1025390625
5
Iteration 27300: Loss = -12391.103515625
6
Iteration 27400: Loss = -12391.1015625
7
Iteration 27500: Loss = -12391.1025390625
8
Iteration 27600: Loss = -12391.1044921875
9
Iteration 27700: Loss = -12391.1025390625
10
Iteration 27800: Loss = -12391.1025390625
11
Iteration 27900: Loss = -12391.1044921875
12
Iteration 28000: Loss = -12391.1015625
13
Iteration 28100: Loss = -12391.1015625
14
Iteration 28200: Loss = -12391.103515625
15
Stopping early at iteration 28200 due to no improvement.
pi: tensor([[9.9997e-01, 2.5787e-05],
        [2.0264e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0375, 0.9625], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2106, 0.2078],
         [0.1253, 0.1992]],

        [[0.1069, 0.2194],
         [0.2525, 0.0068]],

        [[0.9093, 0.2475],
         [0.7415, 0.1577]],

        [[0.0487, 0.1640],
         [0.0077, 0.0564]],

        [[0.9519, 0.2283],
         [0.1450, 0.6628]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.35871777622624434, 0.0] [0.9681575134563596, 0.0] [11852.1875, 12391.103515625]
-------------------------------------
This iteration is 64
True Objective function: Loss = -11870.916908844172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -40415.859375
Iteration 100: Loss = -21592.791015625
Iteration 200: Loss = -14204.54296875
Iteration 300: Loss = -12843.146484375
Iteration 400: Loss = -12613.771484375
Iteration 500: Loss = -12530.5166015625
Iteration 600: Loss = -12488.93359375
Iteration 700: Loss = -12470.0654296875
Iteration 800: Loss = -12457.4111328125
Iteration 900: Loss = -12448.1376953125
Iteration 1000: Loss = -12431.1357421875
Iteration 1100: Loss = -12421.86328125
Iteration 1200: Loss = -12418.185546875
Iteration 1300: Loss = -12415.248046875
Iteration 1400: Loss = -12412.83203125
Iteration 1500: Loss = -12410.80859375
Iteration 1600: Loss = -12409.0927734375
Iteration 1700: Loss = -12407.6171875
Iteration 1800: Loss = -12406.3369140625
Iteration 1900: Loss = -12405.2177734375
Iteration 2000: Loss = -12404.2333984375
Iteration 2100: Loss = -12403.3623046875
Iteration 2200: Loss = -12402.5849609375
Iteration 2300: Loss = -12401.8876953125
Iteration 2400: Loss = -12401.26171875
Iteration 2500: Loss = -12400.6953125
Iteration 2600: Loss = -12400.1796875
Iteration 2700: Loss = -12399.708984375
Iteration 2800: Loss = -12399.2822265625
Iteration 2900: Loss = -12398.888671875
Iteration 3000: Loss = -12398.52734375
Iteration 3100: Loss = -12398.1953125
Iteration 3200: Loss = -12397.8857421875
Iteration 3300: Loss = -12397.603515625
Iteration 3400: Loss = -12397.33984375
Iteration 3500: Loss = -12397.09765625
Iteration 3600: Loss = -12396.869140625
Iteration 3700: Loss = -12396.658203125
Iteration 3800: Loss = -12396.4619140625
Iteration 3900: Loss = -12396.279296875
Iteration 4000: Loss = -12396.107421875
Iteration 4100: Loss = -12395.9453125
Iteration 4200: Loss = -12395.7958984375
Iteration 4300: Loss = -12395.654296875
Iteration 4400: Loss = -12395.5224609375
Iteration 4500: Loss = -12395.3984375
Iteration 4600: Loss = -12395.2802734375
Iteration 4700: Loss = -12395.171875
Iteration 4800: Loss = -12395.06640625
Iteration 4900: Loss = -12394.9697265625
Iteration 5000: Loss = -12394.8779296875
Iteration 5100: Loss = -12394.7919921875
Iteration 5200: Loss = -12394.7099609375
Iteration 5300: Loss = -12394.6337890625
Iteration 5400: Loss = -12394.560546875
Iteration 5500: Loss = -12394.4912109375
Iteration 5600: Loss = -12394.427734375
Iteration 5700: Loss = -12394.3662109375
Iteration 5800: Loss = -12394.3095703125
Iteration 5900: Loss = -12394.2529296875
Iteration 6000: Loss = -12394.2021484375
Iteration 6100: Loss = -12394.1552734375
Iteration 6200: Loss = -12394.107421875
Iteration 6300: Loss = -12394.0634765625
Iteration 6400: Loss = -12394.0224609375
Iteration 6500: Loss = -12393.9814453125
Iteration 6600: Loss = -12393.9462890625
Iteration 6700: Loss = -12393.9091796875
Iteration 6800: Loss = -12393.8779296875
Iteration 6900: Loss = -12393.8447265625
Iteration 7000: Loss = -12393.8154296875
Iteration 7100: Loss = -12393.787109375
Iteration 7200: Loss = -12393.7587890625
Iteration 7300: Loss = -12393.7333984375
Iteration 7400: Loss = -12393.7109375
Iteration 7500: Loss = -12393.6875
Iteration 7600: Loss = -12393.6650390625
Iteration 7700: Loss = -12393.64453125
Iteration 7800: Loss = -12393.625
Iteration 7900: Loss = -12393.607421875
Iteration 8000: Loss = -12393.5888671875
Iteration 8100: Loss = -12393.572265625
Iteration 8200: Loss = -12393.5556640625
Iteration 8300: Loss = -12393.5400390625
Iteration 8400: Loss = -12393.5263671875
Iteration 8500: Loss = -12393.5146484375
Iteration 8600: Loss = -12393.5
Iteration 8700: Loss = -12393.48828125
Iteration 8800: Loss = -12393.4775390625
Iteration 8900: Loss = -12393.4658203125
Iteration 9000: Loss = -12393.4541015625
Iteration 9100: Loss = -12393.4443359375
Iteration 9200: Loss = -12393.4345703125
Iteration 9300: Loss = -12393.427734375
Iteration 9400: Loss = -12393.4189453125
Iteration 9500: Loss = -12393.4091796875
Iteration 9600: Loss = -12393.40234375
Iteration 9700: Loss = -12393.39453125
Iteration 9800: Loss = -12393.3876953125
Iteration 9900: Loss = -12393.3828125
Iteration 10000: Loss = -12393.3759765625
Iteration 10100: Loss = -12393.369140625
Iteration 10200: Loss = -12393.3642578125
Iteration 10300: Loss = -12393.3583984375
Iteration 10400: Loss = -12393.3515625
Iteration 10500: Loss = -12393.34765625
Iteration 10600: Loss = -12393.341796875
Iteration 10700: Loss = -12393.33984375
Iteration 10800: Loss = -12393.333984375
Iteration 10900: Loss = -12393.3330078125
Iteration 11000: Loss = -12393.3271484375
Iteration 11100: Loss = -12393.322265625
Iteration 11200: Loss = -12393.3193359375
Iteration 11300: Loss = -12393.314453125
Iteration 11400: Loss = -12393.3115234375
Iteration 11500: Loss = -12393.30859375
Iteration 11600: Loss = -12393.306640625
Iteration 11700: Loss = -12393.302734375
Iteration 11800: Loss = -12393.30078125
Iteration 11900: Loss = -12393.2978515625
Iteration 12000: Loss = -12393.2939453125
Iteration 12100: Loss = -12393.2900390625
Iteration 12200: Loss = -12393.2890625
Iteration 12300: Loss = -12393.28515625
Iteration 12400: Loss = -12393.2802734375
Iteration 12500: Loss = -12393.2724609375
Iteration 12600: Loss = -12393.23046875
Iteration 12700: Loss = -12393.015625
Iteration 12800: Loss = -12392.875
Iteration 12900: Loss = -12392.6748046875
Iteration 13000: Loss = -12392.0361328125
Iteration 13100: Loss = -12391.953125
Iteration 13200: Loss = -12391.9091796875
Iteration 13300: Loss = -12391.87890625
Iteration 13400: Loss = -12391.8564453125
Iteration 13500: Loss = -12391.8349609375
Iteration 13600: Loss = -12391.81640625
Iteration 13700: Loss = -12391.7998046875
Iteration 13800: Loss = -12391.783203125
Iteration 13900: Loss = -12391.7646484375
Iteration 14000: Loss = -12391.744140625
Iteration 14100: Loss = -12391.7236328125
Iteration 14200: Loss = -12391.69921875
Iteration 14300: Loss = -12391.669921875
Iteration 14400: Loss = -12391.634765625
Iteration 14500: Loss = -12391.58984375
Iteration 14600: Loss = -12391.537109375
Iteration 14700: Loss = -12391.4677734375
Iteration 14800: Loss = -12391.380859375
Iteration 14900: Loss = -12391.2783203125
Iteration 15000: Loss = -12391.16015625
Iteration 15100: Loss = -12391.0458984375
Iteration 15200: Loss = -12390.951171875
Iteration 15300: Loss = -12390.8720703125
Iteration 15400: Loss = -12390.8193359375
Iteration 15500: Loss = -12390.7861328125
Iteration 15600: Loss = -12390.7578125
Iteration 15700: Loss = -12390.6884765625
Iteration 15800: Loss = -12390.6591796875
Iteration 15900: Loss = -12390.6279296875
Iteration 16000: Loss = -12390.5927734375
Iteration 16100: Loss = -12390.537109375
Iteration 16200: Loss = -12390.484375
Iteration 16300: Loss = -12390.4658203125
Iteration 16400: Loss = -12390.458984375
Iteration 16500: Loss = -12390.455078125
Iteration 16600: Loss = -12390.453125
Iteration 16700: Loss = -12390.4541015625
1
Iteration 16800: Loss = -12390.451171875
Iteration 16900: Loss = -12390.44921875
Iteration 17000: Loss = -12390.44921875
Iteration 17100: Loss = -12390.4501953125
1
Iteration 17200: Loss = -12390.4482421875
Iteration 17300: Loss = -12390.4482421875
Iteration 17400: Loss = -12390.447265625
Iteration 17500: Loss = -12390.4482421875
1
Iteration 17600: Loss = -12390.447265625
Iteration 17700: Loss = -12390.4462890625
Iteration 17800: Loss = -12390.4462890625
Iteration 17900: Loss = -12390.44921875
1
Iteration 18000: Loss = -12390.4482421875
2
Iteration 18100: Loss = -12390.447265625
3
Iteration 18200: Loss = -12390.4482421875
4
Iteration 18300: Loss = -12390.447265625
5
Iteration 18400: Loss = -12390.44921875
6
Iteration 18500: Loss = -12390.44921875
7
Iteration 18600: Loss = -12390.447265625
8
Iteration 18700: Loss = -12390.4482421875
9
Iteration 18800: Loss = -12390.447265625
10
Iteration 18900: Loss = -12390.4462890625
Iteration 19000: Loss = -12390.4462890625
Iteration 19100: Loss = -12390.4482421875
1
Iteration 19200: Loss = -12390.447265625
2
Iteration 19300: Loss = -12390.4482421875
3
Iteration 19400: Loss = -12390.44921875
4
Iteration 19500: Loss = -12390.4482421875
5
Iteration 19600: Loss = -12390.4453125
Iteration 19700: Loss = -12390.447265625
1
Iteration 19800: Loss = -12390.4482421875
2
Iteration 19900: Loss = -12390.4482421875
3
Iteration 20000: Loss = -12390.4482421875
4
Iteration 20100: Loss = -12390.4482421875
5
Iteration 20200: Loss = -12390.44921875
6
Iteration 20300: Loss = -12390.4462890625
7
Iteration 20400: Loss = -12390.447265625
8
Iteration 20500: Loss = -12390.447265625
9
Iteration 20600: Loss = -12390.4462890625
10
Iteration 20700: Loss = -12390.447265625
11
Iteration 20800: Loss = -12390.4462890625
12
Iteration 20900: Loss = -12390.4462890625
13
Iteration 21000: Loss = -12390.447265625
14
Iteration 21100: Loss = -12390.447265625
15
Stopping early at iteration 21100 due to no improvement.
pi: tensor([[0.9870, 0.0130],
        [0.8315, 0.1685]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8102, 0.1898], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2012, 0.2128],
         [0.0070, 0.2197]],

        [[0.7850, 0.1642],
         [0.9738, 0.4654]],

        [[0.0649, 0.1024],
         [0.0727, 0.9431]],

        [[0.9309, 0.0940],
         [0.9920, 0.9787]],

        [[0.9029, 0.2422],
         [0.0367, 0.7812]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0002646099377518749
Average Adjusted Rand Index: 0.0007576155189725879
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21319.728515625
Iteration 100: Loss = -15598.1845703125
Iteration 200: Loss = -13006.056640625
Iteration 300: Loss = -12546.8662109375
Iteration 400: Loss = -12477.001953125
Iteration 500: Loss = -12444.833984375
Iteration 600: Loss = -12425.8994140625
Iteration 700: Loss = -12413.5751953125
Iteration 800: Loss = -12407.2548828125
Iteration 900: Loss = -12403.9599609375
Iteration 1000: Loss = -12401.77734375
Iteration 1100: Loss = -12400.236328125
Iteration 1200: Loss = -12399.087890625
Iteration 1300: Loss = -12398.19921875
Iteration 1400: Loss = -12397.49609375
Iteration 1500: Loss = -12396.9306640625
Iteration 1600: Loss = -12396.4638671875
Iteration 1700: Loss = -12396.07421875
Iteration 1800: Loss = -12395.7451171875
Iteration 1900: Loss = -12395.46484375
Iteration 2000: Loss = -12395.220703125
Iteration 2100: Loss = -12395.0087890625
Iteration 2200: Loss = -12394.82421875
Iteration 2300: Loss = -12394.6591796875
Iteration 2400: Loss = -12394.509765625
Iteration 2500: Loss = -12394.37890625
Iteration 2600: Loss = -12394.2568359375
Iteration 2700: Loss = -12394.1435546875
Iteration 2800: Loss = -12394.0341796875
Iteration 2900: Loss = -12393.94140625
Iteration 3000: Loss = -12393.85546875
Iteration 3100: Loss = -12393.7744140625
Iteration 3200: Loss = -12393.697265625
Iteration 3300: Loss = -12393.62109375
Iteration 3400: Loss = -12393.5419921875
Iteration 3500: Loss = -12393.4609375
Iteration 3600: Loss = -12393.369140625
Iteration 3700: Loss = -12393.2607421875
Iteration 3800: Loss = -12393.1240234375
Iteration 3900: Loss = -12392.9404296875
Iteration 4000: Loss = -12392.6826171875
Iteration 4100: Loss = -12392.337890625
Iteration 4200: Loss = -12391.9677734375
Iteration 4300: Loss = -12391.69140625
Iteration 4400: Loss = -12391.5302734375
Iteration 4500: Loss = -12391.4296875
Iteration 4600: Loss = -12391.353515625
Iteration 4700: Loss = -12391.29296875
Iteration 4800: Loss = -12391.240234375
Iteration 4900: Loss = -12391.1943359375
Iteration 5000: Loss = -12391.1552734375
Iteration 5100: Loss = -12391.12109375
Iteration 5200: Loss = -12391.0908203125
Iteration 5300: Loss = -12391.0634765625
Iteration 5400: Loss = -12391.0380859375
Iteration 5500: Loss = -12391.015625
Iteration 5600: Loss = -12390.9931640625
Iteration 5700: Loss = -12390.974609375
Iteration 5800: Loss = -12390.9560546875
Iteration 5900: Loss = -12390.939453125
Iteration 6000: Loss = -12390.921875
Iteration 6100: Loss = -12390.9072265625
Iteration 6200: Loss = -12390.8935546875
Iteration 6300: Loss = -12390.880859375
Iteration 6400: Loss = -12390.869140625
Iteration 6500: Loss = -12390.8583984375
Iteration 6600: Loss = -12390.8486328125
Iteration 6700: Loss = -12390.83984375
Iteration 6800: Loss = -12390.828125
Iteration 6900: Loss = -12390.8203125
Iteration 7000: Loss = -12390.8125
Iteration 7100: Loss = -12390.8046875
Iteration 7200: Loss = -12390.7958984375
Iteration 7300: Loss = -12390.7861328125
Iteration 7400: Loss = -12390.77734375
Iteration 7500: Loss = -12390.7734375
Iteration 7600: Loss = -12390.7685546875
Iteration 7700: Loss = -12390.7626953125
Iteration 7800: Loss = -12390.7578125
Iteration 7900: Loss = -12390.7548828125
Iteration 8000: Loss = -12390.7509765625
Iteration 8100: Loss = -12390.748046875
Iteration 8200: Loss = -12390.7431640625
Iteration 8300: Loss = -12390.740234375
Iteration 8400: Loss = -12390.73828125
Iteration 8500: Loss = -12390.7353515625
Iteration 8600: Loss = -12390.732421875
Iteration 8700: Loss = -12390.7314453125
Iteration 8800: Loss = -12390.7294921875
Iteration 8900: Loss = -12390.7255859375
Iteration 9000: Loss = -12390.7255859375
Iteration 9100: Loss = -12390.7236328125
Iteration 9200: Loss = -12390.7236328125
Iteration 9300: Loss = -12390.720703125
Iteration 9400: Loss = -12390.7197265625
Iteration 9500: Loss = -12390.7197265625
Iteration 9600: Loss = -12390.7177734375
Iteration 9700: Loss = -12390.716796875
Iteration 9800: Loss = -12390.716796875
Iteration 9900: Loss = -12390.716796875
Iteration 10000: Loss = -12390.7158203125
Iteration 10100: Loss = -12390.7158203125
Iteration 10200: Loss = -12390.71484375
Iteration 10300: Loss = -12390.7138671875
Iteration 10400: Loss = -12390.7158203125
1
Iteration 10500: Loss = -12390.7138671875
Iteration 10600: Loss = -12390.71484375
1
Iteration 10700: Loss = -12390.7119140625
Iteration 10800: Loss = -12390.712890625
1
Iteration 10900: Loss = -12390.7119140625
Iteration 11000: Loss = -12390.712890625
1
Iteration 11100: Loss = -12390.7109375
Iteration 11200: Loss = -12390.7109375
Iteration 11300: Loss = -12390.7109375
Iteration 11400: Loss = -12390.7109375
Iteration 11500: Loss = -12390.7099609375
Iteration 11600: Loss = -12390.708984375
Iteration 11700: Loss = -12390.7080078125
Iteration 11800: Loss = -12390.7080078125
Iteration 11900: Loss = -12390.70703125
Iteration 12000: Loss = -12390.705078125
Iteration 12100: Loss = -12390.6953125
Iteration 12200: Loss = -12390.689453125
Iteration 12300: Loss = -12390.681640625
Iteration 12400: Loss = -12390.671875
Iteration 12500: Loss = -12390.658203125
Iteration 12600: Loss = -12390.64453125
Iteration 12700: Loss = -12390.6220703125
Iteration 12800: Loss = -12390.59375
Iteration 12900: Loss = -12390.55078125
Iteration 13000: Loss = -12390.5
Iteration 13100: Loss = -12390.4619140625
Iteration 13200: Loss = -12390.4521484375
Iteration 13300: Loss = -12390.44921875
Iteration 13400: Loss = -12390.44921875
Iteration 13500: Loss = -12390.4482421875
Iteration 13600: Loss = -12390.447265625
Iteration 13700: Loss = -12390.4462890625
Iteration 13800: Loss = -12390.447265625
1
Iteration 13900: Loss = -12390.4482421875
2
Iteration 14000: Loss = -12390.447265625
3
Iteration 14100: Loss = -12390.4462890625
Iteration 14200: Loss = -12390.4482421875
1
Iteration 14300: Loss = -12390.447265625
2
Iteration 14400: Loss = -12390.447265625
3
Iteration 14500: Loss = -12390.447265625
4
Iteration 14600: Loss = -12390.447265625
5
Iteration 14700: Loss = -12390.4462890625
Iteration 14800: Loss = -12390.4462890625
Iteration 14900: Loss = -12390.4462890625
Iteration 15000: Loss = -12390.447265625
1
Iteration 15100: Loss = -12390.447265625
2
Iteration 15200: Loss = -12390.447265625
3
Iteration 15300: Loss = -12390.4462890625
Iteration 15400: Loss = -12390.4453125
Iteration 15500: Loss = -12390.447265625
1
Iteration 15600: Loss = -12390.4462890625
2
Iteration 15700: Loss = -12390.4453125
Iteration 15800: Loss = -12390.447265625
1
Iteration 15900: Loss = -12390.4482421875
2
Iteration 16000: Loss = -12390.4482421875
3
Iteration 16100: Loss = -12390.447265625
4
Iteration 16200: Loss = -12390.4462890625
5
Iteration 16300: Loss = -12390.4453125
Iteration 16400: Loss = -12390.4462890625
1
Iteration 16500: Loss = -12390.447265625
2
Iteration 16600: Loss = -12390.4462890625
3
Iteration 16700: Loss = -12390.4462890625
4
Iteration 16800: Loss = -12390.447265625
5
Iteration 16900: Loss = -12390.447265625
6
Iteration 17000: Loss = -12390.447265625
7
Iteration 17100: Loss = -12390.447265625
8
Iteration 17200: Loss = -12390.4462890625
9
Iteration 17300: Loss = -12390.4462890625
10
Iteration 17400: Loss = -12390.447265625
11
Iteration 17500: Loss = -12390.447265625
12
Iteration 17600: Loss = -12390.4462890625
13
Iteration 17700: Loss = -12390.447265625
14
Iteration 17800: Loss = -12390.447265625
15
Stopping early at iteration 17800 due to no improvement.
pi: tensor([[0.1684, 0.8316],
        [0.0130, 0.9870]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1908, 0.8092], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2197, 0.2127],
         [0.9150, 0.2011]],

        [[0.8917, 0.1644],
         [0.9442, 0.1253]],

        [[0.6508, 0.1025],
         [0.9872, 0.5264]],

        [[0.0106, 0.0940],
         [0.9064, 0.0081]],

        [[0.9562, 0.2422],
         [0.1415, 0.7814]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0002646099377518749
Average Adjusted Rand Index: 0.0007576155189725879
[0.0002646099377518749, 0.0002646099377518749] [0.0007576155189725879, 0.0007576155189725879] [12390.447265625, 12390.447265625]
-------------------------------------
This iteration is 65
True Objective function: Loss = -11913.95680423985
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28398.806640625
Iteration 100: Loss = -18549.76953125
Iteration 200: Loss = -13386.2587890625
Iteration 300: Loss = -12700.5009765625
Iteration 400: Loss = -12592.2998046875
Iteration 500: Loss = -12542.349609375
Iteration 600: Loss = -12508.380859375
Iteration 700: Loss = -12488.42578125
Iteration 800: Loss = -12472.02734375
Iteration 900: Loss = -12454.5908203125
Iteration 1000: Loss = -12433.6962890625
Iteration 1100: Loss = -12422.6552734375
Iteration 1200: Loss = -12410.138671875
Iteration 1300: Loss = -12395.244140625
Iteration 1400: Loss = -12382.0009765625
Iteration 1500: Loss = -12361.6376953125
Iteration 1600: Loss = -12347.3466796875
Iteration 1700: Loss = -12334.7060546875
Iteration 1800: Loss = -12328.53125
Iteration 1900: Loss = -12325.74609375
Iteration 2000: Loss = -12323.9306640625
Iteration 2100: Loss = -12322.4697265625
Iteration 2200: Loss = -12320.9873046875
Iteration 2300: Loss = -12316.99609375
Iteration 2400: Loss = -12315.677734375
Iteration 2500: Loss = -12312.7978515625
Iteration 2600: Loss = -12311.091796875
Iteration 2700: Loss = -12310.5859375
Iteration 2800: Loss = -12310.18359375
Iteration 2900: Loss = -12309.8466796875
Iteration 3000: Loss = -12309.5615234375
Iteration 3100: Loss = -12309.3125
Iteration 3200: Loss = -12309.095703125
Iteration 3300: Loss = -12308.9033203125
Iteration 3400: Loss = -12308.732421875
Iteration 3500: Loss = -12308.580078125
Iteration 3600: Loss = -12308.4423828125
Iteration 3700: Loss = -12308.3173828125
Iteration 3800: Loss = -12308.2021484375
Iteration 3900: Loss = -12308.0712890625
Iteration 4000: Loss = -12304.2451171875
Iteration 4100: Loss = -12303.734375
Iteration 4200: Loss = -12303.5263671875
Iteration 4300: Loss = -12303.3740234375
Iteration 4400: Loss = -12303.251953125
Iteration 4500: Loss = -12303.146484375
Iteration 4600: Loss = -12303.0546875
Iteration 4700: Loss = -12302.9736328125
Iteration 4800: Loss = -12302.8984375
Iteration 4900: Loss = -12302.833984375
Iteration 5000: Loss = -12302.7724609375
Iteration 5100: Loss = -12302.7177734375
Iteration 5200: Loss = -12302.666015625
Iteration 5300: Loss = -12302.6181640625
Iteration 5400: Loss = -12302.5771484375
Iteration 5500: Loss = -12302.53515625
Iteration 5600: Loss = -12302.4990234375
Iteration 5700: Loss = -12302.46484375
Iteration 5800: Loss = -12302.4306640625
Iteration 5900: Loss = -12302.4013671875
Iteration 6000: Loss = -12302.373046875
Iteration 6100: Loss = -12302.345703125
Iteration 6200: Loss = -12302.3212890625
Iteration 6300: Loss = -12302.2978515625
Iteration 6400: Loss = -12302.27734375
Iteration 6500: Loss = -12302.255859375
Iteration 6600: Loss = -12302.2373046875
Iteration 6700: Loss = -12302.2177734375
Iteration 6800: Loss = -12302.2021484375
Iteration 6900: Loss = -12302.18359375
Iteration 7000: Loss = -12302.1689453125
Iteration 7100: Loss = -12302.154296875
Iteration 7200: Loss = -12302.140625
Iteration 7300: Loss = -12302.126953125
Iteration 7400: Loss = -12302.1162109375
Iteration 7500: Loss = -12302.1044921875
Iteration 7600: Loss = -12302.09375
Iteration 7700: Loss = -12302.083984375
Iteration 7800: Loss = -12302.0732421875
Iteration 7900: Loss = -12302.0654296875
Iteration 8000: Loss = -12302.056640625
Iteration 8100: Loss = -12302.048828125
Iteration 8200: Loss = -12302.041015625
Iteration 8300: Loss = -12302.033203125
Iteration 8400: Loss = -12302.0263671875
Iteration 8500: Loss = -12302.021484375
Iteration 8600: Loss = -12302.0146484375
Iteration 8700: Loss = -12302.009765625
Iteration 8800: Loss = -12302.0029296875
Iteration 8900: Loss = -12301.998046875
Iteration 9000: Loss = -12301.9921875
Iteration 9100: Loss = -12301.9873046875
Iteration 9200: Loss = -12301.984375
Iteration 9300: Loss = -12301.9794921875
Iteration 9400: Loss = -12301.9755859375
Iteration 9500: Loss = -12301.970703125
Iteration 9600: Loss = -12301.9677734375
Iteration 9700: Loss = -12301.9638671875
Iteration 9800: Loss = -12301.9619140625
Iteration 9900: Loss = -12301.958984375
Iteration 10000: Loss = -12301.9541015625
Iteration 10100: Loss = -12301.951171875
Iteration 10200: Loss = -12301.94921875
Iteration 10300: Loss = -12301.947265625
Iteration 10400: Loss = -12301.943359375
Iteration 10500: Loss = -12301.9423828125
Iteration 10600: Loss = -12301.939453125
Iteration 10700: Loss = -12301.9375
Iteration 10800: Loss = -12301.935546875
Iteration 10900: Loss = -12301.93359375
Iteration 11000: Loss = -12301.9306640625
Iteration 11100: Loss = -12301.9296875
Iteration 11200: Loss = -12301.9296875
Iteration 11300: Loss = -12301.927734375
Iteration 11400: Loss = -12301.9267578125
Iteration 11500: Loss = -12301.9248046875
Iteration 11600: Loss = -12301.921875
Iteration 11700: Loss = -12301.921875
Iteration 11800: Loss = -12301.9208984375
Iteration 11900: Loss = -12301.9208984375
Iteration 12000: Loss = -12301.9189453125
Iteration 12100: Loss = -12301.9169921875
Iteration 12200: Loss = -12301.9169921875
Iteration 12300: Loss = -12301.916015625
Iteration 12400: Loss = -12301.916015625
Iteration 12500: Loss = -12301.9140625
Iteration 12600: Loss = -12301.912109375
Iteration 12700: Loss = -12301.912109375
Iteration 12800: Loss = -12301.912109375
Iteration 12900: Loss = -12301.912109375
Iteration 13000: Loss = -12301.9111328125
Iteration 13100: Loss = -12301.9111328125
Iteration 13200: Loss = -12301.91015625
Iteration 13300: Loss = -12301.9091796875
Iteration 13400: Loss = -12301.908203125
Iteration 13500: Loss = -12301.9091796875
1
Iteration 13600: Loss = -12301.9072265625
Iteration 13700: Loss = -12301.9072265625
Iteration 13800: Loss = -12301.90625
Iteration 13900: Loss = -12301.916015625
1
Iteration 14000: Loss = -12301.9052734375
Iteration 14100: Loss = -12301.9052734375
Iteration 14200: Loss = -12301.9033203125
Iteration 14300: Loss = -12301.9052734375
1
Iteration 14400: Loss = -12301.904296875
2
Iteration 14500: Loss = -12301.904296875
3
Iteration 14600: Loss = -12301.9052734375
4
Iteration 14700: Loss = -12301.9033203125
Iteration 14800: Loss = -12301.90234375
Iteration 14900: Loss = -12301.904296875
1
Iteration 15000: Loss = -12301.9033203125
2
Iteration 15100: Loss = -12301.904296875
3
Iteration 15200: Loss = -12301.9033203125
4
Iteration 15300: Loss = -12301.904296875
5
Iteration 15400: Loss = -12301.90234375
Iteration 15500: Loss = -12301.90234375
Iteration 15600: Loss = -12301.90234375
Iteration 15700: Loss = -12301.900390625
Iteration 15800: Loss = -12301.9013671875
1
Iteration 15900: Loss = -12301.90234375
2
Iteration 16000: Loss = -12301.900390625
Iteration 16100: Loss = -12301.900390625
Iteration 16200: Loss = -12301.900390625
Iteration 16300: Loss = -12301.900390625
Iteration 16400: Loss = -12301.9013671875
1
Iteration 16500: Loss = -12301.900390625
Iteration 16600: Loss = -12301.900390625
Iteration 16700: Loss = -12301.900390625
Iteration 16800: Loss = -12301.8984375
Iteration 16900: Loss = -12301.900390625
1
Iteration 17000: Loss = -12301.8994140625
2
Iteration 17100: Loss = -12301.8994140625
3
Iteration 17200: Loss = -12301.900390625
4
Iteration 17300: Loss = -12301.8994140625
5
Iteration 17400: Loss = -12301.8994140625
6
Iteration 17500: Loss = -12301.8984375
Iteration 17600: Loss = -12301.900390625
1
Iteration 17700: Loss = -12301.900390625
2
Iteration 17800: Loss = -12301.8994140625
3
Iteration 17900: Loss = -12301.9013671875
4
Iteration 18000: Loss = -12301.8984375
Iteration 18100: Loss = -12301.8994140625
1
Iteration 18200: Loss = -12301.8984375
Iteration 18300: Loss = -12301.8994140625
1
Iteration 18400: Loss = -12301.8974609375
Iteration 18500: Loss = -12301.8994140625
1
Iteration 18600: Loss = -12301.8984375
2
Iteration 18700: Loss = -12301.8974609375
Iteration 18800: Loss = -12301.8974609375
Iteration 18900: Loss = -12301.8984375
1
Iteration 19000: Loss = -12301.8994140625
2
Iteration 19100: Loss = -12301.8984375
3
Iteration 19200: Loss = -12301.8974609375
Iteration 19300: Loss = -12301.900390625
1
Iteration 19400: Loss = -12301.8994140625
2
Iteration 19500: Loss = -12301.8984375
3
Iteration 19600: Loss = -12301.8984375
4
Iteration 19700: Loss = -12301.8974609375
Iteration 19800: Loss = -12301.8974609375
Iteration 19900: Loss = -12301.8994140625
1
Iteration 20000: Loss = -12301.8984375
2
Iteration 20100: Loss = -12301.8994140625
3
Iteration 20200: Loss = -12301.8994140625
4
Iteration 20300: Loss = -12301.8984375
5
Iteration 20400: Loss = -12301.8984375
6
Iteration 20500: Loss = -12301.8974609375
Iteration 20600: Loss = -12301.900390625
1
Iteration 20700: Loss = -12301.8994140625
2
Iteration 20800: Loss = -12301.900390625
3
Iteration 20900: Loss = -12301.8984375
4
Iteration 21000: Loss = -12301.8984375
5
Iteration 21100: Loss = -12301.8984375
6
Iteration 21200: Loss = -12301.9013671875
7
Iteration 21300: Loss = -12301.900390625
8
Iteration 21400: Loss = -12301.8974609375
Iteration 21500: Loss = -12301.8994140625
1
Iteration 21600: Loss = -12301.8994140625
2
Iteration 21700: Loss = -12301.8994140625
3
Iteration 21800: Loss = -12301.8984375
4
Iteration 21900: Loss = -12301.8994140625
5
Iteration 22000: Loss = -12301.90234375
6
Iteration 22100: Loss = -12301.8994140625
7
Iteration 22200: Loss = -12301.900390625
8
Iteration 22300: Loss = -12301.900390625
9
Iteration 22400: Loss = -12301.8994140625
10
Iteration 22500: Loss = -12301.8994140625
11
Iteration 22600: Loss = -12301.8994140625
12
Iteration 22700: Loss = -12301.8994140625
13
Iteration 22800: Loss = -12301.8994140625
14
Iteration 22900: Loss = -12301.900390625
15
Stopping early at iteration 22900 due to no improvement.
pi: tensor([[3.9395e-07, 1.0000e+00],
        [3.2425e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4957, 0.5043], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3134, 0.0975],
         [0.9560, 0.2055]],

        [[0.8101, 0.2357],
         [0.0659, 0.6620]],

        [[0.4188, 0.2048],
         [0.9929, 0.1183]],

        [[0.7240, 0.2124],
         [0.4042, 0.9860]],

        [[0.9923, 0.2113],
         [0.7918, 0.8882]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0393093436463403
Average Adjusted Rand Index: 0.1764829135620341
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -34566.2421875
Iteration 100: Loss = -20448.693359375
Iteration 200: Loss = -13752.4833984375
Iteration 300: Loss = -12688.4296875
Iteration 400: Loss = -12528.9609375
Iteration 500: Loss = -12457.0732421875
Iteration 600: Loss = -12429.2353515625
Iteration 700: Loss = -12416.5322265625
Iteration 800: Loss = -12408.5224609375
Iteration 900: Loss = -12402.96484375
Iteration 1000: Loss = -12398.7119140625
Iteration 1100: Loss = -12395.609375
Iteration 1200: Loss = -12393.16015625
Iteration 1300: Loss = -12391.181640625
Iteration 1400: Loss = -12389.548828125
Iteration 1500: Loss = -12388.18359375
Iteration 1600: Loss = -12387.0234375
Iteration 1700: Loss = -12386.029296875
Iteration 1800: Loss = -12385.166015625
Iteration 1900: Loss = -12384.4111328125
Iteration 2000: Loss = -12383.7275390625
Iteration 2100: Loss = -12383.1435546875
Iteration 2200: Loss = -12382.6201171875
Iteration 2300: Loss = -12382.1552734375
Iteration 2400: Loss = -12381.7333984375
Iteration 2500: Loss = -12381.353515625
Iteration 2600: Loss = -12381.0107421875
Iteration 2700: Loss = -12380.6982421875
Iteration 2800: Loss = -12380.412109375
Iteration 2900: Loss = -12380.15234375
Iteration 3000: Loss = -12379.912109375
Iteration 3100: Loss = -12379.693359375
Iteration 3200: Loss = -12379.4912109375
Iteration 3300: Loss = -12379.3056640625
Iteration 3400: Loss = -12379.134765625
Iteration 3500: Loss = -12378.9755859375
Iteration 3600: Loss = -12378.8271484375
Iteration 3700: Loss = -12378.6904296875
Iteration 3800: Loss = -12378.5634765625
Iteration 3900: Loss = -12378.4453125
Iteration 4000: Loss = -12378.3330078125
Iteration 4100: Loss = -12378.2314453125
Iteration 4200: Loss = -12378.134765625
Iteration 4300: Loss = -12378.044921875
Iteration 4400: Loss = -12377.962890625
Iteration 4500: Loss = -12377.8818359375
Iteration 4600: Loss = -12377.80859375
Iteration 4700: Loss = -12377.740234375
Iteration 4800: Loss = -12377.67578125
Iteration 4900: Loss = -12377.615234375
Iteration 5000: Loss = -12377.5595703125
Iteration 5100: Loss = -12377.5078125
Iteration 5200: Loss = -12377.4560546875
Iteration 5300: Loss = -12377.4091796875
Iteration 5400: Loss = -12377.365234375
Iteration 5500: Loss = -12377.3251953125
Iteration 5600: Loss = -12377.2861328125
Iteration 5700: Loss = -12377.248046875
Iteration 5800: Loss = -12377.2138671875
Iteration 5900: Loss = -12377.1806640625
Iteration 6000: Loss = -12377.1494140625
Iteration 6100: Loss = -12377.12109375
Iteration 6200: Loss = -12377.0927734375
Iteration 6300: Loss = -12377.0673828125
Iteration 6400: Loss = -12377.04296875
Iteration 6500: Loss = -12377.0185546875
Iteration 6600: Loss = -12376.998046875
Iteration 6700: Loss = -12376.9775390625
Iteration 6800: Loss = -12376.95703125
Iteration 6900: Loss = -12376.939453125
Iteration 7000: Loss = -12376.919921875
Iteration 7100: Loss = -12376.9033203125
Iteration 7200: Loss = -12376.8876953125
Iteration 7300: Loss = -12376.873046875
Iteration 7400: Loss = -12376.8583984375
Iteration 7500: Loss = -12376.845703125
Iteration 7600: Loss = -12376.83203125
Iteration 7700: Loss = -12376.8193359375
Iteration 7800: Loss = -12376.80859375
Iteration 7900: Loss = -12376.798828125
Iteration 8000: Loss = -12376.787109375
Iteration 8100: Loss = -12376.77734375
Iteration 8200: Loss = -12376.7685546875
Iteration 8300: Loss = -12376.7587890625
Iteration 8400: Loss = -12376.75
Iteration 8500: Loss = -12376.7412109375
Iteration 8600: Loss = -12376.734375
Iteration 8700: Loss = -12376.7275390625
Iteration 8800: Loss = -12376.720703125
Iteration 8900: Loss = -12376.7119140625
Iteration 9000: Loss = -12376.7080078125
Iteration 9100: Loss = -12376.7001953125
Iteration 9200: Loss = -12376.6943359375
Iteration 9300: Loss = -12376.6865234375
Iteration 9400: Loss = -12376.681640625
Iteration 9500: Loss = -12376.6767578125
Iteration 9600: Loss = -12376.6708984375
Iteration 9700: Loss = -12376.666015625
Iteration 9800: Loss = -12376.6591796875
Iteration 9900: Loss = -12376.65234375
Iteration 10000: Loss = -12376.6494140625
Iteration 10100: Loss = -12376.6435546875
Iteration 10200: Loss = -12376.638671875
Iteration 10300: Loss = -12376.6357421875
Iteration 10400: Loss = -12376.6298828125
Iteration 10500: Loss = -12376.625
Iteration 10600: Loss = -12376.6201171875
Iteration 10700: Loss = -12376.6142578125
Iteration 10800: Loss = -12376.609375
Iteration 10900: Loss = -12376.6064453125
Iteration 11000: Loss = -12376.6005859375
Iteration 11100: Loss = -12376.595703125
Iteration 11200: Loss = -12376.591796875
Iteration 11300: Loss = -12376.5888671875
Iteration 11400: Loss = -12376.58203125
Iteration 11500: Loss = -12376.5771484375
Iteration 11600: Loss = -12376.5751953125
Iteration 11700: Loss = -12376.5693359375
Iteration 11800: Loss = -12376.5673828125
Iteration 11900: Loss = -12376.5615234375
Iteration 12000: Loss = -12376.55859375
Iteration 12100: Loss = -12376.5546875
Iteration 12200: Loss = -12376.55078125
Iteration 12300: Loss = -12376.5478515625
Iteration 12400: Loss = -12376.54296875
Iteration 12500: Loss = -12376.5380859375
Iteration 12600: Loss = -12376.53125
Iteration 12700: Loss = -12376.5244140625
Iteration 12800: Loss = -12376.5185546875
Iteration 12900: Loss = -12376.509765625
Iteration 13000: Loss = -12376.4990234375
Iteration 13100: Loss = -12376.482421875
Iteration 13200: Loss = -12376.412109375
Iteration 13300: Loss = -12375.935546875
Iteration 13400: Loss = -12375.494140625
Iteration 13500: Loss = -12374.849609375
Iteration 13600: Loss = -12374.443359375
Iteration 13700: Loss = -12374.3125
Iteration 13800: Loss = -12374.251953125
Iteration 13900: Loss = -12374.2158203125
Iteration 14000: Loss = -12374.1904296875
Iteration 14100: Loss = -12374.171875
Iteration 14200: Loss = -12374.1611328125
Iteration 14300: Loss = -12374.15625
Iteration 14400: Loss = -12374.1494140625
Iteration 14500: Loss = -12374.1455078125
Iteration 14600: Loss = -12374.1416015625
Iteration 14700: Loss = -12374.138671875
Iteration 14800: Loss = -12374.130859375
Iteration 14900: Loss = -12374.1259765625
Iteration 15000: Loss = -12374.123046875
Iteration 15100: Loss = -12374.1201171875
Iteration 15200: Loss = -12374.1181640625
Iteration 15300: Loss = -12374.1142578125
Iteration 15400: Loss = -12374.1123046875
Iteration 15500: Loss = -12374.1103515625
Iteration 15600: Loss = -12374.109375
Iteration 15700: Loss = -12374.1064453125
Iteration 15800: Loss = -12374.103515625
Iteration 15900: Loss = -12374.1025390625
Iteration 16000: Loss = -12374.1005859375
Iteration 16100: Loss = -12374.1005859375
Iteration 16200: Loss = -12374.099609375
Iteration 16300: Loss = -12374.099609375
Iteration 16400: Loss = -12374.0966796875
Iteration 16500: Loss = -12374.095703125
Iteration 16600: Loss = -12374.095703125
Iteration 16700: Loss = -12374.0947265625
Iteration 16800: Loss = -12374.09375
Iteration 16900: Loss = -12374.091796875
Iteration 17000: Loss = -12374.0888671875
Iteration 17100: Loss = -12374.0908203125
1
Iteration 17200: Loss = -12374.0888671875
Iteration 17300: Loss = -12374.087890625
Iteration 17400: Loss = -12374.087890625
Iteration 17500: Loss = -12374.087890625
Iteration 17600: Loss = -12374.0859375
Iteration 17700: Loss = -12374.0859375
Iteration 17800: Loss = -12374.0859375
Iteration 17900: Loss = -12374.0849609375
Iteration 18000: Loss = -12374.0859375
1
Iteration 18100: Loss = -12374.083984375
Iteration 18200: Loss = -12374.083984375
Iteration 18300: Loss = -12374.0849609375
1
Iteration 18400: Loss = -12374.0830078125
Iteration 18500: Loss = -12374.0849609375
1
Iteration 18600: Loss = -12374.083984375
2
Iteration 18700: Loss = -12374.08203125
Iteration 18800: Loss = -12374.0830078125
1
Iteration 18900: Loss = -12374.083984375
2
Iteration 19000: Loss = -12374.08203125
Iteration 19100: Loss = -12374.083984375
1
Iteration 19200: Loss = -12374.0810546875
Iteration 19300: Loss = -12374.0810546875
Iteration 19400: Loss = -12374.08203125
1
Iteration 19500: Loss = -12374.0830078125
2
Iteration 19600: Loss = -12374.0830078125
3
Iteration 19700: Loss = -12374.080078125
Iteration 19800: Loss = -12374.0771484375
Iteration 19900: Loss = -12372.083984375
Iteration 20000: Loss = -12371.9697265625
Iteration 20100: Loss = -12371.95703125
Iteration 20200: Loss = -12371.9462890625
Iteration 20300: Loss = -12371.9375
Iteration 20400: Loss = -12371.923828125
Iteration 20500: Loss = -12371.9228515625
Iteration 20600: Loss = -12371.921875
Iteration 20700: Loss = -12371.9208984375
Iteration 20800: Loss = -12371.919921875
Iteration 20900: Loss = -12371.91796875
Iteration 21000: Loss = -12371.91796875
Iteration 21100: Loss = -12371.9169921875
Iteration 21200: Loss = -12371.916015625
Iteration 21300: Loss = -12371.9169921875
1
Iteration 21400: Loss = -12371.91796875
2
Iteration 21500: Loss = -12371.9169921875
3
Iteration 21600: Loss = -12371.916015625
Iteration 21700: Loss = -12371.9013671875
Iteration 21800: Loss = -12371.8994140625
Iteration 21900: Loss = -12371.8984375
Iteration 22000: Loss = -12371.900390625
1
Iteration 22100: Loss = -12371.8994140625
2
Iteration 22200: Loss = -12371.8984375
Iteration 22300: Loss = -12371.8994140625
1
Iteration 22400: Loss = -12371.8994140625
2
Iteration 22500: Loss = -12371.900390625
3
Iteration 22600: Loss = -12371.9013671875
4
Iteration 22700: Loss = -12371.8984375
Iteration 22800: Loss = -12371.896484375
Iteration 22900: Loss = -12371.896484375
Iteration 23000: Loss = -12371.8955078125
Iteration 23100: Loss = -12371.8974609375
1
Iteration 23200: Loss = -12371.8974609375
2
Iteration 23300: Loss = -12371.8994140625
3
Iteration 23400: Loss = -12371.896484375
4
Iteration 23500: Loss = -12371.8974609375
5
Iteration 23600: Loss = -12371.8974609375
6
Iteration 23700: Loss = -12371.896484375
7
Iteration 23800: Loss = -12371.896484375
8
Iteration 23900: Loss = -12371.8955078125
Iteration 24000: Loss = -12371.8955078125
Iteration 24100: Loss = -12371.8984375
1
Iteration 24200: Loss = -12371.8974609375
2
Iteration 24300: Loss = -12371.8974609375
3
Iteration 24400: Loss = -12371.896484375
4
Iteration 24500: Loss = -12371.896484375
5
Iteration 24600: Loss = -12371.8974609375
6
Iteration 24700: Loss = -12371.8974609375
7
Iteration 24800: Loss = -12371.8955078125
Iteration 24900: Loss = -12371.8955078125
Iteration 25000: Loss = -12371.8974609375
1
Iteration 25100: Loss = -12371.896484375
2
Iteration 25200: Loss = -12371.896484375
3
Iteration 25300: Loss = -12371.8974609375
4
Iteration 25400: Loss = -12371.8974609375
5
Iteration 25500: Loss = -12371.8984375
6
Iteration 25600: Loss = -12371.896484375
7
Iteration 25700: Loss = -12371.8955078125
Iteration 25800: Loss = -12371.896484375
1
Iteration 25900: Loss = -12371.896484375
2
Iteration 26000: Loss = -12371.896484375
3
Iteration 26100: Loss = -12371.8955078125
Iteration 26200: Loss = -12371.8955078125
Iteration 26300: Loss = -12371.8984375
1
Iteration 26400: Loss = -12371.8955078125
Iteration 26500: Loss = -12371.8955078125
Iteration 26600: Loss = -12371.8955078125
Iteration 26700: Loss = -12371.8955078125
Iteration 26800: Loss = -12371.8955078125
Iteration 26900: Loss = -12371.896484375
1
Iteration 27000: Loss = -12371.896484375
2
Iteration 27100: Loss = -12371.8955078125
Iteration 27200: Loss = -12371.8955078125
Iteration 27300: Loss = -12371.896484375
1
Iteration 27400: Loss = -12371.8974609375
2
Iteration 27500: Loss = -12371.896484375
3
Iteration 27600: Loss = -12371.896484375
4
Iteration 27700: Loss = -12371.896484375
5
Iteration 27800: Loss = -12371.8994140625
6
Iteration 27900: Loss = -12371.8955078125
Iteration 28000: Loss = -12371.8974609375
1
Iteration 28100: Loss = -12371.896484375
2
Iteration 28200: Loss = -12371.8955078125
Iteration 28300: Loss = -12371.8955078125
Iteration 28400: Loss = -12371.8974609375
1
Iteration 28500: Loss = -12371.8974609375
2
Iteration 28600: Loss = -12371.8974609375
3
Iteration 28700: Loss = -12371.896484375
4
Iteration 28800: Loss = -12371.896484375
5
Iteration 28900: Loss = -12371.8955078125
Iteration 29000: Loss = -12371.896484375
1
Iteration 29100: Loss = -12371.896484375
2
Iteration 29200: Loss = -12371.896484375
3
Iteration 29300: Loss = -12371.896484375
4
Iteration 29400: Loss = -12371.896484375
5
Iteration 29500: Loss = -12371.8974609375
6
Iteration 29600: Loss = -12371.8974609375
7
Iteration 29700: Loss = -12371.8955078125
Iteration 29800: Loss = -12371.8955078125
Iteration 29900: Loss = -12371.8955078125
pi: tensor([[1.0000e+00, 1.4887e-06],
        [2.4525e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9670, 0.0330], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1996, 0.2103],
         [0.0158, 0.4337]],

        [[0.9364, 0.1996],
         [0.0192, 0.8388]],

        [[0.0377, 0.1801],
         [0.9557, 0.0663]],

        [[0.4728, 0.1214],
         [0.9312, 0.8315]],

        [[0.3490, 0.2894],
         [0.2697, 0.3413]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.002193311857154732
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.010091437982433116
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
Global Adjusted Rand Index: 0.0013529343156903567
Average Adjusted Rand Index: 0.0006023357500667752
[0.0393093436463403, 0.0013529343156903567] [0.1764829135620341, 0.0006023357500667752] [12301.900390625, 12371.896484375]
-------------------------------------
This iteration is 66
True Objective function: Loss = -11729.059425397678
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19867.1484375
Iteration 100: Loss = -14863.7783203125
Iteration 200: Loss = -12902.55078125
Iteration 300: Loss = -12463.9912109375
Iteration 400: Loss = -12370.7138671875
Iteration 500: Loss = -12341.4599609375
Iteration 600: Loss = -12327.3330078125
Iteration 700: Loss = -12314.837890625
Iteration 800: Loss = -12308.8193359375
Iteration 900: Loss = -12304.4228515625
Iteration 1000: Loss = -12301.0107421875
Iteration 1100: Loss = -12298.224609375
Iteration 1200: Loss = -12295.939453125
Iteration 1300: Loss = -12294.2060546875
Iteration 1400: Loss = -12292.8896484375
Iteration 1500: Loss = -12291.8857421875
Iteration 1600: Loss = -12291.1025390625
Iteration 1700: Loss = -12290.4736328125
Iteration 1800: Loss = -12289.955078125
Iteration 1900: Loss = -12289.515625
Iteration 2000: Loss = -12289.142578125
Iteration 2100: Loss = -12288.8193359375
Iteration 2200: Loss = -12288.5341796875
Iteration 2300: Loss = -12288.28125
Iteration 2400: Loss = -12288.0546875
Iteration 2500: Loss = -12287.8486328125
Iteration 2600: Loss = -12287.658203125
Iteration 2700: Loss = -12287.4853515625
Iteration 2800: Loss = -12287.3212890625
Iteration 2900: Loss = -12287.1640625
Iteration 3000: Loss = -12287.0146484375
Iteration 3100: Loss = -12286.8671875
Iteration 3200: Loss = -12286.7255859375
Iteration 3300: Loss = -12286.5849609375
Iteration 3400: Loss = -12286.44921875
Iteration 3500: Loss = -12286.3193359375
Iteration 3600: Loss = -12286.1953125
Iteration 3700: Loss = -12286.0771484375
Iteration 3800: Loss = -12285.9658203125
Iteration 3900: Loss = -12285.875
Iteration 4000: Loss = -12285.79296875
Iteration 4100: Loss = -12285.7314453125
Iteration 4200: Loss = -12285.6806640625
Iteration 4300: Loss = -12285.640625
Iteration 4400: Loss = -12285.6083984375
Iteration 4500: Loss = -12285.578125
Iteration 4600: Loss = -12285.5478515625
Iteration 4700: Loss = -12285.521484375
Iteration 4800: Loss = -12285.4931640625
Iteration 4900: Loss = -12285.46484375
Iteration 5000: Loss = -12285.4365234375
Iteration 5100: Loss = -12285.408203125
Iteration 5200: Loss = -12285.376953125
Iteration 5300: Loss = -12285.3466796875
Iteration 5400: Loss = -12285.3134765625
Iteration 5500: Loss = -12285.28515625
Iteration 5600: Loss = -12285.2578125
Iteration 5700: Loss = -12285.232421875
Iteration 5800: Loss = -12285.208984375
Iteration 5900: Loss = -12285.185546875
Iteration 6000: Loss = -12285.1591796875
Iteration 6100: Loss = -12285.1318359375
Iteration 6200: Loss = -12285.1015625
Iteration 6300: Loss = -12285.0732421875
Iteration 6400: Loss = -12285.0419921875
Iteration 6500: Loss = -12285.009765625
Iteration 6600: Loss = -12284.974609375
Iteration 6700: Loss = -12284.939453125
Iteration 6800: Loss = -12284.9033203125
Iteration 6900: Loss = -12284.8740234375
Iteration 7000: Loss = -12284.8447265625
Iteration 7100: Loss = -12284.8173828125
Iteration 7200: Loss = -12284.7939453125
Iteration 7300: Loss = -12284.76953125
Iteration 7400: Loss = -12284.748046875
Iteration 7500: Loss = -12284.728515625
Iteration 7600: Loss = -12284.7109375
Iteration 7700: Loss = -12284.6953125
Iteration 7800: Loss = -12284.68359375
Iteration 7900: Loss = -12284.669921875
Iteration 8000: Loss = -12284.658203125
Iteration 8100: Loss = -12284.6396484375
Iteration 8200: Loss = -12284.6240234375
Iteration 8300: Loss = -12284.599609375
Iteration 8400: Loss = -12284.5859375
Iteration 8500: Loss = -12284.5703125
Iteration 8600: Loss = -12284.556640625
Iteration 8700: Loss = -12284.544921875
Iteration 8800: Loss = -12284.5302734375
Iteration 8900: Loss = -12284.5166015625
Iteration 9000: Loss = -12284.5048828125
Iteration 9100: Loss = -12284.4931640625
Iteration 9200: Loss = -12284.484375
Iteration 9300: Loss = -12284.474609375
Iteration 9400: Loss = -12284.462890625
Iteration 9500: Loss = -12284.4560546875
Iteration 9600: Loss = -12284.4462890625
Iteration 9700: Loss = -12284.4365234375
Iteration 9800: Loss = -12284.42578125
Iteration 9900: Loss = -12284.4150390625
Iteration 10000: Loss = -12284.41015625
Iteration 10100: Loss = -12284.4033203125
Iteration 10200: Loss = -12284.400390625
Iteration 10300: Loss = -12284.3984375
Iteration 10400: Loss = -12284.396484375
Iteration 10500: Loss = -12284.396484375
Iteration 10600: Loss = -12284.3935546875
Iteration 10700: Loss = -12284.39453125
1
Iteration 10800: Loss = -12284.3935546875
Iteration 10900: Loss = -12284.392578125
Iteration 11000: Loss = -12284.392578125
Iteration 11100: Loss = -12284.3896484375
Iteration 11200: Loss = -12284.3896484375
Iteration 11300: Loss = -12284.3896484375
Iteration 11400: Loss = -12284.3896484375
Iteration 11500: Loss = -12284.3876953125
Iteration 11600: Loss = -12284.3876953125
Iteration 11700: Loss = -12284.388671875
1
Iteration 11800: Loss = -12284.3876953125
Iteration 11900: Loss = -12284.3876953125
Iteration 12000: Loss = -12284.38671875
Iteration 12100: Loss = -12284.38671875
Iteration 12200: Loss = -12284.38671875
Iteration 12300: Loss = -12284.3857421875
Iteration 12400: Loss = -12284.384765625
Iteration 12500: Loss = -12284.384765625
Iteration 12600: Loss = -12284.38671875
1
Iteration 12700: Loss = -12284.3837890625
Iteration 12800: Loss = -12284.384765625
1
Iteration 12900: Loss = -12284.3837890625
Iteration 13000: Loss = -12284.3828125
Iteration 13100: Loss = -12284.3828125
Iteration 13200: Loss = -12284.3828125
Iteration 13300: Loss = -12284.3828125
Iteration 13400: Loss = -12284.3828125
Iteration 13500: Loss = -12284.3818359375
Iteration 13600: Loss = -12284.3818359375
Iteration 13700: Loss = -12284.3798828125
Iteration 13800: Loss = -12284.3798828125
Iteration 13900: Loss = -12284.3818359375
1
Iteration 14000: Loss = -12284.380859375
2
Iteration 14100: Loss = -12284.380859375
3
Iteration 14200: Loss = -12284.380859375
4
Iteration 14300: Loss = -12284.3818359375
5
Iteration 14400: Loss = -12284.3798828125
Iteration 14500: Loss = -12284.3798828125
Iteration 14600: Loss = -12284.380859375
1
Iteration 14700: Loss = -12284.3828125
2
Iteration 14800: Loss = -12284.3818359375
3
Iteration 14900: Loss = -12284.3798828125
Iteration 15000: Loss = -12284.3798828125
Iteration 15100: Loss = -12284.3798828125
Iteration 15200: Loss = -12284.380859375
1
Iteration 15300: Loss = -12284.3779296875
Iteration 15400: Loss = -12284.3798828125
1
Iteration 15500: Loss = -12284.37890625
2
Iteration 15600: Loss = -12284.3779296875
Iteration 15700: Loss = -12284.37890625
1
Iteration 15800: Loss = -12284.37890625
2
Iteration 15900: Loss = -12284.37890625
3
Iteration 16000: Loss = -12284.37890625
4
Iteration 16100: Loss = -12284.37890625
5
Iteration 16200: Loss = -12284.37890625
6
Iteration 16300: Loss = -12284.380859375
7
Iteration 16400: Loss = -12284.376953125
Iteration 16500: Loss = -12284.3779296875
1
Iteration 16600: Loss = -12284.37890625
2
Iteration 16700: Loss = -12284.3779296875
3
Iteration 16800: Loss = -12284.3779296875
4
Iteration 16900: Loss = -12284.376953125
Iteration 17000: Loss = -12284.3759765625
Iteration 17100: Loss = -12284.37890625
1
Iteration 17200: Loss = -12284.376953125
2
Iteration 17300: Loss = -12284.37890625
3
Iteration 17400: Loss = -12284.37890625
4
Iteration 17500: Loss = -12284.3779296875
5
Iteration 17600: Loss = -12284.37890625
6
Iteration 17700: Loss = -12284.3779296875
7
Iteration 17800: Loss = -12284.3779296875
8
Iteration 17900: Loss = -12284.376953125
9
Iteration 18000: Loss = -12284.3779296875
10
Iteration 18100: Loss = -12284.37890625
11
Iteration 18200: Loss = -12284.3779296875
12
Iteration 18300: Loss = -12284.380859375
13
Iteration 18400: Loss = -12284.376953125
14
Iteration 18500: Loss = -12284.3779296875
15
Stopping early at iteration 18500 due to no improvement.
pi: tensor([[2.9052e-04, 9.9971e-01],
        [9.6259e-02, 9.0374e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([2.7586e-04, 9.9972e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2494, 0.1947],
         [0.0137, 0.1936]],

        [[0.0670, 0.2416],
         [0.6917, 0.9834]],

        [[0.0454, 0.2075],
         [0.9672, 0.8476]],

        [[0.0355, 0.2061],
         [0.0426, 0.3246]],

        [[0.0082, 0.2121],
         [0.4084, 0.0514]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: -0.006658343736995423
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0004528243715857524
Average Adjusted Rand Index: -0.0013316687473990845
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21947.40234375
Iteration 100: Loss = -16234.6484375
Iteration 200: Loss = -13230.212890625
Iteration 300: Loss = -12547.177734375
Iteration 400: Loss = -12426.232421875
Iteration 500: Loss = -12380.1943359375
Iteration 600: Loss = -12346.3251953125
Iteration 700: Loss = -12331.724609375
Iteration 800: Loss = -12321.55859375
Iteration 900: Loss = -12312.4267578125
Iteration 1000: Loss = -12303.865234375
Iteration 1100: Loss = -12299.8388671875
Iteration 1200: Loss = -12297.2666015625
Iteration 1300: Loss = -12295.4951171875
Iteration 1400: Loss = -12294.1640625
Iteration 1500: Loss = -12293.1123046875
Iteration 1600: Loss = -12292.25390625
Iteration 1700: Loss = -12291.5439453125
Iteration 1800: Loss = -12290.9453125
Iteration 1900: Loss = -12290.4326171875
Iteration 2000: Loss = -12289.9921875
Iteration 2100: Loss = -12289.609375
Iteration 2200: Loss = -12289.2744140625
Iteration 2300: Loss = -12288.9794921875
Iteration 2400: Loss = -12288.71875
Iteration 2500: Loss = -12288.4853515625
Iteration 2600: Loss = -12288.27734375
Iteration 2700: Loss = -12288.0908203125
Iteration 2800: Loss = -12287.9248046875
Iteration 2900: Loss = -12287.771484375
Iteration 3000: Loss = -12287.6337890625
Iteration 3100: Loss = -12287.509765625
Iteration 3200: Loss = -12287.39453125
Iteration 3300: Loss = -12287.29296875
Iteration 3400: Loss = -12287.1962890625
Iteration 3500: Loss = -12287.107421875
Iteration 3600: Loss = -12287.0263671875
Iteration 3700: Loss = -12286.9541015625
Iteration 3800: Loss = -12286.884765625
Iteration 3900: Loss = -12286.822265625
Iteration 4000: Loss = -12286.7626953125
Iteration 4100: Loss = -12286.70703125
Iteration 4200: Loss = -12286.65625
Iteration 4300: Loss = -12286.611328125
Iteration 4400: Loss = -12286.568359375
Iteration 4500: Loss = -12286.5263671875
Iteration 4600: Loss = -12286.4892578125
Iteration 4700: Loss = -12286.4521484375
Iteration 4800: Loss = -12286.419921875
Iteration 4900: Loss = -12286.3896484375
Iteration 5000: Loss = -12286.3603515625
Iteration 5100: Loss = -12286.3330078125
Iteration 5200: Loss = -12286.3076171875
Iteration 5300: Loss = -12286.2841796875
Iteration 5400: Loss = -12286.26171875
Iteration 5500: Loss = -12286.240234375
Iteration 5600: Loss = -12286.2197265625
Iteration 5700: Loss = -12286.2021484375
Iteration 5800: Loss = -12286.1845703125
Iteration 5900: Loss = -12286.1689453125
Iteration 6000: Loss = -12286.154296875
Iteration 6100: Loss = -12286.13671875
Iteration 6200: Loss = -12286.125
Iteration 6300: Loss = -12286.1123046875
Iteration 6400: Loss = -12286.1005859375
Iteration 6500: Loss = -12286.0888671875
Iteration 6600: Loss = -12286.0791015625
Iteration 6700: Loss = -12286.0693359375
Iteration 6800: Loss = -12286.0576171875
Iteration 6900: Loss = -12286.0498046875
Iteration 7000: Loss = -12286.041015625
Iteration 7100: Loss = -12286.0341796875
Iteration 7200: Loss = -12286.0263671875
Iteration 7300: Loss = -12286.01953125
Iteration 7400: Loss = -12286.0107421875
Iteration 7500: Loss = -12286.0068359375
Iteration 7600: Loss = -12285.9990234375
Iteration 7700: Loss = -12285.9951171875
Iteration 7800: Loss = -12285.98828125
Iteration 7900: Loss = -12285.984375
Iteration 8000: Loss = -12285.978515625
Iteration 8100: Loss = -12285.9755859375
Iteration 8200: Loss = -12285.9716796875
Iteration 8300: Loss = -12285.9658203125
Iteration 8400: Loss = -12285.9609375
Iteration 8500: Loss = -12285.958984375
Iteration 8600: Loss = -12285.9580078125
Iteration 8700: Loss = -12285.953125
Iteration 8800: Loss = -12285.94921875
Iteration 8900: Loss = -12285.9462890625
Iteration 9000: Loss = -12285.9453125
Iteration 9100: Loss = -12285.9423828125
Iteration 9200: Loss = -12285.939453125
Iteration 9300: Loss = -12285.9365234375
Iteration 9400: Loss = -12285.9345703125
Iteration 9500: Loss = -12285.931640625
Iteration 9600: Loss = -12285.9306640625
Iteration 9700: Loss = -12285.9296875
Iteration 9800: Loss = -12285.9267578125
Iteration 9900: Loss = -12285.9248046875
Iteration 10000: Loss = -12285.923828125
Iteration 10100: Loss = -12285.921875
Iteration 10200: Loss = -12285.9228515625
1
Iteration 10300: Loss = -12285.91796875
Iteration 10400: Loss = -12285.9189453125
1
Iteration 10500: Loss = -12285.9169921875
Iteration 10600: Loss = -12285.9169921875
Iteration 10700: Loss = -12285.9150390625
Iteration 10800: Loss = -12285.9130859375
Iteration 10900: Loss = -12285.912109375
Iteration 11000: Loss = -12285.912109375
Iteration 11100: Loss = -12285.9111328125
Iteration 11200: Loss = -12285.9091796875
Iteration 11300: Loss = -12285.91015625
1
Iteration 11400: Loss = -12285.9111328125
2
Iteration 11500: Loss = -12285.9072265625
Iteration 11600: Loss = -12285.90625
Iteration 11700: Loss = -12285.90625
Iteration 11800: Loss = -12285.90625
Iteration 11900: Loss = -12285.9033203125
Iteration 12000: Loss = -12285.904296875
1
Iteration 12100: Loss = -12285.9033203125
Iteration 12200: Loss = -12285.9033203125
Iteration 12300: Loss = -12285.9033203125
Iteration 12400: Loss = -12285.9033203125
Iteration 12500: Loss = -12285.90234375
Iteration 12600: Loss = -12285.9013671875
Iteration 12700: Loss = -12285.9013671875
Iteration 12800: Loss = -12285.8994140625
Iteration 12900: Loss = -12285.8994140625
Iteration 13000: Loss = -12285.8974609375
Iteration 13100: Loss = -12285.8984375
1
Iteration 13200: Loss = -12285.896484375
Iteration 13300: Loss = -12285.8984375
1
Iteration 13400: Loss = -12285.896484375
Iteration 13500: Loss = -12285.896484375
Iteration 13600: Loss = -12285.896484375
Iteration 13700: Loss = -12285.89453125
Iteration 13800: Loss = -12285.8955078125
1
Iteration 13900: Loss = -12285.8955078125
2
Iteration 14000: Loss = -12285.89453125
Iteration 14100: Loss = -12285.8916015625
Iteration 14200: Loss = -12285.892578125
1
Iteration 14300: Loss = -12285.890625
Iteration 14400: Loss = -12285.890625
Iteration 14500: Loss = -12285.890625
Iteration 14600: Loss = -12285.88671875
Iteration 14700: Loss = -12285.8828125
Iteration 14800: Loss = -12285.87109375
Iteration 14900: Loss = -12285.83203125
Iteration 15000: Loss = -12285.7548828125
Iteration 15100: Loss = -12284.3876953125
Iteration 15200: Loss = -12284.2431640625
Iteration 15300: Loss = -12284.23046875
Iteration 15400: Loss = -12284.2255859375
Iteration 15500: Loss = -12284.1982421875
Iteration 15600: Loss = -12284.1943359375
Iteration 15700: Loss = -12284.1943359375
Iteration 15800: Loss = -12284.1943359375
Iteration 15900: Loss = -12284.1533203125
Iteration 16000: Loss = -12284.1552734375
1
Iteration 16100: Loss = -12284.1533203125
Iteration 16200: Loss = -12284.1279296875
Iteration 16300: Loss = -12283.6357421875
Iteration 16400: Loss = -12283.6337890625
Iteration 16500: Loss = -12283.630859375
Iteration 16600: Loss = -12283.6298828125
Iteration 16700: Loss = -12283.6259765625
Iteration 16800: Loss = -12283.623046875
Iteration 16900: Loss = -12283.6162109375
Iteration 17000: Loss = -12283.6123046875
Iteration 17100: Loss = -12283.607421875
Iteration 17200: Loss = -12283.6044921875
Iteration 17300: Loss = -12283.6015625
Iteration 17400: Loss = -12283.6015625
Iteration 17500: Loss = -12283.599609375
Iteration 17600: Loss = -12283.5986328125
Iteration 17700: Loss = -12283.5986328125
Iteration 17800: Loss = -12283.59765625
Iteration 17900: Loss = -12283.59765625
Iteration 18000: Loss = -12283.595703125
Iteration 18100: Loss = -12283.5966796875
1
Iteration 18200: Loss = -12283.595703125
Iteration 18300: Loss = -12283.5947265625
Iteration 18400: Loss = -12283.5869140625
Iteration 18500: Loss = -12283.583984375
Iteration 18600: Loss = -12283.5830078125
Iteration 18700: Loss = -12283.58203125
Iteration 18800: Loss = -12283.578125
Iteration 18900: Loss = -12283.57421875
Iteration 19000: Loss = -12283.56640625
Iteration 19100: Loss = -12283.556640625
Iteration 19200: Loss = -12283.548828125
Iteration 19300: Loss = -12283.5458984375
Iteration 19400: Loss = -12283.544921875
Iteration 19500: Loss = -12283.54296875
Iteration 19600: Loss = -12283.5419921875
Iteration 19700: Loss = -12283.5390625
Iteration 19800: Loss = -12283.5380859375
Iteration 19900: Loss = -12283.5400390625
1
Iteration 20000: Loss = -12283.513671875
Iteration 20100: Loss = -12283.189453125
Iteration 20200: Loss = -12283.1904296875
1
Iteration 20300: Loss = -12283.1875
Iteration 20400: Loss = -12283.17578125
Iteration 20500: Loss = -12283.1669921875
Iteration 20600: Loss = -12283.1630859375
Iteration 20700: Loss = -12283.1640625
1
Iteration 20800: Loss = -12283.1630859375
Iteration 20900: Loss = -12283.1640625
1
Iteration 21000: Loss = -12283.1630859375
Iteration 21100: Loss = -12283.1630859375
Iteration 21200: Loss = -12283.169921875
1
Iteration 21300: Loss = -12283.1591796875
Iteration 21400: Loss = -12283.1474609375
Iteration 21500: Loss = -12283.1416015625
Iteration 21600: Loss = -12283.138671875
Iteration 21700: Loss = -12283.134765625
Iteration 21800: Loss = -12283.134765625
Iteration 21900: Loss = -12283.134765625
Iteration 22000: Loss = -12283.1318359375
Iteration 22100: Loss = -12283.1318359375
Iteration 22200: Loss = -12283.1298828125
Iteration 22300: Loss = -12283.1201171875
Iteration 22400: Loss = -12283.119140625
Iteration 22500: Loss = -12283.1162109375
Iteration 22600: Loss = -12283.1171875
1
Iteration 22700: Loss = -12283.11328125
Iteration 22800: Loss = -12283.1083984375
Iteration 22900: Loss = -12283.0517578125
Iteration 23000: Loss = -12283.0517578125
Iteration 23100: Loss = -12283.05078125
Iteration 23200: Loss = -12283.0439453125
Iteration 23300: Loss = -12282.9755859375
Iteration 23400: Loss = -12282.97265625
Iteration 23500: Loss = -12282.9677734375
Iteration 23600: Loss = -12282.962890625
Iteration 23700: Loss = -12282.95703125
Iteration 23800: Loss = -12282.9541015625
Iteration 23900: Loss = -12282.9541015625
Iteration 24000: Loss = -12282.953125
Iteration 24100: Loss = -12282.9462890625
Iteration 24200: Loss = -12282.9443359375
Iteration 24300: Loss = -12282.8603515625
Iteration 24400: Loss = -12282.8564453125
Iteration 24500: Loss = -12282.8466796875
Iteration 24600: Loss = -12282.8447265625
Iteration 24700: Loss = -12282.84375
Iteration 24800: Loss = -12282.841796875
Iteration 24900: Loss = -12282.8271484375
Iteration 25000: Loss = -12282.78125
Iteration 25100: Loss = -12282.7841796875
1
Iteration 25200: Loss = -12282.7783203125
Iteration 25300: Loss = -12282.7626953125
Iteration 25400: Loss = -12282.7587890625
Iteration 25500: Loss = -12282.7548828125
Iteration 25600: Loss = -12282.755859375
1
Iteration 25700: Loss = -12282.7509765625
Iteration 25800: Loss = -12282.7509765625
Iteration 25900: Loss = -12282.75
Iteration 26000: Loss = -12282.75
Iteration 26100: Loss = -12282.7509765625
1
Iteration 26200: Loss = -12282.748046875
Iteration 26300: Loss = -12282.74609375
Iteration 26400: Loss = -12282.7470703125
1
Iteration 26500: Loss = -12282.7431640625
Iteration 26600: Loss = -12282.7451171875
1
Iteration 26700: Loss = -12282.74609375
2
Iteration 26800: Loss = -12282.74609375
3
Iteration 26900: Loss = -12282.7451171875
4
Iteration 27000: Loss = -12282.744140625
5
Iteration 27100: Loss = -12282.7451171875
6
Iteration 27200: Loss = -12282.7451171875
7
Iteration 27300: Loss = -12282.7412109375
Iteration 27400: Loss = -12282.7392578125
Iteration 27500: Loss = -12282.736328125
Iteration 27600: Loss = -12282.736328125
Iteration 27700: Loss = -12282.7373046875
1
Iteration 27800: Loss = -12282.732421875
Iteration 27900: Loss = -12282.7255859375
Iteration 28000: Loss = -12282.7255859375
Iteration 28100: Loss = -12282.7255859375
Iteration 28200: Loss = -12282.7275390625
1
Iteration 28300: Loss = -12282.7265625
2
Iteration 28400: Loss = -12282.7265625
3
Iteration 28500: Loss = -12282.7275390625
4
Iteration 28600: Loss = -12282.7265625
5
Iteration 28700: Loss = -12282.7255859375
Iteration 28800: Loss = -12282.724609375
Iteration 28900: Loss = -12282.7275390625
1
Iteration 29000: Loss = -12282.7265625
2
Iteration 29100: Loss = -12282.7255859375
3
Iteration 29200: Loss = -12282.7265625
4
Iteration 29300: Loss = -12282.7275390625
5
Iteration 29400: Loss = -12282.7275390625
6
Iteration 29500: Loss = -12282.7265625
7
Iteration 29600: Loss = -12282.7265625
8
Iteration 29700: Loss = -12282.7255859375
9
Iteration 29800: Loss = -12282.7265625
10
Iteration 29900: Loss = -12282.7275390625
11
pi: tensor([[1.0000e+00, 3.0480e-06],
        [2.6255e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0530, 0.9470], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1595, 0.2185],
         [0.0136, 0.1970]],

        [[0.8975, 0.2713],
         [0.9862, 0.0784]],

        [[0.1420, 0.1971],
         [0.7820, 0.1595]],

        [[0.7144, 0.1488],
         [0.4985, 0.1149]],

        [[0.9674, 0.1629],
         [0.0494, 0.9055]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.005661239846689405
time is 1
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.019407251391605235
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.007431593898976406
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0049666994303792225
Global Adjusted Rand Index: -0.003232785416051113
Average Adjusted Rand Index: -0.003826968874717095
[-0.0004528243715857524, -0.003232785416051113] [-0.0013316687473990845, -0.003826968874717095] [12284.3779296875, 12282.7265625]
-------------------------------------
This iteration is 67
True Objective function: Loss = -11755.01030993903
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36402.7421875
Iteration 100: Loss = -23119.591796875
Iteration 200: Loss = -14898.802734375
Iteration 300: Loss = -12932.09765625
Iteration 400: Loss = -12598.283203125
Iteration 500: Loss = -12473.3017578125
Iteration 600: Loss = -12409.0439453125
Iteration 700: Loss = -12370.6123046875
Iteration 800: Loss = -12345.3837890625
Iteration 900: Loss = -12327.7421875
Iteration 1000: Loss = -12314.8330078125
Iteration 1100: Loss = -12305.0439453125
Iteration 1200: Loss = -12297.4130859375
Iteration 1300: Loss = -12291.33984375
Iteration 1400: Loss = -12286.4033203125
Iteration 1500: Loss = -12282.3349609375
Iteration 1600: Loss = -12278.9326171875
Iteration 1700: Loss = -12276.05859375
Iteration 1800: Loss = -12273.607421875
Iteration 1900: Loss = -12271.5
Iteration 2000: Loss = -12269.66796875
Iteration 2100: Loss = -12268.0712890625
Iteration 2200: Loss = -12266.6650390625
Iteration 2300: Loss = -12265.4248046875
Iteration 2400: Loss = -12264.3203125
Iteration 2500: Loss = -12263.3369140625
Iteration 2600: Loss = -12262.45703125
Iteration 2700: Loss = -12261.6640625
Iteration 2800: Loss = -12260.9462890625
Iteration 2900: Loss = -12260.3037109375
Iteration 3000: Loss = -12259.7099609375
Iteration 3100: Loss = -12259.173828125
Iteration 3200: Loss = -12258.685546875
Iteration 3300: Loss = -12258.236328125
Iteration 3400: Loss = -12257.826171875
Iteration 3500: Loss = -12257.4462890625
Iteration 3600: Loss = -12257.09765625
Iteration 3700: Loss = -12256.7763671875
Iteration 3800: Loss = -12256.4794921875
Iteration 3900: Loss = -12256.2041015625
Iteration 4000: Loss = -12255.947265625
Iteration 4100: Loss = -12255.7099609375
Iteration 4200: Loss = -12255.4892578125
Iteration 4300: Loss = -12255.2841796875
Iteration 4400: Loss = -12255.0908203125
Iteration 4500: Loss = -12254.9111328125
Iteration 4600: Loss = -12254.7421875
Iteration 4700: Loss = -12254.5849609375
Iteration 4800: Loss = -12254.4326171875
Iteration 4900: Loss = -12254.2919921875
Iteration 5000: Loss = -12254.15625
Iteration 5100: Loss = -12254.0224609375
Iteration 5200: Loss = -12253.8935546875
Iteration 5300: Loss = -12253.767578125
Iteration 5400: Loss = -12253.646484375
Iteration 5500: Loss = -12253.5322265625
Iteration 5600: Loss = -12253.4228515625
Iteration 5700: Loss = -12253.318359375
Iteration 5800: Loss = -12253.22265625
Iteration 5900: Loss = -12253.1337890625
Iteration 6000: Loss = -12253.0546875
Iteration 6100: Loss = -12252.98046875
Iteration 6200: Loss = -12252.916015625
Iteration 6300: Loss = -12252.8544921875
Iteration 6400: Loss = -12252.7998046875
Iteration 6500: Loss = -12252.748046875
Iteration 6600: Loss = -12252.7001953125
Iteration 6700: Loss = -12252.6572265625
Iteration 6800: Loss = -12252.6162109375
Iteration 6900: Loss = -12252.578125
Iteration 7000: Loss = -12252.5458984375
Iteration 7100: Loss = -12252.515625
Iteration 7200: Loss = -12252.4873046875
Iteration 7300: Loss = -12252.462890625
Iteration 7400: Loss = -12252.4375
Iteration 7500: Loss = -12252.4140625
Iteration 7600: Loss = -12252.3916015625
Iteration 7700: Loss = -12252.3720703125
Iteration 7800: Loss = -12252.3515625
Iteration 7900: Loss = -12252.33203125
Iteration 8000: Loss = -12252.3134765625
Iteration 8100: Loss = -12252.2958984375
Iteration 8200: Loss = -12252.279296875
Iteration 8300: Loss = -12252.26171875
Iteration 8400: Loss = -12252.24609375
Iteration 8500: Loss = -12252.232421875
Iteration 8600: Loss = -12252.2158203125
Iteration 8700: Loss = -12252.205078125
Iteration 8800: Loss = -12252.1923828125
Iteration 8900: Loss = -12252.1806640625
Iteration 9000: Loss = -12252.16796875
Iteration 9100: Loss = -12252.1591796875
Iteration 9200: Loss = -12252.146484375
Iteration 9300: Loss = -12252.1357421875
Iteration 9400: Loss = -12252.1259765625
Iteration 9500: Loss = -12252.1171875
Iteration 9600: Loss = -12252.1064453125
Iteration 9700: Loss = -12252.0986328125
Iteration 9800: Loss = -12252.0908203125
Iteration 9900: Loss = -12252.0849609375
Iteration 10000: Loss = -12252.07421875
Iteration 10100: Loss = -12252.06640625
Iteration 10200: Loss = -12252.0595703125
Iteration 10300: Loss = -12252.0517578125
Iteration 10400: Loss = -12252.0458984375
Iteration 10500: Loss = -12252.037109375
Iteration 10600: Loss = -12252.03125
Iteration 10700: Loss = -12252.025390625
Iteration 10800: Loss = -12252.0166015625
Iteration 10900: Loss = -12252.009765625
Iteration 11000: Loss = -12252.0029296875
Iteration 11100: Loss = -12251.99609375
Iteration 11200: Loss = -12251.98828125
Iteration 11300: Loss = -12251.9814453125
Iteration 11400: Loss = -12251.9716796875
Iteration 11500: Loss = -12251.9619140625
Iteration 11600: Loss = -12251.953125
Iteration 11700: Loss = -12251.9423828125
Iteration 11800: Loss = -12251.9296875
Iteration 11900: Loss = -12251.9208984375
Iteration 12000: Loss = -12251.90234375
Iteration 12100: Loss = -12251.8857421875
Iteration 12200: Loss = -12251.86328125
Iteration 12300: Loss = -12251.83984375
Iteration 12400: Loss = -12251.8076171875
Iteration 12500: Loss = -12251.767578125
Iteration 12600: Loss = -12251.7177734375
Iteration 12700: Loss = -12251.65234375
Iteration 12800: Loss = -12251.5625
Iteration 12900: Loss = -12251.4541015625
Iteration 13000: Loss = -12251.3359375
Iteration 13100: Loss = -12251.2431640625
Iteration 13200: Loss = -12251.1875
Iteration 13300: Loss = -12251.15234375
Iteration 13400: Loss = -12251.12109375
Iteration 13500: Loss = -12251.0947265625
Iteration 13600: Loss = -12251.07421875
Iteration 13700: Loss = -12251.0546875
Iteration 13800: Loss = -12251.0341796875
Iteration 13900: Loss = -12251.0107421875
Iteration 14000: Loss = -12250.9765625
Iteration 14100: Loss = -12250.8994140625
Iteration 14200: Loss = -12250.77734375
Iteration 14300: Loss = -12250.7412109375
Iteration 14400: Loss = -12250.6123046875
Iteration 14500: Loss = -12250.4873046875
Iteration 14600: Loss = -12250.2822265625
Iteration 14700: Loss = -12250.1884765625
Iteration 14800: Loss = -12250.1806640625
Iteration 14900: Loss = -12250.177734375
Iteration 15000: Loss = -12250.1767578125
Iteration 15100: Loss = -12250.1767578125
Iteration 15200: Loss = -12250.1767578125
Iteration 15300: Loss = -12250.17578125
Iteration 15400: Loss = -12250.1767578125
1
Iteration 15500: Loss = -12250.1748046875
Iteration 15600: Loss = -12250.17578125
1
Iteration 15700: Loss = -12250.17578125
2
Iteration 15800: Loss = -12250.1748046875
Iteration 15900: Loss = -12250.1748046875
Iteration 16000: Loss = -12250.17578125
1
Iteration 16100: Loss = -12250.17578125
2
Iteration 16200: Loss = -12250.173828125
Iteration 16300: Loss = -12250.1748046875
1
Iteration 16400: Loss = -12250.17578125
2
Iteration 16500: Loss = -12250.1728515625
Iteration 16600: Loss = -12250.1728515625
Iteration 16700: Loss = -12250.173828125
1
Iteration 16800: Loss = -12250.173828125
2
Iteration 16900: Loss = -12250.173828125
3
Iteration 17000: Loss = -12250.1708984375
Iteration 17100: Loss = -12250.173828125
1
Iteration 17200: Loss = -12250.173828125
2
Iteration 17300: Loss = -12250.1708984375
Iteration 17400: Loss = -12250.173828125
1
Iteration 17500: Loss = -12250.1728515625
2
Iteration 17600: Loss = -12250.1728515625
3
Iteration 17700: Loss = -12250.171875
4
Iteration 17800: Loss = -12250.1728515625
5
Iteration 17900: Loss = -12250.171875
6
Iteration 18000: Loss = -12250.1748046875
7
Iteration 18100: Loss = -12250.171875
8
Iteration 18200: Loss = -12250.171875
9
Iteration 18300: Loss = -12250.1728515625
10
Iteration 18400: Loss = -12250.1708984375
Iteration 18500: Loss = -12250.171875
1
Iteration 18600: Loss = -12250.171875
2
Iteration 18700: Loss = -12250.171875
3
Iteration 18800: Loss = -12250.1728515625
4
Iteration 18900: Loss = -12250.1708984375
Iteration 19000: Loss = -12250.171875
1
Iteration 19100: Loss = -12250.1708984375
Iteration 19200: Loss = -12250.1708984375
Iteration 19300: Loss = -12250.1728515625
1
Iteration 19400: Loss = -12250.171875
2
Iteration 19500: Loss = -12250.1708984375
Iteration 19600: Loss = -12250.1708984375
Iteration 19700: Loss = -12250.1708984375
Iteration 19800: Loss = -12250.171875
1
Iteration 19900: Loss = -12250.171875
2
Iteration 20000: Loss = -12250.1728515625
3
Iteration 20100: Loss = -12250.171875
4
Iteration 20200: Loss = -12250.171875
5
Iteration 20300: Loss = -12250.1728515625
6
Iteration 20400: Loss = -12250.1708984375
Iteration 20500: Loss = -12250.171875
1
Iteration 20600: Loss = -12250.171875
2
Iteration 20700: Loss = -12250.171875
3
Iteration 20800: Loss = -12250.1728515625
4
Iteration 20900: Loss = -12250.171875
5
Iteration 21000: Loss = -12250.1708984375
Iteration 21100: Loss = -12250.171875
1
Iteration 21200: Loss = -12250.171875
2
Iteration 21300: Loss = -12250.1728515625
3
Iteration 21400: Loss = -12250.171875
4
Iteration 21500: Loss = -12250.1728515625
5
Iteration 21600: Loss = -12250.1728515625
6
Iteration 21700: Loss = -12250.1728515625
7
Iteration 21800: Loss = -12250.1728515625
8
Iteration 21900: Loss = -12250.173828125
9
Iteration 22000: Loss = -12250.1708984375
Iteration 22100: Loss = -12250.171875
1
Iteration 22200: Loss = -12250.171875
2
Iteration 22300: Loss = -12250.171875
3
Iteration 22400: Loss = -12250.173828125
4
Iteration 22500: Loss = -12250.171875
5
Iteration 22600: Loss = -12250.171875
6
Iteration 22700: Loss = -12250.1728515625
7
Iteration 22800: Loss = -12250.1728515625
8
Iteration 22900: Loss = -12250.1728515625
9
Iteration 23000: Loss = -12250.171875
10
Iteration 23100: Loss = -12250.171875
11
Iteration 23200: Loss = -12250.1728515625
12
Iteration 23300: Loss = -12250.1728515625
13
Iteration 23400: Loss = -12250.1728515625
14
Iteration 23500: Loss = -12250.1767578125
15
Stopping early at iteration 23500 due to no improvement.
pi: tensor([[0.8215, 0.1785],
        [0.3605, 0.6395]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 1.6093e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2028, 0.2206],
         [0.0385, 0.1736]],

        [[0.9443, 0.1938],
         [0.5927, 0.0089]],

        [[0.8996, 0.1818],
         [0.3572, 0.9902]],

        [[0.8954, 0.1795],
         [0.8171, 0.9923]],

        [[0.9114, 0.1941],
         [0.7843, 0.3794]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.02664820903270462
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0014729757840434622
Average Adjusted Rand Index: 0.0054929698557314895
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33159.5546875
Iteration 100: Loss = -23189.404296875
Iteration 200: Loss = -15539.6875
Iteration 300: Loss = -13212.693359375
Iteration 400: Loss = -12675.0244140625
Iteration 500: Loss = -12550.0654296875
Iteration 600: Loss = -12490.9921875
Iteration 700: Loss = -12451.7763671875
Iteration 800: Loss = -12420.2353515625
Iteration 900: Loss = -12393.806640625
Iteration 1000: Loss = -12375.6337890625
Iteration 1100: Loss = -12361.1103515625
Iteration 1200: Loss = -12345.7822265625
Iteration 1300: Loss = -12332.7998046875
Iteration 1400: Loss = -12321.8349609375
Iteration 1500: Loss = -12314.025390625
Iteration 1600: Loss = -12306.423828125
Iteration 1700: Loss = -12299.37890625
Iteration 1800: Loss = -12294.4921875
Iteration 1900: Loss = -12289.091796875
Iteration 2000: Loss = -12285.4931640625
Iteration 2100: Loss = -12279.970703125
Iteration 2200: Loss = -12275.95703125
Iteration 2300: Loss = -12273.92578125
Iteration 2400: Loss = -12272.55078125
Iteration 2500: Loss = -12271.1953125
Iteration 2600: Loss = -12269.66015625
Iteration 2700: Loss = -12268.9267578125
Iteration 2800: Loss = -12268.3232421875
Iteration 2900: Loss = -12267.80078125
Iteration 3000: Loss = -12267.3369140625
Iteration 3100: Loss = -12266.9150390625
Iteration 3200: Loss = -12266.5244140625
Iteration 3300: Loss = -12266.1220703125
Iteration 3400: Loss = -12265.716796875
Iteration 3500: Loss = -12265.2705078125
Iteration 3600: Loss = -12264.9599609375
Iteration 3700: Loss = -12264.5732421875
Iteration 3800: Loss = -12262.1904296875
Iteration 3900: Loss = -12261.6015625
Iteration 4000: Loss = -12261.3291015625
Iteration 4100: Loss = -12261.1259765625
Iteration 4200: Loss = -12260.9482421875
Iteration 4300: Loss = -12260.7880859375
Iteration 4400: Loss = -12260.64453125
Iteration 4500: Loss = -12260.5078125
Iteration 4600: Loss = -12260.3740234375
Iteration 4700: Loss = -12258.7841796875
Iteration 4800: Loss = -12258.23828125
Iteration 4900: Loss = -12258.123046875
Iteration 5000: Loss = -12257.7724609375
Iteration 5100: Loss = -12256.333984375
Iteration 5200: Loss = -12256.236328125
Iteration 5300: Loss = -12256.15625
Iteration 5400: Loss = -12256.0859375
Iteration 5500: Loss = -12256.017578125
Iteration 5600: Loss = -12255.9560546875
Iteration 5700: Loss = -12255.89453125
Iteration 5800: Loss = -12255.8408203125
Iteration 5900: Loss = -12255.7880859375
Iteration 6000: Loss = -12255.7392578125
Iteration 6100: Loss = -12255.6923828125
Iteration 6200: Loss = -12255.6494140625
Iteration 6300: Loss = -12255.6083984375
Iteration 6400: Loss = -12255.5693359375
Iteration 6500: Loss = -12255.5322265625
Iteration 6600: Loss = -12255.498046875
Iteration 6700: Loss = -12255.4658203125
Iteration 6800: Loss = -12255.43359375
Iteration 6900: Loss = -12255.40625
Iteration 7000: Loss = -12255.376953125
Iteration 7100: Loss = -12255.3515625
Iteration 7200: Loss = -12255.3271484375
Iteration 7300: Loss = -12255.302734375
Iteration 7400: Loss = -12255.28125
Iteration 7500: Loss = -12255.259765625
Iteration 7600: Loss = -12255.2412109375
Iteration 7700: Loss = -12255.2216796875
Iteration 7800: Loss = -12255.2041015625
Iteration 7900: Loss = -12255.1865234375
Iteration 8000: Loss = -12255.171875
Iteration 8100: Loss = -12255.1552734375
Iteration 8200: Loss = -12255.140625
Iteration 8300: Loss = -12255.1279296875
Iteration 8400: Loss = -12255.115234375
Iteration 8500: Loss = -12255.1015625
Iteration 8600: Loss = -12255.0888671875
Iteration 8700: Loss = -12255.0791015625
Iteration 8800: Loss = -12255.068359375
Iteration 8900: Loss = -12255.0595703125
Iteration 9000: Loss = -12255.048828125
Iteration 9100: Loss = -12255.0400390625
Iteration 9200: Loss = -12255.0322265625
Iteration 9300: Loss = -12255.025390625
Iteration 9400: Loss = -12255.017578125
Iteration 9500: Loss = -12255.0087890625
Iteration 9600: Loss = -12255.0009765625
Iteration 9700: Loss = -12254.99609375
Iteration 9800: Loss = -12254.990234375
Iteration 9900: Loss = -12254.9833984375
Iteration 10000: Loss = -12254.9775390625
Iteration 10100: Loss = -12254.9716796875
Iteration 10200: Loss = -12254.966796875
Iteration 10300: Loss = -12254.962890625
Iteration 10400: Loss = -12254.9580078125
Iteration 10500: Loss = -12254.9541015625
Iteration 10600: Loss = -12254.94921875
Iteration 10700: Loss = -12254.9453125
Iteration 10800: Loss = -12254.94140625
Iteration 10900: Loss = -12254.939453125
Iteration 11000: Loss = -12254.9345703125
Iteration 11100: Loss = -12254.93359375
Iteration 11200: Loss = -12254.9296875
Iteration 11300: Loss = -12254.92578125
Iteration 11400: Loss = -12254.9248046875
Iteration 11500: Loss = -12254.919921875
Iteration 11600: Loss = -12254.9189453125
Iteration 11700: Loss = -12254.916015625
Iteration 11800: Loss = -12254.9150390625
Iteration 11900: Loss = -12254.9111328125
Iteration 12000: Loss = -12254.9091796875
Iteration 12100: Loss = -12254.9091796875
Iteration 12200: Loss = -12254.90625
Iteration 12300: Loss = -12254.9033203125
Iteration 12400: Loss = -12254.9033203125
Iteration 12500: Loss = -12254.900390625
Iteration 12600: Loss = -12254.8984375
Iteration 12700: Loss = -12254.8974609375
Iteration 12800: Loss = -12254.8974609375
Iteration 12900: Loss = -12254.89453125
Iteration 13000: Loss = -12254.8935546875
Iteration 13100: Loss = -12254.892578125
Iteration 13200: Loss = -12254.8916015625
Iteration 13300: Loss = -12254.892578125
1
Iteration 13400: Loss = -12254.890625
Iteration 13500: Loss = -12254.888671875
Iteration 13600: Loss = -12254.888671875
Iteration 13700: Loss = -12254.8876953125
Iteration 13800: Loss = -12254.88671875
Iteration 13900: Loss = -12254.88671875
Iteration 14000: Loss = -12254.88671875
Iteration 14100: Loss = -12254.8857421875
Iteration 14200: Loss = -12254.884765625
Iteration 14300: Loss = -12254.884765625
Iteration 14400: Loss = -12254.8818359375
Iteration 14500: Loss = -12254.8818359375
Iteration 14600: Loss = -12254.8818359375
Iteration 14700: Loss = -12254.8818359375
Iteration 14800: Loss = -12254.8798828125
Iteration 14900: Loss = -12254.8798828125
Iteration 15000: Loss = -12254.87890625
Iteration 15100: Loss = -12254.87890625
Iteration 15200: Loss = -12254.8798828125
1
Iteration 15300: Loss = -12254.87890625
Iteration 15400: Loss = -12254.87890625
Iteration 15500: Loss = -12254.8779296875
Iteration 15600: Loss = -12254.87890625
1
Iteration 15700: Loss = -12254.8779296875
Iteration 15800: Loss = -12254.876953125
Iteration 15900: Loss = -12254.8779296875
1
Iteration 16000: Loss = -12254.8779296875
2
Iteration 16100: Loss = -12254.8779296875
3
Iteration 16200: Loss = -12254.8759765625
Iteration 16300: Loss = -12254.875
Iteration 16400: Loss = -12254.875
Iteration 16500: Loss = -12254.8759765625
1
Iteration 16600: Loss = -12254.8759765625
2
Iteration 16700: Loss = -12254.875
Iteration 16800: Loss = -12254.8740234375
Iteration 16900: Loss = -12254.875
1
Iteration 17000: Loss = -12254.8740234375
Iteration 17100: Loss = -12254.8740234375
Iteration 17200: Loss = -12254.8740234375
Iteration 17300: Loss = -12254.873046875
Iteration 17400: Loss = -12254.873046875
Iteration 17500: Loss = -12254.875
1
Iteration 17600: Loss = -12254.873046875
Iteration 17700: Loss = -12254.873046875
Iteration 17800: Loss = -12254.873046875
Iteration 17900: Loss = -12254.8740234375
1
Iteration 18000: Loss = -12254.8740234375
2
Iteration 18100: Loss = -12254.8740234375
3
Iteration 18200: Loss = -12254.8740234375
4
Iteration 18300: Loss = -12254.873046875
Iteration 18400: Loss = -12254.873046875
Iteration 18500: Loss = -12254.8720703125
Iteration 18600: Loss = -12254.8740234375
1
Iteration 18700: Loss = -12254.8720703125
Iteration 18800: Loss = -12254.8720703125
Iteration 18900: Loss = -12254.8720703125
Iteration 19000: Loss = -12254.8720703125
Iteration 19100: Loss = -12254.873046875
1
Iteration 19200: Loss = -12254.8720703125
Iteration 19300: Loss = -12254.873046875
1
Iteration 19400: Loss = -12254.8740234375
2
Iteration 19500: Loss = -12254.8720703125
Iteration 19600: Loss = -12254.8720703125
Iteration 19700: Loss = -12254.8720703125
Iteration 19800: Loss = -12254.8720703125
Iteration 19900: Loss = -12254.8720703125
Iteration 20000: Loss = -12254.87109375
Iteration 20100: Loss = -12254.8720703125
1
Iteration 20200: Loss = -12254.8740234375
2
Iteration 20300: Loss = -12254.87109375
Iteration 20400: Loss = -12254.8720703125
1
Iteration 20500: Loss = -12254.8720703125
2
Iteration 20600: Loss = -12254.8720703125
3
Iteration 20700: Loss = -12254.87109375
Iteration 20800: Loss = -12254.8720703125
1
Iteration 20900: Loss = -12254.7294921875
Iteration 21000: Loss = -12254.7314453125
1
Iteration 21100: Loss = -12254.7314453125
2
Iteration 21200: Loss = -12254.7314453125
3
Iteration 21300: Loss = -12254.7294921875
Iteration 21400: Loss = -12254.724609375
Iteration 21500: Loss = -12254.72265625
Iteration 21600: Loss = -12254.6552734375
Iteration 21700: Loss = -12254.6474609375
Iteration 21800: Loss = -12254.62109375
Iteration 21900: Loss = -12254.595703125
Iteration 22000: Loss = -12254.5869140625
Iteration 22100: Loss = -12254.5712890625
Iteration 22200: Loss = -12254.541015625
Iteration 22300: Loss = -12254.4794921875
Iteration 22400: Loss = -12254.4091796875
Iteration 22500: Loss = -12254.1689453125
Iteration 22600: Loss = -12254.1142578125
Iteration 22700: Loss = -12253.9580078125
Iteration 22800: Loss = -12253.8251953125
Iteration 22900: Loss = -12253.57421875
Iteration 23000: Loss = -12253.4501953125
Iteration 23100: Loss = -12253.2978515625
Iteration 23200: Loss = -12253.078125
Iteration 23300: Loss = -12252.609375
Iteration 23400: Loss = -12252.3701171875
Iteration 23500: Loss = -12251.6826171875
Iteration 23600: Loss = -12251.6484375
Iteration 23700: Loss = -12250.76171875
Iteration 23800: Loss = -12250.4423828125
Iteration 23900: Loss = -12250.44140625
Iteration 24000: Loss = -12250.44140625
Iteration 24100: Loss = -12250.4404296875
Iteration 24200: Loss = -12250.4404296875
Iteration 24300: Loss = -12250.4404296875
Iteration 24400: Loss = -12250.44140625
1
Iteration 24500: Loss = -12250.44140625
2
Iteration 24600: Loss = -12250.4404296875
Iteration 24700: Loss = -12250.4404296875
Iteration 24800: Loss = -12250.439453125
Iteration 24900: Loss = -12250.44140625
1
Iteration 25000: Loss = -12250.44140625
2
Iteration 25100: Loss = -12250.353515625
Iteration 25200: Loss = -12250.3505859375
Iteration 25300: Loss = -12250.3525390625
1
Iteration 25400: Loss = -12250.3525390625
2
Iteration 25500: Loss = -12250.3525390625
3
Iteration 25600: Loss = -12250.3525390625
4
Iteration 25700: Loss = -12250.353515625
5
Iteration 25800: Loss = -12250.353515625
6
Iteration 25900: Loss = -12250.353515625
7
Iteration 26000: Loss = -12250.3525390625
8
Iteration 26100: Loss = -12250.3525390625
9
Iteration 26200: Loss = -12250.353515625
10
Iteration 26300: Loss = -12250.353515625
11
Iteration 26400: Loss = -12250.353515625
12
Iteration 26500: Loss = -12250.3564453125
13
Iteration 26600: Loss = -12250.3525390625
14
Iteration 26700: Loss = -12250.3544921875
15
Stopping early at iteration 26700 due to no improvement.
pi: tensor([[9.1462e-01, 8.5385e-02],
        [4.9621e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 8.9460e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2014, 0.2173],
         [0.0078, 0.1722]],

        [[0.1049, 0.1944],
         [0.0288, 0.6637]],

        [[0.0079, 0.1740],
         [0.0203, 0.9514]],

        [[0.1183, 0.1759],
         [0.9038, 0.0374]],

        [[0.6084, 0.1942],
         [0.8493, 0.9719]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.012368369840205852
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.015770452305333478
Global Adjusted Rand Index: 0.002667524851177691
Average Adjusted Rand Index: 0.005627764429107866
[0.0014729757840434622, 0.002667524851177691] [0.0054929698557314895, 0.005627764429107866] [12250.1767578125, 12250.3544921875]
-------------------------------------
This iteration is 68
True Objective function: Loss = -11864.030899846859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -43150.78515625
Iteration 100: Loss = -26345.978515625
Iteration 200: Loss = -15596.328125
Iteration 300: Loss = -13167.6025390625
Iteration 400: Loss = -12795.9794921875
Iteration 500: Loss = -12595.869140625
Iteration 600: Loss = -12525.6533203125
Iteration 700: Loss = -12483.8125
Iteration 800: Loss = -12456.0048828125
Iteration 900: Loss = -12436.3525390625
Iteration 1000: Loss = -12421.8447265625
Iteration 1100: Loss = -12410.7705078125
Iteration 1200: Loss = -12402.099609375
Iteration 1300: Loss = -12395.162109375
Iteration 1400: Loss = -12389.5146484375
Iteration 1500: Loss = -12384.84765625
Iteration 1600: Loss = -12380.939453125
Iteration 1700: Loss = -12377.6328125
Iteration 1800: Loss = -12374.8076171875
Iteration 1900: Loss = -12372.3779296875
Iteration 2000: Loss = -12370.265625
Iteration 2100: Loss = -12368.4189453125
Iteration 2200: Loss = -12366.796875
Iteration 2300: Loss = -12365.3623046875
Iteration 2400: Loss = -12364.0869140625
Iteration 2500: Loss = -12362.951171875
Iteration 2600: Loss = -12361.93359375
Iteration 2700: Loss = -12361.017578125
Iteration 2800: Loss = -12360.1904296875
Iteration 2900: Loss = -12359.4404296875
Iteration 3000: Loss = -12358.7626953125
Iteration 3100: Loss = -12358.1435546875
Iteration 3200: Loss = -12357.580078125
Iteration 3300: Loss = -12357.0634765625
Iteration 3400: Loss = -12356.58984375
Iteration 3500: Loss = -12356.1572265625
Iteration 3600: Loss = -12355.7548828125
Iteration 3700: Loss = -12355.384765625
Iteration 3800: Loss = -12355.0458984375
Iteration 3900: Loss = -12354.7294921875
Iteration 4000: Loss = -12354.4384765625
Iteration 4100: Loss = -12354.1669921875
Iteration 4200: Loss = -12353.9140625
Iteration 4300: Loss = -12353.6806640625
Iteration 4400: Loss = -12353.4638671875
Iteration 4500: Loss = -12353.259765625
Iteration 4600: Loss = -12353.0693359375
Iteration 4700: Loss = -12352.8935546875
Iteration 4800: Loss = -12352.728515625
Iteration 4900: Loss = -12352.57421875
Iteration 5000: Loss = -12352.4287109375
Iteration 5100: Loss = -12352.2919921875
Iteration 5200: Loss = -12352.1640625
Iteration 5300: Loss = -12352.044921875
Iteration 5400: Loss = -12351.931640625
Iteration 5500: Loss = -12351.82421875
Iteration 5600: Loss = -12351.7236328125
Iteration 5700: Loss = -12351.6298828125
Iteration 5800: Loss = -12351.5400390625
Iteration 5900: Loss = -12351.4560546875
Iteration 6000: Loss = -12351.3759765625
Iteration 6100: Loss = -12351.3017578125
Iteration 6200: Loss = -12351.2314453125
Iteration 6300: Loss = -12351.162109375
Iteration 6400: Loss = -12351.0966796875
Iteration 6500: Loss = -12351.03125
Iteration 6600: Loss = -12350.9619140625
Iteration 6700: Loss = -12350.892578125
Iteration 6800: Loss = -12350.822265625
Iteration 6900: Loss = -12350.7607421875
Iteration 7000: Loss = -12350.703125
Iteration 7100: Loss = -12350.658203125
Iteration 7200: Loss = -12350.615234375
Iteration 7300: Loss = -12350.5791015625
Iteration 7400: Loss = -12350.546875
Iteration 7500: Loss = -12350.517578125
Iteration 7600: Loss = -12350.494140625
Iteration 7700: Loss = -12350.4755859375
Iteration 7800: Loss = -12350.4560546875
Iteration 7900: Loss = -12350.4404296875
Iteration 8000: Loss = -12350.4228515625
Iteration 8100: Loss = -12350.4072265625
Iteration 8200: Loss = -12350.39453125
Iteration 8300: Loss = -12350.3798828125
Iteration 8400: Loss = -12350.3662109375
Iteration 8500: Loss = -12350.353515625
Iteration 8600: Loss = -12350.3408203125
Iteration 8700: Loss = -12350.330078125
Iteration 8800: Loss = -12350.3193359375
Iteration 8900: Loss = -12350.3095703125
Iteration 9000: Loss = -12350.2998046875
Iteration 9100: Loss = -12350.291015625
Iteration 9200: Loss = -12350.2822265625
Iteration 9300: Loss = -12350.2734375
Iteration 9400: Loss = -12350.265625
Iteration 9500: Loss = -12350.2587890625
Iteration 9600: Loss = -12350.251953125
Iteration 9700: Loss = -12350.24609375
Iteration 9800: Loss = -12350.23828125
Iteration 9900: Loss = -12350.232421875
Iteration 10000: Loss = -12350.2275390625
Iteration 10100: Loss = -12350.22265625
Iteration 10200: Loss = -12350.2158203125
Iteration 10300: Loss = -12350.20703125
Iteration 10400: Loss = -12350.185546875
Iteration 10500: Loss = -12349.9677734375
Iteration 10600: Loss = -12349.890625
Iteration 10700: Loss = -12349.8525390625
Iteration 10800: Loss = -12349.826171875
Iteration 10900: Loss = -12349.8076171875
Iteration 11000: Loss = -12349.7919921875
Iteration 11100: Loss = -12349.77734375
Iteration 11200: Loss = -12349.767578125
Iteration 11300: Loss = -12349.7529296875
Iteration 11400: Loss = -12349.7451171875
Iteration 11500: Loss = -12349.7333984375
Iteration 11600: Loss = -12349.724609375
Iteration 11700: Loss = -12349.7158203125
Iteration 11800: Loss = -12349.7041015625
Iteration 11900: Loss = -12349.6953125
Iteration 12000: Loss = -12349.6845703125
Iteration 12100: Loss = -12349.673828125
Iteration 12200: Loss = -12349.6630859375
Iteration 12300: Loss = -12349.6513671875
Iteration 12400: Loss = -12349.640625
Iteration 12500: Loss = -12349.6279296875
Iteration 12600: Loss = -12349.6142578125
Iteration 12700: Loss = -12349.6025390625
Iteration 12800: Loss = -12349.5869140625
Iteration 12900: Loss = -12349.5712890625
Iteration 13000: Loss = -12349.5498046875
Iteration 13100: Loss = -12349.52734375
Iteration 13200: Loss = -12349.2822265625
Iteration 13300: Loss = -12349.15234375
Iteration 13400: Loss = -12349.1025390625
Iteration 13500: Loss = -12349.0693359375
Iteration 13600: Loss = -12349.041015625
Iteration 13700: Loss = -12349.0166015625
Iteration 13800: Loss = -12348.9921875
Iteration 13900: Loss = -12348.9716796875
Iteration 14000: Loss = -12348.9482421875
Iteration 14100: Loss = -12348.927734375
Iteration 14200: Loss = -12348.9140625
Iteration 14300: Loss = -12348.8994140625
Iteration 14400: Loss = -12348.8876953125
Iteration 14500: Loss = -12348.8798828125
Iteration 14600: Loss = -12348.865234375
Iteration 14700: Loss = -12348.8544921875
Iteration 14800: Loss = -12348.8466796875
Iteration 14900: Loss = -12348.84375
Iteration 15000: Loss = -12348.8408203125
Iteration 15100: Loss = -12348.8369140625
Iteration 15200: Loss = -12348.822265625
Iteration 15300: Loss = -12348.8203125
Iteration 15400: Loss = -12348.8173828125
Iteration 15500: Loss = -12348.818359375
1
Iteration 15600: Loss = -12348.818359375
2
Iteration 15700: Loss = -12348.81640625
Iteration 15800: Loss = -12348.81640625
Iteration 15900: Loss = -12348.81640625
Iteration 16000: Loss = -12348.8173828125
1
Iteration 16100: Loss = -12348.8173828125
2
Iteration 16200: Loss = -12348.81640625
Iteration 16300: Loss = -12348.81640625
Iteration 16400: Loss = -12348.8154296875
Iteration 16500: Loss = -12348.81640625
1
Iteration 16600: Loss = -12348.8154296875
Iteration 16700: Loss = -12348.81640625
1
Iteration 16800: Loss = -12348.814453125
Iteration 16900: Loss = -12348.8154296875
1
Iteration 17000: Loss = -12348.8154296875
2
Iteration 17100: Loss = -12348.814453125
Iteration 17200: Loss = -12348.814453125
Iteration 17300: Loss = -12348.814453125
Iteration 17400: Loss = -12348.814453125
Iteration 17500: Loss = -12348.814453125
Iteration 17600: Loss = -12348.8134765625
Iteration 17700: Loss = -12348.8134765625
Iteration 17800: Loss = -12348.814453125
1
Iteration 17900: Loss = -12348.814453125
2
Iteration 18000: Loss = -12348.8134765625
Iteration 18100: Loss = -12348.8125
Iteration 18200: Loss = -12348.8125
Iteration 18300: Loss = -12348.8134765625
1
Iteration 18400: Loss = -12348.8115234375
Iteration 18500: Loss = -12348.814453125
1
Iteration 18600: Loss = -12348.8125
2
Iteration 18700: Loss = -12348.8134765625
3
Iteration 18800: Loss = -12348.8115234375
Iteration 18900: Loss = -12348.8125
1
Iteration 19000: Loss = -12348.8134765625
2
Iteration 19100: Loss = -12348.8134765625
3
Iteration 19200: Loss = -12348.8134765625
4
Iteration 19300: Loss = -12348.8125
5
Iteration 19400: Loss = -12348.8125
6
Iteration 19500: Loss = -12348.8125
7
Iteration 19600: Loss = -12348.8125
8
Iteration 19700: Loss = -12348.8125
9
Iteration 19800: Loss = -12348.8134765625
10
Iteration 19900: Loss = -12348.8125
11
Iteration 20000: Loss = -12348.8125
12
Iteration 20100: Loss = -12348.8125
13
Iteration 20200: Loss = -12348.8134765625
14
Iteration 20300: Loss = -12348.810546875
Iteration 20400: Loss = -12348.8125
1
Iteration 20500: Loss = -12348.8134765625
2
Iteration 20600: Loss = -12348.8115234375
3
Iteration 20700: Loss = -12348.8125
4
Iteration 20800: Loss = -12348.8115234375
5
Iteration 20900: Loss = -12348.8125
6
Iteration 21000: Loss = -12348.8125
7
Iteration 21100: Loss = -12348.8125
8
Iteration 21200: Loss = -12348.8115234375
9
Iteration 21300: Loss = -12348.8115234375
10
Iteration 21400: Loss = -12348.8125
11
Iteration 21500: Loss = -12348.8125
12
Iteration 21600: Loss = -12348.810546875
Iteration 21700: Loss = -12348.8125
1
Iteration 21800: Loss = -12348.8125
2
Iteration 21900: Loss = -12348.8115234375
3
Iteration 22000: Loss = -12348.8115234375
4
Iteration 22100: Loss = -12348.814453125
5
Iteration 22200: Loss = -12348.8125
6
Iteration 22300: Loss = -12348.8115234375
7
Iteration 22400: Loss = -12348.8134765625
8
Iteration 22500: Loss = -12348.8125
9
Iteration 22600: Loss = -12348.8125
10
Iteration 22700: Loss = -12348.8134765625
11
Iteration 22800: Loss = -12348.8134765625
12
Iteration 22900: Loss = -12348.8125
13
Iteration 23000: Loss = -12348.8115234375
14
Iteration 23100: Loss = -12348.8115234375
15
Stopping early at iteration 23100 due to no improvement.
pi: tensor([[9.8833e-01, 1.1669e-02],
        [9.9997e-01, 2.7922e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 1.3989e-05], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1985, 0.2100],
         [0.7405, 0.2394]],

        [[0.9920, 0.1930],
         [0.0169, 0.1776]],

        [[0.0711, 0.2951],
         [0.1760, 0.9505]],

        [[0.8867, 0.0964],
         [0.0100, 0.9713]],

        [[0.9692, 0.3250],
         [0.1590, 0.9395]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: -3.18091359493932e-05
Average Adjusted Rand Index: -0.00015673616107960255
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28320.611328125
Iteration 100: Loss = -18594.89453125
Iteration 200: Loss = -13603.595703125
Iteration 300: Loss = -12790.7607421875
Iteration 400: Loss = -12592.2236328125
Iteration 500: Loss = -12513.5556640625
Iteration 600: Loss = -12470.7431640625
Iteration 700: Loss = -12446.8984375
Iteration 800: Loss = -12428.1474609375
Iteration 900: Loss = -12415.7509765625
Iteration 1000: Loss = -12402.6923828125
Iteration 1100: Loss = -12392.138671875
Iteration 1200: Loss = -12385.9140625
Iteration 1300: Loss = -12380.5947265625
Iteration 1400: Loss = -12373.6572265625
Iteration 1500: Loss = -12370.4814453125
Iteration 1600: Loss = -12367.0107421875
Iteration 1700: Loss = -12364.0205078125
Iteration 1800: Loss = -12362.4013671875
Iteration 1900: Loss = -12361.2177734375
Iteration 2000: Loss = -12360.275390625
Iteration 2100: Loss = -12359.4892578125
Iteration 2200: Loss = -12358.8095703125
Iteration 2300: Loss = -12358.21484375
Iteration 2400: Loss = -12357.66796875
Iteration 2500: Loss = -12356.037109375
Iteration 2600: Loss = -12355.4990234375
Iteration 2700: Loss = -12355.0615234375
Iteration 2800: Loss = -12354.6796875
Iteration 2900: Loss = -12354.345703125
Iteration 3000: Loss = -12354.0458984375
Iteration 3100: Loss = -12353.77734375
Iteration 3200: Loss = -12353.5322265625
Iteration 3300: Loss = -12353.3115234375
Iteration 3400: Loss = -12353.1103515625
Iteration 3500: Loss = -12352.92578125
Iteration 3600: Loss = -12352.75390625
Iteration 3700: Loss = -12352.5986328125
Iteration 3800: Loss = -12352.455078125
Iteration 3900: Loss = -12352.3203125
Iteration 4000: Loss = -12352.1953125
Iteration 4100: Loss = -12352.0810546875
Iteration 4200: Loss = -12351.9716796875
Iteration 4300: Loss = -12351.873046875
Iteration 4400: Loss = -12351.7802734375
Iteration 4500: Loss = -12351.69140625
Iteration 4600: Loss = -12351.611328125
Iteration 4700: Loss = -12351.53515625
Iteration 4800: Loss = -12351.4609375
Iteration 4900: Loss = -12351.396484375
Iteration 5000: Loss = -12351.33203125
Iteration 5100: Loss = -12351.271484375
Iteration 5200: Loss = -12351.216796875
Iteration 5300: Loss = -12351.1630859375
Iteration 5400: Loss = -12351.11328125
Iteration 5500: Loss = -12351.0673828125
Iteration 5600: Loss = -12351.021484375
Iteration 5700: Loss = -12350.9814453125
Iteration 5800: Loss = -12350.94140625
Iteration 5900: Loss = -12350.90625
Iteration 6000: Loss = -12350.8701171875
Iteration 6100: Loss = -12350.8359375
Iteration 6200: Loss = -12350.8046875
Iteration 6300: Loss = -12350.775390625
Iteration 6400: Loss = -12350.7451171875
Iteration 6500: Loss = -12350.7177734375
Iteration 6600: Loss = -12350.6962890625
Iteration 6700: Loss = -12350.66796875
Iteration 6800: Loss = -12350.6455078125
Iteration 6900: Loss = -12350.6240234375
Iteration 7000: Loss = -12350.6015625
Iteration 7100: Loss = -12350.58203125
Iteration 7200: Loss = -12350.564453125
Iteration 7300: Loss = -12350.5458984375
Iteration 7400: Loss = -12350.5283203125
Iteration 7500: Loss = -12350.5107421875
Iteration 7600: Loss = -12350.49609375
Iteration 7700: Loss = -12350.4814453125
Iteration 7800: Loss = -12350.466796875
Iteration 7900: Loss = -12350.4521484375
Iteration 8000: Loss = -12350.4375
Iteration 8100: Loss = -12350.4248046875
Iteration 8200: Loss = -12350.412109375
Iteration 8300: Loss = -12350.3974609375
Iteration 8400: Loss = -12350.384765625
Iteration 8500: Loss = -12350.37109375
Iteration 8600: Loss = -12350.357421875
Iteration 8700: Loss = -12350.341796875
Iteration 8800: Loss = -12350.3251953125
Iteration 8900: Loss = -12350.3037109375
Iteration 9000: Loss = -12350.279296875
Iteration 9100: Loss = -12350.2470703125
Iteration 9200: Loss = -12350.208984375
Iteration 9300: Loss = -12350.171875
Iteration 9400: Loss = -12350.142578125
Iteration 9500: Loss = -12350.1201171875
Iteration 9600: Loss = -12350.099609375
Iteration 9700: Loss = -12350.080078125
Iteration 9800: Loss = -12350.0546875
Iteration 9900: Loss = -12350.0341796875
Iteration 10000: Loss = -12350.01171875
Iteration 10100: Loss = -12349.98828125
Iteration 10200: Loss = -12349.9599609375
Iteration 10300: Loss = -12349.9306640625
Iteration 10400: Loss = -12349.8994140625
Iteration 10500: Loss = -12349.865234375
Iteration 10600: Loss = -12349.8291015625
Iteration 10700: Loss = -12349.791015625
Iteration 10800: Loss = -12349.7548828125
Iteration 10900: Loss = -12349.7236328125
Iteration 11000: Loss = -12349.697265625
Iteration 11100: Loss = -12349.671875
Iteration 11200: Loss = -12349.65625
Iteration 11300: Loss = -12349.6435546875
Iteration 11400: Loss = -12349.6357421875
Iteration 11500: Loss = -12349.6279296875
Iteration 11600: Loss = -12349.625
Iteration 11700: Loss = -12349.62109375
Iteration 11800: Loss = -12349.6171875
Iteration 11900: Loss = -12349.61328125
Iteration 12000: Loss = -12349.611328125
Iteration 12100: Loss = -12349.6103515625
Iteration 12200: Loss = -12349.6083984375
Iteration 12300: Loss = -12349.60546875
Iteration 12400: Loss = -12349.60546875
Iteration 12500: Loss = -12349.603515625
Iteration 12600: Loss = -12349.6025390625
Iteration 12700: Loss = -12349.6015625
Iteration 12800: Loss = -12349.599609375
Iteration 12900: Loss = -12349.599609375
Iteration 13000: Loss = -12349.599609375
Iteration 13100: Loss = -12349.595703125
Iteration 13200: Loss = -12349.5947265625
Iteration 13300: Loss = -12349.5947265625
Iteration 13400: Loss = -12349.59375
Iteration 13500: Loss = -12349.5927734375
Iteration 13600: Loss = -12349.5927734375
Iteration 13700: Loss = -12349.591796875
Iteration 13800: Loss = -12349.5927734375
1
Iteration 13900: Loss = -12349.591796875
Iteration 14000: Loss = -12349.5888671875
Iteration 14100: Loss = -12349.5888671875
Iteration 14200: Loss = -12349.5908203125
1
Iteration 14300: Loss = -12349.5888671875
Iteration 14400: Loss = -12349.587890625
Iteration 14500: Loss = -12349.5888671875
1
Iteration 14600: Loss = -12349.5869140625
Iteration 14700: Loss = -12349.5859375
Iteration 14800: Loss = -12349.5859375
Iteration 14900: Loss = -12349.5849609375
Iteration 15000: Loss = -12349.5849609375
Iteration 15100: Loss = -12349.5849609375
Iteration 15200: Loss = -12349.583984375
Iteration 15300: Loss = -12349.583984375
Iteration 15400: Loss = -12349.580078125
Iteration 15500: Loss = -12349.5791015625
Iteration 15600: Loss = -12349.578125
Iteration 15700: Loss = -12349.5771484375
Iteration 15800: Loss = -12349.5751953125
Iteration 15900: Loss = -12349.5732421875
Iteration 16000: Loss = -12349.5703125
Iteration 16100: Loss = -12349.5693359375
Iteration 16200: Loss = -12349.5634765625
Iteration 16300: Loss = -12349.55078125
Iteration 16400: Loss = -12349.3271484375
Iteration 16500: Loss = -12349.30078125
Iteration 16600: Loss = -12349.291015625
Iteration 16700: Loss = -12349.2841796875
Iteration 16800: Loss = -12349.287109375
1
Iteration 16900: Loss = -12349.279296875
Iteration 17000: Loss = -12349.2783203125
Iteration 17100: Loss = -12349.2666015625
Iteration 17200: Loss = -12348.7919921875
Iteration 17300: Loss = -12348.7900390625
Iteration 17400: Loss = -12348.7861328125
Iteration 17500: Loss = -12348.7841796875
Iteration 17600: Loss = -12348.78515625
1
Iteration 17700: Loss = -12348.7763671875
Iteration 17800: Loss = -12348.7685546875
Iteration 17900: Loss = -12348.6259765625
Iteration 18000: Loss = -12348.5986328125
Iteration 18100: Loss = -12348.59765625
Iteration 18200: Loss = -12348.59765625
Iteration 18300: Loss = -12348.5810546875
Iteration 18400: Loss = -12348.4931640625
Iteration 18500: Loss = -12348.4912109375
Iteration 18600: Loss = -12348.490234375
Iteration 18700: Loss = -12348.4755859375
Iteration 18800: Loss = -12348.45703125
Iteration 18900: Loss = -12348.4296875
Iteration 19000: Loss = -12348.4111328125
Iteration 19100: Loss = -12348.4130859375
1
Iteration 19200: Loss = -12348.4091796875
Iteration 19300: Loss = -12348.408203125
Iteration 19400: Loss = -12348.4033203125
Iteration 19500: Loss = -12348.3916015625
Iteration 19600: Loss = -12348.3916015625
Iteration 19700: Loss = -12348.390625
Iteration 19800: Loss = -12348.388671875
Iteration 19900: Loss = -12348.38671875
Iteration 20000: Loss = -12348.3857421875
Iteration 20100: Loss = -12348.3828125
Iteration 20200: Loss = -12348.3798828125
Iteration 20300: Loss = -12348.3779296875
Iteration 20400: Loss = -12348.3779296875
Iteration 20500: Loss = -12348.375
Iteration 20600: Loss = -12348.3759765625
1
Iteration 20700: Loss = -12348.375
Iteration 20800: Loss = -12348.376953125
1
Iteration 20900: Loss = -12348.376953125
2
Iteration 21000: Loss = -12348.3759765625
3
Iteration 21100: Loss = -12348.375
Iteration 21200: Loss = -12348.375
Iteration 21300: Loss = -12348.3759765625
1
Iteration 21400: Loss = -12348.3740234375
Iteration 21500: Loss = -12348.375
1
Iteration 21600: Loss = -12348.375
2
Iteration 21700: Loss = -12348.3740234375
Iteration 21800: Loss = -12348.3740234375
Iteration 21900: Loss = -12348.375
1
Iteration 22000: Loss = -12348.3740234375
Iteration 22100: Loss = -12348.375
1
Iteration 22200: Loss = -12348.373046875
Iteration 22300: Loss = -12348.373046875
Iteration 22400: Loss = -12348.3740234375
1
Iteration 22500: Loss = -12348.3720703125
Iteration 22600: Loss = -12348.373046875
1
Iteration 22700: Loss = -12348.3740234375
2
Iteration 22800: Loss = -12348.3720703125
Iteration 22900: Loss = -12348.373046875
1
Iteration 23000: Loss = -12348.373046875
2
Iteration 23100: Loss = -12348.3720703125
Iteration 23200: Loss = -12348.37109375
Iteration 23300: Loss = -12348.3720703125
1
Iteration 23400: Loss = -12348.3720703125
2
Iteration 23500: Loss = -12348.3720703125
3
Iteration 23600: Loss = -12348.3798828125
4
Iteration 23700: Loss = -12348.373046875
5
Iteration 23800: Loss = -12348.373046875
6
Iteration 23900: Loss = -12348.373046875
7
Iteration 24000: Loss = -12348.37109375
Iteration 24100: Loss = -12348.373046875
1
Iteration 24200: Loss = -12348.373046875
2
Iteration 24300: Loss = -12348.373046875
3
Iteration 24400: Loss = -12348.3720703125
4
Iteration 24500: Loss = -12348.373046875
5
Iteration 24600: Loss = -12348.3740234375
6
Iteration 24700: Loss = -12348.3720703125
7
Iteration 24800: Loss = -12348.3740234375
8
Iteration 24900: Loss = -12348.3720703125
9
Iteration 25000: Loss = -12348.373046875
10
Iteration 25100: Loss = -12348.3720703125
11
Iteration 25200: Loss = -12348.3720703125
12
Iteration 25300: Loss = -12348.373046875
13
Iteration 25400: Loss = -12348.37109375
Iteration 25500: Loss = -12348.3720703125
1
Iteration 25600: Loss = -12348.3720703125
2
Iteration 25700: Loss = -12348.37109375
Iteration 25800: Loss = -12348.37109375
Iteration 25900: Loss = -12348.3720703125
1
Iteration 26000: Loss = -12348.37109375
Iteration 26100: Loss = -12348.3701171875
Iteration 26200: Loss = -12348.369140625
Iteration 26300: Loss = -12348.37109375
1
Iteration 26400: Loss = -12348.3701171875
2
Iteration 26500: Loss = -12348.37109375
3
Iteration 26600: Loss = -12348.3701171875
4
Iteration 26700: Loss = -12348.369140625
Iteration 26800: Loss = -12348.369140625
Iteration 26900: Loss = -12348.37109375
1
Iteration 27000: Loss = -12348.3701171875
2
Iteration 27100: Loss = -12348.369140625
Iteration 27200: Loss = -12348.3701171875
1
Iteration 27300: Loss = -12348.369140625
Iteration 27400: Loss = -12348.3701171875
1
Iteration 27500: Loss = -12348.37109375
2
Iteration 27600: Loss = -12348.3701171875
3
Iteration 27700: Loss = -12348.37109375
4
Iteration 27800: Loss = -12348.3701171875
5
Iteration 27900: Loss = -12348.3701171875
6
Iteration 28000: Loss = -12348.369140625
Iteration 28100: Loss = -12348.369140625
Iteration 28200: Loss = -12348.37109375
1
Iteration 28300: Loss = -12348.3701171875
2
Iteration 28400: Loss = -12348.37109375
3
Iteration 28500: Loss = -12348.37109375
4
Iteration 28600: Loss = -12348.3701171875
5
Iteration 28700: Loss = -12348.3701171875
6
Iteration 28800: Loss = -12348.369140625
Iteration 28900: Loss = -12348.369140625
Iteration 29000: Loss = -12348.369140625
Iteration 29100: Loss = -12348.373046875
1
Iteration 29200: Loss = -12348.3681640625
Iteration 29300: Loss = -12348.37109375
1
Iteration 29400: Loss = -12348.369140625
2
Iteration 29500: Loss = -12348.3701171875
3
Iteration 29600: Loss = -12348.369140625
4
Iteration 29700: Loss = -12348.3701171875
5
Iteration 29800: Loss = -12348.3701171875
6
Iteration 29900: Loss = -12348.369140625
7
pi: tensor([[9.8996e-01, 1.0038e-02],
        [9.9999e-01, 7.4110e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9436, 0.0564], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1997, 0.1485],
         [0.7477, 0.1335]],

        [[0.6579, 0.1823],
         [0.9312, 0.9152]],

        [[0.0756, 0.3072],
         [0.0154, 0.6325]],

        [[0.4574, 0.0953],
         [0.9841, 0.9905]],

        [[0.0361, 0.3308],
         [0.0510, 0.8254]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: -3.18091359493932e-05
Average Adjusted Rand Index: -0.00015673616107960255
[-3.18091359493932e-05, -3.18091359493932e-05] [-0.00015673616107960255, -0.00015673616107960255] [12348.8115234375, 12348.369140625]
-------------------------------------
This iteration is 69
True Objective function: Loss = -11890.060135150363
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23521.818359375
Iteration 100: Loss = -16265.8603515625
Iteration 200: Loss = -13037.29296875
Iteration 300: Loss = -12580.2978515625
Iteration 400: Loss = -12493.4296875
Iteration 500: Loss = -12456.765625
Iteration 600: Loss = -12437.9912109375
Iteration 700: Loss = -12425.6474609375
Iteration 800: Loss = -12414.0966796875
Iteration 900: Loss = -12406.0576171875
Iteration 1000: Loss = -12397.927734375
Iteration 1100: Loss = -12390.75
Iteration 1200: Loss = -12385.1005859375
Iteration 1300: Loss = -12377.720703125
Iteration 1400: Loss = -12372.5693359375
Iteration 1500: Loss = -12367.99609375
Iteration 1600: Loss = -12364.6884765625
Iteration 1700: Loss = -12361.1904296875
Iteration 1800: Loss = -12358.5712890625
Iteration 1900: Loss = -12356.310546875
Iteration 2000: Loss = -12354.34375
Iteration 2100: Loss = -12352.8076171875
Iteration 2200: Loss = -12351.703125
Iteration 2300: Loss = -12350.8916015625
Iteration 2400: Loss = -12350.2646484375
Iteration 2500: Loss = -12349.7587890625
Iteration 2600: Loss = -12349.328125
Iteration 2700: Loss = -12348.9423828125
Iteration 2800: Loss = -12348.5166015625
Iteration 2900: Loss = -12347.6328125
Iteration 3000: Loss = -12347.23828125
Iteration 3100: Loss = -12346.9931640625
Iteration 3200: Loss = -12346.794921875
Iteration 3300: Loss = -12346.6240234375
Iteration 3400: Loss = -12346.4736328125
Iteration 3500: Loss = -12346.3408203125
Iteration 3600: Loss = -12346.220703125
Iteration 3700: Loss = -12346.1142578125
Iteration 3800: Loss = -12346.0126953125
Iteration 3900: Loss = -12345.9228515625
Iteration 4000: Loss = -12345.8388671875
Iteration 4100: Loss = -12345.76171875
Iteration 4200: Loss = -12345.6923828125
Iteration 4300: Loss = -12345.6259765625
Iteration 4400: Loss = -12345.5625
Iteration 4500: Loss = -12345.5078125
Iteration 4600: Loss = -12345.4521484375
Iteration 4700: Loss = -12345.40234375
Iteration 4800: Loss = -12345.35546875
Iteration 4900: Loss = -12345.310546875
Iteration 5000: Loss = -12345.2685546875
Iteration 5100: Loss = -12345.23046875
Iteration 5200: Loss = -12345.1923828125
Iteration 5300: Loss = -12345.158203125
Iteration 5400: Loss = -12345.125
Iteration 5500: Loss = -12345.0947265625
Iteration 5600: Loss = -12345.064453125
Iteration 5700: Loss = -12345.0361328125
Iteration 5800: Loss = -12345.0087890625
Iteration 5900: Loss = -12344.9833984375
Iteration 6000: Loss = -12344.958984375
Iteration 6100: Loss = -12344.935546875
Iteration 6200: Loss = -12344.9130859375
Iteration 6300: Loss = -12344.892578125
Iteration 6400: Loss = -12344.8720703125
Iteration 6500: Loss = -12344.8525390625
Iteration 6600: Loss = -12344.8330078125
Iteration 6700: Loss = -12344.8134765625
Iteration 6800: Loss = -12344.796875
Iteration 6900: Loss = -12344.779296875
Iteration 7000: Loss = -12344.765625
Iteration 7100: Loss = -12344.7509765625
Iteration 7200: Loss = -12344.7373046875
Iteration 7300: Loss = -12344.7236328125
Iteration 7400: Loss = -12344.708984375
Iteration 7500: Loss = -12344.6982421875
Iteration 7600: Loss = -12344.6845703125
Iteration 7700: Loss = -12344.6728515625
Iteration 7800: Loss = -12344.6630859375
Iteration 7900: Loss = -12344.65234375
Iteration 8000: Loss = -12344.642578125
Iteration 8100: Loss = -12344.630859375
Iteration 8200: Loss = -12344.6240234375
Iteration 8300: Loss = -12344.6142578125
Iteration 8400: Loss = -12344.607421875
Iteration 8500: Loss = -12344.5986328125
Iteration 8600: Loss = -12344.5908203125
Iteration 8700: Loss = -12344.5830078125
Iteration 8800: Loss = -12344.5791015625
Iteration 8900: Loss = -12344.5712890625
Iteration 9000: Loss = -12344.56640625
Iteration 9100: Loss = -12344.560546875
Iteration 9200: Loss = -12344.5537109375
Iteration 9300: Loss = -12344.548828125
Iteration 9400: Loss = -12344.544921875
Iteration 9500: Loss = -12344.5400390625
Iteration 9600: Loss = -12344.533203125
Iteration 9700: Loss = -12344.5302734375
Iteration 9800: Loss = -12344.525390625
Iteration 9900: Loss = -12344.5224609375
Iteration 10000: Loss = -12344.51953125
Iteration 10100: Loss = -12344.5146484375
Iteration 10200: Loss = -12344.5146484375
Iteration 10300: Loss = -12344.5107421875
Iteration 10400: Loss = -12344.5078125
Iteration 10500: Loss = -12344.505859375
Iteration 10600: Loss = -12344.501953125
Iteration 10700: Loss = -12344.5
Iteration 10800: Loss = -12344.498046875
Iteration 10900: Loss = -12344.49609375
Iteration 11000: Loss = -12344.4951171875
Iteration 11100: Loss = -12344.4921875
Iteration 11200: Loss = -12344.4912109375
Iteration 11300: Loss = -12344.490234375
Iteration 11400: Loss = -12344.48828125
Iteration 11500: Loss = -12344.4873046875
Iteration 11600: Loss = -12344.4853515625
Iteration 11700: Loss = -12344.4853515625
Iteration 11800: Loss = -12344.4833984375
Iteration 11900: Loss = -12344.4833984375
Iteration 12000: Loss = -12344.482421875
Iteration 12100: Loss = -12344.4814453125
Iteration 12200: Loss = -12344.4794921875
Iteration 12300: Loss = -12344.4775390625
Iteration 12400: Loss = -12344.478515625
1
Iteration 12500: Loss = -12344.478515625
2
Iteration 12600: Loss = -12344.4775390625
Iteration 12700: Loss = -12344.4775390625
Iteration 12800: Loss = -12344.4755859375
Iteration 12900: Loss = -12344.474609375
Iteration 13000: Loss = -12344.4736328125
Iteration 13100: Loss = -12344.474609375
1
Iteration 13200: Loss = -12344.47265625
Iteration 13300: Loss = -12344.4736328125
1
Iteration 13400: Loss = -12344.47265625
Iteration 13500: Loss = -12344.4716796875
Iteration 13600: Loss = -12344.4697265625
Iteration 13700: Loss = -12344.470703125
1
Iteration 13800: Loss = -12344.470703125
2
Iteration 13900: Loss = -12344.470703125
3
Iteration 14000: Loss = -12344.4697265625
Iteration 14100: Loss = -12344.4697265625
Iteration 14200: Loss = -12344.46875
Iteration 14300: Loss = -12344.4697265625
1
Iteration 14400: Loss = -12344.4697265625
2
Iteration 14500: Loss = -12344.46875
Iteration 14600: Loss = -12344.46875
Iteration 14700: Loss = -12344.466796875
Iteration 14800: Loss = -12344.4677734375
1
Iteration 14900: Loss = -12344.46875
2
Iteration 15000: Loss = -12344.4658203125
Iteration 15100: Loss = -12344.466796875
1
Iteration 15200: Loss = -12344.4677734375
2
Iteration 15300: Loss = -12344.4658203125
Iteration 15400: Loss = -12344.4677734375
1
Iteration 15500: Loss = -12344.4755859375
2
Iteration 15600: Loss = -12344.4658203125
Iteration 15700: Loss = -12344.4658203125
Iteration 15800: Loss = -12344.46484375
Iteration 15900: Loss = -12344.4658203125
1
Iteration 16000: Loss = -12344.4658203125
2
Iteration 16100: Loss = -12344.46484375
Iteration 16200: Loss = -12344.4638671875
Iteration 16300: Loss = -12344.46484375
1
Iteration 16400: Loss = -12344.4658203125
2
Iteration 16500: Loss = -12344.46484375
3
Iteration 16600: Loss = -12344.46484375
4
Iteration 16700: Loss = -12344.4638671875
Iteration 16800: Loss = -12344.46484375
1
Iteration 16900: Loss = -12344.46484375
2
Iteration 17000: Loss = -12344.462890625
Iteration 17100: Loss = -12344.4638671875
1
Iteration 17200: Loss = -12344.4638671875
2
Iteration 17300: Loss = -12344.46484375
3
Iteration 17400: Loss = -12344.4638671875
4
Iteration 17500: Loss = -12344.4658203125
5
Iteration 17600: Loss = -12344.4638671875
6
Iteration 17700: Loss = -12344.46484375
7
Iteration 17800: Loss = -12344.4638671875
8
Iteration 17900: Loss = -12344.4638671875
9
Iteration 18000: Loss = -12344.4638671875
10
Iteration 18100: Loss = -12344.46484375
11
Iteration 18200: Loss = -12344.4638671875
12
Iteration 18300: Loss = -12344.462890625
Iteration 18400: Loss = -12344.46484375
1
Iteration 18500: Loss = -12344.4638671875
2
Iteration 18600: Loss = -12344.46484375
3
Iteration 18700: Loss = -12344.46484375
4
Iteration 18800: Loss = -12344.46484375
5
Iteration 18900: Loss = -12344.462890625
Iteration 19000: Loss = -12344.46484375
1
Iteration 19100: Loss = -12344.466796875
2
Iteration 19200: Loss = -12344.46484375
3
Iteration 19300: Loss = -12344.462890625
Iteration 19400: Loss = -12344.4638671875
1
Iteration 19500: Loss = -12344.4638671875
2
Iteration 19600: Loss = -12344.462890625
Iteration 19700: Loss = -12344.462890625
Iteration 19800: Loss = -12344.46484375
1
Iteration 19900: Loss = -12344.462890625
Iteration 20000: Loss = -12344.4638671875
1
Iteration 20100: Loss = -12344.462890625
Iteration 20200: Loss = -12344.46484375
1
Iteration 20300: Loss = -12344.462890625
Iteration 20400: Loss = -12344.46484375
1
Iteration 20500: Loss = -12344.4638671875
2
Iteration 20600: Loss = -12344.462890625
Iteration 20700: Loss = -12344.462890625
Iteration 20800: Loss = -12344.4638671875
1
Iteration 20900: Loss = -12344.462890625
Iteration 21000: Loss = -12344.4619140625
Iteration 21100: Loss = -12344.462890625
1
Iteration 21200: Loss = -12344.462890625
2
Iteration 21300: Loss = -12344.462890625
3
Iteration 21400: Loss = -12344.46484375
4
Iteration 21500: Loss = -12344.462890625
5
Iteration 21600: Loss = -12344.4638671875
6
Iteration 21700: Loss = -12344.46484375
7
Iteration 21800: Loss = -12344.4638671875
8
Iteration 21900: Loss = -12344.46484375
9
Iteration 22000: Loss = -12344.46484375
10
Iteration 22100: Loss = -12344.4658203125
11
Iteration 22200: Loss = -12344.46484375
12
Iteration 22300: Loss = -12344.462890625
13
Iteration 22400: Loss = -12344.46484375
14
Iteration 22500: Loss = -12344.4638671875
15
Stopping early at iteration 22500 due to no improvement.
pi: tensor([[1.1400e-05, 9.9999e-01],
        [6.3807e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0100, 0.9900], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1764, 0.2972],
         [0.1184, 0.1984]],

        [[0.9632, 0.3152],
         [0.9727, 0.0094]],

        [[0.2955, 0.1882],
         [0.7497, 0.2869]],

        [[0.1987, 0.1864],
         [0.8361, 0.5903]],

        [[0.4821, 0.2091],
         [0.8836, 0.1283]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 70%|███████   | 70/100 [55:20:56<23:38:17, 2836.59s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 71%|███████   | 71/100 [56:01:34<21:53:19, 2717.23s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 72%|███████▏  | 72/100 [56:57:03<22:33:37, 2900.63s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 73%|███████▎  | 73/100 [57:49:26<22:18:03, 2973.46s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 74%|███████▍  | 74/100 [58:30:45<20:24:06, 2824.88s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 75%|███████▌  | 75/100 [59:09:09<18:31:58, 2668.73s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 76%|███████▌  | 76/100 [59:47:52<17:05:59, 2564.98s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 77%|███████▋  | 77/100 [60:35:01<16:53:39, 2644.34s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 78%|███████▊  | 78/100 [61:26:30<16:58:28, 2777.65s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 79%|███████▉  | 79/100 [62:17:35<16:42:18, 2863.74s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 80%|████████  | 80/100 [62:59:32<15:19:57, 2759.88s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 81%|████████  | 81/100 [63:49:26<14:56:08, 2829.93s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 82%|████████▏ | 82/100 [64:39:12<14:23:06, 2877.00s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 83%|████████▎ | 83/100 [65:23:01<13:14:03, 2802.58s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 84%|████████▍ | 84/100 [66:05:13<12:05:39, 2721.21s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 85%|████████▌ | 85/100 [66:59:28<12:00:21, 2881.44s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 86%|████████▌ | 86/100 [67:51:04<11:27:20, 2945.75s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 87%|████████▋ | 87/100 [68:36:26<10:23:44, 2878.81s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 88%|████████▊ | 88/100 [69:31:13<10:00:13, 3001.08s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 89%|████████▉ | 89/100 [70:11:27<8:37:56, 2825.12s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 90%|█████████ | 90/100 [70:51:01<7:28:17, 2689.72s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 91%|█████████ | 91/100 [71:36:51<6:46:08, 2707.62s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 92%|█████████▏| 92/100 [72:29:40<6:19:29, 2846.17s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 93%|█████████▎| 93/100 [73:24:16<5:47:05, 2975.07s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 94%|█████████▍| 94/100 [74:18:03<5:05:03, 3050.65s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 95%|█████████▌| 95/100 [75:03:59<4:06:50, 2962.19s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 96%|█████████▌| 96/100 [75:44:54<3:07:20, 2810.04s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 97%|█████████▋| 97/100 [76:28:29<2:17:35, 2751.79s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 98%|█████████▊| 98/100 [77:10:14<1:29:14, 2677.47s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 99%|█████████▉| 99/100 [78:05:11<47:43, 2863.53s/it]  /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
100%|██████████| 100/100 [78:48:09<00:00, 2777.95s/it]100%|██████████| 100/100 [78:48:09<00:00, 2836.90s/it]
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00030769329388876244
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -32130.42578125
Iteration 100: Loss = -22495.04296875
Iteration 200: Loss = -14838.1005859375
Iteration 300: Loss = -13198.904296875
Iteration 400: Loss = -12761.4931640625
Iteration 500: Loss = -12678.0869140625
Iteration 600: Loss = -12646.833984375
Iteration 700: Loss = -12626.482421875
Iteration 800: Loss = -12611.2734375
Iteration 900: Loss = -12600.091796875
Iteration 1000: Loss = -12591.8046875
Iteration 1100: Loss = -12584.408203125
Iteration 1200: Loss = -12579.7470703125
Iteration 1300: Loss = -12574.3994140625
Iteration 1400: Loss = -12569.966796875
Iteration 1500: Loss = -12565.232421875
Iteration 1600: Loss = -12561.1181640625
Iteration 1700: Loss = -12555.904296875
Iteration 1800: Loss = -12551.583984375
Iteration 1900: Loss = -12546.9228515625
Iteration 2000: Loss = -12542.9638671875
Iteration 2100: Loss = -12539.1796875
Iteration 2200: Loss = -12534.943359375
Iteration 2300: Loss = -12529.59375
Iteration 2400: Loss = -12524.125
Iteration 2500: Loss = -12519.6103515625
Iteration 2600: Loss = -12515.20703125
Iteration 2700: Loss = -12511.068359375
Iteration 2800: Loss = -12506.669921875
Iteration 2900: Loss = -12503.251953125
Iteration 3000: Loss = -12500.2587890625
Iteration 3100: Loss = -12496.578125
Iteration 3200: Loss = -12493.0615234375
Iteration 3300: Loss = -12490.7646484375
Iteration 3400: Loss = -12488.517578125
Iteration 3500: Loss = -12487.205078125
Iteration 3600: Loss = -12486.078125
Iteration 3700: Loss = -12485.0380859375
Iteration 3800: Loss = -12483.3369140625
Iteration 3900: Loss = -12482.59375
Iteration 4000: Loss = -12481.94140625
Iteration 4100: Loss = -12480.5458984375
Iteration 4200: Loss = -12479.97265625
Iteration 4300: Loss = -12479.4052734375
Iteration 4400: Loss = -12478.900390625
Iteration 4500: Loss = -12478.439453125
Iteration 4600: Loss = -12477.9130859375
Iteration 4700: Loss = -12477.4482421875
Iteration 4800: Loss = -12477.13671875
Iteration 4900: Loss = -12476.5498046875
Iteration 5000: Loss = -12476.271484375
Iteration 5100: Loss = -12476.087890625
Iteration 5200: Loss = -12475.3388671875
Iteration 5300: Loss = -12475.162109375
Iteration 5400: Loss = -12475.05078125
Iteration 5500: Loss = -12474.9580078125
Iteration 5600: Loss = -12474.8740234375
Iteration 5700: Loss = -12474.7978515625
Iteration 5800: Loss = -12474.7265625
Iteration 5900: Loss = -12474.6611328125
Iteration 6000: Loss = -12474.59765625
Iteration 6100: Loss = -12474.5322265625
Iteration 6200: Loss = -12474.4326171875
Iteration 6300: Loss = -12473.9208984375
Iteration 6400: Loss = -12473.6845703125
Iteration 6500: Loss = -12473.4462890625
Iteration 6600: Loss = -12473.2783203125
Iteration 6700: Loss = -12473.1796875
Iteration 6800: Loss = -12473.11328125
Iteration 6900: Loss = -12473.0625
Iteration 7000: Loss = -12473.01953125
Iteration 7100: Loss = -12472.9833984375
Iteration 7200: Loss = -12472.951171875
Iteration 7300: Loss = -12472.9208984375
Iteration 7400: Loss = -12472.8935546875
Iteration 7500: Loss = -12472.869140625
Iteration 7600: Loss = -12472.8447265625
Iteration 7700: Loss = -12472.8251953125
Iteration 7800: Loss = -12472.8037109375
Iteration 7900: Loss = -12472.7861328125
Iteration 8000: Loss = -12472.7685546875
Iteration 8100: Loss = -12472.75
Iteration 8200: Loss = -12472.732421875
Iteration 8300: Loss = -12472.6884765625
Iteration 8400: Loss = -12472.1630859375
Iteration 8500: Loss = -12472.0751953125
Iteration 8600: Loss = -12472.044921875
Iteration 8700: Loss = -12471.9091796875
Iteration 8800: Loss = -12470.794921875
Iteration 8900: Loss = -12470.6708984375
Iteration 9000: Loss = -12470.63671875
Iteration 9100: Loss = -12470.615234375
Iteration 9200: Loss = -12470.5986328125
Iteration 9300: Loss = -12470.5859375
Iteration 9400: Loss = -12470.57421875
Iteration 9500: Loss = -12470.5634765625
Iteration 9600: Loss = -12470.5556640625
Iteration 9700: Loss = -12470.5478515625
Iteration 9800: Loss = -12470.5390625
Iteration 9900: Loss = -12470.5322265625
Iteration 10000: Loss = -12470.5244140625
Iteration 10100: Loss = -12470.51953125
Iteration 10200: Loss = -12470.5126953125
Iteration 10300: Loss = -12470.5068359375
Iteration 10400: Loss = -12470.501953125
Iteration 10500: Loss = -12470.4990234375
Iteration 10600: Loss = -12470.4931640625
Iteration 10700: Loss = -12470.48828125
Iteration 10800: Loss = -12470.48046875
Iteration 10900: Loss = -12469.0947265625
Iteration 11000: Loss = -12469.08203125
Iteration 11100: Loss = -12469.076171875
Iteration 11200: Loss = -12469.0712890625
Iteration 11300: Loss = -12468.4970703125
Iteration 11400: Loss = -12468.41015625
Iteration 11500: Loss = -12468.4013671875
Iteration 11600: Loss = -12468.3974609375
Iteration 11700: Loss = -12468.3916015625
Iteration 11800: Loss = -12467.5869140625
Iteration 11900: Loss = -12466.9169921875
Iteration 12000: Loss = -12466.6630859375
Iteration 12100: Loss = -12466.15625
Iteration 12200: Loss = -12466.1494140625
Iteration 12300: Loss = -12466.1455078125
Iteration 12400: Loss = -12466.142578125
Iteration 12500: Loss = -12466.1396484375
Iteration 12600: Loss = -12466.1396484375
Iteration 12700: Loss = -12466.1357421875
Iteration 12800: Loss = -12466.134765625
Iteration 12900: Loss = -12466.1318359375
Iteration 13000: Loss = -12466.130859375
Iteration 13100: Loss = -12465.7216796875
Iteration 13200: Loss = -12465.4765625
Iteration 13300: Loss = -12465.4755859375
Iteration 13400: Loss = -12465.4736328125
Iteration 13500: Loss = -12465.47265625
Iteration 13600: Loss = -12465.4716796875
Iteration 13700: Loss = -12465.46875
Iteration 13800: Loss = -12465.4697265625
1
Iteration 13900: Loss = -12465.46875
Iteration 14000: Loss = -12465.46875
Iteration 14100: Loss = -12465.466796875
Iteration 14200: Loss = -12465.466796875
Iteration 14300: Loss = -12465.4658203125
Iteration 14400: Loss = -12465.4658203125
Iteration 14500: Loss = -12465.46484375
Iteration 14600: Loss = -12465.4638671875
Iteration 14700: Loss = -12465.462890625
Iteration 14800: Loss = -12465.462890625
Iteration 14900: Loss = -12465.462890625
Iteration 15000: Loss = -12465.4609375
Iteration 15100: Loss = -12465.4599609375
Iteration 15200: Loss = -12465.4609375
1
Iteration 15300: Loss = -12465.4599609375
Iteration 15400: Loss = -12465.458984375
Iteration 15500: Loss = -12465.4599609375
1
Iteration 15600: Loss = -12465.458984375
Iteration 15700: Loss = -12465.458984375
Iteration 15800: Loss = -12465.45703125
Iteration 15900: Loss = -12465.458984375
1
Iteration 16000: Loss = -12465.4580078125
2
Iteration 16100: Loss = -12465.45703125
Iteration 16200: Loss = -12465.4580078125
1
Iteration 16300: Loss = -12465.458984375
2
Iteration 16400: Loss = -12465.4560546875
Iteration 16500: Loss = -12465.45703125
1
Iteration 16600: Loss = -12465.4560546875
Iteration 16700: Loss = -12465.4560546875
Iteration 16800: Loss = -12465.4560546875
Iteration 16900: Loss = -12465.4541015625
Iteration 17000: Loss = -12465.455078125
1
Iteration 17100: Loss = -12465.4560546875
2
Iteration 17200: Loss = -12465.455078125
3
Iteration 17300: Loss = -12465.455078125
4
Iteration 17400: Loss = -12465.455078125
5
Iteration 17500: Loss = -12465.455078125
6
Iteration 17600: Loss = -12465.455078125
7
Iteration 17700: Loss = -12465.453125
Iteration 17800: Loss = -12465.455078125
1
Iteration 17900: Loss = -12465.453125
Iteration 18000: Loss = -12465.4541015625
1
Iteration 18100: Loss = -12465.453125
Iteration 18200: Loss = -12465.4521484375
Iteration 18300: Loss = -12465.4521484375
Iteration 18400: Loss = -12465.453125
1
Iteration 18500: Loss = -12465.453125
2
Iteration 18600: Loss = -12465.4541015625
3
Iteration 18700: Loss = -12465.453125
4
Iteration 18800: Loss = -12465.4521484375
Iteration 18900: Loss = -12465.453125
1
Iteration 19000: Loss = -12465.4521484375
Iteration 19100: Loss = -12465.453125
1
Iteration 19200: Loss = -12465.453125
2
Iteration 19300: Loss = -12465.453125
3
Iteration 19400: Loss = -12465.4541015625
4
Iteration 19500: Loss = -12465.4521484375
Iteration 19600: Loss = -12465.453125
1
Iteration 19700: Loss = -12465.453125
2
Iteration 19800: Loss = -12465.453125
3
Iteration 19900: Loss = -12465.4521484375
Iteration 20000: Loss = -12465.453125
1
Iteration 20100: Loss = -12465.453125
2
Iteration 20200: Loss = -12465.453125
3
Iteration 20300: Loss = -12465.453125
4
Iteration 20400: Loss = -12465.453125
5
Iteration 20500: Loss = -12465.4521484375
Iteration 20600: Loss = -12465.451171875
Iteration 20700: Loss = -12465.4521484375
1
Iteration 20800: Loss = -12465.4521484375
2
Iteration 20900: Loss = -12465.4521484375
3
Iteration 21000: Loss = -12465.4521484375
4
Iteration 21100: Loss = -12465.451171875
Iteration 21200: Loss = -12465.453125
1
Iteration 21300: Loss = -12465.4521484375
2
Iteration 21400: Loss = -12465.453125
3
Iteration 21500: Loss = -12465.4521484375
4
Iteration 21600: Loss = -12465.453125
5
Iteration 21700: Loss = -12465.4521484375
6
Iteration 21800: Loss = -12465.453125
7
Iteration 21900: Loss = -12465.453125
8
Iteration 22000: Loss = -12465.4521484375
9
Iteration 22100: Loss = -12465.427734375
Iteration 22200: Loss = -12461.876953125
Iteration 22300: Loss = -12457.666015625
Iteration 22400: Loss = -12454.685546875
Iteration 22500: Loss = -12453.5537109375
Iteration 22600: Loss = -12450.3935546875
Iteration 22700: Loss = -12448.1435546875
Iteration 22800: Loss = -12444.6201171875
Iteration 22900: Loss = -12440.3876953125
Iteration 23000: Loss = -12437.890625
Iteration 23100: Loss = -12435.1279296875
Iteration 23200: Loss = -12432.41015625
Iteration 23300: Loss = -12429.953125
Iteration 23400: Loss = -12428.83203125
Iteration 23500: Loss = -12428.58984375
Iteration 23600: Loss = -12425.9501953125
Iteration 23700: Loss = -12423.6513671875
Iteration 23800: Loss = -12423.060546875
Iteration 23900: Loss = -12421.1240234375
Iteration 24000: Loss = -12418.060546875
Iteration 24100: Loss = -12415.8388671875
Iteration 24200: Loss = -12396.9931640625
Iteration 24300: Loss = -12374.337890625
Iteration 24400: Loss = -12372.310546875
Iteration 24500: Loss = -12294.716796875
Iteration 24600: Loss = -12262.4853515625
Iteration 24700: Loss = -12233.5166015625
Iteration 24800: Loss = -12231.0625
Iteration 24900: Loss = -12226.71875
Iteration 25000: Loss = -12214.9638671875
Iteration 25100: Loss = -12209.787109375
Iteration 25200: Loss = -12209.5107421875
Iteration 25300: Loss = -12209.1474609375
Iteration 25400: Loss = -12199.078125
Iteration 25500: Loss = -12198.9375
Iteration 25600: Loss = -12198.8369140625
Iteration 25700: Loss = -12192.666015625
Iteration 25800: Loss = -12191.779296875
Iteration 25900: Loss = -12179.8935546875
Iteration 26000: Loss = -12168.53125
Iteration 26100: Loss = -12152.8720703125
Iteration 26200: Loss = -12105.17578125
Iteration 26300: Loss = -11973.6083984375
Iteration 26400: Loss = -11923.3525390625
Iteration 26500: Loss = -11916.556640625
Iteration 26600: Loss = -11915.0048828125
Iteration 26700: Loss = -11914.7294921875
Iteration 26800: Loss = -11914.5654296875
Iteration 26900: Loss = -11914.453125
Iteration 27000: Loss = -11914.37109375
Iteration 27100: Loss = -11914.310546875
Iteration 27200: Loss = -11914.2607421875
Iteration 27300: Loss = -11907.369140625
Iteration 27400: Loss = -11903.1298828125
Iteration 27500: Loss = -11899.634765625
Iteration 27600: Loss = -11899.5380859375
Iteration 27700: Loss = -11899.5
Iteration 27800: Loss = -11899.470703125
Iteration 27900: Loss = -11899.447265625
Iteration 28000: Loss = -11899.4287109375
Iteration 28100: Loss = -11884.4580078125
Iteration 28200: Loss = -11881.6220703125
Iteration 28300: Loss = -11881.5166015625
Iteration 28400: Loss = -11881.4697265625
Iteration 28500: Loss = -11881.439453125
Iteration 28600: Loss = -11881.41796875
Iteration 28700: Loss = -11881.4013671875
Iteration 28800: Loss = -11881.3876953125
Iteration 28900: Loss = -11881.375
Iteration 29000: Loss = -11881.3671875
Iteration 29100: Loss = -11881.3583984375
Iteration 29200: Loss = -11881.3505859375
Iteration 29300: Loss = -11881.3447265625
Iteration 29400: Loss = -11881.337890625
Iteration 29500: Loss = -11881.33203125
Iteration 29600: Loss = -11881.3271484375
Iteration 29700: Loss = -11881.3232421875
Iteration 29800: Loss = -11881.3193359375
Iteration 29900: Loss = -11881.3154296875
pi: tensor([[0.2134, 0.7866],
        [0.6809, 0.3191]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5015, 0.4985], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3027, 0.1074],
         [0.9913, 0.2898]],

        [[0.0389, 0.1040],
         [0.3440, 0.2699]],

        [[0.9901, 0.1023],
         [0.9919, 0.0216]],

        [[0.9871, 0.0948],
         [0.9556, 0.0851]],

        [[0.0154, 0.1002],
         [0.3671, 0.1242]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03969566228820517
Average Adjusted Rand Index: 0.9919993417272899
[-0.00030769329388876244, 0.03969566228820517] [0.0, 0.9919993417272899] [12344.4638671875, 11881.3115234375]
-------------------------------------
This iteration is 70
True Objective function: Loss = -11781.076150103168
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -34283.69140625
Iteration 100: Loss = -22158.853515625
Iteration 200: Loss = -14573.865234375
Iteration 300: Loss = -13104.70703125
Iteration 400: Loss = -12782.1455078125
Iteration 500: Loss = -12653.4677734375
Iteration 600: Loss = -12583.5712890625
Iteration 700: Loss = -12535.369140625
Iteration 800: Loss = -12502.32421875
Iteration 900: Loss = -12477.4052734375
Iteration 1000: Loss = -12462.865234375
Iteration 1100: Loss = -12449.564453125
Iteration 1200: Loss = -12439.2548828125
Iteration 1300: Loss = -12425.916015625
Iteration 1400: Loss = -12415.744140625
Iteration 1500: Loss = -12407.228515625
Iteration 1600: Loss = -12400.896484375
Iteration 1700: Loss = -12395.9443359375
Iteration 1800: Loss = -12392.6669921875
Iteration 1900: Loss = -12389.3662109375
Iteration 2000: Loss = -12385.9716796875
Iteration 2100: Loss = -12382.140625
Iteration 2200: Loss = -12378.5703125
Iteration 2300: Loss = -12376.97265625
Iteration 2400: Loss = -12375.7646484375
Iteration 2500: Loss = -12374.7333984375
Iteration 2600: Loss = -12370.7763671875
Iteration 2700: Loss = -12369.91015625
Iteration 2800: Loss = -12367.40234375
Iteration 2900: Loss = -12364.453125
Iteration 3000: Loss = -12363.7529296875
Iteration 3100: Loss = -12363.1767578125
Iteration 3200: Loss = -12362.67578125
Iteration 3300: Loss = -12362.2373046875
Iteration 3400: Loss = -12361.8466796875
Iteration 3500: Loss = -12361.494140625
Iteration 3600: Loss = -12361.1787109375
Iteration 3700: Loss = -12360.890625
Iteration 3800: Loss = -12360.6279296875
Iteration 3900: Loss = -12360.0419921875
Iteration 4000: Loss = -12355.537109375
Iteration 4100: Loss = -12355.1982421875
Iteration 4200: Loss = -12354.9267578125
Iteration 4300: Loss = -12354.689453125
Iteration 4400: Loss = -12354.4775390625
Iteration 4500: Loss = -12354.2861328125
Iteration 4600: Loss = -12354.1142578125
Iteration 4700: Loss = -12353.953125
Iteration 4800: Loss = -12353.806640625
Iteration 4900: Loss = -12353.6728515625
Iteration 5000: Loss = -12353.546875
Iteration 5100: Loss = -12353.431640625
Iteration 5200: Loss = -12353.3251953125
Iteration 5300: Loss = -12353.224609375
Iteration 5400: Loss = -12353.1328125
Iteration 5500: Loss = -12353.044921875
Iteration 5600: Loss = -12352.96484375
Iteration 5700: Loss = -12352.8876953125
Iteration 5800: Loss = -12352.81640625
Iteration 5900: Loss = -12352.748046875
Iteration 6000: Loss = -12352.6865234375
Iteration 6100: Loss = -12352.626953125
Iteration 6200: Loss = -12352.572265625
Iteration 6300: Loss = -12352.5185546875
Iteration 6400: Loss = -12352.470703125
Iteration 6500: Loss = -12352.42578125
Iteration 6600: Loss = -12352.3818359375
Iteration 6700: Loss = -12352.33984375
Iteration 6800: Loss = -12352.302734375
Iteration 6900: Loss = -12352.265625
Iteration 7000: Loss = -12352.2314453125
Iteration 7100: Loss = -12352.19921875
Iteration 7200: Loss = -12352.16796875
Iteration 7300: Loss = -12352.138671875
Iteration 7400: Loss = -12352.1103515625
Iteration 7500: Loss = -12352.0859375
Iteration 7600: Loss = -12352.0615234375
Iteration 7700: Loss = -12352.0390625
Iteration 7800: Loss = -12352.0166015625
Iteration 7900: Loss = -12351.99609375
Iteration 8000: Loss = -12351.9755859375
Iteration 8100: Loss = -12351.95703125
Iteration 8200: Loss = -12351.939453125
Iteration 8300: Loss = -12351.9228515625
Iteration 8400: Loss = -12351.908203125
Iteration 8500: Loss = -12351.892578125
Iteration 8600: Loss = -12351.87890625
Iteration 8700: Loss = -12351.865234375
Iteration 8800: Loss = -12351.8525390625
Iteration 8900: Loss = -12351.83984375
Iteration 9000: Loss = -12351.8271484375
Iteration 9100: Loss = -12351.8193359375
Iteration 9200: Loss = -12351.806640625
Iteration 9300: Loss = -12351.798828125
Iteration 9400: Loss = -12351.787109375
Iteration 9500: Loss = -12351.779296875
Iteration 9600: Loss = -12351.7705078125
Iteration 9700: Loss = -12351.763671875
Iteration 9800: Loss = -12351.755859375
Iteration 9900: Loss = -12351.748046875
Iteration 10000: Loss = -12351.7421875
Iteration 10100: Loss = -12351.736328125
Iteration 10200: Loss = -12351.728515625
Iteration 10300: Loss = -12351.72265625
Iteration 10400: Loss = -12351.7177734375
Iteration 10500: Loss = -12351.712890625
Iteration 10600: Loss = -12351.7080078125
Iteration 10700: Loss = -12351.705078125
Iteration 10800: Loss = -12351.6982421875
Iteration 10900: Loss = -12351.6953125
Iteration 11000: Loss = -12351.6904296875
Iteration 11100: Loss = -12351.6865234375
Iteration 11200: Loss = -12351.6826171875
Iteration 11300: Loss = -12351.6796875
Iteration 11400: Loss = -12351.67578125
Iteration 11500: Loss = -12351.6728515625
Iteration 11600: Loss = -12351.669921875
Iteration 11700: Loss = -12351.6669921875
Iteration 11800: Loss = -12351.6650390625
Iteration 11900: Loss = -12351.6611328125
Iteration 12000: Loss = -12351.66015625
Iteration 12100: Loss = -12351.658203125
Iteration 12200: Loss = -12351.65625
Iteration 12300: Loss = -12351.6533203125
Iteration 12400: Loss = -12351.65234375
Iteration 12500: Loss = -12351.6494140625
Iteration 12600: Loss = -12351.646484375
Iteration 12700: Loss = -12351.64453125
Iteration 12800: Loss = -12351.64453125
Iteration 12900: Loss = -12351.6435546875
Iteration 13000: Loss = -12351.6416015625
Iteration 13100: Loss = -12351.642578125
1
Iteration 13200: Loss = -12351.638671875
Iteration 13300: Loss = -12351.6376953125
Iteration 13400: Loss = -12351.63671875
Iteration 13500: Loss = -12351.6376953125
1
Iteration 13600: Loss = -12351.6357421875
Iteration 13700: Loss = -12351.6337890625
Iteration 13800: Loss = -12351.6337890625
Iteration 13900: Loss = -12351.6318359375
Iteration 14000: Loss = -12351.6318359375
Iteration 14100: Loss = -12351.630859375
Iteration 14200: Loss = -12351.6298828125
Iteration 14300: Loss = -12351.62890625
Iteration 14400: Loss = -12351.62890625
Iteration 14500: Loss = -12351.6279296875
Iteration 14600: Loss = -12351.6259765625
Iteration 14700: Loss = -12351.625
Iteration 14800: Loss = -12351.626953125
1
Iteration 14900: Loss = -12351.6240234375
Iteration 15000: Loss = -12351.625
1
Iteration 15100: Loss = -12351.6240234375
Iteration 15200: Loss = -12351.623046875
Iteration 15300: Loss = -12351.6220703125
Iteration 15400: Loss = -12351.623046875
1
Iteration 15500: Loss = -12351.6220703125
Iteration 15600: Loss = -12351.6220703125
Iteration 15700: Loss = -12351.623046875
1
Iteration 15800: Loss = -12351.623046875
2
Iteration 15900: Loss = -12351.6201171875
Iteration 16000: Loss = -12351.619140625
Iteration 16100: Loss = -12351.6201171875
1
Iteration 16200: Loss = -12351.6181640625
Iteration 16300: Loss = -12351.619140625
1
Iteration 16400: Loss = -12351.619140625
2
Iteration 16500: Loss = -12351.619140625
3
Iteration 16600: Loss = -12351.6181640625
Iteration 16700: Loss = -12351.6181640625
Iteration 16800: Loss = -12351.619140625
1
Iteration 16900: Loss = -12351.619140625
2
Iteration 17000: Loss = -12351.6201171875
3
Iteration 17100: Loss = -12351.6181640625
Iteration 17200: Loss = -12351.6181640625
Iteration 17300: Loss = -12351.6171875
Iteration 17400: Loss = -12351.6171875
Iteration 17500: Loss = -12351.6162109375
Iteration 17600: Loss = -12351.6181640625
1
Iteration 17700: Loss = -12351.6171875
2
Iteration 17800: Loss = -12351.6162109375
Iteration 17900: Loss = -12351.6171875
1
Iteration 18000: Loss = -12351.6162109375
Iteration 18100: Loss = -12351.6171875
1
Iteration 18200: Loss = -12351.6171875
2
Iteration 18300: Loss = -12351.6171875
3
Iteration 18400: Loss = -12351.6162109375
Iteration 18500: Loss = -12351.6171875
1
Iteration 18600: Loss = -12351.6162109375
Iteration 18700: Loss = -12351.6171875
1
Iteration 18800: Loss = -12351.6162109375
Iteration 18900: Loss = -12351.6171875
1
Iteration 19000: Loss = -12351.6162109375
Iteration 19100: Loss = -12351.6181640625
1
Iteration 19200: Loss = -12351.6162109375
Iteration 19300: Loss = -12351.6162109375
Iteration 19400: Loss = -12351.6162109375
Iteration 19500: Loss = -12351.6171875
1
Iteration 19600: Loss = -12351.6162109375
Iteration 19700: Loss = -12351.6162109375
Iteration 19800: Loss = -12351.6142578125
Iteration 19900: Loss = -12351.6162109375
1
Iteration 20000: Loss = -12351.6171875
2
Iteration 20100: Loss = -12351.6162109375
3
Iteration 20200: Loss = -12351.6171875
4
Iteration 20300: Loss = -12351.615234375
5
Iteration 20400: Loss = -12351.615234375
6
Iteration 20500: Loss = -12351.615234375
7
Iteration 20600: Loss = -12351.615234375
8
Iteration 20700: Loss = -12351.615234375
9
Iteration 20800: Loss = -12351.6171875
10
Iteration 20900: Loss = -12351.615234375
11
Iteration 21000: Loss = -12351.6162109375
12
Iteration 21100: Loss = -12351.615234375
13
Iteration 21200: Loss = -12351.6171875
14
Iteration 21300: Loss = -12351.615234375
15
Stopping early at iteration 21300 due to no improvement.
pi: tensor([[9.9998e-01, 2.2122e-05],
        [9.8075e-01, 1.9249e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 4.9909e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1990, 0.2850],
         [0.8656, 0.1549]],

        [[0.0083, 0.2135],
         [0.9718, 0.0147]],

        [[0.0127, 0.1877],
         [0.0160, 0.7273]],

        [[0.3887, 0.1047],
         [0.9872, 0.2636]],

        [[0.3792, 0.4239],
         [0.2483, 0.0073]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22590.162109375
Iteration 100: Loss = -16829.96875
Iteration 200: Loss = -13321.58984375
Iteration 300: Loss = -12561.8359375
Iteration 400: Loss = -12433.87890625
Iteration 500: Loss = -12397.9990234375
Iteration 600: Loss = -12381.2666015625
Iteration 700: Loss = -12372.1669921875
Iteration 800: Loss = -12366.677734375
Iteration 900: Loss = -12363.1025390625
Iteration 1000: Loss = -12360.6298828125
Iteration 1100: Loss = -12358.8330078125
Iteration 1200: Loss = -12357.4794921875
Iteration 1300: Loss = -12356.4375
Iteration 1400: Loss = -12355.615234375
Iteration 1500: Loss = -12354.951171875
Iteration 1600: Loss = -12354.40625
Iteration 1700: Loss = -12353.953125
Iteration 1800: Loss = -12353.572265625
Iteration 1900: Loss = -12353.2431640625
Iteration 2000: Loss = -12352.9619140625
Iteration 2100: Loss = -12352.716796875
Iteration 2200: Loss = -12352.50390625
Iteration 2300: Loss = -12352.3154296875
Iteration 2400: Loss = -12352.150390625
Iteration 2500: Loss = -12352.0
Iteration 2600: Loss = -12351.8701171875
Iteration 2700: Loss = -12351.75
Iteration 2800: Loss = -12351.642578125
Iteration 2900: Loss = -12351.544921875
Iteration 3000: Loss = -12351.4580078125
Iteration 3100: Loss = -12351.3759765625
Iteration 3200: Loss = -12351.3037109375
Iteration 3300: Loss = -12351.236328125
Iteration 3400: Loss = -12351.1748046875
Iteration 3500: Loss = -12351.115234375
Iteration 3600: Loss = -12351.0615234375
Iteration 3700: Loss = -12351.01171875
Iteration 3800: Loss = -12350.962890625
Iteration 3900: Loss = -12350.91796875
Iteration 4000: Loss = -12350.8759765625
Iteration 4100: Loss = -12350.8349609375
Iteration 4200: Loss = -12350.79296875
Iteration 4300: Loss = -12350.7568359375
Iteration 4400: Loss = -12350.71875
Iteration 4500: Loss = -12350.6787109375
Iteration 4600: Loss = -12350.640625
Iteration 4700: Loss = -12350.6015625
Iteration 4800: Loss = -12350.5625
Iteration 4900: Loss = -12350.5234375
Iteration 5000: Loss = -12350.4814453125
Iteration 5100: Loss = -12350.439453125
Iteration 5200: Loss = -12350.3974609375
Iteration 5300: Loss = -12350.3544921875
Iteration 5400: Loss = -12350.3134765625
Iteration 5500: Loss = -12350.2763671875
Iteration 5600: Loss = -12350.2412109375
Iteration 5700: Loss = -12350.2080078125
Iteration 5800: Loss = -12350.181640625
Iteration 5900: Loss = -12350.158203125
Iteration 6000: Loss = -12350.1357421875
Iteration 6100: Loss = -12350.1162109375
Iteration 6200: Loss = -12350.0986328125
Iteration 6300: Loss = -12350.0810546875
Iteration 6400: Loss = -12350.0673828125
Iteration 6500: Loss = -12350.056640625
Iteration 6600: Loss = -12350.0439453125
Iteration 6700: Loss = -12350.0361328125
Iteration 6800: Loss = -12350.0302734375
Iteration 6900: Loss = -12350.0224609375
Iteration 7000: Loss = -12350.01953125
Iteration 7100: Loss = -12350.0126953125
Iteration 7200: Loss = -12350.0087890625
Iteration 7300: Loss = -12350.00390625
Iteration 7400: Loss = -12350.0009765625
Iteration 7500: Loss = -12349.9970703125
Iteration 7600: Loss = -12349.9921875
Iteration 7700: Loss = -12349.990234375
Iteration 7800: Loss = -12349.98828125
Iteration 7900: Loss = -12349.9853515625
Iteration 8000: Loss = -12349.982421875
Iteration 8100: Loss = -12349.9814453125
Iteration 8200: Loss = -12349.9794921875
Iteration 8300: Loss = -12349.978515625
Iteration 8400: Loss = -12349.974609375
Iteration 8500: Loss = -12349.9736328125
Iteration 8600: Loss = -12349.97265625
Iteration 8700: Loss = -12349.970703125
Iteration 8800: Loss = -12349.96875
Iteration 8900: Loss = -12349.966796875
Iteration 9000: Loss = -12349.96484375
Iteration 9100: Loss = -12349.9638671875
Iteration 9200: Loss = -12349.962890625
Iteration 9300: Loss = -12349.962890625
Iteration 9400: Loss = -12349.9619140625
Iteration 9500: Loss = -12349.9609375
Iteration 9600: Loss = -12349.9599609375
Iteration 9700: Loss = -12349.958984375
Iteration 9800: Loss = -12349.9580078125
Iteration 9900: Loss = -12349.9599609375
1
Iteration 10000: Loss = -12349.958984375
2
Iteration 10100: Loss = -12349.95703125
Iteration 10200: Loss = -12349.9560546875
Iteration 10300: Loss = -12349.9560546875
Iteration 10400: Loss = -12349.955078125
Iteration 10500: Loss = -12349.9541015625
Iteration 10600: Loss = -12349.9541015625
Iteration 10700: Loss = -12349.953125
Iteration 10800: Loss = -12349.9541015625
1
Iteration 10900: Loss = -12349.9521484375
Iteration 11000: Loss = -12349.951171875
Iteration 11100: Loss = -12349.9501953125
Iteration 11200: Loss = -12349.9482421875
Iteration 11300: Loss = -12349.9462890625
Iteration 11400: Loss = -12349.9404296875
Iteration 11500: Loss = -12349.927734375
Iteration 11600: Loss = -12349.9052734375
Iteration 11700: Loss = -12349.880859375
Iteration 11800: Loss = -12349.8623046875
Iteration 11900: Loss = -12349.84765625
Iteration 12000: Loss = -12349.837890625
Iteration 12100: Loss = -12349.83203125
Iteration 12200: Loss = -12349.8251953125
Iteration 12300: Loss = -12349.8212890625
Iteration 12400: Loss = -12349.8193359375
Iteration 12500: Loss = -12349.8154296875
Iteration 12600: Loss = -12349.8154296875
Iteration 12700: Loss = -12349.8115234375
Iteration 12800: Loss = -12349.8095703125
Iteration 12900: Loss = -12349.80859375
Iteration 13000: Loss = -12349.806640625
Iteration 13100: Loss = -12349.806640625
Iteration 13200: Loss = -12349.8046875
Iteration 13300: Loss = -12349.8037109375
Iteration 13400: Loss = -12349.8037109375
Iteration 13500: Loss = -12349.8017578125
Iteration 13600: Loss = -12349.802734375
1
Iteration 13700: Loss = -12349.80078125
Iteration 13800: Loss = -12349.80078125
Iteration 13900: Loss = -12349.7998046875
Iteration 14000: Loss = -12349.7998046875
Iteration 14100: Loss = -12349.798828125
Iteration 14200: Loss = -12349.7978515625
Iteration 14300: Loss = -12349.798828125
1
Iteration 14400: Loss = -12349.798828125
2
Iteration 14500: Loss = -12349.798828125
3
Iteration 14600: Loss = -12349.7978515625
Iteration 14700: Loss = -12349.796875
Iteration 14800: Loss = -12349.798828125
1
Iteration 14900: Loss = -12349.7958984375
Iteration 15000: Loss = -12349.80078125
1
Iteration 15100: Loss = -12349.796875
2
Iteration 15200: Loss = -12349.796875
3
Iteration 15300: Loss = -12349.796875
4
Iteration 15400: Loss = -12349.7958984375
Iteration 15500: Loss = -12349.796875
1
Iteration 15600: Loss = -12349.796875
2
Iteration 15700: Loss = -12349.7958984375
Iteration 15800: Loss = -12349.7958984375
Iteration 15900: Loss = -12349.7939453125
Iteration 16000: Loss = -12349.7958984375
1
Iteration 16100: Loss = -12349.7939453125
Iteration 16200: Loss = -12349.796875
1
Iteration 16300: Loss = -12349.7939453125
Iteration 16400: Loss = -12349.7958984375
1
Iteration 16500: Loss = -12349.7939453125
Iteration 16600: Loss = -12349.7880859375
Iteration 16700: Loss = -12348.9443359375
Iteration 16800: Loss = -12348.91015625
Iteration 16900: Loss = -12348.900390625
Iteration 17000: Loss = -12348.5439453125
Iteration 17100: Loss = -12348.3115234375
Iteration 17200: Loss = -12348.3017578125
Iteration 17300: Loss = -12348.2998046875
Iteration 17400: Loss = -12348.296875
Iteration 17500: Loss = -12348.296875
Iteration 17600: Loss = -12348.2939453125
Iteration 17700: Loss = -12348.298828125
1
Iteration 17800: Loss = -12348.294921875
2
Iteration 17900: Loss = -12348.294921875
3
Iteration 18000: Loss = -12348.2939453125
Iteration 18100: Loss = -12348.29296875
Iteration 18200: Loss = -12348.2939453125
1
Iteration 18300: Loss = -12348.2919921875
Iteration 18400: Loss = -12348.2939453125
1
Iteration 18500: Loss = -12348.2919921875
Iteration 18600: Loss = -12348.2919921875
Iteration 18700: Loss = -12348.29296875
1
Iteration 18800: Loss = -12348.29296875
2
Iteration 18900: Loss = -12348.2919921875
Iteration 19000: Loss = -12348.2919921875
Iteration 19100: Loss = -12348.2919921875
Iteration 19200: Loss = -12348.2919921875
Iteration 19300: Loss = -12348.291015625
Iteration 19400: Loss = -12348.2919921875
1
Iteration 19500: Loss = -12348.291015625
Iteration 19600: Loss = -12348.2900390625
Iteration 19700: Loss = -12348.2919921875
1
Iteration 19800: Loss = -12348.291015625
2
Iteration 19900: Loss = -12348.291015625
3
Iteration 20000: Loss = -12348.2919921875
4
Iteration 20100: Loss = -12348.2900390625
Iteration 20200: Loss = -12348.291015625
1
Iteration 20300: Loss = -12348.2919921875
2
Iteration 20400: Loss = -12348.2919921875
3
Iteration 20500: Loss = -12348.2939453125
4
Iteration 20600: Loss = -12348.291015625
5
Iteration 20700: Loss = -12348.291015625
6
Iteration 20800: Loss = -12348.2900390625
Iteration 20900: Loss = -12348.2939453125
1
Iteration 21000: Loss = -12348.2919921875
2
Iteration 21100: Loss = -12348.291015625
3
Iteration 21200: Loss = -12348.2919921875
4
Iteration 21300: Loss = -12348.2919921875
5
Iteration 21400: Loss = -12348.2919921875
6
Iteration 21500: Loss = -12348.2919921875
7
Iteration 21600: Loss = -12348.291015625
8
Iteration 21700: Loss = -12348.2919921875
9
Iteration 21800: Loss = -12348.2919921875
10
Iteration 21900: Loss = -12348.2919921875
11
Iteration 22000: Loss = -12348.291015625
12
Iteration 22100: Loss = -12348.291015625
13
Iteration 22200: Loss = -12348.2919921875
14
Iteration 22300: Loss = -12348.29296875
15
Stopping early at iteration 22300 due to no improvement.
pi: tensor([[1.0000e+00, 1.0133e-06],
        [1.0627e-04, 9.9989e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9814, 0.0186], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1995, 0.2546],
         [0.0515, 0.5617]],

        [[0.5236, 0.2292],
         [0.6459, 0.8987]],

        [[0.0253, 0.1925],
         [0.0216, 0.6581]],

        [[0.8523, 0.1337],
         [0.7677, 0.0575]],

        [[0.6347, 0.1094],
         [0.9661, 0.4235]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0059174737657959825
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
Global Adjusted Rand Index: 0.003277124183283542
Average Adjusted Rand Index: 0.0022648737139377545
[0.0, 0.003277124183283542] [0.0, 0.0022648737139377545] [12351.615234375, 12348.29296875]
-------------------------------------
This iteration is 71
True Objective function: Loss = -11911.11218029651
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -45992.1328125
Iteration 100: Loss = -25827.451171875
Iteration 200: Loss = -15158.4794921875
Iteration 300: Loss = -13381.6875
Iteration 400: Loss = -12923.5146484375
Iteration 500: Loss = -12738.7314453125
Iteration 600: Loss = -12632.2265625
Iteration 700: Loss = -12562.28515625
Iteration 800: Loss = -12527.5703125
Iteration 900: Loss = -12497.083984375
Iteration 1000: Loss = -12477.2109375
Iteration 1100: Loss = -12465.3134765625
Iteration 1200: Loss = -12456.8291015625
Iteration 1300: Loss = -12447.66015625
Iteration 1400: Loss = -12434.4501953125
Iteration 1500: Loss = -12423.9833984375
Iteration 1600: Loss = -12419.943359375
Iteration 1700: Loss = -12416.689453125
Iteration 1800: Loss = -12413.974609375
Iteration 1900: Loss = -12411.66796875
Iteration 2000: Loss = -12409.6845703125
Iteration 2100: Loss = -12407.9599609375
Iteration 2200: Loss = -12406.451171875
Iteration 2300: Loss = -12405.1181640625
Iteration 2400: Loss = -12403.921875
Iteration 2500: Loss = -12398.0615234375
Iteration 2600: Loss = -12396.41015625
Iteration 2700: Loss = -12395.3837890625
Iteration 2800: Loss = -12394.5224609375
Iteration 2900: Loss = -12393.765625
Iteration 3000: Loss = -12393.09375
Iteration 3100: Loss = -12392.48828125
Iteration 3200: Loss = -12391.94140625
Iteration 3300: Loss = -12391.4443359375
Iteration 3400: Loss = -12390.9892578125
Iteration 3500: Loss = -12390.5732421875
Iteration 3600: Loss = -12390.19140625
Iteration 3700: Loss = -12389.841796875
Iteration 3800: Loss = -12389.5146484375
Iteration 3900: Loss = -12389.2080078125
Iteration 4000: Loss = -12388.931640625
Iteration 4100: Loss = -12388.6748046875
Iteration 4200: Loss = -12388.4384765625
Iteration 4300: Loss = -12388.2197265625
Iteration 4400: Loss = -12388.0146484375
Iteration 4500: Loss = -12387.8232421875
Iteration 4600: Loss = -12387.646484375
Iteration 4700: Loss = -12387.4833984375
Iteration 4800: Loss = -12387.3291015625
Iteration 4900: Loss = -12387.185546875
Iteration 5000: Loss = -12387.052734375
Iteration 5100: Loss = -12386.9287109375
Iteration 5200: Loss = -12386.8134765625
Iteration 5300: Loss = -12386.705078125
Iteration 5400: Loss = -12386.60546875
Iteration 5500: Loss = -12386.51171875
Iteration 5600: Loss = -12386.4248046875
Iteration 5700: Loss = -12386.341796875
Iteration 5800: Loss = -12386.2646484375
Iteration 5900: Loss = -12386.193359375
Iteration 6000: Loss = -12386.1259765625
Iteration 6100: Loss = -12386.0625
Iteration 6200: Loss = -12386.001953125
Iteration 6300: Loss = -12385.9453125
Iteration 6400: Loss = -12385.892578125
Iteration 6500: Loss = -12385.84375
Iteration 6600: Loss = -12385.796875
Iteration 6700: Loss = -12385.7509765625
Iteration 6800: Loss = -12385.7099609375
Iteration 6900: Loss = -12385.6708984375
Iteration 7000: Loss = -12385.6337890625
Iteration 7100: Loss = -12385.59765625
Iteration 7200: Loss = -12385.5654296875
Iteration 7300: Loss = -12385.5322265625
Iteration 7400: Loss = -12385.501953125
Iteration 7500: Loss = -12385.474609375
Iteration 7600: Loss = -12385.447265625
Iteration 7700: Loss = -12385.4208984375
Iteration 7800: Loss = -12385.396484375
Iteration 7900: Loss = -12385.373046875
Iteration 8000: Loss = -12385.3505859375
Iteration 8100: Loss = -12385.330078125
Iteration 8200: Loss = -12385.310546875
Iteration 8300: Loss = -12385.291015625
Iteration 8400: Loss = -12385.2724609375
Iteration 8500: Loss = -12385.25390625
Iteration 8600: Loss = -12385.2373046875
Iteration 8700: Loss = -12385.216796875
Iteration 8800: Loss = -12385.19921875
Iteration 8900: Loss = -12385.1826171875
Iteration 9000: Loss = -12385.16796875
Iteration 9100: Loss = -12385.1533203125
Iteration 9200: Loss = -12385.140625
Iteration 9300: Loss = -12385.12890625
Iteration 9400: Loss = -12385.1162109375
Iteration 9500: Loss = -12385.1044921875
Iteration 9600: Loss = -12385.0966796875
Iteration 9700: Loss = -12385.0849609375
Iteration 9800: Loss = -12385.076171875
Iteration 9900: Loss = -12385.0673828125
Iteration 10000: Loss = -12385.05859375
Iteration 10100: Loss = -12385.05078125
Iteration 10200: Loss = -12385.044921875
Iteration 10300: Loss = -12385.0390625
Iteration 10400: Loss = -12385.0341796875
Iteration 10500: Loss = -12385.029296875
Iteration 10600: Loss = -12385.0234375
Iteration 10700: Loss = -12385.0185546875
Iteration 10800: Loss = -12385.0146484375
Iteration 10900: Loss = -12385.0087890625
Iteration 11000: Loss = -12385.005859375
Iteration 11100: Loss = -12385.0029296875
Iteration 11200: Loss = -12384.9990234375
Iteration 11300: Loss = -12384.99609375
Iteration 11400: Loss = -12384.9921875
Iteration 11500: Loss = -12384.990234375
Iteration 11600: Loss = -12384.9873046875
Iteration 11700: Loss = -12384.9873046875
Iteration 11800: Loss = -12384.9833984375
Iteration 11900: Loss = -12384.9814453125
Iteration 12000: Loss = -12384.978515625
Iteration 12100: Loss = -12384.9775390625
Iteration 12200: Loss = -12384.9775390625
Iteration 12300: Loss = -12384.9765625
Iteration 12400: Loss = -12384.9736328125
Iteration 12500: Loss = -12384.96875
Iteration 12600: Loss = -12384.96875
Iteration 12700: Loss = -12384.9677734375
Iteration 12800: Loss = -12384.9658203125
Iteration 12900: Loss = -12384.9638671875
Iteration 13000: Loss = -12384.9609375
Iteration 13100: Loss = -12384.9609375
Iteration 13200: Loss = -12384.962890625
1
Iteration 13300: Loss = -12384.9580078125
Iteration 13400: Loss = -12384.9560546875
Iteration 13500: Loss = -12384.955078125
Iteration 13600: Loss = -12384.9541015625
Iteration 13700: Loss = -12384.9462890625
Iteration 13800: Loss = -12384.595703125
Iteration 13900: Loss = -12384.177734375
Iteration 14000: Loss = -12384.05078125
Iteration 14100: Loss = -12383.9453125
Iteration 14200: Loss = -12383.828125
Iteration 14300: Loss = -12383.66796875
Iteration 14400: Loss = -12383.427734375
Iteration 14500: Loss = -12383.0517578125
Iteration 14600: Loss = -12382.5517578125
Iteration 14700: Loss = -12382.1201171875
Iteration 14800: Loss = -12381.9326171875
Iteration 14900: Loss = -12381.81640625
Iteration 15000: Loss = -12381.724609375
Iteration 15100: Loss = -12381.6474609375
Iteration 15200: Loss = -12381.5966796875
Iteration 15300: Loss = -12381.5673828125
Iteration 15400: Loss = -12381.5458984375
Iteration 15500: Loss = -12381.5341796875
Iteration 15600: Loss = -12381.51953125
Iteration 15700: Loss = -12381.517578125
Iteration 15800: Loss = -12381.515625
Iteration 15900: Loss = -12381.513671875
Iteration 16000: Loss = -12381.513671875
Iteration 16100: Loss = -12381.5126953125
Iteration 16200: Loss = -12381.4892578125
Iteration 16300: Loss = -12381.484375
Iteration 16400: Loss = -12381.484375
Iteration 16500: Loss = -12381.4814453125
Iteration 16600: Loss = -12381.4814453125
Iteration 16700: Loss = -12381.4814453125
Iteration 16800: Loss = -12381.4814453125
Iteration 16900: Loss = -12381.4814453125
Iteration 17000: Loss = -12381.4814453125
Iteration 17100: Loss = -12381.48046875
Iteration 17200: Loss = -12381.48046875
Iteration 17300: Loss = -12381.48046875
Iteration 17400: Loss = -12381.48046875
Iteration 17500: Loss = -12381.48046875
Iteration 17600: Loss = -12381.478515625
Iteration 17700: Loss = -12381.48046875
1
Iteration 17800: Loss = -12381.48046875
2
Iteration 17900: Loss = -12381.4794921875
3
Iteration 18000: Loss = -12381.4794921875
4
Iteration 18100: Loss = -12381.4775390625
Iteration 18200: Loss = -12381.4794921875
1
Iteration 18300: Loss = -12381.4794921875
2
Iteration 18400: Loss = -12381.478515625
3
Iteration 18500: Loss = -12381.4775390625
Iteration 18600: Loss = -12381.48046875
1
Iteration 18700: Loss = -12381.478515625
2
Iteration 18800: Loss = -12381.478515625
3
Iteration 18900: Loss = -12381.4775390625
Iteration 19000: Loss = -12381.4794921875
1
Iteration 19100: Loss = -12381.478515625
2
Iteration 19200: Loss = -12381.478515625
3
Iteration 19300: Loss = -12381.478515625
4
Iteration 19400: Loss = -12381.4794921875
5
Iteration 19500: Loss = -12381.478515625
6
Iteration 19600: Loss = -12381.4794921875
7
Iteration 19700: Loss = -12381.48046875
8
Iteration 19800: Loss = -12381.478515625
9
Iteration 19900: Loss = -12381.4765625
Iteration 20000: Loss = -12381.478515625
1
Iteration 20100: Loss = -12381.4775390625
2
Iteration 20200: Loss = -12381.4775390625
3
Iteration 20300: Loss = -12381.478515625
4
Iteration 20400: Loss = -12381.478515625
5
Iteration 20500: Loss = -12381.4775390625
6
Iteration 20600: Loss = -12381.4775390625
7
Iteration 20700: Loss = -12381.4775390625
8
Iteration 20800: Loss = -12381.4775390625
9
Iteration 20900: Loss = -12381.4765625
Iteration 21000: Loss = -12381.4775390625
1
Iteration 21100: Loss = -12381.4794921875
2
Iteration 21200: Loss = -12381.4775390625
3
Iteration 21300: Loss = -12381.478515625
4
Iteration 21400: Loss = -12381.4775390625
5
Iteration 21500: Loss = -12381.4775390625
6
Iteration 21600: Loss = -12381.478515625
7
Iteration 21700: Loss = -12381.4775390625
8
Iteration 21800: Loss = -12381.478515625
9
Iteration 21900: Loss = -12381.4765625
Iteration 22000: Loss = -12381.4794921875
1
Iteration 22100: Loss = -12381.4765625
Iteration 22200: Loss = -12381.4775390625
1
Iteration 22300: Loss = -12381.4765625
Iteration 22400: Loss = -12381.4775390625
1
Iteration 22500: Loss = -12381.478515625
2
Iteration 22600: Loss = -12381.4775390625
3
Iteration 22700: Loss = -12381.4775390625
4
Iteration 22800: Loss = -12381.4765625
Iteration 22900: Loss = -12381.4560546875
Iteration 23000: Loss = -12381.4560546875
Iteration 23100: Loss = -12381.4541015625
Iteration 23200: Loss = -12381.455078125
1
Iteration 23300: Loss = -12381.4541015625
Iteration 23400: Loss = -12381.4541015625
Iteration 23500: Loss = -12381.4560546875
1
Iteration 23600: Loss = -12381.455078125
2
Iteration 23700: Loss = -12381.45703125
3
Iteration 23800: Loss = -12381.455078125
4
Iteration 23900: Loss = -12381.455078125
5
Iteration 24000: Loss = -12381.4541015625
Iteration 24100: Loss = -12381.4560546875
1
Iteration 24200: Loss = -12381.4541015625
Iteration 24300: Loss = -12381.455078125
1
Iteration 24400: Loss = -12381.455078125
2
Iteration 24500: Loss = -12381.4541015625
Iteration 24600: Loss = -12381.4541015625
Iteration 24700: Loss = -12381.4560546875
1
Iteration 24800: Loss = -12381.455078125
2
Iteration 24900: Loss = -12381.4541015625
Iteration 25000: Loss = -12381.4541015625
Iteration 25100: Loss = -12381.455078125
1
Iteration 25200: Loss = -12381.4541015625
Iteration 25300: Loss = -12381.4560546875
1
Iteration 25400: Loss = -12381.4560546875
2
Iteration 25500: Loss = -12381.455078125
3
Iteration 25600: Loss = -12381.455078125
4
Iteration 25700: Loss = -12381.4541015625
Iteration 25800: Loss = -12381.4560546875
1
Iteration 25900: Loss = -12381.455078125
2
Iteration 26000: Loss = -12381.4560546875
3
Iteration 26100: Loss = -12381.4541015625
Iteration 26200: Loss = -12381.455078125
1
Iteration 26300: Loss = -12381.4560546875
2
Iteration 26400: Loss = -12381.455078125
3
Iteration 26500: Loss = -12381.4541015625
Iteration 26600: Loss = -12381.4580078125
1
Iteration 26700: Loss = -12381.4541015625
Iteration 26800: Loss = -12381.4541015625
Iteration 26900: Loss = -12381.4560546875
1
Iteration 27000: Loss = -12381.4541015625
Iteration 27100: Loss = -12381.4541015625
Iteration 27200: Loss = -12381.455078125
1
Iteration 27300: Loss = -12381.458984375
2
Iteration 27400: Loss = -12381.455078125
3
Iteration 27500: Loss = -12381.455078125
4
Iteration 27600: Loss = -12381.4560546875
5
Iteration 27700: Loss = -12381.455078125
6
Iteration 27800: Loss = -12381.4541015625
Iteration 27900: Loss = -12381.4560546875
1
Iteration 28000: Loss = -12381.4541015625
Iteration 28100: Loss = -12381.4560546875
1
Iteration 28200: Loss = -12381.4560546875
2
Iteration 28300: Loss = -12381.455078125
3
Iteration 28400: Loss = -12381.4560546875
4
Iteration 28500: Loss = -12381.4541015625
Iteration 28600: Loss = -12381.4541015625
Iteration 28700: Loss = -12381.455078125
1
Iteration 28800: Loss = -12381.4560546875
2
Iteration 28900: Loss = -12381.4560546875
3
Iteration 29000: Loss = -12381.455078125
4
Iteration 29100: Loss = -12381.4541015625
Iteration 29200: Loss = -12381.4541015625
Iteration 29300: Loss = -12381.4560546875
1
Iteration 29400: Loss = -12381.455078125
2
Iteration 29500: Loss = -12381.4541015625
Iteration 29600: Loss = -12381.455078125
1
Iteration 29700: Loss = -12381.4541015625
Iteration 29800: Loss = -12381.455078125
1
Iteration 29900: Loss = -12381.4560546875
2
pi: tensor([[2.4424e-06, 1.0000e+00],
        [2.8542e-02, 9.7146e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.7238e-04, 9.9903e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1525, 0.1946],
         [0.9599, 0.1995]],

        [[0.6070, 0.2114],
         [0.5993, 0.0132]],

        [[0.2420, 0.0884],
         [0.9879, 0.3041]],

        [[0.8458, 0.3051],
         [0.2956, 0.0717]],

        [[0.1859, 0.2081],
         [0.3154, 0.9532]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.005396094422863973
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00021038625119266626
Average Adjusted Rand Index: 0.00010427586858640197
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -48888.95703125
Iteration 100: Loss = -27579.9609375
Iteration 200: Loss = -15985.2177734375
Iteration 300: Loss = -13586.865234375
Iteration 400: Loss = -12983.9150390625
Iteration 500: Loss = -12754.81640625
Iteration 600: Loss = -12653.0791015625
Iteration 700: Loss = -12573.4755859375
Iteration 800: Loss = -12528.30078125
Iteration 900: Loss = -12490.0556640625
Iteration 1000: Loss = -12467.5205078125
Iteration 1100: Loss = -12453.513671875
Iteration 1200: Loss = -12444.166015625
Iteration 1300: Loss = -12437.375
Iteration 1400: Loss = -12432.0390625
Iteration 1500: Loss = -12427.6923828125
Iteration 1600: Loss = -12424.083984375
Iteration 1700: Loss = -12421.037109375
Iteration 1800: Loss = -12418.4375
Iteration 1900: Loss = -12416.2001953125
Iteration 2000: Loss = -12414.26171875
Iteration 2100: Loss = -12408.1220703125
Iteration 2200: Loss = -12406.1064453125
Iteration 2300: Loss = -12404.720703125
Iteration 2400: Loss = -12403.5390625
Iteration 2500: Loss = -12402.5048828125
Iteration 2600: Loss = -12401.59375
Iteration 2700: Loss = -12400.78125
Iteration 2800: Loss = -12400.05078125
Iteration 2900: Loss = -12399.396484375
Iteration 3000: Loss = -12398.802734375
Iteration 3100: Loss = -12398.263671875
Iteration 3200: Loss = -12397.7744140625
Iteration 3300: Loss = -12397.3251953125
Iteration 3400: Loss = -12396.9169921875
Iteration 3500: Loss = -12396.541015625
Iteration 3600: Loss = -12396.1923828125
Iteration 3700: Loss = -12395.8720703125
Iteration 3800: Loss = -12395.5751953125
Iteration 3900: Loss = -12395.302734375
Iteration 4000: Loss = -12395.0478515625
Iteration 4100: Loss = -12394.8125
Iteration 4200: Loss = -12394.5947265625
Iteration 4300: Loss = -12394.390625
Iteration 4400: Loss = -12394.197265625
Iteration 4500: Loss = -12394.021484375
Iteration 4600: Loss = -12393.857421875
Iteration 4700: Loss = -12393.7021484375
Iteration 4800: Loss = -12393.5556640625
Iteration 4900: Loss = -12393.419921875
Iteration 5000: Loss = -12393.2919921875
Iteration 5100: Loss = -12393.171875
Iteration 5200: Loss = -12393.0625
Iteration 5300: Loss = -12392.9560546875
Iteration 5400: Loss = -12392.8564453125
Iteration 5500: Loss = -12392.7646484375
Iteration 5600: Loss = -12392.677734375
Iteration 5700: Loss = -12392.5947265625
Iteration 5800: Loss = -12392.5166015625
Iteration 5900: Loss = -12392.4443359375
Iteration 6000: Loss = -12392.3759765625
Iteration 6100: Loss = -12392.3095703125
Iteration 6200: Loss = -12392.2431640625
Iteration 6300: Loss = -12392.1708984375
Iteration 6400: Loss = -12392.109375
Iteration 6500: Loss = -12391.1064453125
Iteration 6600: Loss = -12386.345703125
Iteration 6700: Loss = -12386.099609375
Iteration 6800: Loss = -12385.9638671875
Iteration 6900: Loss = -12385.869140625
Iteration 7000: Loss = -12385.794921875
Iteration 7100: Loss = -12385.7314453125
Iteration 7200: Loss = -12385.677734375
Iteration 7300: Loss = -12385.6279296875
Iteration 7400: Loss = -12385.583984375
Iteration 7500: Loss = -12385.54296875
Iteration 7600: Loss = -12385.509765625
Iteration 7700: Loss = -12385.474609375
Iteration 7800: Loss = -12385.4443359375
Iteration 7900: Loss = -12385.4150390625
Iteration 8000: Loss = -12385.3876953125
Iteration 8100: Loss = -12385.3642578125
Iteration 8200: Loss = -12385.3408203125
Iteration 8300: Loss = -12385.3193359375
Iteration 8400: Loss = -12385.296875
Iteration 8500: Loss = -12385.2783203125
Iteration 8600: Loss = -12385.26171875
Iteration 8700: Loss = -12385.244140625
Iteration 8800: Loss = -12385.2275390625
Iteration 8900: Loss = -12385.2119140625
Iteration 9000: Loss = -12385.19921875
Iteration 9100: Loss = -12385.185546875
Iteration 9200: Loss = -12385.173828125
Iteration 9300: Loss = -12385.162109375
Iteration 9400: Loss = -12385.15234375
Iteration 9500: Loss = -12385.1416015625
Iteration 9600: Loss = -12385.1298828125
Iteration 9700: Loss = -12385.1220703125
Iteration 9800: Loss = -12385.1123046875
Iteration 9900: Loss = -12385.1064453125
Iteration 10000: Loss = -12385.09765625
Iteration 10100: Loss = -12385.08984375
Iteration 10200: Loss = -12385.0830078125
Iteration 10300: Loss = -12385.076171875
Iteration 10400: Loss = -12385.0703125
Iteration 10500: Loss = -12385.0654296875
Iteration 10600: Loss = -12385.0595703125
Iteration 10700: Loss = -12385.0537109375
Iteration 10800: Loss = -12385.048828125
Iteration 10900: Loss = -12385.04296875
Iteration 11000: Loss = -12385.041015625
Iteration 11100: Loss = -12385.03515625
Iteration 11200: Loss = -12385.033203125
Iteration 11300: Loss = -12385.0263671875
Iteration 11400: Loss = -12385.0244140625
Iteration 11500: Loss = -12385.0205078125
Iteration 11600: Loss = -12385.0185546875
Iteration 11700: Loss = -12385.015625
Iteration 11800: Loss = -12385.0107421875
Iteration 11900: Loss = -12385.0087890625
Iteration 12000: Loss = -12385.0078125
Iteration 12100: Loss = -12385.0048828125
Iteration 12200: Loss = -12385.001953125
Iteration 12300: Loss = -12384.998046875
Iteration 12400: Loss = -12384.99609375
Iteration 12500: Loss = -12384.994140625
Iteration 12600: Loss = -12384.9921875
Iteration 12700: Loss = -12384.9892578125
Iteration 12800: Loss = -12384.9873046875
Iteration 12900: Loss = -12384.986328125
Iteration 13000: Loss = -12384.9833984375
Iteration 13100: Loss = -12384.9833984375
Iteration 13200: Loss = -12384.978515625
Iteration 13300: Loss = -12384.978515625
Iteration 13400: Loss = -12384.9775390625
Iteration 13500: Loss = -12384.9765625
Iteration 13600: Loss = -12384.9755859375
Iteration 13700: Loss = -12384.974609375
Iteration 13800: Loss = -12384.9736328125
Iteration 13900: Loss = -12384.970703125
Iteration 14000: Loss = -12384.974609375
1
Iteration 14100: Loss = -12384.970703125
Iteration 14200: Loss = -12384.96875
Iteration 14300: Loss = -12384.9697265625
1
Iteration 14400: Loss = -12384.9677734375
Iteration 14500: Loss = -12384.966796875
Iteration 14600: Loss = -12384.9677734375
1
Iteration 14700: Loss = -12384.966796875
Iteration 14800: Loss = -12384.9658203125
Iteration 14900: Loss = -12384.96484375
Iteration 15000: Loss = -12384.9638671875
Iteration 15100: Loss = -12384.9638671875
Iteration 15200: Loss = -12384.9638671875
Iteration 15300: Loss = -12384.9638671875
Iteration 15400: Loss = -12384.96484375
1
Iteration 15500: Loss = -12384.9619140625
Iteration 15600: Loss = -12384.962890625
1
Iteration 15700: Loss = -12384.9619140625
Iteration 15800: Loss = -12384.9619140625
Iteration 15900: Loss = -12384.9609375
Iteration 16000: Loss = -12384.9619140625
1
Iteration 16100: Loss = -12384.9609375
Iteration 16200: Loss = -12384.9609375
Iteration 16300: Loss = -12384.9609375
Iteration 16400: Loss = -12384.9609375
Iteration 16500: Loss = -12384.9599609375
Iteration 16600: Loss = -12384.9609375
1
Iteration 16700: Loss = -12384.9599609375
Iteration 16800: Loss = -12384.9599609375
Iteration 16900: Loss = -12384.9599609375
Iteration 17000: Loss = -12384.9599609375
Iteration 17100: Loss = -12384.953125
Iteration 17200: Loss = -12384.509765625
Iteration 17300: Loss = -12382.419921875
Iteration 17400: Loss = -12382.203125
Iteration 17500: Loss = -12382.169921875
Iteration 17600: Loss = -12382.1572265625
Iteration 17700: Loss = -12382.146484375
Iteration 17800: Loss = -12382.1396484375
Iteration 17900: Loss = -12382.1376953125
Iteration 18000: Loss = -12382.1337890625
Iteration 18100: Loss = -12382.12890625
Iteration 18200: Loss = -12382.12890625
Iteration 18300: Loss = -12382.126953125
Iteration 18400: Loss = -12382.125
Iteration 18500: Loss = -12382.1240234375
Iteration 18600: Loss = -12382.123046875
Iteration 18700: Loss = -12382.123046875
Iteration 18800: Loss = -12382.12109375
Iteration 18900: Loss = -12382.12109375
Iteration 19000: Loss = -12382.1201171875
Iteration 19100: Loss = -12382.119140625
Iteration 19200: Loss = -12382.1181640625
Iteration 19300: Loss = -12382.1171875
Iteration 19400: Loss = -12382.1171875
Iteration 19500: Loss = -12382.1171875
Iteration 19600: Loss = -12382.1171875
Iteration 19700: Loss = -12382.1162109375
Iteration 19800: Loss = -12382.1171875
1
Iteration 19900: Loss = -12382.119140625
2
Iteration 20000: Loss = -12382.115234375
Iteration 20100: Loss = -12382.115234375
Iteration 20200: Loss = -12382.1142578125
Iteration 20300: Loss = -12382.1142578125
Iteration 20400: Loss = -12382.1142578125
Iteration 20500: Loss = -12382.1142578125
Iteration 20600: Loss = -12382.11328125
Iteration 20700: Loss = -12382.1142578125
1
Iteration 20800: Loss = -12382.115234375
2
Iteration 20900: Loss = -12382.11328125
Iteration 21000: Loss = -12382.11328125
Iteration 21100: Loss = -12382.115234375
1
Iteration 21200: Loss = -12382.11328125
Iteration 21300: Loss = -12382.11328125
Iteration 21400: Loss = -12382.11328125
Iteration 21500: Loss = -12382.1142578125
1
Iteration 21600: Loss = -12382.1123046875
Iteration 21700: Loss = -12382.11328125
1
Iteration 21800: Loss = -12382.11328125
2
Iteration 21900: Loss = -12382.11328125
3
Iteration 22000: Loss = -12382.1171875
4
Iteration 22100: Loss = -12382.111328125
Iteration 22200: Loss = -12379.7900390625
Iteration 22300: Loss = -12379.6572265625
Iteration 22400: Loss = -12379.630859375
Iteration 22500: Loss = -12379.6201171875
Iteration 22600: Loss = -12379.6142578125
Iteration 22700: Loss = -12379.6083984375
Iteration 22800: Loss = -12379.60546875
Iteration 22900: Loss = -12379.6015625
Iteration 23000: Loss = -12379.6005859375
Iteration 23100: Loss = -12379.599609375
Iteration 23200: Loss = -12379.599609375
Iteration 23300: Loss = -12379.5986328125
Iteration 23400: Loss = -12379.59765625
Iteration 23500: Loss = -12379.5966796875
Iteration 23600: Loss = -12379.5947265625
Iteration 23700: Loss = -12379.595703125
1
Iteration 23800: Loss = -12379.5947265625
Iteration 23900: Loss = -12379.5927734375
Iteration 24000: Loss = -12379.5927734375
Iteration 24100: Loss = -12379.59375
1
Iteration 24200: Loss = -12379.591796875
Iteration 24300: Loss = -12379.59375
1
Iteration 24400: Loss = -12379.59375
2
Iteration 24500: Loss = -12379.591796875
Iteration 24600: Loss = -12379.591796875
Iteration 24700: Loss = -12379.58984375
Iteration 24800: Loss = -12379.591796875
1
Iteration 24900: Loss = -12379.5927734375
2
Iteration 25000: Loss = -12379.5927734375
3
Iteration 25100: Loss = -12379.5927734375
4
Iteration 25200: Loss = -12379.5908203125
5
Iteration 25300: Loss = -12379.5908203125
6
Iteration 25400: Loss = -12379.591796875
7
Iteration 25500: Loss = -12379.5908203125
8
Iteration 25600: Loss = -12379.5908203125
9
Iteration 25700: Loss = -12379.587890625
Iteration 25800: Loss = -12379.537109375
Iteration 25900: Loss = -12379.5361328125
Iteration 26000: Loss = -12379.537109375
1
Iteration 26100: Loss = -12379.529296875
Iteration 26200: Loss = -12379.529296875
Iteration 26300: Loss = -12379.53125
1
Iteration 26400: Loss = -12379.5234375
Iteration 26500: Loss = -12379.505859375
Iteration 26600: Loss = -12379.505859375
Iteration 26700: Loss = -12379.505859375
Iteration 26800: Loss = -12379.505859375
Iteration 26900: Loss = -12379.505859375
Iteration 27000: Loss = -12379.505859375
Iteration 27100: Loss = -12379.505859375
Iteration 27200: Loss = -12379.5048828125
Iteration 27300: Loss = -12379.5048828125
Iteration 27400: Loss = -12379.509765625
1
Iteration 27500: Loss = -12379.505859375
2
Iteration 27600: Loss = -12379.505859375
3
Iteration 27700: Loss = -12379.50390625
Iteration 27800: Loss = -12379.50390625
Iteration 27900: Loss = -12379.5048828125
1
Iteration 28000: Loss = -12379.50390625
Iteration 28100: Loss = -12379.50390625
Iteration 28200: Loss = -12379.50390625
Iteration 28300: Loss = -12379.50390625
Iteration 28400: Loss = -12379.505859375
1
Iteration 28500: Loss = -12379.5029296875
Iteration 28600: Loss = -12379.4931640625
Iteration 28700: Loss = -12379.494140625
1
Iteration 28800: Loss = -12379.494140625
2
Iteration 28900: Loss = -12379.494140625
3
Iteration 29000: Loss = -12379.4931640625
Iteration 29100: Loss = -12379.494140625
1
Iteration 29200: Loss = -12379.494140625
2
Iteration 29300: Loss = -12379.494140625
3
Iteration 29400: Loss = -12379.4931640625
Iteration 29500: Loss = -12379.4931640625
Iteration 29600: Loss = -12379.4951171875
1
Iteration 29700: Loss = -12379.4931640625
Iteration 29800: Loss = -12379.4931640625
Iteration 29900: Loss = -12379.4931640625
pi: tensor([[9.9993e-01, 7.3097e-05],
        [1.0665e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0208, 0.9792], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[2.4179e-04, 1.8884e-01],
         [3.0656e-02, 2.0167e-01]],

        [[5.0024e-01, 1.9793e-01],
         [7.6333e-01, 2.1284e-02]],

        [[8.2505e-01, 7.9921e-02],
         [6.1034e-02, 8.2297e-01]],

        [[6.4527e-01, 1.4550e-01],
         [9.3762e-02, 9.3443e-02]],

        [[1.6388e-02, 1.9919e-01],
         [9.7801e-01, 7.8576e-01]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 40
Adjusted Rand Index: 0.016025857647765086
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
Global Adjusted Rand Index: 0.006529181242556134
Average Adjusted Rand Index: 0.007722917069641755
[0.00021038625119266626, 0.006529181242556134] [0.00010427586858640197, 0.007722917069641755] [12381.4541015625, 12379.4931640625]
-------------------------------------
This iteration is 72
True Objective function: Loss = -11912.270096087863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22575.26953125
Iteration 100: Loss = -15435.9580078125
Iteration 200: Loss = -12972.0126953125
Iteration 300: Loss = -12572.9375
Iteration 400: Loss = -12487.390625
Iteration 500: Loss = -12449.8525390625
Iteration 600: Loss = -12426.6748046875
Iteration 700: Loss = -12408.0634765625
Iteration 800: Loss = -12393.279296875
Iteration 900: Loss = -12382.38671875
Iteration 1000: Loss = -12370.912109375
Iteration 1100: Loss = -12365.44140625
Iteration 1200: Loss = -12361.689453125
Iteration 1300: Loss = -12358.5771484375
Iteration 1400: Loss = -12356.150390625
Iteration 1500: Loss = -12354.2392578125
Iteration 1600: Loss = -12352.861328125
Iteration 1700: Loss = -12351.841796875
Iteration 1800: Loss = -12351.0546875
Iteration 1900: Loss = -12350.4287109375
Iteration 2000: Loss = -12349.9189453125
Iteration 2100: Loss = -12349.5
Iteration 2200: Loss = -12349.1474609375
Iteration 2300: Loss = -12348.8466796875
Iteration 2400: Loss = -12348.591796875
Iteration 2500: Loss = -12348.3642578125
Iteration 2600: Loss = -12348.16796875
Iteration 2700: Loss = -12347.9970703125
Iteration 2800: Loss = -12347.84375
Iteration 2900: Loss = -12347.7080078125
Iteration 3000: Loss = -12347.5869140625
Iteration 3100: Loss = -12347.4765625
Iteration 3200: Loss = -12347.3759765625
Iteration 3300: Loss = -12347.28515625
Iteration 3400: Loss = -12347.201171875
Iteration 3500: Loss = -12347.1259765625
Iteration 3600: Loss = -12347.0576171875
Iteration 3700: Loss = -12346.994140625
Iteration 3800: Loss = -12346.93359375
Iteration 3900: Loss = -12346.87890625
Iteration 4000: Loss = -12346.8271484375
Iteration 4100: Loss = -12346.779296875
Iteration 4200: Loss = -12346.734375
Iteration 4300: Loss = -12346.6953125
Iteration 4400: Loss = -12346.65625
Iteration 4500: Loss = -12346.6201171875
Iteration 4600: Loss = -12346.5859375
Iteration 4700: Loss = -12346.552734375
Iteration 4800: Loss = -12346.5224609375
Iteration 4900: Loss = -12346.494140625
Iteration 5000: Loss = -12346.466796875
Iteration 5100: Loss = -12346.44140625
Iteration 5200: Loss = -12346.4169921875
Iteration 5300: Loss = -12346.3955078125
Iteration 5400: Loss = -12346.373046875
Iteration 5500: Loss = -12346.3525390625
Iteration 5600: Loss = -12346.3330078125
Iteration 5700: Loss = -12346.314453125
Iteration 5800: Loss = -12346.2958984375
Iteration 5900: Loss = -12346.2802734375
Iteration 6000: Loss = -12346.2666015625
Iteration 6100: Loss = -12346.25
Iteration 6200: Loss = -12346.236328125
Iteration 6300: Loss = -12346.220703125
Iteration 6400: Loss = -12346.2060546875
Iteration 6500: Loss = -12346.1943359375
Iteration 6600: Loss = -12346.1796875
Iteration 6700: Loss = -12346.169921875
Iteration 6800: Loss = -12346.158203125
Iteration 6900: Loss = -12346.1455078125
Iteration 7000: Loss = -12346.134765625
Iteration 7100: Loss = -12346.1240234375
Iteration 7200: Loss = -12346.1142578125
Iteration 7300: Loss = -12346.1044921875
Iteration 7400: Loss = -12346.0947265625
Iteration 7500: Loss = -12346.083984375
Iteration 7600: Loss = -12346.07421875
Iteration 7700: Loss = -12346.0634765625
Iteration 7800: Loss = -12346.0537109375
Iteration 7900: Loss = -12346.04296875
Iteration 8000: Loss = -12346.0322265625
Iteration 8100: Loss = -12346.021484375
Iteration 8200: Loss = -12346.013671875
Iteration 8300: Loss = -12346.0
Iteration 8400: Loss = -12345.9873046875
Iteration 8500: Loss = -12345.974609375
Iteration 8600: Loss = -12345.9609375
Iteration 8700: Loss = -12345.9453125
Iteration 8800: Loss = -12345.9287109375
Iteration 8900: Loss = -12345.9072265625
Iteration 9000: Loss = -12345.8828125
Iteration 9100: Loss = -12345.85546875
Iteration 9200: Loss = -12345.82421875
Iteration 9300: Loss = -12345.7880859375
Iteration 9400: Loss = -12345.7490234375
Iteration 9500: Loss = -12345.705078125
Iteration 9600: Loss = -12345.6630859375
Iteration 9700: Loss = -12345.61328125
Iteration 9800: Loss = -12345.568359375
Iteration 9900: Loss = -12345.521484375
Iteration 10000: Loss = -12345.4794921875
Iteration 10100: Loss = -12345.4365234375
Iteration 10200: Loss = -12345.396484375
Iteration 10300: Loss = -12345.357421875
Iteration 10400: Loss = -12345.318359375
Iteration 10500: Loss = -12345.2822265625
Iteration 10600: Loss = -12345.248046875
Iteration 10700: Loss = -12345.21484375
Iteration 10800: Loss = -12345.1826171875
Iteration 10900: Loss = -12345.1552734375
Iteration 11000: Loss = -12345.12890625
Iteration 11100: Loss = -12345.1064453125
Iteration 11200: Loss = -12345.0859375
Iteration 11300: Loss = -12345.06640625
Iteration 11400: Loss = -12345.046875
Iteration 11500: Loss = -12345.03125
Iteration 11600: Loss = -12345.0146484375
Iteration 11700: Loss = -12344.99609375
Iteration 11800: Loss = -12344.98046875
Iteration 11900: Loss = -12344.9677734375
Iteration 12000: Loss = -12344.9580078125
Iteration 12100: Loss = -12344.94921875
Iteration 12200: Loss = -12344.91796875
Iteration 12300: Loss = -12344.8515625
Iteration 12400: Loss = -12344.662109375
Iteration 12500: Loss = -12344.5576171875
Iteration 12600: Loss = -12344.541015625
Iteration 12700: Loss = -12344.5361328125
Iteration 12800: Loss = -12344.533203125
Iteration 12900: Loss = -12344.5322265625
Iteration 13000: Loss = -12344.5302734375
Iteration 13100: Loss = -12344.5302734375
Iteration 13200: Loss = -12344.529296875
Iteration 13300: Loss = -12344.5283203125
Iteration 13400: Loss = -12344.5263671875
Iteration 13500: Loss = -12344.52734375
1
Iteration 13600: Loss = -12344.525390625
Iteration 13700: Loss = -12344.5263671875
1
Iteration 13800: Loss = -12344.5263671875
2
Iteration 13900: Loss = -12344.525390625
Iteration 14000: Loss = -12344.5244140625
Iteration 14100: Loss = -12344.5234375
Iteration 14200: Loss = -12344.5234375
Iteration 14300: Loss = -12344.5234375
Iteration 14400: Loss = -12344.5244140625
1
Iteration 14500: Loss = -12344.5234375
Iteration 14600: Loss = -12344.5205078125
Iteration 14700: Loss = -12344.521484375
1
Iteration 14800: Loss = -12344.5224609375
2
Iteration 14900: Loss = -12344.521484375
3
Iteration 15000: Loss = -12344.5185546875
Iteration 15100: Loss = -12344.515625
Iteration 15200: Loss = -12344.51171875
Iteration 15300: Loss = -12344.51171875
Iteration 15400: Loss = -12344.5107421875
Iteration 15500: Loss = -12344.509765625
Iteration 15600: Loss = -12344.505859375
Iteration 15700: Loss = -12344.5048828125
Iteration 15800: Loss = -12344.5009765625
Iteration 15900: Loss = -12344.4990234375
Iteration 16000: Loss = -12344.49609375
Iteration 16100: Loss = -12344.494140625
Iteration 16200: Loss = -12344.4931640625
Iteration 16300: Loss = -12344.494140625
1
Iteration 16400: Loss = -12344.4921875
Iteration 16500: Loss = -12344.4921875
Iteration 16600: Loss = -12344.4921875
Iteration 16700: Loss = -12344.4921875
Iteration 16800: Loss = -12344.490234375
Iteration 16900: Loss = -12344.4921875
1
Iteration 17000: Loss = -12344.4921875
2
Iteration 17100: Loss = -12344.4912109375
3
Iteration 17200: Loss = -12344.4921875
4
Iteration 17300: Loss = -12344.4921875
5
Iteration 17400: Loss = -12344.4912109375
6
Iteration 17500: Loss = -12344.4921875
7
Iteration 17600: Loss = -12344.4912109375
8
Iteration 17700: Loss = -12344.490234375
Iteration 17800: Loss = -12344.4912109375
1
Iteration 17900: Loss = -12344.490234375
Iteration 18000: Loss = -12344.4921875
1
Iteration 18100: Loss = -12344.4912109375
2
Iteration 18200: Loss = -12344.490234375
Iteration 18300: Loss = -12344.4921875
1
Iteration 18400: Loss = -12344.4912109375
2
Iteration 18500: Loss = -12344.4912109375
3
Iteration 18600: Loss = -12344.490234375
Iteration 18700: Loss = -12344.4921875
1
Iteration 18800: Loss = -12344.490234375
Iteration 18900: Loss = -12344.4912109375
1
Iteration 19000: Loss = -12344.4912109375
2
Iteration 19100: Loss = -12344.490234375
Iteration 19200: Loss = -12344.4892578125
Iteration 19300: Loss = -12344.490234375
1
Iteration 19400: Loss = -12344.490234375
2
Iteration 19500: Loss = -12344.490234375
3
Iteration 19600: Loss = -12344.4912109375
4
Iteration 19700: Loss = -12344.490234375
5
Iteration 19800: Loss = -12344.490234375
6
Iteration 19900: Loss = -12344.4892578125
Iteration 20000: Loss = -12344.4892578125
Iteration 20100: Loss = -12344.490234375
1
Iteration 20200: Loss = -12344.490234375
2
Iteration 20300: Loss = -12344.4892578125
Iteration 20400: Loss = -12344.4931640625
1
Iteration 20500: Loss = -12344.490234375
2
Iteration 20600: Loss = -12344.4892578125
Iteration 20700: Loss = -12344.4912109375
1
Iteration 20800: Loss = -12344.490234375
2
Iteration 20900: Loss = -12344.490234375
3
Iteration 21000: Loss = -12344.4892578125
Iteration 21100: Loss = -12344.490234375
1
Iteration 21200: Loss = -12344.4892578125
Iteration 21300: Loss = -12344.490234375
1
Iteration 21400: Loss = -12344.490234375
2
Iteration 21500: Loss = -12344.490234375
3
Iteration 21600: Loss = -12344.490234375
4
Iteration 21700: Loss = -12344.4892578125
Iteration 21800: Loss = -12344.4892578125
Iteration 21900: Loss = -12344.4892578125
Iteration 22000: Loss = -12344.490234375
1
Iteration 22100: Loss = -12344.4892578125
Iteration 22200: Loss = -12344.4892578125
Iteration 22300: Loss = -12344.490234375
1
Iteration 22400: Loss = -12344.490234375
2
Iteration 22500: Loss = -12344.4892578125
Iteration 22600: Loss = -12344.490234375
1
Iteration 22700: Loss = -12344.4912109375
2
Iteration 22800: Loss = -12344.4912109375
3
Iteration 22900: Loss = -12344.4892578125
Iteration 23000: Loss = -12344.490234375
1
Iteration 23100: Loss = -12344.4892578125
Iteration 23200: Loss = -12344.4912109375
1
Iteration 23300: Loss = -12344.4912109375
2
Iteration 23400: Loss = -12344.4912109375
3
Iteration 23500: Loss = -12344.490234375
4
Iteration 23600: Loss = -12344.490234375
5
Iteration 23700: Loss = -12344.4892578125
Iteration 23800: Loss = -12344.4892578125
Iteration 23900: Loss = -12344.4912109375
1
Iteration 24000: Loss = -12344.4912109375
2
Iteration 24100: Loss = -12344.490234375
3
Iteration 24200: Loss = -12344.490234375
4
Iteration 24300: Loss = -12344.4912109375
5
Iteration 24400: Loss = -12344.4892578125
Iteration 24500: Loss = -12344.490234375
1
Iteration 24600: Loss = -12344.4912109375
2
Iteration 24700: Loss = -12344.490234375
3
Iteration 24800: Loss = -12344.4912109375
4
Iteration 24900: Loss = -12344.4912109375
5
Iteration 25000: Loss = -12344.4912109375
6
Iteration 25100: Loss = -12344.4892578125
Iteration 25200: Loss = -12344.490234375
1
Iteration 25300: Loss = -12344.490234375
2
Iteration 25400: Loss = -12344.4912109375
3
Iteration 25500: Loss = -12344.490234375
4
Iteration 25600: Loss = -12344.4931640625
5
Iteration 25700: Loss = -12344.490234375
6
Iteration 25800: Loss = -12344.4912109375
7
Iteration 25900: Loss = -12344.4921875
8
Iteration 26000: Loss = -12344.4892578125
Iteration 26100: Loss = -12344.4912109375
1
Iteration 26200: Loss = -12344.4912109375
2
Iteration 26300: Loss = -12344.4912109375
3
Iteration 26400: Loss = -12344.490234375
4
Iteration 26500: Loss = -12344.4912109375
5
Iteration 26600: Loss = -12344.4892578125
Iteration 26700: Loss = -12344.4912109375
1
Iteration 26800: Loss = -12344.490234375
2
Iteration 26900: Loss = -12344.4892578125
Iteration 27000: Loss = -12344.4912109375
1
Iteration 27100: Loss = -12344.490234375
2
Iteration 27200: Loss = -12344.490234375
3
Iteration 27300: Loss = -12344.4892578125
Iteration 27400: Loss = -12344.4912109375
1
Iteration 27500: Loss = -12344.490234375
2
Iteration 27600: Loss = -12344.4921875
3
Iteration 27700: Loss = -12344.4912109375
4
Iteration 27800: Loss = -12344.4912109375
5
Iteration 27900: Loss = -12344.4912109375
6
Iteration 28000: Loss = -12344.4912109375
7
Iteration 28100: Loss = -12344.4912109375
8
Iteration 28200: Loss = -12344.4912109375
9
Iteration 28300: Loss = -12344.490234375
10
Iteration 28400: Loss = -12344.4912109375
11
Iteration 28500: Loss = -12344.4912109375
12
Iteration 28600: Loss = -12344.4921875
13
Iteration 28700: Loss = -12344.4912109375
14
Iteration 28800: Loss = -12344.490234375
15
Stopping early at iteration 28800 due to no improvement.
pi: tensor([[0.9754, 0.0246],
        [0.9957, 0.0043]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.8803e-04, 9.9981e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2019, 0.1984],
         [0.9180, 0.1941]],

        [[0.9782, 0.1150],
         [0.4127, 0.2305]],

        [[0.9389, 0.1279],
         [0.9404, 0.0982]],

        [[0.0473, 0.1153],
         [0.9505, 0.1315]],

        [[0.2618, 0.2191],
         [0.8303, 0.1110]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0007416216469663104
Average Adjusted Rand Index: -0.0005547302265410098
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27087.865234375
Iteration 100: Loss = -17550.244140625
Iteration 200: Loss = -13514.8173828125
Iteration 300: Loss = -12797.7841796875
Iteration 400: Loss = -12632.70703125
Iteration 500: Loss = -12557.865234375
Iteration 600: Loss = -12516.3193359375
Iteration 700: Loss = -12488.80078125
Iteration 800: Loss = -12461.982421875
Iteration 900: Loss = -12446.5712890625
Iteration 1000: Loss = -12427.8857421875
Iteration 1100: Loss = -12417.1142578125
Iteration 1200: Loss = -12405.111328125
Iteration 1300: Loss = -12397.818359375
Iteration 1400: Loss = -12390.83203125
Iteration 1500: Loss = -12382.6376953125
Iteration 1600: Loss = -12378.5078125
Iteration 1700: Loss = -12375.8515625
Iteration 1800: Loss = -12373.5693359375
Iteration 1900: Loss = -12371.083984375
Iteration 2000: Loss = -12367.7431640625
Iteration 2100: Loss = -12365.046875
Iteration 2200: Loss = -12359.7001953125
Iteration 2300: Loss = -12358.6298828125
Iteration 2400: Loss = -12357.8701171875
Iteration 2500: Loss = -12357.2490234375
Iteration 2600: Loss = -12356.720703125
Iteration 2700: Loss = -12356.267578125
Iteration 2800: Loss = -12355.87109375
Iteration 2900: Loss = -12355.5234375
Iteration 3000: Loss = -12355.2119140625
Iteration 3100: Loss = -12354.927734375
Iteration 3200: Loss = -12354.66015625
Iteration 3300: Loss = -12354.2177734375
Iteration 3400: Loss = -12350.1259765625
Iteration 3500: Loss = -12349.580078125
Iteration 3600: Loss = -12349.255859375
Iteration 3700: Loss = -12349.001953125
Iteration 3800: Loss = -12348.7861328125
Iteration 3900: Loss = -12348.591796875
Iteration 4000: Loss = -12348.4208984375
Iteration 4100: Loss = -12348.2646484375
Iteration 4200: Loss = -12348.123046875
Iteration 4300: Loss = -12347.9931640625
Iteration 4400: Loss = -12347.8720703125
Iteration 4500: Loss = -12347.759765625
Iteration 4600: Loss = -12347.658203125
Iteration 4700: Loss = -12347.5625
Iteration 4800: Loss = -12347.4736328125
Iteration 4900: Loss = -12347.3896484375
Iteration 5000: Loss = -12347.314453125
Iteration 5100: Loss = -12347.2412109375
Iteration 5200: Loss = -12347.173828125
Iteration 5300: Loss = -12347.109375
Iteration 5400: Loss = -12347.0498046875
Iteration 5500: Loss = -12346.9951171875
Iteration 5600: Loss = -12346.939453125
Iteration 5700: Loss = -12346.8896484375
Iteration 5800: Loss = -12346.845703125
Iteration 5900: Loss = -12346.8017578125
Iteration 6000: Loss = -12346.7587890625
Iteration 6100: Loss = -12346.7216796875
Iteration 6200: Loss = -12346.6845703125
Iteration 6300: Loss = -12346.6494140625
Iteration 6400: Loss = -12346.615234375
Iteration 6500: Loss = -12346.5849609375
Iteration 6600: Loss = -12346.552734375
Iteration 6700: Loss = -12346.5244140625
Iteration 6800: Loss = -12346.4990234375
Iteration 6900: Loss = -12346.4755859375
Iteration 7000: Loss = -12346.4521484375
Iteration 7100: Loss = -12346.431640625
Iteration 7200: Loss = -12346.41015625
Iteration 7300: Loss = -12346.390625
Iteration 7400: Loss = -12346.373046875
Iteration 7500: Loss = -12346.35546875
Iteration 7600: Loss = -12346.337890625
Iteration 7700: Loss = -12346.322265625
Iteration 7800: Loss = -12346.306640625
Iteration 7900: Loss = -12346.294921875
Iteration 8000: Loss = -12346.28125
Iteration 8100: Loss = -12346.26953125
Iteration 8200: Loss = -12346.2568359375
Iteration 8300: Loss = -12346.24609375
Iteration 8400: Loss = -12346.236328125
Iteration 8500: Loss = -12346.2265625
Iteration 8600: Loss = -12346.21484375
Iteration 8700: Loss = -12346.2060546875
Iteration 8800: Loss = -12346.197265625
Iteration 8900: Loss = -12346.1884765625
Iteration 9000: Loss = -12346.181640625
Iteration 9100: Loss = -12346.173828125
Iteration 9200: Loss = -12346.1669921875
Iteration 9300: Loss = -12346.1611328125
Iteration 9400: Loss = -12346.1552734375
Iteration 9500: Loss = -12346.1474609375
Iteration 9600: Loss = -12346.142578125
Iteration 9700: Loss = -12346.138671875
Iteration 9800: Loss = -12346.1328125
Iteration 9900: Loss = -12346.1259765625
Iteration 10000: Loss = -12346.123046875
Iteration 10100: Loss = -12346.119140625
Iteration 10200: Loss = -12346.1142578125
Iteration 10300: Loss = -12346.1103515625
Iteration 10400: Loss = -12346.10546875
Iteration 10500: Loss = -12346.1025390625
Iteration 10600: Loss = -12346.099609375
Iteration 10700: Loss = -12346.0947265625
Iteration 10800: Loss = -12346.0927734375
Iteration 10900: Loss = -12346.08984375
Iteration 11000: Loss = -12346.0869140625
Iteration 11100: Loss = -12346.0849609375
Iteration 11200: Loss = -12346.08203125
Iteration 11300: Loss = -12346.080078125
Iteration 11400: Loss = -12346.0771484375
Iteration 11500: Loss = -12346.0732421875
Iteration 11600: Loss = -12346.072265625
Iteration 11700: Loss = -12346.0693359375
Iteration 11800: Loss = -12346.068359375
Iteration 11900: Loss = -12346.0654296875
Iteration 12000: Loss = -12346.0634765625
Iteration 12100: Loss = -12346.0625
Iteration 12200: Loss = -12346.0595703125
Iteration 12300: Loss = -12346.05859375
Iteration 12400: Loss = -12346.05859375
Iteration 12500: Loss = -12346.0546875
Iteration 12600: Loss = -12346.052734375
Iteration 12700: Loss = -12346.052734375
Iteration 12800: Loss = -12346.05078125
Iteration 12900: Loss = -12346.048828125
Iteration 13000: Loss = -12346.048828125
Iteration 13100: Loss = -12346.044921875
Iteration 13200: Loss = -12346.0419921875
Iteration 13300: Loss = -12346.041015625
Iteration 13400: Loss = -12346.0341796875
Iteration 13500: Loss = -12346.029296875
Iteration 13600: Loss = -12346.0205078125
Iteration 13700: Loss = -12346.0048828125
Iteration 13800: Loss = -12345.958984375
Iteration 13900: Loss = -12345.8037109375
Iteration 14000: Loss = -12345.2626953125
Iteration 14100: Loss = -12344.896484375
Iteration 14200: Loss = -12344.83984375
Iteration 14300: Loss = -12344.8203125
Iteration 14400: Loss = -12344.8115234375
Iteration 14500: Loss = -12344.8056640625
Iteration 14600: Loss = -12344.802734375
Iteration 14700: Loss = -12344.7998046875
Iteration 14800: Loss = -12344.7978515625
Iteration 14900: Loss = -12344.794921875
Iteration 15000: Loss = -12344.7939453125
Iteration 15100: Loss = -12344.7919921875
Iteration 15200: Loss = -12344.791015625
Iteration 15300: Loss = -12344.7919921875
1
Iteration 15400: Loss = -12344.7880859375
Iteration 15500: Loss = -12344.7890625
1
Iteration 15600: Loss = -12344.7880859375
Iteration 15700: Loss = -12344.7880859375
Iteration 15800: Loss = -12344.7861328125
Iteration 15900: Loss = -12344.7861328125
Iteration 16000: Loss = -12344.7861328125
Iteration 16100: Loss = -12344.7841796875
Iteration 16200: Loss = -12344.78515625
1
Iteration 16300: Loss = -12344.7841796875
Iteration 16400: Loss = -12344.78515625
1
Iteration 16500: Loss = -12344.7841796875
Iteration 16600: Loss = -12344.7822265625
Iteration 16700: Loss = -12344.7822265625
Iteration 16800: Loss = -12344.7822265625
Iteration 16900: Loss = -12344.78125
Iteration 17000: Loss = -12344.783203125
1
Iteration 17100: Loss = -12344.7822265625
2
Iteration 17200: Loss = -12344.7802734375
Iteration 17300: Loss = -12344.7744140625
Iteration 17400: Loss = -12344.7734375
Iteration 17500: Loss = -12344.7734375
Iteration 17600: Loss = -12344.771484375
Iteration 17700: Loss = -12344.7734375
1
Iteration 17800: Loss = -12344.7734375
2
Iteration 17900: Loss = -12344.7724609375
3
Iteration 18000: Loss = -12344.771484375
Iteration 18100: Loss = -12344.7724609375
1
Iteration 18200: Loss = -12344.7734375
2
Iteration 18300: Loss = -12344.7724609375
3
Iteration 18400: Loss = -12344.7724609375
4
Iteration 18500: Loss = -12344.771484375
Iteration 18600: Loss = -12344.7705078125
Iteration 18700: Loss = -12344.7724609375
1
Iteration 18800: Loss = -12344.771484375
2
Iteration 18900: Loss = -12344.771484375
3
Iteration 19000: Loss = -12344.7734375
4
Iteration 19100: Loss = -12344.771484375
5
Iteration 19200: Loss = -12344.771484375
6
Iteration 19300: Loss = -12344.7705078125
Iteration 19400: Loss = -12344.7705078125
Iteration 19500: Loss = -12344.771484375
1
Iteration 19600: Loss = -12344.7685546875
Iteration 19700: Loss = -12344.7666015625
Iteration 19800: Loss = -12344.765625
Iteration 19900: Loss = -12344.7666015625
1
Iteration 20000: Loss = -12344.7646484375
Iteration 20100: Loss = -12344.765625
1
Iteration 20200: Loss = -12344.7646484375
Iteration 20300: Loss = -12344.763671875
Iteration 20400: Loss = -12344.7626953125
Iteration 20500: Loss = -12344.75
Iteration 20600: Loss = -12344.74609375
Iteration 20700: Loss = -12344.7470703125
1
Iteration 20800: Loss = -12344.5732421875
Iteration 20900: Loss = -12344.5654296875
Iteration 21000: Loss = -12344.564453125
Iteration 21100: Loss = -12344.5634765625
Iteration 21200: Loss = -12344.5537109375
Iteration 21300: Loss = -12344.552734375
Iteration 21400: Loss = -12344.55078125
Iteration 21500: Loss = -12344.5537109375
1
Iteration 21600: Loss = -12344.5517578125
2
Iteration 21700: Loss = -12344.552734375
3
Iteration 21800: Loss = -12344.5517578125
4
Iteration 21900: Loss = -12344.5517578125
5
Iteration 22000: Loss = -12344.5498046875
Iteration 22100: Loss = -12344.5498046875
Iteration 22200: Loss = -12344.5498046875
Iteration 22300: Loss = -12344.55078125
1
Iteration 22400: Loss = -12344.548828125
Iteration 22500: Loss = -12344.548828125
Iteration 22600: Loss = -12344.548828125
Iteration 22700: Loss = -12344.548828125
Iteration 22800: Loss = -12344.5498046875
1
Iteration 22900: Loss = -12344.548828125
Iteration 23000: Loss = -12344.55078125
1
Iteration 23100: Loss = -12344.5478515625
Iteration 23200: Loss = -12344.5498046875
1
Iteration 23300: Loss = -12344.5498046875
2
Iteration 23400: Loss = -12344.5517578125
3
Iteration 23500: Loss = -12344.5498046875
4
Iteration 23600: Loss = -12344.548828125
5
Iteration 23700: Loss = -12344.548828125
6
Iteration 23800: Loss = -12344.5498046875
7
Iteration 23900: Loss = -12344.548828125
8
Iteration 24000: Loss = -12344.548828125
9
Iteration 24100: Loss = -12344.548828125
10
Iteration 24200: Loss = -12344.548828125
11
Iteration 24300: Loss = -12344.5478515625
Iteration 24400: Loss = -12344.548828125
1
Iteration 24500: Loss = -12344.548828125
2
Iteration 24600: Loss = -12344.5498046875
3
Iteration 24700: Loss = -12344.5478515625
Iteration 24800: Loss = -12344.548828125
1
Iteration 24900: Loss = -12344.5478515625
Iteration 25000: Loss = -12344.548828125
1
Iteration 25100: Loss = -12344.5498046875
2
Iteration 25200: Loss = -12344.5478515625
Iteration 25300: Loss = -12344.548828125
1
Iteration 25400: Loss = -12344.548828125
2
Iteration 25500: Loss = -12344.548828125
3
Iteration 25600: Loss = -12344.5478515625
Iteration 25700: Loss = -12344.548828125
1
Iteration 25800: Loss = -12344.548828125
2
Iteration 25900: Loss = -12344.5498046875
3
Iteration 26000: Loss = -12344.548828125
4
Iteration 26100: Loss = -12344.548828125
5
Iteration 26200: Loss = -12344.548828125
6
Iteration 26300: Loss = -12344.546875
Iteration 26400: Loss = -12344.548828125
1
Iteration 26500: Loss = -12344.546875
Iteration 26600: Loss = -12344.548828125
1
Iteration 26700: Loss = -12344.548828125
2
Iteration 26800: Loss = -12344.5478515625
3
Iteration 26900: Loss = -12344.5478515625
4
Iteration 27000: Loss = -12344.5478515625
5
Iteration 27100: Loss = -12344.548828125
6
Iteration 27200: Loss = -12344.548828125
7
Iteration 27300: Loss = -12344.5478515625
8
Iteration 27400: Loss = -12344.5478515625
9
Iteration 27500: Loss = -12344.5498046875
10
Iteration 27600: Loss = -12344.548828125
11
Iteration 27700: Loss = -12344.548828125
12
Iteration 27800: Loss = -12344.548828125
13
Iteration 27900: Loss = -12344.548828125
14
Iteration 28000: Loss = -12344.5478515625
15
Stopping early at iteration 28000 due to no improvement.
pi: tensor([[1.0000e+00, 2.6901e-06],
        [7.8570e-01, 2.1430e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([6.2135e-04, 9.9938e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2022, 0.1928],
         [0.9410, 0.1942]],

        [[0.9073, 0.1995],
         [0.9111, 0.9916]],

        [[0.0125, 0.1403],
         [0.5256, 0.0273]],

        [[0.1062, 0.1044],
         [0.9172, 0.9855]],

        [[0.4546, 0.1755],
         [0.9790, 0.0157]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00031163694582847545
Average Adjusted Rand Index: -0.0008916167537547832
[-0.0007416216469663104, -0.00031163694582847545] [-0.0005547302265410098, -0.0008916167537547832] [12344.490234375, 12344.5478515625]
-------------------------------------
This iteration is 73
True Objective function: Loss = -11835.014380379685
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35589.73046875
Iteration 100: Loss = -21800.45703125
Iteration 200: Loss = -14395.5234375
Iteration 300: Loss = -13160.6630859375
Iteration 400: Loss = -12822.90234375
Iteration 500: Loss = -12678.455078125
Iteration 600: Loss = -12606.033203125
Iteration 700: Loss = -12553.552734375
Iteration 800: Loss = -12511.46484375
Iteration 900: Loss = -12485.5908203125
Iteration 1000: Loss = -12465.5966796875
Iteration 1100: Loss = -12450.98828125
Iteration 1200: Loss = -12440.1025390625
Iteration 1300: Loss = -12432.607421875
Iteration 1400: Loss = -12425.732421875
Iteration 1500: Loss = -12408.90625
Iteration 1600: Loss = -12398.1328125
Iteration 1700: Loss = -12392.9296875
Iteration 1800: Loss = -12389.60546875
Iteration 1900: Loss = -12387.7939453125
Iteration 2000: Loss = -12381.2763671875
Iteration 2100: Loss = -12379.1943359375
Iteration 2200: Loss = -12378.09375
Iteration 2300: Loss = -12373.3154296875
Iteration 2400: Loss = -12370.892578125
Iteration 2500: Loss = -12370.0302734375
Iteration 2600: Loss = -12369.3671875
Iteration 2700: Loss = -12368.7880859375
Iteration 2800: Loss = -12368.2724609375
Iteration 2900: Loss = -12367.8466796875
Iteration 3000: Loss = -12367.4619140625
Iteration 3100: Loss = -12367.10546875
Iteration 3200: Loss = -12366.76171875
Iteration 3300: Loss = -12362.3544921875
Iteration 3400: Loss = -12359.927734375
Iteration 3500: Loss = -12358.3388671875
Iteration 3600: Loss = -12357.7783203125
Iteration 3700: Loss = -12357.4228515625
Iteration 3800: Loss = -12357.146484375
Iteration 3900: Loss = -12356.921875
Iteration 4000: Loss = -12356.724609375
Iteration 4100: Loss = -12356.5498046875
Iteration 4200: Loss = -12356.39453125
Iteration 4300: Loss = -12356.25390625
Iteration 4400: Loss = -12356.123046875
Iteration 4500: Loss = -12356.00390625
Iteration 4600: Loss = -12355.89453125
Iteration 4700: Loss = -12355.7958984375
Iteration 4800: Loss = -12355.7041015625
Iteration 4900: Loss = -12355.6171875
Iteration 5000: Loss = -12355.5380859375
Iteration 5100: Loss = -12355.4609375
Iteration 5200: Loss = -12355.390625
Iteration 5300: Loss = -12355.322265625
Iteration 5400: Loss = -12355.2578125
Iteration 5500: Loss = -12355.1982421875
Iteration 5600: Loss = -12355.1376953125
Iteration 5700: Loss = -12355.0849609375
Iteration 5800: Loss = -12355.033203125
Iteration 5900: Loss = -12354.9853515625
Iteration 6000: Loss = -12354.939453125
Iteration 6100: Loss = -12354.8974609375
Iteration 6200: Loss = -12354.8544921875
Iteration 6300: Loss = -12354.8173828125
Iteration 6400: Loss = -12354.77734375
Iteration 6500: Loss = -12354.7421875
Iteration 6600: Loss = -12354.708984375
Iteration 6700: Loss = -12354.6767578125
Iteration 6800: Loss = -12354.6474609375
Iteration 6900: Loss = -12354.619140625
Iteration 7000: Loss = -12354.59375
Iteration 7100: Loss = -12354.568359375
Iteration 7200: Loss = -12354.544921875
Iteration 7300: Loss = -12354.5205078125
Iteration 7400: Loss = -12354.5
Iteration 7500: Loss = -12354.4775390625
Iteration 7600: Loss = -12354.45703125
Iteration 7700: Loss = -12354.4375
Iteration 7800: Loss = -12354.419921875
Iteration 7900: Loss = -12354.4013671875
Iteration 8000: Loss = -12354.384765625
Iteration 8100: Loss = -12354.3681640625
Iteration 8200: Loss = -12354.3525390625
Iteration 8300: Loss = -12354.337890625
Iteration 8400: Loss = -12354.3232421875
Iteration 8500: Loss = -12354.3095703125
Iteration 8600: Loss = -12354.296875
Iteration 8700: Loss = -12354.28515625
Iteration 8800: Loss = -12354.2734375
Iteration 8900: Loss = -12354.26171875
Iteration 9000: Loss = -12354.2490234375
Iteration 9100: Loss = -12354.23828125
Iteration 9200: Loss = -12354.2265625
Iteration 9300: Loss = -12354.2177734375
Iteration 9400: Loss = -12354.2099609375
Iteration 9500: Loss = -12354.2001953125
Iteration 9600: Loss = -12354.19140625
Iteration 9700: Loss = -12354.18359375
Iteration 9800: Loss = -12354.1748046875
Iteration 9900: Loss = -12354.166015625
Iteration 10000: Loss = -12354.16015625
Iteration 10100: Loss = -12354.15234375
Iteration 10200: Loss = -12354.1484375
Iteration 10300: Loss = -12354.140625
Iteration 10400: Loss = -12354.1357421875
Iteration 10500: Loss = -12354.130859375
Iteration 10600: Loss = -12354.125
Iteration 10700: Loss = -12354.1201171875
Iteration 10800: Loss = -12354.115234375
Iteration 10900: Loss = -12354.109375
Iteration 11000: Loss = -12354.1044921875
Iteration 11100: Loss = -12354.1005859375
Iteration 11200: Loss = -12350.9765625
Iteration 11300: Loss = -12349.240234375
Iteration 11400: Loss = -12349.107421875
Iteration 11500: Loss = -12349.04296875
Iteration 11600: Loss = -12349.0029296875
Iteration 11700: Loss = -12348.9765625
Iteration 11800: Loss = -12348.9560546875
Iteration 11900: Loss = -12348.939453125
Iteration 12000: Loss = -12348.9287109375
Iteration 12100: Loss = -12348.9189453125
Iteration 12200: Loss = -12348.9091796875
Iteration 12300: Loss = -12348.9013671875
Iteration 12400: Loss = -12348.89453125
Iteration 12500: Loss = -12348.890625
Iteration 12600: Loss = -12348.884765625
Iteration 12700: Loss = -12348.8798828125
Iteration 12800: Loss = -12348.8759765625
Iteration 12900: Loss = -12348.875
Iteration 13000: Loss = -12348.87109375
Iteration 13100: Loss = -12348.8671875
Iteration 13200: Loss = -12348.865234375
Iteration 13300: Loss = -12348.861328125
Iteration 13400: Loss = -12348.861328125
Iteration 13500: Loss = -12348.859375
Iteration 13600: Loss = -12348.85546875
Iteration 13700: Loss = -12348.853515625
Iteration 13800: Loss = -12348.8515625
Iteration 13900: Loss = -12348.8515625
Iteration 14000: Loss = -12348.8505859375
Iteration 14100: Loss = -12348.84765625
Iteration 14200: Loss = -12348.8466796875
Iteration 14300: Loss = -12348.84765625
1
Iteration 14400: Loss = -12348.8447265625
Iteration 14500: Loss = -12348.84375
Iteration 14600: Loss = -12348.841796875
Iteration 14700: Loss = -12348.841796875
Iteration 14800: Loss = -12348.8408203125
Iteration 14900: Loss = -12348.8408203125
Iteration 15000: Loss = -12348.837890625
Iteration 15100: Loss = -12348.8388671875
1
Iteration 15200: Loss = -12348.837890625
Iteration 15300: Loss = -12348.837890625
Iteration 15400: Loss = -12348.8359375
Iteration 15500: Loss = -12348.8359375
Iteration 15600: Loss = -12348.8359375
Iteration 15700: Loss = -12348.8349609375
Iteration 15800: Loss = -12348.833984375
Iteration 15900: Loss = -12348.833984375
Iteration 16000: Loss = -12348.8330078125
Iteration 16100: Loss = -12348.83203125
Iteration 16200: Loss = -12348.8310546875
Iteration 16300: Loss = -12348.830078125
Iteration 16400: Loss = -12348.8291015625
Iteration 16500: Loss = -12348.828125
Iteration 16600: Loss = -12348.8271484375
Iteration 16700: Loss = -12348.8232421875
Iteration 16800: Loss = -12348.8212890625
Iteration 16900: Loss = -12348.8134765625
Iteration 17000: Loss = -12348.791015625
Iteration 17100: Loss = -12348.6494140625
Iteration 17200: Loss = -12348.5029296875
Iteration 17300: Loss = -12348.4716796875
Iteration 17400: Loss = -12348.458984375
Iteration 17500: Loss = -12348.455078125
Iteration 17600: Loss = -12348.4521484375
Iteration 17700: Loss = -12348.4501953125
Iteration 17800: Loss = -12348.4482421875
Iteration 17900: Loss = -12348.447265625
Iteration 18000: Loss = -12348.4453125
Iteration 18100: Loss = -12348.4462890625
1
Iteration 18200: Loss = -12348.4453125
Iteration 18300: Loss = -12348.443359375
Iteration 18400: Loss = -12348.443359375
Iteration 18500: Loss = -12348.4443359375
1
Iteration 18600: Loss = -12348.4443359375
2
Iteration 18700: Loss = -12348.443359375
Iteration 18800: Loss = -12348.443359375
Iteration 18900: Loss = -12348.4423828125
Iteration 19000: Loss = -12348.443359375
1
Iteration 19100: Loss = -12348.4296875
Iteration 19200: Loss = -12348.427734375
Iteration 19300: Loss = -12348.427734375
Iteration 19400: Loss = -12348.4267578125
Iteration 19500: Loss = -12348.42578125
Iteration 19600: Loss = -12348.42578125
Iteration 19700: Loss = -12348.4267578125
1
Iteration 19800: Loss = -12348.42578125
Iteration 19900: Loss = -12348.4248046875
Iteration 20000: Loss = -12348.4248046875
Iteration 20100: Loss = -12348.4248046875
Iteration 20200: Loss = -12348.423828125
Iteration 20300: Loss = -12348.423828125
Iteration 20400: Loss = -12348.4248046875
1
Iteration 20500: Loss = -12348.423828125
Iteration 20600: Loss = -12348.4248046875
1
Iteration 20700: Loss = -12348.423828125
Iteration 20800: Loss = -12348.4248046875
1
Iteration 20900: Loss = -12348.4208984375
Iteration 21000: Loss = -12348.419921875
Iteration 21100: Loss = -12348.41796875
Iteration 21200: Loss = -12348.4169921875
Iteration 21300: Loss = -12348.4169921875
Iteration 21400: Loss = -12348.4169921875
Iteration 21500: Loss = -12348.4169921875
Iteration 21600: Loss = -12348.4169921875
Iteration 21700: Loss = -12348.4169921875
Iteration 21800: Loss = -12348.4150390625
Iteration 21900: Loss = -12348.4169921875
1
Iteration 22000: Loss = -12348.416015625
2
Iteration 22100: Loss = -12348.4169921875
3
Iteration 22200: Loss = -12348.4169921875
4
Iteration 22300: Loss = -12348.4169921875
5
Iteration 22400: Loss = -12348.416015625
6
Iteration 22500: Loss = -12348.4169921875
7
Iteration 22600: Loss = -12348.4169921875
8
Iteration 22700: Loss = -12348.41796875
9
Iteration 22800: Loss = -12348.4169921875
10
Iteration 22900: Loss = -12348.4169921875
11
Iteration 23000: Loss = -12348.4169921875
12
Iteration 23100: Loss = -12348.4150390625
Iteration 23200: Loss = -12348.41796875
1
Iteration 23300: Loss = -12348.4169921875
2
Iteration 23400: Loss = -12348.4169921875
3
Iteration 23500: Loss = -12348.4169921875
4
Iteration 23600: Loss = -12348.4169921875
5
Iteration 23700: Loss = -12348.416015625
6
Iteration 23800: Loss = -12348.4169921875
7
Iteration 23900: Loss = -12348.4150390625
Iteration 24000: Loss = -12348.416015625
1
Iteration 24100: Loss = -12348.416015625
2
Iteration 24200: Loss = -12348.4150390625
Iteration 24300: Loss = -12348.416015625
1
Iteration 24400: Loss = -12348.4150390625
Iteration 24500: Loss = -12348.416015625
1
Iteration 24600: Loss = -12348.4140625
Iteration 24700: Loss = -12348.4150390625
1
Iteration 24800: Loss = -12348.416015625
2
Iteration 24900: Loss = -12348.4140625
Iteration 25000: Loss = -12348.4150390625
1
Iteration 25100: Loss = -12348.4140625
Iteration 25200: Loss = -12348.4150390625
1
Iteration 25300: Loss = -12348.4130859375
Iteration 25400: Loss = -12348.4150390625
1
Iteration 25500: Loss = -12348.416015625
2
Iteration 25600: Loss = -12348.4150390625
3
Iteration 25700: Loss = -12348.416015625
4
Iteration 25800: Loss = -12348.4150390625
5
Iteration 25900: Loss = -12348.4130859375
Iteration 26000: Loss = -12348.4150390625
1
Iteration 26100: Loss = -12348.4150390625
2
Iteration 26200: Loss = -12348.4150390625
3
Iteration 26300: Loss = -12348.416015625
4
Iteration 26400: Loss = -12348.4150390625
5
Iteration 26500: Loss = -12348.416015625
6
Iteration 26600: Loss = -12348.416015625
7
Iteration 26700: Loss = -12348.416015625
8
Iteration 26800: Loss = -12348.416015625
9
Iteration 26900: Loss = -12348.4150390625
10
Iteration 27000: Loss = -12348.4150390625
11
Iteration 27100: Loss = -12348.4150390625
12
Iteration 27200: Loss = -12348.416015625
13
Iteration 27300: Loss = -12348.416015625
14
Iteration 27400: Loss = -12348.416015625
15
Stopping early at iteration 27400 due to no improvement.
pi: tensor([[1.3098e-02, 9.8690e-01],
        [2.0327e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9935, 0.0065], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1945, 0.1945],
         [0.6649, 0.1998]],

        [[0.7022, 0.2438],
         [0.9886, 0.1862]],

        [[0.5952, 0.2846],
         [0.9691, 0.0407]],

        [[0.2004, 0.1925],
         [0.5789, 0.0878]],

        [[0.9218, 0.1950],
         [0.6963, 0.9376]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.002055342448579068
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18270.69921875
Iteration 100: Loss = -13955.9677734375
Iteration 200: Loss = -12665.634765625
Iteration 300: Loss = -12509.4169921875
Iteration 400: Loss = -12470.119140625
Iteration 500: Loss = -12451.1376953125
Iteration 600: Loss = -12433.9033203125
Iteration 700: Loss = -12420.513671875
Iteration 800: Loss = -12410.5712890625
Iteration 900: Loss = -12400.00390625
Iteration 1000: Loss = -12391.8232421875
Iteration 1100: Loss = -12382.4638671875
Iteration 1200: Loss = -12376.0712890625
Iteration 1300: Loss = -12369.96875
Iteration 1400: Loss = -12367.1123046875
Iteration 1500: Loss = -12365.1123046875
Iteration 1600: Loss = -12363.1025390625
Iteration 1700: Loss = -12360.3212890625
Iteration 1800: Loss = -12357.9677734375
Iteration 1900: Loss = -12356.7958984375
Iteration 2000: Loss = -12355.9287109375
Iteration 2100: Loss = -12355.109375
Iteration 2200: Loss = -12354.4267578125
Iteration 2300: Loss = -12353.77734375
Iteration 2400: Loss = -12352.5732421875
Iteration 2500: Loss = -12352.025390625
Iteration 2600: Loss = -12351.64453125
Iteration 2700: Loss = -12351.326171875
Iteration 2800: Loss = -12351.0537109375
Iteration 2900: Loss = -12350.8173828125
Iteration 3000: Loss = -12350.611328125
Iteration 3100: Loss = -12350.427734375
Iteration 3200: Loss = -12350.267578125
Iteration 3300: Loss = -12350.1240234375
Iteration 3400: Loss = -12349.99609375
Iteration 3500: Loss = -12349.880859375
Iteration 3600: Loss = -12349.7763671875
Iteration 3700: Loss = -12349.6826171875
Iteration 3800: Loss = -12349.5947265625
Iteration 3900: Loss = -12349.517578125
Iteration 4000: Loss = -12349.4453125
Iteration 4100: Loss = -12349.3779296875
Iteration 4200: Loss = -12349.3173828125
Iteration 4300: Loss = -12349.259765625
Iteration 4400: Loss = -12349.20703125
Iteration 4500: Loss = -12349.158203125
Iteration 4600: Loss = -12349.11328125
Iteration 4700: Loss = -12349.0703125
Iteration 4800: Loss = -12349.0322265625
Iteration 4900: Loss = -12348.99609375
Iteration 5000: Loss = -12348.962890625
Iteration 5100: Loss = -12348.9326171875
Iteration 5200: Loss = -12348.90234375
Iteration 5300: Loss = -12348.875
Iteration 5400: Loss = -12348.849609375
Iteration 5500: Loss = -12348.82421875
Iteration 5600: Loss = -12348.80078125
Iteration 5700: Loss = -12348.7802734375
Iteration 5800: Loss = -12348.7607421875
Iteration 5900: Loss = -12348.7412109375
Iteration 6000: Loss = -12348.7236328125
Iteration 6100: Loss = -12348.708984375
Iteration 6200: Loss = -12348.6923828125
Iteration 6300: Loss = -12348.677734375
Iteration 6400: Loss = -12348.6611328125
Iteration 6500: Loss = -12348.6494140625
Iteration 6600: Loss = -12348.6376953125
Iteration 6700: Loss = -12348.6240234375
Iteration 6800: Loss = -12348.611328125
Iteration 6900: Loss = -12348.599609375
Iteration 7000: Loss = -12348.5888671875
Iteration 7100: Loss = -12348.578125
Iteration 7200: Loss = -12348.5712890625
Iteration 7300: Loss = -12348.5625
Iteration 7400: Loss = -12348.5576171875
Iteration 7500: Loss = -12348.546875
Iteration 7600: Loss = -12348.5400390625
Iteration 7700: Loss = -12348.5322265625
Iteration 7800: Loss = -12348.52734375
Iteration 7900: Loss = -12348.5185546875
Iteration 8000: Loss = -12348.5166015625
Iteration 8100: Loss = -12348.509765625
Iteration 8200: Loss = -12348.505859375
Iteration 8300: Loss = -12348.501953125
Iteration 8400: Loss = -12348.498046875
Iteration 8500: Loss = -12348.4921875
Iteration 8600: Loss = -12348.490234375
Iteration 8700: Loss = -12348.486328125
Iteration 8800: Loss = -12348.482421875
Iteration 8900: Loss = -12348.4794921875
Iteration 9000: Loss = -12348.4775390625
Iteration 9100: Loss = -12348.4736328125
Iteration 9200: Loss = -12348.470703125
Iteration 9300: Loss = -12348.46875
Iteration 9400: Loss = -12348.466796875
Iteration 9500: Loss = -12348.4619140625
Iteration 9600: Loss = -12348.4619140625
Iteration 9700: Loss = -12348.4599609375
Iteration 9800: Loss = -12348.4580078125
Iteration 9900: Loss = -12348.4560546875
Iteration 10000: Loss = -12348.455078125
Iteration 10100: Loss = -12348.4521484375
Iteration 10200: Loss = -12348.4521484375
Iteration 10300: Loss = -12348.4501953125
Iteration 10400: Loss = -12348.4482421875
Iteration 10500: Loss = -12348.447265625
Iteration 10600: Loss = -12348.447265625
Iteration 10700: Loss = -12348.4443359375
Iteration 10800: Loss = -12348.44140625
Iteration 10900: Loss = -12348.44140625
Iteration 11000: Loss = -12348.439453125
Iteration 11100: Loss = -12348.439453125
Iteration 11200: Loss = -12348.4384765625
Iteration 11300: Loss = -12348.4375
Iteration 11400: Loss = -12348.4365234375
Iteration 11500: Loss = -12348.43359375
Iteration 11600: Loss = -12348.4326171875
Iteration 11700: Loss = -12348.431640625
Iteration 11800: Loss = -12348.4287109375
Iteration 11900: Loss = -12348.4287109375
Iteration 12000: Loss = -12348.427734375
Iteration 12100: Loss = -12348.42578125
Iteration 12200: Loss = -12348.4228515625
Iteration 12300: Loss = -12348.4248046875
1
Iteration 12400: Loss = -12348.423828125
2
Iteration 12500: Loss = -12348.4228515625
Iteration 12600: Loss = -12348.4228515625
Iteration 12700: Loss = -12348.423828125
1
Iteration 12800: Loss = -12348.421875
Iteration 12900: Loss = -12348.4208984375
Iteration 13000: Loss = -12348.421875
1
Iteration 13100: Loss = -12348.4208984375
Iteration 13200: Loss = -12348.421875
1
Iteration 13300: Loss = -12348.419921875
Iteration 13400: Loss = -12348.419921875
Iteration 13500: Loss = -12348.4189453125
Iteration 13600: Loss = -12348.4208984375
1
Iteration 13700: Loss = -12348.4189453125
Iteration 13800: Loss = -12348.4189453125
Iteration 13900: Loss = -12348.41796875
Iteration 14000: Loss = -12348.4189453125
1
Iteration 14100: Loss = -12348.4189453125
2
Iteration 14200: Loss = -12348.4189453125
3
Iteration 14300: Loss = -12348.4169921875
Iteration 14400: Loss = -12348.4169921875
Iteration 14500: Loss = -12348.4169921875
Iteration 14600: Loss = -12348.4169921875
Iteration 14700: Loss = -12348.41796875
1
Iteration 14800: Loss = -12348.41796875
2
Iteration 14900: Loss = -12348.41796875
3
Iteration 15000: Loss = -12348.4169921875
Iteration 15100: Loss = -12348.416015625
Iteration 15200: Loss = -12348.41796875
1
Iteration 15300: Loss = -12348.41796875
2
Iteration 15400: Loss = -12348.4169921875
3
Iteration 15500: Loss = -12348.416015625
Iteration 15600: Loss = -12348.4169921875
1
Iteration 15700: Loss = -12348.4169921875
2
Iteration 15800: Loss = -12348.416015625
Iteration 15900: Loss = -12348.4169921875
1
Iteration 16000: Loss = -12348.416015625
Iteration 16100: Loss = -12348.4150390625
Iteration 16200: Loss = -12348.4150390625
Iteration 16300: Loss = -12348.4140625
Iteration 16400: Loss = -12348.4150390625
1
Iteration 16500: Loss = -12348.4150390625
2
Iteration 16600: Loss = -12348.4150390625
3
Iteration 16700: Loss = -12348.416015625
4
Iteration 16800: Loss = -12348.416015625
5
Iteration 16900: Loss = -12348.416015625
6
Iteration 17000: Loss = -12348.4169921875
7
Iteration 17100: Loss = -12348.4150390625
8
Iteration 17200: Loss = -12348.416015625
9
Iteration 17300: Loss = -12348.416015625
10
Iteration 17400: Loss = -12348.4150390625
11
Iteration 17500: Loss = -12348.4150390625
12
Iteration 17600: Loss = -12348.416015625
13
Iteration 17700: Loss = -12348.416015625
14
Iteration 17800: Loss = -12348.4150390625
15
Stopping early at iteration 17800 due to no improvement.
pi: tensor([[9.9998e-01, 1.8002e-05],
        [9.8691e-01, 1.3090e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.2504e-04, 9.9987e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1998, 0.1970],
         [0.9257, 0.1946]],

        [[0.9902, 0.2438],
         [0.0987, 0.0080]],

        [[0.2776, 0.2831],
         [0.9674, 0.9300]],

        [[0.1287, 0.2050],
         [0.4610, 0.0270]],

        [[0.0521, 0.1793],
         [0.9758, 0.9909]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.002055342448579068
Average Adjusted Rand Index: 0.0
[0.002055342448579068, 0.002055342448579068] [0.0, 0.0] [12348.416015625, 12348.4150390625]
-------------------------------------
This iteration is 74
True Objective function: Loss = -11931.399248623846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28777.25390625
Iteration 100: Loss = -18499.998046875
Iteration 200: Loss = -13889.8837890625
Iteration 300: Loss = -12833.462890625
Iteration 400: Loss = -12653.765625
Iteration 500: Loss = -12594.0888671875
Iteration 600: Loss = -12559.716796875
Iteration 700: Loss = -12542.955078125
Iteration 800: Loss = -12532.0849609375
Iteration 900: Loss = -12525.7421875
Iteration 1000: Loss = -12520.48046875
Iteration 1100: Loss = -12516.3671875
Iteration 1200: Loss = -12513.697265625
Iteration 1300: Loss = -12511.7783203125
Iteration 1400: Loss = -12510.2880859375
Iteration 1500: Loss = -12509.091796875
Iteration 1600: Loss = -12508.11328125
Iteration 1700: Loss = -12507.2900390625
Iteration 1800: Loss = -12506.6005859375
Iteration 1900: Loss = -12506.0029296875
Iteration 2000: Loss = -12505.4892578125
Iteration 2100: Loss = -12505.041015625
Iteration 2200: Loss = -12504.6494140625
Iteration 2300: Loss = -12504.302734375
Iteration 2400: Loss = -12503.9931640625
Iteration 2500: Loss = -12503.7177734375
Iteration 2600: Loss = -12503.470703125
Iteration 2700: Loss = -12503.2451171875
Iteration 2800: Loss = -12503.04296875
Iteration 2900: Loss = -12502.8583984375
Iteration 3000: Loss = -12502.69140625
Iteration 3100: Loss = -12502.5390625
Iteration 3200: Loss = -12502.3984375
Iteration 3300: Loss = -12502.271484375
Iteration 3400: Loss = -12502.154296875
Iteration 3500: Loss = -12502.0458984375
Iteration 3600: Loss = -12501.9453125
Iteration 3700: Loss = -12501.8544921875
Iteration 3800: Loss = -12501.7685546875
Iteration 3900: Loss = -12501.6904296875
Iteration 4000: Loss = -12501.6181640625
Iteration 4100: Loss = -12501.55078125
Iteration 4200: Loss = -12501.4853515625
Iteration 4300: Loss = -12501.42578125
Iteration 4400: Loss = -12501.3701171875
Iteration 4500: Loss = -12501.3193359375
Iteration 4600: Loss = -12501.26953125
Iteration 4700: Loss = -12501.2255859375
Iteration 4800: Loss = -12501.18359375
Iteration 4900: Loss = -12501.1435546875
Iteration 5000: Loss = -12501.107421875
Iteration 5100: Loss = -12501.072265625
Iteration 5200: Loss = -12501.0400390625
Iteration 5300: Loss = -12501.0107421875
Iteration 5400: Loss = -12500.982421875
Iteration 5500: Loss = -12500.955078125
Iteration 5600: Loss = -12500.9306640625
Iteration 5700: Loss = -12500.9072265625
Iteration 5800: Loss = -12500.8857421875
Iteration 5900: Loss = -12500.86328125
Iteration 6000: Loss = -12500.8447265625
Iteration 6100: Loss = -12500.8251953125
Iteration 6200: Loss = -12500.80859375
Iteration 6300: Loss = -12500.79296875
Iteration 6400: Loss = -12500.7744140625
Iteration 6500: Loss = -12500.76171875
Iteration 6600: Loss = -12500.7470703125
Iteration 6700: Loss = -12500.734375
Iteration 6800: Loss = -12500.720703125
Iteration 6900: Loss = -12500.708984375
Iteration 7000: Loss = -12500.69921875
Iteration 7100: Loss = -12500.6875
Iteration 7200: Loss = -12500.6767578125
Iteration 7300: Loss = -12500.6689453125
Iteration 7400: Loss = -12500.6630859375
Iteration 7500: Loss = -12500.6533203125
Iteration 7600: Loss = -12500.6455078125
Iteration 7700: Loss = -12500.6376953125
Iteration 7800: Loss = -12500.6298828125
Iteration 7900: Loss = -12500.6240234375
Iteration 8000: Loss = -12500.6171875
Iteration 8100: Loss = -12500.6103515625
Iteration 8200: Loss = -12500.60546875
Iteration 8300: Loss = -12500.599609375
Iteration 8400: Loss = -12500.595703125
Iteration 8500: Loss = -12500.58984375
Iteration 8600: Loss = -12500.5859375
Iteration 8700: Loss = -12500.5810546875
Iteration 8800: Loss = -12500.580078125
Iteration 8900: Loss = -12500.5732421875
Iteration 9000: Loss = -12500.5693359375
Iteration 9100: Loss = -12500.568359375
Iteration 9200: Loss = -12500.564453125
Iteration 9300: Loss = -12500.5634765625
Iteration 9400: Loss = -12500.5576171875
Iteration 9500: Loss = -12500.5546875
Iteration 9600: Loss = -12500.5517578125
Iteration 9700: Loss = -12500.55078125
Iteration 9800: Loss = -12500.5478515625
Iteration 9900: Loss = -12500.544921875
Iteration 10000: Loss = -12500.544921875
Iteration 10100: Loss = -12500.5419921875
Iteration 10200: Loss = -12500.5400390625
Iteration 10300: Loss = -12500.5380859375
Iteration 10400: Loss = -12500.537109375
Iteration 10500: Loss = -12500.53515625
Iteration 10600: Loss = -12500.5322265625
Iteration 10700: Loss = -12500.53125
Iteration 10800: Loss = -12500.529296875
Iteration 10900: Loss = -12500.5283203125
Iteration 11000: Loss = -12500.5283203125
Iteration 11100: Loss = -12500.52734375
Iteration 11200: Loss = -12500.52734375
Iteration 11300: Loss = -12500.5244140625
Iteration 11400: Loss = -12500.5244140625
Iteration 11500: Loss = -12500.5224609375
Iteration 11600: Loss = -12500.5224609375
Iteration 11700: Loss = -12500.521484375
Iteration 11800: Loss = -12500.51953125
Iteration 11900: Loss = -12500.51953125
Iteration 12000: Loss = -12500.5185546875
Iteration 12100: Loss = -12500.5185546875
Iteration 12200: Loss = -12500.5185546875
Iteration 12300: Loss = -12500.5185546875
Iteration 12400: Loss = -12500.515625
Iteration 12500: Loss = -12500.5166015625
1
Iteration 12600: Loss = -12500.515625
Iteration 12700: Loss = -12500.5146484375
Iteration 12800: Loss = -12500.515625
1
Iteration 12900: Loss = -12500.5126953125
Iteration 13000: Loss = -12500.515625
1
Iteration 13100: Loss = -12500.51171875
Iteration 13200: Loss = -12500.513671875
1
Iteration 13300: Loss = -12500.513671875
2
Iteration 13400: Loss = -12500.515625
3
Iteration 13500: Loss = -12500.51171875
Iteration 13600: Loss = -12500.513671875
1
Iteration 13700: Loss = -12500.5126953125
2
Iteration 13800: Loss = -12500.5107421875
Iteration 13900: Loss = -12500.51171875
1
Iteration 14000: Loss = -12500.5107421875
Iteration 14100: Loss = -12500.509765625
Iteration 14200: Loss = -12500.5087890625
Iteration 14300: Loss = -12500.5107421875
1
Iteration 14400: Loss = -12500.5107421875
2
Iteration 14500: Loss = -12500.5107421875
3
Iteration 14600: Loss = -12500.5078125
Iteration 14700: Loss = -12500.5078125
Iteration 14800: Loss = -12500.5087890625
1
Iteration 14900: Loss = -12500.5078125
Iteration 15000: Loss = -12500.5087890625
1
Iteration 15100: Loss = -12500.5068359375
Iteration 15200: Loss = -12500.505859375
Iteration 15300: Loss = -12500.5068359375
1
Iteration 15400: Loss = -12500.5048828125
Iteration 15500: Loss = -12500.50390625
Iteration 15600: Loss = -12500.50390625
Iteration 15700: Loss = -12500.5029296875
Iteration 15800: Loss = -12500.5009765625
Iteration 15900: Loss = -12500.5
Iteration 16000: Loss = -12500.4990234375
Iteration 16100: Loss = -12500.498046875
Iteration 16200: Loss = -12500.4970703125
Iteration 16300: Loss = -12500.4931640625
Iteration 16400: Loss = -12500.48828125
Iteration 16500: Loss = -12500.484375
Iteration 16600: Loss = -12500.474609375
Iteration 16700: Loss = -12500.4609375
Iteration 16800: Loss = -12500.4248046875
Iteration 16900: Loss = -12500.2841796875
Iteration 17000: Loss = -12499.9033203125
Iteration 17100: Loss = -12499.5712890625
Iteration 17200: Loss = -12499.3779296875
Iteration 17300: Loss = -12499.365234375
Iteration 17400: Loss = -12499.36328125
Iteration 17500: Loss = -12499.36328125
Iteration 17600: Loss = -12499.3642578125
1
Iteration 17700: Loss = -12499.36328125
Iteration 17800: Loss = -12499.361328125
Iteration 17900: Loss = -12499.3623046875
1
Iteration 18000: Loss = -12499.361328125
Iteration 18100: Loss = -12499.36328125
1
Iteration 18200: Loss = -12499.3623046875
2
Iteration 18300: Loss = -12499.36328125
3
Iteration 18400: Loss = -12499.3623046875
4
Iteration 18500: Loss = -12499.36328125
5
Iteration 18600: Loss = -12499.3642578125
6
Iteration 18700: Loss = -12499.3623046875
7
Iteration 18800: Loss = -12499.36328125
8
Iteration 18900: Loss = -12499.36328125
9
Iteration 19000: Loss = -12499.3642578125
10
Iteration 19100: Loss = -12499.36328125
11
Iteration 19200: Loss = -12499.3662109375
12
Iteration 19300: Loss = -12499.3642578125
13
Iteration 19400: Loss = -12499.3623046875
14
Iteration 19500: Loss = -12499.36328125
15
Stopping early at iteration 19500 due to no improvement.
pi: tensor([[0.0566, 0.9434],
        [0.8284, 0.1716]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0021, 0.9979], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2087, 0.2130],
         [0.5729, 0.2001]],

        [[0.9833, 0.2136],
         [0.1111, 0.0089]],

        [[0.0150, 0.2019],
         [0.9057, 0.4425]],

        [[0.0889, 0.1998],
         [0.9240, 0.0394]],

        [[0.0132, 0.2029],
         [0.3723, 0.0084]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0019121367385944272
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -32263.4921875
Iteration 100: Loss = -20532.478515625
Iteration 200: Loss = -14222.1240234375
Iteration 300: Loss = -13088.421875
Iteration 400: Loss = -12834.1044921875
Iteration 500: Loss = -12730.9033203125
Iteration 600: Loss = -12670.30859375
Iteration 700: Loss = -12636.1533203125
Iteration 800: Loss = -12607.1416015625
Iteration 900: Loss = -12591.369140625
Iteration 1000: Loss = -12579.6015625
Iteration 1100: Loss = -12568.177734375
Iteration 1200: Loss = -12555.7978515625
Iteration 1300: Loss = -12548.984375
Iteration 1400: Loss = -12540.7607421875
Iteration 1500: Loss = -12537.0615234375
Iteration 1600: Loss = -12534.3154296875
Iteration 1700: Loss = -12532.14453125
Iteration 1800: Loss = -12527.5654296875
Iteration 1900: Loss = -12523.6494140625
Iteration 2000: Loss = -12519.263671875
Iteration 2100: Loss = -12515.529296875
Iteration 2200: Loss = -12514.3125
Iteration 2300: Loss = -12513.373046875
Iteration 2400: Loss = -12512.599609375
Iteration 2500: Loss = -12511.943359375
Iteration 2600: Loss = -12511.373046875
Iteration 2700: Loss = -12510.87890625
Iteration 2800: Loss = -12510.443359375
Iteration 2900: Loss = -12510.0556640625
Iteration 3000: Loss = -12509.7109375
Iteration 3100: Loss = -12509.4013671875
Iteration 3200: Loss = -12509.125
Iteration 3300: Loss = -12508.880859375
Iteration 3400: Loss = -12508.6611328125
Iteration 3500: Loss = -12508.462890625
Iteration 3600: Loss = -12508.265625
Iteration 3700: Loss = -12503.56640625
Iteration 3800: Loss = -12503.1494140625
Iteration 3900: Loss = -12502.904296875
Iteration 4000: Loss = -12502.7080078125
Iteration 4100: Loss = -12502.54296875
Iteration 4200: Loss = -12502.3984375
Iteration 4300: Loss = -12502.2685546875
Iteration 4400: Loss = -12502.1494140625
Iteration 4500: Loss = -12502.0439453125
Iteration 4600: Loss = -12501.9453125
Iteration 4700: Loss = -12501.8544921875
Iteration 4800: Loss = -12501.7705078125
Iteration 4900: Loss = -12501.6923828125
Iteration 5000: Loss = -12501.6220703125
Iteration 5100: Loss = -12501.5556640625
Iteration 5200: Loss = -12501.4951171875
Iteration 5300: Loss = -12501.43359375
Iteration 5400: Loss = -12501.380859375
Iteration 5500: Loss = -12501.3310546875
Iteration 5600: Loss = -12501.2841796875
Iteration 5700: Loss = -12501.240234375
Iteration 5800: Loss = -12501.19921875
Iteration 5900: Loss = -12501.1611328125
Iteration 6000: Loss = -12501.125
Iteration 6100: Loss = -12501.08984375
Iteration 6200: Loss = -12501.0595703125
Iteration 6300: Loss = -12501.0283203125
Iteration 6400: Loss = -12501.0009765625
Iteration 6500: Loss = -12500.9736328125
Iteration 6600: Loss = -12500.9521484375
Iteration 6700: Loss = -12500.9267578125
Iteration 6800: Loss = -12500.9052734375
Iteration 6900: Loss = -12500.884765625
Iteration 7000: Loss = -12500.8671875
Iteration 7100: Loss = -12500.849609375
Iteration 7200: Loss = -12500.8310546875
Iteration 7300: Loss = -12500.8173828125
Iteration 7400: Loss = -12500.8017578125
Iteration 7500: Loss = -12500.787109375
Iteration 7600: Loss = -12500.7724609375
Iteration 7700: Loss = -12500.76171875
Iteration 7800: Loss = -12500.748046875
Iteration 7900: Loss = -12500.73828125
Iteration 8000: Loss = -12500.7275390625
Iteration 8100: Loss = -12500.71484375
Iteration 8200: Loss = -12500.705078125
Iteration 8300: Loss = -12500.6982421875
Iteration 8400: Loss = -12500.689453125
Iteration 8500: Loss = -12500.681640625
Iteration 8600: Loss = -12500.673828125
Iteration 8700: Loss = -12500.66796875
Iteration 8800: Loss = -12500.658203125
Iteration 8900: Loss = -12500.6533203125
Iteration 9000: Loss = -12500.6474609375
Iteration 9100: Loss = -12500.640625
Iteration 9200: Loss = -12500.6357421875
Iteration 9300: Loss = -12500.6298828125
Iteration 9400: Loss = -12500.625
Iteration 9500: Loss = -12500.6181640625
Iteration 9600: Loss = -12500.6142578125
Iteration 9700: Loss = -12500.611328125
Iteration 9800: Loss = -12500.60546875
Iteration 9900: Loss = -12500.6025390625
Iteration 10000: Loss = -12500.6005859375
Iteration 10100: Loss = -12500.595703125
Iteration 10200: Loss = -12500.5908203125
Iteration 10300: Loss = -12500.5888671875
Iteration 10400: Loss = -12500.583984375
Iteration 10500: Loss = -12500.5830078125
Iteration 10600: Loss = -12500.580078125
Iteration 10700: Loss = -12500.5751953125
Iteration 10800: Loss = -12500.57421875
Iteration 10900: Loss = -12500.5712890625
Iteration 11000: Loss = -12500.5712890625
Iteration 11100: Loss = -12500.568359375
Iteration 11200: Loss = -12500.5654296875
Iteration 11300: Loss = -12500.5615234375
Iteration 11400: Loss = -12500.5615234375
Iteration 11500: Loss = -12500.560546875
Iteration 11600: Loss = -12500.5556640625
Iteration 11700: Loss = -12500.5556640625
Iteration 11800: Loss = -12500.552734375
Iteration 11900: Loss = -12500.552734375
Iteration 12000: Loss = -12500.5517578125
Iteration 12100: Loss = -12500.548828125
Iteration 12200: Loss = -12500.5478515625
Iteration 12300: Loss = -12500.544921875
Iteration 12400: Loss = -12500.54296875
Iteration 12500: Loss = -12500.544921875
1
Iteration 12600: Loss = -12500.54296875
Iteration 12700: Loss = -12500.5419921875
Iteration 12800: Loss = -12500.5400390625
Iteration 12900: Loss = -12500.5390625
Iteration 13000: Loss = -12500.5380859375
Iteration 13100: Loss = -12500.5380859375
Iteration 13200: Loss = -12500.5361328125
Iteration 13300: Loss = -12500.5361328125
Iteration 13400: Loss = -12500.53515625
Iteration 13500: Loss = -12500.53515625
Iteration 13600: Loss = -12500.5341796875
Iteration 13700: Loss = -12500.5322265625
Iteration 13800: Loss = -12500.5341796875
1
Iteration 13900: Loss = -12500.533203125
2
Iteration 14000: Loss = -12500.5322265625
Iteration 14100: Loss = -12500.53125
Iteration 14200: Loss = -12500.5302734375
Iteration 14300: Loss = -12500.5302734375
Iteration 14400: Loss = -12500.529296875
Iteration 14500: Loss = -12500.5302734375
1
Iteration 14600: Loss = -12500.5283203125
Iteration 14700: Loss = -12500.529296875
1
Iteration 14800: Loss = -12500.529296875
2
Iteration 14900: Loss = -12500.5283203125
Iteration 15000: Loss = -12500.5263671875
Iteration 15100: Loss = -12500.5283203125
1
Iteration 15200: Loss = -12500.5283203125
2
Iteration 15300: Loss = -12500.52734375
3
Iteration 15400: Loss = -12500.5263671875
Iteration 15500: Loss = -12500.52734375
1
Iteration 15600: Loss = -12500.5263671875
Iteration 15700: Loss = -12500.5263671875
Iteration 15800: Loss = -12500.525390625
Iteration 15900: Loss = -12500.525390625
Iteration 16000: Loss = -12500.525390625
Iteration 16100: Loss = -12500.5244140625
Iteration 16200: Loss = -12500.5244140625
Iteration 16300: Loss = -12500.5234375
Iteration 16400: Loss = -12500.5244140625
1
Iteration 16500: Loss = -12500.5234375
Iteration 16600: Loss = -12500.525390625
1
Iteration 16700: Loss = -12500.5244140625
2
Iteration 16800: Loss = -12500.5234375
Iteration 16900: Loss = -12500.5234375
Iteration 17000: Loss = -12500.5224609375
Iteration 17100: Loss = -12500.5224609375
Iteration 17200: Loss = -12500.5234375
1
Iteration 17300: Loss = -12500.521484375
Iteration 17400: Loss = -12500.5224609375
1
Iteration 17500: Loss = -12500.521484375
Iteration 17600: Loss = -12500.5234375
1
Iteration 17700: Loss = -12500.5234375
2
Iteration 17800: Loss = -12500.521484375
Iteration 17900: Loss = -12500.521484375
Iteration 18000: Loss = -12500.521484375
Iteration 18100: Loss = -12500.521484375
Iteration 18200: Loss = -12500.521484375
Iteration 18300: Loss = -12500.521484375
Iteration 18400: Loss = -12500.521484375
Iteration 18500: Loss = -12500.5205078125
Iteration 18600: Loss = -12500.521484375
1
Iteration 18700: Loss = -12500.5205078125
Iteration 18800: Loss = -12500.521484375
1
Iteration 18900: Loss = -12500.5205078125
Iteration 19000: Loss = -12500.521484375
1
Iteration 19100: Loss = -12500.5205078125
Iteration 19200: Loss = -12500.521484375
1
Iteration 19300: Loss = -12500.521484375
2
Iteration 19400: Loss = -12500.521484375
3
Iteration 19500: Loss = -12500.5224609375
4
Iteration 19600: Loss = -12500.5224609375
5
Iteration 19700: Loss = -12500.521484375
6
Iteration 19800: Loss = -12500.521484375
7
Iteration 19900: Loss = -12500.5224609375
8
Iteration 20000: Loss = -12500.5224609375
9
Iteration 20100: Loss = -12500.521484375
10
Iteration 20200: Loss = -12500.51953125
Iteration 20300: Loss = -12500.5224609375
1
Iteration 20400: Loss = -12500.521484375
2
Iteration 20500: Loss = -12500.5224609375
3
Iteration 20600: Loss = -12500.521484375
4
Iteration 20700: Loss = -12500.5224609375
5
Iteration 20800: Loss = -12500.521484375
6
Iteration 20900: Loss = -12500.5224609375
7
Iteration 21000: Loss = -12500.5205078125
8
Iteration 21100: Loss = -12500.5224609375
9
Iteration 21200: Loss = -12500.5205078125
10
Iteration 21300: Loss = -12500.5205078125
11
Iteration 21400: Loss = -12500.5205078125
12
Iteration 21500: Loss = -12500.5234375
13
Iteration 21600: Loss = -12500.5224609375
14
Iteration 21700: Loss = -12500.5224609375
15
Stopping early at iteration 21700 due to no improvement.
pi: tensor([[9.9998e-01, 1.5318e-05],
        [9.3136e-01, 6.8640e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9982e-01, 1.7698e-04], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2034, 0.1738],
         [0.0293, 0.3813]],

        [[0.7587, 0.2343],
         [0.9042, 0.0751]],

        [[0.8623, 0.1947],
         [0.9703, 0.1040]],

        [[0.9642, 0.1947],
         [0.6608, 0.9418]],

        [[0.8615, 0.2649],
         [0.0154, 0.0242]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[-0.0019121367385944272, 0.0] [0.0, 0.0] [12499.36328125, 12500.5224609375]
-------------------------------------
This iteration is 75
True Objective function: Loss = -11869.495033844172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17827.9375
Iteration 100: Loss = -13992.013671875
Iteration 200: Loss = -12730.611328125
Iteration 300: Loss = -12461.736328125
Iteration 400: Loss = -12388.416015625
Iteration 500: Loss = -12356.7880859375
Iteration 600: Loss = -12341.1435546875
Iteration 700: Loss = -12332.017578125
Iteration 800: Loss = -12326.5966796875
Iteration 900: Loss = -12323.5556640625
Iteration 1000: Loss = -12321.6572265625
Iteration 1100: Loss = -12320.4462890625
Iteration 1200: Loss = -12319.6201171875
Iteration 1300: Loss = -12319.044921875
Iteration 1400: Loss = -12318.6396484375
Iteration 1500: Loss = -12318.3486328125
Iteration 1600: Loss = -12318.1357421875
Iteration 1700: Loss = -12317.9755859375
Iteration 1800: Loss = -12317.8505859375
Iteration 1900: Loss = -12317.7490234375
Iteration 2000: Loss = -12317.6650390625
Iteration 2100: Loss = -12317.591796875
Iteration 2200: Loss = -12317.5263671875
Iteration 2300: Loss = -12317.4658203125
Iteration 2400: Loss = -12317.4091796875
Iteration 2500: Loss = -12317.3525390625
Iteration 2600: Loss = -12317.2919921875
Iteration 2700: Loss = -12317.2265625
Iteration 2800: Loss = -12317.1572265625
Iteration 2900: Loss = -12317.09375
Iteration 3000: Loss = -12317.052734375
Iteration 3100: Loss = -12316.9990234375
Iteration 3200: Loss = -12316.9580078125
Iteration 3300: Loss = -12316.94140625
Iteration 3400: Loss = -12316.9267578125
Iteration 3500: Loss = -12316.9140625
Iteration 3600: Loss = -12316.90234375
Iteration 3700: Loss = -12316.8916015625
Iteration 3800: Loss = -12316.8818359375
Iteration 3900: Loss = -12316.8720703125
Iteration 4000: Loss = -12316.86328125
Iteration 4100: Loss = -12316.853515625
Iteration 4200: Loss = -12316.8447265625
Iteration 4300: Loss = -12316.837890625
Iteration 4400: Loss = -12316.828125
Iteration 4500: Loss = -12316.8203125
Iteration 4600: Loss = -12316.8115234375
Iteration 4700: Loss = -12316.80078125
Iteration 4800: Loss = -12316.7841796875
Iteration 4900: Loss = -12316.7705078125
Iteration 5000: Loss = -12316.763671875
Iteration 5100: Loss = -12316.755859375
Iteration 5200: Loss = -12316.7509765625
Iteration 5300: Loss = -12316.744140625
Iteration 5400: Loss = -12316.7373046875
Iteration 5500: Loss = -12316.73046875
Iteration 5600: Loss = -12316.724609375
Iteration 5700: Loss = -12316.7197265625
Iteration 5800: Loss = -12316.7119140625
Iteration 5900: Loss = -12316.7060546875
Iteration 6000: Loss = -12316.701171875
Iteration 6100: Loss = -12316.693359375
Iteration 6200: Loss = -12316.6865234375
Iteration 6300: Loss = -12316.6806640625
Iteration 6400: Loss = -12316.6728515625
Iteration 6500: Loss = -12316.666015625
Iteration 6600: Loss = -12316.6591796875
Iteration 6700: Loss = -12316.650390625
Iteration 6800: Loss = -12316.64453125
Iteration 6900: Loss = -12316.6376953125
Iteration 7000: Loss = -12316.630859375
Iteration 7100: Loss = -12316.623046875
Iteration 7200: Loss = -12316.6162109375
Iteration 7300: Loss = -12316.5986328125
Iteration 7400: Loss = -12316.5888671875
Iteration 7500: Loss = -12316.5830078125
Iteration 7600: Loss = -12316.576171875
Iteration 7700: Loss = -12316.5712890625
Iteration 7800: Loss = -12316.5625
Iteration 7900: Loss = -12316.556640625
Iteration 8000: Loss = -12316.548828125
Iteration 8100: Loss = -12316.54296875
Iteration 8200: Loss = -12316.5361328125
Iteration 8300: Loss = -12316.5302734375
Iteration 8400: Loss = -12316.5244140625
Iteration 8500: Loss = -12316.515625
Iteration 8600: Loss = -12316.51171875
Iteration 8700: Loss = -12316.5078125
Iteration 8800: Loss = -12316.501953125
Iteration 8900: Loss = -12316.5
Iteration 9000: Loss = -12316.49609375
Iteration 9100: Loss = -12316.4921875
Iteration 9200: Loss = -12316.490234375
Iteration 9300: Loss = -12316.48828125
Iteration 9400: Loss = -12316.486328125
Iteration 9500: Loss = -12316.482421875
Iteration 9600: Loss = -12316.482421875
Iteration 9700: Loss = -12316.478515625
Iteration 9800: Loss = -12316.4775390625
Iteration 9900: Loss = -12316.4765625
Iteration 10000: Loss = -12316.47265625
Iteration 10100: Loss = -12316.4697265625
Iteration 10200: Loss = -12316.4658203125
Iteration 10300: Loss = -12316.462890625
Iteration 10400: Loss = -12316.4609375
Iteration 10500: Loss = -12316.45703125
Iteration 10600: Loss = -12316.4521484375
Iteration 10700: Loss = -12316.4453125
Iteration 10800: Loss = -12316.4423828125
Iteration 10900: Loss = -12316.43359375
Iteration 11000: Loss = -12316.4228515625
Iteration 11100: Loss = -12316.412109375
Iteration 11200: Loss = -12316.3974609375
Iteration 11300: Loss = -12316.3828125
Iteration 11400: Loss = -12316.369140625
Iteration 11500: Loss = -12316.3603515625
Iteration 11600: Loss = -12316.3564453125
Iteration 11700: Loss = -12316.353515625
Iteration 11800: Loss = -12316.353515625
Iteration 11900: Loss = -12316.353515625
Iteration 12000: Loss = -12316.3525390625
Iteration 12100: Loss = -12316.3515625
Iteration 12200: Loss = -12316.3515625
Iteration 12300: Loss = -12316.3525390625
1
Iteration 12400: Loss = -12316.3525390625
2
Iteration 12500: Loss = -12316.3505859375
Iteration 12600: Loss = -12316.3525390625
1
Iteration 12700: Loss = -12316.3515625
2
Iteration 12800: Loss = -12316.3525390625
3
Iteration 12900: Loss = -12316.3505859375
Iteration 13000: Loss = -12316.3515625
1
Iteration 13100: Loss = -12316.3505859375
Iteration 13200: Loss = -12316.3515625
1
Iteration 13300: Loss = -12316.3515625
2
Iteration 13400: Loss = -12316.3515625
3
Iteration 13500: Loss = -12316.349609375
Iteration 13600: Loss = -12316.349609375
Iteration 13700: Loss = -12316.349609375
Iteration 13800: Loss = -12316.3515625
1
Iteration 13900: Loss = -12316.3515625
2
Iteration 14000: Loss = -12316.3515625
3
Iteration 14100: Loss = -12316.3505859375
4
Iteration 14200: Loss = -12316.349609375
Iteration 14300: Loss = -12316.3515625
1
Iteration 14400: Loss = -12316.3505859375
2
Iteration 14500: Loss = -12316.3505859375
3
Iteration 14600: Loss = -12316.349609375
Iteration 14700: Loss = -12316.3505859375
1
Iteration 14800: Loss = -12316.3505859375
2
Iteration 14900: Loss = -12316.349609375
Iteration 15000: Loss = -12316.349609375
Iteration 15100: Loss = -12316.3505859375
1
Iteration 15200: Loss = -12316.3505859375
2
Iteration 15300: Loss = -12316.349609375
Iteration 15400: Loss = -12316.349609375
Iteration 15500: Loss = -12316.349609375
Iteration 15600: Loss = -12316.3505859375
1
Iteration 15700: Loss = -12316.349609375
Iteration 15800: Loss = -12316.349609375
Iteration 15900: Loss = -12316.3505859375
1
Iteration 16000: Loss = -12316.3505859375
2
Iteration 16100: Loss = -12316.349609375
Iteration 16200: Loss = -12316.3515625
1
Iteration 16300: Loss = -12316.3486328125
Iteration 16400: Loss = -12316.3505859375
1
Iteration 16500: Loss = -12316.349609375
2
Iteration 16600: Loss = -12316.3515625
3
Iteration 16700: Loss = -12316.349609375
4
Iteration 16800: Loss = -12316.3505859375
5
Iteration 16900: Loss = -12316.3505859375
6
Iteration 17000: Loss = -12316.349609375
7
Iteration 17100: Loss = -12316.3505859375
8
Iteration 17200: Loss = -12316.349609375
9
Iteration 17300: Loss = -12316.3505859375
10
Iteration 17400: Loss = -12316.349609375
11
Iteration 17500: Loss = -12316.3505859375
12
Iteration 17600: Loss = -12316.349609375
13
Iteration 17700: Loss = -12316.349609375
14
Iteration 17800: Loss = -12316.349609375
15
Stopping early at iteration 17800 due to no improvement.
pi: tensor([[0.5131, 0.4869],
        [0.1398, 0.8602]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9976, 0.0024], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2038, 0.2022],
         [0.9889, 0.1929]],

        [[0.1866, 0.1993],
         [0.8036, 0.4134]],

        [[0.8616, 0.2014],
         [0.4648, 0.0133]],

        [[0.1841, 0.1866],
         [0.9879, 0.9477]],

        [[0.9739, 0.2064],
         [0.2336, 0.0080]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.008903356543859271
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00024221373267521354
Average Adjusted Rand Index: -0.0017806713087718542
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24484.08984375
Iteration 100: Loss = -17169.62890625
Iteration 200: Loss = -13315.94921875
Iteration 300: Loss = -12593.4365234375
Iteration 400: Loss = -12466.5888671875
Iteration 500: Loss = -12404.60546875
Iteration 600: Loss = -12378.3818359375
Iteration 700: Loss = -12363.611328125
Iteration 800: Loss = -12353.9892578125
Iteration 900: Loss = -12347.25
Iteration 1000: Loss = -12342.3046875
Iteration 1100: Loss = -12338.5400390625
Iteration 1200: Loss = -12335.5966796875
Iteration 1300: Loss = -12333.23828125
Iteration 1400: Loss = -12331.3173828125
Iteration 1500: Loss = -12329.7216796875
Iteration 1600: Loss = -12328.3837890625
Iteration 1700: Loss = -12327.2529296875
Iteration 1800: Loss = -12326.28125
Iteration 1900: Loss = -12325.443359375
Iteration 2000: Loss = -12324.71484375
Iteration 2100: Loss = -12324.076171875
Iteration 2200: Loss = -12323.51171875
Iteration 2300: Loss = -12323.009765625
Iteration 2400: Loss = -12322.5634765625
Iteration 2500: Loss = -12322.162109375
Iteration 2600: Loss = -12321.798828125
Iteration 2700: Loss = -12321.47265625
Iteration 2800: Loss = -12321.1787109375
Iteration 2900: Loss = -12320.908203125
Iteration 3000: Loss = -12320.6640625
Iteration 3100: Loss = -12320.4443359375
Iteration 3200: Loss = -12320.244140625
Iteration 3300: Loss = -12320.0654296875
Iteration 3400: Loss = -12319.8984375
Iteration 3500: Loss = -12319.748046875
Iteration 3600: Loss = -12319.6064453125
Iteration 3700: Loss = -12319.4755859375
Iteration 3800: Loss = -12319.3544921875
Iteration 3900: Loss = -12319.2353515625
Iteration 4000: Loss = -12319.119140625
Iteration 4100: Loss = -12319.0029296875
Iteration 4200: Loss = -12318.8857421875
Iteration 4300: Loss = -12318.7607421875
Iteration 4400: Loss = -12318.6220703125
Iteration 4500: Loss = -12318.4677734375
Iteration 4600: Loss = -12318.3017578125
Iteration 4700: Loss = -12318.1357421875
Iteration 4800: Loss = -12317.986328125
Iteration 4900: Loss = -12317.8642578125
Iteration 5000: Loss = -12317.7685546875
Iteration 5100: Loss = -12317.6875
Iteration 5200: Loss = -12317.6171875
Iteration 5300: Loss = -12317.5556640625
Iteration 5400: Loss = -12317.5
Iteration 5500: Loss = -12317.447265625
Iteration 5600: Loss = -12317.4013671875
Iteration 5700: Loss = -12317.3564453125
Iteration 5800: Loss = -12317.3154296875
Iteration 5900: Loss = -12317.2783203125
Iteration 6000: Loss = -12317.2431640625
Iteration 6100: Loss = -12317.2109375
Iteration 6200: Loss = -12317.1796875
Iteration 6300: Loss = -12317.1513671875
Iteration 6400: Loss = -12317.1220703125
Iteration 6500: Loss = -12317.09765625
Iteration 6600: Loss = -12317.072265625
Iteration 6700: Loss = -12317.0478515625
Iteration 6800: Loss = -12317.025390625
Iteration 6900: Loss = -12317.00390625
Iteration 7000: Loss = -12316.9833984375
Iteration 7100: Loss = -12316.9609375
Iteration 7200: Loss = -12316.9423828125
Iteration 7300: Loss = -12316.9248046875
Iteration 7400: Loss = -12316.90625
Iteration 7500: Loss = -12316.888671875
Iteration 7600: Loss = -12316.8701171875
Iteration 7700: Loss = -12316.8544921875
Iteration 7800: Loss = -12316.8369140625
Iteration 7900: Loss = -12316.818359375
Iteration 8000: Loss = -12316.802734375
Iteration 8100: Loss = -12316.78515625
Iteration 8200: Loss = -12316.7666015625
Iteration 8300: Loss = -12316.7470703125
Iteration 8400: Loss = -12316.7294921875
Iteration 8500: Loss = -12316.7099609375
Iteration 8600: Loss = -12316.6884765625
Iteration 8700: Loss = -12316.66796875
Iteration 8800: Loss = -12316.64453125
Iteration 8900: Loss = -12316.6201171875
Iteration 9000: Loss = -12316.5947265625
Iteration 9100: Loss = -12316.5654296875
Iteration 9200: Loss = -12316.53515625
Iteration 9300: Loss = -12316.5029296875
Iteration 9400: Loss = -12316.4697265625
Iteration 9500: Loss = -12316.4365234375
Iteration 9600: Loss = -12316.4013671875
Iteration 9700: Loss = -12316.3671875
Iteration 9800: Loss = -12316.3310546875
Iteration 9900: Loss = -12316.298828125
Iteration 10000: Loss = -12316.26171875
Iteration 10100: Loss = -12316.23046875
Iteration 10200: Loss = -12316.1982421875
Iteration 10300: Loss = -12316.1689453125
Iteration 10400: Loss = -12316.140625
Iteration 10500: Loss = -12316.11328125
Iteration 10600: Loss = -12316.0859375
Iteration 10700: Loss = -12316.06640625
Iteration 10800: Loss = -12316.037109375
Iteration 10900: Loss = -12316.017578125
Iteration 11000: Loss = -12315.9970703125
Iteration 11100: Loss = -12315.9765625
Iteration 11200: Loss = -12315.9599609375
Iteration 11300: Loss = -12315.9443359375
Iteration 11400: Loss = -12315.927734375
Iteration 11500: Loss = -12315.916015625
Iteration 11600: Loss = -12315.9013671875
Iteration 11700: Loss = -12315.888671875
Iteration 11800: Loss = -12315.87890625
Iteration 11900: Loss = -12315.8681640625
Iteration 12000: Loss = -12315.859375
Iteration 12100: Loss = -12315.8505859375
Iteration 12200: Loss = -12315.84375
Iteration 12300: Loss = -12315.8359375
Iteration 12400: Loss = -12315.830078125
Iteration 12500: Loss = -12315.822265625
Iteration 12600: Loss = -12315.8193359375
Iteration 12700: Loss = -12315.8115234375
Iteration 12800: Loss = -12315.8076171875
Iteration 12900: Loss = -12315.80078125
Iteration 13000: Loss = -12315.7978515625
Iteration 13100: Loss = -12315.7939453125
Iteration 13200: Loss = -12315.7919921875
Iteration 13300: Loss = -12315.7880859375
Iteration 13400: Loss = -12315.7841796875
Iteration 13500: Loss = -12315.78125
Iteration 13600: Loss = -12315.779296875
Iteration 13700: Loss = -12315.7763671875
Iteration 13800: Loss = -12315.775390625
Iteration 13900: Loss = -12315.7734375
Iteration 14000: Loss = -12315.7685546875
Iteration 14100: Loss = -12315.7685546875
Iteration 14200: Loss = -12315.765625
Iteration 14300: Loss = -12315.763671875
Iteration 14400: Loss = -12315.7626953125
Iteration 14500: Loss = -12315.7607421875
Iteration 14600: Loss = -12315.76171875
1
Iteration 14700: Loss = -12315.759765625
Iteration 14800: Loss = -12315.7578125
Iteration 14900: Loss = -12315.7578125
Iteration 15000: Loss = -12315.7548828125
Iteration 15100: Loss = -12315.7548828125
Iteration 15200: Loss = -12315.75390625
Iteration 15300: Loss = -12315.7529296875
Iteration 15400: Loss = -12315.751953125
Iteration 15500: Loss = -12315.75
Iteration 15600: Loss = -12315.7529296875
1
Iteration 15700: Loss = -12315.75
Iteration 15800: Loss = -12315.748046875
Iteration 15900: Loss = -12315.748046875
Iteration 16000: Loss = -12315.7490234375
1
Iteration 16100: Loss = -12315.74609375
Iteration 16200: Loss = -12315.74609375
Iteration 16300: Loss = -12315.7451171875
Iteration 16400: Loss = -12315.7451171875
Iteration 16500: Loss = -12315.74609375
1
Iteration 16600: Loss = -12315.74609375
2
Iteration 16700: Loss = -12315.7451171875
Iteration 16800: Loss = -12315.744140625
Iteration 16900: Loss = -12315.7431640625
Iteration 17000: Loss = -12315.7431640625
Iteration 17100: Loss = -12315.7431640625
Iteration 17200: Loss = -12315.7421875
Iteration 17300: Loss = -12315.7431640625
1
Iteration 17400: Loss = -12315.7431640625
2
Iteration 17500: Loss = -12315.7421875
Iteration 17600: Loss = -12315.7421875
Iteration 17700: Loss = -12315.7421875
Iteration 17800: Loss = -12315.740234375
Iteration 17900: Loss = -12315.7421875
1
Iteration 18000: Loss = -12315.7412109375
2
Iteration 18100: Loss = -12315.740234375
Iteration 18200: Loss = -12315.740234375
Iteration 18300: Loss = -12315.7431640625
1
Iteration 18400: Loss = -12315.7392578125
Iteration 18500: Loss = -12315.73828125
Iteration 18600: Loss = -12315.7412109375
1
Iteration 18700: Loss = -12315.740234375
2
Iteration 18800: Loss = -12315.7392578125
3
Iteration 18900: Loss = -12315.7412109375
4
Iteration 19000: Loss = -12315.740234375
5
Iteration 19100: Loss = -12315.740234375
6
Iteration 19200: Loss = -12315.7392578125
7
Iteration 19300: Loss = -12315.740234375
8
Iteration 19400: Loss = -12315.73828125
Iteration 19500: Loss = -12315.7373046875
Iteration 19600: Loss = -12315.73828125
1
Iteration 19700: Loss = -12315.73828125
2
Iteration 19800: Loss = -12315.740234375
3
Iteration 19900: Loss = -12315.73828125
4
Iteration 20000: Loss = -12315.7373046875
Iteration 20100: Loss = -12315.73828125
1
Iteration 20200: Loss = -12315.7373046875
Iteration 20300: Loss = -12315.7373046875
Iteration 20400: Loss = -12315.740234375
1
Iteration 20500: Loss = -12315.7373046875
Iteration 20600: Loss = -12315.7373046875
Iteration 20700: Loss = -12315.7392578125
1
Iteration 20800: Loss = -12315.73828125
2
Iteration 20900: Loss = -12315.7373046875
Iteration 21000: Loss = -12315.736328125
Iteration 21100: Loss = -12315.73828125
1
Iteration 21200: Loss = -12315.7353515625
Iteration 21300: Loss = -12315.66015625
Iteration 21400: Loss = -12315.66015625
Iteration 21500: Loss = -12315.658203125
Iteration 21600: Loss = -12315.6611328125
1
Iteration 21700: Loss = -12315.6591796875
2
Iteration 21800: Loss = -12315.6591796875
3
Iteration 21900: Loss = -12315.6591796875
4
Iteration 22000: Loss = -12315.658203125
Iteration 22100: Loss = -12315.6591796875
1
Iteration 22200: Loss = -12315.658203125
Iteration 22300: Loss = -12315.662109375
1
Iteration 22400: Loss = -12315.6591796875
2
Iteration 22500: Loss = -12315.66015625
3
Iteration 22600: Loss = -12315.658203125
Iteration 22700: Loss = -12315.6591796875
1
Iteration 22800: Loss = -12315.658203125
Iteration 22900: Loss = -12315.6591796875
1
Iteration 23000: Loss = -12315.65625
Iteration 23100: Loss = -12315.658203125
1
Iteration 23200: Loss = -12315.6591796875
2
Iteration 23300: Loss = -12315.658203125
3
Iteration 23400: Loss = -12315.658203125
4
Iteration 23500: Loss = -12315.6591796875
5
Iteration 23600: Loss = -12315.658203125
6
Iteration 23700: Loss = -12315.658203125
7
Iteration 23800: Loss = -12315.6591796875
8
Iteration 23900: Loss = -12315.66015625
9
Iteration 24000: Loss = -12315.6572265625
10
Iteration 24100: Loss = -12315.6591796875
11
Iteration 24200: Loss = -12315.658203125
12
Iteration 24300: Loss = -12315.658203125
13
Iteration 24400: Loss = -12315.6591796875
14
Iteration 24500: Loss = -12315.662109375
15
Stopping early at iteration 24500 due to no improvement.
pi: tensor([[9.9653e-01, 3.4697e-03],
        [7.0034e-05, 9.9993e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9954, 0.0046], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1991, 0.1818],
         [0.4399, 0.9998]],

        [[0.6307, 0.1616],
         [0.0323, 0.9922]],

        [[0.9640, 0.2828],
         [0.1955, 0.9316]],

        [[0.9210, 0.1427],
         [0.9377, 0.9818]],

        [[0.8536, 0.1028],
         [0.5104, 0.0123]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.005453571602473584
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.00776683106263838
Global Adjusted Rand Index: -0.00038791679950578474
Average Adjusted Rand Index: 0.0038063611465970703
[-0.00024221373267521354, -0.00038791679950578474] [-0.0017806713087718542, 0.0038063611465970703] [12316.349609375, 12315.662109375]
-------------------------------------
This iteration is 76
True Objective function: Loss = -11880.646339460178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30133.580078125
Iteration 100: Loss = -20728.34375
Iteration 200: Loss = -14190.833984375
Iteration 300: Loss = -12971.001953125
Iteration 400: Loss = -12705.595703125
Iteration 500: Loss = -12617.994140625
Iteration 600: Loss = -12576.6953125
Iteration 700: Loss = -12551.8173828125
Iteration 800: Loss = -12534.806640625
Iteration 900: Loss = -12521.5166015625
Iteration 1000: Loss = -12511.2109375
Iteration 1100: Loss = -12504.431640625
Iteration 1200: Loss = -12498.642578125
Iteration 1300: Loss = -12490.4375
Iteration 1400: Loss = -12484.544921875
Iteration 1500: Loss = -12480.841796875
Iteration 1600: Loss = -12477.5869140625
Iteration 1700: Loss = -12474.8310546875
Iteration 1800: Loss = -12472.05859375
Iteration 1900: Loss = -12469.3486328125
Iteration 2000: Loss = -12464.421875
Iteration 2100: Loss = -12460.8037109375
Iteration 2200: Loss = -12457.96484375
Iteration 2300: Loss = -12454.47265625
Iteration 2400: Loss = -12448.681640625
Iteration 2500: Loss = -12440.3818359375
Iteration 2600: Loss = -12429.4873046875
Iteration 2700: Loss = -12420.765625
Iteration 2800: Loss = -12413.1806640625
Iteration 2900: Loss = -12397.1279296875
Iteration 3000: Loss = -12386.21484375
Iteration 3100: Loss = -12380.9638671875
Iteration 3200: Loss = -12378.8154296875
Iteration 3300: Loss = -12377.5380859375
Iteration 3400: Loss = -12376.638671875
Iteration 3500: Loss = -12375.951171875
Iteration 3600: Loss = -12375.3974609375
Iteration 3700: Loss = -12374.9384765625
Iteration 3800: Loss = -12374.5517578125
Iteration 3900: Loss = -12374.21484375
Iteration 4000: Loss = -12373.9208984375
Iteration 4100: Loss = -12373.662109375
Iteration 4200: Loss = -12373.4296875
Iteration 4300: Loss = -12373.22265625
Iteration 4400: Loss = -12373.03515625
Iteration 4500: Loss = -12372.865234375
Iteration 4600: Loss = -12372.7109375
Iteration 4700: Loss = -12372.568359375
Iteration 4800: Loss = -12372.439453125
Iteration 4900: Loss = -12372.318359375
Iteration 5000: Loss = -12372.208984375
Iteration 5100: Loss = -12372.107421875
Iteration 5200: Loss = -12372.013671875
Iteration 5300: Loss = -12371.92578125
Iteration 5400: Loss = -12371.84375
Iteration 5500: Loss = -12371.7685546875
Iteration 5600: Loss = -12371.69921875
Iteration 5700: Loss = -12371.6328125
Iteration 5800: Loss = -12371.572265625
Iteration 5900: Loss = -12371.5146484375
Iteration 6000: Loss = -12371.4619140625
Iteration 6100: Loss = -12371.4111328125
Iteration 6200: Loss = -12371.36328125
Iteration 6300: Loss = -12371.318359375
Iteration 6400: Loss = -12371.2763671875
Iteration 6500: Loss = -12371.2373046875
Iteration 6600: Loss = -12371.19921875
Iteration 6700: Loss = -12371.1640625
Iteration 6800: Loss = -12371.1328125
Iteration 6900: Loss = -12371.1015625
Iteration 7000: Loss = -12371.07421875
Iteration 7100: Loss = -12371.046875
Iteration 7200: Loss = -12371.021484375
Iteration 7300: Loss = -12370.9970703125
Iteration 7400: Loss = -12370.9755859375
Iteration 7500: Loss = -12370.953125
Iteration 7600: Loss = -12370.9306640625
Iteration 7700: Loss = -12370.9130859375
Iteration 7800: Loss = -12370.8955078125
Iteration 7900: Loss = -12370.8759765625
Iteration 8000: Loss = -12370.8603515625
Iteration 8100: Loss = -12370.84375
Iteration 8200: Loss = -12370.8291015625
Iteration 8300: Loss = -12370.8173828125
Iteration 8400: Loss = -12370.8037109375
Iteration 8500: Loss = -12370.791015625
Iteration 8600: Loss = -12370.7783203125
Iteration 8700: Loss = -12370.7685546875
Iteration 8800: Loss = -12370.7578125
Iteration 8900: Loss = -12370.748046875
Iteration 9000: Loss = -12370.7392578125
Iteration 9100: Loss = -12370.73046875
Iteration 9200: Loss = -12370.720703125
Iteration 9300: Loss = -12370.7138671875
Iteration 9400: Loss = -12370.7041015625
Iteration 9500: Loss = -12370.6982421875
Iteration 9600: Loss = -12370.69140625
Iteration 9700: Loss = -12370.6845703125
Iteration 9800: Loss = -12370.677734375
Iteration 9900: Loss = -12370.671875
Iteration 10000: Loss = -12370.6669921875
Iteration 10100: Loss = -12370.6611328125
Iteration 10200: Loss = -12370.6552734375
Iteration 10300: Loss = -12370.65234375
Iteration 10400: Loss = -12370.6455078125
Iteration 10500: Loss = -12370.642578125
Iteration 10600: Loss = -12370.6376953125
Iteration 10700: Loss = -12370.638671875
1
Iteration 10800: Loss = -12370.630859375
Iteration 10900: Loss = -12370.6279296875
Iteration 11000: Loss = -12370.625
Iteration 11100: Loss = -12370.6201171875
Iteration 11200: Loss = -12370.6181640625
Iteration 11300: Loss = -12370.6142578125
Iteration 11400: Loss = -12370.6123046875
Iteration 11500: Loss = -12370.609375
Iteration 11600: Loss = -12370.6064453125
Iteration 11700: Loss = -12370.60546875
Iteration 11800: Loss = -12370.6015625
Iteration 11900: Loss = -12370.5986328125
Iteration 12000: Loss = -12370.599609375
1
Iteration 12100: Loss = -12370.595703125
Iteration 12200: Loss = -12370.59375
Iteration 12300: Loss = -12370.591796875
Iteration 12400: Loss = -12370.5908203125
Iteration 12500: Loss = -12370.58984375
Iteration 12600: Loss = -12370.587890625
Iteration 12700: Loss = -12370.5859375
Iteration 12800: Loss = -12370.5849609375
Iteration 12900: Loss = -12370.5849609375
Iteration 13000: Loss = -12370.5830078125
Iteration 13100: Loss = -12370.5810546875
Iteration 13200: Loss = -12370.580078125
Iteration 13300: Loss = -12370.580078125
Iteration 13400: Loss = -12370.5791015625
Iteration 13500: Loss = -12370.576171875
Iteration 13600: Loss = -12370.576171875
Iteration 13700: Loss = -12370.576171875
Iteration 13800: Loss = -12370.576171875
Iteration 13900: Loss = -12370.57421875
Iteration 14000: Loss = -12370.57421875
Iteration 14100: Loss = -12370.5732421875
Iteration 14200: Loss = -12370.572265625
Iteration 14300: Loss = -12370.572265625
Iteration 14400: Loss = -12370.5703125
Iteration 14500: Loss = -12370.5703125
Iteration 14600: Loss = -12370.5703125
Iteration 14700: Loss = -12370.568359375
Iteration 14800: Loss = -12370.568359375
Iteration 14900: Loss = -12370.568359375
Iteration 15000: Loss = -12370.568359375
Iteration 15100: Loss = -12370.56640625
Iteration 15200: Loss = -12370.56640625
Iteration 15300: Loss = -12370.5654296875
Iteration 15400: Loss = -12370.56640625
1
Iteration 15500: Loss = -12370.564453125
Iteration 15600: Loss = -12370.5654296875
1
Iteration 15700: Loss = -12370.5673828125
2
Iteration 15800: Loss = -12370.5654296875
3
Iteration 15900: Loss = -12370.564453125
Iteration 16000: Loss = -12370.564453125
Iteration 16100: Loss = -12370.5654296875
1
Iteration 16200: Loss = -12370.564453125
Iteration 16300: Loss = -12370.5634765625
Iteration 16400: Loss = -12370.5634765625
Iteration 16500: Loss = -12370.5634765625
Iteration 16600: Loss = -12370.5634765625
Iteration 16700: Loss = -12370.5625
Iteration 16800: Loss = -12370.5634765625
1
Iteration 16900: Loss = -12370.5634765625
2
Iteration 17000: Loss = -12370.5615234375
Iteration 17100: Loss = -12370.5625
1
Iteration 17200: Loss = -12370.5625
2
Iteration 17300: Loss = -12370.5625
3
Iteration 17400: Loss = -12370.5625
4
Iteration 17500: Loss = -12370.5615234375
Iteration 17600: Loss = -12370.560546875
Iteration 17700: Loss = -12370.5615234375
1
Iteration 17800: Loss = -12370.5625
2
Iteration 17900: Loss = -12370.560546875
Iteration 18000: Loss = -12370.5615234375
1
Iteration 18100: Loss = -12370.5615234375
2
Iteration 18200: Loss = -12370.5615234375
3
Iteration 18300: Loss = -12370.560546875
Iteration 18400: Loss = -12370.5595703125
Iteration 18500: Loss = -12370.560546875
1
Iteration 18600: Loss = -12370.5625
2
Iteration 18700: Loss = -12370.5595703125
Iteration 18800: Loss = -12370.5595703125
Iteration 18900: Loss = -12370.55859375
Iteration 19000: Loss = -12370.560546875
1
Iteration 19100: Loss = -12370.55859375
Iteration 19200: Loss = -12370.5595703125
1
Iteration 19300: Loss = -12370.5615234375
2
Iteration 19400: Loss = -12370.5615234375
3
Iteration 19500: Loss = -12370.5595703125
4
Iteration 19600: Loss = -12370.5595703125
5
Iteration 19700: Loss = -12370.560546875
6
Iteration 19800: Loss = -12370.560546875
7
Iteration 19900: Loss = -12370.5625
8
Iteration 20000: Loss = -12370.5595703125
9
Iteration 20100: Loss = -12370.5595703125
10
Iteration 20200: Loss = -12370.560546875
11
Iteration 20300: Loss = -12370.55859375
Iteration 20400: Loss = -12370.5595703125
1
Iteration 20500: Loss = -12370.55859375
Iteration 20600: Loss = -12370.560546875
1
Iteration 20700: Loss = -12370.5615234375
2
Iteration 20800: Loss = -12370.5595703125
3
Iteration 20900: Loss = -12370.5615234375
4
Iteration 21000: Loss = -12370.5595703125
5
Iteration 21100: Loss = -12370.5595703125
6
Iteration 21200: Loss = -12370.560546875
7
Iteration 21300: Loss = -12370.5595703125
8
Iteration 21400: Loss = -12370.5595703125
9
Iteration 21500: Loss = -12370.560546875
10
Iteration 21600: Loss = -12370.560546875
11
Iteration 21700: Loss = -12370.5615234375
12
Iteration 21800: Loss = -12370.560546875
13
Iteration 21900: Loss = -12370.560546875
14
Iteration 22000: Loss = -12370.5615234375
15
Stopping early at iteration 22000 due to no improvement.
pi: tensor([[9.9999e-01, 5.7376e-06],
        [1.0000e+00, 1.4017e-07]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5321, 0.4679], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2096, 0.0963],
         [0.0255, 0.2807]],

        [[0.0541, 0.6551],
         [0.3049, 0.1152]],

        [[0.5756, 0.2342],
         [0.9415, 0.1572]],

        [[0.0111, 0.2156],
         [0.9677, 0.8799]],

        [[0.9826, 0.1099],
         [0.9366, 0.9847]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.05361372965741745
Average Adjusted Rand Index: 0.19199956116356304
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -32997.609375
Iteration 100: Loss = -22129.251953125
Iteration 200: Loss = -15029.01953125
Iteration 300: Loss = -13192.076171875
Iteration 400: Loss = -12816.70703125
Iteration 500: Loss = -12695.0322265625
Iteration 600: Loss = -12613.2548828125
Iteration 700: Loss = -12566.0087890625
Iteration 800: Loss = -12546.71484375
Iteration 900: Loss = -12529.7900390625
Iteration 1000: Loss = -12509.8203125
Iteration 1100: Loss = -12495.4736328125
Iteration 1200: Loss = -12485.5302734375
Iteration 1300: Loss = -12477.435546875
Iteration 1400: Loss = -12468.576171875
Iteration 1500: Loss = -12459.4814453125
Iteration 1600: Loss = -12454.783203125
Iteration 1700: Loss = -12452.0615234375
Iteration 1800: Loss = -12450.068359375
Iteration 1900: Loss = -12448.4912109375
Iteration 2000: Loss = -12447.1865234375
Iteration 2100: Loss = -12446.08203125
Iteration 2200: Loss = -12445.1181640625
Iteration 2300: Loss = -12444.2470703125
Iteration 2400: Loss = -12443.4443359375
Iteration 2500: Loss = -12442.7109375
Iteration 2600: Loss = -12441.7890625
Iteration 2700: Loss = -12440.076171875
Iteration 2800: Loss = -12439.5224609375
Iteration 2900: Loss = -12438.7021484375
Iteration 3000: Loss = -12435.740234375
Iteration 3100: Loss = -12435.291015625
Iteration 3200: Loss = -12434.923828125
Iteration 3300: Loss = -12434.6025390625
Iteration 3400: Loss = -12434.318359375
Iteration 3500: Loss = -12434.0615234375
Iteration 3600: Loss = -12433.828125
Iteration 3700: Loss = -12433.6142578125
Iteration 3800: Loss = -12433.4150390625
Iteration 3900: Loss = -12433.2255859375
Iteration 4000: Loss = -12433.033203125
Iteration 4100: Loss = -12432.822265625
Iteration 4200: Loss = -12432.625
Iteration 4300: Loss = -12432.4736328125
Iteration 4400: Loss = -12432.3486328125
Iteration 4500: Loss = -12432.2392578125
Iteration 4600: Loss = -12432.13671875
Iteration 4700: Loss = -12432.04296875
Iteration 4800: Loss = -12431.9541015625
Iteration 4900: Loss = -12431.873046875
Iteration 5000: Loss = -12431.7978515625
Iteration 5100: Loss = -12431.7255859375
Iteration 5200: Loss = -12431.6572265625
Iteration 5300: Loss = -12431.591796875
Iteration 5400: Loss = -12431.525390625
Iteration 5500: Loss = -12431.4521484375
Iteration 5600: Loss = -12431.3544921875
Iteration 5700: Loss = -12431.212890625
Iteration 5800: Loss = -12431.1044921875
Iteration 5900: Loss = -12430.9892578125
Iteration 6000: Loss = -12430.87109375
Iteration 6100: Loss = -12430.8232421875
Iteration 6200: Loss = -12430.7822265625
Iteration 6300: Loss = -12430.7373046875
Iteration 6400: Loss = -12430.6962890625
Iteration 6500: Loss = -12430.6455078125
Iteration 6600: Loss = -12430.572265625
Iteration 6700: Loss = -12430.501953125
Iteration 6800: Loss = -12430.40234375
Iteration 6900: Loss = -12430.2294921875
Iteration 7000: Loss = -12430.09375
Iteration 7100: Loss = -12430.0048828125
Iteration 7200: Loss = -12429.93359375
Iteration 7300: Loss = -12429.8759765625
Iteration 7400: Loss = -12429.7431640625
Iteration 7500: Loss = -12428.2607421875
Iteration 7600: Loss = -12428.1845703125
Iteration 7700: Loss = -12428.142578125
Iteration 7800: Loss = -12428.11328125
Iteration 7900: Loss = -12428.0888671875
Iteration 8000: Loss = -12428.0703125
Iteration 8100: Loss = -12428.052734375
Iteration 8200: Loss = -12428.0361328125
Iteration 8300: Loss = -12428.021484375
Iteration 8400: Loss = -12428.005859375
Iteration 8500: Loss = -12427.9921875
Iteration 8600: Loss = -12427.9794921875
Iteration 8700: Loss = -12427.9609375
Iteration 8800: Loss = -12427.74609375
Iteration 8900: Loss = -12427.6982421875
Iteration 9000: Loss = -12427.666015625
Iteration 9100: Loss = -12427.642578125
Iteration 9200: Loss = -12427.6279296875
Iteration 9300: Loss = -12427.6162109375
Iteration 9400: Loss = -12427.6083984375
Iteration 9500: Loss = -12427.6005859375
Iteration 9600: Loss = -12427.591796875
Iteration 9700: Loss = -12427.5869140625
Iteration 9800: Loss = -12427.58203125
Iteration 9900: Loss = -12427.5751953125
Iteration 10000: Loss = -12427.5693359375
Iteration 10100: Loss = -12427.56640625
Iteration 10200: Loss = -12427.560546875
Iteration 10300: Loss = -12427.556640625
Iteration 10400: Loss = -12427.552734375
Iteration 10500: Loss = -12427.5498046875
Iteration 10600: Loss = -12427.546875
Iteration 10700: Loss = -12427.54296875
Iteration 10800: Loss = -12427.5419921875
Iteration 10900: Loss = -12427.537109375
Iteration 11000: Loss = -12427.53515625
Iteration 11100: Loss = -12427.5322265625
Iteration 11200: Loss = -12427.529296875
Iteration 11300: Loss = -12427.52734375
Iteration 11400: Loss = -12427.5263671875
Iteration 11500: Loss = -12427.5244140625
Iteration 11600: Loss = -12427.5224609375
Iteration 11700: Loss = -12427.521484375
Iteration 11800: Loss = -12427.5185546875
Iteration 11900: Loss = -12427.5166015625
Iteration 12000: Loss = -12427.5146484375
Iteration 12100: Loss = -12427.5146484375
Iteration 12200: Loss = -12427.51171875
Iteration 12300: Loss = -12427.51171875
Iteration 12400: Loss = -12427.5087890625
Iteration 12500: Loss = -12427.5078125
Iteration 12600: Loss = -12427.5078125
Iteration 12700: Loss = -12427.5068359375
Iteration 12800: Loss = -12427.505859375
Iteration 12900: Loss = -12427.50390625
Iteration 13000: Loss = -12427.5029296875
Iteration 13100: Loss = -12427.50390625
1
Iteration 13200: Loss = -12427.501953125
Iteration 13300: Loss = -12427.5029296875
1
Iteration 13400: Loss = -12427.4990234375
Iteration 13500: Loss = -12427.5
1
Iteration 13600: Loss = -12427.4990234375
Iteration 13700: Loss = -12427.498046875
Iteration 13800: Loss = -12427.49609375
Iteration 13900: Loss = -12427.498046875
1
Iteration 14000: Loss = -12427.498046875
2
Iteration 14100: Loss = -12427.4970703125
3
Iteration 14200: Loss = -12427.49609375
Iteration 14300: Loss = -12427.4951171875
Iteration 14400: Loss = -12427.494140625
Iteration 14500: Loss = -12427.4951171875
1
Iteration 14600: Loss = -12427.4931640625
Iteration 14700: Loss = -12427.4931640625
Iteration 14800: Loss = -12427.4931640625
Iteration 14900: Loss = -12427.4931640625
Iteration 15000: Loss = -12427.4931640625
Iteration 15100: Loss = -12427.4921875
Iteration 15200: Loss = -12427.4931640625
1
Iteration 15300: Loss = -12427.4912109375
Iteration 15400: Loss = -12427.4921875
1
Iteration 15500: Loss = -12427.4912109375
Iteration 15600: Loss = -12427.490234375
Iteration 15700: Loss = -12427.4921875
1
Iteration 15800: Loss = -12427.4912109375
2
Iteration 15900: Loss = -12427.4921875
3
Iteration 16000: Loss = -12427.490234375
Iteration 16100: Loss = -12427.48828125
Iteration 16200: Loss = -12427.490234375
1
Iteration 16300: Loss = -12427.490234375
2
Iteration 16400: Loss = -12427.48828125
Iteration 16500: Loss = -12427.48828125
Iteration 16600: Loss = -12427.486328125
Iteration 16700: Loss = -12427.423828125
Iteration 16800: Loss = -12427.421875
Iteration 16900: Loss = -12427.421875
Iteration 17000: Loss = -12427.421875
Iteration 17100: Loss = -12427.4228515625
1
Iteration 17200: Loss = -12427.4208984375
Iteration 17300: Loss = -12427.4189453125
Iteration 17400: Loss = -12427.3359375
Iteration 17500: Loss = -12427.3251953125
Iteration 17600: Loss = -12427.3232421875
Iteration 17700: Loss = -12427.3134765625
Iteration 17800: Loss = -12427.310546875
Iteration 17900: Loss = -12427.3115234375
1
Iteration 18000: Loss = -12427.30859375
Iteration 18100: Loss = -12427.2919921875
Iteration 18200: Loss = -12427.2734375
Iteration 18300: Loss = -12427.1689453125
Iteration 18400: Loss = -12427.0869140625
Iteration 18500: Loss = -12427.056640625
Iteration 18600: Loss = -12427.0224609375
Iteration 18700: Loss = -12426.9423828125
Iteration 18800: Loss = -12426.47265625
Iteration 18900: Loss = -12360.1259765625
Iteration 19000: Loss = -12297.763671875
Iteration 19100: Loss = -12283.0400390625
Iteration 19200: Loss = -12236.4990234375
Iteration 19300: Loss = -12199.2197265625
Iteration 19400: Loss = -12157.046875
Iteration 19500: Loss = -12101.58984375
Iteration 19600: Loss = -12085.814453125
Iteration 19700: Loss = -12062.564453125
Iteration 19800: Loss = -12055.205078125
Iteration 19900: Loss = -12049.6787109375
Iteration 20000: Loss = -12047.568359375
Iteration 20100: Loss = -12015.6171875
Iteration 20200: Loss = -11989.2880859375
Iteration 20300: Loss = -11978.03515625
Iteration 20400: Loss = -11972.796875
Iteration 20500: Loss = -11968.8173828125
Iteration 20600: Loss = -11953.650390625
Iteration 20700: Loss = -11953.2041015625
Iteration 20800: Loss = -11943.80078125
Iteration 20900: Loss = -11941.7421875
Iteration 21000: Loss = -11931.46875
Iteration 21100: Loss = -11931.33203125
Iteration 21200: Loss = -11931.2607421875
Iteration 21300: Loss = -11925.62109375
Iteration 21400: Loss = -11925.380859375
Iteration 21500: Loss = -11925.3359375
Iteration 21600: Loss = -11925.306640625
Iteration 21700: Loss = -11925.283203125
Iteration 21800: Loss = -11925.2646484375
Iteration 21900: Loss = -11925.2490234375
Iteration 22000: Loss = -11925.2353515625
Iteration 22100: Loss = -11925.22265625
Iteration 22200: Loss = -11925.2119140625
Iteration 22300: Loss = -11925.2021484375
Iteration 22400: Loss = -11925.1943359375
Iteration 22500: Loss = -11925.1875
Iteration 22600: Loss = -11925.1796875
Iteration 22700: Loss = -11925.173828125
Iteration 22800: Loss = -11918.21875
Iteration 22900: Loss = -11915.55859375
Iteration 23000: Loss = -11915.5107421875
Iteration 23100: Loss = -11914.2705078125
Iteration 23200: Loss = -11903.1337890625
Iteration 23300: Loss = -11903.0419921875
Iteration 23400: Loss = -11903.00390625
Iteration 23500: Loss = -11902.982421875
Iteration 23600: Loss = -11902.9677734375
Iteration 23700: Loss = -11902.9580078125
Iteration 23800: Loss = -11902.9501953125
Iteration 23900: Loss = -11902.94140625
Iteration 24000: Loss = -11902.9365234375
Iteration 24100: Loss = -11902.9326171875
Iteration 24200: Loss = -11902.9267578125
Iteration 24300: Loss = -11902.923828125
Iteration 24400: Loss = -11902.919921875
Iteration 24500: Loss = -11902.916015625
Iteration 24600: Loss = -11902.9140625
Iteration 24700: Loss = -11902.9111328125
Iteration 24800: Loss = -11902.908203125
Iteration 24900: Loss = -11902.90625
Iteration 25000: Loss = -11902.8125
Iteration 25100: Loss = -11892.48828125
Iteration 25200: Loss = -11892.3955078125
Iteration 25300: Loss = -11892.3662109375
Iteration 25400: Loss = -11892.3525390625
Iteration 25500: Loss = -11892.34375
Iteration 25600: Loss = -11892.337890625
Iteration 25700: Loss = -11892.3330078125
Iteration 25800: Loss = -11892.328125
Iteration 25900: Loss = -11892.326171875
Iteration 26000: Loss = -11892.32421875
Iteration 26100: Loss = -11892.3203125
Iteration 26200: Loss = -11892.318359375
Iteration 26300: Loss = -11892.318359375
Iteration 26400: Loss = -11892.31640625
Iteration 26500: Loss = -11892.3154296875
Iteration 26600: Loss = -11892.3134765625
Iteration 26700: Loss = -11892.3134765625
Iteration 26800: Loss = -11892.3115234375
Iteration 26900: Loss = -11892.3115234375
Iteration 27000: Loss = -11892.3095703125
Iteration 27100: Loss = -11892.3095703125
Iteration 27200: Loss = -11892.30859375
Iteration 27300: Loss = -11892.30859375
Iteration 27400: Loss = -11892.3076171875
Iteration 27500: Loss = -11892.306640625
Iteration 27600: Loss = -11892.306640625
Iteration 27700: Loss = -11892.3046875
Iteration 27800: Loss = -11892.3046875
Iteration 27900: Loss = -11892.3056640625
1
Iteration 28000: Loss = -11892.3046875
Iteration 28100: Loss = -11892.302734375
Iteration 28200: Loss = -11892.302734375
Iteration 28300: Loss = -11892.302734375
Iteration 28400: Loss = -11892.3017578125
Iteration 28500: Loss = -11892.3037109375
1
Iteration 28600: Loss = -11892.302734375
2
Iteration 28700: Loss = -11892.302734375
3
Iteration 28800: Loss = -11892.3017578125
Iteration 28900: Loss = -11892.3017578125
Iteration 29000: Loss = -11892.30078125
Iteration 29100: Loss = -11892.2998046875
Iteration 29200: Loss = -11892.298828125
Iteration 29300: Loss = -11892.2998046875
1
Iteration 29400: Loss = -11892.2998046875
2
Iteration 29500: Loss = -11892.30078125
3
Iteration 29600: Loss = -11892.298828125
Iteration 29700: Loss = -11892.298828125
Iteration 29800: Loss = -11892.298828125
Iteration 29900: Loss = -11892.298828125
pi: tensor([[0.7847, 0.2153],
        [0.2460, 0.7540]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5097, 0.4903], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3078, 0.0977],
         [0.0280, 0.2968]],

        [[0.0090, 0.0960],
         [0.2499, 0.0077]],

        [[0.5783, 0.1029],
         [0.0160, 0.0981]],

        [[0.1767, 0.1121],
         [0.0263, 0.2648]],

        [[0.1152, 0.0944],
         [0.1081, 0.9843]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.960320642057664
Average Adjusted Rand Index: 0.960158084092878
[0.05361372965741745, 0.960320642057664] [0.19199956116356304, 0.960158084092878] [12370.5615234375, 11892.298828125]
-------------------------------------
This iteration is 77
True Objective function: Loss = -11863.937621894991
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -43681.859375
Iteration 100: Loss = -26610.216796875
Iteration 200: Loss = -15894.1708984375
Iteration 300: Loss = -13455.607421875
Iteration 400: Loss = -13011.126953125
Iteration 500: Loss = -12834.25
Iteration 600: Loss = -12714.326171875
Iteration 700: Loss = -12630.5478515625
Iteration 800: Loss = -12588.25
Iteration 900: Loss = -12563.6708984375
Iteration 1000: Loss = -12541.701171875
Iteration 1100: Loss = -12533.015625
Iteration 1200: Loss = -12525.7021484375
Iteration 1300: Loss = -12518.6435546875
Iteration 1400: Loss = -12511.53515625
Iteration 1500: Loss = -12505.8173828125
Iteration 1600: Loss = -12502.1259765625
Iteration 1700: Loss = -12493.5166015625
Iteration 1800: Loss = -12491.15234375
Iteration 1900: Loss = -12489.4453125
Iteration 2000: Loss = -12488.03125
Iteration 2100: Loss = -12486.8193359375
Iteration 2200: Loss = -12485.76953125
Iteration 2300: Loss = -12484.84765625
Iteration 2400: Loss = -12484.0283203125
Iteration 2500: Loss = -12483.302734375
Iteration 2600: Loss = -12482.6494140625
Iteration 2700: Loss = -12482.05859375
Iteration 2800: Loss = -12481.52734375
Iteration 2900: Loss = -12481.0439453125
Iteration 3000: Loss = -12480.603515625
Iteration 3100: Loss = -12480.2001953125
Iteration 3200: Loss = -12479.8330078125
Iteration 3300: Loss = -12479.4921875
Iteration 3400: Loss = -12479.1787109375
Iteration 3500: Loss = -12478.890625
Iteration 3600: Loss = -12478.625
Iteration 3700: Loss = -12478.376953125
Iteration 3800: Loss = -12478.1494140625
Iteration 3900: Loss = -12477.9365234375
Iteration 4000: Loss = -12477.740234375
Iteration 4100: Loss = -12477.5556640625
Iteration 4200: Loss = -12477.3837890625
Iteration 4300: Loss = -12477.2236328125
Iteration 4400: Loss = -12477.0751953125
Iteration 4500: Loss = -12476.9326171875
Iteration 4600: Loss = -12476.802734375
Iteration 4700: Loss = -12476.6806640625
Iteration 4800: Loss = -12476.56640625
Iteration 4900: Loss = -12476.45703125
Iteration 5000: Loss = -12476.35546875
Iteration 5100: Loss = -12476.2587890625
Iteration 5200: Loss = -12476.16796875
Iteration 5300: Loss = -12476.083984375
Iteration 5400: Loss = -12476.00390625
Iteration 5500: Loss = -12475.927734375
Iteration 5600: Loss = -12475.85546875
Iteration 5700: Loss = -12475.7890625
Iteration 5800: Loss = -12475.7255859375
Iteration 5900: Loss = -12475.666015625
Iteration 6000: Loss = -12475.609375
Iteration 6100: Loss = -12475.5537109375
Iteration 6200: Loss = -12475.5029296875
Iteration 6300: Loss = -12475.4560546875
Iteration 6400: Loss = -12475.41015625
Iteration 6500: Loss = -12475.365234375
Iteration 6600: Loss = -12475.326171875
Iteration 6700: Loss = -12475.287109375
Iteration 6800: Loss = -12475.25
Iteration 6900: Loss = -12475.21484375
Iteration 7000: Loss = -12475.181640625
Iteration 7100: Loss = -12475.150390625
Iteration 7200: Loss = -12475.1201171875
Iteration 7300: Loss = -12475.09375
Iteration 7400: Loss = -12475.0654296875
Iteration 7500: Loss = -12475.0390625
Iteration 7600: Loss = -12475.015625
Iteration 7700: Loss = -12474.9912109375
Iteration 7800: Loss = -12474.9697265625
Iteration 7900: Loss = -12474.9482421875
Iteration 8000: Loss = -12474.9287109375
Iteration 8100: Loss = -12474.9111328125
Iteration 8200: Loss = -12474.892578125
Iteration 8300: Loss = -12474.8740234375
Iteration 8400: Loss = -12474.8603515625
Iteration 8500: Loss = -12474.84375
Iteration 8600: Loss = -12474.8271484375
Iteration 8700: Loss = -12474.8134765625
Iteration 8800: Loss = -12474.80078125
Iteration 8900: Loss = -12474.7880859375
Iteration 9000: Loss = -12474.775390625
Iteration 9100: Loss = -12474.7646484375
Iteration 9200: Loss = -12474.7548828125
Iteration 9300: Loss = -12474.744140625
Iteration 9400: Loss = -12474.732421875
Iteration 9500: Loss = -12474.7236328125
Iteration 9600: Loss = -12474.716796875
Iteration 9700: Loss = -12474.7080078125
Iteration 9800: Loss = -12474.7001953125
Iteration 9900: Loss = -12474.6923828125
Iteration 10000: Loss = -12474.68359375
Iteration 10100: Loss = -12474.677734375
Iteration 10200: Loss = -12474.6708984375
Iteration 10300: Loss = -12474.6640625
Iteration 10400: Loss = -12474.658203125
Iteration 10500: Loss = -12474.65234375
Iteration 10600: Loss = -12474.6484375
Iteration 10700: Loss = -12474.642578125
Iteration 10800: Loss = -12474.63671875
Iteration 10900: Loss = -12474.6328125
Iteration 11000: Loss = -12474.626953125
Iteration 11100: Loss = -12474.6240234375
Iteration 11200: Loss = -12474.6181640625
Iteration 11300: Loss = -12474.615234375
Iteration 11400: Loss = -12474.611328125
Iteration 11500: Loss = -12474.6083984375
Iteration 11600: Loss = -12474.603515625
Iteration 11700: Loss = -12474.599609375
Iteration 11800: Loss = -12474.59765625
Iteration 11900: Loss = -12474.5947265625
Iteration 12000: Loss = -12474.591796875
Iteration 12100: Loss = -12474.5869140625
Iteration 12200: Loss = -12474.5849609375
Iteration 12300: Loss = -12474.5830078125
Iteration 12400: Loss = -12474.5810546875
Iteration 12500: Loss = -12474.5791015625
Iteration 12600: Loss = -12474.576171875
Iteration 12700: Loss = -12474.5751953125
Iteration 12800: Loss = -12474.5732421875
Iteration 12900: Loss = -12474.5693359375
Iteration 13000: Loss = -12474.5693359375
Iteration 13100: Loss = -12474.5673828125
Iteration 13200: Loss = -12474.5634765625
Iteration 13300: Loss = -12474.560546875
Iteration 13400: Loss = -12474.556640625
Iteration 13500: Loss = -12474.5458984375
Iteration 13600: Loss = -12474.4306640625
Iteration 13700: Loss = -12474.32421875
Iteration 13800: Loss = -12469.7587890625
Iteration 13900: Loss = -12468.703125
Iteration 14000: Loss = -12468.5751953125
Iteration 14100: Loss = -12468.5517578125
Iteration 14200: Loss = -12468.5419921875
Iteration 14300: Loss = -12468.5361328125
Iteration 14400: Loss = -12468.5322265625
Iteration 14500: Loss = -12468.5322265625
Iteration 14600: Loss = -12468.52734375
Iteration 14700: Loss = -12468.525390625
Iteration 14800: Loss = -12468.525390625
Iteration 14900: Loss = -12468.525390625
Iteration 15000: Loss = -12468.5234375
Iteration 15100: Loss = -12468.5166015625
Iteration 15200: Loss = -12468.4970703125
Iteration 15300: Loss = -12468.498046875
1
Iteration 15400: Loss = -12468.49609375
Iteration 15500: Loss = -12468.4951171875
Iteration 15600: Loss = -12468.4931640625
Iteration 15700: Loss = -12468.4951171875
1
Iteration 15800: Loss = -12468.4931640625
Iteration 15900: Loss = -12468.4931640625
Iteration 16000: Loss = -12468.4921875
Iteration 16100: Loss = -12468.4912109375
Iteration 16200: Loss = -12468.4912109375
Iteration 16300: Loss = -12468.4892578125
Iteration 16400: Loss = -12468.490234375
1
Iteration 16500: Loss = -12468.4892578125
Iteration 16600: Loss = -12468.4892578125
Iteration 16700: Loss = -12468.4892578125
Iteration 16800: Loss = -12468.48828125
Iteration 16900: Loss = -12468.48828125
Iteration 17000: Loss = -12468.48828125
Iteration 17100: Loss = -12468.48828125
Iteration 17200: Loss = -12468.48828125
Iteration 17300: Loss = -12468.4873046875
Iteration 17400: Loss = -12468.48828125
1
Iteration 17500: Loss = -12468.4873046875
Iteration 17600: Loss = -12468.486328125
Iteration 17700: Loss = -12468.4853515625
Iteration 17800: Loss = -12468.4853515625
Iteration 17900: Loss = -12468.4853515625
Iteration 18000: Loss = -12468.4853515625
Iteration 18100: Loss = -12468.4853515625
Iteration 18200: Loss = -12468.4853515625
Iteration 18300: Loss = -12468.4853515625
Iteration 18400: Loss = -12468.484375
Iteration 18500: Loss = -12468.4853515625
1
Iteration 18600: Loss = -12468.484375
Iteration 18700: Loss = -12468.4833984375
Iteration 18800: Loss = -12468.4853515625
1
Iteration 18900: Loss = -12468.484375
2
Iteration 19000: Loss = -12468.484375
3
Iteration 19100: Loss = -12468.482421875
Iteration 19200: Loss = -12468.482421875
Iteration 19300: Loss = -12468.486328125
1
Iteration 19400: Loss = -12468.4833984375
2
Iteration 19500: Loss = -12468.484375
3
Iteration 19600: Loss = -12468.4833984375
4
Iteration 19700: Loss = -12468.484375
5
Iteration 19800: Loss = -12468.484375
6
Iteration 19900: Loss = -12468.4833984375
7
Iteration 20000: Loss = -12468.482421875
Iteration 20100: Loss = -12468.482421875
Iteration 20200: Loss = -12468.482421875
Iteration 20300: Loss = -12468.482421875
Iteration 20400: Loss = -12468.4833984375
1
Iteration 20500: Loss = -12468.4833984375
2
Iteration 20600: Loss = -12468.4833984375
3
Iteration 20700: Loss = -12468.482421875
Iteration 20800: Loss = -12468.4833984375
1
Iteration 20900: Loss = -12468.482421875
Iteration 21000: Loss = -12468.482421875
Iteration 21100: Loss = -12468.4833984375
1
Iteration 21200: Loss = -12468.4833984375
2
Iteration 21300: Loss = -12468.482421875
Iteration 21400: Loss = -12468.482421875
Iteration 21500: Loss = -12468.4833984375
1
Iteration 21600: Loss = -12468.482421875
Iteration 21700: Loss = -12468.482421875
Iteration 21800: Loss = -12468.4833984375
1
Iteration 21900: Loss = -12468.4833984375
2
Iteration 22000: Loss = -12468.482421875
Iteration 22100: Loss = -12468.4833984375
1
Iteration 22200: Loss = -12468.482421875
Iteration 22300: Loss = -12468.4833984375
1
Iteration 22400: Loss = -12468.4833984375
2
Iteration 22500: Loss = -12468.482421875
Iteration 22600: Loss = -12468.482421875
Iteration 22700: Loss = -12468.482421875
Iteration 22800: Loss = -12468.482421875
Iteration 22900: Loss = -12468.482421875
Iteration 23000: Loss = -12468.484375
1
Iteration 23100: Loss = -12468.482421875
Iteration 23200: Loss = -12468.484375
1
Iteration 23300: Loss = -12468.4814453125
Iteration 23400: Loss = -12468.4833984375
1
Iteration 23500: Loss = -12468.482421875
2
Iteration 23600: Loss = -12468.4833984375
3
Iteration 23700: Loss = -12468.482421875
4
Iteration 23800: Loss = -12468.484375
5
Iteration 23900: Loss = -12468.4833984375
6
Iteration 24000: Loss = -12468.4833984375
7
Iteration 24100: Loss = -12468.4814453125
Iteration 24200: Loss = -12468.482421875
1
Iteration 24300: Loss = -12468.482421875
2
Iteration 24400: Loss = -12468.4833984375
3
Iteration 24500: Loss = -12468.4814453125
Iteration 24600: Loss = -12468.4833984375
1
Iteration 24700: Loss = -12468.482421875
2
Iteration 24800: Loss = -12468.4833984375
3
Iteration 24900: Loss = -12468.482421875
4
Iteration 25000: Loss = -12468.4833984375
5
Iteration 25100: Loss = -12468.4833984375
6
Iteration 25200: Loss = -12468.4833984375
7
Iteration 25300: Loss = -12468.482421875
8
Iteration 25400: Loss = -12468.482421875
9
Iteration 25500: Loss = -12468.4833984375
10
Iteration 25600: Loss = -12468.482421875
11
Iteration 25700: Loss = -12468.482421875
12
Iteration 25800: Loss = -12468.4833984375
13
Iteration 25900: Loss = -12468.482421875
14
Iteration 26000: Loss = -12468.4833984375
15
Stopping early at iteration 26000 due to no improvement.
pi: tensor([[8.7216e-06, 9.9999e-01],
        [5.5454e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0617, 0.9383], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.5710, 0.1038],
         [0.9726, 0.2047]],

        [[0.9924, 0.1867],
         [0.5572, 0.5682]],

        [[0.7280, 0.2012],
         [0.1801, 0.9929]],

        [[0.0959, 0.2163],
         [0.0075, 0.9472]],

        [[0.0077, 0.2872],
         [0.7519, 0.0293]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 63
Adjusted Rand Index: 0.046034600359645315
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0016430190266618317
Average Adjusted Rand Index: 0.009206920071929064
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25542.609375
Iteration 100: Loss = -16421.28515625
Iteration 200: Loss = -13197.857421875
Iteration 300: Loss = -12637.4541015625
Iteration 400: Loss = -12556.1552734375
Iteration 500: Loss = -12519.822265625
Iteration 600: Loss = -12502.3310546875
Iteration 700: Loss = -12493.54296875
Iteration 800: Loss = -12488.69140625
Iteration 900: Loss = -12485.744140625
Iteration 1000: Loss = -12483.740234375
Iteration 1100: Loss = -12482.267578125
Iteration 1200: Loss = -12481.115234375
Iteration 1300: Loss = -12480.232421875
Iteration 1400: Loss = -12479.50390625
Iteration 1500: Loss = -12478.9140625
Iteration 1600: Loss = -12478.423828125
Iteration 1700: Loss = -12478.013671875
Iteration 1800: Loss = -12477.6640625
Iteration 1900: Loss = -12477.365234375
Iteration 2000: Loss = -12477.1025390625
Iteration 2100: Loss = -12476.8720703125
Iteration 2200: Loss = -12476.669921875
Iteration 2300: Loss = -12476.48828125
Iteration 2400: Loss = -12476.328125
Iteration 2500: Loss = -12476.185546875
Iteration 2600: Loss = -12476.0556640625
Iteration 2700: Loss = -12475.9375
Iteration 2800: Loss = -12475.830078125
Iteration 2900: Loss = -12475.734375
Iteration 3000: Loss = -12475.6435546875
Iteration 3100: Loss = -12475.5615234375
Iteration 3200: Loss = -12475.48828125
Iteration 3300: Loss = -12475.41796875
Iteration 3400: Loss = -12475.35546875
Iteration 3500: Loss = -12475.294921875
Iteration 3600: Loss = -12475.2392578125
Iteration 3700: Loss = -12475.1875
Iteration 3800: Loss = -12475.1376953125
Iteration 3900: Loss = -12475.0927734375
Iteration 4000: Loss = -12475.046875
Iteration 4100: Loss = -12475.00390625
Iteration 4200: Loss = -12474.9599609375
Iteration 4300: Loss = -12474.912109375
Iteration 4400: Loss = -12474.857421875
Iteration 4500: Loss = -12474.7939453125
Iteration 4600: Loss = -12474.705078125
Iteration 4700: Loss = -12474.587890625
Iteration 4800: Loss = -12474.470703125
Iteration 4900: Loss = -12474.3779296875
Iteration 5000: Loss = -12474.310546875
Iteration 5100: Loss = -12474.2607421875
Iteration 5200: Loss = -12474.2177734375
Iteration 5300: Loss = -12474.181640625
Iteration 5400: Loss = -12474.1474609375
Iteration 5500: Loss = -12474.11328125
Iteration 5600: Loss = -12474.0771484375
Iteration 5700: Loss = -12474.0341796875
Iteration 5800: Loss = -12473.9638671875
Iteration 5900: Loss = -12473.7861328125
Iteration 6000: Loss = -12473.15234375
Iteration 6100: Loss = -12472.6376953125
Iteration 6200: Loss = -12472.181640625
Iteration 6300: Loss = -12471.951171875
Iteration 6400: Loss = -12471.7919921875
Iteration 6500: Loss = -12471.658203125
Iteration 6600: Loss = -12471.541015625
Iteration 6700: Loss = -12471.4443359375
Iteration 6800: Loss = -12471.369140625
Iteration 6900: Loss = -12471.30859375
Iteration 7000: Loss = -12471.2587890625
Iteration 7100: Loss = -12471.2138671875
Iteration 7200: Loss = -12471.173828125
Iteration 7300: Loss = -12471.138671875
Iteration 7400: Loss = -12471.109375
Iteration 7500: Loss = -12471.0849609375
Iteration 7600: Loss = -12471.0634765625
Iteration 7700: Loss = -12471.044921875
Iteration 7800: Loss = -12471.02734375
Iteration 7900: Loss = -12471.013671875
Iteration 8000: Loss = -12471.0009765625
Iteration 8100: Loss = -12470.9873046875
Iteration 8200: Loss = -12470.9765625
Iteration 8300: Loss = -12470.9658203125
Iteration 8400: Loss = -12470.955078125
Iteration 8500: Loss = -12470.9462890625
Iteration 8600: Loss = -12470.9365234375
Iteration 8700: Loss = -12470.9296875
Iteration 8800: Loss = -12470.919921875
Iteration 8900: Loss = -12470.912109375
Iteration 9000: Loss = -12470.904296875
Iteration 9100: Loss = -12470.896484375
Iteration 9200: Loss = -12470.888671875
Iteration 9300: Loss = -12470.8798828125
Iteration 9400: Loss = -12470.8720703125
Iteration 9500: Loss = -12470.86328125
Iteration 9600: Loss = -12470.8583984375
Iteration 9700: Loss = -12470.84765625
Iteration 9800: Loss = -12470.841796875
Iteration 9900: Loss = -12470.8310546875
Iteration 10000: Loss = -12470.8232421875
Iteration 10100: Loss = -12470.814453125
Iteration 10200: Loss = -12470.806640625
Iteration 10300: Loss = -12470.794921875
Iteration 10400: Loss = -12470.783203125
Iteration 10500: Loss = -12470.7607421875
Iteration 10600: Loss = -12470.6259765625
Iteration 10700: Loss = -12470.4169921875
Iteration 10800: Loss = -12470.35546875
Iteration 10900: Loss = -12470.3203125
Iteration 11000: Loss = -12470.298828125
Iteration 11100: Loss = -12470.2802734375
Iteration 11200: Loss = -12470.26953125
Iteration 11300: Loss = -12470.259765625
Iteration 11400: Loss = -12470.2470703125
Iteration 11500: Loss = -12470.2333984375
Iteration 11600: Loss = -12470.181640625
Iteration 11700: Loss = -12470.048828125
Iteration 11800: Loss = -12469.9521484375
Iteration 11900: Loss = -12469.8837890625
Iteration 12000: Loss = -12469.837890625
Iteration 12100: Loss = -12469.8095703125
Iteration 12200: Loss = -12469.7890625
Iteration 12300: Loss = -12469.7763671875
Iteration 12400: Loss = -12469.7685546875
Iteration 12500: Loss = -12469.7607421875
Iteration 12600: Loss = -12469.75390625
Iteration 12700: Loss = -12469.7490234375
Iteration 12800: Loss = -12469.74609375
Iteration 12900: Loss = -12469.744140625
Iteration 13000: Loss = -12469.740234375
Iteration 13100: Loss = -12469.7392578125
Iteration 13200: Loss = -12469.734375
Iteration 13300: Loss = -12469.734375
Iteration 13400: Loss = -12469.732421875
Iteration 13500: Loss = -12469.7314453125
Iteration 13600: Loss = -12469.7294921875
Iteration 13700: Loss = -12469.728515625
Iteration 13800: Loss = -12469.7265625
Iteration 13900: Loss = -12469.7275390625
1
Iteration 14000: Loss = -12469.7275390625
2
Iteration 14100: Loss = -12469.7255859375
Iteration 14200: Loss = -12469.7236328125
Iteration 14300: Loss = -12469.7236328125
Iteration 14400: Loss = -12469.7216796875
Iteration 14500: Loss = -12469.7216796875
Iteration 14600: Loss = -12469.720703125
Iteration 14700: Loss = -12469.7216796875
1
Iteration 14800: Loss = -12469.7197265625
Iteration 14900: Loss = -12469.71875
Iteration 15000: Loss = -12469.71875
Iteration 15100: Loss = -12469.71875
Iteration 15200: Loss = -12469.71875
Iteration 15300: Loss = -12469.7177734375
Iteration 15400: Loss = -12469.7177734375
Iteration 15500: Loss = -12469.716796875
Iteration 15600: Loss = -12469.716796875
Iteration 15700: Loss = -12469.716796875
Iteration 15800: Loss = -12469.716796875
Iteration 15900: Loss = -12469.7177734375
1
Iteration 16000: Loss = -12469.716796875
Iteration 16100: Loss = -12469.7158203125
Iteration 16200: Loss = -12469.71484375
Iteration 16300: Loss = -12469.7158203125
1
Iteration 16400: Loss = -12469.7158203125
2
Iteration 16500: Loss = -12469.7138671875
Iteration 16600: Loss = -12469.71484375
1
Iteration 16700: Loss = -12469.712890625
Iteration 16800: Loss = -12469.71484375
1
Iteration 16900: Loss = -12469.712890625
Iteration 17000: Loss = -12469.7109375
Iteration 17100: Loss = -12469.708984375
Iteration 17200: Loss = -12469.708984375
Iteration 17300: Loss = -12469.7119140625
1
Iteration 17400: Loss = -12469.708984375
Iteration 17500: Loss = -12469.70703125
Iteration 17600: Loss = -12469.7080078125
1
Iteration 17700: Loss = -12469.7060546875
Iteration 17800: Loss = -12469.70703125
1
Iteration 17900: Loss = -12469.70703125
2
Iteration 18000: Loss = -12469.7060546875
Iteration 18100: Loss = -12469.7080078125
1
Iteration 18200: Loss = -12469.705078125
Iteration 18300: Loss = -12469.705078125
Iteration 18400: Loss = -12469.70703125
1
Iteration 18500: Loss = -12469.705078125
Iteration 18600: Loss = -12469.7080078125
1
Iteration 18700: Loss = -12469.705078125
Iteration 18800: Loss = -12469.7060546875
1
Iteration 18900: Loss = -12469.7060546875
2
Iteration 19000: Loss = -12469.705078125
Iteration 19100: Loss = -12469.705078125
Iteration 19200: Loss = -12469.705078125
Iteration 19300: Loss = -12469.7041015625
Iteration 19400: Loss = -12469.705078125
1
Iteration 19500: Loss = -12469.7041015625
Iteration 19600: Loss = -12469.7041015625
Iteration 19700: Loss = -12469.703125
Iteration 19800: Loss = -12469.7041015625
1
Iteration 19900: Loss = -12469.703125
Iteration 20000: Loss = -12469.7021484375
Iteration 20100: Loss = -12469.703125
1
Iteration 20200: Loss = -12469.703125
2
Iteration 20300: Loss = -12469.703125
3
Iteration 20400: Loss = -12469.7021484375
Iteration 20500: Loss = -12469.703125
1
Iteration 20600: Loss = -12469.7021484375
Iteration 20700: Loss = -12469.703125
1
Iteration 20800: Loss = -12469.703125
2
Iteration 20900: Loss = -12469.7021484375
Iteration 21000: Loss = -12469.703125
1
Iteration 21100: Loss = -12469.7021484375
Iteration 21200: Loss = -12469.7021484375
Iteration 21300: Loss = -12469.705078125
1
Iteration 21400: Loss = -12469.703125
2
Iteration 21500: Loss = -12469.703125
3
Iteration 21600: Loss = -12469.7041015625
4
Iteration 21700: Loss = -12469.703125
5
Iteration 21800: Loss = -12469.7041015625
6
Iteration 21900: Loss = -12469.703125
7
Iteration 22000: Loss = -12469.703125
8
Iteration 22100: Loss = -12469.7021484375
Iteration 22200: Loss = -12469.701171875
Iteration 22300: Loss = -12469.7001953125
Iteration 22400: Loss = -12469.7021484375
1
Iteration 22500: Loss = -12469.701171875
2
Iteration 22600: Loss = -12469.7021484375
3
Iteration 22700: Loss = -12469.7021484375
4
Iteration 22800: Loss = -12469.703125
5
Iteration 22900: Loss = -12469.7021484375
6
Iteration 23000: Loss = -12469.703125
7
Iteration 23100: Loss = -12469.701171875
8
Iteration 23200: Loss = -12469.701171875
9
Iteration 23300: Loss = -12469.701171875
10
Iteration 23400: Loss = -12469.7021484375
11
Iteration 23500: Loss = -12469.701171875
12
Iteration 23600: Loss = -12469.701171875
13
Iteration 23700: Loss = -12469.7001953125
Iteration 23800: Loss = -12469.701171875
1
Iteration 23900: Loss = -12469.703125
2
Iteration 24000: Loss = -12469.701171875
3
Iteration 24100: Loss = -12469.7021484375
4
Iteration 24200: Loss = -12469.7001953125
Iteration 24300: Loss = -12469.701171875
1
Iteration 24400: Loss = -12469.7021484375
2
Iteration 24500: Loss = -12469.701171875
3
Iteration 24600: Loss = -12469.7001953125
Iteration 24700: Loss = -12469.701171875
1
Iteration 24800: Loss = -12469.701171875
2
Iteration 24900: Loss = -12469.701171875
3
Iteration 25000: Loss = -12469.701171875
4
Iteration 25100: Loss = -12469.701171875
5
Iteration 25200: Loss = -12469.7021484375
6
Iteration 25300: Loss = -12469.703125
7
Iteration 25400: Loss = -12469.701171875
8
Iteration 25500: Loss = -12469.701171875
9
Iteration 25600: Loss = -12469.701171875
10
Iteration 25700: Loss = -12469.701171875
11
Iteration 25800: Loss = -12469.7001953125
Iteration 25900: Loss = -12469.701171875
1
Iteration 26000: Loss = -12469.701171875
2
Iteration 26100: Loss = -12469.7001953125
Iteration 26200: Loss = -12469.701171875
1
Iteration 26300: Loss = -12469.701171875
2
Iteration 26400: Loss = -12469.7021484375
3
Iteration 26500: Loss = -12469.7001953125
Iteration 26600: Loss = -12469.703125
1
Iteration 26700: Loss = -12469.701171875
2
Iteration 26800: Loss = -12469.701171875
3
Iteration 26900: Loss = -12469.701171875
4
Iteration 27000: Loss = -12469.701171875
5
Iteration 27100: Loss = -12469.7021484375
6
Iteration 27200: Loss = -12469.701171875
7
Iteration 27300: Loss = -12469.7021484375
8
Iteration 27400: Loss = -12469.701171875
9
Iteration 27500: Loss = -12469.701171875
10
Iteration 27600: Loss = -12469.703125
11
Iteration 27700: Loss = -12469.701171875
12
Iteration 27800: Loss = -12469.7001953125
Iteration 27900: Loss = -12469.7021484375
1
Iteration 28000: Loss = -12469.7041015625
2
Iteration 28100: Loss = -12469.7021484375
3
Iteration 28200: Loss = -12469.701171875
4
Iteration 28300: Loss = -12469.69921875
Iteration 28400: Loss = -12469.7021484375
1
Iteration 28500: Loss = -12469.7021484375
2
Iteration 28600: Loss = -12469.7001953125
3
Iteration 28700: Loss = -12469.69921875
Iteration 28800: Loss = -12469.703125
1
Iteration 28900: Loss = -12469.7001953125
2
Iteration 29000: Loss = -12469.7001953125
3
Iteration 29100: Loss = -12469.703125
4
Iteration 29200: Loss = -12469.701171875
5
Iteration 29300: Loss = -12469.7021484375
6
Iteration 29400: Loss = -12469.7021484375
7
Iteration 29500: Loss = -12469.69921875
Iteration 29600: Loss = -12469.701171875
1
Iteration 29700: Loss = -12469.7001953125
2
Iteration 29800: Loss = -12469.69921875
Iteration 29900: Loss = -12469.701171875
1
pi: tensor([[9.9177e-01, 8.2284e-03],
        [6.6706e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9850, 0.0150], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2005, 0.2267],
         [0.2557, 0.5001]],

        [[0.6863, 0.1021],
         [0.9226, 0.9703]],

        [[0.0920, 0.2261],
         [0.4960, 0.1851]],

        [[0.8180, 0.2804],
         [0.4629, 0.9789]],

        [[0.1998, 0.2457],
         [0.0874, 0.0126]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.010213452814961178
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.00776683106263838
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0014922741295917668
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.006738692547152536
Global Adjusted Rand Index: -0.0007386745572127346
Average Adjusted Rand Index: -0.0016469993709880251
[0.0016430190266618317, -0.0007386745572127346] [0.009206920071929064, -0.0016469993709880251] [12468.4833984375, 12469.701171875]
-------------------------------------
This iteration is 78
True Objective function: Loss = -12009.810443007842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25193.42578125
Iteration 100: Loss = -18144.4453125
Iteration 200: Loss = -13929.8955078125
Iteration 300: Loss = -12982.623046875
Iteration 400: Loss = -12759.5205078125
Iteration 500: Loss = -12701.888671875
Iteration 600: Loss = -12677.0361328125
Iteration 700: Loss = -12660.3974609375
Iteration 800: Loss = -12646.7734375
Iteration 900: Loss = -12634.0546875
Iteration 1000: Loss = -12621.7705078125
Iteration 1100: Loss = -12612.78515625
Iteration 1200: Loss = -12606.5888671875
Iteration 1300: Loss = -12600.318359375
Iteration 1400: Loss = -12592.966796875
Iteration 1500: Loss = -12585.578125
Iteration 1600: Loss = -12580.01953125
Iteration 1700: Loss = -12574.6982421875
Iteration 1800: Loss = -12572.16796875
Iteration 1900: Loss = -12570.2109375
Iteration 2000: Loss = -12568.6396484375
Iteration 2100: Loss = -12567.333984375
Iteration 2200: Loss = -12566.265625
Iteration 2300: Loss = -12565.23828125
Iteration 2400: Loss = -12562.576171875
Iteration 2500: Loss = -12560.333984375
Iteration 2600: Loss = -12558.23828125
Iteration 2700: Loss = -12557.318359375
Iteration 2800: Loss = -12556.5615234375
Iteration 2900: Loss = -12555.73828125
Iteration 3000: Loss = -12554.201171875
Iteration 3100: Loss = -12552.62890625
Iteration 3200: Loss = -12551.63671875
Iteration 3300: Loss = -12550.111328125
Iteration 3400: Loss = -12547.609375
Iteration 3500: Loss = -12545.9716796875
Iteration 3600: Loss = -12545.390625
Iteration 3700: Loss = -12543.5419921875
Iteration 3800: Loss = -12542.79296875
Iteration 3900: Loss = -12540.833984375
Iteration 4000: Loss = -12540.4267578125
Iteration 4100: Loss = -12538.521484375
Iteration 4200: Loss = -12536.578125
Iteration 4300: Loss = -12536.3486328125
Iteration 4400: Loss = -12536.134765625
Iteration 4500: Loss = -12535.830078125
Iteration 4600: Loss = -12534.9931640625
Iteration 4700: Loss = -12534.3759765625
Iteration 4800: Loss = -12534.1064453125
Iteration 4900: Loss = -12533.9375
Iteration 5000: Loss = -12532.373046875
Iteration 5100: Loss = -12532.240234375
Iteration 5200: Loss = -12532.111328125
Iteration 5300: Loss = -12531.974609375
Iteration 5400: Loss = -12531.904296875
Iteration 5500: Loss = -12531.15625
Iteration 5600: Loss = -12530.279296875
Iteration 5700: Loss = -12530.236328125
Iteration 5800: Loss = -12530.19921875
Iteration 5900: Loss = -12530.1640625
Iteration 6000: Loss = -12530.1328125
Iteration 6100: Loss = -12530.1015625
Iteration 6200: Loss = -12530.0625
Iteration 6300: Loss = -12529.9375
Iteration 6400: Loss = -12529.8857421875
Iteration 6500: Loss = -12529.861328125
Iteration 6600: Loss = -12529.83984375
Iteration 6700: Loss = -12529.8193359375
Iteration 6800: Loss = -12529.7998046875
Iteration 6900: Loss = -12529.7802734375
Iteration 7000: Loss = -12529.765625
Iteration 7100: Loss = -12529.748046875
Iteration 7200: Loss = -12529.732421875
Iteration 7300: Loss = -12529.720703125
Iteration 7400: Loss = -12529.70703125
Iteration 7500: Loss = -12529.6943359375
Iteration 7600: Loss = -12529.6826171875
Iteration 7700: Loss = -12529.671875
Iteration 7800: Loss = -12529.66015625
Iteration 7900: Loss = -12529.650390625
Iteration 8000: Loss = -12529.640625
Iteration 8100: Loss = -12529.630859375
Iteration 8200: Loss = -12529.623046875
Iteration 8300: Loss = -12529.61328125
Iteration 8400: Loss = -12529.607421875
Iteration 8500: Loss = -12529.599609375
Iteration 8600: Loss = -12529.5927734375
Iteration 8700: Loss = -12529.5869140625
Iteration 8800: Loss = -12529.5810546875
Iteration 8900: Loss = -12529.5732421875
Iteration 9000: Loss = -12529.5673828125
Iteration 9100: Loss = -12529.560546875
Iteration 9200: Loss = -12529.5537109375
Iteration 9300: Loss = -12529.5458984375
Iteration 9400: Loss = -12529.5380859375
Iteration 9500: Loss = -12529.5302734375
Iteration 9600: Loss = -12529.5234375
Iteration 9700: Loss = -12529.517578125
Iteration 9800: Loss = -12529.51171875
Iteration 9900: Loss = -12529.5068359375
Iteration 10000: Loss = -12529.5029296875
Iteration 10100: Loss = -12529.4970703125
Iteration 10200: Loss = -12529.4951171875
Iteration 10300: Loss = -12529.490234375
Iteration 10400: Loss = -12529.486328125
Iteration 10500: Loss = -12529.484375
Iteration 10600: Loss = -12529.48046875
Iteration 10700: Loss = -12529.4755859375
Iteration 10800: Loss = -12529.4677734375
Iteration 10900: Loss = -12529.455078125
Iteration 11000: Loss = -12529.447265625
Iteration 11100: Loss = -12529.4423828125
Iteration 11200: Loss = -12529.439453125
Iteration 11300: Loss = -12529.4375
Iteration 11400: Loss = -12529.43359375
Iteration 11500: Loss = -12529.4326171875
Iteration 11600: Loss = -12529.4306640625
Iteration 11700: Loss = -12529.4296875
Iteration 11800: Loss = -12529.4267578125
Iteration 11900: Loss = -12529.4248046875
Iteration 12000: Loss = -12529.42578125
1
Iteration 12100: Loss = -12529.4228515625
Iteration 12200: Loss = -12529.4208984375
Iteration 12300: Loss = -12529.419921875
Iteration 12400: Loss = -12529.4189453125
Iteration 12500: Loss = -12529.4169921875
Iteration 12600: Loss = -12529.416015625
Iteration 12700: Loss = -12529.4169921875
1
Iteration 12800: Loss = -12529.4150390625
Iteration 12900: Loss = -12529.4140625
Iteration 13000: Loss = -12529.4140625
Iteration 13100: Loss = -12529.412109375
Iteration 13200: Loss = -12529.412109375
Iteration 13300: Loss = -12529.4111328125
Iteration 13400: Loss = -12529.4130859375
1
Iteration 13500: Loss = -12529.41015625
Iteration 13600: Loss = -12529.4091796875
Iteration 13700: Loss = -12529.408203125
Iteration 13800: Loss = -12529.4072265625
Iteration 13900: Loss = -12529.359375
Iteration 14000: Loss = -12529.353515625
Iteration 14100: Loss = -12529.353515625
Iteration 14200: Loss = -12529.3505859375
Iteration 14300: Loss = -12529.3505859375
Iteration 14400: Loss = -12529.349609375
Iteration 14500: Loss = -12529.349609375
Iteration 14600: Loss = -12529.349609375
Iteration 14700: Loss = -12529.34765625
Iteration 14800: Loss = -12529.3486328125
1
Iteration 14900: Loss = -12529.34765625
Iteration 15000: Loss = -12529.34765625
Iteration 15100: Loss = -12529.3466796875
Iteration 15200: Loss = -12529.3466796875
Iteration 15300: Loss = -12529.3466796875
Iteration 15400: Loss = -12529.34765625
1
Iteration 15500: Loss = -12529.345703125
Iteration 15600: Loss = -12529.345703125
Iteration 15700: Loss = -12529.345703125
Iteration 15800: Loss = -12529.34765625
1
Iteration 15900: Loss = -12529.345703125
Iteration 16000: Loss = -12529.3447265625
Iteration 16100: Loss = -12529.345703125
1
Iteration 16200: Loss = -12529.345703125
2
Iteration 16300: Loss = -12529.3447265625
Iteration 16400: Loss = -12529.34375
Iteration 16500: Loss = -12529.345703125
1
Iteration 16600: Loss = -12529.34375
Iteration 16700: Loss = -12529.3447265625
1
Iteration 16800: Loss = -12529.34375
Iteration 16900: Loss = -12529.3447265625
1
Iteration 17000: Loss = -12529.34375
Iteration 17100: Loss = -12529.3447265625
1
Iteration 17200: Loss = -12529.3447265625
2
Iteration 17300: Loss = -12529.3447265625
3
Iteration 17400: Loss = -12529.34375
Iteration 17500: Loss = -12529.34375
Iteration 17600: Loss = -12529.3427734375
Iteration 17700: Loss = -12529.34375
1
Iteration 17800: Loss = -12529.34375
2
Iteration 17900: Loss = -12529.34375
3
Iteration 18000: Loss = -12529.3427734375
Iteration 18100: Loss = -12529.3427734375
Iteration 18200: Loss = -12529.3427734375
Iteration 18300: Loss = -12529.34375
1
Iteration 18400: Loss = -12529.341796875
Iteration 18500: Loss = -12529.3427734375
1
Iteration 18600: Loss = -12529.341796875
Iteration 18700: Loss = -12529.341796875
Iteration 18800: Loss = -12529.3447265625
1
Iteration 18900: Loss = -12529.3427734375
2
Iteration 19000: Loss = -12529.3427734375
3
Iteration 19100: Loss = -12529.341796875
Iteration 19200: Loss = -12529.341796875
Iteration 19300: Loss = -12529.3427734375
1
Iteration 19400: Loss = -12529.341796875
Iteration 19500: Loss = -12529.34375
1
Iteration 19600: Loss = -12529.34375
2
Iteration 19700: Loss = -12529.3427734375
3
Iteration 19800: Loss = -12529.3427734375
4
Iteration 19900: Loss = -12529.283203125
Iteration 20000: Loss = -12527.4970703125
Iteration 20100: Loss = -12527.4970703125
Iteration 20200: Loss = -12527.49609375
Iteration 20300: Loss = -12527.4951171875
Iteration 20400: Loss = -12527.4951171875
Iteration 20500: Loss = -12527.4951171875
Iteration 20600: Loss = -12527.4951171875
Iteration 20700: Loss = -12527.49609375
1
Iteration 20800: Loss = -12527.4951171875
Iteration 20900: Loss = -12527.4951171875
Iteration 21000: Loss = -12527.49609375
1
Iteration 21100: Loss = -12526.966796875
Iteration 21200: Loss = -12525.3896484375
Iteration 21300: Loss = -12525.3876953125
Iteration 21400: Loss = -12525.388671875
1
Iteration 21500: Loss = -12525.388671875
2
Iteration 21600: Loss = -12525.3876953125
Iteration 21700: Loss = -12525.38671875
Iteration 21800: Loss = -12525.38671875
Iteration 21900: Loss = -12525.388671875
1
Iteration 22000: Loss = -12525.388671875
2
Iteration 22100: Loss = -12525.388671875
3
Iteration 22200: Loss = -12525.3876953125
4
Iteration 22300: Loss = -12525.38671875
Iteration 22400: Loss = -12525.38671875
Iteration 22500: Loss = -12525.388671875
1
Iteration 22600: Loss = -12525.3876953125
2
Iteration 22700: Loss = -12525.3876953125
3
Iteration 22800: Loss = -12525.38671875
Iteration 22900: Loss = -12525.388671875
1
Iteration 23000: Loss = -12525.3876953125
2
Iteration 23100: Loss = -12525.38671875
Iteration 23200: Loss = -12525.38671875
Iteration 23300: Loss = -12525.388671875
1
Iteration 23400: Loss = -12525.3876953125
2
Iteration 23500: Loss = -12525.38671875
Iteration 23600: Loss = -12525.38671875
Iteration 23700: Loss = -12525.38671875
Iteration 23800: Loss = -12525.38671875
Iteration 23900: Loss = -12525.3876953125
1
Iteration 24000: Loss = -12525.38671875
Iteration 24100: Loss = -12525.3876953125
1
Iteration 24200: Loss = -12525.38671875
Iteration 24300: Loss = -12525.3876953125
1
Iteration 24400: Loss = -12525.38671875
Iteration 24500: Loss = -12525.3876953125
1
Iteration 24600: Loss = -12525.390625
2
Iteration 24700: Loss = -12525.38671875
Iteration 24800: Loss = -12525.38671875
Iteration 24900: Loss = -12525.38671875
Iteration 25000: Loss = -12525.388671875
1
Iteration 25100: Loss = -12525.3876953125
2
Iteration 25200: Loss = -12525.3876953125
3
Iteration 25300: Loss = -12525.3759765625
Iteration 25400: Loss = -12525.3759765625
Iteration 25500: Loss = -12525.375
Iteration 25600: Loss = -12525.3720703125
Iteration 25700: Loss = -12525.3720703125
Iteration 25800: Loss = -12525.3720703125
Iteration 25900: Loss = -12525.37109375
Iteration 26000: Loss = -12525.3720703125
1
Iteration 26100: Loss = -12525.369140625
Iteration 26200: Loss = -12525.3681640625
Iteration 26300: Loss = -12525.3681640625
Iteration 26400: Loss = -12525.3662109375
Iteration 26500: Loss = -12525.365234375
Iteration 26600: Loss = -12525.3564453125
Iteration 26700: Loss = -12525.35546875
Iteration 26800: Loss = -12525.3515625
Iteration 26900: Loss = -12525.349609375
Iteration 27000: Loss = -12525.349609375
Iteration 27100: Loss = -12525.3486328125
Iteration 27200: Loss = -12525.3408203125
Iteration 27300: Loss = -12525.33203125
Iteration 27400: Loss = -12525.3173828125
Iteration 27500: Loss = -12525.3056640625
Iteration 27600: Loss = -12525.2890625
Iteration 27700: Loss = -12525.28125
Iteration 27800: Loss = -12525.263671875
Iteration 27900: Loss = -12525.2177734375
Iteration 28000: Loss = -12525.203125
Iteration 28100: Loss = -12525.1923828125
Iteration 28200: Loss = -12525.193359375
1
Iteration 28300: Loss = -12525.169921875
Iteration 28400: Loss = -12525.150390625
Iteration 28500: Loss = -12525.14453125
Iteration 28600: Loss = -12525.1357421875
Iteration 28700: Loss = -12525.1337890625
Iteration 28800: Loss = -12525.12109375
Iteration 28900: Loss = -12525.1142578125
Iteration 29000: Loss = -12525.1123046875
Iteration 29100: Loss = -12525.1123046875
Iteration 29200: Loss = -12525.1142578125
1
Iteration 29300: Loss = -12525.11328125
2
Iteration 29400: Loss = -12525.11328125
3
Iteration 29500: Loss = -12525.111328125
Iteration 29600: Loss = -12525.11328125
1
Iteration 29700: Loss = -12525.111328125
Iteration 29800: Loss = -12525.1123046875
1
Iteration 29900: Loss = -12525.1123046875
2
pi: tensor([[2.3807e-01, 7.6193e-01],
        [4.3250e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9994e-01, 6.0966e-05], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1974, 0.1645],
         [0.8852, 0.2054]],

        [[0.1059, 0.2037],
         [0.4934, 0.1175]],

        [[0.0092, 0.2349],
         [0.9878, 0.9644]],

        [[0.9496, 0.1893],
         [0.0726, 0.0686]],

        [[0.0109, 0.2117],
         [0.2141, 0.0818]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00031784252347700457
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29496.8984375
Iteration 100: Loss = -20847.90625
Iteration 200: Loss = -14529.9931640625
Iteration 300: Loss = -13062.962890625
Iteration 400: Loss = -12830.4775390625
Iteration 500: Loss = -12741.044921875
Iteration 600: Loss = -12695.1767578125
Iteration 700: Loss = -12665.478515625
Iteration 800: Loss = -12642.7421875
Iteration 900: Loss = -12622.1591796875
Iteration 1000: Loss = -12603.244140625
Iteration 1100: Loss = -12589.6044921875
Iteration 1200: Loss = -12582.236328125
Iteration 1300: Loss = -12572.9375
Iteration 1400: Loss = -12567.7685546875
Iteration 1500: Loss = -12561.8544921875
Iteration 1600: Loss = -12555.8642578125
Iteration 1700: Loss = -12552.1689453125
Iteration 1800: Loss = -12549.0146484375
Iteration 1900: Loss = -12545.84375
Iteration 2000: Loss = -12542.3154296875
Iteration 2100: Loss = -12540.330078125
Iteration 2200: Loss = -12538.7802734375
Iteration 2300: Loss = -12537.5126953125
Iteration 2400: Loss = -12536.43359375
Iteration 2500: Loss = -12535.4755859375
Iteration 2600: Loss = -12534.599609375
Iteration 2700: Loss = -12533.7861328125
Iteration 2800: Loss = -12533.0390625
Iteration 2900: Loss = -12532.376953125
Iteration 3000: Loss = -12531.8017578125
Iteration 3100: Loss = -12531.3076171875
Iteration 3200: Loss = -12530.876953125
Iteration 3300: Loss = -12530.4990234375
Iteration 3400: Loss = -12530.1640625
Iteration 3500: Loss = -12529.8603515625
Iteration 3600: Loss = -12529.5859375
Iteration 3700: Loss = -12529.3359375
Iteration 3800: Loss = -12529.107421875
Iteration 3900: Loss = -12528.896484375
Iteration 4000: Loss = -12528.703125
Iteration 4100: Loss = -12528.5234375
Iteration 4200: Loss = -12528.35546875
Iteration 4300: Loss = -12528.2021484375
Iteration 4400: Loss = -12528.056640625
Iteration 4500: Loss = -12527.919921875
Iteration 4600: Loss = -12527.7939453125
Iteration 4700: Loss = -12527.677734375
Iteration 4800: Loss = -12527.56640625
Iteration 4900: Loss = -12527.466796875
Iteration 5000: Loss = -12527.3662109375
Iteration 5100: Loss = -12527.2744140625
Iteration 5200: Loss = -12527.1884765625
Iteration 5300: Loss = -12527.1083984375
Iteration 5400: Loss = -12527.0322265625
Iteration 5500: Loss = -12526.95703125
Iteration 5600: Loss = -12526.8896484375
Iteration 5700: Loss = -12526.8251953125
Iteration 5800: Loss = -12526.763671875
Iteration 5900: Loss = -12526.70703125
Iteration 6000: Loss = -12526.650390625
Iteration 6100: Loss = -12526.59765625
Iteration 6200: Loss = -12526.5478515625
Iteration 6300: Loss = -12526.5009765625
Iteration 6400: Loss = -12526.455078125
Iteration 6500: Loss = -12526.416015625
Iteration 6600: Loss = -12526.3720703125
Iteration 6700: Loss = -12526.333984375
Iteration 6800: Loss = -12526.294921875
Iteration 6900: Loss = -12526.2587890625
Iteration 7000: Loss = -12526.224609375
Iteration 7100: Loss = -12526.1923828125
Iteration 7200: Loss = -12526.1611328125
Iteration 7300: Loss = -12526.1318359375
Iteration 7400: Loss = -12526.1005859375
Iteration 7500: Loss = -12526.0751953125
Iteration 7600: Loss = -12526.044921875
Iteration 7700: Loss = -12526.021484375
Iteration 7800: Loss = -12525.9931640625
Iteration 7900: Loss = -12525.9697265625
Iteration 8000: Loss = -12525.9462890625
Iteration 8100: Loss = -12525.923828125
Iteration 8200: Loss = -12525.9033203125
Iteration 8300: Loss = -12525.880859375
Iteration 8400: Loss = -12525.8623046875
Iteration 8500: Loss = -12525.8427734375
Iteration 8600: Loss = -12525.8232421875
Iteration 8700: Loss = -12525.8046875
Iteration 8800: Loss = -12525.7880859375
Iteration 8900: Loss = -12525.7705078125
Iteration 9000: Loss = -12525.755859375
Iteration 9100: Loss = -12525.7392578125
Iteration 9200: Loss = -12525.724609375
Iteration 9300: Loss = -12525.7109375
Iteration 9400: Loss = -12525.6962890625
Iteration 9500: Loss = -12525.6845703125
Iteration 9600: Loss = -12525.671875
Iteration 9700: Loss = -12525.662109375
Iteration 9800: Loss = -12525.6494140625
Iteration 9900: Loss = -12525.6396484375
Iteration 10000: Loss = -12525.6279296875
Iteration 10100: Loss = -12525.619140625
Iteration 10200: Loss = -12525.6103515625
Iteration 10300: Loss = -12525.59765625
Iteration 10400: Loss = -12525.58984375
Iteration 10500: Loss = -12525.5791015625
Iteration 10600: Loss = -12525.5712890625
Iteration 10700: Loss = -12525.5615234375
Iteration 10800: Loss = -12525.5556640625
Iteration 10900: Loss = -12525.546875
Iteration 11000: Loss = -12525.54296875
Iteration 11100: Loss = -12525.5322265625
Iteration 11200: Loss = -12525.5263671875
Iteration 11300: Loss = -12525.51953125
Iteration 11400: Loss = -12525.515625
Iteration 11500: Loss = -12525.5126953125
Iteration 11600: Loss = -12525.505859375
Iteration 11700: Loss = -12525.4990234375
Iteration 11800: Loss = -12525.4970703125
Iteration 11900: Loss = -12525.4912109375
Iteration 12000: Loss = -12525.486328125
Iteration 12100: Loss = -12525.484375
Iteration 12200: Loss = -12525.4814453125
Iteration 12300: Loss = -12525.478515625
Iteration 12400: Loss = -12525.4755859375
Iteration 12500: Loss = -12525.4736328125
Iteration 12600: Loss = -12525.4697265625
Iteration 12700: Loss = -12525.466796875
Iteration 12800: Loss = -12525.46484375
Iteration 12900: Loss = -12525.462890625
Iteration 13000: Loss = -12525.4599609375
Iteration 13100: Loss = -12525.45703125
Iteration 13200: Loss = -12525.45703125
Iteration 13300: Loss = -12525.4541015625
Iteration 13400: Loss = -12525.453125
Iteration 13500: Loss = -12525.4521484375
Iteration 13600: Loss = -12525.4501953125
Iteration 13700: Loss = -12525.4482421875
Iteration 13800: Loss = -12525.4462890625
Iteration 13900: Loss = -12525.4462890625
Iteration 14000: Loss = -12525.443359375
Iteration 14100: Loss = -12525.443359375
Iteration 14200: Loss = -12525.4423828125
Iteration 14300: Loss = -12525.44140625
Iteration 14400: Loss = -12525.439453125
Iteration 14500: Loss = -12525.44140625
1
Iteration 14600: Loss = -12525.4375
Iteration 14700: Loss = -12525.4375
Iteration 14800: Loss = -12525.4375
Iteration 14900: Loss = -12525.435546875
Iteration 15000: Loss = -12525.435546875
Iteration 15100: Loss = -12525.4345703125
Iteration 15200: Loss = -12525.4326171875
Iteration 15300: Loss = -12525.4326171875
Iteration 15400: Loss = -12525.4326171875
Iteration 15500: Loss = -12525.431640625
Iteration 15600: Loss = -12525.43359375
1
Iteration 15700: Loss = -12525.431640625
Iteration 15800: Loss = -12525.4306640625
Iteration 15900: Loss = -12525.431640625
1
Iteration 16000: Loss = -12525.4306640625
Iteration 16100: Loss = -12525.4306640625
Iteration 16200: Loss = -12525.4287109375
Iteration 16300: Loss = -12525.4306640625
1
Iteration 16400: Loss = -12525.427734375
Iteration 16500: Loss = -12525.431640625
1
Iteration 16600: Loss = -12525.4287109375
2
Iteration 16700: Loss = -12525.4296875
3
Iteration 16800: Loss = -12525.427734375
Iteration 16900: Loss = -12525.4287109375
1
Iteration 17000: Loss = -12525.427734375
Iteration 17100: Loss = -12525.4267578125
Iteration 17200: Loss = -12525.4287109375
1
Iteration 17300: Loss = -12525.427734375
2
Iteration 17400: Loss = -12525.4267578125
Iteration 17500: Loss = -12525.42578125
Iteration 17600: Loss = -12525.4267578125
1
Iteration 17700: Loss = -12525.427734375
2
Iteration 17800: Loss = -12525.4248046875
Iteration 17900: Loss = -12525.42578125
1
Iteration 18000: Loss = -12525.4248046875
Iteration 18100: Loss = -12525.4267578125
1
Iteration 18200: Loss = -12525.4248046875
Iteration 18300: Loss = -12525.4248046875
Iteration 18400: Loss = -12525.4267578125
1
Iteration 18500: Loss = -12525.4248046875
Iteration 18600: Loss = -12525.42578125
1
Iteration 18700: Loss = -12525.4248046875
Iteration 18800: Loss = -12525.423828125
Iteration 18900: Loss = -12525.4248046875
1
Iteration 19000: Loss = -12525.4248046875
2
Iteration 19100: Loss = -12525.4248046875
3
Iteration 19200: Loss = -12525.42578125
4
Iteration 19300: Loss = -12525.4248046875
5
Iteration 19400: Loss = -12525.42578125
6
Iteration 19500: Loss = -12525.423828125
Iteration 19600: Loss = -12525.4248046875
1
Iteration 19700: Loss = -12525.42578125
2
Iteration 19800: Loss = -12525.4248046875
3
Iteration 19900: Loss = -12525.42578125
4
Iteration 20000: Loss = -12525.4248046875
5
Iteration 20100: Loss = -12525.4248046875
6
Iteration 20200: Loss = -12525.4248046875
7
Iteration 20300: Loss = -12525.4248046875
8
Iteration 20400: Loss = -12525.423828125
Iteration 20500: Loss = -12525.42578125
1
Iteration 20600: Loss = -12525.4248046875
2
Iteration 20700: Loss = -12525.4248046875
3
Iteration 20800: Loss = -12525.42578125
4
Iteration 20900: Loss = -12525.4248046875
5
Iteration 21000: Loss = -12525.42578125
6
Iteration 21100: Loss = -12525.423828125
Iteration 21200: Loss = -12525.4248046875
1
Iteration 21300: Loss = -12525.4248046875
2
Iteration 21400: Loss = -12525.4248046875
3
Iteration 21500: Loss = -12525.4248046875
4
Iteration 21600: Loss = -12525.4248046875
5
Iteration 21700: Loss = -12525.4248046875
6
Iteration 21800: Loss = -12525.42578125
7
Iteration 21900: Loss = -12525.42578125
8
Iteration 22000: Loss = -12525.4248046875
9
Iteration 22100: Loss = -12525.423828125
Iteration 22200: Loss = -12525.4248046875
1
Iteration 22300: Loss = -12525.42578125
2
Iteration 22400: Loss = -12525.423828125
Iteration 22500: Loss = -12525.4248046875
1
Iteration 22600: Loss = -12525.4248046875
2
Iteration 22700: Loss = -12525.4248046875
3
Iteration 22800: Loss = -12525.423828125
Iteration 22900: Loss = -12525.4267578125
1
Iteration 23000: Loss = -12525.4248046875
2
Iteration 23100: Loss = -12525.423828125
Iteration 23200: Loss = -12525.423828125
Iteration 23300: Loss = -12525.423828125
Iteration 23400: Loss = -12525.42578125
1
Iteration 23500: Loss = -12525.4248046875
2
Iteration 23600: Loss = -12525.423828125
Iteration 23700: Loss = -12525.42578125
1
Iteration 23800: Loss = -12525.4248046875
2
Iteration 23900: Loss = -12525.423828125
Iteration 24000: Loss = -12525.423828125
Iteration 24100: Loss = -12525.423828125
Iteration 24200: Loss = -12525.4248046875
1
Iteration 24300: Loss = -12525.423828125
Iteration 24400: Loss = -12525.4248046875
1
Iteration 24500: Loss = -12525.4248046875
2
Iteration 24600: Loss = -12525.4248046875
3
Iteration 24700: Loss = -12525.4228515625
Iteration 24800: Loss = -12525.423828125
1
Iteration 24900: Loss = -12525.4248046875
2
Iteration 25000: Loss = -12525.4228515625
Iteration 25100: Loss = -12525.42578125
1
Iteration 25200: Loss = -12525.42578125
2
Iteration 25300: Loss = -12525.423828125
3
Iteration 25400: Loss = -12525.423828125
4
Iteration 25500: Loss = -12525.4248046875
5
Iteration 25600: Loss = -12525.4248046875
6
Iteration 25700: Loss = -12525.4248046875
7
Iteration 25800: Loss = -12525.4248046875
8
Iteration 25900: Loss = -12525.4248046875
9
Iteration 26000: Loss = -12525.4248046875
10
Iteration 26100: Loss = -12525.423828125
11
Iteration 26200: Loss = -12525.4248046875
12
Iteration 26300: Loss = -12525.4248046875
13
Iteration 26400: Loss = -12525.42578125
14
Iteration 26500: Loss = -12525.423828125
15
Stopping early at iteration 26500 due to no improvement.
pi: tensor([[1.0000e+00, 3.0374e-06],
        [1.0000e+00, 8.4837e-07]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0174, 0.9826], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2060, 0.2005],
         [0.9767, 0.1968]],

        [[0.0153, 0.2164],
         [0.8515, 0.8884]],

        [[0.3926, 0.2484],
         [0.6643, 0.9351]],

        [[0.1178, 0.1962],
         [0.4855, 0.0347]],

        [[0.2920, 0.2018],
         [0.0129, 0.9876]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00031784252347700457
Average Adjusted Rand Index: 0.0
[0.00031784252347700457, 0.00031784252347700457] [0.0, 0.0] [12525.1123046875, 12525.423828125]
-------------------------------------
This iteration is 79
True Objective function: Loss = -11794.3232409395
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37997.15625
Iteration 100: Loss = -26298.1796875
Iteration 200: Loss = -16552.55078125
Iteration 300: Loss = -13838.154296875
Iteration 400: Loss = -12900.31640625
Iteration 500: Loss = -12611.2197265625
Iteration 600: Loss = -12534.435546875
Iteration 700: Loss = -12492.216796875
Iteration 800: Loss = -12465.447265625
Iteration 900: Loss = -12447.1015625
Iteration 1000: Loss = -12433.845703125
Iteration 1100: Loss = -12423.8828125
Iteration 1200: Loss = -12416.1669921875
Iteration 1300: Loss = -12410.0458984375
Iteration 1400: Loss = -12405.0986328125
Iteration 1500: Loss = -12401.0322265625
Iteration 1600: Loss = -12397.6474609375
Iteration 1700: Loss = -12394.791015625
Iteration 1800: Loss = -12392.3603515625
Iteration 1900: Loss = -12390.2705078125
Iteration 2000: Loss = -12388.4619140625
Iteration 2100: Loss = -12386.8837890625
Iteration 2200: Loss = -12385.5
Iteration 2300: Loss = -12384.2734375
Iteration 2400: Loss = -12383.1875
Iteration 2500: Loss = -12382.220703125
Iteration 2600: Loss = -12381.3515625
Iteration 2700: Loss = -12380.572265625
Iteration 2800: Loss = -12379.8662109375
Iteration 2900: Loss = -12379.23046875
Iteration 3000: Loss = -12378.6513671875
Iteration 3100: Loss = -12378.1220703125
Iteration 3200: Loss = -12377.640625
Iteration 3300: Loss = -12377.1982421875
Iteration 3400: Loss = -12376.7919921875
Iteration 3500: Loss = -12376.4208984375
Iteration 3600: Loss = -12376.078125
Iteration 3700: Loss = -12375.7626953125
Iteration 3800: Loss = -12375.46875
Iteration 3900: Loss = -12375.197265625
Iteration 4000: Loss = -12374.9482421875
Iteration 4100: Loss = -12374.7138671875
Iteration 4200: Loss = -12374.4970703125
Iteration 4300: Loss = -12374.294921875
Iteration 4400: Loss = -12374.1083984375
Iteration 4500: Loss = -12373.9326171875
Iteration 4600: Loss = -12373.767578125
Iteration 4700: Loss = -12373.6142578125
Iteration 4800: Loss = -12373.4716796875
Iteration 4900: Loss = -12373.3359375
Iteration 5000: Loss = -12373.2099609375
Iteration 5100: Loss = -12373.0908203125
Iteration 5200: Loss = -12372.98046875
Iteration 5300: Loss = -12372.876953125
Iteration 5400: Loss = -12372.779296875
Iteration 5500: Loss = -12372.6865234375
Iteration 5600: Loss = -12372.6005859375
Iteration 5700: Loss = -12372.5185546875
Iteration 5800: Loss = -12372.4404296875
Iteration 5900: Loss = -12372.3671875
Iteration 6000: Loss = -12372.2998046875
Iteration 6100: Loss = -12372.234375
Iteration 6200: Loss = -12372.173828125
Iteration 6300: Loss = -12372.1162109375
Iteration 6400: Loss = -12372.0615234375
Iteration 6500: Loss = -12372.01171875
Iteration 6600: Loss = -12371.9619140625
Iteration 6700: Loss = -12371.916015625
Iteration 6800: Loss = -12371.873046875
Iteration 6900: Loss = -12371.83203125
Iteration 7000: Loss = -12371.794921875
Iteration 7100: Loss = -12371.7568359375
Iteration 7200: Loss = -12371.720703125
Iteration 7300: Loss = -12371.689453125
Iteration 7400: Loss = -12371.658203125
Iteration 7500: Loss = -12371.6279296875
Iteration 7600: Loss = -12371.599609375
Iteration 7700: Loss = -12371.57421875
Iteration 7800: Loss = -12371.5478515625
Iteration 7900: Loss = -12371.525390625
Iteration 8000: Loss = -12371.5009765625
Iteration 8100: Loss = -12371.48046875
Iteration 8200: Loss = -12371.4609375
Iteration 8300: Loss = -12371.44140625
Iteration 8400: Loss = -12371.4228515625
Iteration 8500: Loss = -12371.404296875
Iteration 8600: Loss = -12371.3876953125
Iteration 8700: Loss = -12371.373046875
Iteration 8800: Loss = -12371.3583984375
Iteration 8900: Loss = -12371.3447265625
Iteration 9000: Loss = -12371.3310546875
Iteration 9100: Loss = -12371.3173828125
Iteration 9200: Loss = -12371.3056640625
Iteration 9300: Loss = -12371.294921875
Iteration 9400: Loss = -12371.2841796875
Iteration 9500: Loss = -12371.2734375
Iteration 9600: Loss = -12371.2646484375
Iteration 9700: Loss = -12371.2548828125
Iteration 9800: Loss = -12371.2470703125
Iteration 9900: Loss = -12371.23828125
Iteration 10000: Loss = -12371.2314453125
Iteration 10100: Loss = -12371.2236328125
Iteration 10200: Loss = -12371.21484375
Iteration 10300: Loss = -12371.2099609375
Iteration 10400: Loss = -12371.2021484375
Iteration 10500: Loss = -12371.1962890625
Iteration 10600: Loss = -12371.1923828125
Iteration 10700: Loss = -12371.1865234375
Iteration 10800: Loss = -12371.1796875
Iteration 10900: Loss = -12371.17578125
Iteration 11000: Loss = -12371.171875
Iteration 11100: Loss = -12371.1650390625
Iteration 11200: Loss = -12371.16015625
Iteration 11300: Loss = -12371.158203125
Iteration 11400: Loss = -12371.1552734375
Iteration 11500: Loss = -12371.150390625
Iteration 11600: Loss = -12371.146484375
Iteration 11700: Loss = -12371.14453125
Iteration 11800: Loss = -12371.140625
Iteration 11900: Loss = -12371.138671875
Iteration 12000: Loss = -12371.134765625
Iteration 12100: Loss = -12371.1328125
Iteration 12200: Loss = -12371.1298828125
Iteration 12300: Loss = -12371.12890625
Iteration 12400: Loss = -12371.1259765625
Iteration 12500: Loss = -12371.123046875
Iteration 12600: Loss = -12371.12109375
Iteration 12700: Loss = -12371.119140625
Iteration 12800: Loss = -12371.1162109375
Iteration 12900: Loss = -12371.115234375
Iteration 13000: Loss = -12371.1142578125
Iteration 13100: Loss = -12371.1123046875
Iteration 13200: Loss = -12371.111328125
Iteration 13300: Loss = -12371.1103515625
Iteration 13400: Loss = -12371.109375
Iteration 13500: Loss = -12371.10546875
Iteration 13600: Loss = -12371.10546875
Iteration 13700: Loss = -12371.1044921875
Iteration 13800: Loss = -12371.1044921875
Iteration 13900: Loss = -12371.1005859375
Iteration 14000: Loss = -12371.1005859375
Iteration 14100: Loss = -12371.1005859375
Iteration 14200: Loss = -12371.099609375
Iteration 14300: Loss = -12371.0986328125
Iteration 14400: Loss = -12371.09765625
Iteration 14500: Loss = -12371.09765625
Iteration 14600: Loss = -12371.095703125
Iteration 14700: Loss = -12371.095703125
Iteration 14800: Loss = -12371.095703125
Iteration 14900: Loss = -12371.0947265625
Iteration 15000: Loss = -12371.095703125
1
Iteration 15100: Loss = -12371.0927734375
Iteration 15200: Loss = -12371.09375
1
Iteration 15300: Loss = -12371.091796875
Iteration 15400: Loss = -12371.0927734375
1
Iteration 15500: Loss = -12371.091796875
Iteration 15600: Loss = -12371.0927734375
1
Iteration 15700: Loss = -12371.091796875
Iteration 15800: Loss = -12371.08984375
Iteration 15900: Loss = -12371.08984375
Iteration 16000: Loss = -12371.0888671875
Iteration 16100: Loss = -12371.0888671875
Iteration 16200: Loss = -12371.087890625
Iteration 16300: Loss = -12371.0888671875
1
Iteration 16400: Loss = -12371.0869140625
Iteration 16500: Loss = -12371.0869140625
Iteration 16600: Loss = -12371.0869140625
Iteration 16700: Loss = -12371.0869140625
Iteration 16800: Loss = -12371.0869140625
Iteration 16900: Loss = -12371.087890625
1
Iteration 17000: Loss = -12371.0869140625
Iteration 17100: Loss = -12371.087890625
1
Iteration 17200: Loss = -12371.0869140625
Iteration 17300: Loss = -12371.0849609375
Iteration 17400: Loss = -12371.0869140625
1
Iteration 17500: Loss = -12371.0859375
2
Iteration 17600: Loss = -12371.0869140625
3
Iteration 17700: Loss = -12371.0859375
4
Iteration 17800: Loss = -12371.0869140625
5
Iteration 17900: Loss = -12371.0859375
6
Iteration 18000: Loss = -12371.0849609375
Iteration 18100: Loss = -12371.0849609375
Iteration 18200: Loss = -12371.0849609375
Iteration 18300: Loss = -12371.0849609375
Iteration 18400: Loss = -12371.0849609375
Iteration 18500: Loss = -12371.0849609375
Iteration 18600: Loss = -12371.0849609375
Iteration 18700: Loss = -12371.0869140625
1
Iteration 18800: Loss = -12371.0849609375
Iteration 18900: Loss = -12371.083984375
Iteration 19000: Loss = -12371.0849609375
1
Iteration 19100: Loss = -12371.0849609375
2
Iteration 19200: Loss = -12371.0849609375
3
Iteration 19300: Loss = -12371.0849609375
4
Iteration 19400: Loss = -12371.0849609375
5
Iteration 19500: Loss = -12371.0849609375
6
Iteration 19600: Loss = -12371.0849609375
7
Iteration 19700: Loss = -12371.0849609375
8
Iteration 19800: Loss = -12371.0849609375
9
Iteration 19900: Loss = -12371.0849609375
10
Iteration 20000: Loss = -12371.083984375
Iteration 20100: Loss = -12371.0849609375
1
Iteration 20200: Loss = -12371.0849609375
2
Iteration 20300: Loss = -12371.0849609375
3
Iteration 20400: Loss = -12371.0849609375
4
Iteration 20500: Loss = -12371.0888671875
5
Iteration 20600: Loss = -12371.0859375
6
Iteration 20700: Loss = -12371.0849609375
7
Iteration 20800: Loss = -12371.0830078125
Iteration 20900: Loss = -12371.0859375
1
Iteration 21000: Loss = -12371.0849609375
2
Iteration 21100: Loss = -12371.0849609375
3
Iteration 21200: Loss = -12371.0849609375
4
Iteration 21300: Loss = -12371.083984375
5
Iteration 21400: Loss = -12371.083984375
6
Iteration 21500: Loss = -12371.0849609375
7
Iteration 21600: Loss = -12371.083984375
8
Iteration 21700: Loss = -12371.083984375
9
Iteration 21800: Loss = -12371.0849609375
10
Iteration 21900: Loss = -12371.083984375
11
Iteration 22000: Loss = -12371.083984375
12
Iteration 22100: Loss = -12371.083984375
13
Iteration 22200: Loss = -12371.091796875
14
Iteration 22300: Loss = -12371.0849609375
15
Stopping early at iteration 22300 due to no improvement.
pi: tensor([[1.7159e-02, 9.8284e-01],
        [2.7902e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.3709e-06, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0364, 0.2183],
         [0.9926, 0.1996]],

        [[0.0161, 0.3507],
         [0.0267, 0.8286]],

        [[0.2557, 0.2119],
         [0.1157, 0.9169]],

        [[0.4904, 0.2009],
         [0.9862, 0.0341]],

        [[0.6828, 0.2222],
         [0.9652, 0.0570]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31582.013671875
Iteration 100: Loss = -20764.580078125
Iteration 200: Loss = -14231.3232421875
Iteration 300: Loss = -12954.2080078125
Iteration 400: Loss = -12671.107421875
Iteration 500: Loss = -12590.01953125
Iteration 600: Loss = -12549.1123046875
Iteration 700: Loss = -12522.822265625
Iteration 800: Loss = -12506.078125
Iteration 900: Loss = -12494.3134765625
Iteration 1000: Loss = -12485.595703125
Iteration 1100: Loss = -12478.845703125
Iteration 1200: Loss = -12473.3974609375
Iteration 1300: Loss = -12468.7958984375
Iteration 1400: Loss = -12464.8896484375
Iteration 1500: Loss = -12461.5888671875
Iteration 1600: Loss = -12457.06640625
Iteration 1700: Loss = -12454.04296875
Iteration 1800: Loss = -12450.74609375
Iteration 1900: Loss = -12447.8564453125
Iteration 2000: Loss = -12445.6689453125
Iteration 2100: Loss = -12443.1123046875
Iteration 2200: Loss = -12439.64453125
Iteration 2300: Loss = -12435.685546875
Iteration 2400: Loss = -12432.318359375
Iteration 2500: Loss = -12428.611328125
Iteration 2600: Loss = -12424.3564453125
Iteration 2700: Loss = -12420.3916015625
Iteration 2800: Loss = -12416.4912109375
Iteration 2900: Loss = -12412.791015625
Iteration 3000: Loss = -12410.21484375
Iteration 3100: Loss = -12408.0693359375
Iteration 3200: Loss = -12406.138671875
Iteration 3300: Loss = -12404.3037109375
Iteration 3400: Loss = -12402.7099609375
Iteration 3500: Loss = -12401.3115234375
Iteration 3600: Loss = -12399.5751953125
Iteration 3700: Loss = -12397.73828125
Iteration 3800: Loss = -12394.5400390625
Iteration 3900: Loss = -12393.7177734375
Iteration 4000: Loss = -12393.1044921875
Iteration 4100: Loss = -12391.498046875
Iteration 4200: Loss = -12390.2919921875
Iteration 4300: Loss = -12389.9033203125
Iteration 4400: Loss = -12389.57421875
Iteration 4500: Loss = -12387.2607421875
Iteration 4600: Loss = -12385.021484375
Iteration 4700: Loss = -12384.697265625
Iteration 4800: Loss = -12384.50390625
Iteration 4900: Loss = -12384.3427734375
Iteration 5000: Loss = -12384.197265625
Iteration 5100: Loss = -12384.0341796875
Iteration 5200: Loss = -12382.283203125
Iteration 5300: Loss = -12381.6962890625
Iteration 5400: Loss = -12381.521484375
Iteration 5500: Loss = -12381.396484375
Iteration 5600: Loss = -12381.2890625
Iteration 5700: Loss = -12381.193359375
Iteration 5800: Loss = -12381.1044921875
Iteration 5900: Loss = -12380.9970703125
Iteration 6000: Loss = -12378.173828125
Iteration 6100: Loss = -12377.8828125
Iteration 6200: Loss = -12377.7431640625
Iteration 6300: Loss = -12377.6435546875
Iteration 6400: Loss = -12377.5625
Iteration 6500: Loss = -12377.498046875
Iteration 6600: Loss = -12377.439453125
Iteration 6700: Loss = -12377.388671875
Iteration 6800: Loss = -12377.34375
Iteration 6900: Loss = -12377.30078125
Iteration 7000: Loss = -12377.2626953125
Iteration 7100: Loss = -12377.2265625
Iteration 7200: Loss = -12377.193359375
Iteration 7300: Loss = -12377.1630859375
Iteration 7400: Loss = -12377.1328125
Iteration 7500: Loss = -12377.10546875
Iteration 7600: Loss = -12377.0791015625
Iteration 7700: Loss = -12377.0546875
Iteration 7800: Loss = -12377.033203125
Iteration 7900: Loss = -12377.009765625
Iteration 8000: Loss = -12376.9892578125
Iteration 8100: Loss = -12376.970703125
Iteration 8200: Loss = -12376.9521484375
Iteration 8300: Loss = -12376.93359375
Iteration 8400: Loss = -12376.9189453125
Iteration 8500: Loss = -12376.9013671875
Iteration 8600: Loss = -12376.8857421875
Iteration 8700: Loss = -12376.8701171875
Iteration 8800: Loss = -12376.8564453125
Iteration 8900: Loss = -12376.84375
Iteration 9000: Loss = -12376.8310546875
Iteration 9100: Loss = -12376.818359375
Iteration 9200: Loss = -12376.8076171875
Iteration 9300: Loss = -12376.7470703125
Iteration 9400: Loss = -12376.734375
Iteration 9500: Loss = -12376.7216796875
Iteration 9600: Loss = -12376.7138671875
Iteration 9700: Loss = -12376.705078125
Iteration 9800: Loss = -12376.6953125
Iteration 9900: Loss = -12376.685546875
Iteration 10000: Loss = -12376.6728515625
Iteration 10100: Loss = -12376.650390625
Iteration 10200: Loss = -12376.619140625
Iteration 10300: Loss = -12376.5966796875
Iteration 10400: Loss = -12376.5830078125
Iteration 10500: Loss = -12376.572265625
Iteration 10600: Loss = -12376.5634765625
Iteration 10700: Loss = -12376.5556640625
Iteration 10800: Loss = -12376.5517578125
Iteration 10900: Loss = -12376.5458984375
Iteration 11000: Loss = -12376.5439453125
Iteration 11100: Loss = -12376.5380859375
Iteration 11200: Loss = -12376.5322265625
Iteration 11300: Loss = -12376.5283203125
Iteration 11400: Loss = -12376.5244140625
Iteration 11500: Loss = -12376.5205078125
Iteration 11600: Loss = -12376.5166015625
Iteration 11700: Loss = -12376.5146484375
Iteration 11800: Loss = -12376.51171875
Iteration 11900: Loss = -12376.5107421875
Iteration 12000: Loss = -12376.505859375
Iteration 12100: Loss = -12376.5048828125
Iteration 12200: Loss = -12376.501953125
Iteration 12300: Loss = -12376.4970703125
Iteration 12400: Loss = -12376.4775390625
Iteration 12500: Loss = -12373.3388671875
Iteration 12600: Loss = -12373.197265625
Iteration 12700: Loss = -12373.181640625
Iteration 12800: Loss = -12373.1748046875
Iteration 12900: Loss = -12373.1728515625
Iteration 13000: Loss = -12373.1708984375
Iteration 13100: Loss = -12373.16796875
Iteration 13200: Loss = -12373.1650390625
Iteration 13300: Loss = -12373.1640625
Iteration 13400: Loss = -12373.1650390625
1
Iteration 13500: Loss = -12373.1630859375
Iteration 13600: Loss = -12373.162109375
Iteration 13700: Loss = -12373.16015625
Iteration 13800: Loss = -12373.1611328125
1
Iteration 13900: Loss = -12373.16015625
Iteration 14000: Loss = -12373.158203125
Iteration 14100: Loss = -12373.158203125
Iteration 14200: Loss = -12373.1572265625
Iteration 14300: Loss = -12373.154296875
Iteration 14400: Loss = -12373.15234375
Iteration 14500: Loss = -12373.150390625
Iteration 14600: Loss = -12373.1484375
Iteration 14700: Loss = -12373.1474609375
Iteration 14800: Loss = -12373.1484375
1
Iteration 14900: Loss = -12373.1484375
2
Iteration 15000: Loss = -12373.146484375
Iteration 15100: Loss = -12373.146484375
Iteration 15200: Loss = -12373.1474609375
1
Iteration 15300: Loss = -12373.146484375
Iteration 15400: Loss = -12373.1455078125
Iteration 15500: Loss = -12373.1455078125
Iteration 15600: Loss = -12373.14453125
Iteration 15700: Loss = -12373.1435546875
Iteration 15800: Loss = -12373.14453125
1
Iteration 15900: Loss = -12373.1435546875
Iteration 16000: Loss = -12373.1455078125
1
Iteration 16100: Loss = -12373.14453125
2
Iteration 16200: Loss = -12373.1435546875
Iteration 16300: Loss = -12373.1435546875
Iteration 16400: Loss = -12373.1435546875
Iteration 16500: Loss = -12373.1455078125
1
Iteration 16600: Loss = -12373.1435546875
Iteration 16700: Loss = -12373.142578125
Iteration 16800: Loss = -12373.1435546875
1
Iteration 16900: Loss = -12373.14453125
2
Iteration 17000: Loss = -12373.1416015625
Iteration 17100: Loss = -12373.1416015625
Iteration 17200: Loss = -12373.1416015625
Iteration 17300: Loss = -12373.140625
Iteration 17400: Loss = -12373.1416015625
1
Iteration 17500: Loss = -12373.1416015625
2
Iteration 17600: Loss = -12373.1416015625
3
Iteration 17700: Loss = -12373.1396484375
Iteration 17800: Loss = -12373.123046875
Iteration 17900: Loss = -12370.982421875
Iteration 18000: Loss = -12370.9658203125
Iteration 18100: Loss = -12370.9609375
Iteration 18200: Loss = -12370.9580078125
Iteration 18300: Loss = -12370.95703125
Iteration 18400: Loss = -12370.95703125
Iteration 18500: Loss = -12370.955078125
Iteration 18600: Loss = -12370.955078125
Iteration 18700: Loss = -12370.955078125
Iteration 18800: Loss = -12370.953125
Iteration 18900: Loss = -12370.9677734375
1
Iteration 19000: Loss = -12370.9521484375
Iteration 19100: Loss = -12370.953125
1
Iteration 19200: Loss = -12370.951171875
Iteration 19300: Loss = -12370.9521484375
1
Iteration 19400: Loss = -12370.9521484375
2
Iteration 19500: Loss = -12370.951171875
Iteration 19600: Loss = -12370.9521484375
1
Iteration 19700: Loss = -12370.9501953125
Iteration 19800: Loss = -12370.951171875
1
Iteration 19900: Loss = -12370.9521484375
2
Iteration 20000: Loss = -12370.951171875
3
Iteration 20100: Loss = -12370.951171875
4
Iteration 20200: Loss = -12370.9501953125
Iteration 20300: Loss = -12370.953125
1
Iteration 20400: Loss = -12370.9501953125
Iteration 20500: Loss = -12370.9501953125
Iteration 20600: Loss = -12370.951171875
1
Iteration 20700: Loss = -12370.951171875
2
Iteration 20800: Loss = -12370.951171875
3
Iteration 20900: Loss = -12370.9501953125
Iteration 21000: Loss = -12370.9521484375
1
Iteration 21100: Loss = -12370.951171875
2
Iteration 21200: Loss = -12370.9501953125
Iteration 21300: Loss = -12370.951171875
1
Iteration 21400: Loss = -12370.951171875
2
Iteration 21500: Loss = -12370.951171875
3
Iteration 21600: Loss = -12370.9501953125
Iteration 21700: Loss = -12370.9501953125
Iteration 21800: Loss = -12370.9521484375
1
Iteration 21900: Loss = -12370.951171875
2
Iteration 22000: Loss = -12370.94921875
Iteration 22100: Loss = -12370.94921875
Iteration 22200: Loss = -12370.9501953125
1
Iteration 22300: Loss = -12370.9501953125
2
Iteration 22400: Loss = -12370.951171875
3
Iteration 22500: Loss = -12370.9501953125
4
Iteration 22600: Loss = -12370.951171875
5
Iteration 22700: Loss = -12370.951171875
6
Iteration 22800: Loss = -12370.951171875
7
Iteration 22900: Loss = -12370.951171875
8
Iteration 23000: Loss = -12370.951171875
9
Iteration 23100: Loss = -12370.9501953125
10
Iteration 23200: Loss = -12370.951171875
11
Iteration 23300: Loss = -12370.9501953125
12
Iteration 23400: Loss = -12370.951171875
13
Iteration 23500: Loss = -12370.951171875
14
Iteration 23600: Loss = -12370.951171875
15
Stopping early at iteration 23600 due to no improvement.
pi: tensor([[8.2121e-04, 9.9918e-01],
        [2.5003e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 3.5162e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1970, 0.2175],
         [0.0255, 0.2002]],

        [[0.9478, 0.2069],
         [0.1184, 0.9922]],

        [[0.1749, 0.6596],
         [0.9780, 0.0084]],

        [[0.2438, 0.1982],
         [0.0803, 0.1829]],

        [[0.7273, 0.2206],
         [0.0084, 0.0519]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0009354685621591986
Average Adjusted Rand Index: 0.0
[0.0, -0.0009354685621591986] [0.0, 0.0] [12371.0849609375, 12370.951171875]
-------------------------------------
This iteration is 80
True Objective function: Loss = -11934.608376665668
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -38854.09765625
Iteration 100: Loss = -22068.046875
Iteration 200: Loss = -14615.5576171875
Iteration 300: Loss = -13014.6513671875
Iteration 400: Loss = -12724.068359375
Iteration 500: Loss = -12605.9072265625
Iteration 600: Loss = -12541.2216796875
Iteration 700: Loss = -12509.3408203125
Iteration 800: Loss = -12490.6953125
Iteration 900: Loss = -12477.9951171875
Iteration 1000: Loss = -12468.8447265625
Iteration 1100: Loss = -12461.98828125
Iteration 1200: Loss = -12456.7001953125
Iteration 1300: Loss = -12452.5224609375
Iteration 1400: Loss = -12449.154296875
Iteration 1500: Loss = -12446.392578125
Iteration 1600: Loss = -12444.0966796875
Iteration 1700: Loss = -12442.1484375
Iteration 1800: Loss = -12440.462890625
Iteration 1900: Loss = -12439.06640625
Iteration 2000: Loss = -12437.8818359375
Iteration 2100: Loss = -12436.8583984375
Iteration 2200: Loss = -12435.9677734375
Iteration 2300: Loss = -12435.1884765625
Iteration 2400: Loss = -12434.5029296875
Iteration 2500: Loss = -12433.890625
Iteration 2600: Loss = -12433.3447265625
Iteration 2700: Loss = -12432.85546875
Iteration 2800: Loss = -12432.421875
Iteration 2900: Loss = -12432.0322265625
Iteration 3000: Loss = -12431.6796875
Iteration 3100: Loss = -12431.3623046875
Iteration 3200: Loss = -12431.072265625
Iteration 3300: Loss = -12430.8046875
Iteration 3400: Loss = -12430.5634765625
Iteration 3500: Loss = -12430.3427734375
Iteration 3600: Loss = -12430.14453125
Iteration 3700: Loss = -12429.966796875
Iteration 3800: Loss = -12429.806640625
Iteration 3900: Loss = -12429.6611328125
Iteration 4000: Loss = -12429.5234375
Iteration 4100: Loss = -12429.408203125
Iteration 4200: Loss = -12429.302734375
Iteration 4300: Loss = -12429.20703125
Iteration 4400: Loss = -12429.1162109375
Iteration 4500: Loss = -12429.0341796875
Iteration 4600: Loss = -12428.9560546875
Iteration 4700: Loss = -12428.8837890625
Iteration 4800: Loss = -12428.8173828125
Iteration 4900: Loss = -12428.7529296875
Iteration 5000: Loss = -12428.693359375
Iteration 5100: Loss = -12428.6376953125
Iteration 5200: Loss = -12428.5849609375
Iteration 5300: Loss = -12428.5361328125
Iteration 5400: Loss = -12428.48828125
Iteration 5500: Loss = -12428.4443359375
Iteration 5600: Loss = -12428.40234375
Iteration 5700: Loss = -12428.359375
Iteration 5800: Loss = -12428.3212890625
Iteration 5900: Loss = -12428.28515625
Iteration 6000: Loss = -12428.2509765625
Iteration 6100: Loss = -12428.2158203125
Iteration 6200: Loss = -12428.1845703125
Iteration 6300: Loss = -12428.154296875
Iteration 6400: Loss = -12428.123046875
Iteration 6500: Loss = -12428.091796875
Iteration 6600: Loss = -12428.0654296875
Iteration 6700: Loss = -12428.0390625
Iteration 6800: Loss = -12428.0126953125
Iteration 6900: Loss = -12427.98828125
Iteration 7000: Loss = -12427.9658203125
Iteration 7100: Loss = -12427.9423828125
Iteration 7200: Loss = -12427.919921875
Iteration 7300: Loss = -12427.900390625
Iteration 7400: Loss = -12427.8798828125
Iteration 7500: Loss = -12427.8623046875
Iteration 7600: Loss = -12427.841796875
Iteration 7700: Loss = -12427.826171875
Iteration 7800: Loss = -12427.8095703125
Iteration 7900: Loss = -12427.79296875
Iteration 8000: Loss = -12427.7783203125
Iteration 8100: Loss = -12427.7646484375
Iteration 8200: Loss = -12427.751953125
Iteration 8300: Loss = -12427.7373046875
Iteration 8400: Loss = -12427.7236328125
Iteration 8500: Loss = -12427.7109375
Iteration 8600: Loss = -12427.701171875
Iteration 8700: Loss = -12427.6884765625
Iteration 8800: Loss = -12427.67578125
Iteration 8900: Loss = -12427.666015625
Iteration 9000: Loss = -12427.65625
Iteration 9100: Loss = -12427.64453125
Iteration 9200: Loss = -12427.634765625
Iteration 9300: Loss = -12427.625
Iteration 9400: Loss = -12427.615234375
Iteration 9500: Loss = -12427.603515625
Iteration 9600: Loss = -12427.595703125
Iteration 9700: Loss = -12427.5830078125
Iteration 9800: Loss = -12427.5732421875
Iteration 9900: Loss = -12427.560546875
Iteration 10000: Loss = -12427.548828125
Iteration 10100: Loss = -12427.5390625
Iteration 10200: Loss = -12427.5263671875
Iteration 10300: Loss = -12427.5126953125
Iteration 10400: Loss = -12427.498046875
Iteration 10500: Loss = -12427.48046875
Iteration 10600: Loss = -12427.4619140625
Iteration 10700: Loss = -12427.4423828125
Iteration 10800: Loss = -12427.4140625
Iteration 10900: Loss = -12427.3798828125
Iteration 11000: Loss = -12427.150390625
Iteration 11100: Loss = -12426.08984375
Iteration 11200: Loss = -12425.919921875
Iteration 11300: Loss = -12425.7890625
Iteration 11400: Loss = -12424.6064453125
Iteration 11500: Loss = -12418.369140625
Iteration 11600: Loss = -12417.50390625
Iteration 11700: Loss = -12405.685546875
Iteration 11800: Loss = -12399.2421875
Iteration 11900: Loss = -12388.59765625
Iteration 12000: Loss = -12370.1767578125
Iteration 12100: Loss = -12361.6904296875
Iteration 12200: Loss = -12358.072265625
Iteration 12300: Loss = -12353.1064453125
Iteration 12400: Loss = -12352.9716796875
Iteration 12500: Loss = -12321.3583984375
Iteration 12600: Loss = -12255.93359375
Iteration 12700: Loss = -12081.896484375
Iteration 12800: Loss = -11963.52734375
Iteration 12900: Loss = -11929.548828125
Iteration 13000: Loss = -11928.2314453125
Iteration 13100: Loss = -11927.728515625
Iteration 13200: Loss = -11927.4384765625
Iteration 13300: Loss = -11927.244140625
Iteration 13400: Loss = -11927.1083984375
Iteration 13500: Loss = -11927.0048828125
Iteration 13600: Loss = -11926.9248046875
Iteration 13700: Loss = -11926.8623046875
Iteration 13800: Loss = -11926.8095703125
Iteration 13900: Loss = -11926.767578125
Iteration 14000: Loss = -11926.732421875
Iteration 14100: Loss = -11926.7021484375
Iteration 14200: Loss = -11926.67578125
Iteration 14300: Loss = -11926.65234375
Iteration 14400: Loss = -11926.6337890625
Iteration 14500: Loss = -11926.615234375
Iteration 14600: Loss = -11926.599609375
Iteration 14700: Loss = -11926.5859375
Iteration 14800: Loss = -11926.5732421875
Iteration 14900: Loss = -11926.5625
Iteration 15000: Loss = -11926.55078125
Iteration 15100: Loss = -11926.5419921875
Iteration 15200: Loss = -11926.5341796875
Iteration 15300: Loss = -11926.525390625
Iteration 15400: Loss = -11926.51953125
Iteration 15500: Loss = -11926.513671875
Iteration 15600: Loss = -11926.5087890625
Iteration 15700: Loss = -11926.501953125
Iteration 15800: Loss = -11926.4970703125
Iteration 15900: Loss = -11926.4931640625
Iteration 16000: Loss = -11926.48828125
Iteration 16100: Loss = -11926.484375
Iteration 16200: Loss = -11926.4814453125
Iteration 16300: Loss = -11926.4775390625
Iteration 16400: Loss = -11926.474609375
Iteration 16500: Loss = -11926.47265625
Iteration 16600: Loss = -11926.46875
Iteration 16700: Loss = -11926.466796875
Iteration 16800: Loss = -11926.46484375
Iteration 16900: Loss = -11926.4619140625
Iteration 17000: Loss = -11926.4599609375
Iteration 17100: Loss = -11926.458984375
Iteration 17200: Loss = -11926.45703125
Iteration 17300: Loss = -11926.455078125
Iteration 17400: Loss = -11926.4541015625
Iteration 17500: Loss = -11926.453125
Iteration 17600: Loss = -11926.451171875
Iteration 17700: Loss = -11926.4482421875
Iteration 17800: Loss = -11926.447265625
Iteration 17900: Loss = -11926.4462890625
Iteration 18000: Loss = -11926.4453125
Iteration 18100: Loss = -11926.443359375
Iteration 18200: Loss = -11926.4423828125
Iteration 18300: Loss = -11926.4423828125
Iteration 18400: Loss = -11926.44140625
Iteration 18500: Loss = -11926.44140625
Iteration 18600: Loss = -11926.4404296875
Iteration 18700: Loss = -11926.439453125
Iteration 18800: Loss = -11926.4384765625
Iteration 18900: Loss = -11926.4375
Iteration 19000: Loss = -11926.4375
Iteration 19100: Loss = -11926.4375
Iteration 19200: Loss = -11926.4375
Iteration 19300: Loss = -11926.435546875
Iteration 19400: Loss = -11926.435546875
Iteration 19500: Loss = -11926.4345703125
Iteration 19600: Loss = -11926.43359375
Iteration 19700: Loss = -11926.4345703125
1
Iteration 19800: Loss = -11926.43359375
Iteration 19900: Loss = -11926.4326171875
Iteration 20000: Loss = -11926.43359375
1
Iteration 20100: Loss = -11926.431640625
Iteration 20200: Loss = -11926.4326171875
1
Iteration 20300: Loss = -11926.4306640625
Iteration 20400: Loss = -11926.431640625
1
Iteration 20500: Loss = -11926.4326171875
2
Iteration 20600: Loss = -11926.431640625
3
Iteration 20700: Loss = -11926.4296875
Iteration 20800: Loss = -11926.4306640625
1
Iteration 20900: Loss = -11926.4248046875
Iteration 21000: Loss = -11926.4248046875
Iteration 21100: Loss = -11926.4248046875
Iteration 21200: Loss = -11926.423828125
Iteration 21300: Loss = -11926.423828125
Iteration 21400: Loss = -11926.423828125
Iteration 21500: Loss = -11926.421875
Iteration 21600: Loss = -11926.4228515625
1
Iteration 21700: Loss = -11926.4228515625
2
Iteration 21800: Loss = -11926.423828125
3
Iteration 21900: Loss = -11926.421875
Iteration 22000: Loss = -11926.421875
Iteration 22100: Loss = -11926.4228515625
1
Iteration 22200: Loss = -11926.421875
Iteration 22300: Loss = -11926.4228515625
1
Iteration 22400: Loss = -11926.421875
Iteration 22500: Loss = -11926.4208984375
Iteration 22600: Loss = -11926.421875
1
Iteration 22700: Loss = -11926.421875
2
Iteration 22800: Loss = -11926.4208984375
Iteration 22900: Loss = -11926.421875
1
Iteration 23000: Loss = -11926.4208984375
Iteration 23100: Loss = -11926.421875
1
Iteration 23200: Loss = -11926.421875
2
Iteration 23300: Loss = -11926.421875
3
Iteration 23400: Loss = -11926.4208984375
Iteration 23500: Loss = -11926.4208984375
Iteration 23600: Loss = -11926.419921875
Iteration 23700: Loss = -11926.421875
1
Iteration 23800: Loss = -11926.4208984375
2
Iteration 23900: Loss = -11926.4208984375
3
Iteration 24000: Loss = -11926.4208984375
4
Iteration 24100: Loss = -11926.419921875
Iteration 24200: Loss = -11926.41796875
Iteration 24300: Loss = -11926.4189453125
1
Iteration 24400: Loss = -11926.4189453125
2
Iteration 24500: Loss = -11926.41796875
Iteration 24600: Loss = -11926.361328125
Iteration 24700: Loss = -11925.8173828125
Iteration 24800: Loss = -11925.818359375
1
Iteration 24900: Loss = -11925.8173828125
Iteration 25000: Loss = -11925.818359375
1
Iteration 25100: Loss = -11925.8173828125
Iteration 25200: Loss = -11925.8173828125
Iteration 25300: Loss = -11925.8173828125
Iteration 25400: Loss = -11925.818359375
1
Iteration 25500: Loss = -11925.81640625
Iteration 25600: Loss = -11925.8173828125
1
Iteration 25700: Loss = -11925.81640625
Iteration 25800: Loss = -11925.8173828125
1
Iteration 25900: Loss = -11925.818359375
2
Iteration 26000: Loss = -11925.8173828125
3
Iteration 26100: Loss = -11925.818359375
4
Iteration 26200: Loss = -11925.818359375
5
Iteration 26300: Loss = -11925.8173828125
6
Iteration 26400: Loss = -11925.81640625
Iteration 26500: Loss = -11925.8173828125
1
Iteration 26600: Loss = -11925.8173828125
2
Iteration 26700: Loss = -11925.81640625
Iteration 26800: Loss = -11925.81640625
Iteration 26900: Loss = -11925.818359375
1
Iteration 27000: Loss = -11925.818359375
2
Iteration 27100: Loss = -11925.8173828125
3
Iteration 27200: Loss = -11925.81640625
Iteration 27300: Loss = -11925.81640625
Iteration 27400: Loss = -11925.8173828125
1
Iteration 27500: Loss = -11925.81640625
Iteration 27600: Loss = -11925.8173828125
1
Iteration 27700: Loss = -11925.8173828125
2
Iteration 27800: Loss = -11925.8173828125
3
Iteration 27900: Loss = -11925.8125
Iteration 28000: Loss = -11925.8125
Iteration 28100: Loss = -11925.8134765625
1
Iteration 28200: Loss = -11925.8134765625
2
Iteration 28300: Loss = -11925.8125
Iteration 28400: Loss = -11925.8134765625
1
Iteration 28500: Loss = -11925.8134765625
2
Iteration 28600: Loss = -11925.8134765625
3
Iteration 28700: Loss = -11925.8115234375
Iteration 28800: Loss = -11925.8115234375
Iteration 28900: Loss = -11925.8134765625
1
Iteration 29000: Loss = -11925.8125
2
Iteration 29100: Loss = -11925.8125
3
Iteration 29200: Loss = -11925.81640625
4
Iteration 29300: Loss = -11925.8125
5
Iteration 29400: Loss = -11925.8134765625
6
Iteration 29500: Loss = -11925.8046875
Iteration 29600: Loss = -11925.8037109375
Iteration 29700: Loss = -11925.8037109375
Iteration 29800: Loss = -11925.8056640625
1
Iteration 29900: Loss = -11925.8037109375
pi: tensor([[0.7877, 0.2123],
        [0.2292, 0.7708]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4656, 0.5344], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2994, 0.1062],
         [0.0120, 0.3012]],

        [[0.1470, 0.0933],
         [0.8003, 0.9835]],

        [[0.0785, 0.1014],
         [0.0077, 0.9928]],

        [[0.2837, 0.1111],
         [0.4014, 0.8441]],

        [[0.0224, 0.1043],
         [0.8555, 0.9838]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.9840320230862317
Average Adjusted Rand Index: 0.9839991536604262
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -40736.04296875
Iteration 100: Loss = -25672.62890625
Iteration 200: Loss = -15502.013671875
Iteration 300: Loss = -13484.47265625
Iteration 400: Loss = -13067.8994140625
Iteration 500: Loss = -12892.8359375
Iteration 600: Loss = -12781.904296875
Iteration 700: Loss = -12713.1416015625
Iteration 800: Loss = -12664.66796875
Iteration 900: Loss = -12607.4765625
Iteration 1000: Loss = -12559.931640625
Iteration 1100: Loss = -12534.3544921875
Iteration 1200: Loss = -12520.0712890625
Iteration 1300: Loss = -12507.59765625
Iteration 1400: Loss = -12495.6474609375
Iteration 1500: Loss = -12488.37890625
Iteration 1600: Loss = -12482.3564453125
Iteration 1700: Loss = -12476.4931640625
Iteration 1800: Loss = -12471.853515625
Iteration 1900: Loss = -12466.083984375
Iteration 2000: Loss = -12462.6015625
Iteration 2100: Loss = -12459.73046875
Iteration 2200: Loss = -12457.1669921875
Iteration 2300: Loss = -12454.2236328125
Iteration 2400: Loss = -12451.5986328125
Iteration 2500: Loss = -12448.169921875
Iteration 2600: Loss = -12445.666015625
Iteration 2700: Loss = -12444.24609375
Iteration 2800: Loss = -12443.1923828125
Iteration 2900: Loss = -12442.330078125
Iteration 3000: Loss = -12441.595703125
Iteration 3100: Loss = -12440.955078125
Iteration 3200: Loss = -12440.3828125
Iteration 3300: Loss = -12439.8642578125
Iteration 3400: Loss = -12439.3876953125
Iteration 3500: Loss = -12438.9423828125
Iteration 3600: Loss = -12437.8486328125
Iteration 3700: Loss = -12433.2900390625
Iteration 3800: Loss = -12432.6611328125
Iteration 3900: Loss = -12432.158203125
Iteration 4000: Loss = -12431.7099609375
Iteration 4100: Loss = -12431.302734375
Iteration 4200: Loss = -12430.935546875
Iteration 4300: Loss = -12430.611328125
Iteration 4400: Loss = -12430.322265625
Iteration 4500: Loss = -12430.0595703125
Iteration 4600: Loss = -12429.814453125
Iteration 4700: Loss = -12429.5849609375
Iteration 4800: Loss = -12429.369140625
Iteration 4900: Loss = -12429.171875
Iteration 5000: Loss = -12428.9853515625
Iteration 5100: Loss = -12428.8115234375
Iteration 5200: Loss = -12428.64453125
Iteration 5300: Loss = -12428.4853515625
Iteration 5400: Loss = -12428.3310546875
Iteration 5500: Loss = -12428.181640625
Iteration 5600: Loss = -12428.0341796875
Iteration 5700: Loss = -12427.8896484375
Iteration 5800: Loss = -12427.7451171875
Iteration 5900: Loss = -12427.6044921875
Iteration 6000: Loss = -12427.4638671875
Iteration 6100: Loss = -12427.3310546875
Iteration 6200: Loss = -12427.201171875
Iteration 6300: Loss = -12427.0703125
Iteration 6400: Loss = -12426.94140625
Iteration 6500: Loss = -12426.8095703125
Iteration 6600: Loss = -12426.673828125
Iteration 6700: Loss = -12426.529296875
Iteration 6800: Loss = -12426.376953125
Iteration 6900: Loss = -12426.2216796875
Iteration 7000: Loss = -12426.064453125
Iteration 7100: Loss = -12425.9052734375
Iteration 7200: Loss = -12425.7529296875
Iteration 7300: Loss = -12425.6025390625
Iteration 7400: Loss = -12425.4580078125
Iteration 7500: Loss = -12425.3173828125
Iteration 7600: Loss = -12425.1865234375
Iteration 7700: Loss = -12425.0634765625
Iteration 7800: Loss = -12424.94140625
Iteration 7900: Loss = -12424.8193359375
Iteration 8000: Loss = -12424.703125
Iteration 8100: Loss = -12424.5908203125
Iteration 8200: Loss = -12424.4873046875
Iteration 8300: Loss = -12424.3916015625
Iteration 8400: Loss = -12424.30078125
Iteration 8500: Loss = -12424.21484375
Iteration 8600: Loss = -12424.1357421875
Iteration 8700: Loss = -12424.064453125
Iteration 8800: Loss = -12423.9912109375
Iteration 8900: Loss = -12423.9150390625
Iteration 9000: Loss = -12423.8388671875
Iteration 9100: Loss = -12423.765625
Iteration 9200: Loss = -12423.703125
Iteration 9300: Loss = -12423.646484375
Iteration 9400: Loss = -12423.58984375
Iteration 9500: Loss = -12423.53125
Iteration 9600: Loss = -12423.4814453125
Iteration 9700: Loss = -12423.4423828125
Iteration 9800: Loss = -12423.41015625
Iteration 9900: Loss = -12423.384765625
Iteration 10000: Loss = -12423.36328125
Iteration 10100: Loss = -12423.3447265625
Iteration 10200: Loss = -12423.328125
Iteration 10300: Loss = -12423.314453125
Iteration 10400: Loss = -12423.30078125
Iteration 10500: Loss = -12423.29296875
Iteration 10600: Loss = -12423.2802734375
Iteration 10700: Loss = -12423.2724609375
Iteration 10800: Loss = -12423.26171875
Iteration 10900: Loss = -12423.2548828125
Iteration 11000: Loss = -12423.248046875
Iteration 11100: Loss = -12423.2412109375
Iteration 11200: Loss = -12423.234375
Iteration 11300: Loss = -12423.2294921875
Iteration 11400: Loss = -12423.2216796875
Iteration 11500: Loss = -12423.2177734375
Iteration 11600: Loss = -12423.2138671875
Iteration 11700: Loss = -12423.2109375
Iteration 11800: Loss = -12423.2041015625
Iteration 11900: Loss = -12423.2001953125
Iteration 12000: Loss = -12423.197265625
Iteration 12100: Loss = -12423.193359375
Iteration 12200: Loss = -12423.189453125
Iteration 12300: Loss = -12423.1875
Iteration 12400: Loss = -12423.18359375
Iteration 12500: Loss = -12423.181640625
Iteration 12600: Loss = -12423.1767578125
Iteration 12700: Loss = -12423.173828125
Iteration 12800: Loss = -12423.1728515625
Iteration 12900: Loss = -12423.1728515625
Iteration 13000: Loss = -12423.16796875
Iteration 13100: Loss = -12423.1650390625
Iteration 13200: Loss = -12423.1630859375
Iteration 13300: Loss = -12423.162109375
Iteration 13400: Loss = -12423.1611328125
Iteration 13500: Loss = -12423.1591796875
Iteration 13600: Loss = -12423.1572265625
Iteration 13700: Loss = -12423.1552734375
Iteration 13800: Loss = -12423.1533203125
Iteration 13900: Loss = -12423.15234375
Iteration 14000: Loss = -12423.1513671875
Iteration 14100: Loss = -12423.1513671875
Iteration 14200: Loss = -12423.150390625
Iteration 14300: Loss = -12423.1494140625
Iteration 14400: Loss = -12423.1484375
Iteration 14500: Loss = -12423.1455078125
Iteration 14600: Loss = -12423.14453125
Iteration 14700: Loss = -12423.14453125
Iteration 14800: Loss = -12423.1435546875
Iteration 14900: Loss = -12423.14453125
1
Iteration 15000: Loss = -12423.1435546875
Iteration 15100: Loss = -12423.1416015625
Iteration 15200: Loss = -12423.1396484375
Iteration 15300: Loss = -12423.1396484375
Iteration 15400: Loss = -12423.1396484375
Iteration 15500: Loss = -12423.1396484375
Iteration 15600: Loss = -12423.138671875
Iteration 15700: Loss = -12423.13671875
Iteration 15800: Loss = -12423.13671875
Iteration 15900: Loss = -12423.13671875
Iteration 16000: Loss = -12423.1357421875
Iteration 16100: Loss = -12423.1357421875
Iteration 16200: Loss = -12423.1337890625
Iteration 16300: Loss = -12423.134765625
1
Iteration 16400: Loss = -12423.134765625
2
Iteration 16500: Loss = -12423.1328125
Iteration 16600: Loss = -12423.1337890625
1
Iteration 16700: Loss = -12423.1328125
Iteration 16800: Loss = -12423.1337890625
1
Iteration 16900: Loss = -12423.1337890625
2
Iteration 17000: Loss = -12423.1328125
Iteration 17100: Loss = -12423.1318359375
Iteration 17200: Loss = -12423.1318359375
Iteration 17300: Loss = -12423.1318359375
Iteration 17400: Loss = -12423.130859375
Iteration 17500: Loss = -12423.1328125
1
Iteration 17600: Loss = -12423.1318359375
2
Iteration 17700: Loss = -12423.130859375
Iteration 17800: Loss = -12423.1298828125
Iteration 17900: Loss = -12423.130859375
1
Iteration 18000: Loss = -12423.1298828125
Iteration 18100: Loss = -12423.1298828125
Iteration 18200: Loss = -12423.1298828125
Iteration 18300: Loss = -12423.130859375
1
Iteration 18400: Loss = -12423.1298828125
Iteration 18500: Loss = -12423.12890625
Iteration 18600: Loss = -12423.12890625
Iteration 18700: Loss = -12423.1279296875
Iteration 18800: Loss = -12423.12890625
1
Iteration 18900: Loss = -12423.1298828125
2
Iteration 19000: Loss = -12423.12890625
3
Iteration 19100: Loss = -12423.1298828125
4
Iteration 19200: Loss = -12423.1298828125
5
Iteration 19300: Loss = -12423.1279296875
Iteration 19400: Loss = -12423.1279296875
Iteration 19500: Loss = -12423.1298828125
1
Iteration 19600: Loss = -12423.1298828125
2
Iteration 19700: Loss = -12423.1279296875
Iteration 19800: Loss = -12423.12890625
1
Iteration 19900: Loss = -12423.12890625
2
Iteration 20000: Loss = -12423.1298828125
3
Iteration 20100: Loss = -12423.12890625
4
Iteration 20200: Loss = -12423.12890625
5
Iteration 20300: Loss = -12423.1279296875
Iteration 20400: Loss = -12423.1279296875
Iteration 20500: Loss = -12423.12890625
1
Iteration 20600: Loss = -12423.1279296875
Iteration 20700: Loss = -12423.12890625
1
Iteration 20800: Loss = -12423.1279296875
Iteration 20900: Loss = -12423.12890625
1
Iteration 21000: Loss = -12423.126953125
Iteration 21100: Loss = -12423.12890625
1
Iteration 21200: Loss = -12423.12890625
2
Iteration 21300: Loss = -12423.12890625
3
Iteration 21400: Loss = -12423.12890625
4
Iteration 21500: Loss = -12423.12890625
5
Iteration 21600: Loss = -12423.1298828125
6
Iteration 21700: Loss = -12423.1279296875
7
Iteration 21800: Loss = -12423.126953125
Iteration 21900: Loss = -12423.12890625
1
Iteration 22000: Loss = -12423.1279296875
2
Iteration 22100: Loss = -12423.1298828125
3
Iteration 22200: Loss = -12423.1298828125
4
Iteration 22300: Loss = -12423.12890625
5
Iteration 22400: Loss = -12423.12890625
6
Iteration 22500: Loss = -12423.130859375
7
Iteration 22600: Loss = -12423.12890625
8
Iteration 22700: Loss = -12423.1318359375
9
Iteration 22800: Loss = -12423.12890625
10
Iteration 22900: Loss = -12423.1279296875
11
Iteration 23000: Loss = -12423.12890625
12
Iteration 23100: Loss = -12423.126953125
Iteration 23200: Loss = -12423.12890625
1
Iteration 23300: Loss = -12423.130859375
2
Iteration 23400: Loss = -12423.1298828125
3
Iteration 23500: Loss = -12423.1279296875
4
Iteration 23600: Loss = -12423.12890625
5
Iteration 23700: Loss = -12423.1279296875
6
Iteration 23800: Loss = -12423.1279296875
7
Iteration 23900: Loss = -12423.1298828125
8
Iteration 24000: Loss = -12423.12890625
9
Iteration 24100: Loss = -12423.1298828125
10
Iteration 24200: Loss = -12423.130859375
11
Iteration 24300: Loss = -12423.1279296875
12
Iteration 24400: Loss = -12423.1279296875
13
Iteration 24500: Loss = -12423.1279296875
14
Iteration 24600: Loss = -12423.1279296875
15
Stopping early at iteration 24600 due to no improvement.
pi: tensor([[1.0000e+00, 4.6167e-06],
        [1.0000e+00, 1.7043e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6304, 0.3696], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1974, 0.2228],
         [0.0969, 0.2520]],

        [[0.9505, 0.2809],
         [0.0236, 0.3949]],

        [[0.9449, 0.2077],
         [0.9662, 0.8783]],

        [[0.9900, 0.3263],
         [0.8735, 0.1025]],

        [[0.9796, 0.2091],
         [0.9660, 0.7050]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.003845246416696341
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0006069285451943177
Average Adjusted Rand Index: -0.0007690492833392682
[0.9840320230862317, -0.0006069285451943177] [0.9839991536604262, -0.0007690492833392682] [11925.8046875, 12423.1279296875]
-------------------------------------
This iteration is 81
True Objective function: Loss = -12069.830848201182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20631.7109375
Iteration 100: Loss = -15146.439453125
Iteration 200: Loss = -13046.9208984375
Iteration 300: Loss = -12713.6416015625
Iteration 400: Loss = -12634.3095703125
Iteration 500: Loss = -12605.7412109375
Iteration 600: Loss = -12596.509765625
Iteration 700: Loss = -12591.228515625
Iteration 800: Loss = -12587.7490234375
Iteration 900: Loss = -12585.208984375
Iteration 1000: Loss = -12583.318359375
Iteration 1100: Loss = -12581.8642578125
Iteration 1200: Loss = -12580.7041015625
Iteration 1300: Loss = -12579.74609375
Iteration 1400: Loss = -12578.931640625
Iteration 1500: Loss = -12578.2392578125
Iteration 1600: Loss = -12577.65625
Iteration 1700: Loss = -12577.1669921875
Iteration 1800: Loss = -12576.7509765625
Iteration 1900: Loss = -12576.3828125
Iteration 2000: Loss = -12576.0498046875
Iteration 2100: Loss = -12575.7431640625
Iteration 2200: Loss = -12575.4541015625
Iteration 2300: Loss = -12575.1826171875
Iteration 2400: Loss = -12574.9208984375
Iteration 2500: Loss = -12574.669921875
Iteration 2600: Loss = -12574.4287109375
Iteration 2700: Loss = -12574.1962890625
Iteration 2800: Loss = -12573.9765625
Iteration 2900: Loss = -12573.7666015625
Iteration 3000: Loss = -12573.5673828125
Iteration 3100: Loss = -12573.3720703125
Iteration 3200: Loss = -12573.18359375
Iteration 3300: Loss = -12572.9970703125
Iteration 3400: Loss = -12572.8037109375
Iteration 3500: Loss = -12572.6015625
Iteration 3600: Loss = -12572.3798828125
Iteration 3700: Loss = -12572.130859375
Iteration 3800: Loss = -12571.853515625
Iteration 3900: Loss = -12571.56640625
Iteration 4000: Loss = -12571.2900390625
Iteration 4100: Loss = -12571.037109375
Iteration 4200: Loss = -12570.8076171875
Iteration 4300: Loss = -12570.595703125
Iteration 4400: Loss = -12570.3974609375
Iteration 4500: Loss = -12570.2060546875
Iteration 4600: Loss = -12570.0224609375
Iteration 4700: Loss = -12569.8486328125
Iteration 4800: Loss = -12569.68359375
Iteration 4900: Loss = -12569.5283203125
Iteration 5000: Loss = -12569.3828125
Iteration 5100: Loss = -12569.2529296875
Iteration 5200: Loss = -12569.140625
Iteration 5300: Loss = -12569.0361328125
Iteration 5400: Loss = -12568.9423828125
Iteration 5500: Loss = -12568.8564453125
Iteration 5600: Loss = -12568.77734375
Iteration 5700: Loss = -12568.703125
Iteration 5800: Loss = -12568.630859375
Iteration 5900: Loss = -12568.564453125
Iteration 6000: Loss = -12568.4970703125
Iteration 6100: Loss = -12568.4365234375
Iteration 6200: Loss = -12568.376953125
Iteration 6300: Loss = -12568.322265625
Iteration 6400: Loss = -12568.26953125
Iteration 6500: Loss = -12568.220703125
Iteration 6600: Loss = -12568.169921875
Iteration 6700: Loss = -12568.1240234375
Iteration 6800: Loss = -12568.0791015625
Iteration 6900: Loss = -12568.0341796875
Iteration 7000: Loss = -12567.9931640625
Iteration 7100: Loss = -12567.951171875
Iteration 7200: Loss = -12567.9140625
Iteration 7300: Loss = -12567.8759765625
Iteration 7400: Loss = -12567.8388671875
Iteration 7500: Loss = -12567.8056640625
Iteration 7600: Loss = -12567.76953125
Iteration 7700: Loss = -12567.734375
Iteration 7800: Loss = -12567.7021484375
Iteration 7900: Loss = -12567.671875
Iteration 8000: Loss = -12567.642578125
Iteration 8100: Loss = -12567.61328125
Iteration 8200: Loss = -12567.587890625
Iteration 8300: Loss = -12567.55859375
Iteration 8400: Loss = -12567.53515625
Iteration 8500: Loss = -12567.5087890625
Iteration 8600: Loss = -12567.484375
Iteration 8700: Loss = -12567.4619140625
Iteration 8800: Loss = -12567.44140625
Iteration 8900: Loss = -12567.41796875
Iteration 9000: Loss = -12567.3994140625
Iteration 9100: Loss = -12567.380859375
Iteration 9200: Loss = -12567.3623046875
Iteration 9300: Loss = -12567.34375
Iteration 9400: Loss = -12567.330078125
Iteration 9500: Loss = -12567.3134765625
Iteration 9600: Loss = -12567.2998046875
Iteration 9700: Loss = -12567.2841796875
Iteration 9800: Loss = -12567.2705078125
Iteration 9900: Loss = -12567.2568359375
Iteration 10000: Loss = -12567.2451171875
Iteration 10100: Loss = -12567.234375
Iteration 10200: Loss = -12567.2236328125
Iteration 10300: Loss = -12567.212890625
Iteration 10400: Loss = -12567.2041015625
Iteration 10500: Loss = -12567.1943359375
Iteration 10600: Loss = -12567.1845703125
Iteration 10700: Loss = -12567.177734375
Iteration 10800: Loss = -12567.169921875
Iteration 10900: Loss = -12567.1611328125
Iteration 11000: Loss = -12567.154296875
Iteration 11100: Loss = -12567.1474609375
Iteration 11200: Loss = -12567.142578125
Iteration 11300: Loss = -12567.1337890625
Iteration 11400: Loss = -12567.12890625
Iteration 11500: Loss = -12567.123046875
Iteration 11600: Loss = -12567.1181640625
Iteration 11700: Loss = -12567.11328125
Iteration 11800: Loss = -12567.1103515625
Iteration 11900: Loss = -12567.1044921875
Iteration 12000: Loss = -12567.1025390625
Iteration 12100: Loss = -12567.0966796875
Iteration 12200: Loss = -12567.09375
Iteration 12300: Loss = -12567.08984375
Iteration 12400: Loss = -12567.0859375
Iteration 12500: Loss = -12567.0830078125
Iteration 12600: Loss = -12567.0810546875
Iteration 12700: Loss = -12567.0771484375
Iteration 12800: Loss = -12567.0751953125
Iteration 12900: Loss = -12567.0703125
Iteration 13000: Loss = -12567.0703125
Iteration 13100: Loss = -12567.06640625
Iteration 13200: Loss = -12567.064453125
Iteration 13300: Loss = -12567.0625
Iteration 13400: Loss = -12567.0615234375
Iteration 13500: Loss = -12567.0595703125
Iteration 13600: Loss = -12567.0595703125
Iteration 13700: Loss = -12567.056640625
Iteration 13800: Loss = -12567.0537109375
Iteration 13900: Loss = -12567.052734375
Iteration 14000: Loss = -12567.052734375
Iteration 14100: Loss = -12567.0498046875
Iteration 14200: Loss = -12567.048828125
Iteration 14300: Loss = -12567.048828125
Iteration 14400: Loss = -12567.0458984375
Iteration 14500: Loss = -12567.0458984375
Iteration 14600: Loss = -12567.0439453125
Iteration 14700: Loss = -12567.044921875
1
Iteration 14800: Loss = -12567.04296875
Iteration 14900: Loss = -12567.041015625
Iteration 15000: Loss = -12567.0400390625
Iteration 15100: Loss = -12567.0400390625
Iteration 15200: Loss = -12567.0400390625
Iteration 15300: Loss = -12567.037109375
Iteration 15400: Loss = -12567.0400390625
1
Iteration 15500: Loss = -12567.037109375
Iteration 15600: Loss = -12567.037109375
Iteration 15700: Loss = -12567.0380859375
1
Iteration 15800: Loss = -12567.03515625
Iteration 15900: Loss = -12567.03515625
Iteration 16000: Loss = -12567.03515625
Iteration 16100: Loss = -12567.033203125
Iteration 16200: Loss = -12567.033203125
Iteration 16300: Loss = -12567.0322265625
Iteration 16400: Loss = -12567.033203125
1
Iteration 16500: Loss = -12567.0322265625
Iteration 16600: Loss = -12567.0322265625
Iteration 16700: Loss = -12567.03125
Iteration 16800: Loss = -12567.03125
Iteration 16900: Loss = -12567.03125
Iteration 17000: Loss = -12567.03125
Iteration 17100: Loss = -12567.0302734375
Iteration 17200: Loss = -12567.029296875
Iteration 17300: Loss = -12567.0302734375
1
Iteration 17400: Loss = -12567.029296875
Iteration 17500: Loss = -12567.0283203125
Iteration 17600: Loss = -12567.029296875
1
Iteration 17700: Loss = -12567.02734375
Iteration 17800: Loss = -12567.02734375
Iteration 17900: Loss = -12567.0283203125
1
Iteration 18000: Loss = -12567.02734375
Iteration 18100: Loss = -12567.02734375
Iteration 18200: Loss = -12567.0263671875
Iteration 18300: Loss = -12567.0283203125
1
Iteration 18400: Loss = -12567.0263671875
Iteration 18500: Loss = -12567.0263671875
Iteration 18600: Loss = -12567.0283203125
1
Iteration 18700: Loss = -12567.0283203125
2
Iteration 18800: Loss = -12567.0283203125
3
Iteration 18900: Loss = -12567.02734375
4
Iteration 19000: Loss = -12567.02734375
5
Iteration 19100: Loss = -12567.0263671875
Iteration 19200: Loss = -12567.02734375
1
Iteration 19300: Loss = -12567.025390625
Iteration 19400: Loss = -12567.02734375
1
Iteration 19500: Loss = -12567.02734375
2
Iteration 19600: Loss = -12567.0263671875
3
Iteration 19700: Loss = -12567.02734375
4
Iteration 19800: Loss = -12567.02734375
5
Iteration 19900: Loss = -12567.02734375
6
Iteration 20000: Loss = -12567.0263671875
7
Iteration 20100: Loss = -12567.025390625
Iteration 20200: Loss = -12567.0263671875
1
Iteration 20300: Loss = -12567.0263671875
2
Iteration 20400: Loss = -12567.02734375
3
Iteration 20500: Loss = -12567.0263671875
4
Iteration 20600: Loss = -12567.0263671875
5
Iteration 20700: Loss = -12567.0263671875
6
Iteration 20800: Loss = -12567.02734375
7
Iteration 20900: Loss = -12567.0263671875
8
Iteration 21000: Loss = -12567.0263671875
9
Iteration 21100: Loss = -12567.025390625
Iteration 21200: Loss = -12567.025390625
Iteration 21300: Loss = -12567.025390625
Iteration 21400: Loss = -12567.025390625
Iteration 21500: Loss = -12567.025390625
Iteration 21600: Loss = -12567.025390625
Iteration 21700: Loss = -12567.025390625
Iteration 21800: Loss = -12567.025390625
Iteration 21900: Loss = -12567.025390625
Iteration 22000: Loss = -12567.025390625
Iteration 22100: Loss = -12567.025390625
Iteration 22200: Loss = -12567.0263671875
1
Iteration 22300: Loss = -12567.0263671875
2
Iteration 22400: Loss = -12567.025390625
Iteration 22500: Loss = -12567.0244140625
Iteration 22600: Loss = -12567.025390625
1
Iteration 22700: Loss = -12567.0263671875
2
Iteration 22800: Loss = -12567.0244140625
Iteration 22900: Loss = -12567.025390625
1
Iteration 23000: Loss = -12567.025390625
2
Iteration 23100: Loss = -12567.0263671875
3
Iteration 23200: Loss = -12567.025390625
4
Iteration 23300: Loss = -12567.0263671875
5
Iteration 23400: Loss = -12567.0263671875
6
Iteration 23500: Loss = -12567.025390625
7
Iteration 23600: Loss = -12567.0244140625
Iteration 23700: Loss = -12567.0263671875
1
Iteration 23800: Loss = -12567.025390625
2
Iteration 23900: Loss = -12567.025390625
3
Iteration 24000: Loss = -12567.02734375
4
Iteration 24100: Loss = -12567.02734375
5
Iteration 24200: Loss = -12567.025390625
6
Iteration 24300: Loss = -12567.025390625
7
Iteration 24400: Loss = -12567.025390625
8
Iteration 24500: Loss = -12567.02734375
9
Iteration 24600: Loss = -12567.025390625
10
Iteration 24700: Loss = -12567.025390625
11
Iteration 24800: Loss = -12567.025390625
12
Iteration 24900: Loss = -12567.025390625
13
Iteration 25000: Loss = -12567.0263671875
14
Iteration 25100: Loss = -12567.025390625
15
Stopping early at iteration 25100 due to no improvement.
pi: tensor([[9.9998e-01, 1.6272e-05],
        [6.8373e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0389, 0.9611], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[4.4240e-06, 2.5252e-01],
         [8.2158e-01, 2.0509e-01]],

        [[5.7862e-01, 1.6309e-01],
         [7.2169e-03, 1.6953e-01]],

        [[2.1133e-01, 3.1226e-01],
         [1.3573e-02, 9.8043e-01]],

        [[9.8923e-01, 1.6024e-01],
         [9.8815e-01, 1.6137e-02]],

        [[9.4984e-02, 1.9215e-01],
         [2.4220e-01, 9.4478e-01]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0006921612735767433
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0014922741295917668
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: -0.008987974340494725
time is 3
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.008260141115681973
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.0013070675007002147
Global Adjusted Rand Index: -0.0006386242407424537
Average Adjusted Rand Index: -0.003871059162578388
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -47942.5625
Iteration 100: Loss = -27095.765625
Iteration 200: Loss = -15830.576171875
Iteration 300: Loss = -13579.1943359375
Iteration 400: Loss = -13135.001953125
Iteration 500: Loss = -12931.6591796875
Iteration 600: Loss = -12798.8076171875
Iteration 700: Loss = -12749.177734375
Iteration 800: Loss = -12712.634765625
Iteration 900: Loss = -12673.9091796875
Iteration 1000: Loss = -12656.724609375
Iteration 1100: Loss = -12646.1484375
Iteration 1200: Loss = -12627.75
Iteration 1300: Loss = -12615.8486328125
Iteration 1400: Loss = -12610.9501953125
Iteration 1500: Loss = -12607.0087890625
Iteration 1600: Loss = -12603.736328125
Iteration 1700: Loss = -12600.97265625
Iteration 1800: Loss = -12598.6103515625
Iteration 1900: Loss = -12596.5654296875
Iteration 2000: Loss = -12594.78515625
Iteration 2100: Loss = -12593.220703125
Iteration 2200: Loss = -12591.8388671875
Iteration 2300: Loss = -12590.61328125
Iteration 2400: Loss = -12589.5166015625
Iteration 2500: Loss = -12588.533203125
Iteration 2600: Loss = -12587.6474609375
Iteration 2700: Loss = -12586.84765625
Iteration 2800: Loss = -12586.1220703125
Iteration 2900: Loss = -12585.4580078125
Iteration 3000: Loss = -12584.8544921875
Iteration 3100: Loss = -12584.3046875
Iteration 3200: Loss = -12583.7978515625
Iteration 3300: Loss = -12583.3310546875
Iteration 3400: Loss = -12582.904296875
Iteration 3500: Loss = -12582.5068359375
Iteration 3600: Loss = -12582.1416015625
Iteration 3700: Loss = -12581.802734375
Iteration 3800: Loss = -12581.48828125
Iteration 3900: Loss = -12581.1953125
Iteration 4000: Loss = -12580.9248046875
Iteration 4100: Loss = -12580.6708984375
Iteration 4200: Loss = -12580.435546875
Iteration 4300: Loss = -12580.2138671875
Iteration 4400: Loss = -12580.0087890625
Iteration 4500: Loss = -12579.8173828125
Iteration 4600: Loss = -12579.63671875
Iteration 4700: Loss = -12579.4677734375
Iteration 4800: Loss = -12579.3076171875
Iteration 4900: Loss = -12579.1611328125
Iteration 5000: Loss = -12579.0205078125
Iteration 5100: Loss = -12578.8896484375
Iteration 5200: Loss = -12578.7666015625
Iteration 5300: Loss = -12578.650390625
Iteration 5400: Loss = -12578.5390625
Iteration 5500: Loss = -12578.435546875
Iteration 5600: Loss = -12578.337890625
Iteration 5700: Loss = -12578.248046875
Iteration 5800: Loss = -12578.1591796875
Iteration 5900: Loss = -12578.078125
Iteration 6000: Loss = -12578.0
Iteration 6100: Loss = -12577.927734375
Iteration 6200: Loss = -12577.8564453125
Iteration 6300: Loss = -12577.7919921875
Iteration 6400: Loss = -12577.7294921875
Iteration 6500: Loss = -12577.669921875
Iteration 6600: Loss = -12577.6142578125
Iteration 6700: Loss = -12577.5634765625
Iteration 6800: Loss = -12577.5146484375
Iteration 6900: Loss = -12577.4658203125
Iteration 7000: Loss = -12577.421875
Iteration 7100: Loss = -12577.376953125
Iteration 7200: Loss = -12577.3369140625
Iteration 7300: Loss = -12577.2998046875
Iteration 7400: Loss = -12577.2626953125
Iteration 7500: Loss = -12577.228515625
Iteration 7600: Loss = -12577.1962890625
Iteration 7700: Loss = -12577.166015625
Iteration 7800: Loss = -12577.1357421875
Iteration 7900: Loss = -12577.107421875
Iteration 8000: Loss = -12577.0810546875
Iteration 8100: Loss = -12577.056640625
Iteration 8200: Loss = -12577.033203125
Iteration 8300: Loss = -12577.0087890625
Iteration 8400: Loss = -12576.98828125
Iteration 8500: Loss = -12576.9677734375
Iteration 8600: Loss = -12576.947265625
Iteration 8700: Loss = -12576.9296875
Iteration 8800: Loss = -12576.912109375
Iteration 8900: Loss = -12576.896484375
Iteration 9000: Loss = -12576.8818359375
Iteration 9100: Loss = -12576.8642578125
Iteration 9200: Loss = -12576.8505859375
Iteration 9300: Loss = -12576.837890625
Iteration 9400: Loss = -12576.8232421875
Iteration 9500: Loss = -12576.8115234375
Iteration 9600: Loss = -12576.7998046875
Iteration 9700: Loss = -12576.7900390625
Iteration 9800: Loss = -12576.7783203125
Iteration 9900: Loss = -12576.76953125
Iteration 10000: Loss = -12576.7578125
Iteration 10100: Loss = -12576.7509765625
Iteration 10200: Loss = -12576.740234375
Iteration 10300: Loss = -12576.732421875
Iteration 10400: Loss = -12576.7255859375
Iteration 10500: Loss = -12576.7177734375
Iteration 10600: Loss = -12576.7109375
Iteration 10700: Loss = -12576.703125
Iteration 10800: Loss = -12576.69921875
Iteration 10900: Loss = -12576.69140625
Iteration 11000: Loss = -12576.6865234375
Iteration 11100: Loss = -12576.6796875
Iteration 11200: Loss = -12576.67578125
Iteration 11300: Loss = -12576.669921875
Iteration 11400: Loss = -12576.666015625
Iteration 11500: Loss = -12576.66015625
Iteration 11600: Loss = -12576.65625
Iteration 11700: Loss = -12576.65234375
Iteration 11800: Loss = -12576.6474609375
Iteration 11900: Loss = -12576.6474609375
Iteration 12000: Loss = -12576.640625
Iteration 12100: Loss = -12576.63671875
Iteration 12200: Loss = -12576.625
Iteration 12300: Loss = -12576.6103515625
Iteration 12400: Loss = -12576.6025390625
Iteration 12500: Loss = -12576.5966796875
Iteration 12600: Loss = -12576.5927734375
Iteration 12700: Loss = -12576.591796875
Iteration 12800: Loss = -12576.5908203125
Iteration 12900: Loss = -12576.5888671875
Iteration 13000: Loss = -12576.587890625
Iteration 13100: Loss = -12576.5849609375
Iteration 13200: Loss = -12576.5830078125
Iteration 13300: Loss = -12576.576171875
Iteration 13400: Loss = -12576.5712890625
Iteration 13500: Loss = -12576.5673828125
Iteration 13600: Loss = -12576.5654296875
Iteration 13700: Loss = -12576.55859375
Iteration 13800: Loss = -12576.55078125
Iteration 13900: Loss = -12576.546875
Iteration 14000: Loss = -12576.5390625
Iteration 14100: Loss = -12576.53515625
Iteration 14200: Loss = -12576.509765625
Iteration 14300: Loss = -12573.8017578125
Iteration 14400: Loss = -12573.2333984375
Iteration 14500: Loss = -12573.1337890625
Iteration 14600: Loss = -12573.083984375
Iteration 14700: Loss = -12573.0546875
Iteration 14800: Loss = -12573.0341796875
Iteration 14900: Loss = -12573.0185546875
Iteration 15000: Loss = -12573.0078125
Iteration 15100: Loss = -12572.9951171875
Iteration 15200: Loss = -12572.9892578125
Iteration 15300: Loss = -12572.982421875
Iteration 15400: Loss = -12572.9775390625
Iteration 15500: Loss = -12572.97265625
Iteration 15600: Loss = -12572.9677734375
Iteration 15700: Loss = -12572.96484375
Iteration 15800: Loss = -12572.9609375
Iteration 15900: Loss = -12572.958984375
Iteration 16000: Loss = -12572.9541015625
Iteration 16100: Loss = -12572.9521484375
Iteration 16200: Loss = -12572.951171875
Iteration 16300: Loss = -12572.9501953125
Iteration 16400: Loss = -12572.947265625
Iteration 16500: Loss = -12572.9462890625
Iteration 16600: Loss = -12572.9443359375
Iteration 16700: Loss = -12572.9443359375
Iteration 16800: Loss = -12572.9423828125
Iteration 16900: Loss = -12572.9404296875
Iteration 17000: Loss = -12572.9404296875
Iteration 17100: Loss = -12572.9384765625
Iteration 17200: Loss = -12572.9384765625
Iteration 17300: Loss = -12572.9375
Iteration 17400: Loss = -12572.9365234375
Iteration 17500: Loss = -12572.9365234375
Iteration 17600: Loss = -12572.9365234375
Iteration 17700: Loss = -12572.935546875
Iteration 17800: Loss = -12572.93359375
Iteration 17900: Loss = -12572.93359375
Iteration 18000: Loss = -12572.9326171875
Iteration 18100: Loss = -12572.931640625
Iteration 18200: Loss = -12572.93359375
1
Iteration 18300: Loss = -12572.9306640625
Iteration 18400: Loss = -12572.931640625
1
Iteration 18500: Loss = -12572.931640625
2
Iteration 18600: Loss = -12572.9296875
Iteration 18700: Loss = -12572.9306640625
1
Iteration 18800: Loss = -12572.9296875
Iteration 18900: Loss = -12572.9296875
Iteration 19000: Loss = -12572.9296875
Iteration 19100: Loss = -12572.9306640625
1
Iteration 19200: Loss = -12572.9287109375
Iteration 19300: Loss = -12572.9287109375
Iteration 19400: Loss = -12572.927734375
Iteration 19500: Loss = -12572.927734375
Iteration 19600: Loss = -12572.9296875
1
Iteration 19700: Loss = -12572.9287109375
2
Iteration 19800: Loss = -12572.9267578125
Iteration 19900: Loss = -12572.9267578125
Iteration 20000: Loss = -12572.927734375
1
Iteration 20100: Loss = -12572.9267578125
Iteration 20200: Loss = -12572.9267578125
Iteration 20300: Loss = -12572.927734375
1
Iteration 20400: Loss = -12572.9287109375
2
Iteration 20500: Loss = -12572.9248046875
Iteration 20600: Loss = -12572.92578125
1
Iteration 20700: Loss = -12572.9267578125
2
Iteration 20800: Loss = -12572.92578125
3
Iteration 20900: Loss = -12572.9248046875
Iteration 21000: Loss = -12572.9248046875
Iteration 21100: Loss = -12572.9267578125
1
Iteration 21200: Loss = -12572.9248046875
Iteration 21300: Loss = -12572.92578125
1
Iteration 21400: Loss = -12572.9248046875
Iteration 21500: Loss = -12572.9228515625
Iteration 21600: Loss = -12572.9208984375
Iteration 21700: Loss = -12569.580078125
Iteration 21800: Loss = -12569.283203125
Iteration 21900: Loss = -12569.2412109375
Iteration 22000: Loss = -12569.2236328125
Iteration 22100: Loss = -12569.2138671875
Iteration 22200: Loss = -12569.2060546875
Iteration 22300: Loss = -12569.203125
Iteration 22400: Loss = -12569.2001953125
Iteration 22500: Loss = -12569.197265625
Iteration 22600: Loss = -12569.1953125
Iteration 22700: Loss = -12569.1923828125
Iteration 22800: Loss = -12569.19140625
Iteration 22900: Loss = -12568.76953125
Iteration 23000: Loss = -12568.7109375
Iteration 23100: Loss = -12568.708984375
Iteration 23200: Loss = -12568.703125
Iteration 23300: Loss = -12568.7001953125
Iteration 23400: Loss = -12568.6982421875
Iteration 23500: Loss = -12568.697265625
Iteration 23600: Loss = -12568.6982421875
1
Iteration 23700: Loss = -12568.697265625
Iteration 23800: Loss = -12568.6953125
Iteration 23900: Loss = -12568.6962890625
1
Iteration 24000: Loss = -12568.6943359375
Iteration 24100: Loss = -12568.6953125
1
Iteration 24200: Loss = -12568.6943359375
Iteration 24300: Loss = -12568.6953125
1
Iteration 24400: Loss = -12568.6923828125
Iteration 24500: Loss = -12568.693359375
1
Iteration 24600: Loss = -12568.6923828125
Iteration 24700: Loss = -12568.693359375
1
Iteration 24800: Loss = -12568.69140625
Iteration 24900: Loss = -12568.69140625
Iteration 25000: Loss = -12568.6923828125
1
Iteration 25100: Loss = -12568.6923828125
2
Iteration 25200: Loss = -12568.69140625
Iteration 25300: Loss = -12568.69140625
Iteration 25400: Loss = -12568.6904296875
Iteration 25500: Loss = -12568.69140625
1
Iteration 25600: Loss = -12568.6904296875
Iteration 25700: Loss = -12568.6904296875
Iteration 25800: Loss = -12567.0322265625
Iteration 25900: Loss = -12567.029296875
Iteration 26000: Loss = -12567.029296875
Iteration 26100: Loss = -12567.0283203125
Iteration 26200: Loss = -12567.0283203125
Iteration 26300: Loss = -12567.02734375
Iteration 26400: Loss = -12567.0263671875
Iteration 26500: Loss = -12567.0283203125
1
Iteration 26600: Loss = -12567.0263671875
Iteration 26700: Loss = -12567.0263671875
Iteration 26800: Loss = -12567.025390625
Iteration 26900: Loss = -12567.0263671875
1
Iteration 27000: Loss = -12567.02734375
2
Iteration 27100: Loss = -12567.02734375
3
Iteration 27200: Loss = -12567.02734375
4
Iteration 27300: Loss = -12567.025390625
Iteration 27400: Loss = -12567.0263671875
1
Iteration 27500: Loss = -12567.025390625
Iteration 27600: Loss = -12567.025390625
Iteration 27700: Loss = -12567.02734375
1
Iteration 27800: Loss = -12567.025390625
Iteration 27900: Loss = -12567.0263671875
1
Iteration 28000: Loss = -12567.025390625
Iteration 28100: Loss = -12567.025390625
Iteration 28200: Loss = -12567.0263671875
1
Iteration 28300: Loss = -12567.025390625
Iteration 28400: Loss = -12567.025390625
Iteration 28500: Loss = -12567.0263671875
1
Iteration 28600: Loss = -12567.025390625
Iteration 28700: Loss = -12567.025390625
Iteration 28800: Loss = -12567.02734375
1
Iteration 28900: Loss = -12567.025390625
Iteration 29000: Loss = -12567.0263671875
1
Iteration 29100: Loss = -12567.025390625
Iteration 29200: Loss = -12567.0263671875
1
Iteration 29300: Loss = -12567.025390625
Iteration 29400: Loss = -12567.025390625
Iteration 29500: Loss = -12567.025390625
Iteration 29600: Loss = -12567.02734375
1
Iteration 29700: Loss = -12567.025390625
Iteration 29800: Loss = -12567.025390625
Iteration 29900: Loss = -12567.025390625
pi: tensor([[9.9999e-01, 1.2652e-05],
        [4.1534e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0390, 0.9610], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[6.4448e-06, 2.5252e-01],
         [6.2739e-01, 2.0506e-01]],

        [[9.5080e-01, 1.6308e-01],
         [3.3829e-01, 9.3288e-01]],

        [[6.2996e-01, 3.1244e-01],
         [9.8300e-01, 9.7582e-01]],

        [[1.3145e-02, 1.6026e-01],
         [8.7381e-01, 4.8069e-01]],

        [[6.9754e-02, 1.9214e-01],
         [3.7392e-01, 9.5429e-01]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0006921612735767433
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0014922741295917668
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: -0.008987974340494725
time is 3
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.008260141115681973
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.0013070675007002147
Global Adjusted Rand Index: -0.0006386242407424537
Average Adjusted Rand Index: -0.003871059162578388
[-0.0006386242407424537, -0.0006386242407424537] [-0.003871059162578388, -0.003871059162578388] [12567.025390625, 12567.0263671875]
-------------------------------------
This iteration is 82
True Objective function: Loss = -11939.016107742535
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -44537.0546875
Iteration 100: Loss = -25784.23046875
Iteration 200: Loss = -15274.9365234375
Iteration 300: Loss = -12907.193359375
Iteration 400: Loss = -12607.63671875
Iteration 500: Loss = -12525.060546875
Iteration 600: Loss = -12481.5732421875
Iteration 700: Loss = -12453.88671875
Iteration 800: Loss = -12433.77734375
Iteration 900: Loss = -12422.15625
Iteration 1000: Loss = -12412.62109375
Iteration 1100: Loss = -12404.4619140625
Iteration 1200: Loss = -12399.224609375
Iteration 1300: Loss = -12395.4375
Iteration 1400: Loss = -12392.4892578125
Iteration 1500: Loss = -12390.12109375
Iteration 1600: Loss = -12388.1845703125
Iteration 1700: Loss = -12386.578125
Iteration 1800: Loss = -12385.2265625
Iteration 1900: Loss = -12384.0771484375
Iteration 2000: Loss = -12383.0947265625
Iteration 2100: Loss = -12382.240234375
Iteration 2200: Loss = -12381.49609375
Iteration 2300: Loss = -12380.841796875
Iteration 2400: Loss = -12380.2578125
Iteration 2500: Loss = -12379.7392578125
Iteration 2600: Loss = -12379.271484375
Iteration 2700: Loss = -12378.8505859375
Iteration 2800: Loss = -12378.46875
Iteration 2900: Loss = -12378.119140625
Iteration 3000: Loss = -12377.796875
Iteration 3100: Loss = -12377.49609375
Iteration 3200: Loss = -12377.20703125
Iteration 3300: Loss = -12376.939453125
Iteration 3400: Loss = -12376.7021484375
Iteration 3500: Loss = -12376.48046875
Iteration 3600: Loss = -12376.275390625
Iteration 3700: Loss = -12376.0869140625
Iteration 3800: Loss = -12375.9130859375
Iteration 3900: Loss = -12375.75390625
Iteration 4000: Loss = -12375.607421875
Iteration 4100: Loss = -12375.47265625
Iteration 4200: Loss = -12375.34765625
Iteration 4300: Loss = -12375.234375
Iteration 4400: Loss = -12375.1318359375
Iteration 4500: Loss = -12375.0390625
Iteration 4600: Loss = -12374.955078125
Iteration 4700: Loss = -12374.8818359375
Iteration 4800: Loss = -12374.81640625
Iteration 4900: Loss = -12374.75390625
Iteration 5000: Loss = -12374.6962890625
Iteration 5100: Loss = -12374.6455078125
Iteration 5200: Loss = -12374.5966796875
Iteration 5300: Loss = -12374.55078125
Iteration 5400: Loss = -12374.5078125
Iteration 5500: Loss = -12374.4677734375
Iteration 5600: Loss = -12374.4287109375
Iteration 5700: Loss = -12374.39453125
Iteration 5800: Loss = -12374.361328125
Iteration 5900: Loss = -12374.330078125
Iteration 6000: Loss = -12374.3017578125
Iteration 6100: Loss = -12374.2734375
Iteration 6200: Loss = -12374.25
Iteration 6300: Loss = -12374.224609375
Iteration 6400: Loss = -12374.2021484375
Iteration 6500: Loss = -12374.1806640625
Iteration 6600: Loss = -12374.162109375
Iteration 6700: Loss = -12374.1416015625
Iteration 6800: Loss = -12374.1240234375
Iteration 6900: Loss = -12374.107421875
Iteration 7000: Loss = -12374.0908203125
Iteration 7100: Loss = -12374.0751953125
Iteration 7200: Loss = -12374.0615234375
Iteration 7300: Loss = -12374.0478515625
Iteration 7400: Loss = -12374.0341796875
Iteration 7500: Loss = -12374.01953125
Iteration 7600: Loss = -12374.009765625
Iteration 7700: Loss = -12373.9990234375
Iteration 7800: Loss = -12373.984375
Iteration 7900: Loss = -12373.974609375
Iteration 8000: Loss = -12373.9638671875
Iteration 8100: Loss = -12373.9560546875
Iteration 8200: Loss = -12373.9453125
Iteration 8300: Loss = -12373.935546875
Iteration 8400: Loss = -12373.927734375
Iteration 8500: Loss = -12373.919921875
Iteration 8600: Loss = -12373.912109375
Iteration 8700: Loss = -12373.9033203125
Iteration 8800: Loss = -12373.8955078125
Iteration 8900: Loss = -12373.888671875
Iteration 9000: Loss = -12373.8818359375
Iteration 9100: Loss = -12373.8740234375
Iteration 9200: Loss = -12373.8662109375
Iteration 9300: Loss = -12373.8603515625
Iteration 9400: Loss = -12373.8525390625
Iteration 9500: Loss = -12373.8466796875
Iteration 9600: Loss = -12373.83984375
Iteration 9700: Loss = -12373.83203125
Iteration 9800: Loss = -12373.8232421875
Iteration 9900: Loss = -12373.818359375
Iteration 10000: Loss = -12373.80859375
Iteration 10100: Loss = -12373.8046875
Iteration 10200: Loss = -12373.7958984375
Iteration 10300: Loss = -12373.791015625
Iteration 10400: Loss = -12373.7822265625
Iteration 10500: Loss = -12373.775390625
Iteration 10600: Loss = -12373.76953125
Iteration 10700: Loss = -12373.763671875
Iteration 10800: Loss = -12373.7568359375
Iteration 10900: Loss = -12373.7529296875
Iteration 11000: Loss = -12373.74609375
Iteration 11100: Loss = -12373.7412109375
Iteration 11200: Loss = -12373.7353515625
Iteration 11300: Loss = -12373.7294921875
Iteration 11400: Loss = -12373.7216796875
Iteration 11500: Loss = -12373.7158203125
Iteration 11600: Loss = -12373.708984375
Iteration 11700: Loss = -12373.701171875
Iteration 11800: Loss = -12373.6943359375
Iteration 11900: Loss = -12373.6865234375
Iteration 12000: Loss = -12373.6787109375
Iteration 12100: Loss = -12373.669921875
Iteration 12200: Loss = -12373.6591796875
Iteration 12300: Loss = -12373.6513671875
Iteration 12400: Loss = -12373.6396484375
Iteration 12500: Loss = -12373.6279296875
Iteration 12600: Loss = -12373.611328125
Iteration 12700: Loss = -12373.5947265625
Iteration 12800: Loss = -12373.57421875
Iteration 12900: Loss = -12373.546875
Iteration 13000: Loss = -12373.5126953125
Iteration 13100: Loss = -12373.4658203125
Iteration 13200: Loss = -12373.3603515625
Iteration 13300: Loss = -12370.220703125
Iteration 13400: Loss = -12369.1708984375
Iteration 13500: Loss = -12364.7900390625
Iteration 13600: Loss = -12364.6689453125
Iteration 13700: Loss = -12364.623046875
Iteration 13800: Loss = -12364.59375
Iteration 13900: Loss = -12364.576171875
Iteration 14000: Loss = -12364.5625
Iteration 14100: Loss = -12364.5537109375
Iteration 14200: Loss = -12364.544921875
Iteration 14300: Loss = -12364.5380859375
Iteration 14400: Loss = -12364.53125
Iteration 14500: Loss = -12364.5263671875
Iteration 14600: Loss = -12364.5224609375
Iteration 14700: Loss = -12364.5185546875
Iteration 14800: Loss = -12364.517578125
Iteration 14900: Loss = -12364.515625
Iteration 15000: Loss = -12364.5126953125
Iteration 15100: Loss = -12364.51171875
Iteration 15200: Loss = -12364.5078125
Iteration 15300: Loss = -12364.5078125
Iteration 15400: Loss = -12364.5087890625
1
Iteration 15500: Loss = -12364.5048828125
Iteration 15600: Loss = -12364.50390625
Iteration 15700: Loss = -12364.501953125
Iteration 15800: Loss = -12364.501953125
Iteration 15900: Loss = -12364.5048828125
1
Iteration 16000: Loss = -12364.5009765625
Iteration 16100: Loss = -12364.5
Iteration 16200: Loss = -12364.498046875
Iteration 16300: Loss = -12364.498046875
Iteration 16400: Loss = -12364.498046875
Iteration 16500: Loss = -12364.49609375
Iteration 16600: Loss = -12364.4951171875
Iteration 16700: Loss = -12364.4951171875
Iteration 16800: Loss = -12364.494140625
Iteration 16900: Loss = -12364.494140625
Iteration 17000: Loss = -12364.4951171875
1
Iteration 17100: Loss = -12364.494140625
Iteration 17200: Loss = -12364.4921875
Iteration 17300: Loss = -12364.4921875
Iteration 17400: Loss = -12364.4921875
Iteration 17500: Loss = -12364.4921875
Iteration 17600: Loss = -12364.4912109375
Iteration 17700: Loss = -12364.4931640625
1
Iteration 17800: Loss = -12364.4912109375
Iteration 17900: Loss = -12364.4892578125
Iteration 18000: Loss = -12364.490234375
1
Iteration 18100: Loss = -12364.490234375
2
Iteration 18200: Loss = -12364.490234375
3
Iteration 18300: Loss = -12364.490234375
4
Iteration 18400: Loss = -12364.4892578125
Iteration 18500: Loss = -12364.48828125
Iteration 18600: Loss = -12364.4912109375
1
Iteration 18700: Loss = -12364.4892578125
2
Iteration 18800: Loss = -12364.4892578125
3
Iteration 18900: Loss = -12364.4892578125
4
Iteration 19000: Loss = -12364.48828125
Iteration 19100: Loss = -12364.4873046875
Iteration 19200: Loss = -12364.4892578125
1
Iteration 19300: Loss = -12364.4892578125
2
Iteration 19400: Loss = -12364.4873046875
Iteration 19500: Loss = -12364.486328125
Iteration 19600: Loss = -12364.486328125
Iteration 19700: Loss = -12364.48828125
1
Iteration 19800: Loss = -12364.486328125
Iteration 19900: Loss = -12364.486328125
Iteration 20000: Loss = -12364.4853515625
Iteration 20100: Loss = -12364.486328125
1
Iteration 20200: Loss = -12364.486328125
2
Iteration 20300: Loss = -12364.486328125
3
Iteration 20400: Loss = -12364.4853515625
Iteration 20500: Loss = -12364.4873046875
1
Iteration 20600: Loss = -12364.4873046875
2
Iteration 20700: Loss = -12364.486328125
3
Iteration 20800: Loss = -12364.4853515625
Iteration 20900: Loss = -12364.4873046875
1
Iteration 21000: Loss = -12364.4873046875
2
Iteration 21100: Loss = -12364.484375
Iteration 21200: Loss = -12364.486328125
1
Iteration 21300: Loss = -12364.4892578125
2
Iteration 21400: Loss = -12364.484375
Iteration 21500: Loss = -12364.484375
Iteration 21600: Loss = -12364.4873046875
1
Iteration 21700: Loss = -12364.486328125
2
Iteration 21800: Loss = -12364.486328125
3
Iteration 21900: Loss = -12364.484375
Iteration 22000: Loss = -12364.486328125
1
Iteration 22100: Loss = -12364.486328125
2
Iteration 22200: Loss = -12364.486328125
3
Iteration 22300: Loss = -12364.4873046875
4
Iteration 22400: Loss = -12364.484375
Iteration 22500: Loss = -12364.4853515625
1
Iteration 22600: Loss = -12364.486328125
2
Iteration 22700: Loss = -12364.486328125
3
Iteration 22800: Loss = -12364.486328125
4
Iteration 22900: Loss = -12364.486328125
5
Iteration 23000: Loss = -12364.4853515625
6
Iteration 23100: Loss = -12364.4853515625
7
Iteration 23200: Loss = -12364.486328125
8
Iteration 23300: Loss = -12364.4853515625
9
Iteration 23400: Loss = -12364.4853515625
10
Iteration 23500: Loss = -12364.486328125
11
Iteration 23600: Loss = -12364.486328125
12
Iteration 23700: Loss = -12364.4853515625
13
Iteration 23800: Loss = -12364.486328125
14
Iteration 23900: Loss = -12364.4853515625
15
Stopping early at iteration 23900 due to no improvement.
pi: tensor([[8.6395e-01, 1.3605e-01],
        [1.8477e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0518, 0.9482], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.5402, 0.1106],
         [0.9883, 0.2022]],

        [[0.0323, 0.2424],
         [0.2775, 0.9920]],

        [[0.9864, 0.1552],
         [0.0098, 0.9822]],

        [[0.7004, 0.1500],
         [0.9928, 0.8970]],

        [[0.4413, 0.1543],
         [0.9733, 0.5096]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.019103401667042125
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.008484848484848486
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0070073061126442035
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
Global Adjusted Rand Index: -0.0024012287809115265
Average Adjusted Rand Index: -2.262652910467282e-05
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28869.53515625
Iteration 100: Loss = -19662.0703125
Iteration 200: Loss = -14074.67578125
Iteration 300: Loss = -12838.4482421875
Iteration 400: Loss = -12636.30078125
Iteration 500: Loss = -12556.193359375
Iteration 600: Loss = -12502.6171875
Iteration 700: Loss = -12477.9189453125
Iteration 800: Loss = -12454.1279296875
Iteration 900: Loss = -12436.4775390625
Iteration 1000: Loss = -12422.91796875
Iteration 1100: Loss = -12408.9052734375
Iteration 1200: Loss = -12399.70703125
Iteration 1300: Loss = -12394.9990234375
Iteration 1400: Loss = -12391.7041015625
Iteration 1500: Loss = -12389.2470703125
Iteration 1600: Loss = -12387.3193359375
Iteration 1700: Loss = -12385.7705078125
Iteration 1800: Loss = -12384.484375
Iteration 1900: Loss = -12383.3984375
Iteration 2000: Loss = -12382.47265625
Iteration 2100: Loss = -12381.6708984375
Iteration 2200: Loss = -12380.9716796875
Iteration 2300: Loss = -12380.353515625
Iteration 2400: Loss = -12379.8076171875
Iteration 2500: Loss = -12379.3193359375
Iteration 2600: Loss = -12378.8837890625
Iteration 2700: Loss = -12378.4931640625
Iteration 2800: Loss = -12378.1435546875
Iteration 2900: Loss = -12377.82421875
Iteration 3000: Loss = -12377.5361328125
Iteration 3100: Loss = -12377.271484375
Iteration 3200: Loss = -12377.0283203125
Iteration 3300: Loss = -12376.8076171875
Iteration 3400: Loss = -12376.6015625
Iteration 3500: Loss = -12376.4111328125
Iteration 3600: Loss = -12376.2353515625
Iteration 3700: Loss = -12376.0703125
Iteration 3800: Loss = -12375.9140625
Iteration 3900: Loss = -12375.7685546875
Iteration 4000: Loss = -12375.62890625
Iteration 4100: Loss = -12375.49609375
Iteration 4200: Loss = -12375.369140625
Iteration 4300: Loss = -12375.2412109375
Iteration 4400: Loss = -12375.11328125
Iteration 4500: Loss = -12374.98046875
Iteration 4600: Loss = -12374.8408203125
Iteration 4700: Loss = -12374.6875
Iteration 4800: Loss = -12374.5400390625
Iteration 4900: Loss = -12374.4091796875
Iteration 5000: Loss = -12374.3046875
Iteration 5100: Loss = -12374.212890625
Iteration 5200: Loss = -12374.1318359375
Iteration 5300: Loss = -12374.0537109375
Iteration 5400: Loss = -12373.9794921875
Iteration 5500: Loss = -12373.9052734375
Iteration 5600: Loss = -12373.833984375
Iteration 5700: Loss = -12373.7646484375
Iteration 5800: Loss = -12373.693359375
Iteration 5900: Loss = -12373.6201171875
Iteration 6000: Loss = -12373.5517578125
Iteration 6100: Loss = -12373.484375
Iteration 6200: Loss = -12373.4150390625
Iteration 6300: Loss = -12373.3447265625
Iteration 6400: Loss = -12373.271484375
Iteration 6500: Loss = -12373.1962890625
Iteration 6600: Loss = -12373.119140625
Iteration 6700: Loss = -12373.0390625
Iteration 6800: Loss = -12372.9580078125
Iteration 6900: Loss = -12372.876953125
Iteration 7000: Loss = -12372.798828125
Iteration 7100: Loss = -12372.728515625
Iteration 7200: Loss = -12372.666015625
Iteration 7300: Loss = -12372.6123046875
Iteration 7400: Loss = -12372.5703125
Iteration 7500: Loss = -12372.537109375
Iteration 7600: Loss = -12372.505859375
Iteration 7700: Loss = -12372.482421875
Iteration 7800: Loss = -12372.45703125
Iteration 7900: Loss = -12372.4345703125
Iteration 8000: Loss = -12372.412109375
Iteration 8100: Loss = -12372.3857421875
Iteration 8200: Loss = -12372.36328125
Iteration 8300: Loss = -12372.3388671875
Iteration 8400: Loss = -12372.306640625
Iteration 8500: Loss = -12372.25
Iteration 8600: Loss = -12372.2001953125
Iteration 8700: Loss = -12372.1474609375
Iteration 8800: Loss = -12372.095703125
Iteration 8900: Loss = -12372.046875
Iteration 9000: Loss = -12372.0087890625
Iteration 9100: Loss = -12371.982421875
Iteration 9200: Loss = -12371.962890625
Iteration 9300: Loss = -12371.9482421875
Iteration 9400: Loss = -12371.9296875
Iteration 9500: Loss = -12371.8330078125
Iteration 9600: Loss = -12371.6923828125
Iteration 9700: Loss = -12371.6318359375
Iteration 9800: Loss = -12371.5888671875
Iteration 9900: Loss = -12371.55859375
Iteration 10000: Loss = -12371.53515625
Iteration 10100: Loss = -12371.51953125
Iteration 10200: Loss = -12371.5048828125
Iteration 10300: Loss = -12371.4951171875
Iteration 10400: Loss = -12371.4853515625
Iteration 10500: Loss = -12371.4765625
Iteration 10600: Loss = -12371.470703125
Iteration 10700: Loss = -12371.4619140625
Iteration 10800: Loss = -12371.4560546875
Iteration 10900: Loss = -12371.4482421875
Iteration 11000: Loss = -12371.4443359375
Iteration 11100: Loss = -12371.439453125
Iteration 11200: Loss = -12371.43359375
Iteration 11300: Loss = -12371.431640625
Iteration 11400: Loss = -12371.4296875
Iteration 11500: Loss = -12371.4267578125
Iteration 11600: Loss = -12371.4248046875
Iteration 11700: Loss = -12371.421875
Iteration 11800: Loss = -12371.419921875
Iteration 11900: Loss = -12371.4169921875
Iteration 12000: Loss = -12371.4150390625
Iteration 12100: Loss = -12371.4130859375
Iteration 12200: Loss = -12371.4111328125
Iteration 12300: Loss = -12371.4111328125
Iteration 12400: Loss = -12371.408203125
Iteration 12500: Loss = -12371.408203125
Iteration 12600: Loss = -12371.4052734375
Iteration 12700: Loss = -12371.4052734375
Iteration 12800: Loss = -12371.40234375
Iteration 12900: Loss = -12371.404296875
1
Iteration 13000: Loss = -12371.400390625
Iteration 13100: Loss = -12371.3994140625
Iteration 13200: Loss = -12371.3994140625
Iteration 13300: Loss = -12371.396484375
Iteration 13400: Loss = -12371.396484375
Iteration 13500: Loss = -12371.396484375
Iteration 13600: Loss = -12371.39453125
Iteration 13700: Loss = -12371.392578125
Iteration 13800: Loss = -12371.3916015625
Iteration 13900: Loss = -12371.390625
Iteration 14000: Loss = -12371.390625
Iteration 14100: Loss = -12371.3896484375
Iteration 14200: Loss = -12371.3896484375
Iteration 14300: Loss = -12371.3896484375
Iteration 14400: Loss = -12371.388671875
Iteration 14500: Loss = -12371.3876953125
Iteration 14600: Loss = -12371.388671875
1
Iteration 14700: Loss = -12371.388671875
2
Iteration 14800: Loss = -12371.3876953125
Iteration 14900: Loss = -12371.3876953125
Iteration 15000: Loss = -12371.3876953125
Iteration 15100: Loss = -12371.38671875
Iteration 15200: Loss = -12371.38671875
Iteration 15300: Loss = -12371.388671875
1
Iteration 15400: Loss = -12371.388671875
2
Iteration 15500: Loss = -12371.3857421875
Iteration 15600: Loss = -12371.384765625
Iteration 15700: Loss = -12371.3857421875
1
Iteration 15800: Loss = -12371.3857421875
2
Iteration 15900: Loss = -12371.384765625
Iteration 16000: Loss = -12371.384765625
Iteration 16100: Loss = -12371.384765625
Iteration 16200: Loss = -12371.384765625
Iteration 16300: Loss = -12371.384765625
Iteration 16400: Loss = -12371.384765625
Iteration 16500: Loss = -12371.384765625
Iteration 16600: Loss = -12371.384765625
Iteration 16700: Loss = -12371.3837890625
Iteration 16800: Loss = -12371.3857421875
1
Iteration 16900: Loss = -12371.384765625
2
Iteration 17000: Loss = -12371.384765625
3
Iteration 17100: Loss = -12371.3837890625
Iteration 17200: Loss = -12371.384765625
1
Iteration 17300: Loss = -12371.3828125
Iteration 17400: Loss = -12371.3837890625
1
Iteration 17500: Loss = -12371.3837890625
2
Iteration 17600: Loss = -12371.3828125
Iteration 17700: Loss = -12371.384765625
1
Iteration 17800: Loss = -12371.3837890625
2
Iteration 17900: Loss = -12371.3837890625
3
Iteration 18000: Loss = -12371.3837890625
4
Iteration 18100: Loss = -12371.3837890625
5
Iteration 18200: Loss = -12371.3837890625
6
Iteration 18300: Loss = -12371.3837890625
7
Iteration 18400: Loss = -12371.3837890625
8
Iteration 18500: Loss = -12371.3828125
Iteration 18600: Loss = -12371.3828125
Iteration 18700: Loss = -12371.3837890625
1
Iteration 18800: Loss = -12371.3828125
Iteration 18900: Loss = -12371.3828125
Iteration 19000: Loss = -12371.3837890625
1
Iteration 19100: Loss = -12371.3828125
Iteration 19200: Loss = -12371.3837890625
1
Iteration 19300: Loss = -12371.3818359375
Iteration 19400: Loss = -12371.3828125
1
Iteration 19500: Loss = -12371.3828125
2
Iteration 19600: Loss = -12371.3828125
3
Iteration 19700: Loss = -12371.3818359375
Iteration 19800: Loss = -12371.3828125
1
Iteration 19900: Loss = -12371.3837890625
2
Iteration 20000: Loss = -12371.3828125
3
Iteration 20100: Loss = -12371.3828125
4
Iteration 20200: Loss = -12371.3828125
5
Iteration 20300: Loss = -12371.3837890625
6
Iteration 20400: Loss = -12371.3828125
7
Iteration 20500: Loss = -12371.3828125
8
Iteration 20600: Loss = -12371.3818359375
Iteration 20700: Loss = -12371.3828125
1
Iteration 20800: Loss = -12371.3828125
2
Iteration 20900: Loss = -12371.3828125
3
Iteration 21000: Loss = -12371.3818359375
Iteration 21100: Loss = -12371.3818359375
Iteration 21200: Loss = -12371.3818359375
Iteration 21300: Loss = -12371.3828125
1
Iteration 21400: Loss = -12371.3828125
2
Iteration 21500: Loss = -12371.3828125
3
Iteration 21600: Loss = -12371.3828125
4
Iteration 21700: Loss = -12371.3828125
5
Iteration 21800: Loss = -12371.3818359375
Iteration 21900: Loss = -12371.3837890625
1
Iteration 22000: Loss = -12371.3828125
2
Iteration 22100: Loss = -12371.3837890625
3
Iteration 22200: Loss = -12371.3828125
4
Iteration 22300: Loss = -12371.3837890625
5
Iteration 22400: Loss = -12371.3828125
6
Iteration 22500: Loss = -12371.380859375
Iteration 22600: Loss = -12371.3828125
1
Iteration 22700: Loss = -12371.3818359375
2
Iteration 22800: Loss = -12371.3837890625
3
Iteration 22900: Loss = -12371.3828125
4
Iteration 23000: Loss = -12371.3828125
5
Iteration 23100: Loss = -12371.3828125
6
Iteration 23200: Loss = -12371.3828125
7
Iteration 23300: Loss = -12371.3828125
8
Iteration 23400: Loss = -12371.3837890625
9
Iteration 23500: Loss = -12371.3828125
10
Iteration 23600: Loss = -12371.3828125
11
Iteration 23700: Loss = -12371.3828125
12
Iteration 23800: Loss = -12371.3818359375
13
Iteration 23900: Loss = -12371.3828125
14
Iteration 24000: Loss = -12371.3828125
15
Stopping early at iteration 24000 due to no improvement.
pi: tensor([[9.9999e-01, 7.2743e-06],
        [9.7519e-01, 2.4809e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0032, 0.9968], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2016, 0.1882],
         [0.0135, 0.1892]],

        [[0.6239, 0.2648],
         [0.9887, 0.5092]],

        [[0.3121, 0.2414],
         [0.9869, 0.0647]],

        [[0.9925, 0.2866],
         [0.8047, 0.0209]],

        [[0.8790, 0.2887],
         [0.7452, 0.9239]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0025742449927795568
Average Adjusted Rand Index: 0.0
[-0.0024012287809115265, -0.0025742449927795568] [-2.262652910467282e-05, 0.0] [12364.4853515625, 12371.3828125]
-------------------------------------
This iteration is 83
True Objective function: Loss = -11899.367145204336
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16298.2509765625
Iteration 100: Loss = -13584.142578125
Iteration 200: Loss = -12728.5751953125
Iteration 300: Loss = -12532.611328125
Iteration 400: Loss = -12435.4775390625
Iteration 500: Loss = -12324.9541015625
Iteration 600: Loss = -12151.744140625
Iteration 700: Loss = -12034.0400390625
Iteration 800: Loss = -11985.796875
Iteration 900: Loss = -11972.8525390625
Iteration 1000: Loss = -11964.9365234375
Iteration 1100: Loss = -11947.345703125
Iteration 1200: Loss = -11942.5517578125
Iteration 1300: Loss = -11940.5634765625
Iteration 1400: Loss = -11939.515625
Iteration 1500: Loss = -11938.787109375
Iteration 1600: Loss = -11938.2421875
Iteration 1700: Loss = -11937.8173828125
Iteration 1800: Loss = -11937.4775390625
Iteration 1900: Loss = -11937.19921875
Iteration 2000: Loss = -11936.9697265625
Iteration 2100: Loss = -11936.7744140625
Iteration 2200: Loss = -11936.6083984375
Iteration 2300: Loss = -11936.466796875
Iteration 2400: Loss = -11936.34375
Iteration 2500: Loss = -11936.236328125
Iteration 2600: Loss = -11936.142578125
Iteration 2700: Loss = -11936.0576171875
Iteration 2800: Loss = -11935.984375
Iteration 2900: Loss = -11935.9169921875
Iteration 3000: Loss = -11935.857421875
Iteration 3100: Loss = -11935.8037109375
Iteration 3200: Loss = -11935.7548828125
Iteration 3300: Loss = -11935.7109375
Iteration 3400: Loss = -11935.671875
Iteration 3500: Loss = -11935.6357421875
Iteration 3600: Loss = -11935.6015625
Iteration 3700: Loss = -11935.5703125
Iteration 3800: Loss = -11935.54296875
Iteration 3900: Loss = -11935.5166015625
Iteration 4000: Loss = -11935.494140625
Iteration 4100: Loss = -11935.47265625
Iteration 4200: Loss = -11935.451171875
Iteration 4300: Loss = -11935.43359375
Iteration 4400: Loss = -11935.416015625
Iteration 4500: Loss = -11935.400390625
Iteration 4600: Loss = -11935.384765625
Iteration 4700: Loss = -11935.37109375
Iteration 4800: Loss = -11935.357421875
Iteration 4900: Loss = -11935.3447265625
Iteration 5000: Loss = -11935.333984375
Iteration 5100: Loss = -11935.3232421875
Iteration 5200: Loss = -11935.314453125
Iteration 5300: Loss = -11935.3037109375
Iteration 5400: Loss = -11935.2958984375
Iteration 5500: Loss = -11935.2880859375
Iteration 5600: Loss = -11935.28125
Iteration 5700: Loss = -11935.2734375
Iteration 5800: Loss = -11935.2666015625
Iteration 5900: Loss = -11935.2607421875
Iteration 6000: Loss = -11935.25390625
Iteration 6100: Loss = -11935.2490234375
Iteration 6200: Loss = -11935.244140625
Iteration 6300: Loss = -11935.2373046875
Iteration 6400: Loss = -11935.234375
Iteration 6500: Loss = -11935.2294921875
Iteration 6600: Loss = -11935.2265625
Iteration 6700: Loss = -11935.22265625
Iteration 6800: Loss = -11935.2177734375
Iteration 6900: Loss = -11935.2138671875
Iteration 7000: Loss = -11935.2099609375
Iteration 7100: Loss = -11935.19921875
Iteration 7200: Loss = -11935.193359375
Iteration 7300: Loss = -11935.1904296875
Iteration 7400: Loss = -11935.1865234375
Iteration 7500: Loss = -11935.18359375
Iteration 7600: Loss = -11935.1806640625
Iteration 7700: Loss = -11935.1787109375
Iteration 7800: Loss = -11935.17578125
Iteration 7900: Loss = -11935.173828125
Iteration 8000: Loss = -11935.171875
Iteration 8100: Loss = -11935.1708984375
Iteration 8200: Loss = -11935.16796875
Iteration 8300: Loss = -11935.1669921875
Iteration 8400: Loss = -11935.1650390625
Iteration 8500: Loss = -11935.1640625
Iteration 8600: Loss = -11935.1630859375
Iteration 8700: Loss = -11935.1611328125
Iteration 8800: Loss = -11935.1611328125
Iteration 8900: Loss = -11935.158203125
Iteration 9000: Loss = -11935.158203125
Iteration 9100: Loss = -11935.15625
Iteration 9200: Loss = -11935.15625
Iteration 9300: Loss = -11935.15625
Iteration 9400: Loss = -11935.154296875
Iteration 9500: Loss = -11935.1533203125
Iteration 9600: Loss = -11935.1533203125
Iteration 9700: Loss = -11935.1513671875
Iteration 9800: Loss = -11935.1513671875
Iteration 9900: Loss = -11935.150390625
Iteration 10000: Loss = -11935.1494140625
Iteration 10100: Loss = -11935.1494140625
Iteration 10200: Loss = -11935.1474609375
Iteration 10300: Loss = -11935.1494140625
1
Iteration 10400: Loss = -11935.1484375
2
Iteration 10500: Loss = -11935.1474609375
Iteration 10600: Loss = -11935.1474609375
Iteration 10700: Loss = -11935.146484375
Iteration 10800: Loss = -11935.146484375
Iteration 10900: Loss = -11935.14453125
Iteration 11000: Loss = -11935.14453125
Iteration 11100: Loss = -11935.1455078125
1
Iteration 11200: Loss = -11935.146484375
2
Iteration 11300: Loss = -11935.1435546875
Iteration 11400: Loss = -11935.1435546875
Iteration 11500: Loss = -11935.14453125
1
Iteration 11600: Loss = -11935.14453125
2
Iteration 11700: Loss = -11935.1435546875
Iteration 11800: Loss = -11935.14453125
1
Iteration 11900: Loss = -11935.142578125
Iteration 12000: Loss = -11935.142578125
Iteration 12100: Loss = -11935.1435546875
1
Iteration 12200: Loss = -11935.142578125
Iteration 12300: Loss = -11935.142578125
Iteration 12400: Loss = -11935.142578125
Iteration 12500: Loss = -11935.140625
Iteration 12600: Loss = -11935.1416015625
1
Iteration 12700: Loss = -11935.142578125
2
Iteration 12800: Loss = -11935.140625
Iteration 12900: Loss = -11935.140625
Iteration 13000: Loss = -11935.140625
Iteration 13100: Loss = -11935.140625
Iteration 13200: Loss = -11935.140625
Iteration 13300: Loss = -11935.140625
Iteration 13400: Loss = -11935.1396484375
Iteration 13500: Loss = -11935.140625
1
Iteration 13600: Loss = -11935.1396484375
Iteration 13700: Loss = -11935.140625
1
Iteration 13800: Loss = -11935.1396484375
Iteration 13900: Loss = -11935.1396484375
Iteration 14000: Loss = -11935.138671875
Iteration 14100: Loss = -11935.1396484375
1
Iteration 14200: Loss = -11935.1396484375
2
Iteration 14300: Loss = -11935.140625
3
Iteration 14400: Loss = -11935.1396484375
4
Iteration 14500: Loss = -11935.1396484375
5
Iteration 14600: Loss = -11935.1396484375
6
Iteration 14700: Loss = -11935.138671875
Iteration 14800: Loss = -11935.138671875
Iteration 14900: Loss = -11935.1376953125
Iteration 15000: Loss = -11935.1416015625
1
Iteration 15100: Loss = -11935.138671875
2
Iteration 15200: Loss = -11935.1376953125
Iteration 15300: Loss = -11935.138671875
1
Iteration 15400: Loss = -11935.138671875
2
Iteration 15500: Loss = -11935.138671875
3
Iteration 15600: Loss = -11935.1396484375
4
Iteration 15700: Loss = -11935.1396484375
5
Iteration 15800: Loss = -11935.138671875
6
Iteration 15900: Loss = -11935.138671875
7
Iteration 16000: Loss = -11935.1396484375
8
Iteration 16100: Loss = -11935.138671875
9
Iteration 16200: Loss = -11935.1396484375
10
Iteration 16300: Loss = -11935.1416015625
11
Iteration 16400: Loss = -11935.1396484375
12
Iteration 16500: Loss = -11935.138671875
13
Iteration 16600: Loss = -11935.1123046875
Iteration 16700: Loss = -11935.1123046875
Iteration 16800: Loss = -11935.111328125
Iteration 16900: Loss = -11935.11328125
1
Iteration 17000: Loss = -11935.1123046875
2
Iteration 17100: Loss = -11935.1123046875
3
Iteration 17200: Loss = -11935.1123046875
4
Iteration 17300: Loss = -11935.1123046875
5
Iteration 17400: Loss = -11935.1123046875
6
Iteration 17500: Loss = -11935.11328125
7
Iteration 17600: Loss = -11935.11328125
8
Iteration 17700: Loss = -11935.115234375
9
Iteration 17800: Loss = -11935.11328125
10
Iteration 17900: Loss = -11935.1123046875
11
Iteration 18000: Loss = -11935.111328125
Iteration 18100: Loss = -11935.111328125
Iteration 18200: Loss = -11935.111328125
Iteration 18300: Loss = -11935.111328125
Iteration 18400: Loss = -11935.1123046875
1
Iteration 18500: Loss = -11935.1123046875
2
Iteration 18600: Loss = -11935.111328125
Iteration 18700: Loss = -11935.111328125
Iteration 18800: Loss = -11935.1123046875
1
Iteration 18900: Loss = -11935.111328125
Iteration 19000: Loss = -11935.111328125
Iteration 19100: Loss = -11935.1123046875
1
Iteration 19200: Loss = -11935.1103515625
Iteration 19300: Loss = -11935.111328125
1
Iteration 19400: Loss = -11935.1123046875
2
Iteration 19500: Loss = -11935.111328125
3
Iteration 19600: Loss = -11935.1123046875
4
Iteration 19700: Loss = -11935.1123046875
5
Iteration 19800: Loss = -11935.111328125
6
Iteration 19900: Loss = -11935.111328125
7
Iteration 20000: Loss = -11935.111328125
8
Iteration 20100: Loss = -11935.111328125
9
Iteration 20200: Loss = -11935.1123046875
10
Iteration 20300: Loss = -11935.1123046875
11
Iteration 20400: Loss = -11935.1123046875
12
Iteration 20500: Loss = -11935.111328125
13
Iteration 20600: Loss = -11935.1103515625
Iteration 20700: Loss = -11935.111328125
1
Iteration 20800: Loss = -11935.1103515625
Iteration 20900: Loss = -11935.109375
Iteration 21000: Loss = -11935.109375
Iteration 21100: Loss = -11935.1103515625
1
Iteration 21200: Loss = -11935.1103515625
2
Iteration 21300: Loss = -11935.1103515625
3
Iteration 21400: Loss = -11935.109375
Iteration 21500: Loss = -11935.109375
Iteration 21600: Loss = -11935.1103515625
1
Iteration 21700: Loss = -11935.111328125
2
Iteration 21800: Loss = -11935.1083984375
Iteration 21900: Loss = -11935.109375
1
Iteration 22000: Loss = -11935.111328125
2
Iteration 22100: Loss = -11935.109375
3
Iteration 22200: Loss = -11935.109375
4
Iteration 22300: Loss = -11935.1103515625
5
Iteration 22400: Loss = -11935.109375
6
Iteration 22500: Loss = -11935.1103515625
7
Iteration 22600: Loss = -11935.111328125
8
Iteration 22700: Loss = -11935.109375
9
Iteration 22800: Loss = -11935.109375
10
Iteration 22900: Loss = -11935.109375
11
Iteration 23000: Loss = -11935.109375
12
Iteration 23100: Loss = -11935.1123046875
13
Iteration 23200: Loss = -11935.109375
14
Iteration 23300: Loss = -11935.1103515625
15
Stopping early at iteration 23300 due to no improvement.
pi: tensor([[0.6325, 0.3675],
        [0.3649, 0.6351]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5280, 0.4720], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3085, 0.1042],
         [0.0147, 0.2937]],

        [[0.6265, 0.0956],
         [0.4255, 0.9198]],

        [[0.0149, 0.1187],
         [0.9468, 0.1696]],

        [[0.6744, 0.0994],
         [0.0492, 0.1878]],

        [[0.0581, 0.0867],
         [0.5156, 0.8795]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.03808151516791775
Average Adjusted Rand Index: 0.9839991536604262
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21642.787109375
Iteration 100: Loss = -16479.369140625
Iteration 200: Loss = -13736.71875
Iteration 300: Loss = -12803.6533203125
Iteration 400: Loss = -12608.3232421875
Iteration 500: Loss = -12555.6689453125
Iteration 600: Loss = -12524.923828125
Iteration 700: Loss = -12498.7509765625
Iteration 800: Loss = -12478.708984375
Iteration 900: Loss = -12465.0166015625
Iteration 1000: Loss = -12454.123046875
Iteration 1100: Loss = -12446.298828125
Iteration 1200: Loss = -12441.1484375
Iteration 1300: Loss = -12438.0732421875
Iteration 1400: Loss = -12436.154296875
Iteration 1500: Loss = -12434.5166015625
Iteration 1600: Loss = -12432.591796875
Iteration 1700: Loss = -12430.5458984375
Iteration 1800: Loss = -12429.859375
Iteration 1900: Loss = -12429.3671875
Iteration 2000: Loss = -12428.9697265625
Iteration 2100: Loss = -12428.6298828125
Iteration 2200: Loss = -12428.330078125
Iteration 2300: Loss = -12428.056640625
Iteration 2400: Loss = -12427.7802734375
Iteration 2500: Loss = -12423.81640625
Iteration 2600: Loss = -12423.431640625
Iteration 2700: Loss = -12423.1484375
Iteration 2800: Loss = -12422.9013671875
Iteration 2900: Loss = -12422.689453125
Iteration 3000: Loss = -12422.513671875
Iteration 3100: Loss = -12422.37109375
Iteration 3200: Loss = -12422.2548828125
Iteration 3300: Loss = -12422.154296875
Iteration 3400: Loss = -12422.0615234375
Iteration 3500: Loss = -12421.98046875
Iteration 3600: Loss = -12421.904296875
Iteration 3700: Loss = -12421.8369140625
Iteration 3800: Loss = -12421.7744140625
Iteration 3900: Loss = -12421.7158203125
Iteration 4000: Loss = -12421.6630859375
Iteration 4100: Loss = -12421.6142578125
Iteration 4200: Loss = -12421.572265625
Iteration 4300: Loss = -12421.529296875
Iteration 4400: Loss = -12421.4912109375
Iteration 4500: Loss = -12421.4560546875
Iteration 4600: Loss = -12421.4228515625
Iteration 4700: Loss = -12421.3935546875
Iteration 4800: Loss = -12421.365234375
Iteration 4900: Loss = -12421.3388671875
Iteration 5000: Loss = -12421.3154296875
Iteration 5100: Loss = -12421.2900390625
Iteration 5200: Loss = -12421.267578125
Iteration 5300: Loss = -12421.248046875
Iteration 5400: Loss = -12421.2255859375
Iteration 5500: Loss = -12421.2060546875
Iteration 5600: Loss = -12421.181640625
Iteration 5700: Loss = -12421.048828125
Iteration 5800: Loss = -12421.0322265625
Iteration 5900: Loss = -12421.017578125
Iteration 6000: Loss = -12421.0048828125
Iteration 6100: Loss = -12420.9921875
Iteration 6200: Loss = -12420.9833984375
Iteration 6300: Loss = -12420.9716796875
Iteration 6400: Loss = -12420.9638671875
Iteration 6500: Loss = -12420.953125
Iteration 6600: Loss = -12420.9462890625
Iteration 6700: Loss = -12420.9365234375
Iteration 6800: Loss = -12420.921875
Iteration 6900: Loss = -12420.2939453125
Iteration 7000: Loss = -12420.28125
Iteration 7100: Loss = -12420.2705078125
Iteration 7200: Loss = -12420.2626953125
Iteration 7300: Loss = -12420.25390625
Iteration 7400: Loss = -12420.244140625
Iteration 7500: Loss = -12420.236328125
Iteration 7600: Loss = -12420.2255859375
Iteration 7700: Loss = -12420.2138671875
Iteration 7800: Loss = -12420.197265625
Iteration 7900: Loss = -12420.1826171875
Iteration 8000: Loss = -12420.1572265625
Iteration 8100: Loss = -12420.11328125
Iteration 8200: Loss = -12420.0302734375
Iteration 8300: Loss = -12419.8359375
Iteration 8400: Loss = -12419.716796875
Iteration 8500: Loss = -12419.66796875
Iteration 8600: Loss = -12419.6474609375
Iteration 8700: Loss = -12419.640625
Iteration 8800: Loss = -12419.630859375
Iteration 8900: Loss = -12419.5546875
Iteration 9000: Loss = -12419.5107421875
Iteration 9100: Loss = -12419.3369140625
Iteration 9200: Loss = -12419.310546875
Iteration 9300: Loss = -12419.2978515625
Iteration 9400: Loss = -12419.25390625
Iteration 9500: Loss = -12419.232421875
Iteration 9600: Loss = -12419.1728515625
Iteration 9700: Loss = -12419.126953125
Iteration 9800: Loss = -12419.1123046875
Iteration 9900: Loss = -12419.0810546875
Iteration 10000: Loss = -12419.0439453125
Iteration 10100: Loss = -12419.009765625
Iteration 10200: Loss = -12418.98828125
Iteration 10300: Loss = -12418.98046875
Iteration 10400: Loss = -12418.966796875
Iteration 10500: Loss = -12418.9521484375
Iteration 10600: Loss = -12418.943359375
Iteration 10700: Loss = -12418.9287109375
Iteration 10800: Loss = -12418.9091796875
Iteration 10900: Loss = -12418.681640625
Iteration 11000: Loss = -12418.6669921875
Iteration 11100: Loss = -12418.650390625
Iteration 11200: Loss = -12418.6435546875
Iteration 11300: Loss = -12418.640625
Iteration 11400: Loss = -12418.6376953125
Iteration 11500: Loss = -12418.62890625
Iteration 11600: Loss = -12418.623046875
Iteration 11700: Loss = -12418.62109375
Iteration 11800: Loss = -12418.6171875
Iteration 11900: Loss = -12418.615234375
Iteration 12000: Loss = -12418.6142578125
Iteration 12100: Loss = -12418.6103515625
Iteration 12200: Loss = -12418.6044921875
Iteration 12300: Loss = -12418.6005859375
Iteration 12400: Loss = -12418.59765625
Iteration 12500: Loss = -12418.595703125
Iteration 12600: Loss = -12418.595703125
Iteration 12700: Loss = -12418.59375
Iteration 12800: Loss = -12418.591796875
Iteration 12900: Loss = -12418.5908203125
Iteration 13000: Loss = -12418.591796875
1
Iteration 13100: Loss = -12418.5908203125
Iteration 13200: Loss = -12418.5869140625
Iteration 13300: Loss = -12418.5859375
Iteration 13400: Loss = -12418.5869140625
1
Iteration 13500: Loss = -12418.5869140625
2
Iteration 13600: Loss = -12418.5869140625
3
Iteration 13700: Loss = -12418.5859375
Iteration 13800: Loss = -12418.5849609375
Iteration 13900: Loss = -12418.5849609375
Iteration 14000: Loss = -12418.583984375
Iteration 14100: Loss = -12418.5849609375
1
Iteration 14200: Loss = -12418.583984375
Iteration 14300: Loss = -12418.583984375
Iteration 14400: Loss = -12418.5830078125
Iteration 14500: Loss = -12418.583984375
1
Iteration 14600: Loss = -12418.583984375
2
Iteration 14700: Loss = -12418.5810546875
Iteration 14800: Loss = -12418.58203125
1
Iteration 14900: Loss = -12418.58203125
2
Iteration 15000: Loss = -12418.5830078125
3
Iteration 15100: Loss = -12418.5810546875
Iteration 15200: Loss = -12418.583984375
1
Iteration 15300: Loss = -12418.5869140625
2
Iteration 15400: Loss = -12418.5810546875
Iteration 15500: Loss = -12418.5830078125
1
Iteration 15600: Loss = -12418.58203125
2
Iteration 15700: Loss = -12418.5830078125
3
Iteration 15800: Loss = -12418.5830078125
4
Iteration 15900: Loss = -12418.58203125
5
Iteration 16000: Loss = -12418.580078125
Iteration 16100: Loss = -12418.5810546875
1
Iteration 16200: Loss = -12418.5810546875
2
Iteration 16300: Loss = -12418.5810546875
3
Iteration 16400: Loss = -12418.5810546875
4
Iteration 16500: Loss = -12418.58203125
5
Iteration 16600: Loss = -12418.5810546875
6
Iteration 16700: Loss = -12418.5810546875
7
Iteration 16800: Loss = -12418.580078125
Iteration 16900: Loss = -12418.5810546875
1
Iteration 17000: Loss = -12418.580078125
Iteration 17100: Loss = -12418.5810546875
1
Iteration 17200: Loss = -12418.58203125
2
Iteration 17300: Loss = -12418.580078125
Iteration 17400: Loss = -12418.580078125
Iteration 17500: Loss = -12418.5791015625
Iteration 17600: Loss = -12418.5810546875
1
Iteration 17700: Loss = -12418.5810546875
2
Iteration 17800: Loss = -12418.58203125
3
Iteration 17900: Loss = -12418.580078125
4
Iteration 18000: Loss = -12418.5810546875
5
Iteration 18100: Loss = -12418.580078125
6
Iteration 18200: Loss = -12418.5810546875
7
Iteration 18300: Loss = -12418.5791015625
Iteration 18400: Loss = -12418.5810546875
1
Iteration 18500: Loss = -12418.5791015625
Iteration 18600: Loss = -12418.580078125
1
Iteration 18700: Loss = -12418.580078125
2
Iteration 18800: Loss = -12418.580078125
3
Iteration 18900: Loss = -12418.5791015625
Iteration 19000: Loss = -12418.580078125
1
Iteration 19100: Loss = -12418.5810546875
2
Iteration 19200: Loss = -12418.5810546875
3
Iteration 19300: Loss = -12418.580078125
4
Iteration 19400: Loss = -12418.580078125
5
Iteration 19500: Loss = -12418.58203125
6
Iteration 19600: Loss = -12418.5810546875
7
Iteration 19700: Loss = -12418.580078125
8
Iteration 19800: Loss = -12418.5810546875
9
Iteration 19900: Loss = -12418.580078125
10
Iteration 20000: Loss = -12418.5810546875
11
Iteration 20100: Loss = -12418.5791015625
Iteration 20200: Loss = -12418.580078125
1
Iteration 20300: Loss = -12418.580078125
2
Iteration 20400: Loss = -12418.580078125
3
Iteration 20500: Loss = -12418.5810546875
4
Iteration 20600: Loss = -12418.580078125
5
Iteration 20700: Loss = -12418.578125
Iteration 20800: Loss = -12418.58203125
1
Iteration 20900: Loss = -12418.580078125
2
Iteration 21000: Loss = -12418.580078125
3
Iteration 21100: Loss = -12418.58203125
4
Iteration 21200: Loss = -12418.5791015625
5
Iteration 21300: Loss = -12418.5791015625
6
Iteration 21400: Loss = -12418.578125
Iteration 21500: Loss = -12418.580078125
1
Iteration 21600: Loss = -12418.580078125
2
Iteration 21700: Loss = -12418.5810546875
3
Iteration 21800: Loss = -12418.5791015625
4
Iteration 21900: Loss = -12418.580078125
5
Iteration 22000: Loss = -12418.580078125
6
Iteration 22100: Loss = -12418.5791015625
7
Iteration 22200: Loss = -12418.5810546875
8
Iteration 22300: Loss = -12418.58203125
9
Iteration 22400: Loss = -12418.5791015625
10
Iteration 22500: Loss = -12418.580078125
11
Iteration 22600: Loss = -12418.5810546875
12
Iteration 22700: Loss = -12418.580078125
13
Iteration 22800: Loss = -12418.580078125
14
Iteration 22900: Loss = -12418.5791015625
15
Stopping early at iteration 22900 due to no improvement.
pi: tensor([[1.0000e+00, 3.4882e-06],
        [6.9503e-01, 3.0497e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0152, 0.9848], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1970, 0.2128],
         [0.9731, 0.2017]],

        [[0.1912, 0.1947],
         [0.4937, 0.6367]],

        [[0.2793, 0.2783],
         [0.9926, 0.8883]],

        [[0.0089, 0.2032],
         [0.0189, 0.0223]],

        [[0.9615, 0.3083],
         [0.7534, 0.6340]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.007431593898976406
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: -0.015161593975525573
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: -0.002559257511637263
Average Adjusted Rand Index: -0.0014881770499554268
[0.03808151516791775, -0.002559257511637263] [0.9839991536604262, -0.0014881770499554268] [11935.1103515625, 12418.5791015625]
-------------------------------------
This iteration is 84
True Objective function: Loss = -11882.749998752
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -51813.3359375
Iteration 100: Loss = -29693.171875
Iteration 200: Loss = -16839.802734375
Iteration 300: Loss = -13833.626953125
Iteration 400: Loss = -13019.525390625
Iteration 500: Loss = -12779.3681640625
Iteration 600: Loss = -12670.4501953125
Iteration 700: Loss = -12603.125
Iteration 800: Loss = -12528.435546875
Iteration 900: Loss = -12501.0048828125
Iteration 1000: Loss = -12477.255859375
Iteration 1100: Loss = -12461.73828125
Iteration 1200: Loss = -12449.2900390625
Iteration 1300: Loss = -12439.033203125
Iteration 1400: Loss = -12432.84375
Iteration 1500: Loss = -12427.9248046875
Iteration 1600: Loss = -12423.87890625
Iteration 1700: Loss = -12420.4931640625
Iteration 1800: Loss = -12417.6298828125
Iteration 1900: Loss = -12415.1845703125
Iteration 2000: Loss = -12413.08203125
Iteration 2100: Loss = -12411.263671875
Iteration 2200: Loss = -12409.6796875
Iteration 2300: Loss = -12408.2958984375
Iteration 2400: Loss = -12407.080078125
Iteration 2500: Loss = -12406.0068359375
Iteration 2600: Loss = -12405.0546875
Iteration 2700: Loss = -12404.2119140625
Iteration 2800: Loss = -12403.4580078125
Iteration 2900: Loss = -12402.78515625
Iteration 3000: Loss = -12402.181640625
Iteration 3100: Loss = -12401.6376953125
Iteration 3200: Loss = -12401.1455078125
Iteration 3300: Loss = -12400.7041015625
Iteration 3400: Loss = -12400.30078125
Iteration 3500: Loss = -12399.93359375
Iteration 3600: Loss = -12399.6044921875
Iteration 3700: Loss = -12399.3017578125
Iteration 3800: Loss = -12399.025390625
Iteration 3900: Loss = -12398.7734375
Iteration 4000: Loss = -12398.5419921875
Iteration 4100: Loss = -12398.3251953125
Iteration 4200: Loss = -12397.9482421875
Iteration 4300: Loss = -12392.212890625
Iteration 4400: Loss = -12391.9716796875
Iteration 4500: Loss = -12391.7841796875
Iteration 4600: Loss = -12391.6220703125
Iteration 4700: Loss = -12391.474609375
Iteration 4800: Loss = -12391.3388671875
Iteration 4900: Loss = -12391.21484375
Iteration 5000: Loss = -12391.0986328125
Iteration 5100: Loss = -12390.9912109375
Iteration 5200: Loss = -12390.8916015625
Iteration 5300: Loss = -12390.7978515625
Iteration 5400: Loss = -12390.7099609375
Iteration 5500: Loss = -12390.626953125
Iteration 5600: Loss = -12390.548828125
Iteration 5700: Loss = -12390.474609375
Iteration 5800: Loss = -12390.4052734375
Iteration 5900: Loss = -12390.33984375
Iteration 6000: Loss = -12390.2763671875
Iteration 6100: Loss = -12390.2197265625
Iteration 6200: Loss = -12390.1640625
Iteration 6300: Loss = -12390.111328125
Iteration 6400: Loss = -12390.0634765625
Iteration 6500: Loss = -12390.015625
Iteration 6600: Loss = -12389.96875
Iteration 6700: Loss = -12389.927734375
Iteration 6800: Loss = -12389.8896484375
Iteration 6900: Loss = -12389.8505859375
Iteration 7000: Loss = -12389.814453125
Iteration 7100: Loss = -12389.7802734375
Iteration 7200: Loss = -12389.748046875
Iteration 7300: Loss = -12389.7158203125
Iteration 7400: Loss = -12389.6865234375
Iteration 7500: Loss = -12389.658203125
Iteration 7600: Loss = -12389.6298828125
Iteration 7700: Loss = -12389.607421875
Iteration 7800: Loss = -12389.5830078125
Iteration 7900: Loss = -12389.5576171875
Iteration 8000: Loss = -12389.5361328125
Iteration 8100: Loss = -12389.5166015625
Iteration 8200: Loss = -12389.49609375
Iteration 8300: Loss = -12389.4765625
Iteration 8400: Loss = -12389.458984375
Iteration 8500: Loss = -12389.44140625
Iteration 8600: Loss = -12389.4267578125
Iteration 8700: Loss = -12389.41015625
Iteration 8800: Loss = -12389.3955078125
Iteration 8900: Loss = -12389.3798828125
Iteration 9000: Loss = -12389.3681640625
Iteration 9100: Loss = -12389.3544921875
Iteration 9200: Loss = -12389.34375
Iteration 9300: Loss = -12389.33203125
Iteration 9400: Loss = -12389.322265625
Iteration 9500: Loss = -12389.310546875
Iteration 9600: Loss = -12389.298828125
Iteration 9700: Loss = -12389.291015625
Iteration 9800: Loss = -12389.2802734375
Iteration 9900: Loss = -12389.2724609375
Iteration 10000: Loss = -12389.263671875
Iteration 10100: Loss = -12389.2568359375
Iteration 10200: Loss = -12389.2490234375
Iteration 10300: Loss = -12389.2412109375
Iteration 10400: Loss = -12389.2353515625
Iteration 10500: Loss = -12389.23046875
Iteration 10600: Loss = -12389.22265625
Iteration 10700: Loss = -12389.2158203125
Iteration 10800: Loss = -12389.2099609375
Iteration 10900: Loss = -12389.2060546875
Iteration 11000: Loss = -12389.2001953125
Iteration 11100: Loss = -12389.1943359375
Iteration 11200: Loss = -12389.1904296875
Iteration 11300: Loss = -12389.185546875
Iteration 11400: Loss = -12389.18359375
Iteration 11500: Loss = -12389.1796875
Iteration 11600: Loss = -12389.173828125
Iteration 11700: Loss = -12389.169921875
Iteration 11800: Loss = -12389.1689453125
Iteration 11900: Loss = -12389.1630859375
Iteration 12000: Loss = -12389.158203125
Iteration 12100: Loss = -12389.1572265625
Iteration 12200: Loss = -12389.154296875
Iteration 12300: Loss = -12389.1513671875
Iteration 12400: Loss = -12389.1484375
Iteration 12500: Loss = -12389.146484375
Iteration 12600: Loss = -12389.142578125
Iteration 12700: Loss = -12389.1396484375
Iteration 12800: Loss = -12389.1376953125
Iteration 12900: Loss = -12389.1357421875
Iteration 13000: Loss = -12389.1337890625
Iteration 13100: Loss = -12389.1328125
Iteration 13200: Loss = -12389.12890625
Iteration 13300: Loss = -12389.1279296875
Iteration 13400: Loss = -12389.1259765625
Iteration 13500: Loss = -12389.1240234375
Iteration 13600: Loss = -12389.1240234375
Iteration 13700: Loss = -12389.1201171875
Iteration 13800: Loss = -12389.1220703125
1
Iteration 13900: Loss = -12389.119140625
Iteration 14000: Loss = -12389.1171875
Iteration 14100: Loss = -12389.1171875
Iteration 14200: Loss = -12389.1142578125
Iteration 14300: Loss = -12389.119140625
1
Iteration 14400: Loss = -12389.1123046875
Iteration 14500: Loss = -12389.109375
Iteration 14600: Loss = -12389.1083984375
Iteration 14700: Loss = -12389.107421875
Iteration 14800: Loss = -12389.1064453125
Iteration 14900: Loss = -12389.1064453125
Iteration 15000: Loss = -12389.1064453125
Iteration 15100: Loss = -12389.103515625
Iteration 15200: Loss = -12389.103515625
Iteration 15300: Loss = -12389.1015625
Iteration 15400: Loss = -12389.1015625
Iteration 15500: Loss = -12389.1005859375
Iteration 15600: Loss = -12389.1005859375
Iteration 15700: Loss = -12389.0986328125
Iteration 15800: Loss = -12389.1005859375
1
Iteration 15900: Loss = -12389.0986328125
Iteration 16000: Loss = -12389.0986328125
Iteration 16100: Loss = -12389.0986328125
Iteration 16200: Loss = -12389.095703125
Iteration 16300: Loss = -12389.0966796875
1
Iteration 16400: Loss = -12389.0947265625
Iteration 16500: Loss = -12389.095703125
1
Iteration 16600: Loss = -12389.0947265625
Iteration 16700: Loss = -12389.0947265625
Iteration 16800: Loss = -12389.0947265625
Iteration 16900: Loss = -12389.09375
Iteration 17000: Loss = -12389.09375
Iteration 17100: Loss = -12389.0927734375
Iteration 17200: Loss = -12389.09375
1
Iteration 17300: Loss = -12389.09375
2
Iteration 17400: Loss = -12389.0908203125
Iteration 17500: Loss = -12389.0927734375
1
Iteration 17600: Loss = -12389.0908203125
Iteration 17700: Loss = -12389.0908203125
Iteration 17800: Loss = -12389.08984375
Iteration 17900: Loss = -12389.087890625
Iteration 18000: Loss = -12389.0888671875
1
Iteration 18100: Loss = -12389.0849609375
Iteration 18200: Loss = -12389.083984375
Iteration 18300: Loss = -12389.0830078125
Iteration 18400: Loss = -12389.08203125
Iteration 18500: Loss = -12389.0791015625
Iteration 18600: Loss = -12389.0751953125
Iteration 18700: Loss = -12389.068359375
Iteration 18800: Loss = -12389.0615234375
Iteration 18900: Loss = -12389.0439453125
Iteration 19000: Loss = -12389.0
Iteration 19100: Loss = -12388.7099609375
Iteration 19200: Loss = -12388.4384765625
Iteration 19300: Loss = -12388.328125
Iteration 19400: Loss = -12388.244140625
Iteration 19500: Loss = -12388.2109375
Iteration 19600: Loss = -12388.1787109375
Iteration 19700: Loss = -12388.1591796875
Iteration 19800: Loss = -12388.0966796875
Iteration 19900: Loss = -12387.9658203125
Iteration 20000: Loss = -12387.93359375
Iteration 20100: Loss = -12387.9013671875
Iteration 20200: Loss = -12387.8984375
Iteration 20300: Loss = -12387.8974609375
Iteration 20400: Loss = -12387.896484375
Iteration 20500: Loss = -12387.8955078125
Iteration 20600: Loss = -12387.89453125
Iteration 20700: Loss = -12387.8935546875
Iteration 20800: Loss = -12387.8935546875
Iteration 20900: Loss = -12387.8935546875
Iteration 21000: Loss = -12387.892578125
Iteration 21100: Loss = -12387.8955078125
1
Iteration 21200: Loss = -12387.8935546875
2
Iteration 21300: Loss = -12387.8935546875
3
Iteration 21400: Loss = -12387.8935546875
4
Iteration 21500: Loss = -12387.8916015625
Iteration 21600: Loss = -12387.892578125
1
Iteration 21700: Loss = -12387.892578125
2
Iteration 21800: Loss = -12387.890625
Iteration 21900: Loss = -12387.890625
Iteration 22000: Loss = -12387.8896484375
Iteration 22100: Loss = -12387.884765625
Iteration 22200: Loss = -12387.8818359375
Iteration 22300: Loss = -12387.880859375
Iteration 22400: Loss = -12387.8818359375
1
Iteration 22500: Loss = -12387.8818359375
2
Iteration 22600: Loss = -12387.880859375
Iteration 22700: Loss = -12387.880859375
Iteration 22800: Loss = -12387.8818359375
1
Iteration 22900: Loss = -12387.880859375
Iteration 23000: Loss = -12387.8818359375
1
Iteration 23100: Loss = -12387.8818359375
2
Iteration 23200: Loss = -12387.8798828125
Iteration 23300: Loss = -12387.8818359375
1
Iteration 23400: Loss = -12387.880859375
2
Iteration 23500: Loss = -12387.8828125
3
Iteration 23600: Loss = -12387.8798828125
Iteration 23700: Loss = -12387.880859375
1
Iteration 23800: Loss = -12387.8818359375
2
Iteration 23900: Loss = -12387.8818359375
3
Iteration 24000: Loss = -12387.8828125
4
Iteration 24100: Loss = -12387.8818359375
5
Iteration 24200: Loss = -12387.880859375
6
Iteration 24300: Loss = -12387.8798828125
Iteration 24400: Loss = -12387.8818359375
1
Iteration 24500: Loss = -12387.880859375
2
Iteration 24600: Loss = -12387.8798828125
Iteration 24700: Loss = -12387.8828125
1
Iteration 24800: Loss = -12387.880859375
2
Iteration 24900: Loss = -12387.880859375
3
Iteration 25000: Loss = -12387.880859375
4
Iteration 25100: Loss = -12387.8818359375
5
Iteration 25200: Loss = -12387.8798828125
Iteration 25300: Loss = -12387.8818359375
1
Iteration 25400: Loss = -12387.880859375
2
Iteration 25500: Loss = -12387.8798828125
Iteration 25600: Loss = -12387.880859375
1
Iteration 25700: Loss = -12387.880859375
2
Iteration 25800: Loss = -12387.8828125
3
Iteration 25900: Loss = -12387.880859375
4
Iteration 26000: Loss = -12387.8818359375
5
Iteration 26100: Loss = -12387.880859375
6
Iteration 26200: Loss = -12387.880859375
7
Iteration 26300: Loss = -12387.880859375
8
Iteration 26400: Loss = -12387.880859375
9
Iteration 26500: Loss = -12387.8818359375
10
Iteration 26600: Loss = -12387.880859375
11
Iteration 26700: Loss = -12387.880859375
12
Iteration 26800: Loss = -12387.8798828125
Iteration 26900: Loss = -12387.8818359375
1
Iteration 27000: Loss = -12387.880859375
2
Iteration 27100: Loss = -12387.8818359375
3
Iteration 27200: Loss = -12387.880859375
4
Iteration 27300: Loss = -12387.8798828125
Iteration 27400: Loss = -12387.880859375
1
Iteration 27500: Loss = -12387.880859375
2
Iteration 27600: Loss = -12387.880859375
3
Iteration 27700: Loss = -12387.8798828125
Iteration 27800: Loss = -12387.8818359375
1
Iteration 27900: Loss = -12387.880859375
2
Iteration 28000: Loss = -12387.8818359375
3
Iteration 28100: Loss = -12387.8798828125
Iteration 28200: Loss = -12387.8818359375
1
Iteration 28300: Loss = -12387.880859375
2
Iteration 28400: Loss = -12387.8818359375
3
Iteration 28500: Loss = -12387.880859375
4
Iteration 28600: Loss = -12387.880859375
5
Iteration 28700: Loss = -12387.880859375
6
Iteration 28800: Loss = -12387.880859375
7
Iteration 28900: Loss = -12387.8818359375
8
Iteration 29000: Loss = -12387.8828125
9
Iteration 29100: Loss = -12387.880859375
10
Iteration 29200: Loss = -12387.880859375
11
Iteration 29300: Loss = -12387.880859375
12
Iteration 29400: Loss = -12387.880859375
13
Iteration 29500: Loss = -12387.8818359375
14
Iteration 29600: Loss = -12387.880859375
15
Stopping early at iteration 29600 due to no improvement.
pi: tensor([[4.0497e-01, 5.9503e-01],
        [1.3223e-05, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9908, 0.0092], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1945, 0.1520],
         [0.6123, 0.2021]],

        [[0.9687, 0.2003],
         [0.6943, 0.9311]],

        [[0.9902, 0.1971],
         [0.2742, 0.9071]],

        [[0.6554, 0.2227],
         [0.3629, 0.6486]],

        [[0.9920, 0.1949],
         [0.8969, 0.4074]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 64
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0023490653704932355
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -46155.2265625
Iteration 100: Loss = -26910.208984375
Iteration 200: Loss = -15775.6279296875
Iteration 300: Loss = -13452.7294921875
Iteration 400: Loss = -12921.759765625
Iteration 500: Loss = -12747.966796875
Iteration 600: Loss = -12640.9375
Iteration 700: Loss = -12580.189453125
Iteration 800: Loss = -12531.1318359375
Iteration 900: Loss = -12493.5654296875
Iteration 1000: Loss = -12476.3154296875
Iteration 1100: Loss = -12463.228515625
Iteration 1200: Loss = -12448.744140625
Iteration 1300: Loss = -12438.30859375
Iteration 1400: Loss = -12428.728515625
Iteration 1500: Loss = -12422.7529296875
Iteration 1600: Loss = -12418.7392578125
Iteration 1700: Loss = -12415.439453125
Iteration 1800: Loss = -12412.666015625
Iteration 1900: Loss = -12410.2939453125
Iteration 2000: Loss = -12408.248046875
Iteration 2100: Loss = -12406.4677734375
Iteration 2200: Loss = -12404.9013671875
Iteration 2300: Loss = -12403.52734375
Iteration 2400: Loss = -12402.3134765625
Iteration 2500: Loss = -12401.2373046875
Iteration 2600: Loss = -12400.2734375
Iteration 2700: Loss = -12399.412109375
Iteration 2800: Loss = -12398.6376953125
Iteration 2900: Loss = -12397.9375
Iteration 3000: Loss = -12397.3056640625
Iteration 3100: Loss = -12396.73046875
Iteration 3200: Loss = -12396.2109375
Iteration 3300: Loss = -12395.734375
Iteration 3400: Loss = -12395.298828125
Iteration 3500: Loss = -12394.90234375
Iteration 3600: Loss = -12394.5322265625
Iteration 3700: Loss = -12394.1982421875
Iteration 3800: Loss = -12393.888671875
Iteration 3900: Loss = -12393.6015625
Iteration 4000: Loss = -12393.3349609375
Iteration 4100: Loss = -12393.087890625
Iteration 4200: Loss = -12392.8603515625
Iteration 4300: Loss = -12392.6455078125
Iteration 4400: Loss = -12392.4462890625
Iteration 4500: Loss = -12392.2607421875
Iteration 4600: Loss = -12392.0869140625
Iteration 4700: Loss = -12391.923828125
Iteration 4800: Loss = -12391.7705078125
Iteration 4900: Loss = -12391.6279296875
Iteration 5000: Loss = -12391.4931640625
Iteration 5100: Loss = -12391.3671875
Iteration 5200: Loss = -12391.2470703125
Iteration 5300: Loss = -12391.1357421875
Iteration 5400: Loss = -12391.0302734375
Iteration 5500: Loss = -12390.931640625
Iteration 5600: Loss = -12390.8369140625
Iteration 5700: Loss = -12390.748046875
Iteration 5800: Loss = -12390.6640625
Iteration 5900: Loss = -12390.5849609375
Iteration 6000: Loss = -12390.51171875
Iteration 6100: Loss = -12390.439453125
Iteration 6200: Loss = -12390.3720703125
Iteration 6300: Loss = -12390.30859375
Iteration 6400: Loss = -12390.248046875
Iteration 6500: Loss = -12390.1923828125
Iteration 6600: Loss = -12390.13671875
Iteration 6700: Loss = -12390.0859375
Iteration 6800: Loss = -12390.0361328125
Iteration 6900: Loss = -12389.9912109375
Iteration 7000: Loss = -12389.9462890625
Iteration 7100: Loss = -12389.90625
Iteration 7200: Loss = -12389.8671875
Iteration 7300: Loss = -12389.828125
Iteration 7400: Loss = -12389.794921875
Iteration 7500: Loss = -12389.759765625
Iteration 7600: Loss = -12389.728515625
Iteration 7700: Loss = -12389.697265625
Iteration 7800: Loss = -12389.6669921875
Iteration 7900: Loss = -12389.6396484375
Iteration 8000: Loss = -12389.61328125
Iteration 8100: Loss = -12389.5888671875
Iteration 8200: Loss = -12389.5654296875
Iteration 8300: Loss = -12389.5439453125
Iteration 8400: Loss = -12389.521484375
Iteration 8500: Loss = -12389.5009765625
Iteration 8600: Loss = -12389.482421875
Iteration 8700: Loss = -12389.4619140625
Iteration 8800: Loss = -12389.4443359375
Iteration 8900: Loss = -12389.4287109375
Iteration 9000: Loss = -12389.412109375
Iteration 9100: Loss = -12389.3984375
Iteration 9200: Loss = -12389.3828125
Iteration 9300: Loss = -12389.37109375
Iteration 9400: Loss = -12389.35546875
Iteration 9500: Loss = -12389.3427734375
Iteration 9600: Loss = -12389.3330078125
Iteration 9700: Loss = -12389.3203125
Iteration 9800: Loss = -12389.3095703125
Iteration 9900: Loss = -12389.2998046875
Iteration 10000: Loss = -12389.2890625
Iteration 10100: Loss = -12389.28125
Iteration 10200: Loss = -12389.2705078125
Iteration 10300: Loss = -12389.2646484375
Iteration 10400: Loss = -12389.25390625
Iteration 10500: Loss = -12389.2470703125
Iteration 10600: Loss = -12389.240234375
Iteration 10700: Loss = -12389.232421875
Iteration 10800: Loss = -12389.2265625
Iteration 10900: Loss = -12389.2197265625
Iteration 11000: Loss = -12389.212890625
Iteration 11100: Loss = -12389.2080078125
Iteration 11200: Loss = -12389.201171875
Iteration 11300: Loss = -12389.1962890625
Iteration 11400: Loss = -12389.19140625
Iteration 11500: Loss = -12389.1865234375
Iteration 11600: Loss = -12389.1806640625
Iteration 11700: Loss = -12389.173828125
Iteration 11800: Loss = -12389.1708984375
Iteration 11900: Loss = -12389.1650390625
Iteration 12000: Loss = -12389.1611328125
Iteration 12100: Loss = -12389.1552734375
Iteration 12200: Loss = -12389.1513671875
Iteration 12300: Loss = -12389.146484375
Iteration 12400: Loss = -12389.1435546875
Iteration 12500: Loss = -12389.1396484375
Iteration 12600: Loss = -12389.1357421875
Iteration 12700: Loss = -12389.130859375
Iteration 12800: Loss = -12389.12890625
Iteration 12900: Loss = -12389.125
Iteration 13000: Loss = -12389.1220703125
Iteration 13100: Loss = -12389.119140625
Iteration 13200: Loss = -12389.1171875
Iteration 13300: Loss = -12389.115234375
Iteration 13400: Loss = -12389.1123046875
Iteration 13500: Loss = -12389.1083984375
Iteration 13600: Loss = -12389.1083984375
Iteration 13700: Loss = -12389.10546875
Iteration 13800: Loss = -12389.103515625
Iteration 13900: Loss = -12389.1025390625
Iteration 14000: Loss = -12389.0986328125
Iteration 14100: Loss = -12389.09765625
Iteration 14200: Loss = -12389.0966796875
Iteration 14300: Loss = -12389.0947265625
Iteration 14400: Loss = -12389.091796875
Iteration 14500: Loss = -12389.0908203125
Iteration 14600: Loss = -12389.0888671875
Iteration 14700: Loss = -12389.0859375
Iteration 14800: Loss = -12389.0849609375
Iteration 14900: Loss = -12389.0830078125
Iteration 15000: Loss = -12389.08203125
Iteration 15100: Loss = -12389.080078125
Iteration 15200: Loss = -12389.0771484375
Iteration 15300: Loss = -12389.07421875
Iteration 15400: Loss = -12389.0712890625
Iteration 15500: Loss = -12389.0693359375
Iteration 15600: Loss = -12389.064453125
Iteration 15700: Loss = -12389.060546875
Iteration 15800: Loss = -12389.0546875
Iteration 15900: Loss = -12389.0478515625
Iteration 16000: Loss = -12389.037109375
Iteration 16100: Loss = -12389.0205078125
Iteration 16200: Loss = -12388.994140625
Iteration 16300: Loss = -12388.9384765625
Iteration 16400: Loss = -12388.7548828125
Iteration 16500: Loss = -12388.44921875
Iteration 16600: Loss = -12388.3984375
Iteration 16700: Loss = -12388.34765625
Iteration 16800: Loss = -12388.2900390625
Iteration 16900: Loss = -12388.2578125
Iteration 17000: Loss = -12388.2197265625
Iteration 17100: Loss = -12388.189453125
Iteration 17200: Loss = -12388.185546875
Iteration 17300: Loss = -12388.1826171875
Iteration 17400: Loss = -12388.181640625
Iteration 17500: Loss = -12388.1806640625
Iteration 17600: Loss = -12388.1796875
Iteration 17700: Loss = -12388.1806640625
1
Iteration 17800: Loss = -12388.1806640625
2
Iteration 17900: Loss = -12388.1806640625
3
Iteration 18000: Loss = -12388.1796875
Iteration 18100: Loss = -12388.1796875
Iteration 18200: Loss = -12388.177734375
Iteration 18300: Loss = -12388.1796875
1
Iteration 18400: Loss = -12388.1787109375
2
Iteration 18500: Loss = -12388.177734375
Iteration 18600: Loss = -12388.1767578125
Iteration 18700: Loss = -12388.17578125
Iteration 18800: Loss = -12388.1748046875
Iteration 18900: Loss = -12388.173828125
Iteration 19000: Loss = -12388.1748046875
1
Iteration 19100: Loss = -12388.173828125
Iteration 19200: Loss = -12388.1728515625
Iteration 19300: Loss = -12388.1728515625
Iteration 19400: Loss = -12388.173828125
1
Iteration 19500: Loss = -12388.171875
Iteration 19600: Loss = -12388.171875
Iteration 19700: Loss = -12388.1708984375
Iteration 19800: Loss = -12388.1708984375
Iteration 19900: Loss = -12388.1708984375
Iteration 20000: Loss = -12388.169921875
Iteration 20100: Loss = -12388.169921875
Iteration 20200: Loss = -12388.169921875
Iteration 20300: Loss = -12388.1689453125
Iteration 20400: Loss = -12388.16796875
Iteration 20500: Loss = -12388.16796875
Iteration 20600: Loss = -12388.169921875
1
Iteration 20700: Loss = -12388.1689453125
2
Iteration 20800: Loss = -12388.169921875
3
Iteration 20900: Loss = -12388.16796875
Iteration 21000: Loss = -12388.16796875
Iteration 21100: Loss = -12388.1669921875
Iteration 21200: Loss = -12388.1669921875
Iteration 21300: Loss = -12388.169921875
1
Iteration 21400: Loss = -12388.1689453125
2
Iteration 21500: Loss = -12388.1669921875
Iteration 21600: Loss = -12388.1669921875
Iteration 21700: Loss = -12388.16796875
1
Iteration 21800: Loss = -12388.16796875
2
Iteration 21900: Loss = -12388.1689453125
3
Iteration 22000: Loss = -12388.1689453125
4
Iteration 22100: Loss = -12388.1689453125
5
Iteration 22200: Loss = -12388.1669921875
Iteration 22300: Loss = -12388.16796875
1
Iteration 22400: Loss = -12388.1669921875
Iteration 22500: Loss = -12388.1669921875
Iteration 22600: Loss = -12388.1669921875
Iteration 22700: Loss = -12388.16796875
1
Iteration 22800: Loss = -12388.1669921875
Iteration 22900: Loss = -12388.16796875
1
Iteration 23000: Loss = -12388.16796875
2
Iteration 23100: Loss = -12388.16796875
3
Iteration 23200: Loss = -12388.16796875
4
Iteration 23300: Loss = -12388.1669921875
Iteration 23400: Loss = -12388.16796875
1
Iteration 23500: Loss = -12388.166015625
Iteration 23600: Loss = -12388.1669921875
1
Iteration 23700: Loss = -12388.1669921875
2
Iteration 23800: Loss = -12388.16796875
3
Iteration 23900: Loss = -12388.1650390625
Iteration 24000: Loss = -12388.1640625
Iteration 24100: Loss = -12388.162109375
Iteration 24200: Loss = -12388.078125
Iteration 24300: Loss = -12387.8857421875
Iteration 24400: Loss = -12387.8818359375
Iteration 24500: Loss = -12387.880859375
Iteration 24600: Loss = -12387.880859375
Iteration 24700: Loss = -12387.8818359375
1
Iteration 24800: Loss = -12387.8818359375
2
Iteration 24900: Loss = -12387.880859375
Iteration 25000: Loss = -12387.880859375
Iteration 25100: Loss = -12387.8818359375
1
Iteration 25200: Loss = -12387.8828125
2
Iteration 25300: Loss = -12387.880859375
Iteration 25400: Loss = -12387.880859375
Iteration 25500: Loss = -12387.880859375
Iteration 25600: Loss = -12387.8798828125
Iteration 25700: Loss = -12387.880859375
1
Iteration 25800: Loss = -12387.880859375
2
Iteration 25900: Loss = -12387.8818359375
3
Iteration 26000: Loss = -12387.880859375
4
Iteration 26100: Loss = -12387.8828125
5
Iteration 26200: Loss = -12387.8818359375
6
Iteration 26300: Loss = -12387.8798828125
Iteration 26400: Loss = -12387.880859375
1
Iteration 26500: Loss = -12387.8818359375
2
Iteration 26600: Loss = -12387.8828125
3
Iteration 26700: Loss = -12387.880859375
4
Iteration 26800: Loss = -12387.880859375
5
Iteration 26900: Loss = -12387.880859375
6
Iteration 27000: Loss = -12387.8798828125
Iteration 27100: Loss = -12387.8818359375
1
Iteration 27200: Loss = -12387.8798828125
Iteration 27300: Loss = -12387.8798828125
Iteration 27400: Loss = -12387.880859375
1
Iteration 27500: Loss = -12387.880859375
2
Iteration 27600: Loss = -12387.8818359375
3
Iteration 27700: Loss = -12387.8818359375
4
Iteration 27800: Loss = -12387.8828125
5
Iteration 27900: Loss = -12387.880859375
6
Iteration 28000: Loss = -12387.8818359375
7
Iteration 28100: Loss = -12387.8818359375
8
Iteration 28200: Loss = -12387.880859375
9
Iteration 28300: Loss = -12387.880859375
10
Iteration 28400: Loss = -12387.880859375
11
Iteration 28500: Loss = -12387.8828125
12
Iteration 28600: Loss = -12387.8818359375
13
Iteration 28700: Loss = -12387.880859375
14
Iteration 28800: Loss = -12387.880859375
15
Stopping early at iteration 28800 due to no improvement.
pi: tensor([[9.9999e-01, 1.0798e-05],
        [5.9492e-01, 4.0508e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0092, 0.9908], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2021, 0.1520],
         [0.9810, 0.1945]],

        [[0.0112, 0.2003],
         [0.1863, 0.0200]],

        [[0.1582, 0.1970],
         [0.9838, 0.9915]],

        [[0.1317, 0.2226],
         [0.0208, 0.9825]],

        [[0.5524, 0.1948],
         [0.0114, 0.0550]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 36
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0023490653704932355
Average Adjusted Rand Index: 0.0
[0.0023490653704932355, 0.0023490653704932355] [0.0, 0.0] [12387.880859375, 12387.880859375]
-------------------------------------
This iteration is 85
True Objective function: Loss = -11782.353473329336
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -40887.01953125
Iteration 100: Loss = -26102.095703125
Iteration 200: Loss = -16236.0185546875
Iteration 300: Loss = -13592.283203125
Iteration 400: Loss = -12918.0625
Iteration 500: Loss = -12691.7275390625
Iteration 600: Loss = -12591.7236328125
Iteration 700: Loss = -12535.0556640625
Iteration 800: Loss = -12497.5380859375
Iteration 900: Loss = -12468.134765625
Iteration 1000: Loss = -12448.88671875
Iteration 1100: Loss = -12423.275390625
Iteration 1200: Loss = -12396.0107421875
Iteration 1300: Loss = -12376.5927734375
Iteration 1400: Loss = -12360.9443359375
Iteration 1500: Loss = -12342.5009765625
Iteration 1600: Loss = -12334.0703125
Iteration 1700: Loss = -12327.6279296875
Iteration 1800: Loss = -12319.115234375
Iteration 1900: Loss = -12316.609375
Iteration 2000: Loss = -12313.8681640625
Iteration 2100: Loss = -12312.3515625
Iteration 2200: Loss = -12311.078125
Iteration 2300: Loss = -12309.875
Iteration 2400: Loss = -12308.8662109375
Iteration 2500: Loss = -12307.890625
Iteration 2600: Loss = -12306.923828125
Iteration 2700: Loss = -12305.978515625
Iteration 2800: Loss = -12304.943359375
Iteration 2900: Loss = -12303.4931640625
Iteration 3000: Loss = -12302.4248046875
Iteration 3100: Loss = -12300.7490234375
Iteration 3200: Loss = -12298.345703125
Iteration 3300: Loss = -12292.689453125
Iteration 3400: Loss = -12284.26953125
Iteration 3500: Loss = -12277.0849609375
Iteration 3600: Loss = -12267.373046875
Iteration 3700: Loss = -12254.4033203125
Iteration 3800: Loss = -12251.638671875
Iteration 3900: Loss = -12249.8447265625
Iteration 4000: Loss = -12247.2509765625
Iteration 4100: Loss = -12245.4375
Iteration 4200: Loss = -12237.27734375
Iteration 4300: Loss = -12236.3583984375
Iteration 4400: Loss = -12235.947265625
Iteration 4500: Loss = -12235.6298828125
Iteration 4600: Loss = -12235.3115234375
Iteration 4700: Loss = -12235.0009765625
Iteration 4800: Loss = -12234.796875
Iteration 4900: Loss = -12234.6533203125
Iteration 5000: Loss = -12234.5322265625
Iteration 5100: Loss = -12234.3896484375
Iteration 5200: Loss = -12226.068359375
Iteration 5300: Loss = -12225.7236328125
Iteration 5400: Loss = -12222.4970703125
Iteration 5500: Loss = -12203.412109375
Iteration 5600: Loss = -12202.8701171875
Iteration 5700: Loss = -12202.6611328125
Iteration 5800: Loss = -12202.529296875
Iteration 5900: Loss = -12202.4306640625
Iteration 6000: Loss = -12202.3525390625
Iteration 6100: Loss = -12202.2880859375
Iteration 6200: Loss = -12202.2333984375
Iteration 6300: Loss = -12202.1826171875
Iteration 6400: Loss = -12202.1396484375
Iteration 6500: Loss = -12202.099609375
Iteration 6600: Loss = -12202.06640625
Iteration 6700: Loss = -12202.0322265625
Iteration 6800: Loss = -12202.001953125
Iteration 6900: Loss = -12201.974609375
Iteration 7000: Loss = -12201.947265625
Iteration 7100: Loss = -12201.921875
Iteration 7200: Loss = -12201.896484375
Iteration 7300: Loss = -12201.8740234375
Iteration 7400: Loss = -12201.845703125
Iteration 7500: Loss = -12201.8115234375
Iteration 7600: Loss = -12201.7421875
Iteration 7700: Loss = -12201.5478515625
Iteration 7800: Loss = -12199.078125
Iteration 7900: Loss = -12194.0751953125
Iteration 8000: Loss = -12193.1669921875
Iteration 8100: Loss = -12190.8056640625
Iteration 8200: Loss = -12188.1572265625
Iteration 8300: Loss = -12186.419921875
Iteration 8400: Loss = -12178.1904296875
Iteration 8500: Loss = -12167.7431640625
Iteration 8600: Loss = -12160.5390625
Iteration 8700: Loss = -12140.90625
Iteration 8800: Loss = -12109.9189453125
Iteration 8900: Loss = -12103.8125
Iteration 9000: Loss = -12087.3857421875
Iteration 9100: Loss = -12041.05859375
Iteration 9200: Loss = -12022.4775390625
Iteration 9300: Loss = -12021.4921875
Iteration 9400: Loss = -12020.3134765625
Iteration 9500: Loss = -12020.1923828125
Iteration 9600: Loss = -12020.1171875
Iteration 9700: Loss = -12020.064453125
Iteration 9800: Loss = -12020.0244140625
Iteration 9900: Loss = -12019.994140625
Iteration 10000: Loss = -12019.9658203125
Iteration 10100: Loss = -12019.9326171875
Iteration 10200: Loss = -12019.9072265625
Iteration 10300: Loss = -12019.8876953125
Iteration 10400: Loss = -12019.7275390625
Iteration 10500: Loss = -12019.6376953125
Iteration 10600: Loss = -12019.6259765625
Iteration 10700: Loss = -12019.615234375
Iteration 10800: Loss = -12019.607421875
Iteration 10900: Loss = -12019.599609375
Iteration 11000: Loss = -12019.5927734375
Iteration 11100: Loss = -12019.5888671875
Iteration 11200: Loss = -12019.5830078125
Iteration 11300: Loss = -12019.578125
Iteration 11400: Loss = -12019.5712890625
Iteration 11500: Loss = -12019.5673828125
Iteration 11600: Loss = -12019.5625
Iteration 11700: Loss = -12019.544921875
Iteration 11800: Loss = -12019.52734375
Iteration 11900: Loss = -12019.5234375
Iteration 12000: Loss = -12019.5205078125
Iteration 12100: Loss = -12019.51953125
Iteration 12200: Loss = -12019.515625
Iteration 12300: Loss = -12019.5146484375
Iteration 12400: Loss = -12019.5126953125
Iteration 12500: Loss = -12019.5107421875
Iteration 12600: Loss = -12019.5087890625
Iteration 12700: Loss = -12019.5068359375
Iteration 12800: Loss = -12019.5048828125
Iteration 12900: Loss = -12019.50390625
Iteration 13000: Loss = -12019.501953125
Iteration 13100: Loss = -12019.5009765625
Iteration 13200: Loss = -12019.4990234375
Iteration 13300: Loss = -12019.498046875
Iteration 13400: Loss = -12019.49609375
Iteration 13500: Loss = -12019.4951171875
Iteration 13600: Loss = -12019.4951171875
Iteration 13700: Loss = -12019.494140625
Iteration 13800: Loss = -12019.494140625
Iteration 13900: Loss = -12019.4921875
Iteration 14000: Loss = -12019.4912109375
Iteration 14100: Loss = -12019.490234375
Iteration 14200: Loss = -12019.4892578125
Iteration 14300: Loss = -12019.490234375
1
Iteration 14400: Loss = -12019.48828125
Iteration 14500: Loss = -12019.48828125
Iteration 14600: Loss = -12019.4873046875
Iteration 14700: Loss = -12019.4873046875
Iteration 14800: Loss = -12019.4853515625
Iteration 14900: Loss = -12019.486328125
1
Iteration 15000: Loss = -12019.4853515625
Iteration 15100: Loss = -12019.484375
Iteration 15200: Loss = -12019.484375
Iteration 15300: Loss = -12019.4833984375
Iteration 15400: Loss = -12019.484375
1
Iteration 15500: Loss = -12019.4833984375
Iteration 15600: Loss = -12019.4833984375
Iteration 15700: Loss = -12019.482421875
Iteration 15800: Loss = -12019.48046875
Iteration 15900: Loss = -12019.4814453125
1
Iteration 16000: Loss = -12019.4794921875
Iteration 16100: Loss = -12019.4814453125
1
Iteration 16200: Loss = -12019.4794921875
Iteration 16300: Loss = -12019.4794921875
Iteration 16400: Loss = -12019.48046875
1
Iteration 16500: Loss = -12019.4794921875
Iteration 16600: Loss = -12019.478515625
Iteration 16700: Loss = -12019.478515625
Iteration 16800: Loss = -12019.4794921875
1
Iteration 16900: Loss = -12019.4775390625
Iteration 17000: Loss = -12019.4775390625
Iteration 17100: Loss = -12019.4775390625
Iteration 17200: Loss = -12019.4775390625
Iteration 17300: Loss = -12019.4775390625
Iteration 17400: Loss = -12019.478515625
1
Iteration 17500: Loss = -12019.4775390625
Iteration 17600: Loss = -12019.4765625
Iteration 17700: Loss = -12019.478515625
1
Iteration 17800: Loss = -12019.478515625
2
Iteration 17900: Loss = -12019.4765625
Iteration 18000: Loss = -12019.4765625
Iteration 18100: Loss = -12019.4775390625
1
Iteration 18200: Loss = -12019.4775390625
2
Iteration 18300: Loss = -12019.4775390625
3
Iteration 18400: Loss = -12019.478515625
4
Iteration 18500: Loss = -12019.4755859375
Iteration 18600: Loss = -12019.4775390625
1
Iteration 18700: Loss = -12019.48046875
2
Iteration 18800: Loss = -12019.4775390625
3
Iteration 18900: Loss = -12019.4755859375
Iteration 19000: Loss = -12019.4794921875
1
Iteration 19100: Loss = -12019.4765625
2
Iteration 19200: Loss = -12019.4775390625
3
Iteration 19300: Loss = -12019.47265625
Iteration 19400: Loss = -12019.4716796875
Iteration 19500: Loss = -12019.4716796875
Iteration 19600: Loss = -12019.4736328125
1
Iteration 19700: Loss = -12019.4716796875
Iteration 19800: Loss = -12019.470703125
Iteration 19900: Loss = -12019.470703125
Iteration 20000: Loss = -12019.470703125
Iteration 20100: Loss = -12019.4716796875
1
Iteration 20200: Loss = -12019.47265625
2
Iteration 20300: Loss = -12019.470703125
Iteration 20400: Loss = -12019.4697265625
Iteration 20500: Loss = -12019.4697265625
Iteration 20600: Loss = -12019.47265625
1
Iteration 20700: Loss = -12019.470703125
2
Iteration 20800: Loss = -12019.4697265625
Iteration 20900: Loss = -12019.4697265625
Iteration 21000: Loss = -12019.470703125
1
Iteration 21100: Loss = -12019.470703125
2
Iteration 21200: Loss = -12019.4697265625
Iteration 21300: Loss = -12019.470703125
1
Iteration 21400: Loss = -12019.4697265625
Iteration 21500: Loss = -12019.462890625
Iteration 21600: Loss = -12012.5263671875
Iteration 21700: Loss = -12012.4033203125
Iteration 21800: Loss = -12012.3828125
Iteration 21900: Loss = -12012.373046875
Iteration 22000: Loss = -12012.369140625
Iteration 22100: Loss = -12012.3662109375
Iteration 22200: Loss = -12012.36328125
Iteration 22300: Loss = -12012.361328125
Iteration 22400: Loss = -12012.361328125
Iteration 22500: Loss = -12012.3603515625
Iteration 22600: Loss = -12012.3583984375
Iteration 22700: Loss = -12012.3583984375
Iteration 22800: Loss = -12012.357421875
Iteration 22900: Loss = -12012.3564453125
Iteration 23000: Loss = -12012.3564453125
Iteration 23100: Loss = -12012.35546875
Iteration 23200: Loss = -12012.35546875
Iteration 23300: Loss = -12012.35546875
Iteration 23400: Loss = -12012.35546875
Iteration 23500: Loss = -12012.3564453125
1
Iteration 23600: Loss = -12012.3544921875
Iteration 23700: Loss = -12012.3544921875
Iteration 23800: Loss = -12012.3544921875
Iteration 23900: Loss = -12012.3544921875
Iteration 24000: Loss = -12012.3544921875
Iteration 24100: Loss = -12012.3564453125
1
Iteration 24200: Loss = -12012.3564453125
2
Iteration 24300: Loss = -12012.3544921875
Iteration 24400: Loss = -12012.353515625
Iteration 24500: Loss = -12012.353515625
Iteration 24600: Loss = -12012.35546875
1
Iteration 24700: Loss = -12012.3544921875
2
Iteration 24800: Loss = -12012.3544921875
3
Iteration 24900: Loss = -12012.3544921875
4
Iteration 25000: Loss = -12012.35546875
5
Iteration 25100: Loss = -12012.35546875
6
Iteration 25200: Loss = -12012.3544921875
7
Iteration 25300: Loss = -12012.353515625
Iteration 25400: Loss = -12012.357421875
1
Iteration 25500: Loss = -12012.3544921875
2
Iteration 25600: Loss = -12012.353515625
Iteration 25700: Loss = -12012.3017578125
Iteration 25800: Loss = -12012.271484375
Iteration 25900: Loss = -12012.2705078125
Iteration 26000: Loss = -12012.271484375
1
Iteration 26100: Loss = -12012.2705078125
Iteration 26200: Loss = -12012.2705078125
Iteration 26300: Loss = -12012.271484375
1
Iteration 26400: Loss = -12012.271484375
2
Iteration 26500: Loss = -12012.271484375
3
Iteration 26600: Loss = -12012.26953125
Iteration 26700: Loss = -12012.26953125
Iteration 26800: Loss = -12012.2705078125
1
Iteration 26900: Loss = -12012.26953125
Iteration 27000: Loss = -12012.2705078125
1
Iteration 27100: Loss = -12012.2705078125
2
Iteration 27200: Loss = -12012.2705078125
3
Iteration 27300: Loss = -12012.2705078125
4
Iteration 27400: Loss = -12012.2705078125
5
Iteration 27500: Loss = -12012.2705078125
6
Iteration 27600: Loss = -12012.2705078125
7
Iteration 27700: Loss = -12012.26953125
Iteration 27800: Loss = -12012.2705078125
1
Iteration 27900: Loss = -12012.2705078125
2
Iteration 28000: Loss = -12012.2705078125
3
Iteration 28100: Loss = -12012.26953125
Iteration 28200: Loss = -12012.2705078125
1
Iteration 28300: Loss = -12012.267578125
Iteration 28400: Loss = -12012.2666015625
Iteration 28500: Loss = -12012.26953125
1
Iteration 28600: Loss = -12012.271484375
2
Iteration 28700: Loss = -12012.26953125
3
Iteration 28800: Loss = -12012.2705078125
4
Iteration 28900: Loss = -12012.2685546875
5
Iteration 29000: Loss = -12012.2685546875
6
Iteration 29100: Loss = -12012.26953125
7
Iteration 29200: Loss = -12012.26953125
8
Iteration 29300: Loss = -12012.26953125
9
Iteration 29400: Loss = -12012.26953125
10
Iteration 29500: Loss = -12012.2685546875
11
Iteration 29600: Loss = -12012.26953125
12
Iteration 29700: Loss = -12012.26953125
13
Iteration 29800: Loss = -12012.267578125
14
Iteration 29900: Loss = -12012.267578125
15
Stopping early at iteration 29900 due to no improvement.
pi: tensor([[0.5255, 0.4745],
        [0.8926, 0.1074]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5484, 0.4516], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2485, 0.1008],
         [0.0137, 0.3041]],

        [[0.9422, 0.0962],
         [0.0209, 0.2846]],

        [[0.9878, 0.0915],
         [0.0227, 0.9432]],

        [[0.9884, 0.8430],
         [0.3135, 0.9300]],

        [[0.8071, 0.1024],
         [0.9499, 0.9279]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448427857772554
time is 1
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: -0.002012492069741506
Average Adjusted Rand Index: 0.7531288157634738
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -50806.69921875
Iteration 100: Loss = -28548.083984375
Iteration 200: Loss = -16235.92578125
Iteration 300: Loss = -13558.8544921875
Iteration 400: Loss = -13023.2119140625
Iteration 500: Loss = -12765.6865234375
Iteration 600: Loss = -12578.78125
Iteration 700: Loss = -12511.8857421875
Iteration 800: Loss = -12468.6572265625
Iteration 900: Loss = -12438.203125
Iteration 1000: Loss = -12417.1181640625
Iteration 1100: Loss = -12388.0576171875
Iteration 1200: Loss = -12376.62890625
Iteration 1300: Loss = -12366.92578125
Iteration 1400: Loss = -12359.8740234375
Iteration 1500: Loss = -12354.3984375
Iteration 1600: Loss = -12349.748046875
Iteration 1700: Loss = -12340.912109375
Iteration 1800: Loss = -12334.43359375
Iteration 1900: Loss = -12331.1865234375
Iteration 2000: Loss = -12328.6357421875
Iteration 2100: Loss = -12326.5009765625
Iteration 2200: Loss = -12324.6669921875
Iteration 2300: Loss = -12323.0732421875
Iteration 2400: Loss = -12321.6728515625
Iteration 2500: Loss = -12320.435546875
Iteration 2600: Loss = -12319.3359375
Iteration 2700: Loss = -12318.3505859375
Iteration 2800: Loss = -12317.4658203125
Iteration 2900: Loss = -12316.6708984375
Iteration 3000: Loss = -12315.951171875
Iteration 3100: Loss = -12315.298828125
Iteration 3200: Loss = -12314.705078125
Iteration 3300: Loss = -12314.1630859375
Iteration 3400: Loss = -12313.66796875
Iteration 3500: Loss = -12313.21484375
Iteration 3600: Loss = -12312.7998046875
Iteration 3700: Loss = -12312.4169921875
Iteration 3800: Loss = -12312.064453125
Iteration 3900: Loss = -12311.740234375
Iteration 4000: Loss = -12311.439453125
Iteration 4100: Loss = -12311.16015625
Iteration 4200: Loss = -12310.9013671875
Iteration 4300: Loss = -12310.6611328125
Iteration 4400: Loss = -12310.4375
Iteration 4500: Loss = -12310.2275390625
Iteration 4600: Loss = -12310.03125
Iteration 4700: Loss = -12309.8486328125
Iteration 4800: Loss = -12309.677734375
Iteration 4900: Loss = -12309.5166015625
Iteration 5000: Loss = -12309.365234375
Iteration 5100: Loss = -12309.2236328125
Iteration 5200: Loss = -12309.091796875
Iteration 5300: Loss = -12308.9658203125
Iteration 5400: Loss = -12308.8486328125
Iteration 5500: Loss = -12308.7392578125
Iteration 5600: Loss = -12308.63671875
Iteration 5700: Loss = -12308.5390625
Iteration 5800: Loss = -12308.447265625
Iteration 5900: Loss = -12308.3603515625
Iteration 6000: Loss = -12308.2802734375
Iteration 6100: Loss = -12308.201171875
Iteration 6200: Loss = -12308.12890625
Iteration 6300: Loss = -12308.060546875
Iteration 6400: Loss = -12307.998046875
Iteration 6500: Loss = -12307.9365234375
Iteration 6600: Loss = -12307.87890625
Iteration 6700: Loss = -12307.8232421875
Iteration 6800: Loss = -12307.7724609375
Iteration 6900: Loss = -12307.7236328125
Iteration 7000: Loss = -12307.6787109375
Iteration 7100: Loss = -12307.634765625
Iteration 7200: Loss = -12307.59375
Iteration 7300: Loss = -12307.556640625
Iteration 7400: Loss = -12307.517578125
Iteration 7500: Loss = -12307.4814453125
Iteration 7600: Loss = -12307.4521484375
Iteration 7700: Loss = -12307.419921875
Iteration 7800: Loss = -12307.3896484375
Iteration 7900: Loss = -12307.36328125
Iteration 8000: Loss = -12307.3369140625
Iteration 8100: Loss = -12307.3115234375
Iteration 8200: Loss = -12307.2890625
Iteration 8300: Loss = -12307.265625
Iteration 8400: Loss = -12307.244140625
Iteration 8500: Loss = -12307.2236328125
Iteration 8600: Loss = -12307.2041015625
Iteration 8700: Loss = -12307.18359375
Iteration 8800: Loss = -12307.1650390625
Iteration 8900: Loss = -12307.1474609375
Iteration 9000: Loss = -12307.1318359375
Iteration 9100: Loss = -12307.1181640625
Iteration 9200: Loss = -12307.103515625
Iteration 9300: Loss = -12307.08984375
Iteration 9400: Loss = -12307.078125
Iteration 9500: Loss = -12307.0673828125
Iteration 9600: Loss = -12307.0576171875
Iteration 9700: Loss = -12307.0478515625
Iteration 9800: Loss = -12307.0400390625
Iteration 9900: Loss = -12307.0322265625
Iteration 10000: Loss = -12307.0263671875
Iteration 10100: Loss = -12307.0166015625
Iteration 10200: Loss = -12307.01171875
Iteration 10300: Loss = -12307.0078125
Iteration 10400: Loss = -12307.0009765625
Iteration 10500: Loss = -12306.99609375
Iteration 10600: Loss = -12306.9921875
Iteration 10700: Loss = -12306.986328125
Iteration 10800: Loss = -12306.982421875
Iteration 10900: Loss = -12306.9794921875
Iteration 11000: Loss = -12306.9736328125
Iteration 11100: Loss = -12306.9697265625
Iteration 11200: Loss = -12306.966796875
Iteration 11300: Loss = -12306.96484375
Iteration 11400: Loss = -12306.9609375
Iteration 11500: Loss = -12306.95703125
Iteration 11600: Loss = -12306.955078125
Iteration 11700: Loss = -12306.9521484375
Iteration 11800: Loss = -12306.947265625
Iteration 11900: Loss = -12306.943359375
Iteration 12000: Loss = -12306.9375
Iteration 12100: Loss = -12306.9267578125
Iteration 12200: Loss = -12306.890625
Iteration 12300: Loss = -12306.1455078125
Iteration 12400: Loss = -12304.927734375
Iteration 12500: Loss = -12304.083984375
Iteration 12600: Loss = -12302.900390625
Iteration 12700: Loss = -12302.7666015625
Iteration 12800: Loss = -12302.6923828125
Iteration 12900: Loss = -12302.6435546875
Iteration 13000: Loss = -12302.6064453125
Iteration 13100: Loss = -12302.576171875
Iteration 13200: Loss = -12302.5546875
Iteration 13300: Loss = -12302.537109375
Iteration 13400: Loss = -12302.5234375
Iteration 13500: Loss = -12302.5078125
Iteration 13600: Loss = -12302.4970703125
Iteration 13700: Loss = -12302.4873046875
Iteration 13800: Loss = -12302.4794921875
Iteration 13900: Loss = -12302.470703125
Iteration 14000: Loss = -12302.4638671875
Iteration 14100: Loss = -12302.4599609375
Iteration 14200: Loss = -12302.453125
Iteration 14300: Loss = -12302.4482421875
Iteration 14400: Loss = -12302.4423828125
Iteration 14500: Loss = -12302.4404296875
Iteration 14600: Loss = -12302.4345703125
Iteration 14700: Loss = -12302.4326171875
Iteration 14800: Loss = -12302.4287109375
Iteration 14900: Loss = -12302.42578125
Iteration 15000: Loss = -12302.4248046875
Iteration 15100: Loss = -12302.4208984375
Iteration 15200: Loss = -12302.4189453125
Iteration 15300: Loss = -12302.4169921875
Iteration 15400: Loss = -12302.4140625
Iteration 15500: Loss = -12302.4140625
Iteration 15600: Loss = -12302.4091796875
Iteration 15700: Loss = -12302.408203125
Iteration 15800: Loss = -12302.4072265625
Iteration 15900: Loss = -12302.40625
Iteration 16000: Loss = -12302.4052734375
Iteration 16100: Loss = -12302.4033203125
Iteration 16200: Loss = -12302.4013671875
Iteration 16300: Loss = -12302.4013671875
Iteration 16400: Loss = -12302.3994140625
Iteration 16500: Loss = -12302.3984375
Iteration 16600: Loss = -12302.3974609375
Iteration 16700: Loss = -12302.396484375
Iteration 16800: Loss = -12302.3955078125
Iteration 16900: Loss = -12302.3955078125
Iteration 17000: Loss = -12302.39453125
Iteration 17100: Loss = -12302.39453125
Iteration 17200: Loss = -12302.39453125
Iteration 17300: Loss = -12302.3935546875
Iteration 17400: Loss = -12302.3935546875
Iteration 17500: Loss = -12302.390625
Iteration 17600: Loss = -12302.392578125
1
Iteration 17700: Loss = -12302.390625
Iteration 17800: Loss = -12302.388671875
Iteration 17900: Loss = -12302.388671875
Iteration 18000: Loss = -12302.390625
1
Iteration 18100: Loss = -12302.388671875
Iteration 18200: Loss = -12302.388671875
Iteration 18300: Loss = -12302.3876953125
Iteration 18400: Loss = -12302.3876953125
Iteration 18500: Loss = -12302.3876953125
Iteration 18600: Loss = -12302.3876953125
Iteration 18700: Loss = -12302.38671875
Iteration 18800: Loss = -12302.38671875
Iteration 18900: Loss = -12302.3857421875
Iteration 19000: Loss = -12302.38671875
1
Iteration 19100: Loss = -12302.38671875
2
Iteration 19200: Loss = -12302.3857421875
Iteration 19300: Loss = -12302.38671875
1
Iteration 19400: Loss = -12302.384765625
Iteration 19500: Loss = -12302.3857421875
1
Iteration 19600: Loss = -12302.3857421875
2
Iteration 19700: Loss = -12302.3828125
Iteration 19800: Loss = -12302.384765625
1
Iteration 19900: Loss = -12302.384765625
2
Iteration 20000: Loss = -12302.3837890625
3
Iteration 20100: Loss = -12302.3837890625
4
Iteration 20200: Loss = -12302.3837890625
5
Iteration 20300: Loss = -12302.3837890625
6
Iteration 20400: Loss = -12302.3837890625
7
Iteration 20500: Loss = -12302.3828125
Iteration 20600: Loss = -12302.3818359375
Iteration 20700: Loss = -12302.3818359375
Iteration 20800: Loss = -12302.3837890625
1
Iteration 20900: Loss = -12302.3818359375
Iteration 21000: Loss = -12302.3837890625
1
Iteration 21100: Loss = -12302.380859375
Iteration 21200: Loss = -12302.3818359375
1
Iteration 21300: Loss = -12302.37890625
Iteration 21400: Loss = -12302.0556640625
Iteration 21500: Loss = -12301.2568359375
Iteration 21600: Loss = -12301.2548828125
Iteration 21700: Loss = -12301.2548828125
Iteration 21800: Loss = -12301.2548828125
Iteration 21900: Loss = -12301.2548828125
Iteration 22000: Loss = -12301.25390625
Iteration 22100: Loss = -12301.2548828125
1
Iteration 22200: Loss = -12301.2568359375
2
Iteration 22300: Loss = -12301.255859375
3
Iteration 22400: Loss = -12301.2548828125
4
Iteration 22500: Loss = -12301.25390625
Iteration 22600: Loss = -12301.2548828125
1
Iteration 22700: Loss = -12301.25390625
Iteration 22800: Loss = -12301.25390625
Iteration 22900: Loss = -12301.2548828125
1
Iteration 23000: Loss = -12301.2548828125
2
Iteration 23100: Loss = -12301.2548828125
3
Iteration 23200: Loss = -12301.2529296875
Iteration 23300: Loss = -12301.2548828125
1
Iteration 23400: Loss = -12301.2548828125
2
Iteration 23500: Loss = -12301.25390625
3
Iteration 23600: Loss = -12301.255859375
4
Iteration 23700: Loss = -12301.2548828125
5
Iteration 23800: Loss = -12301.2548828125
6
Iteration 23900: Loss = -12301.2548828125
7
Iteration 24000: Loss = -12301.255859375
8
Iteration 24100: Loss = -12301.25390625
9
Iteration 24200: Loss = -12301.2529296875
Iteration 24300: Loss = -12301.06640625
Iteration 24400: Loss = -12300.89453125
Iteration 24500: Loss = -12300.89453125
Iteration 24600: Loss = -12300.8955078125
1
Iteration 24700: Loss = -12300.8935546875
Iteration 24800: Loss = -12300.8935546875
Iteration 24900: Loss = -12300.8955078125
1
Iteration 25000: Loss = -12300.8935546875
Iteration 25100: Loss = -12300.89453125
1
Iteration 25200: Loss = -12300.89453125
2
Iteration 25300: Loss = -12300.89453125
3
Iteration 25400: Loss = -12300.8935546875
Iteration 25500: Loss = -12300.892578125
Iteration 25600: Loss = -12300.89453125
1
Iteration 25700: Loss = -12300.8935546875
2
Iteration 25800: Loss = -12300.89453125
3
Iteration 25900: Loss = -12300.89453125
4
Iteration 26000: Loss = -12300.8935546875
5
Iteration 26100: Loss = -12300.8955078125
6
Iteration 26200: Loss = -12300.89453125
7
Iteration 26300: Loss = -12300.8935546875
8
Iteration 26400: Loss = -12300.89453125
9
Iteration 26500: Loss = -12300.896484375
10
Iteration 26600: Loss = -12300.8935546875
11
Iteration 26700: Loss = -12300.8955078125
12
Iteration 26800: Loss = -12300.89453125
13
Iteration 26900: Loss = -12300.89453125
14
Iteration 27000: Loss = -12300.8935546875
15
Stopping early at iteration 27000 due to no improvement.
pi: tensor([[1.0000e+00, 6.5583e-07],
        [1.8661e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9769, 0.0231], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1991, 0.0793],
         [0.0325, 0.2437]],

        [[0.9275, 0.2297],
         [0.9489, 0.9886]],

        [[0.0119, 0.1538],
         [0.9851, 0.0133]],

        [[0.9913, 0.2201],
         [0.0069, 0.0268]],

        [[0.6824, 0.1629],
         [0.0080, 0.9788]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: -0.011374456256342739
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: -0.0020289408121562723
Average Adjusted Rand Index: -0.0034477442668388166
[-0.002012492069741506, -0.0020289408121562723] [0.7531288157634738, -0.0034477442668388166] [12012.267578125, 12300.8935546875]
-------------------------------------
This iteration is 86
True Objective function: Loss = -11959.42468029651
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22826.15625
Iteration 100: Loss = -15939.6552734375
Iteration 200: Loss = -13283.7353515625
Iteration 300: Loss = -12751.119140625
Iteration 400: Loss = -12624.697265625
Iteration 500: Loss = -12572.505859375
Iteration 600: Loss = -12542.0458984375
Iteration 700: Loss = -12518.5595703125
Iteration 800: Loss = -12503.6591796875
Iteration 900: Loss = -12493.11328125
Iteration 1000: Loss = -12486.6279296875
Iteration 1100: Loss = -12481.900390625
Iteration 1200: Loss = -12478.333984375
Iteration 1300: Loss = -12475.765625
Iteration 1400: Loss = -12473.931640625
Iteration 1500: Loss = -12472.5615234375
Iteration 1600: Loss = -12471.4951171875
Iteration 1700: Loss = -12470.6455078125
Iteration 1800: Loss = -12469.9560546875
Iteration 1900: Loss = -12469.380859375
Iteration 2000: Loss = -12468.9013671875
Iteration 2100: Loss = -12468.50390625
Iteration 2200: Loss = -12468.1669921875
Iteration 2300: Loss = -12467.873046875
Iteration 2400: Loss = -12467.6162109375
Iteration 2500: Loss = -12467.3896484375
Iteration 2600: Loss = -12467.1904296875
Iteration 2700: Loss = -12467.0166015625
Iteration 2800: Loss = -12466.8583984375
Iteration 2900: Loss = -12466.7138671875
Iteration 3000: Loss = -12466.583984375
Iteration 3100: Loss = -12466.4677734375
Iteration 3200: Loss = -12466.3583984375
Iteration 3300: Loss = -12466.26171875
Iteration 3400: Loss = -12466.1669921875
Iteration 3500: Loss = -12466.0830078125
Iteration 3600: Loss = -12466.0048828125
Iteration 3700: Loss = -12465.9326171875
Iteration 3800: Loss = -12465.8671875
Iteration 3900: Loss = -12465.8056640625
Iteration 4000: Loss = -12465.748046875
Iteration 4100: Loss = -12465.6962890625
Iteration 4200: Loss = -12465.646484375
Iteration 4300: Loss = -12465.6005859375
Iteration 4400: Loss = -12465.556640625
Iteration 4500: Loss = -12465.5166015625
Iteration 4600: Loss = -12465.4765625
Iteration 4700: Loss = -12465.4404296875
Iteration 4800: Loss = -12465.4052734375
Iteration 4900: Loss = -12465.373046875
Iteration 5000: Loss = -12465.341796875
Iteration 5100: Loss = -12465.3154296875
Iteration 5200: Loss = -12465.28515625
Iteration 5300: Loss = -12465.259765625
Iteration 5400: Loss = -12465.2353515625
Iteration 5500: Loss = -12465.2099609375
Iteration 5600: Loss = -12465.1884765625
Iteration 5700: Loss = -12465.16796875
Iteration 5800: Loss = -12465.1494140625
Iteration 5900: Loss = -12465.130859375
Iteration 6000: Loss = -12465.1142578125
Iteration 6100: Loss = -12465.1005859375
Iteration 6200: Loss = -12465.0849609375
Iteration 6300: Loss = -12465.0732421875
Iteration 6400: Loss = -12465.0595703125
Iteration 6500: Loss = -12465.0498046875
Iteration 6600: Loss = -12465.041015625
Iteration 6700: Loss = -12465.0322265625
Iteration 6800: Loss = -12465.025390625
Iteration 6900: Loss = -12465.0185546875
Iteration 7000: Loss = -12465.0126953125
Iteration 7100: Loss = -12465.0068359375
Iteration 7200: Loss = -12465.0029296875
Iteration 7300: Loss = -12464.9970703125
Iteration 7400: Loss = -12464.994140625
Iteration 7500: Loss = -12464.9892578125
Iteration 7600: Loss = -12464.9873046875
Iteration 7700: Loss = -12464.982421875
Iteration 7800: Loss = -12464.9794921875
Iteration 7900: Loss = -12464.9775390625
Iteration 8000: Loss = -12464.97265625
Iteration 8100: Loss = -12464.970703125
Iteration 8200: Loss = -12464.9677734375
Iteration 8300: Loss = -12464.9658203125
Iteration 8400: Loss = -12464.9638671875
Iteration 8500: Loss = -12464.9609375
Iteration 8600: Loss = -12464.958984375
Iteration 8700: Loss = -12464.9580078125
Iteration 8800: Loss = -12464.9541015625
Iteration 8900: Loss = -12464.953125
Iteration 9000: Loss = -12464.9541015625
1
Iteration 9100: Loss = -12464.94921875
Iteration 9200: Loss = -12464.9482421875
Iteration 9300: Loss = -12464.947265625
Iteration 9400: Loss = -12464.947265625
Iteration 9500: Loss = -12464.9453125
Iteration 9600: Loss = -12464.9443359375
Iteration 9700: Loss = -12464.9423828125
Iteration 9800: Loss = -12464.94140625
Iteration 9900: Loss = -12464.939453125
Iteration 10000: Loss = -12464.9375
Iteration 10100: Loss = -12464.9345703125
Iteration 10200: Loss = -12464.9345703125
Iteration 10300: Loss = -12464.931640625
Iteration 10400: Loss = -12464.9306640625
Iteration 10500: Loss = -12464.9267578125
Iteration 10600: Loss = -12464.9267578125
Iteration 10700: Loss = -12464.923828125
Iteration 10800: Loss = -12464.921875
Iteration 10900: Loss = -12464.9189453125
Iteration 11000: Loss = -12464.9169921875
Iteration 11100: Loss = -12464.9140625
Iteration 11200: Loss = -12464.9091796875
Iteration 11300: Loss = -12464.9072265625
Iteration 11400: Loss = -12464.8935546875
Iteration 11500: Loss = -12464.8310546875
Iteration 11600: Loss = -12464.197265625
Iteration 11700: Loss = -12463.9697265625
Iteration 11800: Loss = -12463.876953125
Iteration 11900: Loss = -12463.830078125
Iteration 12000: Loss = -12463.798828125
Iteration 12100: Loss = -12463.77734375
Iteration 12200: Loss = -12463.7626953125
Iteration 12300: Loss = -12463.751953125
Iteration 12400: Loss = -12463.744140625
Iteration 12500: Loss = -12463.7353515625
Iteration 12600: Loss = -12463.7294921875
Iteration 12700: Loss = -12463.724609375
Iteration 12800: Loss = -12463.716796875
Iteration 12900: Loss = -12463.712890625
Iteration 13000: Loss = -12463.708984375
Iteration 13100: Loss = -12463.7080078125
Iteration 13200: Loss = -12463.7041015625
Iteration 13300: Loss = -12463.703125
Iteration 13400: Loss = -12463.7041015625
1
Iteration 13500: Loss = -12463.7001953125
Iteration 13600: Loss = -12463.69921875
Iteration 13700: Loss = -12463.697265625
Iteration 13800: Loss = -12463.6953125
Iteration 13900: Loss = -12463.6953125
Iteration 14000: Loss = -12463.6943359375
Iteration 14100: Loss = -12463.6943359375
Iteration 14200: Loss = -12463.693359375
Iteration 14300: Loss = -12463.6923828125
Iteration 14400: Loss = -12463.6904296875
Iteration 14500: Loss = -12463.6904296875
Iteration 14600: Loss = -12463.689453125
Iteration 14700: Loss = -12463.6884765625
Iteration 14800: Loss = -12463.6865234375
Iteration 14900: Loss = -12463.685546875
Iteration 15000: Loss = -12463.6845703125
Iteration 15100: Loss = -12463.68359375
Iteration 15200: Loss = -12463.68359375
Iteration 15300: Loss = -12463.6806640625
Iteration 15400: Loss = -12463.6806640625
Iteration 15500: Loss = -12463.677734375
Iteration 15600: Loss = -12463.67578125
Iteration 15700: Loss = -12463.6767578125
1
Iteration 15800: Loss = -12463.6767578125
2
Iteration 15900: Loss = -12463.6748046875
Iteration 16000: Loss = -12463.67578125
1
Iteration 16100: Loss = -12463.6748046875
Iteration 16200: Loss = -12463.6748046875
Iteration 16300: Loss = -12463.67578125
1
Iteration 16400: Loss = -12463.6748046875
Iteration 16500: Loss = -12463.6748046875
Iteration 16600: Loss = -12463.6728515625
Iteration 16700: Loss = -12463.6748046875
1
Iteration 16800: Loss = -12463.673828125
2
Iteration 16900: Loss = -12463.6728515625
Iteration 17000: Loss = -12463.671875
Iteration 17100: Loss = -12463.671875
Iteration 17200: Loss = -12463.6728515625
1
Iteration 17300: Loss = -12463.669921875
Iteration 17400: Loss = -12463.671875
1
Iteration 17500: Loss = -12463.6689453125
Iteration 17600: Loss = -12463.671875
1
Iteration 17700: Loss = -12463.671875
2
Iteration 17800: Loss = -12463.6708984375
3
Iteration 17900: Loss = -12463.669921875
4
Iteration 18000: Loss = -12463.6708984375
5
Iteration 18100: Loss = -12463.6669921875
Iteration 18200: Loss = -12463.66796875
1
Iteration 18300: Loss = -12463.669921875
2
Iteration 18400: Loss = -12463.669921875
3
Iteration 18500: Loss = -12463.6689453125
4
Iteration 18600: Loss = -12463.666015625
Iteration 18700: Loss = -12463.6669921875
1
Iteration 18800: Loss = -12463.6689453125
2
Iteration 18900: Loss = -12463.6669921875
3
Iteration 19000: Loss = -12463.666015625
Iteration 19100: Loss = -12463.666015625
Iteration 19200: Loss = -12463.6650390625
Iteration 19300: Loss = -12463.6650390625
Iteration 19400: Loss = -12463.6630859375
Iteration 19500: Loss = -12463.6650390625
1
Iteration 19600: Loss = -12463.6630859375
Iteration 19700: Loss = -12463.6640625
1
Iteration 19800: Loss = -12463.6630859375
Iteration 19900: Loss = -12463.6640625
1
Iteration 20000: Loss = -12463.6611328125
Iteration 20100: Loss = -12463.6630859375
1
Iteration 20200: Loss = -12463.6630859375
2
Iteration 20300: Loss = -12463.6640625
3
Iteration 20400: Loss = -12463.6640625
4
Iteration 20500: Loss = -12463.662109375
5
Iteration 20600: Loss = -12463.6630859375
6
Iteration 20700: Loss = -12463.6630859375
7
Iteration 20800: Loss = -12463.662109375
8
Iteration 20900: Loss = -12463.662109375
9
Iteration 21000: Loss = -12463.6630859375
10
Iteration 21100: Loss = -12463.6630859375
11
Iteration 21200: Loss = -12463.6630859375
12
Iteration 21300: Loss = -12463.662109375
13
Iteration 21400: Loss = -12463.6650390625
14
Iteration 21500: Loss = -12463.6630859375
15
Stopping early at iteration 21500 due to no improvement.
pi: tensor([[9.9999e-01, 7.4448e-06],
        [5.9506e-04, 9.9940e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9879, 0.0121], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2022, 0.2062],
         [0.3013, 0.2037]],

        [[0.0106, 0.2180],
         [0.0072, 0.0396]],

        [[0.4975, 0.2767],
         [0.0100, 0.7911]],

        [[0.4327, 0.1077],
         [0.0610, 0.9345]],

        [[0.0204, 0.2392],
         [0.4557, 0.3818]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: -0.0003093013866620859
Average Adjusted Rand Index: -0.000597414405600841
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35216.70703125
Iteration 100: Loss = -19909.689453125
Iteration 200: Loss = -13936.2548828125
Iteration 300: Loss = -12927.65234375
Iteration 400: Loss = -12696.9423828125
Iteration 500: Loss = -12608.8525390625
Iteration 600: Loss = -12566.7880859375
Iteration 700: Loss = -12540.7431640625
Iteration 800: Loss = -12525.8388671875
Iteration 900: Loss = -12515.3466796875
Iteration 1000: Loss = -12508.9921875
Iteration 1100: Loss = -12500.685546875
Iteration 1200: Loss = -12493.9169921875
Iteration 1300: Loss = -12490.333984375
Iteration 1400: Loss = -12486.9501953125
Iteration 1500: Loss = -12484.05859375
Iteration 1600: Loss = -12481.9560546875
Iteration 1700: Loss = -12480.3857421875
Iteration 1800: Loss = -12479.1279296875
Iteration 1900: Loss = -12478.001953125
Iteration 2000: Loss = -12472.63671875
Iteration 2100: Loss = -12471.787109375
Iteration 2200: Loss = -12471.1015625
Iteration 2300: Loss = -12470.5205078125
Iteration 2400: Loss = -12470.017578125
Iteration 2500: Loss = -12469.580078125
Iteration 2600: Loss = -12469.193359375
Iteration 2700: Loss = -12468.8486328125
Iteration 2800: Loss = -12468.541015625
Iteration 2900: Loss = -12468.267578125
Iteration 3000: Loss = -12468.021484375
Iteration 3100: Loss = -12467.798828125
Iteration 3200: Loss = -12467.5966796875
Iteration 3300: Loss = -12467.4150390625
Iteration 3400: Loss = -12467.2470703125
Iteration 3500: Loss = -12467.095703125
Iteration 3600: Loss = -12466.95703125
Iteration 3700: Loss = -12466.8291015625
Iteration 3800: Loss = -12466.7080078125
Iteration 3900: Loss = -12466.59765625
Iteration 4000: Loss = -12466.4970703125
Iteration 4100: Loss = -12466.4033203125
Iteration 4200: Loss = -12466.31640625
Iteration 4300: Loss = -12466.23828125
Iteration 4400: Loss = -12466.1630859375
Iteration 4500: Loss = -12466.0947265625
Iteration 4600: Loss = -12466.0302734375
Iteration 4700: Loss = -12465.9697265625
Iteration 4800: Loss = -12465.9130859375
Iteration 4900: Loss = -12465.8603515625
Iteration 5000: Loss = -12465.810546875
Iteration 5100: Loss = -12465.765625
Iteration 5200: Loss = -12465.7236328125
Iteration 5300: Loss = -12465.681640625
Iteration 5400: Loss = -12465.6435546875
Iteration 5500: Loss = -12465.6064453125
Iteration 5600: Loss = -12465.5693359375
Iteration 5700: Loss = -12465.537109375
Iteration 5800: Loss = -12465.5068359375
Iteration 5900: Loss = -12465.478515625
Iteration 6000: Loss = -12465.44921875
Iteration 6100: Loss = -12465.4248046875
Iteration 6200: Loss = -12465.400390625
Iteration 6300: Loss = -12465.3759765625
Iteration 6400: Loss = -12465.3544921875
Iteration 6500: Loss = -12465.33203125
Iteration 6600: Loss = -12465.3125
Iteration 6700: Loss = -12465.29296875
Iteration 6800: Loss = -12465.27734375
Iteration 6900: Loss = -12465.2568359375
Iteration 7000: Loss = -12465.2421875
Iteration 7100: Loss = -12465.2265625
Iteration 7200: Loss = -12465.2119140625
Iteration 7300: Loss = -12465.1982421875
Iteration 7400: Loss = -12465.1865234375
Iteration 7500: Loss = -12465.1728515625
Iteration 7600: Loss = -12465.1611328125
Iteration 7700: Loss = -12465.15234375
Iteration 7800: Loss = -12465.138671875
Iteration 7900: Loss = -12465.1279296875
Iteration 8000: Loss = -12465.119140625
Iteration 8100: Loss = -12465.109375
Iteration 8200: Loss = -12465.1015625
Iteration 8300: Loss = -12465.0927734375
Iteration 8400: Loss = -12465.0849609375
Iteration 8500: Loss = -12465.078125
Iteration 8600: Loss = -12465.0712890625
Iteration 8700: Loss = -12465.0625
Iteration 8800: Loss = -12465.0576171875
Iteration 8900: Loss = -12465.052734375
Iteration 9000: Loss = -12465.044921875
Iteration 9100: Loss = -12465.0380859375
Iteration 9200: Loss = -12465.03515625
Iteration 9300: Loss = -12465.029296875
Iteration 9400: Loss = -12465.0244140625
Iteration 9500: Loss = -12465.01953125
Iteration 9600: Loss = -12465.0146484375
Iteration 9700: Loss = -12465.0126953125
Iteration 9800: Loss = -12465.0087890625
Iteration 9900: Loss = -12465.00390625
Iteration 10000: Loss = -12464.9990234375
Iteration 10100: Loss = -12464.9970703125
Iteration 10200: Loss = -12464.994140625
Iteration 10300: Loss = -12464.9921875
Iteration 10400: Loss = -12464.9873046875
Iteration 10500: Loss = -12464.986328125
Iteration 10600: Loss = -12464.9833984375
Iteration 10700: Loss = -12464.9794921875
Iteration 10800: Loss = -12464.9775390625
Iteration 10900: Loss = -12464.974609375
Iteration 11000: Loss = -12464.9736328125
Iteration 11100: Loss = -12464.9716796875
Iteration 11200: Loss = -12464.96875
Iteration 11300: Loss = -12464.9677734375
Iteration 11400: Loss = -12464.96484375
Iteration 11500: Loss = -12464.962890625
Iteration 11600: Loss = -12464.9609375
Iteration 11700: Loss = -12464.9599609375
Iteration 11800: Loss = -12464.95703125
Iteration 11900: Loss = -12464.9580078125
1
Iteration 12000: Loss = -12464.9541015625
Iteration 12100: Loss = -12464.9541015625
Iteration 12200: Loss = -12464.951171875
Iteration 12300: Loss = -12464.951171875
Iteration 12400: Loss = -12464.9501953125
Iteration 12500: Loss = -12464.94921875
Iteration 12600: Loss = -12464.947265625
Iteration 12700: Loss = -12464.9453125
Iteration 12800: Loss = -12464.9443359375
Iteration 12900: Loss = -12464.9423828125
Iteration 13000: Loss = -12464.9423828125
Iteration 13100: Loss = -12464.9404296875
Iteration 13200: Loss = -12464.9384765625
Iteration 13300: Loss = -12464.9384765625
Iteration 13400: Loss = -12464.9375
Iteration 13500: Loss = -12464.9365234375
Iteration 13600: Loss = -12464.9345703125
Iteration 13700: Loss = -12464.9345703125
Iteration 13800: Loss = -12464.935546875
1
Iteration 13900: Loss = -12464.93359375
Iteration 14000: Loss = -12464.931640625
Iteration 14100: Loss = -12464.931640625
Iteration 14200: Loss = -12464.9306640625
Iteration 14300: Loss = -12464.9287109375
Iteration 14400: Loss = -12464.9306640625
1
Iteration 14500: Loss = -12464.92578125
Iteration 14600: Loss = -12463.2587890625
Iteration 14700: Loss = -12463.2216796875
Iteration 14800: Loss = -12463.1923828125
Iteration 14900: Loss = -12463.140625
Iteration 15000: Loss = -12461.7373046875
Iteration 15100: Loss = -12460.802734375
Iteration 15200: Loss = -12460.708984375
Iteration 15300: Loss = -12460.66796875
Iteration 15400: Loss = -12460.642578125
Iteration 15500: Loss = -12460.62890625
Iteration 15600: Loss = -12460.619140625
Iteration 15700: Loss = -12460.611328125
Iteration 15800: Loss = -12460.6044921875
Iteration 15900: Loss = -12460.599609375
Iteration 16000: Loss = -12460.595703125
Iteration 16100: Loss = -12460.5927734375
Iteration 16200: Loss = -12460.5908203125
Iteration 16300: Loss = -12460.5869140625
Iteration 16400: Loss = -12460.5859375
Iteration 16500: Loss = -12460.5849609375
Iteration 16600: Loss = -12460.58203125
Iteration 16700: Loss = -12460.58203125
Iteration 16800: Loss = -12460.578125
Iteration 16900: Loss = -12460.578125
Iteration 17000: Loss = -12460.578125
Iteration 17100: Loss = -12460.576171875
Iteration 17200: Loss = -12460.576171875
Iteration 17300: Loss = -12460.5751953125
Iteration 17400: Loss = -12460.5732421875
Iteration 17500: Loss = -12460.572265625
Iteration 17600: Loss = -12460.572265625
Iteration 17700: Loss = -12460.5712890625
Iteration 17800: Loss = -12460.5712890625
Iteration 17900: Loss = -12460.5673828125
Iteration 18000: Loss = -12460.5673828125
Iteration 18100: Loss = -12460.56640625
Iteration 18200: Loss = -12460.564453125
Iteration 18300: Loss = -12460.5634765625
Iteration 18400: Loss = -12460.5615234375
Iteration 18500: Loss = -12460.55859375
Iteration 18600: Loss = -12460.560546875
1
Iteration 18700: Loss = -12460.5517578125
Iteration 18800: Loss = -12460.546875
Iteration 18900: Loss = -12460.5400390625
Iteration 19000: Loss = -12460.53125
Iteration 19100: Loss = -12460.5244140625
Iteration 19200: Loss = -12460.517578125
Iteration 19300: Loss = -12460.5087890625
Iteration 19400: Loss = -12460.50390625
Iteration 19500: Loss = -12460.49609375
Iteration 19600: Loss = -12460.48828125
Iteration 19700: Loss = -12460.4833984375
Iteration 19800: Loss = -12460.4814453125
Iteration 19900: Loss = -12460.478515625
Iteration 20000: Loss = -12460.4775390625
Iteration 20100: Loss = -12460.4755859375
Iteration 20200: Loss = -12460.4716796875
Iteration 20300: Loss = -12460.4697265625
Iteration 20400: Loss = -12460.470703125
1
Iteration 20500: Loss = -12460.4697265625
Iteration 20600: Loss = -12460.4677734375
Iteration 20700: Loss = -12460.4677734375
Iteration 20800: Loss = -12460.4677734375
Iteration 20900: Loss = -12460.4658203125
Iteration 21000: Loss = -12460.46484375
Iteration 21100: Loss = -12460.4658203125
1
Iteration 21200: Loss = -12460.46484375
Iteration 21300: Loss = -12460.46484375
Iteration 21400: Loss = -12460.4658203125
1
Iteration 21500: Loss = -12460.46484375
Iteration 21600: Loss = -12460.4638671875
Iteration 21700: Loss = -12460.46484375
1
Iteration 21800: Loss = -12460.4638671875
Iteration 21900: Loss = -12460.4638671875
Iteration 22000: Loss = -12460.4638671875
Iteration 22100: Loss = -12460.46484375
1
Iteration 22200: Loss = -12460.4638671875
Iteration 22300: Loss = -12460.4638671875
Iteration 22400: Loss = -12460.4638671875
Iteration 22500: Loss = -12460.462890625
Iteration 22600: Loss = -12460.462890625
Iteration 22700: Loss = -12460.4638671875
1
Iteration 22800: Loss = -12460.4638671875
2
Iteration 22900: Loss = -12460.4638671875
3
Iteration 23000: Loss = -12460.4638671875
4
Iteration 23100: Loss = -12460.4638671875
5
Iteration 23200: Loss = -12460.462890625
Iteration 23300: Loss = -12460.4619140625
Iteration 23400: Loss = -12460.4619140625
Iteration 23500: Loss = -12460.462890625
1
Iteration 23600: Loss = -12460.45703125
Iteration 23700: Loss = -12460.4560546875
Iteration 23800: Loss = -12460.45703125
1
Iteration 23900: Loss = -12460.4560546875
Iteration 24000: Loss = -12460.455078125
Iteration 24100: Loss = -12460.45703125
1
Iteration 24200: Loss = -12460.4560546875
2
Iteration 24300: Loss = -12460.455078125
Iteration 24400: Loss = -12460.4560546875
1
Iteration 24500: Loss = -12460.455078125
Iteration 24600: Loss = -12460.45703125
1
Iteration 24700: Loss = -12460.45703125
2
Iteration 24800: Loss = -12460.4560546875
3
Iteration 24900: Loss = -12460.4560546875
4
Iteration 25000: Loss = -12460.4560546875
5
Iteration 25100: Loss = -12460.4560546875
6
Iteration 25200: Loss = -12460.4560546875
7
Iteration 25300: Loss = -12460.455078125
Iteration 25400: Loss = -12460.4560546875
1
Iteration 25500: Loss = -12460.4560546875
2
Iteration 25600: Loss = -12460.455078125
Iteration 25700: Loss = -12460.455078125
Iteration 25800: Loss = -12460.455078125
Iteration 25900: Loss = -12460.455078125
Iteration 26000: Loss = -12460.453125
Iteration 26100: Loss = -12460.4541015625
1
Iteration 26200: Loss = -12460.4560546875
2
Iteration 26300: Loss = -12460.4541015625
3
Iteration 26400: Loss = -12460.455078125
4
Iteration 26500: Loss = -12460.4541015625
5
Iteration 26600: Loss = -12460.4541015625
6
Iteration 26700: Loss = -12460.455078125
7
Iteration 26800: Loss = -12460.4560546875
8
Iteration 26900: Loss = -12460.4541015625
9
Iteration 27000: Loss = -12460.4560546875
10
Iteration 27100: Loss = -12460.455078125
11
Iteration 27200: Loss = -12460.4541015625
12
Iteration 27300: Loss = -12460.4560546875
13
Iteration 27400: Loss = -12460.4541015625
14
Iteration 27500: Loss = -12460.4560546875
15
Stopping early at iteration 27500 due to no improvement.
pi: tensor([[1.0000e+00, 1.3715e-06],
        [4.2301e-05, 9.9996e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9881, 0.0119], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2012, 0.3518],
         [0.0080, 0.0012]],

        [[0.6717, 0.1992],
         [0.9536, 0.9908]],

        [[0.7335, 0.2636],
         [0.9709, 0.0132]],

        [[0.0084, 0.1855],
         [0.0220, 0.9825]],

        [[0.8940, 0.2707],
         [0.5149, 0.0560]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
Global Adjusted Rand Index: 0.0009711150057751206
Average Adjusted Rand Index: 0.0006872920502295121
[-0.0003093013866620859, 0.0009711150057751206] [-0.000597414405600841, 0.0006872920502295121] [12463.6630859375, 12460.4560546875]
-------------------------------------
This iteration is 87
True Objective function: Loss = -11795.018184010527
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30788.841796875
Iteration 100: Loss = -19673.091796875
Iteration 200: Loss = -14191.9775390625
Iteration 300: Loss = -12951.041015625
Iteration 400: Loss = -12627.5634765625
Iteration 500: Loss = -12501.96484375
Iteration 600: Loss = -12433.12890625
Iteration 700: Loss = -12400.6337890625
Iteration 800: Loss = -12376.689453125
Iteration 900: Loss = -12358.7822265625
Iteration 1000: Loss = -12342.798828125
Iteration 1100: Loss = -12330.384765625
Iteration 1200: Loss = -12320.7373046875
Iteration 1300: Loss = -12312.8720703125
Iteration 1400: Loss = -12301.1982421875
Iteration 1500: Loss = -12286.509765625
Iteration 1600: Loss = -12275.2802734375
Iteration 1700: Loss = -12261.908203125
Iteration 1800: Loss = -12254.783203125
Iteration 1900: Loss = -12248.5390625
Iteration 2000: Loss = -12238.7783203125
Iteration 2100: Loss = -12221.8955078125
Iteration 2200: Loss = -12203.1640625
Iteration 2300: Loss = -12185.1796875
Iteration 2400: Loss = -12178.7158203125
Iteration 2500: Loss = -12162.5146484375
Iteration 2600: Loss = -12154.0859375
Iteration 2700: Loss = -12151.7705078125
Iteration 2800: Loss = -12150.6279296875
Iteration 2900: Loss = -12149.86328125
Iteration 3000: Loss = -12149.287109375
Iteration 3100: Loss = -12148.8271484375
Iteration 3200: Loss = -12148.44140625
Iteration 3300: Loss = -12148.103515625
Iteration 3400: Loss = -12147.7705078125
Iteration 3500: Loss = -12144.892578125
Iteration 3600: Loss = -12140.15625
Iteration 3700: Loss = -12139.046875
Iteration 3800: Loss = -12134.7314453125
Iteration 3900: Loss = -12122.19921875
Iteration 4000: Loss = -12117.416015625
Iteration 4100: Loss = -12116.546875
Iteration 4200: Loss = -12115.9453125
Iteration 4300: Loss = -12114.46875
Iteration 4400: Loss = -12109.2109375
Iteration 4500: Loss = -12108.576171875
Iteration 4600: Loss = -12108.17578125
Iteration 4700: Loss = -12106.7333984375
Iteration 4800: Loss = -12103.029296875
Iteration 4900: Loss = -12102.8544921875
Iteration 5000: Loss = -12102.732421875
Iteration 5100: Loss = -12098.7451171875
Iteration 5200: Loss = -12098.52734375
Iteration 5300: Loss = -12098.44140625
Iteration 5400: Loss = -12098.376953125
Iteration 5500: Loss = -12098.3232421875
Iteration 5600: Loss = -12098.263671875
Iteration 5700: Loss = -12098.1044921875
Iteration 5800: Loss = -12091.6806640625
Iteration 5900: Loss = -12091.3720703125
Iteration 6000: Loss = -12091.2763671875
Iteration 6100: Loss = -12091.216796875
Iteration 6200: Loss = -12091.16796875
Iteration 6300: Loss = -12089.720703125
Iteration 6400: Loss = -12087.505859375
Iteration 6500: Loss = -12087.4453125
Iteration 6600: Loss = -12087.4091796875
Iteration 6700: Loss = -12086.0068359375
Iteration 6800: Loss = -12072.5947265625
Iteration 6900: Loss = -12070.25390625
Iteration 7000: Loss = -12070.1455078125
Iteration 7100: Loss = -12070.0888671875
Iteration 7200: Loss = -12070.05078125
Iteration 7300: Loss = -12070.0205078125
Iteration 7400: Loss = -12069.9970703125
Iteration 7500: Loss = -12069.970703125
Iteration 7600: Loss = -12069.90234375
Iteration 7700: Loss = -12059.1669921875
Iteration 7800: Loss = -12058.408203125
Iteration 7900: Loss = -12058.3427734375
Iteration 8000: Loss = -12058.2998046875
Iteration 8100: Loss = -12044.12109375
Iteration 8200: Loss = -12043.8955078125
Iteration 8300: Loss = -12043.8310546875
Iteration 8400: Loss = -12043.794921875
Iteration 8500: Loss = -12043.76953125
Iteration 8600: Loss = -12043.7509765625
Iteration 8700: Loss = -12043.736328125
Iteration 8800: Loss = -12043.7236328125
Iteration 8900: Loss = -12043.712890625
Iteration 9000: Loss = -12043.703125
Iteration 9100: Loss = -12043.6953125
Iteration 9200: Loss = -12043.6884765625
Iteration 9300: Loss = -12043.6806640625
Iteration 9400: Loss = -12043.6748046875
Iteration 9500: Loss = -12043.6689453125
Iteration 9600: Loss = -12043.6630859375
Iteration 9700: Loss = -12043.66015625
Iteration 9800: Loss = -12043.6552734375
Iteration 9900: Loss = -12043.6513671875
Iteration 10000: Loss = -12043.6484375
Iteration 10100: Loss = -12043.64453125
Iteration 10200: Loss = -12043.6416015625
Iteration 10300: Loss = -12043.6376953125
Iteration 10400: Loss = -12043.634765625
Iteration 10500: Loss = -12043.6328125
Iteration 10600: Loss = -12043.630859375
Iteration 10700: Loss = -12043.6279296875
Iteration 10800: Loss = -12043.6259765625
Iteration 10900: Loss = -12043.6240234375
Iteration 11000: Loss = -12043.62109375
Iteration 11100: Loss = -12043.6201171875
Iteration 11200: Loss = -12043.6181640625
Iteration 11300: Loss = -12043.6162109375
Iteration 11400: Loss = -12043.6142578125
Iteration 11500: Loss = -12043.6103515625
Iteration 11600: Loss = -12038.26171875
Iteration 11700: Loss = -12037.3681640625
Iteration 11800: Loss = -12037.3466796875
Iteration 11900: Loss = -12037.337890625
Iteration 12000: Loss = -12037.3330078125
Iteration 12100: Loss = -12037.330078125
Iteration 12200: Loss = -12037.328125
Iteration 12300: Loss = -12037.3251953125
Iteration 12400: Loss = -12037.32421875
Iteration 12500: Loss = -12037.322265625
Iteration 12600: Loss = -12037.3203125
Iteration 12700: Loss = -12037.318359375
Iteration 12800: Loss = -12037.3193359375
1
Iteration 12900: Loss = -12037.3173828125
Iteration 13000: Loss = -12037.31640625
Iteration 13100: Loss = -12037.314453125
Iteration 13200: Loss = -12037.314453125
Iteration 13300: Loss = -12037.3115234375
Iteration 13400: Loss = -12037.3017578125
Iteration 13500: Loss = -12037.3017578125
Iteration 13600: Loss = -12037.302734375
1
Iteration 13700: Loss = -12037.2998046875
Iteration 13800: Loss = -12037.2998046875
Iteration 13900: Loss = -12037.298828125
Iteration 14000: Loss = -12037.2998046875
1
Iteration 14100: Loss = -12037.298828125
Iteration 14200: Loss = -12037.2978515625
Iteration 14300: Loss = -12037.296875
Iteration 14400: Loss = -12037.28515625
Iteration 14500: Loss = -12034.0009765625
Iteration 14600: Loss = -12033.9931640625
Iteration 14700: Loss = -12033.990234375
Iteration 14800: Loss = -12033.990234375
Iteration 14900: Loss = -12033.990234375
Iteration 15000: Loss = -12033.990234375
Iteration 15100: Loss = -12033.9892578125
Iteration 15200: Loss = -12033.98828125
Iteration 15300: Loss = -12033.9892578125
1
Iteration 15400: Loss = -12033.98828125
Iteration 15500: Loss = -12033.98828125
Iteration 15600: Loss = -12033.98828125
Iteration 15700: Loss = -12033.986328125
Iteration 15800: Loss = -12033.9873046875
1
Iteration 15900: Loss = -12033.9873046875
2
Iteration 16000: Loss = -12033.98828125
3
Iteration 16100: Loss = -12033.98828125
4
Iteration 16200: Loss = -12033.98828125
5
Iteration 16300: Loss = -12033.98828125
6
Iteration 16400: Loss = -12033.9873046875
7
Iteration 16500: Loss = -12033.986328125
Iteration 16600: Loss = -12033.98828125
1
Iteration 16700: Loss = -12033.986328125
Iteration 16800: Loss = -12033.986328125
Iteration 16900: Loss = -12033.986328125
Iteration 17000: Loss = -12033.98828125
1
Iteration 17100: Loss = -12033.9853515625
Iteration 17200: Loss = -12033.9853515625
Iteration 17300: Loss = -12031.1611328125
Iteration 17400: Loss = -12029.4306640625
Iteration 17500: Loss = -12029.421875
Iteration 17600: Loss = -12029.4189453125
Iteration 17700: Loss = -12029.416015625
Iteration 17800: Loss = -12029.4150390625
Iteration 17900: Loss = -12029.4169921875
1
Iteration 18000: Loss = -12029.4140625
Iteration 18100: Loss = -12029.4150390625
1
Iteration 18200: Loss = -12029.4140625
Iteration 18300: Loss = -12029.4140625
Iteration 18400: Loss = -12029.4130859375
Iteration 18500: Loss = -12029.40625
Iteration 18600: Loss = -12021.447265625
Iteration 18700: Loss = -12021.34375
Iteration 18800: Loss = -12021.322265625
Iteration 18900: Loss = -12021.3125
Iteration 19000: Loss = -12021.306640625
Iteration 19100: Loss = -12021.302734375
Iteration 19200: Loss = -12021.2998046875
Iteration 19300: Loss = -12021.298828125
Iteration 19400: Loss = -12021.2978515625
Iteration 19500: Loss = -12021.296875
Iteration 19600: Loss = -12021.2861328125
Iteration 19700: Loss = -12019.166015625
Iteration 19800: Loss = -12019.1650390625
Iteration 19900: Loss = -12019.162109375
Iteration 20000: Loss = -12019.162109375
Iteration 20100: Loss = -12019.162109375
Iteration 20200: Loss = -12019.162109375
Iteration 20300: Loss = -12019.16015625
Iteration 20400: Loss = -12019.16015625
Iteration 20500: Loss = -12019.1611328125
1
Iteration 20600: Loss = -12019.1611328125
2
Iteration 20700: Loss = -12019.1611328125
3
Iteration 20800: Loss = -12019.1611328125
4
Iteration 20900: Loss = -12019.16015625
Iteration 21000: Loss = -12019.16015625
Iteration 21100: Loss = -12019.16015625
Iteration 21200: Loss = -12019.16015625
Iteration 21300: Loss = -12019.130859375
Iteration 21400: Loss = -12019.1318359375
1
Iteration 21500: Loss = -12019.130859375
Iteration 21600: Loss = -12019.1318359375
1
Iteration 21700: Loss = -12019.130859375
Iteration 21800: Loss = -12019.1318359375
1
Iteration 21900: Loss = -12019.130859375
Iteration 22000: Loss = -12019.1318359375
1
Iteration 22100: Loss = -12019.130859375
Iteration 22200: Loss = -12019.1328125
1
Iteration 22300: Loss = -12019.1318359375
2
Iteration 22400: Loss = -12019.1318359375
3
Iteration 22500: Loss = -12019.130859375
Iteration 22600: Loss = -12019.130859375
Iteration 22700: Loss = -12019.130859375
Iteration 22800: Loss = -12019.130859375
Iteration 22900: Loss = -12019.130859375
Iteration 23000: Loss = -12019.130859375
Iteration 23100: Loss = -12019.1318359375
1
Iteration 23200: Loss = -12019.130859375
Iteration 23300: Loss = -12019.130859375
Iteration 23400: Loss = -12019.1318359375
1
Iteration 23500: Loss = -12019.130859375
Iteration 23600: Loss = -12019.1318359375
1
Iteration 23700: Loss = -12019.130859375
Iteration 23800: Loss = -12019.130859375
Iteration 23900: Loss = -12019.130859375
Iteration 24000: Loss = -12019.130859375
Iteration 24100: Loss = -12019.130859375
Iteration 24200: Loss = -12019.1298828125
Iteration 24300: Loss = -12019.130859375
1
Iteration 24400: Loss = -12018.7626953125
Iteration 24500: Loss = -12018.2197265625
Iteration 24600: Loss = -12014.6474609375
Iteration 24700: Loss = -12014.6220703125
Iteration 24800: Loss = -12014.603515625
Iteration 24900: Loss = -12014.6025390625
Iteration 25000: Loss = -12014.6015625
Iteration 25100: Loss = -12014.603515625
1
Iteration 25200: Loss = -12014.6044921875
2
Iteration 25300: Loss = -12014.51171875
Iteration 25400: Loss = -12012.953125
Iteration 25500: Loss = -12012.8173828125
Iteration 25600: Loss = -12012.4736328125
Iteration 25700: Loss = -12009.3310546875
Iteration 25800: Loss = -12003.6220703125
Iteration 25900: Loss = -11993.8681640625
Iteration 26000: Loss = -11989.6767578125
Iteration 26100: Loss = -11978.138671875
Iteration 26200: Loss = -11966.5029296875
Iteration 26300: Loss = -11956.728515625
Iteration 26400: Loss = -11942.6748046875
Iteration 26500: Loss = -11934.5986328125
Iteration 26600: Loss = -11930.7099609375
Iteration 26700: Loss = -11930.41796875
Iteration 26800: Loss = -11930.3583984375
Iteration 26900: Loss = -11930.3349609375
Iteration 27000: Loss = -11930.3212890625
Iteration 27100: Loss = -11930.3125
Iteration 27200: Loss = -11930.3037109375
Iteration 27300: Loss = -11928.6083984375
Iteration 27400: Loss = -11917.9111328125
Iteration 27500: Loss = -11917.80859375
Iteration 27600: Loss = -11917.7734375
Iteration 27700: Loss = -11910.841796875
Iteration 27800: Loss = -11896.451171875
Iteration 27900: Loss = -11896.259765625
Iteration 28000: Loss = -11895.9248046875
Iteration 28100: Loss = -11885.5849609375
Iteration 28200: Loss = -11882.00390625
Iteration 28300: Loss = -11869.0966796875
Iteration 28400: Loss = -11863.296875
Iteration 28500: Loss = -11863.1884765625
Iteration 28600: Loss = -11863.1416015625
Iteration 28700: Loss = -11863.11328125
Iteration 28800: Loss = -11863.0927734375
Iteration 28900: Loss = -11863.0791015625
Iteration 29000: Loss = -11863.068359375
Iteration 29100: Loss = -11863.060546875
Iteration 29200: Loss = -11863.0517578125
Iteration 29300: Loss = -11863.044921875
Iteration 29400: Loss = -11863.0400390625
Iteration 29500: Loss = -11863.0361328125
Iteration 29600: Loss = -11863.0322265625
Iteration 29700: Loss = -11863.0283203125
Iteration 29800: Loss = -11863.0263671875
Iteration 29900: Loss = -11863.0244140625
pi: tensor([[0.3045, 0.6955],
        [0.7672, 0.2328]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6019, 0.3981], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2814, 0.0992],
         [0.5127, 0.2973]],

        [[0.2288, 0.0893],
         [0.7858, 0.9824]],

        [[0.1076, 0.1096],
         [0.0683, 0.5713]],

        [[0.3959, 0.1006],
         [0.2398, 0.4603]],

        [[0.8638, 0.0963],
         [0.0653, 0.3591]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 92
Adjusted Rand Index: 0.7004440059200789
time is 3
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.027572095602895648
Average Adjusted Rand Index: 0.9320875785913257
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29002.072265625
Iteration 100: Loss = -19809.404296875
Iteration 200: Loss = -13977.5009765625
Iteration 300: Loss = -12825.3291015625
Iteration 400: Loss = -12621.8994140625
Iteration 500: Loss = -12547.4306640625
Iteration 600: Loss = -12507.0732421875
Iteration 700: Loss = -12480.98828125
Iteration 800: Loss = -12461.8740234375
Iteration 900: Loss = -12445.73828125
Iteration 1000: Loss = -12434.29296875
Iteration 1100: Loss = -12426.1748046875
Iteration 1200: Loss = -12415.966796875
Iteration 1300: Loss = -12407.33203125
Iteration 1400: Loss = -12402.494140625
Iteration 1500: Loss = -12395.9208984375
Iteration 1600: Loss = -12390.6337890625
Iteration 1700: Loss = -12386.3818359375
Iteration 1800: Loss = -12381.1708984375
Iteration 1900: Loss = -12375.4931640625
Iteration 2000: Loss = -12372.8466796875
Iteration 2100: Loss = -12369.49609375
Iteration 2200: Loss = -12367.2802734375
Iteration 2300: Loss = -12364.861328125
Iteration 2400: Loss = -12362.8388671875
Iteration 2500: Loss = -12361.2822265625
Iteration 2600: Loss = -12359.15234375
Iteration 2700: Loss = -12356.318359375
Iteration 2800: Loss = -12354.7314453125
Iteration 2900: Loss = -12352.1083984375
Iteration 3000: Loss = -12351.2060546875
Iteration 3100: Loss = -12350.41796875
Iteration 3200: Loss = -12349.5791015625
Iteration 3300: Loss = -12348.3125
Iteration 3400: Loss = -12345.197265625
Iteration 3500: Loss = -12342.080078125
Iteration 3600: Loss = -12338.4208984375
Iteration 3700: Loss = -12336.9755859375
Iteration 3800: Loss = -12335.953125
Iteration 3900: Loss = -12334.90625
Iteration 4000: Loss = -12333.07421875
Iteration 4100: Loss = -12331.845703125
Iteration 4200: Loss = -12330.8310546875
Iteration 4300: Loss = -12330.2734375
Iteration 4400: Loss = -12327.5107421875
Iteration 4500: Loss = -12326.7236328125
Iteration 4600: Loss = -12326.39453125
Iteration 4700: Loss = -12326.166015625
Iteration 4800: Loss = -12325.984375
Iteration 4900: Loss = -12325.830078125
Iteration 5000: Loss = -12325.697265625
Iteration 5100: Loss = -12325.58203125
Iteration 5200: Loss = -12325.4765625
Iteration 5300: Loss = -12325.3818359375
Iteration 5400: Loss = -12325.2900390625
Iteration 5500: Loss = -12325.1748046875
Iteration 5600: Loss = -12325.0830078125
Iteration 5700: Loss = -12325.0029296875
Iteration 5800: Loss = -12324.9296875
Iteration 5900: Loss = -12324.8505859375
Iteration 6000: Loss = -12324.412109375
Iteration 6100: Loss = -12321.2158203125
Iteration 6200: Loss = -12320.9453125
Iteration 6300: Loss = -12320.826171875
Iteration 6400: Loss = -12320.7421875
Iteration 6500: Loss = -12320.6748046875
Iteration 6600: Loss = -12320.6162109375
Iteration 6700: Loss = -12320.5654296875
Iteration 6800: Loss = -12320.521484375
Iteration 6900: Loss = -12320.4775390625
Iteration 7000: Loss = -12320.439453125
Iteration 7100: Loss = -12320.4013671875
Iteration 7200: Loss = -12320.3662109375
Iteration 7300: Loss = -12320.2763671875
Iteration 7400: Loss = -12317.3310546875
Iteration 7500: Loss = -12317.16796875
Iteration 7600: Loss = -12317.109375
Iteration 7700: Loss = -12317.072265625
Iteration 7800: Loss = -12317.0439453125
Iteration 7900: Loss = -12317.0185546875
Iteration 8000: Loss = -12316.99609375
Iteration 8100: Loss = -12316.9765625
Iteration 8200: Loss = -12316.95703125
Iteration 8300: Loss = -12316.9384765625
Iteration 8400: Loss = -12316.923828125
Iteration 8500: Loss = -12316.9013671875
Iteration 8600: Loss = -12316.8828125
Iteration 8700: Loss = -12316.8701171875
Iteration 8800: Loss = -12316.8564453125
Iteration 8900: Loss = -12316.8447265625
Iteration 9000: Loss = -12316.833984375
Iteration 9100: Loss = -12316.822265625
Iteration 9200: Loss = -12316.810546875
Iteration 9300: Loss = -12316.796875
Iteration 9400: Loss = -12316.783203125
Iteration 9500: Loss = -12316.7734375
Iteration 9600: Loss = -12316.7568359375
Iteration 9700: Loss = -12316.7490234375
Iteration 9800: Loss = -12316.7431640625
Iteration 9900: Loss = -12316.7353515625
Iteration 10000: Loss = -12316.728515625
Iteration 10100: Loss = -12316.7236328125
Iteration 10200: Loss = -12316.716796875
Iteration 10300: Loss = -12316.7119140625
Iteration 10400: Loss = -12316.7060546875
Iteration 10500: Loss = -12316.7001953125
Iteration 10600: Loss = -12316.6962890625
Iteration 10700: Loss = -12316.6904296875
Iteration 10800: Loss = -12316.6865234375
Iteration 10900: Loss = -12316.6826171875
Iteration 11000: Loss = -12316.6796875
Iteration 11100: Loss = -12316.6748046875
Iteration 11200: Loss = -12316.6708984375
Iteration 11300: Loss = -12316.6689453125
Iteration 11400: Loss = -12316.6650390625
Iteration 11500: Loss = -12316.6630859375
Iteration 11600: Loss = -12316.6591796875
Iteration 11700: Loss = -12316.65625
Iteration 11800: Loss = -12316.6552734375
Iteration 11900: Loss = -12316.65234375
Iteration 12000: Loss = -12316.6474609375
Iteration 12100: Loss = -12316.6455078125
Iteration 12200: Loss = -12316.6435546875
Iteration 12300: Loss = -12316.6396484375
Iteration 12400: Loss = -12316.6357421875
Iteration 12500: Loss = -12316.6298828125
Iteration 12600: Loss = -12316.6240234375
Iteration 12700: Loss = -12316.6201171875
Iteration 12800: Loss = -12316.62109375
1
Iteration 12900: Loss = -12316.619140625
Iteration 13000: Loss = -12316.61328125
Iteration 13100: Loss = -12316.61328125
Iteration 13200: Loss = -12316.6123046875
Iteration 13300: Loss = -12316.611328125
Iteration 13400: Loss = -12316.609375
Iteration 13500: Loss = -12316.60546875
Iteration 13600: Loss = -12316.5810546875
Iteration 13700: Loss = -12315.8154296875
Iteration 13800: Loss = -12312.4609375
Iteration 13900: Loss = -12312.4482421875
Iteration 14000: Loss = -12312.443359375
Iteration 14100: Loss = -12312.4404296875
Iteration 14200: Loss = -12312.44140625
1
Iteration 14300: Loss = -12312.439453125
Iteration 14400: Loss = -12312.4072265625
Iteration 14500: Loss = -12307.1533203125
Iteration 14600: Loss = -12307.03515625
Iteration 14700: Loss = -12306.9951171875
Iteration 14800: Loss = -12306.3720703125
Iteration 14900: Loss = -12306.3408203125
Iteration 15000: Loss = -12306.333984375
Iteration 15100: Loss = -12306.3291015625
Iteration 15200: Loss = -12306.3251953125
Iteration 15300: Loss = -12306.3251953125
Iteration 15400: Loss = -12306.322265625
Iteration 15500: Loss = -12306.3193359375
Iteration 15600: Loss = -12306.3193359375
Iteration 15700: Loss = -12306.3173828125
Iteration 15800: Loss = -12306.3173828125
Iteration 15900: Loss = -12306.3173828125
Iteration 16000: Loss = -12306.318359375
1
Iteration 16100: Loss = -12306.314453125
Iteration 16200: Loss = -12306.314453125
Iteration 16300: Loss = -12306.3134765625
Iteration 16400: Loss = -12306.3134765625
Iteration 16500: Loss = -12306.3125
Iteration 16600: Loss = -12306.3134765625
1
Iteration 16700: Loss = -12306.3134765625
2
Iteration 16800: Loss = -12306.3125
Iteration 16900: Loss = -12306.3115234375
Iteration 17000: Loss = -12306.3125
1
Iteration 17100: Loss = -12306.310546875
Iteration 17200: Loss = -12306.3115234375
1
Iteration 17300: Loss = -12306.3125
2
Iteration 17400: Loss = -12306.3115234375
3
Iteration 17500: Loss = -12306.3115234375
4
Iteration 17600: Loss = -12306.3125
5
Iteration 17700: Loss = -12306.310546875
Iteration 17800: Loss = -12306.310546875
Iteration 17900: Loss = -12306.3115234375
1
Iteration 18000: Loss = -12306.3115234375
2
Iteration 18100: Loss = -12306.3095703125
Iteration 18200: Loss = -12306.30859375
Iteration 18300: Loss = -12306.3076171875
Iteration 18400: Loss = -12306.3076171875
Iteration 18500: Loss = -12306.291015625
Iteration 18600: Loss = -12306.251953125
Iteration 18700: Loss = -12306.1962890625
Iteration 18800: Loss = -12306.166015625
Iteration 18900: Loss = -12306.1513671875
Iteration 19000: Loss = -12306.1416015625
Iteration 19100: Loss = -12306.1240234375
Iteration 19200: Loss = -12306.107421875
Iteration 19300: Loss = -12306.1044921875
Iteration 19400: Loss = -12306.09375
Iteration 19500: Loss = -12306.0888671875
Iteration 19600: Loss = -12306.0810546875
Iteration 19700: Loss = -12306.07421875
Iteration 19800: Loss = -12305.904296875
Iteration 19900: Loss = -12305.8916015625
Iteration 20000: Loss = -12305.8623046875
Iteration 20100: Loss = -12305.857421875
Iteration 20200: Loss = -12305.8193359375
Iteration 20300: Loss = -12305.7666015625
Iteration 20400: Loss = -12305.7626953125
Iteration 20500: Loss = -12305.7626953125
Iteration 20600: Loss = -12305.7578125
Iteration 20700: Loss = -12305.7587890625
1
Iteration 20800: Loss = -12305.7568359375
Iteration 20900: Loss = -12305.755859375
Iteration 21000: Loss = -12305.7578125
1
Iteration 21100: Loss = -12305.7578125
2
Iteration 21200: Loss = -12305.73046875
Iteration 21300: Loss = -12305.73046875
Iteration 21400: Loss = -12305.724609375
Iteration 21500: Loss = -12305.724609375
Iteration 21600: Loss = -12305.72265625
Iteration 21700: Loss = -12305.521484375
Iteration 21800: Loss = -12305.521484375
Iteration 21900: Loss = -12305.521484375
Iteration 22000: Loss = -12305.498046875
Iteration 22100: Loss = -12305.498046875
Iteration 22200: Loss = -12305.498046875
Iteration 22300: Loss = -12305.4970703125
Iteration 22400: Loss = -12305.498046875
1
Iteration 22500: Loss = -12305.498046875
2
Iteration 22600: Loss = -12305.498046875
3
Iteration 22700: Loss = -12305.498046875
4
Iteration 22800: Loss = -12305.4970703125
Iteration 22900: Loss = -12305.498046875
1
Iteration 23000: Loss = -12305.4970703125
Iteration 23100: Loss = -12305.498046875
1
Iteration 23200: Loss = -12305.494140625
Iteration 23300: Loss = -12305.49609375
1
Iteration 23400: Loss = -12305.49609375
2
Iteration 23500: Loss = -12305.494140625
Iteration 23600: Loss = -12305.49609375
1
Iteration 23700: Loss = -12305.4951171875
2
Iteration 23800: Loss = -12305.48828125
Iteration 23900: Loss = -12305.404296875
Iteration 24000: Loss = -12305.4033203125
Iteration 24100: Loss = -12305.4072265625
1
Iteration 24200: Loss = -12305.404296875
2
Iteration 24300: Loss = -12305.4052734375
3
Iteration 24400: Loss = -12305.2763671875
Iteration 24500: Loss = -12305.0185546875
Iteration 24600: Loss = -12304.9384765625
Iteration 24700: Loss = -12304.9326171875
Iteration 24800: Loss = -12304.3779296875
Iteration 24900: Loss = -12304.37890625
1
Iteration 25000: Loss = -12304.37109375
Iteration 25100: Loss = -12304.2763671875
Iteration 25200: Loss = -12304.27734375
1
Iteration 25300: Loss = -12304.2509765625
Iteration 25400: Loss = -12304.2001953125
Iteration 25500: Loss = -12304.1884765625
Iteration 25600: Loss = -12304.1845703125
Iteration 25700: Loss = -12304.1689453125
Iteration 25800: Loss = -12304.16796875
Iteration 25900: Loss = -12304.15234375
Iteration 26000: Loss = -12304.150390625
Iteration 26100: Loss = -12304.1484375
Iteration 26200: Loss = -12304.123046875
Iteration 26300: Loss = -12304.1181640625
Iteration 26400: Loss = -12304.1181640625
Iteration 26500: Loss = -12304.1201171875
1
Iteration 26600: Loss = -12304.119140625
2
Iteration 26700: Loss = -12304.1181640625
Iteration 26800: Loss = -12304.1181640625
Iteration 26900: Loss = -12304.1181640625
Iteration 27000: Loss = -12304.115234375
Iteration 27100: Loss = -12304.1123046875
Iteration 27200: Loss = -12304.1005859375
Iteration 27300: Loss = -12304.095703125
Iteration 27400: Loss = -12303.9853515625
Iteration 27500: Loss = -12303.9814453125
Iteration 27600: Loss = -12303.9814453125
Iteration 27700: Loss = -12303.9775390625
Iteration 27800: Loss = -12303.978515625
1
Iteration 27900: Loss = -12303.9775390625
Iteration 28000: Loss = -12303.978515625
1
Iteration 28100: Loss = -12303.9775390625
Iteration 28200: Loss = -12303.978515625
1
Iteration 28300: Loss = -12303.9775390625
Iteration 28400: Loss = -12303.978515625
1
Iteration 28500: Loss = -12303.9775390625
Iteration 28600: Loss = -12303.9775390625
Iteration 28700: Loss = -12303.9794921875
1
Iteration 28800: Loss = -12303.9765625
Iteration 28900: Loss = -12303.9794921875
1
Iteration 29000: Loss = -12303.9765625
Iteration 29100: Loss = -12303.9775390625
1
Iteration 29200: Loss = -12303.9765625
Iteration 29300: Loss = -12303.978515625
1
Iteration 29400: Loss = -12303.9755859375
Iteration 29500: Loss = -12303.9775390625
1
Iteration 29600: Loss = -12303.9775390625
2
Iteration 29700: Loss = -12303.9521484375
Iteration 29800: Loss = -12303.94921875
Iteration 29900: Loss = -12303.9501953125
1
pi: tensor([[8.2454e-07, 1.0000e+00],
        [2.1062e-02, 9.7894e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1400, 0.8600], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2804, 0.2305],
         [0.9930, 0.1939]],

        [[0.1665, 0.1524],
         [0.0330, 0.0091]],

        [[0.4725, 0.3074],
         [0.8233, 0.8646]],

        [[0.9157, 0.2978],
         [0.1349, 0.2134]],

        [[0.0169, 0.2185],
         [0.3636, 0.9030]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: -0.017511416386760764
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 40
Adjusted Rand Index: -0.014617321874876162
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0054269723195367265
Average Adjusted Rand Index: -0.006425747652327385
[0.027572095602895648, -0.0054269723195367265] [0.9320875785913257, -0.006425747652327385] [11863.0224609375, 12303.951171875]
-------------------------------------
This iteration is 88
True Objective function: Loss = -11952.795589332021
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22181.01953125
Iteration 100: Loss = -16670.974609375
Iteration 200: Loss = -13509.5361328125
Iteration 300: Loss = -12803.1435546875
Iteration 400: Loss = -12658.6806640625
Iteration 500: Loss = -12599.0205078125
Iteration 600: Loss = -12566.2392578125
Iteration 700: Loss = -12548.7080078125
Iteration 800: Loss = -12537.5078125
Iteration 900: Loss = -12529.78125
Iteration 1000: Loss = -12524.171875
Iteration 1100: Loss = -12519.94140625
Iteration 1200: Loss = -12516.666015625
Iteration 1300: Loss = -12514.068359375
Iteration 1400: Loss = -12511.9677734375
Iteration 1500: Loss = -12510.2451171875
Iteration 1600: Loss = -12508.8095703125
Iteration 1700: Loss = -12507.599609375
Iteration 1800: Loss = -12506.572265625
Iteration 1900: Loss = -12505.6904296875
Iteration 2000: Loss = -12504.927734375
Iteration 2100: Loss = -12504.26171875
Iteration 2200: Loss = -12503.677734375
Iteration 2300: Loss = -12503.1650390625
Iteration 2400: Loss = -12502.708984375
Iteration 2500: Loss = -12502.30078125
Iteration 2600: Loss = -12501.9384765625
Iteration 2700: Loss = -12501.6123046875
Iteration 2800: Loss = -12501.3203125
Iteration 2900: Loss = -12501.0537109375
Iteration 3000: Loss = -12500.814453125
Iteration 3100: Loss = -12500.5947265625
Iteration 3200: Loss = -12500.39453125
Iteration 3300: Loss = -12500.2119140625
Iteration 3400: Loss = -12500.04296875
Iteration 3500: Loss = -12499.888671875
Iteration 3600: Loss = -12499.744140625
Iteration 3700: Loss = -12499.609375
Iteration 3800: Loss = -12499.4833984375
Iteration 3900: Loss = -12499.3671875
Iteration 4000: Loss = -12499.2529296875
Iteration 4100: Loss = -12499.1474609375
Iteration 4200: Loss = -12499.0439453125
Iteration 4300: Loss = -12498.9482421875
Iteration 4400: Loss = -12498.8525390625
Iteration 4500: Loss = -12498.7626953125
Iteration 4600: Loss = -12498.6796875
Iteration 4700: Loss = -12498.6005859375
Iteration 4800: Loss = -12498.5263671875
Iteration 4900: Loss = -12498.4580078125
Iteration 5000: Loss = -12498.3916015625
Iteration 5100: Loss = -12498.3330078125
Iteration 5200: Loss = -12498.27734375
Iteration 5300: Loss = -12498.2236328125
Iteration 5400: Loss = -12498.173828125
Iteration 5500: Loss = -12498.1259765625
Iteration 5600: Loss = -12498.072265625
Iteration 5700: Loss = -12498.0283203125
Iteration 5800: Loss = -12497.9833984375
Iteration 5900: Loss = -12497.9384765625
Iteration 6000: Loss = -12497.896484375
Iteration 6100: Loss = -12497.8525390625
Iteration 6200: Loss = -12497.8095703125
Iteration 6300: Loss = -12497.767578125
Iteration 6400: Loss = -12497.724609375
Iteration 6500: Loss = -12497.6826171875
Iteration 6600: Loss = -12497.638671875
Iteration 6700: Loss = -12497.595703125
Iteration 6800: Loss = -12497.5537109375
Iteration 6900: Loss = -12497.5146484375
Iteration 7000: Loss = -12497.474609375
Iteration 7100: Loss = -12497.4384765625
Iteration 7200: Loss = -12497.404296875
Iteration 7300: Loss = -12497.3720703125
Iteration 7400: Loss = -12497.3427734375
Iteration 7500: Loss = -12497.3134765625
Iteration 7600: Loss = -12497.2890625
Iteration 7700: Loss = -12497.265625
Iteration 7800: Loss = -12497.2451171875
Iteration 7900: Loss = -12497.2265625
Iteration 8000: Loss = -12497.2080078125
Iteration 8100: Loss = -12497.19140625
Iteration 8200: Loss = -12497.173828125
Iteration 8300: Loss = -12497.16015625
Iteration 8400: Loss = -12497.1494140625
Iteration 8500: Loss = -12497.1376953125
Iteration 8600: Loss = -12497.1259765625
Iteration 8700: Loss = -12497.115234375
Iteration 8800: Loss = -12497.1044921875
Iteration 8900: Loss = -12497.095703125
Iteration 9000: Loss = -12497.0888671875
Iteration 9100: Loss = -12497.0810546875
Iteration 9200: Loss = -12497.07421875
Iteration 9300: Loss = -12497.06640625
Iteration 9400: Loss = -12497.060546875
Iteration 9500: Loss = -12497.0546875
Iteration 9600: Loss = -12497.0498046875
Iteration 9700: Loss = -12497.0439453125
Iteration 9800: Loss = -12497.0390625
Iteration 9900: Loss = -12497.03515625
Iteration 10000: Loss = -12497.0322265625
Iteration 10100: Loss = -12497.0263671875
Iteration 10200: Loss = -12497.025390625
Iteration 10300: Loss = -12497.0205078125
Iteration 10400: Loss = -12497.0166015625
Iteration 10500: Loss = -12497.013671875
Iteration 10600: Loss = -12497.01171875
Iteration 10700: Loss = -12497.013671875
1
Iteration 10800: Loss = -12497.0068359375
Iteration 10900: Loss = -12497.0048828125
Iteration 11000: Loss = -12497.0009765625
Iteration 11100: Loss = -12497.0
Iteration 11200: Loss = -12496.998046875
Iteration 11300: Loss = -12496.99609375
Iteration 11400: Loss = -12496.9951171875
Iteration 11500: Loss = -12496.9931640625
Iteration 11600: Loss = -12496.9921875
Iteration 11700: Loss = -12496.9892578125
Iteration 11800: Loss = -12496.9873046875
Iteration 11900: Loss = -12496.986328125
Iteration 12000: Loss = -12496.986328125
Iteration 12100: Loss = -12496.984375
Iteration 12200: Loss = -12496.982421875
Iteration 12300: Loss = -12496.982421875
Iteration 12400: Loss = -12496.98046875
Iteration 12500: Loss = -12496.98046875
Iteration 12600: Loss = -12496.98046875
Iteration 12700: Loss = -12496.9775390625
Iteration 12800: Loss = -12496.9775390625
Iteration 12900: Loss = -12496.9765625
Iteration 13000: Loss = -12496.9765625
Iteration 13100: Loss = -12496.9755859375
Iteration 13200: Loss = -12496.9755859375
Iteration 13300: Loss = -12496.9755859375
Iteration 13400: Loss = -12496.9736328125
Iteration 13500: Loss = -12496.9736328125
Iteration 13600: Loss = -12496.97265625
Iteration 13700: Loss = -12496.97265625
Iteration 13800: Loss = -12496.97265625
Iteration 13900: Loss = -12496.9716796875
Iteration 14000: Loss = -12496.9716796875
Iteration 14100: Loss = -12496.97265625
1
Iteration 14200: Loss = -12496.970703125
Iteration 14300: Loss = -12496.96875
Iteration 14400: Loss = -12496.96875
Iteration 14500: Loss = -12496.9677734375
Iteration 14600: Loss = -12496.9677734375
Iteration 14700: Loss = -12496.966796875
Iteration 14800: Loss = -12496.966796875
Iteration 14900: Loss = -12496.966796875
Iteration 15000: Loss = -12496.9638671875
Iteration 15100: Loss = -12496.962890625
Iteration 15200: Loss = -12496.9482421875
Iteration 15300: Loss = -12496.4091796875
Iteration 15400: Loss = -12496.40625
Iteration 15500: Loss = -12496.404296875
Iteration 15600: Loss = -12496.4033203125
Iteration 15700: Loss = -12496.404296875
1
Iteration 15800: Loss = -12496.4013671875
Iteration 15900: Loss = -12496.4013671875
Iteration 16000: Loss = -12496.4033203125
1
Iteration 16100: Loss = -12496.4013671875
Iteration 16200: Loss = -12496.4013671875
Iteration 16300: Loss = -12496.40234375
1
Iteration 16400: Loss = -12496.400390625
Iteration 16500: Loss = -12496.400390625
Iteration 16600: Loss = -12496.4013671875
1
Iteration 16700: Loss = -12496.4013671875
2
Iteration 16800: Loss = -12496.4013671875
3
Iteration 16900: Loss = -12496.4013671875
4
Iteration 17000: Loss = -12496.3994140625
Iteration 17100: Loss = -12496.4013671875
1
Iteration 17200: Loss = -12496.400390625
2
Iteration 17300: Loss = -12496.3994140625
Iteration 17400: Loss = -12496.3984375
Iteration 17500: Loss = -12496.3994140625
1
Iteration 17600: Loss = -12496.396484375
Iteration 17700: Loss = -12496.396484375
Iteration 17800: Loss = -12496.3974609375
1
Iteration 17900: Loss = -12496.396484375
Iteration 18000: Loss = -12496.3955078125
Iteration 18100: Loss = -12496.396484375
1
Iteration 18200: Loss = -12496.3955078125
Iteration 18300: Loss = -12496.3955078125
Iteration 18400: Loss = -12496.3955078125
Iteration 18500: Loss = -12496.39453125
Iteration 18600: Loss = -12496.39453125
Iteration 18700: Loss = -12496.3955078125
1
Iteration 18800: Loss = -12496.3955078125
2
Iteration 18900: Loss = -12496.3955078125
3
Iteration 19000: Loss = -12496.3955078125
4
Iteration 19100: Loss = -12496.39453125
Iteration 19200: Loss = -12496.3935546875
Iteration 19300: Loss = -12496.3955078125
1
Iteration 19400: Loss = -12496.396484375
2
Iteration 19500: Loss = -12496.3955078125
3
Iteration 19600: Loss = -12496.3955078125
4
Iteration 19700: Loss = -12496.39453125
5
Iteration 19800: Loss = -12496.3935546875
Iteration 19900: Loss = -12496.39453125
1
Iteration 20000: Loss = -12496.396484375
2
Iteration 20100: Loss = -12496.3955078125
3
Iteration 20200: Loss = -12496.3955078125
4
Iteration 20300: Loss = -12496.39453125
5
Iteration 20400: Loss = -12496.396484375
6
Iteration 20500: Loss = -12496.39453125
7
Iteration 20600: Loss = -12496.39453125
8
Iteration 20700: Loss = -12496.3935546875
Iteration 20800: Loss = -12496.39453125
1
Iteration 20900: Loss = -12496.39453125
2
Iteration 21000: Loss = -12496.3955078125
3
Iteration 21100: Loss = -12496.39453125
4
Iteration 21200: Loss = -12496.3955078125
5
Iteration 21300: Loss = -12496.39453125
6
Iteration 21400: Loss = -12496.39453125
7
Iteration 21500: Loss = -12496.39453125
8
Iteration 21600: Loss = -12496.3955078125
9
Iteration 21700: Loss = -12496.39453125
10
Iteration 21800: Loss = -12496.3955078125
11
Iteration 21900: Loss = -12496.39453125
12
Iteration 22000: Loss = -12496.3955078125
13
Iteration 22100: Loss = -12496.39453125
14
Iteration 22200: Loss = -12496.3955078125
15
Stopping early at iteration 22200 due to no improvement.
pi: tensor([[9.9999e-01, 1.0880e-05],
        [5.4320e-03, 9.9457e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0084, 0.9916], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0328, 0.3225],
         [0.4894, 0.2036]],

        [[0.2118, 0.2127],
         [0.0334, 0.9864]],

        [[0.7374, 0.2585],
         [0.0674, 0.4761]],

        [[0.2348, 0.1689],
         [0.1449, 0.4949]],

        [[0.9884, 0.1362],
         [0.9521, 0.9373]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.00035341041046094813
Global Adjusted Rand Index: 0.0005149712950272512
Average Adjusted Rand Index: 0.0009244163070964248
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35426.2421875
Iteration 100: Loss = -23278.208984375
Iteration 200: Loss = -15629.2490234375
Iteration 300: Loss = -13735.41015625
Iteration 400: Loss = -13089.966796875
Iteration 500: Loss = -12808.6005859375
Iteration 600: Loss = -12684.9140625
Iteration 700: Loss = -12629.291015625
Iteration 800: Loss = -12600.8896484375
Iteration 900: Loss = -12585.7529296875
Iteration 1000: Loss = -12575.8974609375
Iteration 1100: Loss = -12568.7060546875
Iteration 1200: Loss = -12562.142578125
Iteration 1300: Loss = -12557.662109375
Iteration 1400: Loss = -12553.73046875
Iteration 1500: Loss = -12550.1474609375
Iteration 1600: Loss = -12546.126953125
Iteration 1700: Loss = -12542.1494140625
Iteration 1800: Loss = -12539.1728515625
Iteration 1900: Loss = -12536.2978515625
Iteration 2000: Loss = -12534.0322265625
Iteration 2100: Loss = -12531.7255859375
Iteration 2200: Loss = -12528.8916015625
Iteration 2300: Loss = -12526.4833984375
Iteration 2400: Loss = -12523.7626953125
Iteration 2500: Loss = -12521.8984375
Iteration 2600: Loss = -12519.6513671875
Iteration 2700: Loss = -12517.5546875
Iteration 2800: Loss = -12514.744140625
Iteration 2900: Loss = -12513.337890625
Iteration 3000: Loss = -12512.0322265625
Iteration 3100: Loss = -12510.7666015625
Iteration 3200: Loss = -12509.4306640625
Iteration 3300: Loss = -12508.09375
Iteration 3400: Loss = -12506.818359375
Iteration 3500: Loss = -12505.646484375
Iteration 3600: Loss = -12504.7392578125
Iteration 3700: Loss = -12504.076171875
Iteration 3800: Loss = -12503.5498046875
Iteration 3900: Loss = -12503.009765625
Iteration 4000: Loss = -12501.8759765625
Iteration 4100: Loss = -12500.9033203125
Iteration 4200: Loss = -12500.4912109375
Iteration 4300: Loss = -12500.2041015625
Iteration 4400: Loss = -12499.97265625
Iteration 4500: Loss = -12499.7763671875
Iteration 4600: Loss = -12499.603515625
Iteration 4700: Loss = -12499.4521484375
Iteration 4800: Loss = -12499.3154296875
Iteration 4900: Loss = -12499.19140625
Iteration 5000: Loss = -12499.078125
Iteration 5100: Loss = -12498.9775390625
Iteration 5200: Loss = -12498.8837890625
Iteration 5300: Loss = -12498.794921875
Iteration 5400: Loss = -12498.7158203125
Iteration 5500: Loss = -12498.6416015625
Iteration 5600: Loss = -12498.5712890625
Iteration 5700: Loss = -12498.5068359375
Iteration 5800: Loss = -12498.44921875
Iteration 5900: Loss = -12498.3935546875
Iteration 6000: Loss = -12498.3408203125
Iteration 6100: Loss = -12498.2919921875
Iteration 6200: Loss = -12498.2451171875
Iteration 6300: Loss = -12498.2021484375
Iteration 6400: Loss = -12498.1611328125
Iteration 6500: Loss = -12498.123046875
Iteration 6600: Loss = -12498.0869140625
Iteration 6700: Loss = -12498.0546875
Iteration 6800: Loss = -12498.021484375
Iteration 6900: Loss = -12497.9931640625
Iteration 7000: Loss = -12497.96484375
Iteration 7100: Loss = -12497.939453125
Iteration 7200: Loss = -12497.9140625
Iteration 7300: Loss = -12497.890625
Iteration 7400: Loss = -12497.8671875
Iteration 7500: Loss = -12497.8466796875
Iteration 7600: Loss = -12497.826171875
Iteration 7700: Loss = -12497.806640625
Iteration 7800: Loss = -12497.7900390625
Iteration 7900: Loss = -12497.771484375
Iteration 8000: Loss = -12497.755859375
Iteration 8100: Loss = -12497.7412109375
Iteration 8200: Loss = -12497.7265625
Iteration 8300: Loss = -12497.7109375
Iteration 8400: Loss = -12497.6982421875
Iteration 8500: Loss = -12497.685546875
Iteration 8600: Loss = -12497.6728515625
Iteration 8700: Loss = -12497.662109375
Iteration 8800: Loss = -12497.65234375
Iteration 8900: Loss = -12497.6416015625
Iteration 9000: Loss = -12497.6318359375
Iteration 9100: Loss = -12497.625
Iteration 9200: Loss = -12497.615234375
Iteration 9300: Loss = -12497.6083984375
Iteration 9400: Loss = -12497.599609375
Iteration 9500: Loss = -12497.59375
Iteration 9600: Loss = -12497.5869140625
Iteration 9700: Loss = -12497.580078125
Iteration 9800: Loss = -12497.57421875
Iteration 9900: Loss = -12497.5693359375
Iteration 10000: Loss = -12497.5654296875
Iteration 10100: Loss = -12497.55859375
Iteration 10200: Loss = -12497.5556640625
Iteration 10300: Loss = -12497.55078125
Iteration 10400: Loss = -12497.5478515625
Iteration 10500: Loss = -12497.5419921875
Iteration 10600: Loss = -12497.5390625
Iteration 10700: Loss = -12497.53515625
Iteration 10800: Loss = -12497.533203125
Iteration 10900: Loss = -12497.5302734375
Iteration 11000: Loss = -12497.5263671875
Iteration 11100: Loss = -12497.525390625
Iteration 11200: Loss = -12497.521484375
Iteration 11300: Loss = -12497.5185546875
Iteration 11400: Loss = -12497.515625
Iteration 11500: Loss = -12497.513671875
Iteration 11600: Loss = -12497.51171875
Iteration 11700: Loss = -12497.51171875
Iteration 11800: Loss = -12497.5087890625
Iteration 11900: Loss = -12497.5068359375
Iteration 12000: Loss = -12497.505859375
Iteration 12100: Loss = -12497.50390625
Iteration 12200: Loss = -12497.5029296875
Iteration 12300: Loss = -12497.5009765625
Iteration 12400: Loss = -12497.498046875
Iteration 12500: Loss = -12497.498046875
Iteration 12600: Loss = -12497.4970703125
Iteration 12700: Loss = -12497.4970703125
Iteration 12800: Loss = -12497.4951171875
Iteration 12900: Loss = -12497.494140625
Iteration 13000: Loss = -12497.4931640625
Iteration 13100: Loss = -12497.4921875
Iteration 13200: Loss = -12497.4921875
Iteration 13300: Loss = -12497.490234375
Iteration 13400: Loss = -12497.490234375
Iteration 13500: Loss = -12497.4892578125
Iteration 13600: Loss = -12497.48828125
Iteration 13700: Loss = -12497.486328125
Iteration 13800: Loss = -12497.4873046875
1
Iteration 13900: Loss = -12497.4853515625
Iteration 14000: Loss = -12497.4853515625
Iteration 14100: Loss = -12497.484375
Iteration 14200: Loss = -12497.484375
Iteration 14300: Loss = -12497.482421875
Iteration 14400: Loss = -12497.4833984375
1
Iteration 14500: Loss = -12497.484375
2
Iteration 14600: Loss = -12497.4814453125
Iteration 14700: Loss = -12497.4814453125
Iteration 14800: Loss = -12497.48046875
Iteration 14900: Loss = -12497.4814453125
1
Iteration 15000: Loss = -12497.4794921875
Iteration 15100: Loss = -12497.48046875
1
Iteration 15200: Loss = -12497.484375
2
Iteration 15300: Loss = -12497.4794921875
Iteration 15400: Loss = -12497.48046875
1
Iteration 15500: Loss = -12497.478515625
Iteration 15600: Loss = -12497.4794921875
1
Iteration 15700: Loss = -12497.478515625
Iteration 15800: Loss = -12497.4775390625
Iteration 15900: Loss = -12497.478515625
1
Iteration 16000: Loss = -12497.4765625
Iteration 16100: Loss = -12497.4775390625
1
Iteration 16200: Loss = -12497.4775390625
2
Iteration 16300: Loss = -12497.4765625
Iteration 16400: Loss = -12497.478515625
1
Iteration 16500: Loss = -12497.474609375
Iteration 16600: Loss = -12497.474609375
Iteration 16700: Loss = -12497.4755859375
1
Iteration 16800: Loss = -12497.4755859375
2
Iteration 16900: Loss = -12497.4755859375
3
Iteration 17000: Loss = -12497.4755859375
4
Iteration 17100: Loss = -12497.48046875
5
Iteration 17200: Loss = -12497.474609375
Iteration 17300: Loss = -12497.474609375
Iteration 17400: Loss = -12497.4755859375
1
Iteration 17500: Loss = -12497.474609375
Iteration 17600: Loss = -12497.4736328125
Iteration 17700: Loss = -12497.4736328125
Iteration 17800: Loss = -12497.474609375
1
Iteration 17900: Loss = -12497.474609375
2
Iteration 18000: Loss = -12497.474609375
3
Iteration 18100: Loss = -12497.474609375
4
Iteration 18200: Loss = -12497.474609375
5
Iteration 18300: Loss = -12497.474609375
6
Iteration 18400: Loss = -12497.4736328125
Iteration 18500: Loss = -12497.4736328125
Iteration 18600: Loss = -12497.4736328125
Iteration 18700: Loss = -12497.4736328125
Iteration 18800: Loss = -12497.4736328125
Iteration 18900: Loss = -12497.47265625
Iteration 19000: Loss = -12497.474609375
1
Iteration 19100: Loss = -12497.474609375
2
Iteration 19200: Loss = -12497.4755859375
3
Iteration 19300: Loss = -12497.4755859375
4
Iteration 19400: Loss = -12497.4755859375
5
Iteration 19500: Loss = -12497.4736328125
6
Iteration 19600: Loss = -12497.4775390625
7
Iteration 19700: Loss = -12497.4736328125
8
Iteration 19800: Loss = -12497.4736328125
9
Iteration 19900: Loss = -12497.4716796875
Iteration 20000: Loss = -12497.4736328125
1
Iteration 20100: Loss = -12497.474609375
2
Iteration 20200: Loss = -12497.4736328125
3
Iteration 20300: Loss = -12497.4736328125
4
Iteration 20400: Loss = -12497.474609375
5
Iteration 20500: Loss = -12497.4736328125
6
Iteration 20600: Loss = -12497.4736328125
7
Iteration 20700: Loss = -12497.474609375
8
Iteration 20800: Loss = -12497.4755859375
9
Iteration 20900: Loss = -12497.4736328125
10
Iteration 21000: Loss = -12497.474609375
11
Iteration 21100: Loss = -12497.474609375
12
Iteration 21200: Loss = -12497.474609375
13
Iteration 21300: Loss = -12497.474609375
14
Iteration 21400: Loss = -12497.474609375
15
Stopping early at iteration 21400 due to no improvement.
pi: tensor([[2.1137e-07, 1.0000e+00],
        [7.4776e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9790, 0.0210], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2090, 0.1634],
         [0.0067, 0.2024]],

        [[0.0121, 0.5079],
         [0.7016, 0.7876]],

        [[0.9894, 0.2483],
         [0.1906, 0.4806]],

        [[0.3680, 0.2202],
         [0.6292, 0.0876]],

        [[0.5138, 0.2380],
         [0.1343, 0.9863]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0015885126575489166
Average Adjusted Rand Index: 0.0
[0.0005149712950272512, -0.0015885126575489166] [0.0009244163070964248, 0.0] [12496.3955078125, 12497.474609375]
-------------------------------------
This iteration is 89
True Objective function: Loss = -11810.695126537514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29539.75390625
Iteration 100: Loss = -19749.537109375
Iteration 200: Loss = -13912.2001953125
Iteration 300: Loss = -12836.1416015625
Iteration 400: Loss = -12584.0087890625
Iteration 500: Loss = -12516.8818359375
Iteration 600: Loss = -12486.4052734375
Iteration 700: Loss = -12468.587890625
Iteration 800: Loss = -12456.990234375
Iteration 900: Loss = -12448.9150390625
Iteration 1000: Loss = -12443.021484375
Iteration 1100: Loss = -12438.5654296875
Iteration 1200: Loss = -12435.1064453125
Iteration 1300: Loss = -12432.3564453125
Iteration 1400: Loss = -12430.1337890625
Iteration 1500: Loss = -12428.3046875
Iteration 1600: Loss = -12426.7841796875
Iteration 1700: Loss = -12425.501953125
Iteration 1800: Loss = -12424.4130859375
Iteration 1900: Loss = -12423.4775390625
Iteration 2000: Loss = -12422.671875
Iteration 2100: Loss = -12421.9658203125
Iteration 2200: Loss = -12421.3515625
Iteration 2300: Loss = -12420.806640625
Iteration 2400: Loss = -12420.326171875
Iteration 2500: Loss = -12419.8984375
Iteration 2600: Loss = -12419.5166015625
Iteration 2700: Loss = -12419.173828125
Iteration 2800: Loss = -12418.865234375
Iteration 2900: Loss = -12418.5869140625
Iteration 3000: Loss = -12418.3330078125
Iteration 3100: Loss = -12418.1025390625
Iteration 3200: Loss = -12417.89453125
Iteration 3300: Loss = -12417.7001953125
Iteration 3400: Loss = -12417.5234375
Iteration 3500: Loss = -12417.361328125
Iteration 3600: Loss = -12417.208984375
Iteration 3700: Loss = -12417.06640625
Iteration 3800: Loss = -12416.9326171875
Iteration 3900: Loss = -12416.8056640625
Iteration 4000: Loss = -12416.6865234375
Iteration 4100: Loss = -12416.5703125
Iteration 4200: Loss = -12416.4658203125
Iteration 4300: Loss = -12416.37109375
Iteration 4400: Loss = -12416.28515625
Iteration 4500: Loss = -12416.212890625
Iteration 4600: Loss = -12416.1494140625
Iteration 4700: Loss = -12416.0927734375
Iteration 4800: Loss = -12416.044921875
Iteration 4900: Loss = -12415.9970703125
Iteration 5000: Loss = -12415.9560546875
Iteration 5100: Loss = -12415.9169921875
Iteration 5200: Loss = -12415.8798828125
Iteration 5300: Loss = -12415.8466796875
Iteration 5400: Loss = -12415.81640625
Iteration 5500: Loss = -12415.7861328125
Iteration 5600: Loss = -12415.7568359375
Iteration 5700: Loss = -12415.73046875
Iteration 5800: Loss = -12415.7060546875
Iteration 5900: Loss = -12415.6806640625
Iteration 6000: Loss = -12415.658203125
Iteration 6100: Loss = -12415.634765625
Iteration 6200: Loss = -12415.6142578125
Iteration 6300: Loss = -12415.59375
Iteration 6400: Loss = -12415.576171875
Iteration 6500: Loss = -12415.556640625
Iteration 6600: Loss = -12415.5390625
Iteration 6700: Loss = -12415.525390625
Iteration 6800: Loss = -12415.5087890625
Iteration 6900: Loss = -12415.4951171875
Iteration 7000: Loss = -12415.4814453125
Iteration 7100: Loss = -12415.4697265625
Iteration 7200: Loss = -12415.45703125
Iteration 7300: Loss = -12415.4462890625
Iteration 7400: Loss = -12415.435546875
Iteration 7500: Loss = -12415.42578125
Iteration 7600: Loss = -12415.416015625
Iteration 7700: Loss = -12415.408203125
Iteration 7800: Loss = -12415.400390625
Iteration 7900: Loss = -12415.390625
Iteration 8000: Loss = -12415.384765625
Iteration 8100: Loss = -12415.376953125
Iteration 8200: Loss = -12415.369140625
Iteration 8300: Loss = -12415.361328125
Iteration 8400: Loss = -12415.3544921875
Iteration 8500: Loss = -12415.3486328125
Iteration 8600: Loss = -12415.34375
Iteration 8700: Loss = -12415.3359375
Iteration 8800: Loss = -12415.3310546875
Iteration 8900: Loss = -12415.3251953125
Iteration 9000: Loss = -12415.3203125
Iteration 9100: Loss = -12415.3134765625
Iteration 9200: Loss = -12415.30859375
Iteration 9300: Loss = -12415.3056640625
Iteration 9400: Loss = -12415.2998046875
Iteration 9500: Loss = -12415.2958984375
Iteration 9600: Loss = -12415.2900390625
Iteration 9700: Loss = -12415.287109375
Iteration 9800: Loss = -12415.28125
Iteration 9900: Loss = -12415.2763671875
Iteration 10000: Loss = -12415.2705078125
Iteration 10100: Loss = -12415.267578125
Iteration 10200: Loss = -12415.26171875
Iteration 10300: Loss = -12415.2568359375
Iteration 10400: Loss = -12415.251953125
Iteration 10500: Loss = -12415.2470703125
Iteration 10600: Loss = -12415.2412109375
Iteration 10700: Loss = -12415.2353515625
Iteration 10800: Loss = -12415.2294921875
Iteration 10900: Loss = -12415.2216796875
Iteration 11000: Loss = -12415.21484375
Iteration 11100: Loss = -12415.2060546875
Iteration 11200: Loss = -12415.193359375
Iteration 11300: Loss = -12415.1826171875
Iteration 11400: Loss = -12415.16796875
Iteration 11500: Loss = -12415.1455078125
Iteration 11600: Loss = -12415.1162109375
Iteration 11700: Loss = -12415.0751953125
Iteration 11800: Loss = -12415.0087890625
Iteration 11900: Loss = -12414.892578125
Iteration 12000: Loss = -12414.775390625
Iteration 12100: Loss = -12414.7236328125
Iteration 12200: Loss = -12414.6748046875
Iteration 12300: Loss = -12414.62890625
Iteration 12400: Loss = -12414.583984375
Iteration 12500: Loss = -12414.544921875
Iteration 12600: Loss = -12414.5029296875
Iteration 12700: Loss = -12414.451171875
Iteration 12800: Loss = -12414.40625
Iteration 12900: Loss = -12414.3740234375
Iteration 13000: Loss = -12414.3466796875
Iteration 13100: Loss = -12414.3212890625
Iteration 13200: Loss = -12414.28125
Iteration 13300: Loss = -12414.248046875
Iteration 13400: Loss = -12414.2314453125
Iteration 13500: Loss = -12414.220703125
Iteration 13600: Loss = -12414.2158203125
Iteration 13700: Loss = -12414.2099609375
Iteration 13800: Loss = -12414.20703125
Iteration 13900: Loss = -12414.2021484375
Iteration 14000: Loss = -12414.19921875
Iteration 14100: Loss = -12414.1962890625
Iteration 14200: Loss = -12414.1953125
Iteration 14300: Loss = -12414.1923828125
Iteration 14400: Loss = -12414.1923828125
Iteration 14500: Loss = -12414.1904296875
Iteration 14600: Loss = -12414.1884765625
Iteration 14700: Loss = -12414.1865234375
Iteration 14800: Loss = -12414.185546875
Iteration 14900: Loss = -12414.1845703125
Iteration 15000: Loss = -12414.18359375
Iteration 15100: Loss = -12414.1826171875
Iteration 15200: Loss = -12414.1826171875
Iteration 15300: Loss = -12414.181640625
Iteration 15400: Loss = -12414.1796875
Iteration 15500: Loss = -12414.1796875
Iteration 15600: Loss = -12414.1787109375
Iteration 15700: Loss = -12414.1796875
1
Iteration 15800: Loss = -12414.1787109375
Iteration 15900: Loss = -12414.1787109375
Iteration 16000: Loss = -12414.1787109375
Iteration 16100: Loss = -12414.177734375
Iteration 16200: Loss = -12414.1767578125
Iteration 16300: Loss = -12414.177734375
1
Iteration 16400: Loss = -12414.1767578125
Iteration 16500: Loss = -12414.1767578125
Iteration 16600: Loss = -12414.17578125
Iteration 16700: Loss = -12414.1748046875
Iteration 16800: Loss = -12414.173828125
Iteration 16900: Loss = -12414.173828125
Iteration 17000: Loss = -12414.173828125
Iteration 17100: Loss = -12414.173828125
Iteration 17200: Loss = -12414.173828125
Iteration 17300: Loss = -12414.171875
Iteration 17400: Loss = -12414.1728515625
1
Iteration 17500: Loss = -12414.1708984375
Iteration 17600: Loss = -12414.1708984375
Iteration 17700: Loss = -12414.16796875
Iteration 17800: Loss = -12414.1572265625
Iteration 17900: Loss = -12414.046875
Iteration 18000: Loss = -12413.953125
Iteration 18100: Loss = -12413.8994140625
Iteration 18200: Loss = -12413.8125
Iteration 18300: Loss = -12413.7451171875
Iteration 18400: Loss = -12413.7177734375
Iteration 18500: Loss = -12413.712890625
Iteration 18600: Loss = -12413.712890625
Iteration 18700: Loss = -12413.7119140625
Iteration 18800: Loss = -12413.7109375
Iteration 18900: Loss = -12413.708984375
Iteration 19000: Loss = -12413.7099609375
1
Iteration 19100: Loss = -12413.7099609375
2
Iteration 19200: Loss = -12413.708984375
Iteration 19300: Loss = -12413.708984375
Iteration 19400: Loss = -12413.70703125
Iteration 19500: Loss = -12413.7080078125
1
Iteration 19600: Loss = -12413.7080078125
2
Iteration 19700: Loss = -12413.7099609375
3
Iteration 19800: Loss = -12413.7080078125
4
Iteration 19900: Loss = -12413.70703125
Iteration 20000: Loss = -12413.7080078125
1
Iteration 20100: Loss = -12413.7080078125
2
Iteration 20200: Loss = -12413.70703125
Iteration 20300: Loss = -12413.7080078125
1
Iteration 20400: Loss = -12413.70703125
Iteration 20500: Loss = -12413.70703125
Iteration 20600: Loss = -12413.705078125
Iteration 20700: Loss = -12413.70703125
1
Iteration 20800: Loss = -12413.70703125
2
Iteration 20900: Loss = -12413.7060546875
3
Iteration 21000: Loss = -12413.70703125
4
Iteration 21100: Loss = -12413.708984375
5
Iteration 21200: Loss = -12413.70703125
6
Iteration 21300: Loss = -12413.7060546875
7
Iteration 21400: Loss = -12413.70703125
8
Iteration 21500: Loss = -12413.7080078125
9
Iteration 21600: Loss = -12413.7060546875
10
Iteration 21700: Loss = -12413.70703125
11
Iteration 21800: Loss = -12413.7080078125
12
Iteration 21900: Loss = -12413.70703125
13
Iteration 22000: Loss = -12413.7080078125
14
Iteration 22100: Loss = -12413.70703125
15
Stopping early at iteration 22100 due to no improvement.
pi: tensor([[7.3798e-01, 2.6202e-01],
        [9.9998e-01, 1.6318e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([2.7953e-05, 9.9997e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1973, 0.2074],
         [0.5602, 0.2085]],

        [[0.5752, 0.2393],
         [0.3116, 0.9927]],

        [[0.9234, 0.2036],
         [0.1179, 0.9932]],

        [[0.9904, 0.2092],
         [0.9861, 0.0480]],

        [[0.9872, 0.1935],
         [0.8806, 0.0328]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0015885126575489166
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30012.7421875
Iteration 100: Loss = -22629.439453125
Iteration 200: Loss = -15805.0478515625
Iteration 300: Loss = -13050.189453125
Iteration 400: Loss = -12642.5009765625
Iteration 500: Loss = -12551.2783203125
Iteration 600: Loss = -12509.4052734375
Iteration 700: Loss = -12485.7822265625
Iteration 800: Loss = -12470.4306640625
Iteration 900: Loss = -12459.9658203125
Iteration 1000: Loss = -12452.3408203125
Iteration 1100: Loss = -12446.5146484375
Iteration 1200: Loss = -12441.923828125
Iteration 1300: Loss = -12438.2958984375
Iteration 1400: Loss = -12435.3486328125
Iteration 1500: Loss = -12432.91015625
Iteration 1600: Loss = -12430.8701171875
Iteration 1700: Loss = -12429.1396484375
Iteration 1800: Loss = -12427.6533203125
Iteration 1900: Loss = -12426.3662109375
Iteration 2000: Loss = -12425.244140625
Iteration 2100: Loss = -12424.259765625
Iteration 2200: Loss = -12423.390625
Iteration 2300: Loss = -12422.6171875
Iteration 2400: Loss = -12421.931640625
Iteration 2500: Loss = -12421.3193359375
Iteration 2600: Loss = -12420.7685546875
Iteration 2700: Loss = -12420.271484375
Iteration 2800: Loss = -12419.8203125
Iteration 2900: Loss = -12419.408203125
Iteration 3000: Loss = -12419.03125
Iteration 3100: Loss = -12418.6845703125
Iteration 3200: Loss = -12418.365234375
Iteration 3300: Loss = -12418.0654296875
Iteration 3400: Loss = -12417.791015625
Iteration 3500: Loss = -12417.533203125
Iteration 3600: Loss = -12417.2890625
Iteration 3700: Loss = -12417.0615234375
Iteration 3800: Loss = -12416.8466796875
Iteration 3900: Loss = -12416.64453125
Iteration 4000: Loss = -12416.4521484375
Iteration 4100: Loss = -12416.271484375
Iteration 4200: Loss = -12416.1025390625
Iteration 4300: Loss = -12415.9423828125
Iteration 4400: Loss = -12415.7939453125
Iteration 4500: Loss = -12415.654296875
Iteration 4600: Loss = -12415.525390625
Iteration 4700: Loss = -12415.40625
Iteration 4800: Loss = -12415.2958984375
Iteration 4900: Loss = -12415.1923828125
Iteration 5000: Loss = -12415.0986328125
Iteration 5100: Loss = -12415.01171875
Iteration 5200: Loss = -12414.931640625
Iteration 5300: Loss = -12414.857421875
Iteration 5400: Loss = -12414.7890625
Iteration 5500: Loss = -12414.724609375
Iteration 5600: Loss = -12414.6650390625
Iteration 5700: Loss = -12414.6103515625
Iteration 5800: Loss = -12414.55859375
Iteration 5900: Loss = -12414.5107421875
Iteration 6000: Loss = -12414.4658203125
Iteration 6100: Loss = -12414.4248046875
Iteration 6200: Loss = -12414.384765625
Iteration 6300: Loss = -12414.3486328125
Iteration 6400: Loss = -12414.3134765625
Iteration 6500: Loss = -12414.2822265625
Iteration 6600: Loss = -12414.251953125
Iteration 6700: Loss = -12414.2216796875
Iteration 6800: Loss = -12414.1962890625
Iteration 6900: Loss = -12414.1708984375
Iteration 7000: Loss = -12414.146484375
Iteration 7100: Loss = -12414.12109375
Iteration 7200: Loss = -12414.1015625
Iteration 7300: Loss = -12414.0810546875
Iteration 7400: Loss = -12414.0615234375
Iteration 7500: Loss = -12414.0439453125
Iteration 7600: Loss = -12414.025390625
Iteration 7700: Loss = -12414.009765625
Iteration 7800: Loss = -12413.994140625
Iteration 7900: Loss = -12413.9794921875
Iteration 8000: Loss = -12413.966796875
Iteration 8100: Loss = -12413.953125
Iteration 8200: Loss = -12413.9404296875
Iteration 8300: Loss = -12413.9287109375
Iteration 8400: Loss = -12413.9169921875
Iteration 8500: Loss = -12413.9072265625
Iteration 8600: Loss = -12413.8974609375
Iteration 8700: Loss = -12413.88671875
Iteration 8800: Loss = -12413.87890625
Iteration 8900: Loss = -12413.8701171875
Iteration 9000: Loss = -12413.861328125
Iteration 9100: Loss = -12413.853515625
Iteration 9200: Loss = -12413.8466796875
Iteration 9300: Loss = -12413.83984375
Iteration 9400: Loss = -12413.8310546875
Iteration 9500: Loss = -12413.826171875
Iteration 9600: Loss = -12413.8193359375
Iteration 9700: Loss = -12413.814453125
Iteration 9800: Loss = -12413.80859375
Iteration 9900: Loss = -12413.8046875
Iteration 10000: Loss = -12413.7998046875
Iteration 10100: Loss = -12413.79296875
Iteration 10200: Loss = -12413.791015625
Iteration 10300: Loss = -12413.7861328125
Iteration 10400: Loss = -12413.78125
Iteration 10500: Loss = -12413.7783203125
Iteration 10600: Loss = -12413.7734375
Iteration 10700: Loss = -12413.7724609375
Iteration 10800: Loss = -12413.767578125
Iteration 10900: Loss = -12413.765625
Iteration 11000: Loss = -12413.7607421875
Iteration 11100: Loss = -12413.7578125
Iteration 11200: Loss = -12413.7568359375
Iteration 11300: Loss = -12413.75390625
Iteration 11400: Loss = -12413.751953125
Iteration 11500: Loss = -12413.75
Iteration 11600: Loss = -12413.748046875
Iteration 11700: Loss = -12413.7451171875
Iteration 11800: Loss = -12413.7421875
Iteration 11900: Loss = -12413.7412109375
Iteration 12000: Loss = -12413.7392578125
Iteration 12100: Loss = -12413.73828125
Iteration 12200: Loss = -12413.7373046875
Iteration 12300: Loss = -12413.7353515625
Iteration 12400: Loss = -12413.7333984375
Iteration 12500: Loss = -12413.732421875
Iteration 12600: Loss = -12413.7314453125
Iteration 12700: Loss = -12413.7294921875
Iteration 12800: Loss = -12413.728515625
Iteration 12900: Loss = -12413.7275390625
Iteration 13000: Loss = -12413.724609375
Iteration 13100: Loss = -12413.7265625
1
Iteration 13200: Loss = -12413.724609375
Iteration 13300: Loss = -12413.72265625
Iteration 13400: Loss = -12413.7255859375
1
Iteration 13500: Loss = -12413.72265625
Iteration 13600: Loss = -12413.720703125
Iteration 13700: Loss = -12413.720703125
Iteration 13800: Loss = -12413.71875
Iteration 13900: Loss = -12413.71875
Iteration 14000: Loss = -12413.71875
Iteration 14100: Loss = -12413.716796875
Iteration 14200: Loss = -12413.716796875
Iteration 14300: Loss = -12413.7158203125
Iteration 14400: Loss = -12413.71484375
Iteration 14500: Loss = -12413.71484375
Iteration 14600: Loss = -12413.71484375
Iteration 14700: Loss = -12413.71484375
Iteration 14800: Loss = -12413.71484375
Iteration 14900: Loss = -12413.712890625
Iteration 15000: Loss = -12413.712890625
Iteration 15100: Loss = -12413.7138671875
1
Iteration 15200: Loss = -12413.7109375
Iteration 15300: Loss = -12413.712890625
1
Iteration 15400: Loss = -12413.712890625
2
Iteration 15500: Loss = -12413.7109375
Iteration 15600: Loss = -12413.7119140625
1
Iteration 15700: Loss = -12413.712890625
2
Iteration 15800: Loss = -12413.712890625
3
Iteration 15900: Loss = -12413.7099609375
Iteration 16000: Loss = -12413.7109375
1
Iteration 16100: Loss = -12413.7109375
2
Iteration 16200: Loss = -12413.7099609375
Iteration 16300: Loss = -12413.708984375
Iteration 16400: Loss = -12413.7099609375
1
Iteration 16500: Loss = -12413.7109375
2
Iteration 16600: Loss = -12413.708984375
Iteration 16700: Loss = -12413.708984375
Iteration 16800: Loss = -12413.7099609375
1
Iteration 16900: Loss = -12413.7099609375
2
Iteration 17000: Loss = -12413.708984375
Iteration 17100: Loss = -12413.7080078125
Iteration 17200: Loss = -12413.7099609375
1
Iteration 17300: Loss = -12413.7099609375
2
Iteration 17400: Loss = -12413.708984375
3
Iteration 17500: Loss = -12413.7080078125
Iteration 17600: Loss = -12413.70703125
Iteration 17700: Loss = -12413.708984375
1
Iteration 17800: Loss = -12413.7080078125
2
Iteration 17900: Loss = -12413.70703125
Iteration 18000: Loss = -12413.70703125
Iteration 18100: Loss = -12413.708984375
1
Iteration 18200: Loss = -12413.70703125
Iteration 18300: Loss = -12413.70703125
Iteration 18400: Loss = -12413.70703125
Iteration 18500: Loss = -12413.70703125
Iteration 18600: Loss = -12413.70703125
Iteration 18700: Loss = -12413.7080078125
1
Iteration 18800: Loss = -12413.70703125
Iteration 18900: Loss = -12413.70703125
Iteration 19000: Loss = -12413.7080078125
1
Iteration 19100: Loss = -12413.7060546875
Iteration 19200: Loss = -12413.708984375
1
Iteration 19300: Loss = -12413.70703125
2
Iteration 19400: Loss = -12413.70703125
3
Iteration 19500: Loss = -12413.7080078125
4
Iteration 19600: Loss = -12413.7080078125
5
Iteration 19700: Loss = -12413.7080078125
6
Iteration 19800: Loss = -12413.7080078125
7
Iteration 19900: Loss = -12413.70703125
8
Iteration 20000: Loss = -12413.708984375
9
Iteration 20100: Loss = -12413.708984375
10
Iteration 20200: Loss = -12413.70703125
11
Iteration 20300: Loss = -12413.7080078125
12
Iteration 20400: Loss = -12413.70703125
13
Iteration 20500: Loss = -12413.7109375
14
Iteration 20600: Loss = -12413.70703125
15
Stopping early at iteration 20600 due to no improvement.
pi: tensor([[7.3752e-01, 2.6248e-01],
        [1.0000e+00, 1.0002e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([6.4130e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1973, 0.3205],
         [0.9214, 0.2085]],

        [[0.0347, 0.4848],
         [0.7165, 0.1162]],

        [[0.6386, 0.2036],
         [0.0097, 0.0125]],

        [[0.2392, 0.2092],
         [0.8176, 0.9803]],

        [[0.9919, 0.1935],
         [0.6789, 0.0699]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0015885126575489166
Average Adjusted Rand Index: 0.0
[-0.0015885126575489166, -0.0015885126575489166] [0.0, 0.0] [12413.70703125, 12413.70703125]
-------------------------------------
This iteration is 90
True Objective function: Loss = -11861.832760278518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -34560.30078125
Iteration 100: Loss = -20730.306640625
Iteration 200: Loss = -13969.7158203125
Iteration 300: Loss = -12892.763671875
Iteration 400: Loss = -12681.6630859375
Iteration 500: Loss = -12586.013671875
Iteration 600: Loss = -12538.138671875
Iteration 700: Loss = -12510.546875
Iteration 800: Loss = -12489.330078125
Iteration 900: Loss = -12466.2783203125
Iteration 1000: Loss = -12456.8017578125
Iteration 1100: Loss = -12450.2919921875
Iteration 1200: Loss = -12445.1884765625
Iteration 1300: Loss = -12441.490234375
Iteration 1400: Loss = -12438.671875
Iteration 1500: Loss = -12436.369140625
Iteration 1600: Loss = -12433.7265625
Iteration 1700: Loss = -12429.45703125
Iteration 1800: Loss = -12427.904296875
Iteration 1900: Loss = -12426.724609375
Iteration 2000: Loss = -12425.7490234375
Iteration 2100: Loss = -12424.9189453125
Iteration 2200: Loss = -12419.984375
Iteration 2300: Loss = -12418.8720703125
Iteration 2400: Loss = -12418.2314453125
Iteration 2500: Loss = -12417.697265625
Iteration 2600: Loss = -12417.23828125
Iteration 2700: Loss = -12416.8427734375
Iteration 2800: Loss = -12416.4951171875
Iteration 2900: Loss = -12416.1865234375
Iteration 3000: Loss = -12415.9140625
Iteration 3100: Loss = -12415.669921875
Iteration 3200: Loss = -12415.44921875
Iteration 3300: Loss = -12415.25
Iteration 3400: Loss = -12415.0703125
Iteration 3500: Loss = -12414.90625
Iteration 3600: Loss = -12414.7568359375
Iteration 3700: Loss = -12414.62109375
Iteration 3800: Loss = -12414.4970703125
Iteration 3900: Loss = -12414.3828125
Iteration 4000: Loss = -12414.27734375
Iteration 4100: Loss = -12414.1806640625
Iteration 4200: Loss = -12414.0888671875
Iteration 4300: Loss = -12414.005859375
Iteration 4400: Loss = -12413.927734375
Iteration 4500: Loss = -12413.8544921875
Iteration 4600: Loss = -12413.7861328125
Iteration 4700: Loss = -12413.72265625
Iteration 4800: Loss = -12413.662109375
Iteration 4900: Loss = -12413.60546875
Iteration 5000: Loss = -12413.552734375
Iteration 5100: Loss = -12413.50390625
Iteration 5200: Loss = -12413.4560546875
Iteration 5300: Loss = -12413.4091796875
Iteration 5400: Loss = -12413.3701171875
Iteration 5500: Loss = -12413.330078125
Iteration 5600: Loss = -12413.29296875
Iteration 5700: Loss = -12413.2578125
Iteration 5800: Loss = -12413.2255859375
Iteration 5900: Loss = -12413.1943359375
Iteration 6000: Loss = -12413.1650390625
Iteration 6100: Loss = -12413.13671875
Iteration 6200: Loss = -12413.1103515625
Iteration 6300: Loss = -12413.0830078125
Iteration 6400: Loss = -12413.0595703125
Iteration 6500: Loss = -12413.0361328125
Iteration 6600: Loss = -12413.013671875
Iteration 6700: Loss = -12412.9931640625
Iteration 6800: Loss = -12412.9716796875
Iteration 6900: Loss = -12412.9541015625
Iteration 7000: Loss = -12412.93359375
Iteration 7100: Loss = -12412.916015625
Iteration 7200: Loss = -12412.8974609375
Iteration 7300: Loss = -12412.8798828125
Iteration 7400: Loss = -12412.86328125
Iteration 7500: Loss = -12412.845703125
Iteration 7600: Loss = -12412.828125
Iteration 7700: Loss = -12412.8125
Iteration 7800: Loss = -12412.798828125
Iteration 7900: Loss = -12412.78515625
Iteration 8000: Loss = -12412.7734375
Iteration 8100: Loss = -12412.7607421875
Iteration 8200: Loss = -12412.75
Iteration 8300: Loss = -12412.7392578125
Iteration 8400: Loss = -12412.7294921875
Iteration 8500: Loss = -12412.7177734375
Iteration 8600: Loss = -12412.708984375
Iteration 8700: Loss = -12412.701171875
Iteration 8800: Loss = -12412.6923828125
Iteration 8900: Loss = -12412.68359375
Iteration 9000: Loss = -12412.6748046875
Iteration 9100: Loss = -12412.6650390625
Iteration 9200: Loss = -12412.654296875
Iteration 9300: Loss = -12412.6435546875
Iteration 9400: Loss = -12412.6328125
Iteration 9500: Loss = -12412.62109375
Iteration 9600: Loss = -12412.6123046875
Iteration 9700: Loss = -12412.6025390625
Iteration 9800: Loss = -12412.5947265625
Iteration 9900: Loss = -12412.5888671875
Iteration 10000: Loss = -12412.5830078125
Iteration 10100: Loss = -12412.5771484375
Iteration 10200: Loss = -12412.5703125
Iteration 10300: Loss = -12412.564453125
Iteration 10400: Loss = -12412.5595703125
Iteration 10500: Loss = -12412.5517578125
Iteration 10600: Loss = -12412.546875
Iteration 10700: Loss = -12412.5419921875
Iteration 10800: Loss = -12412.53515625
Iteration 10900: Loss = -12412.52734375
Iteration 11000: Loss = -12412.5224609375
Iteration 11100: Loss = -12412.515625
Iteration 11200: Loss = -12412.5068359375
Iteration 11300: Loss = -12412.49609375
Iteration 11400: Loss = -12412.48828125
Iteration 11500: Loss = -12412.4775390625
Iteration 11600: Loss = -12412.46875
Iteration 11700: Loss = -12412.4580078125
Iteration 11800: Loss = -12412.4443359375
Iteration 11900: Loss = -12412.4296875
Iteration 12000: Loss = -12412.412109375
Iteration 12100: Loss = -12412.39453125
Iteration 12200: Loss = -12412.369140625
Iteration 12300: Loss = -12412.33984375
Iteration 12400: Loss = -12412.298828125
Iteration 12500: Loss = -12412.244140625
Iteration 12600: Loss = -12412.15625
Iteration 12700: Loss = -12412.0009765625
Iteration 12800: Loss = -12411.677734375
Iteration 12900: Loss = -12411.1630859375
Iteration 13000: Loss = -12410.7978515625
Iteration 13100: Loss = -12410.6162109375
Iteration 13200: Loss = -12410.4326171875
Iteration 13300: Loss = -12410.310546875
Iteration 13400: Loss = -12410.255859375
Iteration 13500: Loss = -12410.1826171875
Iteration 13600: Loss = -12410.1025390625
Iteration 13700: Loss = -12409.998046875
Iteration 13800: Loss = -12409.8427734375
Iteration 13900: Loss = -12409.828125
Iteration 14000: Loss = -12409.7998046875
Iteration 14100: Loss = -12409.7236328125
Iteration 14200: Loss = -12409.6826171875
Iteration 14300: Loss = -12409.6083984375
Iteration 14400: Loss = -12409.43359375
Iteration 14500: Loss = -12409.19921875
Iteration 14600: Loss = -12409.166015625
Iteration 14700: Loss = -12409.15625
Iteration 14800: Loss = -12409.1044921875
Iteration 14900: Loss = -12409.1005859375
Iteration 15000: Loss = -12409.099609375
Iteration 15100: Loss = -12409.09765625
Iteration 15200: Loss = -12409.095703125
Iteration 15300: Loss = -12409.0966796875
1
Iteration 15400: Loss = -12409.0966796875
2
Iteration 15500: Loss = -12409.0927734375
Iteration 15600: Loss = -12409.0927734375
Iteration 15700: Loss = -12409.0927734375
Iteration 15800: Loss = -12409.0947265625
1
Iteration 15900: Loss = -12409.0927734375
Iteration 16000: Loss = -12409.0908203125
Iteration 16100: Loss = -12409.0908203125
Iteration 16200: Loss = -12409.08984375
Iteration 16300: Loss = -12409.091796875
1
Iteration 16400: Loss = -12409.08984375
Iteration 16500: Loss = -12409.0908203125
1
Iteration 16600: Loss = -12409.08984375
Iteration 16700: Loss = -12409.0888671875
Iteration 16800: Loss = -12409.0888671875
Iteration 16900: Loss = -12409.0888671875
Iteration 17000: Loss = -12409.08984375
1
Iteration 17100: Loss = -12409.0888671875
Iteration 17200: Loss = -12409.087890625
Iteration 17300: Loss = -12409.0888671875
1
Iteration 17400: Loss = -12409.08984375
2
Iteration 17500: Loss = -12409.087890625
Iteration 17600: Loss = -12409.0888671875
1
Iteration 17700: Loss = -12409.08984375
2
Iteration 17800: Loss = -12409.0869140625
Iteration 17900: Loss = -12409.087890625
1
Iteration 18000: Loss = -12409.0888671875
2
Iteration 18100: Loss = -12409.087890625
3
Iteration 18200: Loss = -12409.087890625
4
Iteration 18300: Loss = -12409.087890625
5
Iteration 18400: Loss = -12409.087890625
6
Iteration 18500: Loss = -12409.0869140625
Iteration 18600: Loss = -12409.087890625
1
Iteration 18700: Loss = -12409.0869140625
Iteration 18800: Loss = -12409.0869140625
Iteration 18900: Loss = -12409.087890625
1
Iteration 19000: Loss = -12409.087890625
2
Iteration 19100: Loss = -12409.0869140625
Iteration 19200: Loss = -12409.087890625
1
Iteration 19300: Loss = -12409.087890625
2
Iteration 19400: Loss = -12409.0869140625
Iteration 19500: Loss = -12409.0869140625
Iteration 19600: Loss = -12409.087890625
1
Iteration 19700: Loss = -12409.0859375
Iteration 19800: Loss = -12409.0869140625
1
Iteration 19900: Loss = -12409.0869140625
2
Iteration 20000: Loss = -12409.0869140625
3
Iteration 20100: Loss = -12409.087890625
4
Iteration 20200: Loss = -12409.0859375
Iteration 20300: Loss = -12409.0869140625
1
Iteration 20400: Loss = -12409.0869140625
2
Iteration 20500: Loss = -12409.0869140625
3
Iteration 20600: Loss = -12409.0869140625
4
Iteration 20700: Loss = -12409.08984375
5
Iteration 20800: Loss = -12409.0869140625
6
Iteration 20900: Loss = -12409.087890625
7
Iteration 21000: Loss = -12409.0869140625
8
Iteration 21100: Loss = -12409.087890625
9
Iteration 21200: Loss = -12409.087890625
10
Iteration 21300: Loss = -12409.0869140625
11
Iteration 21400: Loss = -12409.0869140625
12
Iteration 21500: Loss = -12409.0869140625
13
Iteration 21600: Loss = -12409.0859375
Iteration 21700: Loss = -12409.087890625
1
Iteration 21800: Loss = -12409.0869140625
2
Iteration 21900: Loss = -12409.0869140625
3
Iteration 22000: Loss = -12409.0869140625
4
Iteration 22100: Loss = -12409.087890625
5
Iteration 22200: Loss = -12409.0869140625
6
Iteration 22300: Loss = -12409.087890625
7
Iteration 22400: Loss = -12409.0859375
Iteration 22500: Loss = -12409.0859375
Iteration 22600: Loss = -12409.087890625
1
Iteration 22700: Loss = -12409.0859375
Iteration 22800: Loss = -12409.0869140625
1
Iteration 22900: Loss = -12409.0869140625
2
Iteration 23000: Loss = -12409.0869140625
3
Iteration 23100: Loss = -12409.0859375
Iteration 23200: Loss = -12409.0869140625
1
Iteration 23300: Loss = -12409.0859375
Iteration 23400: Loss = -12409.0869140625
1
Iteration 23500: Loss = -12409.087890625
2
Iteration 23600: Loss = -12409.087890625
3
Iteration 23700: Loss = -12409.0869140625
4
Iteration 23800: Loss = -12409.0869140625
5
Iteration 23900: Loss = -12409.0859375
Iteration 24000: Loss = -12409.087890625
1
Iteration 24100: Loss = -12409.0869140625
2
Iteration 24200: Loss = -12409.0849609375
Iteration 24300: Loss = -12409.0869140625
1
Iteration 24400: Loss = -12409.0849609375
Iteration 24500: Loss = -12409.0869140625
1
Iteration 24600: Loss = -12409.0859375
2
Iteration 24700: Loss = -12409.0869140625
3
Iteration 24800: Loss = -12409.0859375
4
Iteration 24900: Loss = -12409.0859375
5
Iteration 25000: Loss = -12409.087890625
6
Iteration 25100: Loss = -12409.0859375
7
Iteration 25200: Loss = -12409.0869140625
8
Iteration 25300: Loss = -12409.0859375
9
Iteration 25400: Loss = -12409.087890625
10
Iteration 25500: Loss = -12409.087890625
11
Iteration 25600: Loss = -12409.087890625
12
Iteration 25700: Loss = -12409.0869140625
13
Iteration 25800: Loss = -12409.087890625
14
Iteration 25900: Loss = -12409.0869140625
15
Stopping early at iteration 25900 due to no improvement.
pi: tensor([[1.5697e-04, 9.9984e-01],
        [9.0422e-02, 9.0958e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3507, 0.6493], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1834, 0.1944],
         [0.6308, 0.2058]],

        [[0.6373, 0.2125],
         [0.9893, 0.2788]],

        [[0.2184, 0.1225],
         [0.2522, 0.9652]],

        [[0.1116, 0.1907],
         [0.9833, 0.6742]],

        [[0.0188, 0.1934],
         [0.0116, 0.4594]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.002236184874153835
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00023263629338801767
Average Adjusted Rand Index: -0.00044723697483076694
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31175.294921875
Iteration 100: Loss = -20324.15625
Iteration 200: Loss = -13933.2236328125
Iteration 300: Loss = -12967.66015625
Iteration 400: Loss = -12749.517578125
Iteration 500: Loss = -12663.7216796875
Iteration 600: Loss = -12613.759765625
Iteration 700: Loss = -12580.4853515625
Iteration 800: Loss = -12556.9091796875
Iteration 900: Loss = -12540.3173828125
Iteration 1000: Loss = -12521.4033203125
Iteration 1100: Loss = -12510.6357421875
Iteration 1200: Loss = -12501.603515625
Iteration 1300: Loss = -12494.2373046875
Iteration 1400: Loss = -12488.0732421875
Iteration 1500: Loss = -12481.49609375
Iteration 1600: Loss = -12472.525390625
Iteration 1700: Loss = -12463.5908203125
Iteration 1800: Loss = -12458.3720703125
Iteration 1900: Loss = -12452.32421875
Iteration 2000: Loss = -12448.5859375
Iteration 2100: Loss = -12444.9619140625
Iteration 2200: Loss = -12442.2734375
Iteration 2300: Loss = -12437.8369140625
Iteration 2400: Loss = -12434.6962890625
Iteration 2500: Loss = -12432.4580078125
Iteration 2600: Loss = -12430.7978515625
Iteration 2700: Loss = -12429.150390625
Iteration 2800: Loss = -12427.8056640625
Iteration 2900: Loss = -12426.9140625
Iteration 3000: Loss = -12426.2744140625
Iteration 3100: Loss = -12425.7548828125
Iteration 3200: Loss = -12425.3173828125
Iteration 3300: Loss = -12424.9365234375
Iteration 3400: Loss = -12424.6025390625
Iteration 3500: Loss = -12424.3046875
Iteration 3600: Loss = -12424.029296875
Iteration 3700: Loss = -12423.78125
Iteration 3800: Loss = -12423.5654296875
Iteration 3900: Loss = -12423.369140625
Iteration 4000: Loss = -12423.19140625
Iteration 4100: Loss = -12423.029296875
Iteration 4200: Loss = -12422.880859375
Iteration 4300: Loss = -12422.744140625
Iteration 4400: Loss = -12422.619140625
Iteration 4500: Loss = -12422.5029296875
Iteration 4600: Loss = -12422.39453125
Iteration 4700: Loss = -12422.29296875
Iteration 4800: Loss = -12422.197265625
Iteration 4900: Loss = -12422.10546875
Iteration 5000: Loss = -12422.0068359375
Iteration 5100: Loss = -12420.837890625
Iteration 5200: Loss = -12418.681640625
Iteration 5300: Loss = -12418.4267578125
Iteration 5400: Loss = -12418.2861328125
Iteration 5500: Loss = -12418.181640625
Iteration 5600: Loss = -12418.09375
Iteration 5700: Loss = -12418.0205078125
Iteration 5800: Loss = -12417.953125
Iteration 5900: Loss = -12417.8740234375
Iteration 6000: Loss = -12413.69140625
Iteration 6100: Loss = -12413.51953125
Iteration 6200: Loss = -12413.416015625
Iteration 6300: Loss = -12413.3369140625
Iteration 6400: Loss = -12413.2685546875
Iteration 6500: Loss = -12413.212890625
Iteration 6600: Loss = -12413.1640625
Iteration 6700: Loss = -12413.1201171875
Iteration 6800: Loss = -12413.0810546875
Iteration 6900: Loss = -12413.044921875
Iteration 7000: Loss = -12413.013671875
Iteration 7100: Loss = -12412.984375
Iteration 7200: Loss = -12412.9580078125
Iteration 7300: Loss = -12412.9345703125
Iteration 7400: Loss = -12412.9111328125
Iteration 7500: Loss = -12412.888671875
Iteration 7600: Loss = -12412.8681640625
Iteration 7700: Loss = -12412.8515625
Iteration 7800: Loss = -12412.833984375
Iteration 7900: Loss = -12412.818359375
Iteration 8000: Loss = -12412.8037109375
Iteration 8100: Loss = -12412.7900390625
Iteration 8200: Loss = -12412.7783203125
Iteration 8300: Loss = -12412.7646484375
Iteration 8400: Loss = -12412.7548828125
Iteration 8500: Loss = -12412.7421875
Iteration 8600: Loss = -12412.7333984375
Iteration 8700: Loss = -12412.7236328125
Iteration 8800: Loss = -12412.7158203125
Iteration 8900: Loss = -12412.70703125
Iteration 9000: Loss = -12412.7001953125
Iteration 9100: Loss = -12412.693359375
Iteration 9200: Loss = -12412.685546875
Iteration 9300: Loss = -12412.6796875
Iteration 9400: Loss = -12412.6748046875
Iteration 9500: Loss = -12412.6689453125
Iteration 9600: Loss = -12412.6650390625
Iteration 9700: Loss = -12412.6591796875
Iteration 9800: Loss = -12412.6533203125
Iteration 9900: Loss = -12412.6484375
Iteration 10000: Loss = -12412.6455078125
Iteration 10100: Loss = -12412.640625
Iteration 10200: Loss = -12412.63671875
Iteration 10300: Loss = -12412.6337890625
Iteration 10400: Loss = -12412.630859375
Iteration 10500: Loss = -12412.6279296875
Iteration 10600: Loss = -12412.6259765625
Iteration 10700: Loss = -12412.623046875
Iteration 10800: Loss = -12412.6220703125
Iteration 10900: Loss = -12412.6171875
Iteration 11000: Loss = -12412.615234375
Iteration 11100: Loss = -12412.61328125
Iteration 11200: Loss = -12412.6103515625
Iteration 11300: Loss = -12412.6103515625
Iteration 11400: Loss = -12412.607421875
Iteration 11500: Loss = -12412.60546875
Iteration 11600: Loss = -12412.603515625
Iteration 11700: Loss = -12412.6025390625
Iteration 11800: Loss = -12412.6005859375
Iteration 11900: Loss = -12412.5966796875
Iteration 12000: Loss = -12412.5947265625
Iteration 12100: Loss = -12412.5927734375
Iteration 12200: Loss = -12412.5927734375
Iteration 12300: Loss = -12412.58984375
Iteration 12400: Loss = -12412.587890625
Iteration 12500: Loss = -12412.587890625
Iteration 12600: Loss = -12412.5849609375
Iteration 12700: Loss = -12412.5830078125
Iteration 12800: Loss = -12412.583984375
1
Iteration 12900: Loss = -12412.5810546875
Iteration 13000: Loss = -12412.5791015625
Iteration 13100: Loss = -12412.578125
Iteration 13200: Loss = -12412.576171875
Iteration 13300: Loss = -12412.5751953125
Iteration 13400: Loss = -12412.57421875
Iteration 13500: Loss = -12412.57421875
Iteration 13600: Loss = -12412.57421875
Iteration 13700: Loss = -12412.5732421875
Iteration 13800: Loss = -12412.572265625
Iteration 13900: Loss = -12412.5712890625
Iteration 14000: Loss = -12412.5693359375
Iteration 14100: Loss = -12412.5693359375
Iteration 14200: Loss = -12412.5673828125
Iteration 14300: Loss = -12412.5654296875
Iteration 14400: Loss = -12412.564453125
Iteration 14500: Loss = -12412.5625
Iteration 14600: Loss = -12412.5625
Iteration 14700: Loss = -12412.560546875
Iteration 14800: Loss = -12412.5615234375
1
Iteration 14900: Loss = -12412.556640625
Iteration 15000: Loss = -12412.5556640625
Iteration 15100: Loss = -12412.5556640625
Iteration 15200: Loss = -12412.55859375
1
Iteration 15300: Loss = -12412.5546875
Iteration 15400: Loss = -12412.5546875
Iteration 15500: Loss = -12412.5517578125
Iteration 15600: Loss = -12412.5517578125
Iteration 15700: Loss = -12412.5517578125
Iteration 15800: Loss = -12412.55078125
Iteration 15900: Loss = -12412.5498046875
Iteration 16000: Loss = -12412.5478515625
Iteration 16100: Loss = -12412.5498046875
1
Iteration 16200: Loss = -12412.548828125
2
Iteration 16300: Loss = -12412.548828125
3
Iteration 16400: Loss = -12412.548828125
4
Iteration 16500: Loss = -12412.5498046875
5
Iteration 16600: Loss = -12412.5498046875
6
Iteration 16700: Loss = -12412.548828125
7
Iteration 16800: Loss = -12412.548828125
8
Iteration 16900: Loss = -12412.548828125
9
Iteration 17000: Loss = -12412.548828125
10
Iteration 17100: Loss = -12412.548828125
11
Iteration 17200: Loss = -12412.5478515625
Iteration 17300: Loss = -12412.5478515625
Iteration 17400: Loss = -12412.5458984375
Iteration 17500: Loss = -12412.546875
1
Iteration 17600: Loss = -12412.5478515625
2
Iteration 17700: Loss = -12412.548828125
3
Iteration 17800: Loss = -12412.546875
4
Iteration 17900: Loss = -12412.546875
5
Iteration 18000: Loss = -12412.5478515625
6
Iteration 18100: Loss = -12412.5478515625
7
Iteration 18200: Loss = -12412.546875
8
Iteration 18300: Loss = -12412.546875
9
Iteration 18400: Loss = -12412.5478515625
10
Iteration 18500: Loss = -12412.546875
11
Iteration 18600: Loss = -12412.5458984375
Iteration 18700: Loss = -12412.5478515625
1
Iteration 18800: Loss = -12412.5478515625
2
Iteration 18900: Loss = -12412.5478515625
3
Iteration 19000: Loss = -12412.5478515625
4
Iteration 19100: Loss = -12412.5458984375
Iteration 19200: Loss = -12412.544921875
Iteration 19300: Loss = -12412.546875
1
Iteration 19400: Loss = -12412.546875
2
Iteration 19500: Loss = -12412.546875
3
Iteration 19600: Loss = -12412.5439453125
Iteration 19700: Loss = -12412.5439453125
Iteration 19800: Loss = -12412.5439453125
Iteration 19900: Loss = -12412.5458984375
1
Iteration 20000: Loss = -12412.5439453125
Iteration 20100: Loss = -12412.544921875
1
Iteration 20200: Loss = -12412.544921875
2
Iteration 20300: Loss = -12412.5478515625
3
Iteration 20400: Loss = -12412.544921875
4
Iteration 20500: Loss = -12412.544921875
5
Iteration 20600: Loss = -12412.544921875
6
Iteration 20700: Loss = -12412.5439453125
Iteration 20800: Loss = -12412.54296875
Iteration 20900: Loss = -12412.5439453125
1
Iteration 21000: Loss = -12412.544921875
2
Iteration 21100: Loss = -12412.54296875
Iteration 21200: Loss = -12412.5439453125
1
Iteration 21300: Loss = -12412.544921875
2
Iteration 21400: Loss = -12412.5439453125
3
Iteration 21500: Loss = -12412.5419921875
Iteration 21600: Loss = -12412.54296875
1
Iteration 21700: Loss = -12412.5439453125
2
Iteration 21800: Loss = -12412.54296875
3
Iteration 21900: Loss = -12412.5439453125
4
Iteration 22000: Loss = -12412.5439453125
5
Iteration 22100: Loss = -12412.5439453125
6
Iteration 22200: Loss = -12412.54296875
7
Iteration 22300: Loss = -12412.5439453125
8
Iteration 22400: Loss = -12412.546875
9
Iteration 22500: Loss = -12412.5439453125
10
Iteration 22600: Loss = -12412.5439453125
11
Iteration 22700: Loss = -12412.544921875
12
Iteration 22800: Loss = -12412.54296875
13
Iteration 22900: Loss = -12412.544921875
14
Iteration 23000: Loss = -12412.5439453125
15
Stopping early at iteration 23000 due to no improvement.
pi: tensor([[9.9431e-01, 5.6855e-03],
        [9.9971e-01, 2.8842e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9996e-01, 4.1861e-05], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2010, 0.1902],
         [0.4992, 0.1443]],

        [[0.2052, 0.2287],
         [0.7269, 0.9865]],

        [[0.1510, 0.1127],
         [0.9574, 0.9768]],

        [[0.1332, 0.1982],
         [0.0165, 0.9376]],

        [[0.9925, 0.6109],
         [0.1500, 0.9875]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[-0.00023263629338801767, 0.0] [-0.00044723697483076694, 0.0] [12409.0869140625, 12412.5439453125]
-------------------------------------
This iteration is 91
True Objective function: Loss = -11815.598262135527
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -42744.390625
Iteration 100: Loss = -26399.5390625
Iteration 200: Loss = -15779.0810546875
Iteration 300: Loss = -13577.7412109375
Iteration 400: Loss = -13069.14453125
Iteration 500: Loss = -12821.7763671875
Iteration 600: Loss = -12688.2177734375
Iteration 700: Loss = -12608.6240234375
Iteration 800: Loss = -12538.3505859375
Iteration 900: Loss = -12500.3447265625
Iteration 1000: Loss = -12474.4814453125
Iteration 1100: Loss = -12456.208984375
Iteration 1200: Loss = -12440.7685546875
Iteration 1300: Loss = -12423.1142578125
Iteration 1400: Loss = -12400.330078125
Iteration 1500: Loss = -12392.578125
Iteration 1600: Loss = -12387.3916015625
Iteration 1700: Loss = -12382.6328125
Iteration 1800: Loss = -12376.5263671875
Iteration 1900: Loss = -12372.5263671875
Iteration 2000: Loss = -12367.3564453125
Iteration 2100: Loss = -12363.390625
Iteration 2200: Loss = -12360.9072265625
Iteration 2300: Loss = -12358.8515625
Iteration 2400: Loss = -12357.201171875
Iteration 2500: Loss = -12355.8623046875
Iteration 2600: Loss = -12354.7490234375
Iteration 2700: Loss = -12353.8037109375
Iteration 2800: Loss = -12352.9736328125
Iteration 2900: Loss = -12352.232421875
Iteration 3000: Loss = -12351.5615234375
Iteration 3100: Loss = -12350.9423828125
Iteration 3200: Loss = -12346.779296875
Iteration 3300: Loss = -12346.197265625
Iteration 3400: Loss = -12345.732421875
Iteration 3500: Loss = -12345.3251953125
Iteration 3600: Loss = -12344.9453125
Iteration 3700: Loss = -12344.6181640625
Iteration 3800: Loss = -12344.322265625
Iteration 3900: Loss = -12344.05078125
Iteration 4000: Loss = -12343.802734375
Iteration 4100: Loss = -12343.57421875
Iteration 4200: Loss = -12343.361328125
Iteration 4300: Loss = -12343.166015625
Iteration 4400: Loss = -12342.984375
Iteration 4500: Loss = -12342.814453125
Iteration 4600: Loss = -12342.658203125
Iteration 4700: Loss = -12342.5087890625
Iteration 4800: Loss = -12342.37109375
Iteration 4900: Loss = -12342.23828125
Iteration 5000: Loss = -12342.1162109375
Iteration 5100: Loss = -12342.0009765625
Iteration 5200: Loss = -12341.896484375
Iteration 5300: Loss = -12341.794921875
Iteration 5400: Loss = -12341.7001953125
Iteration 5500: Loss = -12341.61328125
Iteration 5600: Loss = -12341.5283203125
Iteration 5700: Loss = -12341.44921875
Iteration 5800: Loss = -12341.3740234375
Iteration 5900: Loss = -12341.302734375
Iteration 6000: Loss = -12341.2314453125
Iteration 6100: Loss = -12341.1650390625
Iteration 6200: Loss = -12341.1044921875
Iteration 6300: Loss = -12341.0458984375
Iteration 6400: Loss = -12340.9912109375
Iteration 6500: Loss = -12340.939453125
Iteration 6600: Loss = -12340.890625
Iteration 6700: Loss = -12340.841796875
Iteration 6800: Loss = -12340.796875
Iteration 6900: Loss = -12340.75390625
Iteration 7000: Loss = -12340.712890625
Iteration 7100: Loss = -12340.671875
Iteration 7200: Loss = -12340.6357421875
Iteration 7300: Loss = -12340.595703125
Iteration 7400: Loss = -12340.5576171875
Iteration 7500: Loss = -12334.8359375
Iteration 7600: Loss = -12334.5400390625
Iteration 7700: Loss = -12334.4462890625
Iteration 7800: Loss = -12330.4677734375
Iteration 7900: Loss = -12330.2392578125
Iteration 8000: Loss = -12330.11328125
Iteration 8100: Loss = -12330.0244140625
Iteration 8200: Loss = -12329.95703125
Iteration 8300: Loss = -12329.9013671875
Iteration 8400: Loss = -12329.853515625
Iteration 8500: Loss = -12329.8125
Iteration 8600: Loss = -12329.775390625
Iteration 8700: Loss = -12329.744140625
Iteration 8800: Loss = -12329.7158203125
Iteration 8900: Loss = -12329.689453125
Iteration 9000: Loss = -12329.6669921875
Iteration 9100: Loss = -12329.6435546875
Iteration 9200: Loss = -12329.6240234375
Iteration 9300: Loss = -12329.6064453125
Iteration 9400: Loss = -12329.587890625
Iteration 9500: Loss = -12329.572265625
Iteration 9600: Loss = -12329.5576171875
Iteration 9700: Loss = -12329.5439453125
Iteration 9800: Loss = -12329.529296875
Iteration 9900: Loss = -12329.51953125
Iteration 10000: Loss = -12329.5078125
Iteration 10100: Loss = -12329.4970703125
Iteration 10200: Loss = -12329.486328125
Iteration 10300: Loss = -12329.4765625
Iteration 10400: Loss = -12329.466796875
Iteration 10500: Loss = -12329.4619140625
Iteration 10600: Loss = -12329.4521484375
Iteration 10700: Loss = -12329.4453125
Iteration 10800: Loss = -12329.4375
Iteration 10900: Loss = -12329.431640625
Iteration 11000: Loss = -12329.423828125
Iteration 11100: Loss = -12329.41796875
Iteration 11200: Loss = -12329.412109375
Iteration 11300: Loss = -12329.40625
Iteration 11400: Loss = -12329.4013671875
Iteration 11500: Loss = -12329.3974609375
Iteration 11600: Loss = -12329.392578125
Iteration 11700: Loss = -12329.3876953125
Iteration 11800: Loss = -12329.3837890625
Iteration 11900: Loss = -12329.37890625
Iteration 12000: Loss = -12329.375
Iteration 12100: Loss = -12329.3740234375
Iteration 12200: Loss = -12329.3701171875
Iteration 12300: Loss = -12329.3662109375
Iteration 12400: Loss = -12329.3623046875
Iteration 12500: Loss = -12329.3603515625
Iteration 12600: Loss = -12329.357421875
Iteration 12700: Loss = -12329.3544921875
Iteration 12800: Loss = -12329.3505859375
Iteration 12900: Loss = -12329.3505859375
Iteration 13000: Loss = -12329.34765625
Iteration 13100: Loss = -12329.3427734375
Iteration 13200: Loss = -12329.34375
1
Iteration 13300: Loss = -12329.341796875
Iteration 13400: Loss = -12329.3408203125
Iteration 13500: Loss = -12329.3369140625
Iteration 13600: Loss = -12329.3369140625
Iteration 13700: Loss = -12329.3330078125
Iteration 13800: Loss = -12329.3330078125
Iteration 13900: Loss = -12329.33203125
Iteration 14000: Loss = -12329.330078125
Iteration 14100: Loss = -12329.330078125
Iteration 14200: Loss = -12329.328125
Iteration 14300: Loss = -12329.3271484375
Iteration 14400: Loss = -12329.3271484375
Iteration 14500: Loss = -12329.326171875
Iteration 14600: Loss = -12329.32421875
Iteration 14700: Loss = -12329.322265625
Iteration 14800: Loss = -12329.3212890625
Iteration 14900: Loss = -12329.322265625
1
Iteration 15000: Loss = -12329.322265625
2
Iteration 15100: Loss = -12329.3212890625
Iteration 15200: Loss = -12329.3193359375
Iteration 15300: Loss = -12329.318359375
Iteration 15400: Loss = -12329.318359375
Iteration 15500: Loss = -12329.3173828125
Iteration 15600: Loss = -12329.3154296875
Iteration 15700: Loss = -12329.3173828125
1
Iteration 15800: Loss = -12329.3134765625
Iteration 15900: Loss = -12329.3134765625
Iteration 16000: Loss = -12329.3125
Iteration 16100: Loss = -12329.3134765625
1
Iteration 16200: Loss = -12329.314453125
2
Iteration 16300: Loss = -12329.3134765625
3
Iteration 16400: Loss = -12329.3125
Iteration 16500: Loss = -12329.3125
Iteration 16600: Loss = -12329.3115234375
Iteration 16700: Loss = -12329.3115234375
Iteration 16800: Loss = -12329.3115234375
Iteration 16900: Loss = -12329.310546875
Iteration 17000: Loss = -12329.3115234375
1
Iteration 17100: Loss = -12329.3095703125
Iteration 17200: Loss = -12329.310546875
1
Iteration 17300: Loss = -12329.310546875
2
Iteration 17400: Loss = -12329.310546875
3
Iteration 17500: Loss = -12329.3125
4
Iteration 17600: Loss = -12329.30859375
Iteration 17700: Loss = -12329.3095703125
1
Iteration 17800: Loss = -12329.3095703125
2
Iteration 17900: Loss = -12329.30859375
Iteration 18000: Loss = -12329.3076171875
Iteration 18100: Loss = -12329.30859375
1
Iteration 18200: Loss = -12329.30859375
2
Iteration 18300: Loss = -12329.3076171875
Iteration 18400: Loss = -12329.3076171875
Iteration 18500: Loss = -12329.30859375
1
Iteration 18600: Loss = -12329.30859375
2
Iteration 18700: Loss = -12329.3056640625
Iteration 18800: Loss = -12329.3076171875
1
Iteration 18900: Loss = -12329.30859375
2
Iteration 19000: Loss = -12329.3076171875
3
Iteration 19100: Loss = -12329.306640625
4
Iteration 19200: Loss = -12329.3056640625
Iteration 19300: Loss = -12329.306640625
1
Iteration 19400: Loss = -12329.306640625
2
Iteration 19500: Loss = -12329.3046875
Iteration 19600: Loss = -12329.3056640625
1
Iteration 19700: Loss = -12329.3037109375
Iteration 19800: Loss = -12329.3037109375
Iteration 19900: Loss = -12329.3056640625
1
Iteration 20000: Loss = -12329.3056640625
2
Iteration 20100: Loss = -12329.306640625
3
Iteration 20200: Loss = -12329.3056640625
4
Iteration 20300: Loss = -12329.3056640625
5
Iteration 20400: Loss = -12329.3056640625
6
Iteration 20500: Loss = -12329.3046875
7
Iteration 20600: Loss = -12329.3046875
8
Iteration 20700: Loss = -12329.3056640625
9
Iteration 20800: Loss = -12329.3056640625
10
Iteration 20900: Loss = -12329.3046875
11
Iteration 21000: Loss = -12329.3046875
12
Iteration 21100: Loss = -12329.3037109375
Iteration 21200: Loss = -12329.3037109375
Iteration 21300: Loss = -12329.3046875
1
Iteration 21400: Loss = -12329.3046875
2
Iteration 21500: Loss = -12329.3046875
3
Iteration 21600: Loss = -12329.3037109375
Iteration 21700: Loss = -12329.3017578125
Iteration 21800: Loss = -12329.29296875
Iteration 21900: Loss = -12329.2529296875
Iteration 22000: Loss = -12329.240234375
Iteration 22100: Loss = -12329.2373046875
Iteration 22200: Loss = -12329.2353515625
Iteration 22300: Loss = -12329.2314453125
Iteration 22400: Loss = -12329.232421875
1
Iteration 22500: Loss = -12329.23046875
Iteration 22600: Loss = -12329.2294921875
Iteration 22700: Loss = -12329.228515625
Iteration 22800: Loss = -12329.2294921875
1
Iteration 22900: Loss = -12329.228515625
Iteration 23000: Loss = -12329.228515625
Iteration 23100: Loss = -12329.2275390625
Iteration 23200: Loss = -12329.2275390625
Iteration 23300: Loss = -12329.2265625
Iteration 23400: Loss = -12329.2275390625
1
Iteration 23500: Loss = -12329.2265625
Iteration 23600: Loss = -12329.2275390625
1
Iteration 23700: Loss = -12329.228515625
2
Iteration 23800: Loss = -12329.2275390625
3
Iteration 23900: Loss = -12329.228515625
4
Iteration 24000: Loss = -12329.2275390625
5
Iteration 24100: Loss = -12329.2265625
Iteration 24200: Loss = -12329.2255859375
Iteration 24300: Loss = -12329.2265625
1
Iteration 24400: Loss = -12329.228515625
2
Iteration 24500: Loss = -12329.2255859375
Iteration 24600: Loss = -12329.2265625
1
Iteration 24700: Loss = -12329.2265625
2
Iteration 24800: Loss = -12329.228515625
3
Iteration 24900: Loss = -12329.2265625
4
Iteration 25000: Loss = -12329.2275390625
5
Iteration 25100: Loss = -12329.228515625
6
Iteration 25200: Loss = -12329.2265625
7
Iteration 25300: Loss = -12329.2265625
8
Iteration 25400: Loss = -12329.2255859375
Iteration 25500: Loss = -12329.2275390625
1
Iteration 25600: Loss = -12329.2255859375
Iteration 25700: Loss = -12329.2275390625
1
Iteration 25800: Loss = -12329.224609375
Iteration 25900: Loss = -12329.2265625
1
Iteration 26000: Loss = -12329.2255859375
2
Iteration 26100: Loss = -12329.2255859375
3
Iteration 26200: Loss = -12329.228515625
4
Iteration 26300: Loss = -12329.2255859375
5
Iteration 26400: Loss = -12329.2255859375
6
Iteration 26500: Loss = -12329.2255859375
7
Iteration 26600: Loss = -12329.2265625
8
Iteration 26700: Loss = -12329.2255859375
9
Iteration 26800: Loss = -12329.2265625
10
Iteration 26900: Loss = -12329.228515625
11
Iteration 27000: Loss = -12329.2255859375
12
Iteration 27100: Loss = -12329.2255859375
13
Iteration 27200: Loss = -12329.2265625
14
Iteration 27300: Loss = -12329.2265625
15
Stopping early at iteration 27300 due to no improvement.
pi: tensor([[1.0000e+00, 8.8639e-07],
        [9.9960e-01, 3.9777e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0856, 0.9144], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1989, 0.1970],
         [0.0198, 0.1962]],

        [[0.1552, 0.1963],
         [0.0132, 0.0477]],

        [[0.9844, 0.2330],
         [0.9198, 0.0403]],

        [[0.8311, 0.1996],
         [0.9640, 0.9921]],

        [[0.7046, 0.2427],
         [0.6400, 0.0177]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0026107014482745863
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37284.0546875
Iteration 100: Loss = -21582.525390625
Iteration 200: Loss = -14230.947265625
Iteration 300: Loss = -12981.8115234375
Iteration 400: Loss = -12677.158203125
Iteration 500: Loss = -12547.7841796875
Iteration 600: Loss = -12459.455078125
Iteration 700: Loss = -12429.2236328125
Iteration 800: Loss = -12409.669921875
Iteration 900: Loss = -12391.6708984375
Iteration 1000: Loss = -12382.6494140625
Iteration 1100: Loss = -12375.4404296875
Iteration 1200: Loss = -12369.763671875
Iteration 1300: Loss = -12365.51953125
Iteration 1400: Loss = -12358.05078125
Iteration 1500: Loss = -12351.8564453125
Iteration 1600: Loss = -12349.013671875
Iteration 1700: Loss = -12346.42578125
Iteration 1800: Loss = -12344.318359375
Iteration 1900: Loss = -12342.703125
Iteration 2000: Loss = -12341.40234375
Iteration 2100: Loss = -12340.30859375
Iteration 2200: Loss = -12339.3642578125
Iteration 2300: Loss = -12338.5341796875
Iteration 2400: Loss = -12337.8017578125
Iteration 2500: Loss = -12337.1484375
Iteration 2600: Loss = -12336.5654296875
Iteration 2700: Loss = -12336.0400390625
Iteration 2800: Loss = -12335.56640625
Iteration 2900: Loss = -12335.1328125
Iteration 3000: Loss = -12334.7392578125
Iteration 3100: Loss = -12334.3798828125
Iteration 3200: Loss = -12334.0498046875
Iteration 3300: Loss = -12333.748046875
Iteration 3400: Loss = -12333.46875
Iteration 3500: Loss = -12333.2138671875
Iteration 3600: Loss = -12332.974609375
Iteration 3700: Loss = -12332.75390625
Iteration 3800: Loss = -12332.5478515625
Iteration 3900: Loss = -12332.3583984375
Iteration 4000: Loss = -12332.1796875
Iteration 4100: Loss = -12332.0146484375
Iteration 4200: Loss = -12331.8583984375
Iteration 4300: Loss = -12331.7138671875
Iteration 4400: Loss = -12331.580078125
Iteration 4500: Loss = -12331.4560546875
Iteration 4600: Loss = -12331.3359375
Iteration 4700: Loss = -12331.224609375
Iteration 4800: Loss = -12331.119140625
Iteration 4900: Loss = -12331.0224609375
Iteration 5000: Loss = -12330.931640625
Iteration 5100: Loss = -12330.845703125
Iteration 5200: Loss = -12330.763671875
Iteration 5300: Loss = -12330.6875
Iteration 5400: Loss = -12330.6142578125
Iteration 5500: Loss = -12330.544921875
Iteration 5600: Loss = -12330.4833984375
Iteration 5700: Loss = -12330.4228515625
Iteration 5800: Loss = -12330.365234375
Iteration 5900: Loss = -12330.3095703125
Iteration 6000: Loss = -12330.259765625
Iteration 6100: Loss = -12330.2119140625
Iteration 6200: Loss = -12330.166015625
Iteration 6300: Loss = -12330.1220703125
Iteration 6400: Loss = -12330.080078125
Iteration 6500: Loss = -12330.0400390625
Iteration 6600: Loss = -12330.0048828125
Iteration 6700: Loss = -12329.9697265625
Iteration 6800: Loss = -12329.9365234375
Iteration 6900: Loss = -12329.9052734375
Iteration 7000: Loss = -12329.8759765625
Iteration 7100: Loss = -12329.8466796875
Iteration 7200: Loss = -12329.8193359375
Iteration 7300: Loss = -12329.79296875
Iteration 7400: Loss = -12329.7685546875
Iteration 7500: Loss = -12329.74609375
Iteration 7600: Loss = -12329.7255859375
Iteration 7700: Loss = -12329.7041015625
Iteration 7800: Loss = -12329.68359375
Iteration 7900: Loss = -12329.666015625
Iteration 8000: Loss = -12329.6484375
Iteration 8100: Loss = -12329.630859375
Iteration 8200: Loss = -12329.615234375
Iteration 8300: Loss = -12329.599609375
Iteration 8400: Loss = -12329.583984375
Iteration 8500: Loss = -12329.5712890625
Iteration 8600: Loss = -12329.5576171875
Iteration 8700: Loss = -12329.544921875
Iteration 8800: Loss = -12329.5322265625
Iteration 8900: Loss = -12329.521484375
Iteration 9000: Loss = -12329.51171875
Iteration 9100: Loss = -12329.4990234375
Iteration 9200: Loss = -12329.490234375
Iteration 9300: Loss = -12329.4814453125
Iteration 9400: Loss = -12329.4716796875
Iteration 9500: Loss = -12329.4638671875
Iteration 9600: Loss = -12329.45703125
Iteration 9700: Loss = -12329.4453125
Iteration 9800: Loss = -12329.4404296875
Iteration 9900: Loss = -12329.4306640625
Iteration 10000: Loss = -12329.4228515625
Iteration 10100: Loss = -12329.416015625
Iteration 10200: Loss = -12329.408203125
Iteration 10300: Loss = -12329.40234375
Iteration 10400: Loss = -12329.3955078125
Iteration 10500: Loss = -12329.3876953125
Iteration 10600: Loss = -12329.3818359375
Iteration 10700: Loss = -12329.3759765625
Iteration 10800: Loss = -12329.3701171875
Iteration 10900: Loss = -12329.3642578125
Iteration 11000: Loss = -12329.3583984375
Iteration 11100: Loss = -12329.3544921875
Iteration 11200: Loss = -12329.34765625
Iteration 11300: Loss = -12329.3466796875
Iteration 11400: Loss = -12329.341796875
Iteration 11500: Loss = -12329.3388671875
Iteration 11600: Loss = -12329.333984375
Iteration 11700: Loss = -12329.333984375
Iteration 11800: Loss = -12329.3310546875
Iteration 11900: Loss = -12329.3291015625
Iteration 12000: Loss = -12329.328125
Iteration 12100: Loss = -12329.3251953125
Iteration 12200: Loss = -12329.32421875
Iteration 12300: Loss = -12329.3232421875
Iteration 12400: Loss = -12329.3203125
Iteration 12500: Loss = -12329.3193359375
Iteration 12600: Loss = -12329.318359375
Iteration 12700: Loss = -12329.318359375
Iteration 12800: Loss = -12329.3154296875
Iteration 12900: Loss = -12329.31640625
1
Iteration 13000: Loss = -12329.314453125
Iteration 13100: Loss = -12329.314453125
Iteration 13200: Loss = -12329.3134765625
Iteration 13300: Loss = -12329.3134765625
Iteration 13400: Loss = -12329.3115234375
Iteration 13500: Loss = -12329.3115234375
Iteration 13600: Loss = -12329.3125
1
Iteration 13700: Loss = -12329.310546875
Iteration 13800: Loss = -12329.3095703125
Iteration 13900: Loss = -12329.310546875
1
Iteration 14000: Loss = -12329.3095703125
Iteration 14100: Loss = -12329.3076171875
Iteration 14200: Loss = -12329.3095703125
1
Iteration 14300: Loss = -12329.30859375
2
Iteration 14400: Loss = -12329.3095703125
3
Iteration 14500: Loss = -12329.3056640625
Iteration 14600: Loss = -12329.306640625
1
Iteration 14700: Loss = -12329.306640625
2
Iteration 14800: Loss = -12329.306640625
3
Iteration 14900: Loss = -12329.3046875
Iteration 15000: Loss = -12329.3056640625
1
Iteration 15100: Loss = -12329.306640625
2
Iteration 15200: Loss = -12329.3056640625
3
Iteration 15300: Loss = -12329.3046875
Iteration 15400: Loss = -12329.306640625
1
Iteration 15500: Loss = -12329.3037109375
Iteration 15600: Loss = -12329.3037109375
Iteration 15700: Loss = -12329.306640625
1
Iteration 15800: Loss = -12329.3037109375
Iteration 15900: Loss = -12329.3037109375
Iteration 16000: Loss = -12329.3037109375
Iteration 16100: Loss = -12329.3037109375
Iteration 16200: Loss = -12329.3046875
1
Iteration 16300: Loss = -12329.3037109375
Iteration 16400: Loss = -12329.3056640625
1
Iteration 16500: Loss = -12329.3046875
2
Iteration 16600: Loss = -12329.3037109375
Iteration 16700: Loss = -12329.302734375
Iteration 16800: Loss = -12329.302734375
Iteration 16900: Loss = -12329.310546875
1
Iteration 17000: Loss = -12329.3017578125
Iteration 17100: Loss = -12329.3017578125
Iteration 17200: Loss = -12329.302734375
1
Iteration 17300: Loss = -12329.302734375
2
Iteration 17400: Loss = -12329.3037109375
3
Iteration 17500: Loss = -12329.302734375
4
Iteration 17600: Loss = -12329.30078125
Iteration 17700: Loss = -12329.3017578125
1
Iteration 17800: Loss = -12329.30078125
Iteration 17900: Loss = -12329.30078125
Iteration 18000: Loss = -12329.3017578125
1
Iteration 18100: Loss = -12329.30078125
Iteration 18200: Loss = -12329.30078125
Iteration 18300: Loss = -12329.2998046875
Iteration 18400: Loss = -12329.30078125
1
Iteration 18500: Loss = -12329.2998046875
Iteration 18600: Loss = -12329.30078125
1
Iteration 18700: Loss = -12329.3017578125
2
Iteration 18800: Loss = -12329.30078125
3
Iteration 18900: Loss = -12329.30078125
4
Iteration 19000: Loss = -12329.3017578125
5
Iteration 19100: Loss = -12329.30078125
6
Iteration 19200: Loss = -12329.298828125
Iteration 19300: Loss = -12329.298828125
Iteration 19400: Loss = -12329.2998046875
1
Iteration 19500: Loss = -12329.30078125
2
Iteration 19600: Loss = -12329.2998046875
3
Iteration 19700: Loss = -12329.298828125
Iteration 19800: Loss = -12329.298828125
Iteration 19900: Loss = -12329.296875
Iteration 20000: Loss = -12329.296875
Iteration 20100: Loss = -12329.2958984375
Iteration 20200: Loss = -12329.2939453125
Iteration 20300: Loss = -12329.2890625
Iteration 20400: Loss = -12329.283203125
Iteration 20500: Loss = -12329.25
Iteration 20600: Loss = -12329.1337890625
Iteration 20700: Loss = -12329.103515625
Iteration 20800: Loss = -12329.095703125
Iteration 20900: Loss = -12329.0947265625
Iteration 21000: Loss = -12329.09375
Iteration 21100: Loss = -12329.09375
Iteration 21200: Loss = -12329.09375
Iteration 21300: Loss = -12329.0888671875
Iteration 21400: Loss = -12329.0830078125
Iteration 21500: Loss = -12329.0703125
Iteration 21600: Loss = -12329.05859375
Iteration 21700: Loss = -12329.0498046875
Iteration 21800: Loss = -12329.0419921875
Iteration 21900: Loss = -12329.0341796875
Iteration 22000: Loss = -12329.025390625
Iteration 22100: Loss = -12329.021484375
Iteration 22200: Loss = -12329.015625
Iteration 22300: Loss = -12329.0126953125
Iteration 22400: Loss = -12329.0107421875
Iteration 22500: Loss = -12329.009765625
Iteration 22600: Loss = -12329.0087890625
Iteration 22700: Loss = -12329.005859375
Iteration 22800: Loss = -12329.0
Iteration 22900: Loss = -12328.95703125
Iteration 23000: Loss = -12327.3701171875
Iteration 23100: Loss = -12327.1982421875
Iteration 23200: Loss = -12327.173828125
Iteration 23300: Loss = -12327.162109375
Iteration 23400: Loss = -12327.1552734375
Iteration 23500: Loss = -12327.1533203125
Iteration 23600: Loss = -12327.1494140625
Iteration 23700: Loss = -12327.146484375
Iteration 23800: Loss = -12327.146484375
Iteration 23900: Loss = -12327.14453125
Iteration 24000: Loss = -12327.1435546875
Iteration 24100: Loss = -12327.14453125
1
Iteration 24200: Loss = -12327.142578125
Iteration 24300: Loss = -12327.142578125
Iteration 24400: Loss = -12327.140625
Iteration 24500: Loss = -12327.1416015625
1
Iteration 24600: Loss = -12327.1396484375
Iteration 24700: Loss = -12327.1416015625
1
Iteration 24800: Loss = -12327.1416015625
2
Iteration 24900: Loss = -12327.1396484375
Iteration 25000: Loss = -12327.1396484375
Iteration 25100: Loss = -12327.1396484375
Iteration 25200: Loss = -12327.1396484375
Iteration 25300: Loss = -12327.1376953125
Iteration 25400: Loss = -12327.1376953125
Iteration 25500: Loss = -12327.13671875
Iteration 25600: Loss = -12327.138671875
1
Iteration 25700: Loss = -12327.13671875
Iteration 25800: Loss = -12327.138671875
1
Iteration 25900: Loss = -12327.138671875
2
Iteration 26000: Loss = -12327.138671875
3
Iteration 26100: Loss = -12327.1376953125
4
Iteration 26200: Loss = -12327.1376953125
5
Iteration 26300: Loss = -12327.1376953125
6
Iteration 26400: Loss = -12327.13671875
Iteration 26500: Loss = -12327.1376953125
1
Iteration 26600: Loss = -12327.1376953125
2
Iteration 26700: Loss = -12327.1376953125
3
Iteration 26800: Loss = -12327.138671875
4
Iteration 26900: Loss = -12327.13671875
Iteration 27000: Loss = -12327.13671875
Iteration 27100: Loss = -12327.138671875
1
Iteration 27200: Loss = -12327.13671875
Iteration 27300: Loss = -12327.13671875
Iteration 27400: Loss = -12327.13671875
Iteration 27500: Loss = -12327.138671875
1
Iteration 27600: Loss = -12327.1376953125
2
Iteration 27700: Loss = -12327.13671875
Iteration 27800: Loss = -12327.13671875
Iteration 27900: Loss = -12327.1376953125
1
Iteration 28000: Loss = -12327.1357421875
Iteration 28100: Loss = -12327.13671875
1
Iteration 28200: Loss = -12327.13671875
2
Iteration 28300: Loss = -12327.1376953125
3
Iteration 28400: Loss = -12327.13671875
4
Iteration 28500: Loss = -12327.138671875
5
Iteration 28600: Loss = -12327.1376953125
6
Iteration 28700: Loss = -12327.1376953125
7
Iteration 28800: Loss = -12327.13671875
8
Iteration 28900: Loss = -12327.13671875
9
Iteration 29000: Loss = -12327.13671875
10
Iteration 29100: Loss = -12327.1376953125
11
Iteration 29200: Loss = -12327.1376953125
12
Iteration 29300: Loss = -12327.13671875
13
Iteration 29400: Loss = -12327.13671875
14
Iteration 29500: Loss = -12327.13671875
15
Stopping early at iteration 29500 due to no improvement.
pi: tensor([[8.5476e-06, 9.9999e-01],
        [9.9999e-01, 5.1786e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9666, 0.0334], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1985, 0.1552],
         [0.9803, 0.2011]],

        [[0.9779, 0.1852],
         [0.9621, 0.0363]],

        [[0.2716, 0.1425],
         [0.1120, 0.9477]],

        [[0.0098, 0.1646],
         [0.4800, 0.3899]],

        [[0.8655, 0.2595],
         [0.9868, 0.8892]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: -0.003937327268695544
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.012298440577296121
Global Adjusted Rand Index: -0.0006577071124670139
Average Adjusted Rand Index: -0.0036384408812710468
[0.0026107014482745863, -0.0006577071124670139] [0.0, -0.0036384408812710468] [12329.2265625, 12327.13671875]
-------------------------------------
This iteration is 92
True Objective function: Loss = -11935.14605212653
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36922.1796875
Iteration 100: Loss = -23243.390625
Iteration 200: Loss = -14772.8349609375
Iteration 300: Loss = -13151.283203125
Iteration 400: Loss = -12817.50390625
Iteration 500: Loss = -12679.6484375
Iteration 600: Loss = -12594.67578125
Iteration 700: Loss = -12548.5634765625
Iteration 800: Loss = -12516.388671875
Iteration 900: Loss = -12498.0537109375
Iteration 1000: Loss = -12484.4931640625
Iteration 1100: Loss = -12474.0673828125
Iteration 1200: Loss = -12466.34765625
Iteration 1300: Loss = -12449.216796875
Iteration 1400: Loss = -12443.55078125
Iteration 1500: Loss = -12439.87109375
Iteration 1600: Loss = -12436.9599609375
Iteration 1700: Loss = -12434.5517578125
Iteration 1800: Loss = -12432.4775390625
Iteration 1900: Loss = -12430.669921875
Iteration 2000: Loss = -12429.1162109375
Iteration 2100: Loss = -12427.7099609375
Iteration 2200: Loss = -12426.2392578125
Iteration 2300: Loss = -12424.62109375
Iteration 2400: Loss = -12423.39453125
Iteration 2500: Loss = -12422.44140625
Iteration 2600: Loss = -12421.6279296875
Iteration 2700: Loss = -12420.9306640625
Iteration 2800: Loss = -12420.302734375
Iteration 2900: Loss = -12419.7490234375
Iteration 3000: Loss = -12419.2587890625
Iteration 3100: Loss = -12418.8212890625
Iteration 3200: Loss = -12418.423828125
Iteration 3300: Loss = -12418.0625
Iteration 3400: Loss = -12417.7353515625
Iteration 3500: Loss = -12417.4384765625
Iteration 3600: Loss = -12417.166015625
Iteration 3700: Loss = -12416.9130859375
Iteration 3800: Loss = -12416.68359375
Iteration 3900: Loss = -12416.46875
Iteration 4000: Loss = -12416.2705078125
Iteration 4100: Loss = -12415.2705078125
Iteration 4200: Loss = -12411.7451171875
Iteration 4300: Loss = -12411.443359375
Iteration 4400: Loss = -12411.220703125
Iteration 4500: Loss = -12406.970703125
Iteration 4600: Loss = -12406.7197265625
Iteration 4700: Loss = -12406.5419921875
Iteration 4800: Loss = -12406.390625
Iteration 4900: Loss = -12406.2578125
Iteration 5000: Loss = -12406.1376953125
Iteration 5100: Loss = -12406.0302734375
Iteration 5200: Loss = -12405.92578125
Iteration 5300: Loss = -12405.8349609375
Iteration 5400: Loss = -12405.7470703125
Iteration 5500: Loss = -12405.6689453125
Iteration 5600: Loss = -12405.5947265625
Iteration 5700: Loss = -12405.5244140625
Iteration 5800: Loss = -12405.4580078125
Iteration 5900: Loss = -12405.3984375
Iteration 6000: Loss = -12405.3388671875
Iteration 6100: Loss = -12405.2861328125
Iteration 6200: Loss = -12405.2392578125
Iteration 6300: Loss = -12405.1865234375
Iteration 6400: Loss = -12405.1396484375
Iteration 6500: Loss = -12405.09765625
Iteration 6600: Loss = -12405.0537109375
Iteration 6700: Loss = -12405.015625
Iteration 6800: Loss = -12404.978515625
Iteration 6900: Loss = -12404.9453125
Iteration 7000: Loss = -12404.912109375
Iteration 7100: Loss = -12404.8818359375
Iteration 7200: Loss = -12404.853515625
Iteration 7300: Loss = -12404.826171875
Iteration 7400: Loss = -12404.798828125
Iteration 7500: Loss = -12404.7724609375
Iteration 7600: Loss = -12404.7490234375
Iteration 7700: Loss = -12404.7275390625
Iteration 7800: Loss = -12404.7060546875
Iteration 7900: Loss = -12404.6845703125
Iteration 8000: Loss = -12404.66796875
Iteration 8100: Loss = -12404.6484375
Iteration 8200: Loss = -12404.6328125
Iteration 8300: Loss = -12404.6162109375
Iteration 8400: Loss = -12404.6015625
Iteration 8500: Loss = -12404.5869140625
Iteration 8600: Loss = -12404.57421875
Iteration 8700: Loss = -12404.5615234375
Iteration 8800: Loss = -12404.5478515625
Iteration 8900: Loss = -12404.5341796875
Iteration 9000: Loss = -12404.5244140625
Iteration 9100: Loss = -12404.51171875
Iteration 9200: Loss = -12404.5029296875
Iteration 9300: Loss = -12404.4921875
Iteration 9400: Loss = -12404.482421875
Iteration 9500: Loss = -12404.47265625
Iteration 9600: Loss = -12404.4658203125
Iteration 9700: Loss = -12404.455078125
Iteration 9800: Loss = -12404.44921875
Iteration 9900: Loss = -12404.439453125
Iteration 10000: Loss = -12404.4326171875
Iteration 10100: Loss = -12404.4248046875
Iteration 10200: Loss = -12404.41796875
Iteration 10300: Loss = -12404.41015625
Iteration 10400: Loss = -12404.4072265625
Iteration 10500: Loss = -12404.3984375
Iteration 10600: Loss = -12404.392578125
Iteration 10700: Loss = -12404.3857421875
Iteration 10800: Loss = -12404.380859375
Iteration 10900: Loss = -12404.375
Iteration 11000: Loss = -12404.369140625
Iteration 11100: Loss = -12404.36328125
Iteration 11200: Loss = -12404.3583984375
Iteration 11300: Loss = -12404.3515625
Iteration 11400: Loss = -12404.3486328125
Iteration 11500: Loss = -12404.3447265625
Iteration 11600: Loss = -12404.3388671875
Iteration 11700: Loss = -12404.3349609375
Iteration 11800: Loss = -12404.3291015625
Iteration 11900: Loss = -12404.3251953125
Iteration 12000: Loss = -12404.3203125
Iteration 12100: Loss = -12404.3154296875
Iteration 12200: Loss = -12404.3095703125
Iteration 12300: Loss = -12404.306640625
Iteration 12400: Loss = -12404.30078125
Iteration 12500: Loss = -12404.2958984375
Iteration 12600: Loss = -12404.2919921875
Iteration 12700: Loss = -12404.2890625
Iteration 12800: Loss = -12404.2841796875
Iteration 12900: Loss = -12404.28125
Iteration 13000: Loss = -12404.2763671875
Iteration 13100: Loss = -12404.2724609375
Iteration 13200: Loss = -12404.26953125
Iteration 13300: Loss = -12404.267578125
Iteration 13400: Loss = -12404.263671875
Iteration 13500: Loss = -12404.2607421875
Iteration 13600: Loss = -12404.25390625
Iteration 13700: Loss = -12404.251953125
Iteration 13800: Loss = -12404.248046875
Iteration 13900: Loss = -12404.244140625
Iteration 14000: Loss = -12404.2392578125
Iteration 14100: Loss = -12404.2333984375
Iteration 14200: Loss = -12404.2265625
Iteration 14300: Loss = -12404.22265625
Iteration 14400: Loss = -12404.2158203125
Iteration 14500: Loss = -12404.2080078125
Iteration 14600: Loss = -12404.1982421875
Iteration 14700: Loss = -12404.1904296875
Iteration 14800: Loss = -12404.177734375
Iteration 14900: Loss = -12404.1640625
Iteration 15000: Loss = -12404.1435546875
Iteration 15100: Loss = -12404.11328125
Iteration 15200: Loss = -12403.931640625
Iteration 15300: Loss = -12402.919921875
Iteration 15400: Loss = -12399.2880859375
Iteration 15500: Loss = -12398.931640625
Iteration 15600: Loss = -12398.9189453125
Iteration 15700: Loss = -12398.9140625
Iteration 15800: Loss = -12398.912109375
Iteration 15900: Loss = -12398.908203125
Iteration 16000: Loss = -12398.9052734375
Iteration 16100: Loss = -12398.9013671875
Iteration 16200: Loss = -12398.896484375
Iteration 16300: Loss = -12398.8818359375
Iteration 16400: Loss = -12398.8837890625
1
Iteration 16500: Loss = -12398.87890625
Iteration 16600: Loss = -12398.873046875
Iteration 16700: Loss = -12398.8720703125
Iteration 16800: Loss = -12398.87109375
Iteration 16900: Loss = -12398.8701171875
Iteration 17000: Loss = -12398.8720703125
1
Iteration 17100: Loss = -12398.8701171875
Iteration 17200: Loss = -12398.869140625
Iteration 17300: Loss = -12398.869140625
Iteration 17400: Loss = -12398.8701171875
1
Iteration 17500: Loss = -12398.869140625
Iteration 17600: Loss = -12398.8681640625
Iteration 17700: Loss = -12398.869140625
1
Iteration 17800: Loss = -12398.8701171875
2
Iteration 17900: Loss = -12398.8681640625
Iteration 18000: Loss = -12398.8671875
Iteration 18100: Loss = -12398.8681640625
1
Iteration 18200: Loss = -12398.869140625
2
Iteration 18300: Loss = -12398.8671875
Iteration 18400: Loss = -12398.8671875
Iteration 18500: Loss = -12398.8681640625
1
Iteration 18600: Loss = -12398.8662109375
Iteration 18700: Loss = -12398.8671875
1
Iteration 18800: Loss = -12398.8671875
2
Iteration 18900: Loss = -12398.8671875
3
Iteration 19000: Loss = -12398.8671875
4
Iteration 19100: Loss = -12398.8662109375
Iteration 19200: Loss = -12398.8662109375
Iteration 19300: Loss = -12398.8662109375
Iteration 19400: Loss = -12398.8671875
1
Iteration 19500: Loss = -12398.8671875
2
Iteration 19600: Loss = -12398.8662109375
Iteration 19700: Loss = -12398.8671875
1
Iteration 19800: Loss = -12398.8662109375
Iteration 19900: Loss = -12398.865234375
Iteration 20000: Loss = -12398.8671875
1
Iteration 20100: Loss = -12398.8662109375
2
Iteration 20200: Loss = -12398.865234375
Iteration 20300: Loss = -12398.8681640625
1
Iteration 20400: Loss = -12398.8662109375
2
Iteration 20500: Loss = -12398.8671875
3
Iteration 20600: Loss = -12398.865234375
Iteration 20700: Loss = -12398.8662109375
1
Iteration 20800: Loss = -12398.8662109375
2
Iteration 20900: Loss = -12398.865234375
Iteration 21000: Loss = -12398.865234375
Iteration 21100: Loss = -12398.8662109375
1
Iteration 21200: Loss = -12398.865234375
Iteration 21300: Loss = -12398.8671875
1
Iteration 21400: Loss = -12398.865234375
Iteration 21500: Loss = -12398.8662109375
1
Iteration 21600: Loss = -12398.8642578125
Iteration 21700: Loss = -12398.8662109375
1
Iteration 21800: Loss = -12398.865234375
2
Iteration 21900: Loss = -12398.8662109375
3
Iteration 22000: Loss = -12398.8662109375
4
Iteration 22100: Loss = -12398.8662109375
5
Iteration 22200: Loss = -12398.865234375
6
Iteration 22300: Loss = -12398.865234375
7
Iteration 22400: Loss = -12398.865234375
8
Iteration 22500: Loss = -12398.8671875
9
Iteration 22600: Loss = -12398.8662109375
10
Iteration 22700: Loss = -12398.865234375
11
Iteration 22800: Loss = -12398.8671875
12
Iteration 22900: Loss = -12398.8642578125
Iteration 23000: Loss = -12398.8662109375
1
Iteration 23100: Loss = -12398.8671875
2
Iteration 23200: Loss = -12398.865234375
3
Iteration 23300: Loss = -12398.865234375
4
Iteration 23400: Loss = -12398.8662109375
5
Iteration 23500: Loss = -12398.8642578125
Iteration 23600: Loss = -12398.865234375
1
Iteration 23700: Loss = -12398.8662109375
2
Iteration 23800: Loss = -12398.8671875
3
Iteration 23900: Loss = -12398.865234375
4
Iteration 24000: Loss = -12398.865234375
5
Iteration 24100: Loss = -12398.865234375
6
Iteration 24200: Loss = -12398.865234375
7
Iteration 24300: Loss = -12398.865234375
8
Iteration 24400: Loss = -12398.8642578125
Iteration 24500: Loss = -12398.8662109375
1
Iteration 24600: Loss = -12398.865234375
2
Iteration 24700: Loss = -12398.8662109375
3
Iteration 24800: Loss = -12398.8671875
4
Iteration 24900: Loss = -12398.8671875
5
Iteration 25000: Loss = -12398.865234375
6
Iteration 25100: Loss = -12398.865234375
7
Iteration 25200: Loss = -12398.8662109375
8
Iteration 25300: Loss = -12398.8662109375
9
Iteration 25400: Loss = -12398.8662109375
10
Iteration 25500: Loss = -12398.865234375
11
Iteration 25600: Loss = -12398.865234375
12
Iteration 25700: Loss = -12398.865234375
13
Iteration 25800: Loss = -12398.865234375
14
Iteration 25900: Loss = -12398.8642578125
Iteration 26000: Loss = -12398.8671875
1
Iteration 26100: Loss = -12398.865234375
2
Iteration 26200: Loss = -12398.8662109375
3
Iteration 26300: Loss = -12398.8662109375
4
Iteration 26400: Loss = -12398.8662109375
5
Iteration 26500: Loss = -12398.8662109375
6
Iteration 26600: Loss = -12398.865234375
7
Iteration 26700: Loss = -12398.8662109375
8
Iteration 26800: Loss = -12398.865234375
9
Iteration 26900: Loss = -12398.865234375
10
Iteration 27000: Loss = -12398.8662109375
11
Iteration 27100: Loss = -12398.8662109375
12
Iteration 27200: Loss = -12398.865234375
13
Iteration 27300: Loss = -12398.8662109375
14
Iteration 27400: Loss = -12398.8642578125
Iteration 27500: Loss = -12398.8662109375
1
Iteration 27600: Loss = -12398.8642578125
Iteration 27700: Loss = -12398.865234375
1
Iteration 27800: Loss = -12398.865234375
2
Iteration 27900: Loss = -12398.865234375
3
Iteration 28000: Loss = -12398.865234375
4
Iteration 28100: Loss = -12398.8662109375
5
Iteration 28200: Loss = -12398.8662109375
6
Iteration 28300: Loss = -12398.865234375
7
Iteration 28400: Loss = -12398.865234375
8
Iteration 28500: Loss = -12398.865234375
9
Iteration 28600: Loss = -12398.8662109375
10
Iteration 28700: Loss = -12398.8642578125
Iteration 28800: Loss = -12398.8662109375
1
Iteration 28900: Loss = -12398.865234375
2
Iteration 29000: Loss = -12398.8671875
3
Iteration 29100: Loss = -12398.865234375
4
Iteration 29200: Loss = -12398.865234375
5
Iteration 29300: Loss = -12398.865234375
6
Iteration 29400: Loss = -12398.8642578125
Iteration 29500: Loss = -12398.8642578125
Iteration 29600: Loss = -12398.865234375
1
Iteration 29700: Loss = -12398.865234375
2
Iteration 29800: Loss = -12398.865234375
3
Iteration 29900: Loss = -12398.8671875
4
pi: tensor([[1.0000e+00, 2.8324e-06],
        [4.1739e-01, 5.8261e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8972, 0.1028], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2029, 0.1333],
         [0.9845, 0.4721]],

        [[0.7806, 0.1358],
         [0.0638, 0.9664]],

        [[0.7879, 0.2125],
         [0.0870, 0.0127]],

        [[0.0335, 0.3129],
         [0.1971, 0.0167]],

        [[0.3102, 0.1948],
         [0.9836, 0.8960]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.02918749315918129
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.001684040076915292
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 3
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.00035341041046094813
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0059174737657959825
Global Adjusted Rand Index: -0.0028171196126616236
Average Adjusted Rand Index: 0.006701210755197975
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -40714.23046875
Iteration 100: Loss = -27418.32421875
Iteration 200: Loss = -16410.46484375
Iteration 300: Loss = -13364.2177734375
Iteration 400: Loss = -12931.2255859375
Iteration 500: Loss = -12792.322265625
Iteration 600: Loss = -12724.0146484375
Iteration 700: Loss = -12679.8583984375
Iteration 800: Loss = -12647.5966796875
Iteration 900: Loss = -12626.5048828125
Iteration 1000: Loss = -12609.2744140625
Iteration 1100: Loss = -12595.7080078125
Iteration 1200: Loss = -12585.1357421875
Iteration 1300: Loss = -12575.5146484375
Iteration 1400: Loss = -12567.36328125
Iteration 1500: Loss = -12560.755859375
Iteration 1600: Loss = -12554.4013671875
Iteration 1700: Loss = -12550.181640625
Iteration 1800: Loss = -12546.64453125
Iteration 1900: Loss = -12543.5361328125
Iteration 2000: Loss = -12540.37109375
Iteration 2100: Loss = -12537.123046875
Iteration 2200: Loss = -12533.27734375
Iteration 2300: Loss = -12529.578125
Iteration 2400: Loss = -12526.75390625
Iteration 2500: Loss = -12524.4541015625
Iteration 2600: Loss = -12522.5927734375
Iteration 2700: Loss = -12520.7294921875
Iteration 2800: Loss = -12519.19140625
Iteration 2900: Loss = -12517.8173828125
Iteration 3000: Loss = -12516.0732421875
Iteration 3100: Loss = -12514.6376953125
Iteration 3200: Loss = -12513.689453125
Iteration 3300: Loss = -12512.7373046875
Iteration 3400: Loss = -12511.658203125
Iteration 3500: Loss = -12510.4755859375
Iteration 3600: Loss = -12507.837890625
Iteration 3700: Loss = -12506.37109375
Iteration 3800: Loss = -12505.6591796875
Iteration 3900: Loss = -12504.8173828125
Iteration 4000: Loss = -12503.9365234375
Iteration 4100: Loss = -12502.7265625
Iteration 4200: Loss = -12501.212890625
Iteration 4300: Loss = -12499.9296875
Iteration 4400: Loss = -12499.3310546875
Iteration 4500: Loss = -12498.6162109375
Iteration 4600: Loss = -12498.0712890625
Iteration 4700: Loss = -12497.708984375
Iteration 4800: Loss = -12497.3876953125
Iteration 4900: Loss = -12496.955078125
Iteration 5000: Loss = -12496.4609375
Iteration 5100: Loss = -12496.220703125
Iteration 5200: Loss = -12495.8935546875
Iteration 5300: Loss = -12495.669921875
Iteration 5400: Loss = -12495.4052734375
Iteration 5500: Loss = -12495.076171875
Iteration 5600: Loss = -12494.2998046875
Iteration 5700: Loss = -12493.423828125
Iteration 5800: Loss = -12493.16015625
Iteration 5900: Loss = -12493.0
Iteration 6000: Loss = -12492.87890625
Iteration 6100: Loss = -12492.7734375
Iteration 6200: Loss = -12492.6640625
Iteration 6300: Loss = -12492.5732421875
Iteration 6400: Loss = -12491.779296875
Iteration 6500: Loss = -12491.50390625
Iteration 6600: Loss = -12491.0986328125
Iteration 6700: Loss = -12490.9541015625
Iteration 6800: Loss = -12490.791015625
Iteration 6900: Loss = -12490.2197265625
Iteration 7000: Loss = -12490.138671875
Iteration 7100: Loss = -12490.0908203125
Iteration 7200: Loss = -12490.03515625
Iteration 7300: Loss = -12489.8037109375
Iteration 7400: Loss = -12488.9404296875
Iteration 7500: Loss = -12488.8447265625
Iteration 7600: Loss = -12488.794921875
Iteration 7700: Loss = -12488.7548828125
Iteration 7800: Loss = -12488.7216796875
Iteration 7900: Loss = -12488.6904296875
Iteration 8000: Loss = -12488.6611328125
Iteration 8100: Loss = -12488.6357421875
Iteration 8200: Loss = -12488.6123046875
Iteration 8300: Loss = -12488.587890625
Iteration 8400: Loss = -12488.568359375
Iteration 8500: Loss = -12488.546875
Iteration 8600: Loss = -12488.5302734375
Iteration 8700: Loss = -12488.5107421875
Iteration 8800: Loss = -12488.4931640625
Iteration 8900: Loss = -12488.478515625
Iteration 9000: Loss = -12488.4609375
Iteration 9100: Loss = -12488.447265625
Iteration 9200: Loss = -12488.423828125
Iteration 9300: Loss = -12487.7822265625
Iteration 9400: Loss = -12487.7685546875
Iteration 9500: Loss = -12487.755859375
Iteration 9600: Loss = -12487.7451171875
Iteration 9700: Loss = -12487.7353515625
Iteration 9800: Loss = -12487.724609375
Iteration 9900: Loss = -12487.71484375
Iteration 10000: Loss = -12487.70703125
Iteration 10100: Loss = -12487.697265625
Iteration 10200: Loss = -12487.6904296875
Iteration 10300: Loss = -12487.6826171875
Iteration 10400: Loss = -12487.6748046875
Iteration 10500: Loss = -12487.6689453125
Iteration 10600: Loss = -12487.662109375
Iteration 10700: Loss = -12487.6572265625
Iteration 10800: Loss = -12487.6494140625
Iteration 10900: Loss = -12487.64453125
Iteration 11000: Loss = -12487.6396484375
Iteration 11100: Loss = -12487.634765625
Iteration 11200: Loss = -12487.6298828125
Iteration 11300: Loss = -12487.625
Iteration 11400: Loss = -12487.62109375
Iteration 11500: Loss = -12487.6162109375
Iteration 11600: Loss = -12487.611328125
Iteration 11700: Loss = -12486.927734375
Iteration 11800: Loss = -12486.9228515625
Iteration 11900: Loss = -12486.9208984375
Iteration 12000: Loss = -12486.9169921875
Iteration 12100: Loss = -12486.9150390625
Iteration 12200: Loss = -12486.9111328125
Iteration 12300: Loss = -12486.908203125
Iteration 12400: Loss = -12486.90625
Iteration 12500: Loss = -12486.904296875
Iteration 12600: Loss = -12486.90234375
Iteration 12700: Loss = -12486.8984375
Iteration 12800: Loss = -12486.896484375
Iteration 12900: Loss = -12486.8955078125
Iteration 13000: Loss = -12486.892578125
Iteration 13100: Loss = -12486.8916015625
Iteration 13200: Loss = -12486.8896484375
Iteration 13300: Loss = -12486.8876953125
Iteration 13400: Loss = -12486.8857421875
Iteration 13500: Loss = -12486.884765625
Iteration 13600: Loss = -12486.8837890625
Iteration 13700: Loss = -12486.880859375
Iteration 13800: Loss = -12486.880859375
Iteration 13900: Loss = -12486.8798828125
Iteration 14000: Loss = -12486.8779296875
Iteration 14100: Loss = -12486.876953125
Iteration 14200: Loss = -12486.876953125
Iteration 14300: Loss = -12486.80859375
Iteration 14400: Loss = -12486.2080078125
Iteration 14500: Loss = -12486.2099609375
1
Iteration 14600: Loss = -12486.205078125
Iteration 14700: Loss = -12486.2041015625
Iteration 14800: Loss = -12486.2041015625
Iteration 14900: Loss = -12486.2041015625
Iteration 15000: Loss = -12486.205078125
1
Iteration 15100: Loss = -12486.203125
Iteration 15200: Loss = -12486.2021484375
Iteration 15300: Loss = -12486.2001953125
Iteration 15400: Loss = -12486.2001953125
Iteration 15500: Loss = -12486.19921875
Iteration 15600: Loss = -12486.19921875
Iteration 15700: Loss = -12486.19921875
Iteration 15800: Loss = -12486.1982421875
Iteration 15900: Loss = -12486.1982421875
Iteration 16000: Loss = -12486.197265625
Iteration 16100: Loss = -12486.197265625
Iteration 16200: Loss = -12486.1962890625
Iteration 16300: Loss = -12486.1962890625
Iteration 16400: Loss = -12485.5625
Iteration 16500: Loss = -12485.2333984375
Iteration 16600: Loss = -12485.234375
1
Iteration 16700: Loss = -12485.2333984375
Iteration 16800: Loss = -12485.2314453125
Iteration 16900: Loss = -12485.2333984375
1
Iteration 17000: Loss = -12485.2314453125
Iteration 17100: Loss = -12485.2333984375
1
Iteration 17200: Loss = -12485.2314453125
Iteration 17300: Loss = -12484.68359375
Iteration 17400: Loss = -12483.5908203125
Iteration 17500: Loss = -12483.5693359375
Iteration 17600: Loss = -12483.568359375
Iteration 17700: Loss = -12483.2431640625
Iteration 17800: Loss = -12479.423828125
Iteration 17900: Loss = -12478.0615234375
Iteration 18000: Loss = -12474.859375
Iteration 18100: Loss = -12472.900390625
Iteration 18200: Loss = -12470.642578125
Iteration 18300: Loss = -12470.640625
Iteration 18400: Loss = -12469.7919921875
Iteration 18500: Loss = -12469.7158203125
Iteration 18600: Loss = -12469.1083984375
Iteration 18700: Loss = -12469.1103515625
1
Iteration 18800: Loss = -12469.064453125
Iteration 18900: Loss = -12468.7783203125
Iteration 19000: Loss = -12467.5341796875
Iteration 19100: Loss = -12467.330078125
Iteration 19200: Loss = -12466.5322265625
Iteration 19300: Loss = -12464.2392578125
Iteration 19400: Loss = -12464.2060546875
Iteration 19500: Loss = -12463.2822265625
Iteration 19600: Loss = -12463.1572265625
Iteration 19700: Loss = -12461.494140625
Iteration 19800: Loss = -12460.33203125
Iteration 19900: Loss = -12459.1435546875
Iteration 20000: Loss = -12457.5869140625
Iteration 20100: Loss = -12455.7998046875
Iteration 20200: Loss = -12455.1044921875
Iteration 20300: Loss = -12454.7197265625
Iteration 20400: Loss = -12454.7197265625
Iteration 20500: Loss = -12454.71875
Iteration 20600: Loss = -12453.9140625
Iteration 20700: Loss = -12453.6591796875
Iteration 20800: Loss = -12452.2548828125
Iteration 20900: Loss = -12452.0400390625
Iteration 21000: Loss = -12451.0341796875
Iteration 21100: Loss = -12449.994140625
Iteration 21200: Loss = -12449.0009765625
Iteration 21300: Loss = -12448.9951171875
Iteration 21400: Loss = -12448.9951171875
Iteration 21500: Loss = -12448.9873046875
Iteration 21600: Loss = -12448.3134765625
Iteration 21700: Loss = -12448.3095703125
Iteration 21800: Loss = -12445.9423828125
Iteration 21900: Loss = -12445.5185546875
Iteration 22000: Loss = -12444.9287109375
Iteration 22100: Loss = -12443.662109375
Iteration 22200: Loss = -12442.669921875
Iteration 22300: Loss = -12442.1650390625
Iteration 22400: Loss = -12442.1650390625
Iteration 22500: Loss = -12442.166015625
1
Iteration 22600: Loss = -12442.1650390625
Iteration 22700: Loss = -12441.24609375
Iteration 22800: Loss = -12441.2275390625
Iteration 22900: Loss = -12440.4013671875
Iteration 23000: Loss = -12439.8779296875
Iteration 23100: Loss = -12439.267578125
Iteration 23200: Loss = -12439.177734375
Iteration 23300: Loss = -12438.537109375
Iteration 23400: Loss = -12438.537109375
Iteration 23500: Loss = -12438.53515625
Iteration 23600: Loss = -12438.5361328125
1
Iteration 23700: Loss = -12437.5478515625
Iteration 23800: Loss = -12437.1708984375
Iteration 23900: Loss = -12437.1640625
Iteration 24000: Loss = -12436.4580078125
Iteration 24100: Loss = -12436.458984375
1
Iteration 24200: Loss = -12436.458984375
2
Iteration 24300: Loss = -12436.458984375
3
Iteration 24400: Loss = -12436.45703125
Iteration 24500: Loss = -12436.2021484375
Iteration 24600: Loss = -12435.435546875
Iteration 24700: Loss = -12435.4189453125
Iteration 24800: Loss = -12435.4189453125
Iteration 24900: Loss = -12435.4072265625
Iteration 25000: Loss = -12434.83203125
Iteration 25100: Loss = -12434.8330078125
1
Iteration 25200: Loss = -12434.833984375
2
Iteration 25300: Loss = -12434.8154296875
Iteration 25400: Loss = -12434.291015625
Iteration 25500: Loss = -12434.291015625
Iteration 25600: Loss = -12433.619140625
Iteration 25700: Loss = -12433.6162109375
Iteration 25800: Loss = -12433.615234375
Iteration 25900: Loss = -12433.6162109375
1
Iteration 26000: Loss = -12433.6171875
2
Iteration 26100: Loss = -12433.6162109375
3
Iteration 26200: Loss = -12433.6162109375
4
Iteration 26300: Loss = -12433.3681640625
Iteration 26400: Loss = -12431.1279296875
Iteration 26500: Loss = -12430.58203125
Iteration 26600: Loss = -12430.08984375
Iteration 26700: Loss = -12429.2607421875
Iteration 26800: Loss = -12428.435546875
Iteration 26900: Loss = -12428.2861328125
Iteration 27000: Loss = -12427.49609375
Iteration 27100: Loss = -12427.3076171875
Iteration 27200: Loss = -12427.0634765625
Iteration 27300: Loss = -12426.8369140625
Iteration 27400: Loss = -12426.83203125
Iteration 27500: Loss = -12425.9267578125
Iteration 27600: Loss = -12425.7255859375
Iteration 27700: Loss = -12425.6494140625
Iteration 27800: Loss = -12425.474609375
Iteration 27900: Loss = -12425.4638671875
Iteration 28000: Loss = -12425.4228515625
Iteration 28100: Loss = -12425.3828125
Iteration 28200: Loss = -12425.3603515625
Iteration 28300: Loss = -12425.1103515625
Iteration 28400: Loss = -12425.1103515625
Iteration 28500: Loss = -12425.06640625
Iteration 28600: Loss = -12424.33984375
Iteration 28700: Loss = -12424.1279296875
Iteration 28800: Loss = -12423.931640625
Iteration 28900: Loss = -12423.392578125
Iteration 29000: Loss = -12423.1298828125
Iteration 29100: Loss = -12422.71484375
Iteration 29200: Loss = -12422.0341796875
Iteration 29300: Loss = -12421.46484375
Iteration 29400: Loss = -12420.611328125
Iteration 29500: Loss = -12419.8603515625
Iteration 29600: Loss = -12416.4052734375
Iteration 29700: Loss = -12413.9658203125
Iteration 29800: Loss = -12408.9375
Iteration 29900: Loss = -12407.5693359375
pi: tensor([[9.5719e-01, 4.2808e-02],
        [7.7078e-08, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 1.9566e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1962, 0.1935],
         [0.3807, 0.2446]],

        [[0.9439, 0.2687],
         [0.0222, 0.5636]],

        [[0.0204, 0.1930],
         [0.0824, 0.9913]],

        [[0.5522, 0.2557],
         [0.5475, 0.0158]],

        [[0.0424, 0.2079],
         [0.1164, 0.1728]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0007766707522784365
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.007419103651150718
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.0014658390783536795
Global Adjusted Rand Index: -0.001976883675391374
Average Adjusted Rand Index: -0.003629292393326264
[-0.0028171196126616236, -0.001976883675391374] [0.006701210755197975, -0.003629292393326264] [12398.8662109375, 12404.681640625]
-------------------------------------
This iteration is 93
True Objective function: Loss = -11812.296689037514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -45272.1796875
Iteration 100: Loss = -26656.833984375
Iteration 200: Loss = -16206.4609375
Iteration 300: Loss = -13275.3515625
Iteration 400: Loss = -12831.9482421875
Iteration 500: Loss = -12693.9619140625
Iteration 600: Loss = -12619.4951171875
Iteration 700: Loss = -12544.5185546875
Iteration 800: Loss = -12496.6865234375
Iteration 900: Loss = -12481.619140625
Iteration 1000: Loss = -12468.68359375
Iteration 1100: Loss = -12459.939453125
Iteration 1200: Loss = -12454.302734375
Iteration 1300: Loss = -12449.8974609375
Iteration 1400: Loss = -12446.3349609375
Iteration 1500: Loss = -12443.3994140625
Iteration 1600: Loss = -12440.9423828125
Iteration 1700: Loss = -12438.869140625
Iteration 1800: Loss = -12437.0986328125
Iteration 1900: Loss = -12435.5703125
Iteration 2000: Loss = -12434.2431640625
Iteration 2100: Loss = -12433.0810546875
Iteration 2200: Loss = -12432.0576171875
Iteration 2300: Loss = -12431.1494140625
Iteration 2400: Loss = -12430.3369140625
Iteration 2500: Loss = -12429.609375
Iteration 2600: Loss = -12428.9609375
Iteration 2700: Loss = -12428.376953125
Iteration 2800: Loss = -12427.8486328125
Iteration 2900: Loss = -12427.3671875
Iteration 3000: Loss = -12426.9306640625
Iteration 3100: Loss = -12426.5302734375
Iteration 3200: Loss = -12426.166015625
Iteration 3300: Loss = -12425.830078125
Iteration 3400: Loss = -12425.5185546875
Iteration 3500: Loss = -12425.2333984375
Iteration 3600: Loss = -12424.9697265625
Iteration 3700: Loss = -12424.724609375
Iteration 3800: Loss = -12424.4990234375
Iteration 3900: Loss = -12424.287109375
Iteration 4000: Loss = -12424.091796875
Iteration 4100: Loss = -12423.9091796875
Iteration 4200: Loss = -12423.740234375
Iteration 4300: Loss = -12423.5810546875
Iteration 4400: Loss = -12423.4296875
Iteration 4500: Loss = -12423.2939453125
Iteration 4600: Loss = -12423.1630859375
Iteration 4700: Loss = -12423.041015625
Iteration 4800: Loss = -12422.9248046875
Iteration 4900: Loss = -12422.818359375
Iteration 5000: Loss = -12422.71875
Iteration 5100: Loss = -12422.623046875
Iteration 5200: Loss = -12422.5322265625
Iteration 5300: Loss = -12422.4482421875
Iteration 5400: Loss = -12422.3701171875
Iteration 5500: Loss = -12422.2939453125
Iteration 5600: Loss = -12422.2236328125
Iteration 5700: Loss = -12422.15625
Iteration 5800: Loss = -12422.09375
Iteration 5900: Loss = -12422.03515625
Iteration 6000: Loss = -12421.9765625
Iteration 6100: Loss = -12421.9228515625
Iteration 6200: Loss = -12421.873046875
Iteration 6300: Loss = -12421.8251953125
Iteration 6400: Loss = -12421.7802734375
Iteration 6500: Loss = -12421.7373046875
Iteration 6600: Loss = -12421.6962890625
Iteration 6700: Loss = -12421.6591796875
Iteration 6800: Loss = -12421.6220703125
Iteration 6900: Loss = -12421.5888671875
Iteration 7000: Loss = -12421.5537109375
Iteration 7100: Loss = -12421.5244140625
Iteration 7200: Loss = -12421.4931640625
Iteration 7300: Loss = -12421.4658203125
Iteration 7400: Loss = -12421.4375
Iteration 7500: Loss = -12421.4130859375
Iteration 7600: Loss = -12421.390625
Iteration 7700: Loss = -12421.3681640625
Iteration 7800: Loss = -12421.345703125
Iteration 7900: Loss = -12421.3251953125
Iteration 8000: Loss = -12421.3046875
Iteration 8100: Loss = -12421.287109375
Iteration 8200: Loss = -12421.267578125
Iteration 8300: Loss = -12421.251953125
Iteration 8400: Loss = -12421.2353515625
Iteration 8500: Loss = -12421.220703125
Iteration 8600: Loss = -12421.2060546875
Iteration 8700: Loss = -12421.19140625
Iteration 8800: Loss = -12421.1796875
Iteration 8900: Loss = -12421.166015625
Iteration 9000: Loss = -12421.154296875
Iteration 9100: Loss = -12421.142578125
Iteration 9200: Loss = -12421.1318359375
Iteration 9300: Loss = -12421.1220703125
Iteration 9400: Loss = -12421.1123046875
Iteration 9500: Loss = -12421.1025390625
Iteration 9600: Loss = -12421.0947265625
Iteration 9700: Loss = -12421.0859375
Iteration 9800: Loss = -12421.078125
Iteration 9900: Loss = -12421.0703125
Iteration 10000: Loss = -12421.0625
Iteration 10100: Loss = -12421.0556640625
Iteration 10200: Loss = -12421.048828125
Iteration 10300: Loss = -12421.04296875
Iteration 10400: Loss = -12421.0361328125
Iteration 10500: Loss = -12421.0322265625
Iteration 10600: Loss = -12421.025390625
Iteration 10700: Loss = -12421.021484375
Iteration 10800: Loss = -12421.015625
Iteration 10900: Loss = -12421.01171875
Iteration 11000: Loss = -12421.0068359375
Iteration 11100: Loss = -12421.0029296875
Iteration 11200: Loss = -12420.9970703125
Iteration 11300: Loss = -12420.9931640625
Iteration 11400: Loss = -12420.9912109375
Iteration 11500: Loss = -12420.986328125
Iteration 11600: Loss = -12420.984375
Iteration 11700: Loss = -12420.98046875
Iteration 11800: Loss = -12420.9775390625
Iteration 11900: Loss = -12420.974609375
Iteration 12000: Loss = -12420.9716796875
Iteration 12100: Loss = -12420.9677734375
Iteration 12200: Loss = -12420.9658203125
Iteration 12300: Loss = -12420.9638671875
Iteration 12400: Loss = -12420.9609375
Iteration 12500: Loss = -12420.958984375
Iteration 12600: Loss = -12420.95703125
Iteration 12700: Loss = -12420.955078125
Iteration 12800: Loss = -12420.9541015625
Iteration 12900: Loss = -12420.951171875
Iteration 13000: Loss = -12420.9501953125
Iteration 13100: Loss = -12420.9462890625
Iteration 13200: Loss = -12420.9453125
Iteration 13300: Loss = -12420.9462890625
1
Iteration 13400: Loss = -12420.9453125
Iteration 13500: Loss = -12420.94140625
Iteration 13600: Loss = -12420.9404296875
Iteration 13700: Loss = -12420.9404296875
Iteration 13800: Loss = -12420.9375
Iteration 13900: Loss = -12420.9384765625
1
Iteration 14000: Loss = -12420.9365234375
Iteration 14100: Loss = -12420.9345703125
Iteration 14200: Loss = -12420.93359375
Iteration 14300: Loss = -12420.93359375
Iteration 14400: Loss = -12420.9326171875
Iteration 14500: Loss = -12420.931640625
Iteration 14600: Loss = -12420.9306640625
Iteration 14700: Loss = -12420.9296875
Iteration 14800: Loss = -12420.9296875
Iteration 14900: Loss = -12420.9296875
Iteration 15000: Loss = -12420.927734375
Iteration 15100: Loss = -12420.9267578125
Iteration 15200: Loss = -12420.9267578125
Iteration 15300: Loss = -12420.92578125
Iteration 15400: Loss = -12420.92578125
Iteration 15500: Loss = -12420.9248046875
Iteration 15600: Loss = -12420.9228515625
Iteration 15700: Loss = -12420.919921875
Iteration 15800: Loss = -12420.916015625
Iteration 15900: Loss = -12420.908203125
Iteration 16000: Loss = -12420.896484375
Iteration 16100: Loss = -12420.876953125
Iteration 16200: Loss = -12420.8330078125
Iteration 16300: Loss = -12420.7958984375
Iteration 16400: Loss = -12420.7763671875
Iteration 16500: Loss = -12420.7607421875
Iteration 16600: Loss = -12420.7431640625
Iteration 16700: Loss = -12420.7255859375
Iteration 16800: Loss = -12420.7041015625
Iteration 16900: Loss = -12420.6787109375
Iteration 17000: Loss = -12420.646484375
Iteration 17100: Loss = -12420.603515625
Iteration 17200: Loss = -12420.5517578125
Iteration 17300: Loss = -12420.5078125
Iteration 17400: Loss = -12420.46484375
Iteration 17500: Loss = -12420.4521484375
Iteration 17600: Loss = -12420.4345703125
Iteration 17700: Loss = -12420.4287109375
Iteration 17800: Loss = -12420.4150390625
Iteration 17900: Loss = -12420.40625
Iteration 18000: Loss = -12420.306640625
Iteration 18100: Loss = -12420.15625
Iteration 18200: Loss = -12420.111328125
Iteration 18300: Loss = -12420.0732421875
Iteration 18400: Loss = -12420.0419921875
Iteration 18500: Loss = -12420.01171875
Iteration 18600: Loss = -12420.0009765625
Iteration 18700: Loss = -12419.943359375
Iteration 18800: Loss = -12419.5029296875
Iteration 18900: Loss = -12419.4951171875
Iteration 19000: Loss = -12419.490234375
Iteration 19100: Loss = -12419.4814453125
Iteration 19200: Loss = -12419.4794921875
Iteration 19300: Loss = -12419.474609375
Iteration 19400: Loss = -12419.47265625
Iteration 19500: Loss = -12419.4697265625
Iteration 19600: Loss = -12419.451171875
Iteration 19700: Loss = -12419.447265625
Iteration 19800: Loss = -12419.4462890625
Iteration 19900: Loss = -12419.4443359375
Iteration 20000: Loss = -12419.4287109375
Iteration 20100: Loss = -12419.42578125
Iteration 20200: Loss = -12419.419921875
Iteration 20300: Loss = -12419.3662109375
Iteration 20400: Loss = -12419.36328125
Iteration 20500: Loss = -12419.3603515625
Iteration 20600: Loss = -12419.361328125
1
Iteration 20700: Loss = -12419.361328125
2
Iteration 20800: Loss = -12419.3603515625
Iteration 20900: Loss = -12419.359375
Iteration 21000: Loss = -12419.3515625
Iteration 21100: Loss = -12419.3515625
Iteration 21200: Loss = -12419.3515625
Iteration 21300: Loss = -12419.3515625
Iteration 21400: Loss = -12419.3515625
Iteration 21500: Loss = -12419.3505859375
Iteration 21600: Loss = -12419.34375
Iteration 21700: Loss = -12419.3447265625
1
Iteration 21800: Loss = -12419.3193359375
Iteration 21900: Loss = -12419.3134765625
Iteration 22000: Loss = -12419.3125
Iteration 22100: Loss = -12419.3134765625
1
Iteration 22200: Loss = -12419.3134765625
2
Iteration 22300: Loss = -12419.3056640625
Iteration 22400: Loss = -12419.3046875
Iteration 22500: Loss = -12419.3037109375
Iteration 22600: Loss = -12419.2314453125
Iteration 22700: Loss = -12419.1611328125
Iteration 22800: Loss = -12419.16015625
Iteration 22900: Loss = -12419.162109375
1
Iteration 23000: Loss = -12419.1611328125
2
Iteration 23100: Loss = -12419.1611328125
3
Iteration 23200: Loss = -12419.16015625
Iteration 23300: Loss = -12419.1611328125
1
Iteration 23400: Loss = -12419.16015625
Iteration 23500: Loss = -12419.16015625
Iteration 23600: Loss = -12419.1591796875
Iteration 23700: Loss = -12419.1611328125
1
Iteration 23800: Loss = -12419.162109375
2
Iteration 23900: Loss = -12419.1611328125
3
Iteration 24000: Loss = -12419.16015625
4
Iteration 24100: Loss = -12419.1611328125
5
Iteration 24200: Loss = -12419.1572265625
Iteration 24300: Loss = -12419.15625
Iteration 24400: Loss = -12419.1572265625
1
Iteration 24500: Loss = -12419.1572265625
2
Iteration 24600: Loss = -12419.154296875
Iteration 24700: Loss = -12419.1552734375
1
Iteration 24800: Loss = -12419.154296875
Iteration 24900: Loss = -12419.154296875
Iteration 25000: Loss = -12419.15625
1
Iteration 25100: Loss = -12419.1533203125
Iteration 25200: Loss = -12419.1552734375
1
Iteration 25300: Loss = -12419.1513671875
Iteration 25400: Loss = -12419.1513671875
Iteration 25500: Loss = -12419.1513671875
Iteration 25600: Loss = -12419.1494140625
Iteration 25700: Loss = -12419.150390625
1
Iteration 25800: Loss = -12419.1435546875
Iteration 25900: Loss = -12419.1435546875
Iteration 26000: Loss = -12419.14453125
1
Iteration 26100: Loss = -12419.14453125
2
Iteration 26200: Loss = -12419.142578125
Iteration 26300: Loss = -12419.14453125
1
Iteration 26400: Loss = -12419.142578125
Iteration 26500: Loss = -12419.1435546875
1
Iteration 26600: Loss = -12419.1435546875
2
Iteration 26700: Loss = -12419.1455078125
3
Iteration 26800: Loss = -12419.1435546875
4
Iteration 26900: Loss = -12419.1435546875
5
Iteration 27000: Loss = -12419.1435546875
6
Iteration 27100: Loss = -12419.1435546875
7
Iteration 27200: Loss = -12419.1435546875
8
Iteration 27300: Loss = -12419.1435546875
9
Iteration 27400: Loss = -12419.1435546875
10
Iteration 27500: Loss = -12419.14453125
11
Iteration 27600: Loss = -12419.1435546875
12
Iteration 27700: Loss = -12419.1435546875
13
Iteration 27800: Loss = -12419.1455078125
14
Iteration 27900: Loss = -12419.1435546875
15
Stopping early at iteration 27900 due to no improvement.
pi: tensor([[9.9997e-01, 3.0313e-05],
        [2.3265e-02, 9.7673e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([5.3581e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1837, 0.1945],
         [0.0723, 0.2034]],

        [[0.0928, 0.2427],
         [0.9900, 0.1805]],

        [[0.9694, 0.1279],
         [0.8112, 0.9158]],

        [[0.0718, 0.2002],
         [0.9882, 0.8690]],

        [[0.2117, 0.1685],
         [0.0367, 0.1729]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.003506908785360844
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0017137835707030447
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.018772080923839488
Global Adjusted Rand Index: 0.0031914732339561143
Average Adjusted Rand Index: 0.0043456080940378235
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35405.03515625
Iteration 100: Loss = -19731.716796875
Iteration 200: Loss = -13787.7392578125
Iteration 300: Loss = -12777.482421875
Iteration 400: Loss = -12614.2021484375
Iteration 500: Loss = -12543.3642578125
Iteration 600: Loss = -12507.5478515625
Iteration 700: Loss = -12487.7119140625
Iteration 800: Loss = -12475.345703125
Iteration 900: Loss = -12457.65625
Iteration 1000: Loss = -12451.177734375
Iteration 1100: Loss = -12442.037109375
Iteration 1200: Loss = -12438.853515625
Iteration 1300: Loss = -12436.4404296875
Iteration 1400: Loss = -12434.50390625
Iteration 1500: Loss = -12432.9169921875
Iteration 1600: Loss = -12431.5927734375
Iteration 1700: Loss = -12430.4697265625
Iteration 1800: Loss = -12429.5185546875
Iteration 1900: Loss = -12428.6259765625
Iteration 2000: Loss = -12427.9365234375
Iteration 2100: Loss = -12427.3466796875
Iteration 2200: Loss = -12426.8232421875
Iteration 2300: Loss = -12426.3583984375
Iteration 2400: Loss = -12425.9443359375
Iteration 2500: Loss = -12425.5703125
Iteration 2600: Loss = -12425.234375
Iteration 2700: Loss = -12424.927734375
Iteration 2800: Loss = -12424.650390625
Iteration 2900: Loss = -12424.3994140625
Iteration 3000: Loss = -12424.1630859375
Iteration 3100: Loss = -12423.9501953125
Iteration 3200: Loss = -12423.7548828125
Iteration 3300: Loss = -12423.5771484375
Iteration 3400: Loss = -12423.408203125
Iteration 3500: Loss = -12423.2548828125
Iteration 3600: Loss = -12423.1103515625
Iteration 3700: Loss = -12422.978515625
Iteration 3800: Loss = -12422.8544921875
Iteration 3900: Loss = -12422.7412109375
Iteration 4000: Loss = -12422.6337890625
Iteration 4100: Loss = -12422.5322265625
Iteration 4200: Loss = -12422.439453125
Iteration 4300: Loss = -12422.3525390625
Iteration 4400: Loss = -12422.26953125
Iteration 4500: Loss = -12422.19140625
Iteration 4600: Loss = -12422.1201171875
Iteration 4700: Loss = -12422.052734375
Iteration 4800: Loss = -12421.98828125
Iteration 4900: Loss = -12421.9306640625
Iteration 5000: Loss = -12421.873046875
Iteration 5100: Loss = -12421.8203125
Iteration 5200: Loss = -12421.771484375
Iteration 5300: Loss = -12421.72265625
Iteration 5400: Loss = -12421.6787109375
Iteration 5500: Loss = -12421.6376953125
Iteration 5600: Loss = -12421.59765625
Iteration 5700: Loss = -12421.560546875
Iteration 5800: Loss = -12421.5234375
Iteration 5900: Loss = -12421.490234375
Iteration 6000: Loss = -12421.458984375
Iteration 6100: Loss = -12421.431640625
Iteration 6200: Loss = -12421.4013671875
Iteration 6300: Loss = -12421.375
Iteration 6400: Loss = -12421.349609375
Iteration 6500: Loss = -12421.3251953125
Iteration 6600: Loss = -12421.302734375
Iteration 6700: Loss = -12421.283203125
Iteration 6800: Loss = -12421.26171875
Iteration 6900: Loss = -12421.2421875
Iteration 7000: Loss = -12421.224609375
Iteration 7100: Loss = -12421.2080078125
Iteration 7200: Loss = -12421.1904296875
Iteration 7300: Loss = -12421.1767578125
Iteration 7400: Loss = -12421.16015625
Iteration 7500: Loss = -12421.1474609375
Iteration 7600: Loss = -12421.1337890625
Iteration 7700: Loss = -12421.1220703125
Iteration 7800: Loss = -12421.1083984375
Iteration 7900: Loss = -12421.0966796875
Iteration 8000: Loss = -12421.0869140625
Iteration 8100: Loss = -12421.076171875
Iteration 8200: Loss = -12421.06640625
Iteration 8300: Loss = -12421.056640625
Iteration 8400: Loss = -12421.046875
Iteration 8500: Loss = -12421.0380859375
Iteration 8600: Loss = -12421.029296875
Iteration 8700: Loss = -12421.0224609375
Iteration 8800: Loss = -12421.0126953125
Iteration 8900: Loss = -12421.00390625
Iteration 9000: Loss = -12420.998046875
Iteration 9100: Loss = -12420.98828125
Iteration 9200: Loss = -12420.9794921875
Iteration 9300: Loss = -12420.9697265625
Iteration 9400: Loss = -12420.9580078125
Iteration 9500: Loss = -12420.9423828125
Iteration 9600: Loss = -12420.9248046875
Iteration 9700: Loss = -12420.9072265625
Iteration 9800: Loss = -12420.890625
Iteration 9900: Loss = -12420.87890625
Iteration 10000: Loss = -12420.86328125
Iteration 10100: Loss = -12420.853515625
Iteration 10200: Loss = -12420.841796875
Iteration 10300: Loss = -12420.83203125
Iteration 10400: Loss = -12420.8251953125
Iteration 10500: Loss = -12420.818359375
Iteration 10600: Loss = -12420.810546875
Iteration 10700: Loss = -12420.8046875
Iteration 10800: Loss = -12420.7978515625
Iteration 10900: Loss = -12420.7919921875
Iteration 11000: Loss = -12420.78515625
Iteration 11100: Loss = -12420.779296875
Iteration 11200: Loss = -12420.7744140625
Iteration 11300: Loss = -12420.76953125
Iteration 11400: Loss = -12420.765625
Iteration 11500: Loss = -12420.76171875
Iteration 11600: Loss = -12420.755859375
Iteration 11700: Loss = -12420.751953125
Iteration 11800: Loss = -12420.7470703125
Iteration 11900: Loss = -12420.7412109375
Iteration 12000: Loss = -12420.734375
Iteration 12100: Loss = -12420.7294921875
Iteration 12200: Loss = -12420.7255859375
Iteration 12300: Loss = -12420.71875
Iteration 12400: Loss = -12420.7138671875
Iteration 12500: Loss = -12420.70703125
Iteration 12600: Loss = -12420.7001953125
Iteration 12700: Loss = -12420.693359375
Iteration 12800: Loss = -12420.6875
Iteration 12900: Loss = -12420.6796875
Iteration 13000: Loss = -12420.6708984375
Iteration 13100: Loss = -12420.662109375
Iteration 13200: Loss = -12420.65234375
Iteration 13300: Loss = -12420.642578125
Iteration 13400: Loss = -12420.6298828125
Iteration 13500: Loss = -12420.6171875
Iteration 13600: Loss = -12420.603515625
Iteration 13700: Loss = -12420.583984375
Iteration 13800: Loss = -12420.5673828125
Iteration 13900: Loss = -12420.544921875
Iteration 14000: Loss = -12420.51953125
Iteration 14100: Loss = -12420.490234375
Iteration 14200: Loss = -12420.45703125
Iteration 14300: Loss = -12420.4169921875
Iteration 14400: Loss = -12420.373046875
Iteration 14500: Loss = -12420.3232421875
Iteration 14600: Loss = -12420.26953125
Iteration 14700: Loss = -12420.220703125
Iteration 14800: Loss = -12420.1796875
Iteration 14900: Loss = -12420.1513671875
Iteration 15000: Loss = -12420.130859375
Iteration 15100: Loss = -12420.11328125
Iteration 15200: Loss = -12420.09375
Iteration 15300: Loss = -12420.048828125
Iteration 15400: Loss = -12419.7412109375
Iteration 15500: Loss = -12419.458984375
Iteration 15600: Loss = -12419.306640625
Iteration 15700: Loss = -12419.234375
Iteration 15800: Loss = -12419.19140625
Iteration 15900: Loss = -12419.1708984375
Iteration 16000: Loss = -12419.16015625
Iteration 16100: Loss = -12419.15234375
Iteration 16200: Loss = -12419.1435546875
Iteration 16300: Loss = -12419.1318359375
Iteration 16400: Loss = -12419.126953125
Iteration 16500: Loss = -12419.1142578125
Iteration 16600: Loss = -12419.103515625
Iteration 16700: Loss = -12419.1005859375
Iteration 16800: Loss = -12419.09765625
Iteration 16900: Loss = -12419.0966796875
Iteration 17000: Loss = -12419.09375
Iteration 17100: Loss = -12419.091796875
Iteration 17200: Loss = -12419.08984375
Iteration 17300: Loss = -12419.0908203125
1
Iteration 17400: Loss = -12419.0888671875
Iteration 17500: Loss = -12419.0908203125
1
Iteration 17600: Loss = -12419.0888671875
Iteration 17700: Loss = -12419.087890625
Iteration 17800: Loss = -12419.0859375
Iteration 17900: Loss = -12419.087890625
1
Iteration 18000: Loss = -12419.0859375
Iteration 18100: Loss = -12419.0869140625
1
Iteration 18200: Loss = -12419.0859375
Iteration 18300: Loss = -12419.0849609375
Iteration 18400: Loss = -12419.0849609375
Iteration 18500: Loss = -12419.083984375
Iteration 18600: Loss = -12419.0859375
1
Iteration 18700: Loss = -12419.0849609375
2
Iteration 18800: Loss = -12419.0859375
3
Iteration 18900: Loss = -12419.0849609375
4
Iteration 19000: Loss = -12419.083984375
Iteration 19100: Loss = -12419.083984375
Iteration 19200: Loss = -12419.080078125
Iteration 19300: Loss = -12419.080078125
Iteration 19400: Loss = -12419.08203125
1
Iteration 19500: Loss = -12419.0791015625
Iteration 19600: Loss = -12419.0810546875
1
Iteration 19700: Loss = -12419.0791015625
Iteration 19800: Loss = -12419.080078125
1
Iteration 19900: Loss = -12419.0810546875
2
Iteration 20000: Loss = -12419.0771484375
Iteration 20100: Loss = -12419.078125
1
Iteration 20200: Loss = -12419.0771484375
Iteration 20300: Loss = -12419.0771484375
Iteration 20400: Loss = -12419.078125
1
Iteration 20500: Loss = -12419.0751953125
Iteration 20600: Loss = -12419.0751953125
Iteration 20700: Loss = -12419.07421875
Iteration 20800: Loss = -12419.07421875
Iteration 20900: Loss = -12419.0751953125
1
Iteration 21000: Loss = -12419.0732421875
Iteration 21100: Loss = -12419.0732421875
Iteration 21200: Loss = -12419.0751953125
1
Iteration 21300: Loss = -12419.0732421875
Iteration 21400: Loss = -12419.0732421875
Iteration 21500: Loss = -12419.0732421875
Iteration 21600: Loss = -12419.0732421875
Iteration 21700: Loss = -12419.07421875
1
Iteration 21800: Loss = -12419.0751953125
2
Iteration 21900: Loss = -12419.07421875
3
Iteration 22000: Loss = -12419.0732421875
Iteration 22100: Loss = -12419.07421875
1
Iteration 22200: Loss = -12419.0732421875
Iteration 22300: Loss = -12419.07421875
1
Iteration 22400: Loss = -12419.0732421875
Iteration 22500: Loss = -12419.07421875
1
Iteration 22600: Loss = -12419.0732421875
Iteration 22700: Loss = -12419.0732421875
Iteration 22800: Loss = -12419.0732421875
Iteration 22900: Loss = -12419.0732421875
Iteration 23000: Loss = -12419.072265625
Iteration 23100: Loss = -12419.0732421875
1
Iteration 23200: Loss = -12419.0732421875
2
Iteration 23300: Loss = -12419.072265625
Iteration 23400: Loss = -12419.0732421875
1
Iteration 23500: Loss = -12419.0732421875
2
Iteration 23600: Loss = -12419.0732421875
3
Iteration 23700: Loss = -12419.072265625
Iteration 23800: Loss = -12419.0732421875
1
Iteration 23900: Loss = -12419.072265625
Iteration 24000: Loss = -12419.0732421875
1
Iteration 24100: Loss = -12419.0732421875
2
Iteration 24200: Loss = -12419.0732421875
3
Iteration 24300: Loss = -12419.0712890625
Iteration 24400: Loss = -12419.072265625
1
Iteration 24500: Loss = -12419.072265625
2
Iteration 24600: Loss = -12419.0732421875
3
Iteration 24700: Loss = -12419.072265625
4
Iteration 24800: Loss = -12419.0732421875
5
Iteration 24900: Loss = -12419.0732421875
6
Iteration 25000: Loss = -12419.0712890625
Iteration 25100: Loss = -12419.0732421875
1
Iteration 25200: Loss = -12419.0703125
Iteration 25300: Loss = -12419.072265625
1
Iteration 25400: Loss = -12419.0703125
Iteration 25500: Loss = -12419.0693359375
Iteration 25600: Loss = -12419.0703125
1
Iteration 25700: Loss = -12419.0703125
2
Iteration 25800: Loss = -12419.0712890625
3
Iteration 25900: Loss = -12419.0703125
4
Iteration 26000: Loss = -12419.0693359375
Iteration 26100: Loss = -12419.0703125
1
Iteration 26200: Loss = -12419.0703125
2
Iteration 26300: Loss = -12419.0703125
3
Iteration 26400: Loss = -12419.0693359375
Iteration 26500: Loss = -12419.0712890625
1
Iteration 26600: Loss = -12419.0693359375
Iteration 26700: Loss = -12419.0712890625
1
Iteration 26800: Loss = -12419.0712890625
2
Iteration 26900: Loss = -12419.0712890625
3
Iteration 27000: Loss = -12419.0693359375
Iteration 27100: Loss = -12419.0703125
1
Iteration 27200: Loss = -12419.0703125
2
Iteration 27300: Loss = -12419.072265625
3
Iteration 27400: Loss = -12419.0712890625
4
Iteration 27500: Loss = -12419.0693359375
Iteration 27600: Loss = -12419.0703125
1
Iteration 27700: Loss = -12419.0703125
2
Iteration 27800: Loss = -12419.0703125
3
Iteration 27900: Loss = -12419.0703125
4
Iteration 28000: Loss = -12419.0712890625
5
Iteration 28100: Loss = -12419.0712890625
6
Iteration 28200: Loss = -12419.0703125
7
Iteration 28300: Loss = -12419.0712890625
8
Iteration 28400: Loss = -12419.0693359375
Iteration 28500: Loss = -12419.072265625
1
Iteration 28600: Loss = -12419.0712890625
2
Iteration 28700: Loss = -12419.0703125
3
Iteration 28800: Loss = -12419.0712890625
4
Iteration 28900: Loss = -12419.068359375
Iteration 29000: Loss = -12419.0693359375
1
Iteration 29100: Loss = -12419.0703125
2
Iteration 29200: Loss = -12419.0703125
3
Iteration 29300: Loss = -12419.0703125
4
Iteration 29400: Loss = -12419.0703125
5
Iteration 29500: Loss = -12419.0703125
6
Iteration 29600: Loss = -12419.0703125
7
Iteration 29700: Loss = -12419.0712890625
8
Iteration 29800: Loss = -12419.0693359375
9
Iteration 29900: Loss = -12419.0703125
10
pi: tensor([[9.7472e-01, 2.5284e-02],
        [8.7271e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 1.3798e-05], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2035, 0.2176],
         [0.5765, 0.1808]],

        [[0.9130, 0.2397],
         [0.6357, 0.9712]],

        [[0.1550, 0.1305],
         [0.0086, 0.1194]],

        [[0.0361, 0.2000],
         [0.0127, 0.7855]],

        [[0.6417, 0.1703],
         [0.0867, 0.1768]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.003506908785360844
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0017137835707030447
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.018772080923839488
Global Adjusted Rand Index: 0.0031914732339561143
Average Adjusted Rand Index: 0.0043456080940378235
[0.0031914732339561143, 0.0031914732339561143] [0.0043456080940378235, 0.0043456080940378235] [12419.1435546875, 12419.0693359375]
-------------------------------------
This iteration is 94
True Objective function: Loss = -12041.79058337653
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36029.10546875
Iteration 100: Loss = -22484.01171875
Iteration 200: Loss = -15687.322265625
Iteration 300: Loss = -14002.3515625
Iteration 400: Loss = -13364.021484375
Iteration 500: Loss = -13033.1083984375
Iteration 600: Loss = -12861.4873046875
Iteration 700: Loss = -12783.81640625
Iteration 800: Loss = -12728.87890625
Iteration 900: Loss = -12698.591796875
Iteration 1000: Loss = -12681.33203125
Iteration 1100: Loss = -12663.884765625
Iteration 1200: Loss = -12648.787109375
Iteration 1300: Loss = -12639.4208984375
Iteration 1400: Loss = -12633.64453125
Iteration 1500: Loss = -12628.7255859375
Iteration 1600: Loss = -12623.296875
Iteration 1700: Loss = -12615.7314453125
Iteration 1800: Loss = -12609.55078125
Iteration 1900: Loss = -12601.978515625
Iteration 2000: Loss = -12599.5810546875
Iteration 2100: Loss = -12597.8173828125
Iteration 2200: Loss = -12596.3330078125
Iteration 2300: Loss = -12590.421875
Iteration 2400: Loss = -12589.021484375
Iteration 2500: Loss = -12588.01171875
Iteration 2600: Loss = -12583.255859375
Iteration 2700: Loss = -12582.0693359375
Iteration 2800: Loss = -12581.298828125
Iteration 2900: Loss = -12580.65234375
Iteration 3000: Loss = -12580.0927734375
Iteration 3100: Loss = -12579.5947265625
Iteration 3200: Loss = -12579.150390625
Iteration 3300: Loss = -12578.7509765625
Iteration 3400: Loss = -12578.390625
Iteration 3500: Loss = -12578.060546875
Iteration 3600: Loss = -12577.763671875
Iteration 3700: Loss = -12577.490234375
Iteration 3800: Loss = -12577.2421875
Iteration 3900: Loss = -12577.0107421875
Iteration 4000: Loss = -12576.8017578125
Iteration 4100: Loss = -12576.6083984375
Iteration 4200: Loss = -12576.427734375
Iteration 4300: Loss = -12576.2607421875
Iteration 4400: Loss = -12576.107421875
Iteration 4500: Loss = -12575.9658203125
Iteration 4600: Loss = -12575.83203125
Iteration 4700: Loss = -12575.7080078125
Iteration 4800: Loss = -12575.5947265625
Iteration 4900: Loss = -12575.4853515625
Iteration 5000: Loss = -12575.38671875
Iteration 5100: Loss = -12575.2919921875
Iteration 5200: Loss = -12575.2060546875
Iteration 5300: Loss = -12575.1220703125
Iteration 5400: Loss = -12575.044921875
Iteration 5500: Loss = -12574.97265625
Iteration 5600: Loss = -12574.9052734375
Iteration 5700: Loss = -12574.83984375
Iteration 5800: Loss = -12574.779296875
Iteration 5900: Loss = -12574.720703125
Iteration 6000: Loss = -12574.66796875
Iteration 6100: Loss = -12574.6171875
Iteration 6200: Loss = -12574.568359375
Iteration 6300: Loss = -12574.5234375
Iteration 6400: Loss = -12574.4794921875
Iteration 6500: Loss = -12574.4384765625
Iteration 6600: Loss = -12574.400390625
Iteration 6700: Loss = -12574.3623046875
Iteration 6800: Loss = -12574.3271484375
Iteration 6900: Loss = -12574.2939453125
Iteration 7000: Loss = -12574.2607421875
Iteration 7100: Loss = -12574.2294921875
Iteration 7200: Loss = -12574.19921875
Iteration 7300: Loss = -12574.1689453125
Iteration 7400: Loss = -12574.1416015625
Iteration 7500: Loss = -12574.1142578125
Iteration 7600: Loss = -12574.0869140625
Iteration 7700: Loss = -12574.0595703125
Iteration 7800: Loss = -12574.033203125
Iteration 7900: Loss = -12574.0048828125
Iteration 8000: Loss = -12573.9755859375
Iteration 8100: Loss = -12573.943359375
Iteration 8200: Loss = -12573.91015625
Iteration 8300: Loss = -12573.869140625
Iteration 8400: Loss = -12573.8203125
Iteration 8500: Loss = -12573.7646484375
Iteration 8600: Loss = -12573.6982421875
Iteration 8700: Loss = -12573.62109375
Iteration 8800: Loss = -12573.5458984375
Iteration 8900: Loss = -12573.46875
Iteration 9000: Loss = -12573.3896484375
Iteration 9100: Loss = -12573.3125
Iteration 9200: Loss = -12573.2373046875
Iteration 9300: Loss = -12573.1708984375
Iteration 9400: Loss = -12573.1123046875
Iteration 9500: Loss = -12573.0625
Iteration 9600: Loss = -12573.0224609375
Iteration 9700: Loss = -12572.9853515625
Iteration 9800: Loss = -12572.955078125
Iteration 9900: Loss = -12572.9306640625
Iteration 10000: Loss = -12572.9072265625
Iteration 10100: Loss = -12572.890625
Iteration 10200: Loss = -12572.876953125
Iteration 10300: Loss = -12572.86328125
Iteration 10400: Loss = -12572.8515625
Iteration 10500: Loss = -12572.8427734375
Iteration 10600: Loss = -12572.8330078125
Iteration 10700: Loss = -12572.8251953125
Iteration 10800: Loss = -12572.8193359375
Iteration 10900: Loss = -12572.8115234375
Iteration 11000: Loss = -12572.8046875
Iteration 11100: Loss = -12572.7998046875
Iteration 11200: Loss = -12572.796875
Iteration 11300: Loss = -12572.7958984375
Iteration 11400: Loss = -12572.787109375
Iteration 11500: Loss = -12572.783203125
Iteration 11600: Loss = -12572.7802734375
Iteration 11700: Loss = -12572.77734375
Iteration 11800: Loss = -12572.7744140625
Iteration 11900: Loss = -12572.771484375
Iteration 12000: Loss = -12572.7685546875
Iteration 12100: Loss = -12572.7646484375
Iteration 12200: Loss = -12572.763671875
Iteration 12300: Loss = -12572.759765625
Iteration 12400: Loss = -12572.759765625
Iteration 12500: Loss = -12572.7568359375
Iteration 12600: Loss = -12572.7529296875
Iteration 12700: Loss = -12572.75390625
1
Iteration 12800: Loss = -12572.751953125
Iteration 12900: Loss = -12572.7490234375
Iteration 13000: Loss = -12572.75
1
Iteration 13100: Loss = -12572.7470703125
Iteration 13200: Loss = -12572.744140625
Iteration 13300: Loss = -12572.744140625
Iteration 13400: Loss = -12572.7421875
Iteration 13500: Loss = -12572.7431640625
1
Iteration 13600: Loss = -12572.7412109375
Iteration 13700: Loss = -12572.740234375
Iteration 13800: Loss = -12572.73828125
Iteration 13900: Loss = -12572.73828125
Iteration 14000: Loss = -12572.73828125
Iteration 14100: Loss = -12572.736328125
Iteration 14200: Loss = -12572.7353515625
Iteration 14300: Loss = -12572.7353515625
Iteration 14400: Loss = -12572.734375
Iteration 14500: Loss = -12572.734375
Iteration 14600: Loss = -12572.7314453125
Iteration 14700: Loss = -12572.7314453125
Iteration 14800: Loss = -12572.7314453125
Iteration 14900: Loss = -12572.7314453125
Iteration 15000: Loss = -12572.73046875
Iteration 15100: Loss = -12572.7294921875
Iteration 15200: Loss = -12572.73046875
1
Iteration 15300: Loss = -12572.7294921875
Iteration 15400: Loss = -12572.728515625
Iteration 15500: Loss = -12572.7275390625
Iteration 15600: Loss = -12572.728515625
1
Iteration 15700: Loss = -12572.7275390625
Iteration 15800: Loss = -12572.7275390625
Iteration 15900: Loss = -12572.73046875
1
Iteration 16000: Loss = -12572.7265625
Iteration 16100: Loss = -12572.7255859375
Iteration 16200: Loss = -12572.7255859375
Iteration 16300: Loss = -12572.7255859375
Iteration 16400: Loss = -12572.7265625
1
Iteration 16500: Loss = -12572.7294921875
2
Iteration 16600: Loss = -12572.724609375
Iteration 16700: Loss = -12572.7236328125
Iteration 16800: Loss = -12572.7255859375
1
Iteration 16900: Loss = -12572.7236328125
Iteration 17000: Loss = -12572.7236328125
Iteration 17100: Loss = -12572.724609375
1
Iteration 17200: Loss = -12572.72265625
Iteration 17300: Loss = -12572.7236328125
1
Iteration 17400: Loss = -12572.7255859375
2
Iteration 17500: Loss = -12572.724609375
3
Iteration 17600: Loss = -12572.7216796875
Iteration 17700: Loss = -12572.7236328125
1
Iteration 17800: Loss = -12572.7216796875
Iteration 17900: Loss = -12572.7216796875
Iteration 18000: Loss = -12572.72265625
1
Iteration 18100: Loss = -12572.72265625
2
Iteration 18200: Loss = -12572.72265625
3
Iteration 18300: Loss = -12572.7236328125
4
Iteration 18400: Loss = -12572.7216796875
Iteration 18500: Loss = -12572.7216796875
Iteration 18600: Loss = -12572.7216796875
Iteration 18700: Loss = -12572.72265625
1
Iteration 18800: Loss = -12572.720703125
Iteration 18900: Loss = -12572.720703125
Iteration 19000: Loss = -12572.720703125
Iteration 19100: Loss = -12572.720703125
Iteration 19200: Loss = -12572.7216796875
1
Iteration 19300: Loss = -12572.720703125
Iteration 19400: Loss = -12572.720703125
Iteration 19500: Loss = -12572.720703125
Iteration 19600: Loss = -12572.7197265625
Iteration 19700: Loss = -12572.7216796875
1
Iteration 19800: Loss = -12572.720703125
2
Iteration 19900: Loss = -12572.720703125
3
Iteration 20000: Loss = -12572.720703125
4
Iteration 20100: Loss = -12572.720703125
5
Iteration 20200: Loss = -12572.720703125
6
Iteration 20300: Loss = -12572.7197265625
Iteration 20400: Loss = -12572.720703125
1
Iteration 20500: Loss = -12572.7255859375
2
Iteration 20600: Loss = -12572.720703125
3
Iteration 20700: Loss = -12572.720703125
4
Iteration 20800: Loss = -12572.7197265625
Iteration 20900: Loss = -12572.7197265625
Iteration 21000: Loss = -12572.7197265625
Iteration 21100: Loss = -12572.7197265625
Iteration 21200: Loss = -12572.7197265625
Iteration 21300: Loss = -12572.7197265625
Iteration 21400: Loss = -12572.7197265625
Iteration 21500: Loss = -12572.7216796875
1
Iteration 21600: Loss = -12572.7197265625
Iteration 21700: Loss = -12572.72265625
1
Iteration 21800: Loss = -12572.7197265625
Iteration 21900: Loss = -12572.7197265625
Iteration 22000: Loss = -12572.7197265625
Iteration 22100: Loss = -12572.7216796875
1
Iteration 22200: Loss = -12572.7197265625
Iteration 22300: Loss = -12572.7197265625
Iteration 22400: Loss = -12572.7197265625
Iteration 22500: Loss = -12572.7197265625
Iteration 22600: Loss = -12572.720703125
1
Iteration 22700: Loss = -12572.720703125
2
Iteration 22800: Loss = -12572.7197265625
Iteration 22900: Loss = -12572.720703125
1
Iteration 23000: Loss = -12572.7197265625
Iteration 23100: Loss = -12572.720703125
1
Iteration 23200: Loss = -12572.7197265625
Iteration 23300: Loss = -12572.7197265625
Iteration 23400: Loss = -12572.7197265625
Iteration 23500: Loss = -12572.7197265625
Iteration 23600: Loss = -12572.7197265625
Iteration 23700: Loss = -12572.7197265625
Iteration 23800: Loss = -12572.7197265625
Iteration 23900: Loss = -12572.7197265625
Iteration 24000: Loss = -12572.71875
Iteration 24100: Loss = -12572.7197265625
1
Iteration 24200: Loss = -12572.7197265625
2
Iteration 24300: Loss = -12572.7197265625
3
Iteration 24400: Loss = -12572.7197265625
4
Iteration 24500: Loss = -12572.7197265625
5
Iteration 24600: Loss = -12572.7197265625
6
Iteration 24700: Loss = -12572.720703125
7
Iteration 24800: Loss = -12572.7197265625
8
Iteration 24900: Loss = -12572.7197265625
9
Iteration 25000: Loss = -12572.7197265625
10
Iteration 25100: Loss = -12572.7197265625
11
Iteration 25200: Loss = -12572.7216796875
12
Iteration 25300: Loss = -12572.7197265625
13
Iteration 25400: Loss = -12572.7216796875
14
Iteration 25500: Loss = -12572.7197265625
15
Stopping early at iteration 25500 due to no improvement.
pi: tensor([[5.3901e-05, 9.9995e-01],
        [3.0393e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9983, 0.0017], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1978, 0.1978],
         [0.9866, 0.2075]],

        [[0.0095, 0.2256],
         [0.2127, 0.6465]],

        [[0.1679, 0.2144],
         [0.6692, 0.0448]],

        [[0.9405, 0.2041],
         [0.0785, 0.8660]],

        [[0.0070, 0.6404],
         [0.8960, 0.0159]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0012789331926026277
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22472.19921875
Iteration 100: Loss = -15920.3251953125
Iteration 200: Loss = -13221.6015625
Iteration 300: Loss = -12917.49609375
Iteration 400: Loss = -12836.814453125
Iteration 500: Loss = -12798.9384765625
Iteration 600: Loss = -12776.9189453125
Iteration 700: Loss = -12762.337890625
Iteration 800: Loss = -12750.4775390625
Iteration 900: Loss = -12737.9091796875
Iteration 1000: Loss = -12723.02734375
Iteration 1100: Loss = -12709.66796875
Iteration 1200: Loss = -12692.2890625
Iteration 1300: Loss = -12681.0732421875
Iteration 1400: Loss = -12669.2275390625
Iteration 1500: Loss = -12656.9111328125
Iteration 1600: Loss = -12641.9892578125
Iteration 1700: Loss = -12631.49609375
Iteration 1800: Loss = -12621.1455078125
Iteration 1900: Loss = -12615.751953125
Iteration 2000: Loss = -12603.8662109375
Iteration 2100: Loss = -12595.3037109375
Iteration 2200: Loss = -12589.529296875
Iteration 2300: Loss = -12585.5107421875
Iteration 2400: Loss = -12583.349609375
Iteration 2500: Loss = -12581.8974609375
Iteration 2600: Loss = -12579.7431640625
Iteration 2700: Loss = -12578.7705078125
Iteration 2800: Loss = -12578.1318359375
Iteration 2900: Loss = -12577.6357421875
Iteration 3000: Loss = -12577.234375
Iteration 3100: Loss = -12576.8994140625
Iteration 3200: Loss = -12576.6142578125
Iteration 3300: Loss = -12576.37109375
Iteration 3400: Loss = -12576.15625
Iteration 3500: Loss = -12575.970703125
Iteration 3600: Loss = -12575.802734375
Iteration 3700: Loss = -12575.6552734375
Iteration 3800: Loss = -12575.51953125
Iteration 3900: Loss = -12575.400390625
Iteration 4000: Loss = -12575.2919921875
Iteration 4100: Loss = -12575.193359375
Iteration 4200: Loss = -12575.1044921875
Iteration 4300: Loss = -12575.021484375
Iteration 4400: Loss = -12574.9462890625
Iteration 4500: Loss = -12574.8779296875
Iteration 4600: Loss = -12574.8115234375
Iteration 4700: Loss = -12574.751953125
Iteration 4800: Loss = -12574.6982421875
Iteration 4900: Loss = -12574.6474609375
Iteration 5000: Loss = -12574.6005859375
Iteration 5100: Loss = -12574.5576171875
Iteration 5200: Loss = -12574.5166015625
Iteration 5300: Loss = -12574.48046875
Iteration 5400: Loss = -12574.4443359375
Iteration 5500: Loss = -12574.41015625
Iteration 5600: Loss = -12574.37890625
Iteration 5700: Loss = -12574.3515625
Iteration 5800: Loss = -12574.3251953125
Iteration 5900: Loss = -12574.2998046875
Iteration 6000: Loss = -12574.2763671875
Iteration 6100: Loss = -12574.2529296875
Iteration 6200: Loss = -12574.2314453125
Iteration 6300: Loss = -12574.2138671875
Iteration 6400: Loss = -12574.1943359375
Iteration 6500: Loss = -12574.177734375
Iteration 6600: Loss = -12574.1611328125
Iteration 6700: Loss = -12574.14453125
Iteration 6800: Loss = -12574.1328125
Iteration 6900: Loss = -12574.1171875
Iteration 7000: Loss = -12574.1044921875
Iteration 7100: Loss = -12574.09375
Iteration 7200: Loss = -12574.08203125
Iteration 7300: Loss = -12574.0703125
Iteration 7400: Loss = -12574.060546875
Iteration 7500: Loss = -12574.052734375
Iteration 7600: Loss = -12574.0419921875
Iteration 7700: Loss = -12574.0322265625
Iteration 7800: Loss = -12574.025390625
Iteration 7900: Loss = -12574.0185546875
Iteration 8000: Loss = -12574.0107421875
Iteration 8100: Loss = -12574.0029296875
Iteration 8200: Loss = -12573.99609375
Iteration 8300: Loss = -12573.990234375
Iteration 8400: Loss = -12573.984375
Iteration 8500: Loss = -12573.978515625
Iteration 8600: Loss = -12573.9736328125
Iteration 8700: Loss = -12573.96875
Iteration 8800: Loss = -12573.96484375
Iteration 8900: Loss = -12573.958984375
Iteration 9000: Loss = -12573.9541015625
Iteration 9100: Loss = -12573.951171875
Iteration 9200: Loss = -12573.947265625
Iteration 9300: Loss = -12573.9443359375
Iteration 9400: Loss = -12573.9423828125
Iteration 9500: Loss = -12573.9375
Iteration 9600: Loss = -12573.9345703125
Iteration 9700: Loss = -12573.9306640625
Iteration 9800: Loss = -12573.9287109375
Iteration 9900: Loss = -12573.9267578125
Iteration 10000: Loss = -12573.9228515625
Iteration 10100: Loss = -12573.921875
Iteration 10200: Loss = -12573.919921875
Iteration 10300: Loss = -12573.9169921875
Iteration 10400: Loss = -12573.916015625
Iteration 10500: Loss = -12573.9130859375
Iteration 10600: Loss = -12573.912109375
Iteration 10700: Loss = -12573.912109375
Iteration 10800: Loss = -12573.908203125
Iteration 10900: Loss = -12573.9072265625
Iteration 11000: Loss = -12573.9052734375
Iteration 11100: Loss = -12573.9033203125
Iteration 11200: Loss = -12573.90234375
Iteration 11300: Loss = -12573.9013671875
Iteration 11400: Loss = -12573.900390625
Iteration 11500: Loss = -12573.8994140625
Iteration 11600: Loss = -12573.8994140625
Iteration 11700: Loss = -12573.8984375
Iteration 11800: Loss = -12573.8974609375
Iteration 11900: Loss = -12573.8974609375
Iteration 12000: Loss = -12573.8974609375
Iteration 12100: Loss = -12573.89453125
Iteration 12200: Loss = -12573.8935546875
Iteration 12300: Loss = -12573.8935546875
Iteration 12400: Loss = -12573.892578125
Iteration 12500: Loss = -12573.8916015625
Iteration 12600: Loss = -12573.890625
Iteration 12700: Loss = -12573.888671875
Iteration 12800: Loss = -12573.8896484375
1
Iteration 12900: Loss = -12573.8896484375
2
Iteration 13000: Loss = -12573.888671875
Iteration 13100: Loss = -12573.888671875
Iteration 13200: Loss = -12573.888671875
Iteration 13300: Loss = -12573.88671875
Iteration 13400: Loss = -12573.88671875
Iteration 13500: Loss = -12573.8857421875
Iteration 13600: Loss = -12573.884765625
Iteration 13700: Loss = -12573.8857421875
1
Iteration 13800: Loss = -12573.8857421875
2
Iteration 13900: Loss = -12573.8857421875
3
Iteration 14000: Loss = -12573.8857421875
4
Iteration 14100: Loss = -12573.8828125
Iteration 14200: Loss = -12573.884765625
1
Iteration 14300: Loss = -12573.8837890625
2
Iteration 14400: Loss = -12573.8828125
Iteration 14500: Loss = -12573.8828125
Iteration 14600: Loss = -12573.884765625
1
Iteration 14700: Loss = -12573.8828125
Iteration 14800: Loss = -12573.8837890625
1
Iteration 14900: Loss = -12573.8828125
Iteration 15000: Loss = -12573.8837890625
1
Iteration 15100: Loss = -12573.8818359375
Iteration 15200: Loss = -12573.8818359375
Iteration 15300: Loss = -12573.880859375
Iteration 15400: Loss = -12573.8828125
1
Iteration 15500: Loss = -12573.8828125
2
Iteration 15600: Loss = -12573.8818359375
3
Iteration 15700: Loss = -12573.8818359375
4
Iteration 15800: Loss = -12573.8798828125
Iteration 15900: Loss = -12573.8798828125
Iteration 16000: Loss = -12573.8818359375
1
Iteration 16100: Loss = -12573.880859375
2
Iteration 16200: Loss = -12573.8798828125
Iteration 16300: Loss = -12573.8798828125
Iteration 16400: Loss = -12573.8798828125
Iteration 16500: Loss = -12573.880859375
1
Iteration 16600: Loss = -12573.87890625
Iteration 16700: Loss = -12573.8798828125
1
Iteration 16800: Loss = -12573.8798828125
2
Iteration 16900: Loss = -12573.8779296875
Iteration 17000: Loss = -12573.8759765625
Iteration 17100: Loss = -12573.876953125
1
Iteration 17200: Loss = -12573.8701171875
Iteration 17300: Loss = -12573.826171875
Iteration 17400: Loss = -12572.8876953125
Iteration 17500: Loss = -12572.75390625
Iteration 17600: Loss = -12572.7353515625
Iteration 17700: Loss = -12572.7314453125
Iteration 17800: Loss = -12572.7265625
Iteration 17900: Loss = -12572.7255859375
Iteration 18000: Loss = -12572.72265625
Iteration 18100: Loss = -12572.72265625
Iteration 18200: Loss = -12572.72265625
Iteration 18300: Loss = -12572.7216796875
Iteration 18400: Loss = -12572.7216796875
Iteration 18500: Loss = -12572.7216796875
Iteration 18600: Loss = -12572.720703125
Iteration 18700: Loss = -12572.720703125
Iteration 18800: Loss = -12572.720703125
Iteration 18900: Loss = -12572.7216796875
1
Iteration 19000: Loss = -12572.7197265625
Iteration 19100: Loss = -12572.7197265625
Iteration 19200: Loss = -12572.7216796875
1
Iteration 19300: Loss = -12572.7197265625
Iteration 19400: Loss = -12572.720703125
1
Iteration 19500: Loss = -12572.720703125
2
Iteration 19600: Loss = -12572.720703125
3
Iteration 19700: Loss = -12572.720703125
4
Iteration 19800: Loss = -12572.72265625
5
Iteration 19900: Loss = -12572.720703125
6
Iteration 20000: Loss = -12572.7216796875
7
Iteration 20100: Loss = -12572.7216796875
8
Iteration 20200: Loss = -12572.720703125
9
Iteration 20300: Loss = -12572.7216796875
10
Iteration 20400: Loss = -12572.7216796875
11
Iteration 20500: Loss = -12572.720703125
12
Iteration 20600: Loss = -12572.7197265625
Iteration 20700: Loss = -12572.7216796875
1
Iteration 20800: Loss = -12572.7216796875
2
Iteration 20900: Loss = -12572.7197265625
Iteration 21000: Loss = -12572.7197265625
Iteration 21100: Loss = -12572.720703125
1
Iteration 21200: Loss = -12572.7216796875
2
Iteration 21300: Loss = -12572.720703125
3
Iteration 21400: Loss = -12572.720703125
4
Iteration 21500: Loss = -12572.720703125
5
Iteration 21600: Loss = -12572.71875
Iteration 21700: Loss = -12572.720703125
1
Iteration 21800: Loss = -12572.7197265625
2
Iteration 21900: Loss = -12572.7197265625
3
Iteration 22000: Loss = -12572.71875
Iteration 22100: Loss = -12572.7197265625
1
Iteration 22200: Loss = -12572.7197265625
2
Iteration 22300: Loss = -12572.720703125
3
Iteration 22400: Loss = -12572.720703125
4
Iteration 22500: Loss = -12572.71875
Iteration 22600: Loss = -12572.720703125
1
Iteration 22700: Loss = -12572.7197265625
2
Iteration 22800: Loss = -12572.7197265625
3
Iteration 22900: Loss = -12572.7216796875
4
Iteration 23000: Loss = -12572.720703125
5
Iteration 23100: Loss = -12572.71875
Iteration 23200: Loss = -12572.7197265625
1
Iteration 23300: Loss = -12572.7197265625
2
Iteration 23400: Loss = -12572.71875
Iteration 23500: Loss = -12572.71875
Iteration 23600: Loss = -12572.720703125
1
Iteration 23700: Loss = -12572.720703125
2
Iteration 23800: Loss = -12572.720703125
3
Iteration 23900: Loss = -12572.7197265625
4
Iteration 24000: Loss = -12572.720703125
5
Iteration 24100: Loss = -12572.7197265625
6
Iteration 24200: Loss = -12572.7197265625
7
Iteration 24300: Loss = -12572.7197265625
8
Iteration 24400: Loss = -12572.7197265625
9
Iteration 24500: Loss = -12572.7197265625
10
Iteration 24600: Loss = -12572.7197265625
11
Iteration 24700: Loss = -12572.720703125
12
Iteration 24800: Loss = -12572.7197265625
13
Iteration 24900: Loss = -12572.720703125
14
Iteration 25000: Loss = -12572.7197265625
15
Stopping early at iteration 25000 due to no improvement.
pi: tensor([[1.0000e+00, 4.8757e-06],
        [1.0000e+00, 1.8109e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0033, 0.9967], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2075, 0.1979],
         [0.9890, 0.1978]],

        [[0.5254, 0.2283],
         [0.2207, 0.4648]],

        [[0.6677, 0.2219],
         [0.1338, 0.9521]],

        [[0.0095, 0.1946],
         [0.0834, 0.9222]],

        [[0.7940, 0.1985],
         [0.0632, 0.6234]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0012789331926026277
Average Adjusted Rand Index: 0.0
[-0.0012789331926026277, -0.0012789331926026277] [0.0, 0.0] [12572.7197265625, 12572.7197265625]
-------------------------------------
This iteration is 95
True Objective function: Loss = -11922.93987429604
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37483.4765625
Iteration 100: Loss = -22244.4921875
Iteration 200: Loss = -14526.34765625
Iteration 300: Loss = -12786.1552734375
Iteration 400: Loss = -12525.935546875
Iteration 500: Loss = -12466.677734375
Iteration 600: Loss = -12441.4736328125
Iteration 700: Loss = -12425.5029296875
Iteration 800: Loss = -12415.822265625
Iteration 900: Loss = -12409.3818359375
Iteration 1000: Loss = -12404.7421875
Iteration 1100: Loss = -12401.3046875
Iteration 1200: Loss = -12398.6591796875
Iteration 1300: Loss = -12396.5693359375
Iteration 1400: Loss = -12394.884765625
Iteration 1500: Loss = -12393.5068359375
Iteration 1600: Loss = -12392.3623046875
Iteration 1700: Loss = -12391.40234375
Iteration 1800: Loss = -12390.5888671875
Iteration 1900: Loss = -12389.890625
Iteration 2000: Loss = -12389.2890625
Iteration 2100: Loss = -12388.767578125
Iteration 2200: Loss = -12388.310546875
Iteration 2300: Loss = -12387.9111328125
Iteration 2400: Loss = -12387.5576171875
Iteration 2500: Loss = -12387.2431640625
Iteration 2600: Loss = -12386.96484375
Iteration 2700: Loss = -12386.7109375
Iteration 2800: Loss = -12386.484375
Iteration 2900: Loss = -12386.27734375
Iteration 3000: Loss = -12386.087890625
Iteration 3100: Loss = -12385.91796875
Iteration 3200: Loss = -12385.7626953125
Iteration 3300: Loss = -12385.619140625
Iteration 3400: Loss = -12385.486328125
Iteration 3500: Loss = -12385.3662109375
Iteration 3600: Loss = -12385.2529296875
Iteration 3700: Loss = -12385.1474609375
Iteration 3800: Loss = -12385.0517578125
Iteration 3900: Loss = -12384.962890625
Iteration 4000: Loss = -12384.8798828125
Iteration 4100: Loss = -12384.8046875
Iteration 4200: Loss = -12384.7314453125
Iteration 4300: Loss = -12384.666015625
Iteration 4400: Loss = -12384.6044921875
Iteration 4500: Loss = -12384.5458984375
Iteration 4600: Loss = -12384.490234375
Iteration 4700: Loss = -12384.439453125
Iteration 4800: Loss = -12384.392578125
Iteration 4900: Loss = -12384.3466796875
Iteration 5000: Loss = -12384.302734375
Iteration 5100: Loss = -12384.2646484375
Iteration 5200: Loss = -12384.2265625
Iteration 5300: Loss = -12384.1923828125
Iteration 5400: Loss = -12384.15625
Iteration 5500: Loss = -12384.125
Iteration 5600: Loss = -12384.0966796875
Iteration 5700: Loss = -12384.0693359375
Iteration 5800: Loss = -12384.041015625
Iteration 5900: Loss = -12384.0166015625
Iteration 6000: Loss = -12383.9931640625
Iteration 6100: Loss = -12383.970703125
Iteration 6200: Loss = -12383.94921875
Iteration 6300: Loss = -12383.9296875
Iteration 6400: Loss = -12383.9091796875
Iteration 6500: Loss = -12383.890625
Iteration 6600: Loss = -12383.873046875
Iteration 6700: Loss = -12383.8583984375
Iteration 6800: Loss = -12383.8447265625
Iteration 6900: Loss = -12383.8291015625
Iteration 7000: Loss = -12383.81640625
Iteration 7100: Loss = -12383.80078125
Iteration 7200: Loss = -12383.7890625
Iteration 7300: Loss = -12383.779296875
Iteration 7400: Loss = -12383.7666015625
Iteration 7500: Loss = -12383.7578125
Iteration 7600: Loss = -12383.7470703125
Iteration 7700: Loss = -12383.7373046875
Iteration 7800: Loss = -12383.7275390625
Iteration 7900: Loss = -12383.7197265625
Iteration 8000: Loss = -12383.712890625
Iteration 8100: Loss = -12383.705078125
Iteration 8200: Loss = -12383.697265625
Iteration 8300: Loss = -12383.69140625
Iteration 8400: Loss = -12383.6865234375
Iteration 8500: Loss = -12383.6796875
Iteration 8600: Loss = -12383.67578125
Iteration 8700: Loss = -12383.66796875
Iteration 8800: Loss = -12383.6640625
Iteration 8900: Loss = -12383.658203125
Iteration 9000: Loss = -12383.654296875
Iteration 9100: Loss = -12383.650390625
Iteration 9200: Loss = -12383.6455078125
Iteration 9300: Loss = -12383.640625
Iteration 9400: Loss = -12383.63671875
Iteration 9500: Loss = -12383.6328125
Iteration 9600: Loss = -12383.630859375
Iteration 9700: Loss = -12383.6279296875
Iteration 9800: Loss = -12383.6240234375
Iteration 9900: Loss = -12383.62109375
Iteration 10000: Loss = -12383.619140625
Iteration 10100: Loss = -12383.6171875
Iteration 10200: Loss = -12383.61328125
Iteration 10300: Loss = -12383.6103515625
Iteration 10400: Loss = -12383.609375
Iteration 10500: Loss = -12383.6064453125
Iteration 10600: Loss = -12383.60546875
Iteration 10700: Loss = -12383.6025390625
Iteration 10800: Loss = -12383.6005859375
Iteration 10900: Loss = -12383.6005859375
Iteration 11000: Loss = -12383.59765625
Iteration 11100: Loss = -12383.5966796875
Iteration 11200: Loss = -12383.5947265625
Iteration 11300: Loss = -12383.59375
Iteration 11400: Loss = -12383.5927734375
Iteration 11500: Loss = -12383.58984375
Iteration 11600: Loss = -12383.58984375
Iteration 11700: Loss = -12383.587890625
Iteration 11800: Loss = -12383.5869140625
Iteration 11900: Loss = -12383.5859375
Iteration 12000: Loss = -12383.5830078125
Iteration 12100: Loss = -12383.5703125
Iteration 12200: Loss = -12383.5537109375
Iteration 12300: Loss = -12383.541015625
Iteration 12400: Loss = -12383.53515625
Iteration 12500: Loss = -12383.5283203125
Iteration 12600: Loss = -12383.525390625
Iteration 12700: Loss = -12383.521484375
Iteration 12800: Loss = -12383.517578125
Iteration 12900: Loss = -12383.5146484375
Iteration 13000: Loss = -12383.51171875
Iteration 13100: Loss = -12383.509765625
Iteration 13200: Loss = -12383.5048828125
Iteration 13300: Loss = -12383.50390625
Iteration 13400: Loss = -12383.5
Iteration 13500: Loss = -12383.4931640625
Iteration 13600: Loss = -12383.48046875
Iteration 13700: Loss = -12383.466796875
Iteration 13800: Loss = -12383.4580078125
Iteration 13900: Loss = -12383.451171875
Iteration 14000: Loss = -12383.443359375
Iteration 14100: Loss = -12383.4375
Iteration 14200: Loss = -12383.431640625
Iteration 14300: Loss = -12383.423828125
Iteration 14400: Loss = -12383.416015625
Iteration 14500: Loss = -12383.4072265625
Iteration 14600: Loss = -12383.3955078125
Iteration 14700: Loss = -12383.3837890625
Iteration 14800: Loss = -12383.3681640625
Iteration 14900: Loss = -12383.3486328125
Iteration 15000: Loss = -12383.322265625
Iteration 15100: Loss = -12383.2861328125
Iteration 15200: Loss = -12383.2333984375
Iteration 15300: Loss = -12383.16015625
Iteration 15400: Loss = -12383.07421875
Iteration 15500: Loss = -12383.0078125
Iteration 15600: Loss = -12382.9814453125
Iteration 15700: Loss = -12382.9697265625
Iteration 15800: Loss = -12382.966796875
Iteration 15900: Loss = -12382.9638671875
Iteration 16000: Loss = -12382.9638671875
Iteration 16100: Loss = -12382.9619140625
Iteration 16200: Loss = -12382.962890625
1
Iteration 16300: Loss = -12382.962890625
2
Iteration 16400: Loss = -12382.962890625
3
Iteration 16500: Loss = -12382.9619140625
Iteration 16600: Loss = -12382.962890625
1
Iteration 16700: Loss = -12382.9638671875
2
Iteration 16800: Loss = -12382.9609375
Iteration 16900: Loss = -12382.9599609375
Iteration 17000: Loss = -12382.9619140625
1
Iteration 17100: Loss = -12382.9619140625
2
Iteration 17200: Loss = -12382.9599609375
Iteration 17300: Loss = -12382.962890625
1
Iteration 17400: Loss = -12382.9599609375
Iteration 17500: Loss = -12382.9609375
1
Iteration 17600: Loss = -12382.9599609375
Iteration 17700: Loss = -12382.9599609375
Iteration 17800: Loss = -12382.9599609375
Iteration 17900: Loss = -12382.9599609375
Iteration 18000: Loss = -12382.9599609375
Iteration 18100: Loss = -12382.9580078125
Iteration 18200: Loss = -12382.95703125
Iteration 18300: Loss = -12382.958984375
1
Iteration 18400: Loss = -12382.958984375
2
Iteration 18500: Loss = -12382.9599609375
3
Iteration 18600: Loss = -12382.9580078125
4
Iteration 18700: Loss = -12382.958984375
5
Iteration 18800: Loss = -12382.9599609375
6
Iteration 18900: Loss = -12382.9599609375
7
Iteration 19000: Loss = -12382.958984375
8
Iteration 19100: Loss = -12382.958984375
9
Iteration 19200: Loss = -12382.958984375
10
Iteration 19300: Loss = -12382.9599609375
11
Iteration 19400: Loss = -12382.9580078125
12
Iteration 19500: Loss = -12382.958984375
13
Iteration 19600: Loss = -12382.9580078125
14
Iteration 19700: Loss = -12382.9599609375
15
Stopping early at iteration 19700 due to no improvement.
pi: tensor([[0.2513, 0.7487],
        [0.0118, 0.9882]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([4.0080e-04, 9.9960e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2054, 0.1846],
         [0.3632, 0.1995]],

        [[0.0117, 0.1209],
         [0.7312, 0.4376]],

        [[0.7786, 0.2077],
         [0.0949, 0.0700]],

        [[0.7438, 0.2229],
         [0.0294, 0.1484]],

        [[0.1799, 0.3059],
         [0.2723, 0.0221]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: -0.0004811949056573946
Average Adjusted Rand Index: -0.0004529465619428516
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -34129.81640625
Iteration 100: Loss = -21160.9140625
Iteration 200: Loss = -14244.60546875
Iteration 300: Loss = -12998.2666015625
Iteration 400: Loss = -12723.5087890625
Iteration 500: Loss = -12614.6103515625
Iteration 600: Loss = -12549.96875
Iteration 700: Loss = -12506.390625
Iteration 800: Loss = -12484.173828125
Iteration 900: Loss = -12465.1845703125
Iteration 1000: Loss = -12454.134765625
Iteration 1100: Loss = -12446.640625
Iteration 1200: Loss = -12440.4267578125
Iteration 1300: Loss = -12432.7998046875
Iteration 1400: Loss = -12426.865234375
Iteration 1500: Loss = -12418.294921875
Iteration 1600: Loss = -12410.572265625
Iteration 1700: Loss = -12407.6650390625
Iteration 1800: Loss = -12405.642578125
Iteration 1900: Loss = -12404.0029296875
Iteration 2000: Loss = -12402.380859375
Iteration 2100: Loss = -12399.3291015625
Iteration 2200: Loss = -12397.4443359375
Iteration 2300: Loss = -12395.2392578125
Iteration 2400: Loss = -12393.0546875
Iteration 2500: Loss = -12391.8701171875
Iteration 2600: Loss = -12391.0595703125
Iteration 2700: Loss = -12390.4130859375
Iteration 2800: Loss = -12389.8642578125
Iteration 2900: Loss = -12389.38671875
Iteration 3000: Loss = -12388.9658203125
Iteration 3100: Loss = -12388.587890625
Iteration 3200: Loss = -12388.24609375
Iteration 3300: Loss = -12387.935546875
Iteration 3400: Loss = -12387.6552734375
Iteration 3500: Loss = -12387.39453125
Iteration 3600: Loss = -12387.1572265625
Iteration 3700: Loss = -12386.9365234375
Iteration 3800: Loss = -12386.734375
Iteration 3900: Loss = -12386.5478515625
Iteration 4000: Loss = -12386.373046875
Iteration 4100: Loss = -12386.2119140625
Iteration 4200: Loss = -12386.0634765625
Iteration 4300: Loss = -12385.9228515625
Iteration 4400: Loss = -12385.7919921875
Iteration 4500: Loss = -12385.669921875
Iteration 4600: Loss = -12385.5576171875
Iteration 4700: Loss = -12385.451171875
Iteration 4800: Loss = -12385.3515625
Iteration 4900: Loss = -12385.255859375
Iteration 5000: Loss = -12385.16796875
Iteration 5100: Loss = -12385.0849609375
Iteration 5200: Loss = -12385.005859375
Iteration 5300: Loss = -12384.93359375
Iteration 5400: Loss = -12384.865234375
Iteration 5500: Loss = -12384.798828125
Iteration 5600: Loss = -12384.736328125
Iteration 5700: Loss = -12384.677734375
Iteration 5800: Loss = -12384.623046875
Iteration 5900: Loss = -12384.5703125
Iteration 6000: Loss = -12384.5205078125
Iteration 6100: Loss = -12384.4736328125
Iteration 6200: Loss = -12384.4296875
Iteration 6300: Loss = -12384.3876953125
Iteration 6400: Loss = -12384.34765625
Iteration 6500: Loss = -12384.3115234375
Iteration 6600: Loss = -12384.2724609375
Iteration 6700: Loss = -12384.240234375
Iteration 6800: Loss = -12384.2080078125
Iteration 6900: Loss = -12384.17578125
Iteration 7000: Loss = -12384.1455078125
Iteration 7100: Loss = -12384.119140625
Iteration 7200: Loss = -12384.0927734375
Iteration 7300: Loss = -12384.068359375
Iteration 7400: Loss = -12384.0439453125
Iteration 7500: Loss = -12384.01953125
Iteration 7600: Loss = -12384.0
Iteration 7700: Loss = -12383.9794921875
Iteration 7800: Loss = -12383.9599609375
Iteration 7900: Loss = -12383.9404296875
Iteration 8000: Loss = -12383.9248046875
Iteration 8100: Loss = -12383.90625
Iteration 8200: Loss = -12383.8916015625
Iteration 8300: Loss = -12383.8759765625
Iteration 8400: Loss = -12383.861328125
Iteration 8500: Loss = -12383.8466796875
Iteration 8600: Loss = -12383.8349609375
Iteration 8700: Loss = -12383.822265625
Iteration 8800: Loss = -12383.8095703125
Iteration 8900: Loss = -12383.796875
Iteration 9000: Loss = -12383.7890625
Iteration 9100: Loss = -12383.77734375
Iteration 9200: Loss = -12383.7685546875
Iteration 9300: Loss = -12383.759765625
Iteration 9400: Loss = -12383.748046875
Iteration 9500: Loss = -12383.740234375
Iteration 9600: Loss = -12383.732421875
Iteration 9700: Loss = -12383.724609375
Iteration 9800: Loss = -12383.716796875
Iteration 9900: Loss = -12383.7109375
Iteration 10000: Loss = -12383.703125
Iteration 10100: Loss = -12383.6982421875
Iteration 10200: Loss = -12383.69140625
Iteration 10300: Loss = -12383.6865234375
Iteration 10400: Loss = -12383.6806640625
Iteration 10500: Loss = -12383.6767578125
Iteration 10600: Loss = -12383.671875
Iteration 10700: Loss = -12383.6650390625
Iteration 10800: Loss = -12383.6611328125
Iteration 10900: Loss = -12383.65625
Iteration 11000: Loss = -12383.65234375
Iteration 11100: Loss = -12383.6484375
Iteration 11200: Loss = -12383.6455078125
Iteration 11300: Loss = -12383.6416015625
Iteration 11400: Loss = -12383.6376953125
Iteration 11500: Loss = -12383.634765625
Iteration 11600: Loss = -12383.6328125
Iteration 11700: Loss = -12383.6298828125
Iteration 11800: Loss = -12383.625
Iteration 11900: Loss = -12383.6240234375
Iteration 12000: Loss = -12383.6220703125
Iteration 12100: Loss = -12383.619140625
Iteration 12200: Loss = -12383.6171875
Iteration 12300: Loss = -12383.615234375
Iteration 12400: Loss = -12383.61328125
Iteration 12500: Loss = -12383.609375
Iteration 12600: Loss = -12383.609375
Iteration 12700: Loss = -12383.607421875
Iteration 12800: Loss = -12383.6044921875
Iteration 12900: Loss = -12383.603515625
Iteration 13000: Loss = -12383.603515625
Iteration 13100: Loss = -12383.6005859375
Iteration 13200: Loss = -12383.599609375
Iteration 13300: Loss = -12383.5966796875
Iteration 13400: Loss = -12383.5966796875
Iteration 13500: Loss = -12383.595703125
Iteration 13600: Loss = -12383.595703125
Iteration 13700: Loss = -12383.5927734375
Iteration 13800: Loss = -12383.5908203125
Iteration 13900: Loss = -12383.591796875
1
Iteration 14000: Loss = -12383.5908203125
Iteration 14100: Loss = -12383.5888671875
Iteration 14200: Loss = -12383.5888671875
Iteration 14300: Loss = -12383.587890625
Iteration 14400: Loss = -12383.5869140625
Iteration 14500: Loss = -12383.5859375
Iteration 14600: Loss = -12383.5859375
Iteration 14700: Loss = -12383.583984375
Iteration 14800: Loss = -12383.583984375
Iteration 14900: Loss = -12383.583984375
Iteration 15000: Loss = -12383.5830078125
Iteration 15100: Loss = -12383.583984375
1
Iteration 15200: Loss = -12383.5830078125
Iteration 15300: Loss = -12383.580078125
Iteration 15400: Loss = -12383.580078125
Iteration 15500: Loss = -12383.583984375
1
Iteration 15600: Loss = -12383.580078125
Iteration 15700: Loss = -12383.580078125
Iteration 15800: Loss = -12383.578125
Iteration 15900: Loss = -12383.578125
Iteration 16000: Loss = -12383.5791015625
1
Iteration 16100: Loss = -12383.5791015625
2
Iteration 16200: Loss = -12383.578125
Iteration 16300: Loss = -12383.5771484375
Iteration 16400: Loss = -12383.5771484375
Iteration 16500: Loss = -12383.576171875
Iteration 16600: Loss = -12383.576171875
Iteration 16700: Loss = -12383.576171875
Iteration 16800: Loss = -12383.5771484375
1
Iteration 16900: Loss = -12383.576171875
Iteration 17000: Loss = -12383.576171875
Iteration 17100: Loss = -12383.576171875
Iteration 17200: Loss = -12383.5751953125
Iteration 17300: Loss = -12383.576171875
1
Iteration 17400: Loss = -12383.57421875
Iteration 17500: Loss = -12383.5751953125
1
Iteration 17600: Loss = -12383.5732421875
Iteration 17700: Loss = -12383.5751953125
1
Iteration 17800: Loss = -12383.5751953125
2
Iteration 17900: Loss = -12383.57421875
3
Iteration 18000: Loss = -12383.57421875
4
Iteration 18100: Loss = -12383.5732421875
Iteration 18200: Loss = -12383.5732421875
Iteration 18300: Loss = -12383.5732421875
Iteration 18400: Loss = -12383.57421875
1
Iteration 18500: Loss = -12383.57421875
2
Iteration 18600: Loss = -12383.57421875
3
Iteration 18700: Loss = -12383.5732421875
Iteration 18800: Loss = -12383.57421875
1
Iteration 18900: Loss = -12383.572265625
Iteration 19000: Loss = -12383.5751953125
1
Iteration 19100: Loss = -12383.57421875
2
Iteration 19200: Loss = -12383.5732421875
3
Iteration 19300: Loss = -12383.5732421875
4
Iteration 19400: Loss = -12383.5732421875
5
Iteration 19500: Loss = -12383.57421875
6
Iteration 19600: Loss = -12383.57421875
7
Iteration 19700: Loss = -12383.57421875
8
Iteration 19800: Loss = -12383.5712890625
Iteration 19900: Loss = -12383.5712890625
Iteration 20000: Loss = -12383.572265625
1
Iteration 20100: Loss = -12383.572265625
2
Iteration 20200: Loss = -12383.5712890625
Iteration 20300: Loss = -12383.5712890625
Iteration 20400: Loss = -12383.5703125
Iteration 20500: Loss = -12383.5712890625
1
Iteration 20600: Loss = -12383.5693359375
Iteration 20700: Loss = -12383.5693359375
Iteration 20800: Loss = -12383.5703125
1
Iteration 20900: Loss = -12383.568359375
Iteration 21000: Loss = -12383.56640625
Iteration 21100: Loss = -12383.564453125
Iteration 21200: Loss = -12383.55078125
Iteration 21300: Loss = -12383.3349609375
Iteration 21400: Loss = -12383.310546875
Iteration 21500: Loss = -12383.3046875
Iteration 21600: Loss = -12383.3046875
Iteration 21700: Loss = -12383.302734375
Iteration 21800: Loss = -12383.3017578125
Iteration 21900: Loss = -12383.30078125
Iteration 22000: Loss = -12383.302734375
1
Iteration 22100: Loss = -12383.2998046875
Iteration 22200: Loss = -12383.3017578125
1
Iteration 22300: Loss = -12383.3037109375
2
Iteration 22400: Loss = -12383.2998046875
Iteration 22500: Loss = -12383.2998046875
Iteration 22600: Loss = -12383.2998046875
Iteration 22700: Loss = -12383.2998046875
Iteration 22800: Loss = -12383.2998046875
Iteration 22900: Loss = -12383.3017578125
1
Iteration 23000: Loss = -12383.2998046875
Iteration 23100: Loss = -12383.2978515625
Iteration 23200: Loss = -12383.2998046875
1
Iteration 23300: Loss = -12383.2998046875
2
Iteration 23400: Loss = -12383.2998046875
3
Iteration 23500: Loss = -12383.298828125
4
Iteration 23600: Loss = -12383.2998046875
5
Iteration 23700: Loss = -12383.302734375
6
Iteration 23800: Loss = -12383.2998046875
7
Iteration 23900: Loss = -12383.298828125
8
Iteration 24000: Loss = -12383.2998046875
9
Iteration 24100: Loss = -12383.298828125
10
Iteration 24200: Loss = -12383.2998046875
11
Iteration 24300: Loss = -12383.2998046875
12
Iteration 24400: Loss = -12383.30078125
13
Iteration 24500: Loss = -12383.2998046875
14
Iteration 24600: Loss = -12383.298828125
15
Stopping early at iteration 24600 due to no improvement.
pi: tensor([[1.0000e+00, 3.3045e-06],
        [9.8595e-01, 1.4046e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1627, 0.8373], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2010, 0.1989],
         [0.0085, 0.1973]],

        [[0.0239, 0.1181],
         [0.0074, 0.0854]],

        [[0.0447, 0.2235],
         [0.9600, 0.1984]],

        [[0.3402, 0.2056],
         [0.2772, 0.0075]],

        [[0.3043, 0.2030],
         [0.3223, 0.2812]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00044910763008398647
Average Adjusted Rand Index: 0.0
[-0.0004811949056573946, -0.00044910763008398647] [-0.0004529465619428516, 0.0] [12382.9599609375, 12383.298828125]
-------------------------------------
This iteration is 96
True Objective function: Loss = -11785.052158459708
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -55074.2109375
Iteration 100: Loss = -33146.99609375
Iteration 200: Loss = -17807.640625
Iteration 300: Loss = -13553.0263671875
Iteration 400: Loss = -12932.080078125
Iteration 500: Loss = -12682.70703125
Iteration 600: Loss = -12560.931640625
Iteration 700: Loss = -12486.2744140625
Iteration 800: Loss = -12431.1689453125
Iteration 900: Loss = -12399.4814453125
Iteration 1000: Loss = -12378.1708984375
Iteration 1100: Loss = -12361.2880859375
Iteration 1200: Loss = -12346.8330078125
Iteration 1300: Loss = -12334.708984375
Iteration 1400: Loss = -12325.259765625
Iteration 1500: Loss = -12316.2138671875
Iteration 1600: Loss = -12309.2587890625
Iteration 1700: Loss = -12304.4697265625
Iteration 1800: Loss = -12300.5205078125
Iteration 1900: Loss = -12297.177734375
Iteration 2000: Loss = -12294.3056640625
Iteration 2100: Loss = -12291.814453125
Iteration 2200: Loss = -12289.6337890625
Iteration 2300: Loss = -12287.7158203125
Iteration 2400: Loss = -12286.0146484375
Iteration 2500: Loss = -12284.501953125
Iteration 2600: Loss = -12283.150390625
Iteration 2700: Loss = -12281.9384765625
Iteration 2800: Loss = -12280.845703125
Iteration 2900: Loss = -12279.859375
Iteration 3000: Loss = -12278.9638671875
Iteration 3100: Loss = -12278.1494140625
Iteration 3200: Loss = -12277.4111328125
Iteration 3300: Loss = -12276.7373046875
Iteration 3400: Loss = -12276.1171875
Iteration 3500: Loss = -12275.55078125
Iteration 3600: Loss = -12275.0322265625
Iteration 3700: Loss = -12274.5517578125
Iteration 3800: Loss = -12274.1103515625
Iteration 3900: Loss = -12273.7021484375
Iteration 4000: Loss = -12273.326171875
Iteration 4100: Loss = -12272.9755859375
Iteration 4200: Loss = -12272.650390625
Iteration 4300: Loss = -12272.3505859375
Iteration 4400: Loss = -12272.068359375
Iteration 4500: Loss = -12271.8076171875
Iteration 4600: Loss = -12271.5625
Iteration 4700: Loss = -12271.3359375
Iteration 4800: Loss = -12271.1201171875
Iteration 4900: Loss = -12270.919921875
Iteration 5000: Loss = -12270.732421875
Iteration 5100: Loss = -12270.5576171875
Iteration 5200: Loss = -12270.3935546875
Iteration 5300: Loss = -12270.240234375
Iteration 5400: Loss = -12270.09375
Iteration 5500: Loss = -12269.958984375
Iteration 5600: Loss = -12269.83203125
Iteration 5700: Loss = -12269.7080078125
Iteration 5800: Loss = -12269.595703125
Iteration 5900: Loss = -12269.48828125
Iteration 6000: Loss = -12269.38671875
Iteration 6100: Loss = -12269.29296875
Iteration 6200: Loss = -12269.2021484375
Iteration 6300: Loss = -12269.1162109375
Iteration 6400: Loss = -12269.0361328125
Iteration 6500: Loss = -12268.9599609375
Iteration 6600: Loss = -12268.888671875
Iteration 6700: Loss = -12268.8212890625
Iteration 6800: Loss = -12268.7568359375
Iteration 6900: Loss = -12268.6953125
Iteration 7000: Loss = -12268.638671875
Iteration 7100: Loss = -12268.583984375
Iteration 7200: Loss = -12268.5322265625
Iteration 7300: Loss = -12268.4833984375
Iteration 7400: Loss = -12268.4375
Iteration 7500: Loss = -12268.3935546875
Iteration 7600: Loss = -12268.3525390625
Iteration 7700: Loss = -12268.3125
Iteration 7800: Loss = -12268.2763671875
Iteration 7900: Loss = -12268.2412109375
Iteration 8000: Loss = -12268.2080078125
Iteration 8100: Loss = -12268.1767578125
Iteration 8200: Loss = -12268.14453125
Iteration 8300: Loss = -12268.1171875
Iteration 8400: Loss = -12268.0888671875
Iteration 8500: Loss = -12268.0625
Iteration 8600: Loss = -12268.0400390625
Iteration 8700: Loss = -12268.0166015625
Iteration 8800: Loss = -12267.994140625
Iteration 8900: Loss = -12267.974609375
Iteration 9000: Loss = -12267.955078125
Iteration 9100: Loss = -12267.9345703125
Iteration 9200: Loss = -12267.9150390625
Iteration 9300: Loss = -12267.8974609375
Iteration 9400: Loss = -12267.8818359375
Iteration 9500: Loss = -12267.865234375
Iteration 9600: Loss = -12267.8515625
Iteration 9700: Loss = -12267.833984375
Iteration 9800: Loss = -12267.814453125
Iteration 9900: Loss = -12267.7890625
Iteration 10000: Loss = -12267.7587890625
Iteration 10100: Loss = -12267.7236328125
Iteration 10200: Loss = -12267.701171875
Iteration 10300: Loss = -12267.6875
Iteration 10400: Loss = -12267.6796875
Iteration 10500: Loss = -12267.671875
Iteration 10600: Loss = -12267.666015625
Iteration 10700: Loss = -12267.6572265625
Iteration 10800: Loss = -12267.6484375
Iteration 10900: Loss = -12267.634765625
Iteration 11000: Loss = -12267.623046875
Iteration 11100: Loss = -12267.6103515625
Iteration 11200: Loss = -12267.5966796875
Iteration 11300: Loss = -12267.5732421875
Iteration 11400: Loss = -12267.46484375
Iteration 11500: Loss = -12266.8955078125
Iteration 11600: Loss = -12266.77734375
Iteration 11700: Loss = -12266.73046875
Iteration 11800: Loss = -12266.650390625
Iteration 11900: Loss = -12266.408203125
Iteration 12000: Loss = -12266.380859375
Iteration 12100: Loss = -12266.357421875
Iteration 12200: Loss = -12266.3408203125
Iteration 12300: Loss = -12266.3232421875
Iteration 12400: Loss = -12266.3046875
Iteration 12500: Loss = -12266.2890625
Iteration 12600: Loss = -12266.2734375
Iteration 12700: Loss = -12266.255859375
Iteration 12800: Loss = -12266.2373046875
Iteration 12900: Loss = -12266.212890625
Iteration 13000: Loss = -12266.185546875
Iteration 13100: Loss = -12266.150390625
Iteration 13200: Loss = -12266.0966796875
Iteration 13300: Loss = -12266.029296875
Iteration 13400: Loss = -12265.9697265625
Iteration 13500: Loss = -12265.904296875
Iteration 13600: Loss = -12265.84375
Iteration 13700: Loss = -12265.7841796875
Iteration 13800: Loss = -12265.736328125
Iteration 13900: Loss = -12265.6923828125
Iteration 14000: Loss = -12265.65234375
Iteration 14100: Loss = -12265.6298828125
Iteration 14200: Loss = -12265.6103515625
Iteration 14300: Loss = -12265.5927734375
Iteration 14400: Loss = -12265.580078125
Iteration 14500: Loss = -12265.5712890625
Iteration 14600: Loss = -12265.5498046875
Iteration 14700: Loss = -12265.533203125
Iteration 14800: Loss = -12265.5205078125
Iteration 14900: Loss = -12265.50390625
Iteration 15000: Loss = -12265.4873046875
Iteration 15100: Loss = -12265.4697265625
Iteration 15200: Loss = -12265.3974609375
Iteration 15300: Loss = -12265.3779296875
Iteration 15400: Loss = -12265.36328125
Iteration 15500: Loss = -12265.3564453125
Iteration 15600: Loss = -12265.345703125
Iteration 15700: Loss = -12265.337890625
Iteration 15800: Loss = -12265.3291015625
Iteration 15900: Loss = -12265.3203125
Iteration 16000: Loss = -12265.3173828125
Iteration 16100: Loss = -12265.314453125
Iteration 16200: Loss = -12265.30859375
Iteration 16300: Loss = -12265.302734375
Iteration 16400: Loss = -12265.298828125
Iteration 16500: Loss = -12265.294921875
Iteration 16600: Loss = -12265.2890625
Iteration 16700: Loss = -12265.283203125
Iteration 16800: Loss = -12265.2822265625
Iteration 16900: Loss = -12265.2783203125
Iteration 17000: Loss = -12265.2744140625
Iteration 17100: Loss = -12265.26953125
Iteration 17200: Loss = -12265.25
Iteration 17300: Loss = -12265.0732421875
Iteration 17400: Loss = -12264.9658203125
Iteration 17500: Loss = -12264.9345703125
Iteration 17600: Loss = -12264.908203125
Iteration 17700: Loss = -12264.896484375
Iteration 17800: Loss = -12264.8837890625
Iteration 17900: Loss = -12264.87890625
Iteration 18000: Loss = -12264.8759765625
Iteration 18100: Loss = -12264.87109375
Iteration 18200: Loss = -12264.869140625
Iteration 18300: Loss = -12264.8671875
Iteration 18400: Loss = -12264.8662109375
Iteration 18500: Loss = -12264.8671875
1
Iteration 18600: Loss = -12264.8671875
2
Iteration 18700: Loss = -12264.8681640625
3
Iteration 18800: Loss = -12264.8671875
4
Iteration 18900: Loss = -12264.869140625
5
Iteration 19000: Loss = -12264.869140625
6
Iteration 19100: Loss = -12264.8681640625
7
Iteration 19200: Loss = -12264.8671875
8
Iteration 19300: Loss = -12264.869140625
9
Iteration 19400: Loss = -12264.8662109375
Iteration 19500: Loss = -12264.8662109375
Iteration 19600: Loss = -12264.865234375
Iteration 19700: Loss = -12264.8681640625
1
Iteration 19800: Loss = -12264.8681640625
2
Iteration 19900: Loss = -12264.8662109375
3
Iteration 20000: Loss = -12264.8681640625
4
Iteration 20100: Loss = -12264.8671875
5
Iteration 20200: Loss = -12264.8681640625
6
Iteration 20300: Loss = -12264.8671875
7
Iteration 20400: Loss = -12264.8681640625
8
Iteration 20500: Loss = -12264.8671875
9
Iteration 20600: Loss = -12264.8662109375
10
Iteration 20700: Loss = -12264.8671875
11
Iteration 20800: Loss = -12264.8681640625
12
Iteration 20900: Loss = -12264.8662109375
13
Iteration 21000: Loss = -12264.8662109375
14
Iteration 21100: Loss = -12264.8671875
15
Stopping early at iteration 21100 due to no improvement.
pi: tensor([[0.9385, 0.0615],
        [0.5947, 0.4053]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 9.0005e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1911, 0.1969],
         [0.4152, 0.3021]],

        [[0.5558, 0.2003],
         [0.6467, 0.0155]],

        [[0.8161, 0.2412],
         [0.9056, 0.9712]],

        [[0.0246, 0.2384],
         [0.0091, 0.9382]],

        [[0.9855, 0.2354],
         [0.9872, 0.0086]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 61
Adjusted Rand Index: -0.015623423336712405
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
Global Adjusted Rand Index: -6.336374590047627e-05
Average Adjusted Rand Index: -0.0035356619902333825
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -53507.92578125
Iteration 100: Loss = -40212.84765625
Iteration 200: Loss = -29699.123046875
Iteration 300: Loss = -21807.41015625
Iteration 400: Loss = -16798.6484375
Iteration 500: Loss = -14190.5849609375
Iteration 600: Loss = -13048.091796875
Iteration 700: Loss = -12577.4140625
Iteration 800: Loss = -12372.9951171875
Iteration 900: Loss = -12280.201171875
Iteration 1000: Loss = -12237.80859375
Iteration 1100: Loss = -12211.01953125
Iteration 1200: Loss = -12191.4658203125
Iteration 1300: Loss = -12175.1767578125
Iteration 1400: Loss = -12156.244140625
Iteration 1500: Loss = -12126.857421875
Iteration 1600: Loss = -12105.126953125
Iteration 1700: Loss = -12072.26953125
Iteration 1800: Loss = -12043.46484375
Iteration 1900: Loss = -12003.5078125
Iteration 2000: Loss = -11968.1015625
Iteration 2100: Loss = -11955.5185546875
Iteration 2200: Loss = -11948.501953125
Iteration 2300: Loss = -11939.7431640625
Iteration 2400: Loss = -11920.0263671875
Iteration 2500: Loss = -11907.177734375
Iteration 2600: Loss = -11889.6396484375
Iteration 2700: Loss = -11882.2255859375
Iteration 2800: Loss = -11866.955078125
Iteration 2900: Loss = -11864.0595703125
Iteration 3000: Loss = -11850.9599609375
Iteration 3100: Loss = -11848.912109375
Iteration 3200: Loss = -11848.2861328125
Iteration 3300: Loss = -11847.41015625
Iteration 3400: Loss = -11837.3671875
Iteration 3500: Loss = -11836.373046875
Iteration 3600: Loss = -11836.07421875
Iteration 3700: Loss = -11835.85546875
Iteration 3800: Loss = -11835.6689453125
Iteration 3900: Loss = -11835.48046875
Iteration 4000: Loss = -11835.2236328125
Iteration 4100: Loss = -11833.1826171875
Iteration 4200: Loss = -11818.7099609375
Iteration 4300: Loss = -11814.9150390625
Iteration 4400: Loss = -11814.5986328125
Iteration 4500: Loss = -11814.43359375
Iteration 4600: Loss = -11814.3115234375
Iteration 4700: Loss = -11814.1953125
Iteration 4800: Loss = -11803.701171875
Iteration 4900: Loss = -11801.2548828125
Iteration 5000: Loss = -11799.865234375
Iteration 5100: Loss = -11799.11328125
Iteration 5200: Loss = -11798.9990234375
Iteration 5300: Loss = -11798.9287109375
Iteration 5400: Loss = -11798.87890625
Iteration 5500: Loss = -11798.8349609375
Iteration 5600: Loss = -11798.796875
Iteration 5700: Loss = -11798.763671875
Iteration 5800: Loss = -11798.7353515625
Iteration 5900: Loss = -11798.7080078125
Iteration 6000: Loss = -11798.6826171875
Iteration 6100: Loss = -11798.662109375
Iteration 6200: Loss = -11798.6416015625
Iteration 6300: Loss = -11798.623046875
Iteration 6400: Loss = -11798.6064453125
Iteration 6500: Loss = -11798.5908203125
Iteration 6600: Loss = -11798.576171875
Iteration 6700: Loss = -11798.5615234375
Iteration 6800: Loss = -11798.5498046875
Iteration 6900: Loss = -11798.537109375
Iteration 7000: Loss = -11798.5263671875
Iteration 7100: Loss = -11798.5166015625
Iteration 7200: Loss = -11798.5078125
Iteration 7300: Loss = -11798.5
Iteration 7400: Loss = -11798.4912109375
Iteration 7500: Loss = -11798.4833984375
Iteration 7600: Loss = -11798.4755859375
Iteration 7700: Loss = -11798.470703125
Iteration 7800: Loss = -11798.462890625
Iteration 7900: Loss = -11798.4560546875
Iteration 8000: Loss = -11798.4501953125
Iteration 8100: Loss = -11798.4462890625
Iteration 8200: Loss = -11798.4404296875
Iteration 8300: Loss = -11798.435546875
Iteration 8400: Loss = -11798.431640625
Iteration 8500: Loss = -11798.427734375
Iteration 8600: Loss = -11798.4228515625
Iteration 8700: Loss = -11798.419921875
Iteration 8800: Loss = -11798.416015625
Iteration 8900: Loss = -11798.4140625
Iteration 9000: Loss = -11798.41015625
Iteration 9100: Loss = -11798.408203125
Iteration 9200: Loss = -11798.4052734375
Iteration 9300: Loss = -11798.40234375
Iteration 9400: Loss = -11798.400390625
Iteration 9500: Loss = -11798.396484375
Iteration 9600: Loss = -11798.3955078125
Iteration 9700: Loss = -11798.3935546875
Iteration 9800: Loss = -11798.3916015625
Iteration 9900: Loss = -11798.3896484375
Iteration 10000: Loss = -11798.38671875
Iteration 10100: Loss = -11798.38671875
Iteration 10200: Loss = -11798.3837890625
Iteration 10300: Loss = -11798.3828125
Iteration 10400: Loss = -11798.3818359375
Iteration 10500: Loss = -11798.380859375
Iteration 10600: Loss = -11798.380859375
Iteration 10700: Loss = -11798.3779296875
Iteration 10800: Loss = -11798.3759765625
Iteration 10900: Loss = -11798.3759765625
Iteration 11000: Loss = -11798.3740234375
Iteration 11100: Loss = -11798.375
1
Iteration 11200: Loss = -11798.373046875
Iteration 11300: Loss = -11798.3720703125
Iteration 11400: Loss = -11798.37109375
Iteration 11500: Loss = -11798.37109375
Iteration 11600: Loss = -11798.369140625
Iteration 11700: Loss = -11798.3701171875
1
Iteration 11800: Loss = -11798.369140625
Iteration 11900: Loss = -11798.3681640625
Iteration 12000: Loss = -11798.3662109375
Iteration 12100: Loss = -11798.3681640625
1
Iteration 12200: Loss = -11798.365234375
Iteration 12300: Loss = -11798.365234375
Iteration 12400: Loss = -11798.3642578125
Iteration 12500: Loss = -11798.3642578125
Iteration 12600: Loss = -11798.365234375
1
Iteration 12700: Loss = -11798.36328125
Iteration 12800: Loss = -11798.3642578125
1
Iteration 12900: Loss = -11798.3642578125
2
Iteration 13000: Loss = -11798.36328125
Iteration 13100: Loss = -11798.3642578125
1
Iteration 13200: Loss = -11798.36328125
Iteration 13300: Loss = -11798.3623046875
Iteration 13400: Loss = -11798.3623046875
Iteration 13500: Loss = -11798.3623046875
Iteration 13600: Loss = -11798.3623046875
Iteration 13700: Loss = -11798.361328125
Iteration 13800: Loss = -11798.361328125
Iteration 13900: Loss = -11798.3603515625
Iteration 14000: Loss = -11798.3603515625
Iteration 14100: Loss = -11798.359375
Iteration 14200: Loss = -11798.361328125
1
Iteration 14300: Loss = -11798.3603515625
2
Iteration 14400: Loss = -11798.359375
Iteration 14500: Loss = -11798.3603515625
1
Iteration 14600: Loss = -11798.3603515625
2
Iteration 14700: Loss = -11798.361328125
3
Iteration 14800: Loss = -11798.359375
Iteration 14900: Loss = -11798.359375
Iteration 15000: Loss = -11798.359375
Iteration 15100: Loss = -11798.359375
Iteration 15200: Loss = -11798.3583984375
Iteration 15300: Loss = -11798.359375
1
Iteration 15400: Loss = -11798.357421875
Iteration 15500: Loss = -11798.3583984375
1
Iteration 15600: Loss = -11798.3583984375
2
Iteration 15700: Loss = -11798.3583984375
3
Iteration 15800: Loss = -11798.3583984375
4
Iteration 15900: Loss = -11798.359375
5
Iteration 16000: Loss = -11798.359375
6
Iteration 16100: Loss = -11798.3583984375
7
Iteration 16200: Loss = -11798.3583984375
8
Iteration 16300: Loss = -11798.3583984375
9
Iteration 16400: Loss = -11798.3583984375
10
Iteration 16500: Loss = -11798.3583984375
11
Iteration 16600: Loss = -11798.3583984375
12
Iteration 16700: Loss = -11798.3583984375
13
Iteration 16800: Loss = -11798.357421875
Iteration 16900: Loss = -11798.3583984375
1
Iteration 17000: Loss = -11798.3583984375
2
Iteration 17100: Loss = -11798.357421875
Iteration 17200: Loss = -11798.359375
1
Iteration 17300: Loss = -11798.3583984375
2
Iteration 17400: Loss = -11798.3583984375
3
Iteration 17500: Loss = -11798.357421875
Iteration 17600: Loss = -11798.3583984375
1
Iteration 17700: Loss = -11798.3583984375
2
Iteration 17800: Loss = -11798.357421875
Iteration 17900: Loss = -11798.3583984375
1
Iteration 18000: Loss = -11798.357421875
Iteration 18100: Loss = -11798.3583984375
1
Iteration 18200: Loss = -11798.3564453125
Iteration 18300: Loss = -11798.3583984375
1
Iteration 18400: Loss = -11798.3583984375
2
Iteration 18500: Loss = -11798.3583984375
3
Iteration 18600: Loss = -11798.359375
4
Iteration 18700: Loss = -11798.3583984375
5
Iteration 18800: Loss = -11798.3583984375
6
Iteration 18900: Loss = -11798.357421875
7
Iteration 19000: Loss = -11798.3583984375
8
Iteration 19100: Loss = -11798.357421875
9
Iteration 19200: Loss = -11798.3583984375
10
Iteration 19300: Loss = -11798.357421875
11
Iteration 19400: Loss = -11798.3583984375
12
Iteration 19500: Loss = -11798.3583984375
13
Iteration 19600: Loss = -11798.3564453125
Iteration 19700: Loss = -11798.3583984375
1
Iteration 19800: Loss = -11798.3564453125
Iteration 19900: Loss = -11798.3583984375
1
Iteration 20000: Loss = -11798.357421875
2
Iteration 20100: Loss = -11798.357421875
3
Iteration 20200: Loss = -11798.357421875
4
Iteration 20300: Loss = -11798.3564453125
Iteration 20400: Loss = -11798.3564453125
Iteration 20500: Loss = -11798.3583984375
1
Iteration 20600: Loss = -11798.357421875
2
Iteration 20700: Loss = -11798.3583984375
3
Iteration 20800: Loss = -11798.357421875
4
Iteration 20900: Loss = -11798.3564453125
Iteration 21000: Loss = -11798.3583984375
1
Iteration 21100: Loss = -11798.3564453125
Iteration 21200: Loss = -11798.3564453125
Iteration 21300: Loss = -11798.357421875
1
Iteration 21400: Loss = -11798.357421875
2
Iteration 21500: Loss = -11798.357421875
3
Iteration 21600: Loss = -11798.357421875
4
Iteration 21700: Loss = -11798.357421875
5
Iteration 21800: Loss = -11798.357421875
6
Iteration 21900: Loss = -11798.357421875
7
Iteration 22000: Loss = -11798.357421875
8
Iteration 22100: Loss = -11798.357421875
9
Iteration 22200: Loss = -11798.3603515625
10
Iteration 22300: Loss = -11798.357421875
11
Iteration 22400: Loss = -11798.357421875
12
Iteration 22500: Loss = -11798.357421875
13
Iteration 22600: Loss = -11798.3564453125
Iteration 22700: Loss = -11798.357421875
1
Iteration 22800: Loss = -11798.357421875
2
Iteration 22900: Loss = -11798.3583984375
3
Iteration 23000: Loss = -11798.359375
4
Iteration 23100: Loss = -11798.357421875
5
Iteration 23200: Loss = -11798.357421875
6
Iteration 23300: Loss = -11798.357421875
7
Iteration 23400: Loss = -11798.357421875
8
Iteration 23500: Loss = -11798.3564453125
Iteration 23600: Loss = -11798.357421875
1
Iteration 23700: Loss = -11798.357421875
2
Iteration 23800: Loss = -11798.3583984375
3
Iteration 23900: Loss = -11798.3583984375
4
Iteration 24000: Loss = -11798.3583984375
5
Iteration 24100: Loss = -11798.3583984375
6
Iteration 24200: Loss = -11798.357421875
7
Iteration 24300: Loss = -11798.3564453125
Iteration 24400: Loss = -11798.3564453125
Iteration 24500: Loss = -11798.3583984375
1
Iteration 24600: Loss = -11798.357421875
2
Iteration 24700: Loss = -11798.3583984375
3
Iteration 24800: Loss = -11798.3583984375
4
Iteration 24900: Loss = -11798.357421875
5
Iteration 25000: Loss = -11798.3583984375
6
Iteration 25100: Loss = -11798.357421875
7
Iteration 25200: Loss = -11798.3564453125
Iteration 25300: Loss = -11798.3583984375
1
Iteration 25400: Loss = -11798.357421875
2
Iteration 25500: Loss = -11798.357421875
3
Iteration 25600: Loss = -11798.3583984375
4
Iteration 25700: Loss = -11798.3583984375
5
Iteration 25800: Loss = -11798.357421875
6
Iteration 25900: Loss = -11798.3583984375
7
Iteration 26000: Loss = -11798.357421875
8
Iteration 26100: Loss = -11798.3583984375
9
Iteration 26200: Loss = -11798.357421875
10
Iteration 26300: Loss = -11798.357421875
11
Iteration 26400: Loss = -11798.3583984375
12
Iteration 26500: Loss = -11798.357421875
13
Iteration 26600: Loss = -11798.357421875
14
Iteration 26700: Loss = -11798.357421875
15
Stopping early at iteration 26700 due to no improvement.
pi: tensor([[0.3447, 0.6553],
        [0.5639, 0.4361]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4477, 0.5523], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2988, 0.0949],
         [0.9881, 0.2914]],

        [[0.0994, 0.0935],
         [0.0784, 0.9679]],

        [[0.9752, 0.0913],
         [0.0219, 0.0240]],

        [[0.1773, 0.1027],
         [0.0106, 0.0590]],

        [[0.1241, 0.1009],
         [0.7285, 0.0165]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9598877889442053
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.041352822544112315
Average Adjusted Rand Index: 0.9839773697219775
[-6.336374590047627e-05, 0.041352822544112315] [-0.0035356619902333825, 0.9839773697219775] [12264.8671875, 11798.357421875]
-------------------------------------
This iteration is 97
True Objective function: Loss = -11706.584672355853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35537.27734375
Iteration 100: Loss = -21342.51171875
Iteration 200: Loss = -14246.474609375
Iteration 300: Loss = -12965.25390625
Iteration 400: Loss = -12697.421875
Iteration 500: Loss = -12582.287109375
Iteration 600: Loss = -12519.9716796875
Iteration 700: Loss = -12480.9013671875
Iteration 800: Loss = -12454.1533203125
Iteration 900: Loss = -12431.2822265625
Iteration 1000: Loss = -12412.9580078125
Iteration 1100: Loss = -12399.5947265625
Iteration 1200: Loss = -12390.904296875
Iteration 1300: Loss = -12384.3466796875
Iteration 1400: Loss = -12376.19921875
Iteration 1500: Loss = -12368.9599609375
Iteration 1600: Loss = -12360.6240234375
Iteration 1700: Loss = -12351.5107421875
Iteration 1800: Loss = -12340.5205078125
Iteration 1900: Loss = -12332.83984375
Iteration 2000: Loss = -12325.5947265625
Iteration 2100: Loss = -12319.486328125
Iteration 2200: Loss = -12314.279296875
Iteration 2300: Loss = -12310.1474609375
Iteration 2400: Loss = -12307.3564453125
Iteration 2500: Loss = -12304.9033203125
Iteration 2600: Loss = -12302.3681640625
Iteration 2700: Loss = -12297.61328125
Iteration 2800: Loss = -12296.17578125
Iteration 2900: Loss = -12295.021484375
Iteration 3000: Loss = -12293.62109375
Iteration 3100: Loss = -12289.53515625
Iteration 3200: Loss = -12288.2265625
Iteration 3300: Loss = -12286.6904296875
Iteration 3400: Loss = -12284.98828125
Iteration 3500: Loss = -12278.9619140625
Iteration 3600: Loss = -12276.54296875
Iteration 3700: Loss = -12275.984375
Iteration 3800: Loss = -12275.5869140625
Iteration 3900: Loss = -12275.265625
Iteration 4000: Loss = -12274.9912109375
Iteration 4100: Loss = -12274.748046875
Iteration 4200: Loss = -12274.533203125
Iteration 4300: Loss = -12274.255859375
Iteration 4400: Loss = -12270.0185546875
Iteration 4500: Loss = -12269.78125
Iteration 4600: Loss = -12269.599609375
Iteration 4700: Loss = -12269.443359375
Iteration 4800: Loss = -12269.306640625
Iteration 4900: Loss = -12269.18359375
Iteration 5000: Loss = -12269.0703125
Iteration 5100: Loss = -12268.9658203125
Iteration 5200: Loss = -12268.8701171875
Iteration 5300: Loss = -12268.7802734375
Iteration 5400: Loss = -12268.6982421875
Iteration 5500: Loss = -12268.62109375
Iteration 5600: Loss = -12268.5458984375
Iteration 5700: Loss = -12268.474609375
Iteration 5800: Loss = -12268.39453125
Iteration 5900: Loss = -12267.189453125
Iteration 6000: Loss = -12262.99609375
Iteration 6100: Loss = -12262.515625
Iteration 6200: Loss = -12262.2841796875
Iteration 6300: Loss = -12262.12890625
Iteration 6400: Loss = -12262.0146484375
Iteration 6500: Loss = -12261.9208984375
Iteration 6600: Loss = -12261.8427734375
Iteration 6700: Loss = -12261.7734375
Iteration 6800: Loss = -12261.712890625
Iteration 6900: Loss = -12261.6572265625
Iteration 7000: Loss = -12261.6044921875
Iteration 7100: Loss = -12261.5595703125
Iteration 7200: Loss = -12261.515625
Iteration 7300: Loss = -12261.4765625
Iteration 7400: Loss = -12261.4384765625
Iteration 7500: Loss = -12261.404296875
Iteration 7600: Loss = -12261.3720703125
Iteration 7700: Loss = -12261.3388671875
Iteration 7800: Loss = -12261.3095703125
Iteration 7900: Loss = -12261.2802734375
Iteration 8000: Loss = -12261.251953125
Iteration 8100: Loss = -12257.005859375
Iteration 8200: Loss = -12256.759765625
Iteration 8300: Loss = -12256.6474609375
Iteration 8400: Loss = -12256.576171875
Iteration 8500: Loss = -12256.521484375
Iteration 8600: Loss = -12256.478515625
Iteration 8700: Loss = -12256.4404296875
Iteration 8800: Loss = -12256.4091796875
Iteration 8900: Loss = -12256.3828125
Iteration 9000: Loss = -12256.357421875
Iteration 9100: Loss = -12256.3359375
Iteration 9200: Loss = -12256.3125
Iteration 9300: Loss = -12256.294921875
Iteration 9400: Loss = -12256.275390625
Iteration 9500: Loss = -12256.2578125
Iteration 9600: Loss = -12256.2412109375
Iteration 9700: Loss = -12256.2255859375
Iteration 9800: Loss = -12256.212890625
Iteration 9900: Loss = -12256.1982421875
Iteration 10000: Loss = -12256.1865234375
Iteration 10100: Loss = -12256.171875
Iteration 10200: Loss = -12256.1611328125
Iteration 10300: Loss = -12256.150390625
Iteration 10400: Loss = -12256.1376953125
Iteration 10500: Loss = -12256.12890625
Iteration 10600: Loss = -12256.1171875
Iteration 10700: Loss = -12256.109375
Iteration 10800: Loss = -12256.099609375
Iteration 10900: Loss = -12256.0908203125
Iteration 11000: Loss = -12256.08203125
Iteration 11100: Loss = -12256.0751953125
Iteration 11200: Loss = -12256.0673828125
Iteration 11300: Loss = -12256.0595703125
Iteration 11400: Loss = -12256.052734375
Iteration 11500: Loss = -12256.046875
Iteration 11600: Loss = -12256.0400390625
Iteration 11700: Loss = -12256.0341796875
Iteration 11800: Loss = -12256.0283203125
Iteration 11900: Loss = -12256.0224609375
Iteration 12000: Loss = -12256.017578125
Iteration 12100: Loss = -12256.0126953125
Iteration 12200: Loss = -12256.0068359375
Iteration 12300: Loss = -12256.0029296875
Iteration 12400: Loss = -12255.9970703125
Iteration 12500: Loss = -12255.990234375
Iteration 12600: Loss = -12255.9794921875
Iteration 12700: Loss = -12255.953125
Iteration 12800: Loss = -12255.4267578125
Iteration 12900: Loss = -12254.8037109375
Iteration 13000: Loss = -12254.74609375
Iteration 13100: Loss = -12254.7373046875
Iteration 13200: Loss = -12254.681640625
Iteration 13300: Loss = -12254.6787109375
Iteration 13400: Loss = -12254.6748046875
Iteration 13500: Loss = -12254.666015625
Iteration 13600: Loss = -12254.6396484375
Iteration 13700: Loss = -12254.5986328125
Iteration 13800: Loss = -12254.5751953125
Iteration 13900: Loss = -12254.3544921875
Iteration 14000: Loss = -12254.3427734375
Iteration 14100: Loss = -12254.33203125
Iteration 14200: Loss = -12254.31640625
Iteration 14300: Loss = -12254.306640625
Iteration 14400: Loss = -12254.298828125
Iteration 14500: Loss = -12254.291015625
Iteration 14600: Loss = -12254.271484375
Iteration 14700: Loss = -12254.2421875
Iteration 14800: Loss = -12254.23828125
Iteration 14900: Loss = -12254.234375
Iteration 15000: Loss = -12254.2138671875
Iteration 15100: Loss = -12254.1435546875
Iteration 15200: Loss = -12254.134765625
Iteration 15300: Loss = -12254.1318359375
Iteration 15400: Loss = -12254.126953125
Iteration 15500: Loss = -12254.1240234375
Iteration 15600: Loss = -12254.1201171875
Iteration 15700: Loss = -12254.119140625
Iteration 15800: Loss = -12254.1171875
Iteration 15900: Loss = -12254.1171875
Iteration 16000: Loss = -12254.1162109375
Iteration 16100: Loss = -12254.11328125
Iteration 16200: Loss = -12254.1142578125
1
Iteration 16300: Loss = -12254.1142578125
2
Iteration 16400: Loss = -12254.111328125
Iteration 16500: Loss = -12254.111328125
Iteration 16600: Loss = -12254.1103515625
Iteration 16700: Loss = -12254.1103515625
Iteration 16800: Loss = -12254.1064453125
Iteration 16900: Loss = -12254.1044921875
Iteration 17000: Loss = -12254.10546875
1
Iteration 17100: Loss = -12254.1064453125
2
Iteration 17200: Loss = -12254.1064453125
3
Iteration 17300: Loss = -12254.1044921875
Iteration 17400: Loss = -12254.1044921875
Iteration 17500: Loss = -12254.1015625
Iteration 17600: Loss = -12254.099609375
Iteration 17700: Loss = -12254.099609375
Iteration 17800: Loss = -12254.1005859375
1
Iteration 17900: Loss = -12254.0966796875
Iteration 18000: Loss = -12254.0966796875
Iteration 18100: Loss = -12254.09765625
1
Iteration 18200: Loss = -12254.095703125
Iteration 18300: Loss = -12254.09765625
1
Iteration 18400: Loss = -12254.09765625
2
Iteration 18500: Loss = -12254.0966796875
3
Iteration 18600: Loss = -12254.0966796875
4
Iteration 18700: Loss = -12254.0966796875
5
Iteration 18800: Loss = -12254.0986328125
6
Iteration 18900: Loss = -12254.0966796875
7
Iteration 19000: Loss = -12254.0966796875
8
Iteration 19100: Loss = -12254.0966796875
9
Iteration 19200: Loss = -12254.0966796875
10
Iteration 19300: Loss = -12254.0966796875
11
Iteration 19400: Loss = -12254.095703125
Iteration 19500: Loss = -12254.0947265625
Iteration 19600: Loss = -12254.09765625
1
Iteration 19700: Loss = -12254.095703125
2
Iteration 19800: Loss = -12254.095703125
3
Iteration 19900: Loss = -12254.0966796875
4
Iteration 20000: Loss = -12254.0966796875
5
Iteration 20100: Loss = -12254.095703125
6
Iteration 20200: Loss = -12254.09765625
7
Iteration 20300: Loss = -12254.09765625
8
Iteration 20400: Loss = -12254.0966796875
9
Iteration 20500: Loss = -12254.095703125
10
Iteration 20600: Loss = -12254.0966796875
11
Iteration 20700: Loss = -12254.095703125
12
Iteration 20800: Loss = -12254.095703125
13
Iteration 20900: Loss = -12254.09765625
14
Iteration 21000: Loss = -12254.095703125
15
Stopping early at iteration 21000 due to no improvement.
pi: tensor([[1.0000e+00, 3.8384e-06],
        [9.6886e-01, 3.1135e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0202, 0.9798], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1959, 0.1886],
         [0.8751, 0.1919]],

        [[0.2570, 0.3005],
         [0.1084, 0.2338]],

        [[0.2241, 0.1566],
         [0.9713, 0.3874]],

        [[0.4387, 0.6313],
         [0.3252, 0.1226]],

        [[0.8436, 0.1922],
         [0.7777, 0.0082]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: -0.005513710431743218
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00046717968344020603
Average Adjusted Rand Index: -0.0011027420863486436
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25353.14453125
Iteration 100: Loss = -17534.685546875
Iteration 200: Loss = -13439.4755859375
Iteration 300: Loss = -12706.7236328125
Iteration 400: Loss = -12506.55859375
Iteration 500: Loss = -12442.3623046875
Iteration 600: Loss = -12409.2451171875
Iteration 700: Loss = -12391.744140625
Iteration 800: Loss = -12377.3876953125
Iteration 900: Loss = -12366.6806640625
Iteration 1000: Loss = -12358.4921875
Iteration 1100: Loss = -12352.9326171875
Iteration 1200: Loss = -12347.1611328125
Iteration 1300: Loss = -12342.1259765625
Iteration 1400: Loss = -12335.64453125
Iteration 1500: Loss = -12325.86328125
Iteration 1600: Loss = -12321.41015625
Iteration 1700: Loss = -12318.38671875
Iteration 1800: Loss = -12315.3642578125
Iteration 1900: Loss = -12312.3544921875
Iteration 2000: Loss = -12308.779296875
Iteration 2100: Loss = -12306.2880859375
Iteration 2200: Loss = -12303.8681640625
Iteration 2300: Loss = -12300.5234375
Iteration 2400: Loss = -12298.51171875
Iteration 2500: Loss = -12296.580078125
Iteration 2600: Loss = -12292.6748046875
Iteration 2700: Loss = -12288.4716796875
Iteration 2800: Loss = -12282.515625
Iteration 2900: Loss = -12280.9150390625
Iteration 3000: Loss = -12280.0048828125
Iteration 3100: Loss = -12279.345703125
Iteration 3200: Loss = -12278.8251953125
Iteration 3300: Loss = -12278.39453125
Iteration 3400: Loss = -12278.013671875
Iteration 3500: Loss = -12277.6123046875
Iteration 3600: Loss = -12275.8955078125
Iteration 3700: Loss = -12274.38671875
Iteration 3800: Loss = -12274.03515625
Iteration 3900: Loss = -12273.779296875
Iteration 4000: Loss = -12273.5634765625
Iteration 4100: Loss = -12273.373046875
Iteration 4200: Loss = -12273.1923828125
Iteration 4300: Loss = -12273.0185546875
Iteration 4400: Loss = -12272.8427734375
Iteration 4500: Loss = -12270.396484375
Iteration 4600: Loss = -12269.140625
Iteration 4700: Loss = -12268.8662109375
Iteration 4800: Loss = -12265.23046875
Iteration 4900: Loss = -12265.046875
Iteration 5000: Loss = -12264.912109375
Iteration 5100: Loss = -12264.802734375
Iteration 5200: Loss = -12264.7099609375
Iteration 5300: Loss = -12264.6298828125
Iteration 5400: Loss = -12264.5595703125
Iteration 5500: Loss = -12264.4970703125
Iteration 5600: Loss = -12264.4375
Iteration 5700: Loss = -12264.3818359375
Iteration 5800: Loss = -12264.326171875
Iteration 5900: Loss = -12264.25390625
Iteration 6000: Loss = -12263.771484375
Iteration 6100: Loss = -12260.1240234375
Iteration 6200: Loss = -12259.84375
Iteration 6300: Loss = -12259.7138671875
Iteration 6400: Loss = -12259.62890625
Iteration 6500: Loss = -12259.5625
Iteration 6600: Loss = -12259.5107421875
Iteration 6700: Loss = -12259.466796875
Iteration 6800: Loss = -12259.4296875
Iteration 6900: Loss = -12259.3984375
Iteration 7000: Loss = -12259.3662109375
Iteration 7100: Loss = -12259.341796875
Iteration 7200: Loss = -12259.3193359375
Iteration 7300: Loss = -12259.296875
Iteration 7400: Loss = -12259.2578125
Iteration 7500: Loss = -12255.9833984375
Iteration 7600: Loss = -12255.849609375
Iteration 7700: Loss = -12255.80078125
Iteration 7800: Loss = -12255.775390625
Iteration 7900: Loss = -12255.75390625
Iteration 8000: Loss = -12255.7392578125
Iteration 8100: Loss = -12255.7255859375
Iteration 8200: Loss = -12255.712890625
Iteration 8300: Loss = -12255.7021484375
Iteration 8400: Loss = -12255.6923828125
Iteration 8500: Loss = -12255.68359375
Iteration 8600: Loss = -12255.6748046875
Iteration 8700: Loss = -12255.6689453125
Iteration 8800: Loss = -12255.6611328125
Iteration 8900: Loss = -12255.6513671875
Iteration 9000: Loss = -12255.64453125
Iteration 9100: Loss = -12255.6376953125
Iteration 9200: Loss = -12255.62890625
Iteration 9300: Loss = -12255.6162109375
Iteration 9400: Loss = -12255.5947265625
Iteration 9500: Loss = -12255.501953125
Iteration 9600: Loss = -12255.130859375
Iteration 9700: Loss = -12254.9443359375
Iteration 9800: Loss = -12254.869140625
Iteration 9900: Loss = -12254.822265625
Iteration 10000: Loss = -12254.78515625
Iteration 10100: Loss = -12254.75
Iteration 10200: Loss = -12254.7255859375
Iteration 10300: Loss = -12254.7041015625
Iteration 10400: Loss = -12254.6865234375
Iteration 10500: Loss = -12254.66796875
Iteration 10600: Loss = -12254.6484375
Iteration 10700: Loss = -12254.638671875
Iteration 10800: Loss = -12254.6318359375
Iteration 10900: Loss = -12254.625
Iteration 11000: Loss = -12254.6181640625
Iteration 11100: Loss = -12254.6123046875
Iteration 11200: Loss = -12254.603515625
Iteration 11300: Loss = -12254.4970703125
Iteration 11400: Loss = -12254.458984375
Iteration 11500: Loss = -12254.4521484375
Iteration 11600: Loss = -12254.4404296875
Iteration 11700: Loss = -12254.3896484375
Iteration 11800: Loss = -12254.220703125
Iteration 11900: Loss = -12254.189453125
Iteration 12000: Loss = -12254.1728515625
Iteration 12100: Loss = -12254.1640625
Iteration 12200: Loss = -12254.15625
Iteration 12300: Loss = -12254.150390625
Iteration 12400: Loss = -12254.14453125
Iteration 12500: Loss = -12254.1435546875
Iteration 12600: Loss = -12254.140625
Iteration 12700: Loss = -12254.1376953125
Iteration 12800: Loss = -12254.1337890625
Iteration 12900: Loss = -12254.1328125
Iteration 13000: Loss = -12254.130859375
Iteration 13100: Loss = -12254.130859375
Iteration 13200: Loss = -12254.12890625
Iteration 13300: Loss = -12254.126953125
Iteration 13400: Loss = -12254.125
Iteration 13500: Loss = -12254.123046875
Iteration 13600: Loss = -12254.123046875
Iteration 13700: Loss = -12254.123046875
Iteration 13800: Loss = -12254.123046875
Iteration 13900: Loss = -12254.1201171875
Iteration 14000: Loss = -12254.119140625
Iteration 14100: Loss = -12254.1201171875
1
Iteration 14200: Loss = -12254.1201171875
2
Iteration 14300: Loss = -12254.1181640625
Iteration 14400: Loss = -12254.1181640625
Iteration 14500: Loss = -12254.1162109375
Iteration 14600: Loss = -12254.1171875
1
Iteration 14700: Loss = -12254.1162109375
Iteration 14800: Loss = -12254.1123046875
Iteration 14900: Loss = -12254.109375
Iteration 15000: Loss = -12254.107421875
Iteration 15100: Loss = -12254.1083984375
1
Iteration 15200: Loss = -12254.107421875
Iteration 15300: Loss = -12254.1083984375
1
Iteration 15400: Loss = -12254.107421875
Iteration 15500: Loss = -12254.107421875
Iteration 15600: Loss = -12254.10546875
Iteration 15700: Loss = -12254.1064453125
1
Iteration 15800: Loss = -12254.1044921875
Iteration 15900: Loss = -12254.1044921875
Iteration 16000: Loss = -12254.10546875
1
Iteration 16100: Loss = -12254.103515625
Iteration 16200: Loss = -12254.1044921875
1
Iteration 16300: Loss = -12254.1015625
Iteration 16400: Loss = -12254.1025390625
1
Iteration 16500: Loss = -12254.1025390625
2
Iteration 16600: Loss = -12254.1015625
Iteration 16700: Loss = -12254.1015625
Iteration 16800: Loss = -12254.1015625
Iteration 16900: Loss = -12254.10546875
1
Iteration 17000: Loss = -12254.1015625
Iteration 17100: Loss = -12254.1015625
Iteration 17200: Loss = -12254.1015625
Iteration 17300: Loss = -12254.1005859375
Iteration 17400: Loss = -12254.1005859375
Iteration 17500: Loss = -12254.1015625
1
Iteration 17600: Loss = -12254.1015625
2
Iteration 17700: Loss = -12254.1005859375
Iteration 17800: Loss = -12254.1005859375
Iteration 17900: Loss = -12254.1015625
1
Iteration 18000: Loss = -12254.1015625
2
Iteration 18100: Loss = -12254.1005859375
Iteration 18200: Loss = -12254.099609375
Iteration 18300: Loss = -12254.099609375
Iteration 18400: Loss = -12254.1005859375
1
Iteration 18500: Loss = -12254.1005859375
2
Iteration 18600: Loss = -12254.099609375
Iteration 18700: Loss = -12254.099609375
Iteration 18800: Loss = -12254.099609375
Iteration 18900: Loss = -12254.099609375
Iteration 19000: Loss = -12254.099609375
Iteration 19100: Loss = -12254.1005859375
1
Iteration 19200: Loss = -12254.099609375
Iteration 19300: Loss = -12254.099609375
Iteration 19400: Loss = -12254.099609375
Iteration 19500: Loss = -12254.0986328125
Iteration 19600: Loss = -12254.09765625
Iteration 19700: Loss = -12254.0986328125
1
Iteration 19800: Loss = -12254.0966796875
Iteration 19900: Loss = -12254.09765625
1
Iteration 20000: Loss = -12254.0966796875
Iteration 20100: Loss = -12254.09765625
1
Iteration 20200: Loss = -12254.095703125
Iteration 20300: Loss = -12254.09765625
1
Iteration 20400: Loss = -12254.0966796875
2
Iteration 20500: Loss = -12254.095703125
Iteration 20600: Loss = -12254.0966796875
1
Iteration 20700: Loss = -12254.0947265625
Iteration 20800: Loss = -12254.0947265625
Iteration 20900: Loss = -12254.09375
Iteration 21000: Loss = -12254.0947265625
1
Iteration 21100: Loss = -12254.09375
Iteration 21200: Loss = -12254.0947265625
1
Iteration 21300: Loss = -12254.095703125
2
Iteration 21400: Loss = -12254.0947265625
3
Iteration 21500: Loss = -12254.095703125
4
Iteration 21600: Loss = -12254.09375
Iteration 21700: Loss = -12254.09375
Iteration 21800: Loss = -12254.09375
Iteration 21900: Loss = -12254.0927734375
Iteration 22000: Loss = -12254.0947265625
1
Iteration 22100: Loss = -12254.09375
2
Iteration 22200: Loss = -12254.0947265625
3
Iteration 22300: Loss = -12254.09375
4
Iteration 22400: Loss = -12254.09375
5
Iteration 22500: Loss = -12254.09375
6
Iteration 22600: Loss = -12254.09375
7
Iteration 22700: Loss = -12254.09375
8
Iteration 22800: Loss = -12254.0947265625
9
Iteration 22900: Loss = -12254.0966796875
10
Iteration 23000: Loss = -12254.0947265625
11
Iteration 23100: Loss = -12254.0947265625
12
Iteration 23200: Loss = -12254.0947265625
13
Iteration 23300: Loss = -12254.0927734375
Iteration 23400: Loss = -12254.09375
1
Iteration 23500: Loss = -12254.0947265625
2
Iteration 23600: Loss = -12254.09375
3
Iteration 23700: Loss = -12254.095703125
4
Iteration 23800: Loss = -12254.0966796875
5
Iteration 23900: Loss = -12254.095703125
6
Iteration 24000: Loss = -12254.0947265625
7
Iteration 24100: Loss = -12254.0947265625
8
Iteration 24200: Loss = -12254.0947265625
9
Iteration 24300: Loss = -12254.09375
10
Iteration 24400: Loss = -12254.0947265625
11
Iteration 24500: Loss = -12254.0947265625
12
Iteration 24600: Loss = -12254.09375
13
Iteration 24700: Loss = -12254.0947265625
14
Iteration 24800: Loss = -12254.0947265625
15
Stopping early at iteration 24800 due to no improvement.
pi: tensor([[3.1304e-02, 9.6870e-01],
        [2.3902e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9796, 0.0204], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1919, 0.1885],
         [0.0929, 0.1959]],

        [[0.9002, 0.3003],
         [0.9920, 0.7712]],

        [[0.6872, 0.1566],
         [0.8999, 0.7855]],

        [[0.9489, 0.5264],
         [0.0129, 0.9921]],

        [[0.0092, 0.1931],
         [0.0680, 0.9508]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: -0.005513710431743218
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00046717968344020603
Average Adjusted Rand Index: -0.0011027420863486436
[-0.00046717968344020603, -0.00046717968344020603] [-0.0011027420863486436, -0.0011027420863486436] [12254.095703125, 12254.0947265625]
-------------------------------------
This iteration is 98
True Objective function: Loss = -11989.381526832021
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -38395.5078125
Iteration 100: Loss = -22246.990234375
Iteration 200: Loss = -14588.1484375
Iteration 300: Loss = -13087.984375
Iteration 400: Loss = -12757.263671875
Iteration 500: Loss = -12653.9462890625
Iteration 600: Loss = -12595.669921875
Iteration 700: Loss = -12567.0263671875
Iteration 800: Loss = -12547.8134765625
Iteration 900: Loss = -12520.404296875
Iteration 1000: Loss = -12507.6494140625
Iteration 1100: Loss = -12500.74609375
Iteration 1200: Loss = -12495.6728515625
Iteration 1300: Loss = -12491.7001953125
Iteration 1400: Loss = -12488.498046875
Iteration 1500: Loss = -12485.8720703125
Iteration 1600: Loss = -12483.6875
Iteration 1700: Loss = -12481.8447265625
Iteration 1800: Loss = -12480.2783203125
Iteration 1900: Loss = -12478.93359375
Iteration 2000: Loss = -12477.7705078125
Iteration 2100: Loss = -12476.759765625
Iteration 2200: Loss = -12475.87890625
Iteration 2300: Loss = -12475.1025390625
Iteration 2400: Loss = -12474.41796875
Iteration 2500: Loss = -12473.8134765625
Iteration 2600: Loss = -12473.2783203125
Iteration 2700: Loss = -12472.7998046875
Iteration 2800: Loss = -12472.353515625
Iteration 2900: Loss = -12471.974609375
Iteration 3000: Loss = -12471.634765625
Iteration 3100: Loss = -12471.3271484375
Iteration 3200: Loss = -12471.0517578125
Iteration 3300: Loss = -12470.802734375
Iteration 3400: Loss = -12470.578125
Iteration 3500: Loss = -12470.3720703125
Iteration 3600: Loss = -12470.1845703125
Iteration 3700: Loss = -12470.0146484375
Iteration 3800: Loss = -12469.859375
Iteration 3900: Loss = -12469.7216796875
Iteration 4000: Loss = -12469.59375
Iteration 4100: Loss = -12469.478515625
Iteration 4200: Loss = -12469.369140625
Iteration 4300: Loss = -12469.2685546875
Iteration 4400: Loss = -12469.17578125
Iteration 4500: Loss = -12469.087890625
Iteration 4600: Loss = -12469.0078125
Iteration 4700: Loss = -12468.9306640625
Iteration 4800: Loss = -12468.859375
Iteration 4900: Loss = -12468.791015625
Iteration 5000: Loss = -12468.7265625
Iteration 5100: Loss = -12468.666015625
Iteration 5200: Loss = -12468.6083984375
Iteration 5300: Loss = -12468.552734375
Iteration 5400: Loss = -12468.5009765625
Iteration 5500: Loss = -12468.4521484375
Iteration 5600: Loss = -12468.404296875
Iteration 5700: Loss = -12468.3564453125
Iteration 5800: Loss = -12468.3115234375
Iteration 5900: Loss = -12468.2666015625
Iteration 6000: Loss = -12468.224609375
Iteration 6100: Loss = -12468.177734375
Iteration 6200: Loss = -12468.1337890625
Iteration 6300: Loss = -12468.0888671875
Iteration 6400: Loss = -12468.0458984375
Iteration 6500: Loss = -12468.005859375
Iteration 6600: Loss = -12467.96484375
Iteration 6700: Loss = -12467.931640625
Iteration 6800: Loss = -12467.900390625
Iteration 6900: Loss = -12467.869140625
Iteration 7000: Loss = -12467.8427734375
Iteration 7100: Loss = -12467.8173828125
Iteration 7200: Loss = -12467.79296875
Iteration 7300: Loss = -12467.7734375
Iteration 7400: Loss = -12467.7529296875
Iteration 7500: Loss = -12467.732421875
Iteration 7600: Loss = -12467.7158203125
Iteration 7700: Loss = -12467.697265625
Iteration 7800: Loss = -12467.6787109375
Iteration 7900: Loss = -12467.66015625
Iteration 8000: Loss = -12467.6484375
Iteration 8100: Loss = -12467.63671875
Iteration 8200: Loss = -12467.623046875
Iteration 8300: Loss = -12467.611328125
Iteration 8400: Loss = -12467.6015625
Iteration 8500: Loss = -12467.5908203125
Iteration 8600: Loss = -12467.58203125
Iteration 8700: Loss = -12467.572265625
Iteration 8800: Loss = -12467.564453125
Iteration 8900: Loss = -12467.5576171875
Iteration 9000: Loss = -12467.5478515625
Iteration 9100: Loss = -12467.541015625
Iteration 9200: Loss = -12467.533203125
Iteration 9300: Loss = -12467.52734375
Iteration 9400: Loss = -12467.51953125
Iteration 9500: Loss = -12467.513671875
Iteration 9600: Loss = -12467.50390625
Iteration 9700: Loss = -12467.498046875
Iteration 9800: Loss = -12467.4873046875
Iteration 9900: Loss = -12467.4736328125
Iteration 10000: Loss = -12467.4482421875
Iteration 10100: Loss = -12467.404296875
Iteration 10200: Loss = -12467.3564453125
Iteration 10300: Loss = -12467.32421875
Iteration 10400: Loss = -12467.2939453125
Iteration 10500: Loss = -12467.23046875
Iteration 10600: Loss = -12466.9541015625
Iteration 10700: Loss = -12466.5908203125
Iteration 10800: Loss = -12466.44921875
Iteration 10900: Loss = -12466.3486328125
Iteration 11000: Loss = -12466.2734375
Iteration 11100: Loss = -12466.2109375
Iteration 11200: Loss = -12466.154296875
Iteration 11300: Loss = -12466.103515625
Iteration 11400: Loss = -12466.0546875
Iteration 11500: Loss = -12466.009765625
Iteration 11600: Loss = -12465.970703125
Iteration 11700: Loss = -12465.9345703125
Iteration 11800: Loss = -12465.9033203125
Iteration 11900: Loss = -12465.876953125
Iteration 12000: Loss = -12465.8525390625
Iteration 12100: Loss = -12465.8310546875
Iteration 12200: Loss = -12465.8115234375
Iteration 12300: Loss = -12465.7958984375
Iteration 12400: Loss = -12465.779296875
Iteration 12500: Loss = -12465.767578125
Iteration 12600: Loss = -12465.7548828125
Iteration 12700: Loss = -12465.7421875
Iteration 12800: Loss = -12465.7333984375
Iteration 12900: Loss = -12465.7236328125
Iteration 13000: Loss = -12465.7138671875
Iteration 13100: Loss = -12465.70703125
Iteration 13200: Loss = -12465.6982421875
Iteration 13300: Loss = -12465.69140625
Iteration 13400: Loss = -12465.685546875
Iteration 13500: Loss = -12465.681640625
Iteration 13600: Loss = -12465.6748046875
Iteration 13700: Loss = -12465.669921875
Iteration 13800: Loss = -12465.666015625
Iteration 13900: Loss = -12465.662109375
Iteration 14000: Loss = -12465.6572265625
Iteration 14100: Loss = -12465.65234375
Iteration 14200: Loss = -12465.6484375
Iteration 14300: Loss = -12465.6416015625
Iteration 14400: Loss = -12465.638671875
Iteration 14500: Loss = -12465.6328125
Iteration 14600: Loss = -12465.626953125
Iteration 14700: Loss = -12465.619140625
Iteration 14800: Loss = -12465.6142578125
Iteration 14900: Loss = -12465.609375
Iteration 15000: Loss = -12465.6025390625
Iteration 15100: Loss = -12465.595703125
Iteration 15200: Loss = -12465.5908203125
Iteration 15300: Loss = -12465.58203125
Iteration 15400: Loss = -12465.5751953125
Iteration 15500: Loss = -12465.5634765625
Iteration 15600: Loss = -12465.544921875
Iteration 15700: Loss = -12465.4775390625
Iteration 15800: Loss = -12465.3798828125
Iteration 15900: Loss = -12464.89453125
Iteration 16000: Loss = -12463.8828125
Iteration 16100: Loss = -12463.87109375
Iteration 16200: Loss = -12463.8681640625
Iteration 16300: Loss = -12463.8662109375
Iteration 16400: Loss = -12463.8623046875
Iteration 16500: Loss = -12463.8603515625
Iteration 16600: Loss = -12463.861328125
1
Iteration 16700: Loss = -12463.857421875
Iteration 16800: Loss = -12463.8544921875
Iteration 16900: Loss = -12463.8515625
Iteration 17000: Loss = -12463.849609375
Iteration 17100: Loss = -12463.8466796875
Iteration 17200: Loss = -12463.83984375
Iteration 17300: Loss = -12463.833984375
Iteration 17400: Loss = -12463.8076171875
Iteration 17500: Loss = -12463.7890625
Iteration 17600: Loss = -12463.7841796875
Iteration 17700: Loss = -12463.78515625
1
Iteration 17800: Loss = -12463.78515625
2
Iteration 17900: Loss = -12463.7861328125
3
Iteration 18000: Loss = -12463.783203125
Iteration 18100: Loss = -12463.7841796875
1
Iteration 18200: Loss = -12463.7802734375
Iteration 18300: Loss = -12463.716796875
Iteration 18400: Loss = -12463.5615234375
Iteration 18500: Loss = -12463.5595703125
Iteration 18600: Loss = -12463.5546875
Iteration 18700: Loss = -12463.5556640625
1
Iteration 18800: Loss = -12463.5546875
Iteration 18900: Loss = -12463.5537109375
Iteration 19000: Loss = -12463.5546875
1
Iteration 19100: Loss = -12463.5537109375
Iteration 19200: Loss = -12463.552734375
Iteration 19300: Loss = -12463.55078125
Iteration 19400: Loss = -12463.5498046875
Iteration 19500: Loss = -12463.5498046875
Iteration 19600: Loss = -12463.5498046875
Iteration 19700: Loss = -12463.5478515625
Iteration 19800: Loss = -12463.54296875
Iteration 19900: Loss = -12463.5361328125
Iteration 20000: Loss = -12463.533203125
Iteration 20100: Loss = -12463.5283203125
Iteration 20200: Loss = -12463.52734375
Iteration 20300: Loss = -12463.525390625
Iteration 20400: Loss = -12463.5263671875
1
Iteration 20500: Loss = -12463.5263671875
2
Iteration 20600: Loss = -12463.525390625
Iteration 20700: Loss = -12463.5263671875
1
Iteration 20800: Loss = -12463.5224609375
Iteration 20900: Loss = -12463.5244140625
1
Iteration 21000: Loss = -12463.5244140625
2
Iteration 21100: Loss = -12463.5234375
3
Iteration 21200: Loss = -12463.51953125
Iteration 21300: Loss = -12463.51953125
Iteration 21400: Loss = -12463.517578125
Iteration 21500: Loss = -12463.517578125
Iteration 21600: Loss = -12463.5166015625
Iteration 21700: Loss = -12463.5166015625
Iteration 21800: Loss = -12463.509765625
Iteration 21900: Loss = -12463.5078125
Iteration 22000: Loss = -12463.5078125
Iteration 22100: Loss = -12463.5078125
Iteration 22200: Loss = -12463.5078125
Iteration 22300: Loss = -12463.50390625
Iteration 22400: Loss = -12463.501953125
Iteration 22500: Loss = -12463.5029296875
1
Iteration 22600: Loss = -12463.5
Iteration 22700: Loss = -12463.5
Iteration 22800: Loss = -12463.5
Iteration 22900: Loss = -12463.5
Iteration 23000: Loss = -12463.4990234375
Iteration 23100: Loss = -12463.5
1
Iteration 23200: Loss = -12463.5009765625
2
Iteration 23300: Loss = -12463.4990234375
Iteration 23400: Loss = -12463.4990234375
Iteration 23500: Loss = -12463.4970703125
Iteration 23600: Loss = -12463.5
1
Iteration 23700: Loss = -12463.498046875
2
Iteration 23800: Loss = -12463.49609375
Iteration 23900: Loss = -12463.498046875
1
Iteration 24000: Loss = -12463.498046875
2
Iteration 24100: Loss = -12463.498046875
3
Iteration 24200: Loss = -12463.490234375
Iteration 24300: Loss = -12463.4892578125
Iteration 24400: Loss = -12463.4892578125
Iteration 24500: Loss = -12463.4892578125
Iteration 24600: Loss = -12463.4892578125
Iteration 24700: Loss = -12463.4892578125
Iteration 24800: Loss = -12463.4892578125
Iteration 24900: Loss = -12463.4912109375
1
Iteration 25000: Loss = -12463.4892578125
Iteration 25100: Loss = -12463.4892578125
Iteration 25200: Loss = -12463.4892578125
Iteration 25300: Loss = -12463.48828125
Iteration 25400: Loss = -12463.490234375
1
Iteration 25500: Loss = -12463.48828125
Iteration 25600: Loss = -12463.4873046875
Iteration 25700: Loss = -12463.48828125
1
Iteration 25800: Loss = -12463.4912109375
2
Iteration 25900: Loss = -12463.48828125
3
Iteration 26000: Loss = -12463.4873046875
Iteration 26100: Loss = -12463.4892578125
1
Iteration 26200: Loss = -12463.48828125
2
Iteration 26300: Loss = -12463.4873046875
Iteration 26400: Loss = -12463.48828125
1
Iteration 26500: Loss = -12463.48828125
2
Iteration 26600: Loss = -12463.4873046875
Iteration 26700: Loss = -12463.48828125
1
Iteration 26800: Loss = -12463.4892578125
2
Iteration 26900: Loss = -12463.4873046875
Iteration 27000: Loss = -12463.4892578125
1
Iteration 27100: Loss = -12463.48828125
2
Iteration 27200: Loss = -12463.4873046875
Iteration 27300: Loss = -12463.4873046875
Iteration 27400: Loss = -12463.486328125
Iteration 27500: Loss = -12463.48828125
1
Iteration 27600: Loss = -12463.4873046875
2
Iteration 27700: Loss = -12463.486328125
Iteration 27800: Loss = -12463.490234375
1
Iteration 27900: Loss = -12463.4892578125
2
Iteration 28000: Loss = -12463.4873046875
3
Iteration 28100: Loss = -12463.4873046875
4
Iteration 28200: Loss = -12463.48828125
5
Iteration 28300: Loss = -12463.4892578125
6
Iteration 28400: Loss = -12463.486328125
Iteration 28500: Loss = -12463.4873046875
1
Iteration 28600: Loss = -12463.4873046875
2
Iteration 28700: Loss = -12463.4892578125
3
Iteration 28800: Loss = -12463.48828125
4
Iteration 28900: Loss = -12463.486328125
Iteration 29000: Loss = -12463.4892578125
1
Iteration 29100: Loss = -12463.48828125
2
Iteration 29200: Loss = -12463.4892578125
3
Iteration 29300: Loss = -12463.4873046875
4
Iteration 29400: Loss = -12463.4873046875
5
Iteration 29500: Loss = -12463.4892578125
6
Iteration 29600: Loss = -12463.4873046875
7
Iteration 29700: Loss = -12463.4873046875
8
Iteration 29800: Loss = -12463.4873046875
9
Iteration 29900: Loss = -12463.4873046875
10
pi: tensor([[9.9999e-01, 6.0306e-06],
        [9.8911e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9602, 0.0398], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2003, 0.2145],
         [0.1924, 0.1008]],

        [[0.2546, 0.1586],
         [0.0134, 0.9887]],

        [[0.9724, 0.2309],
         [0.2869, 0.7760]],

        [[0.9407, 0.2336],
         [0.9869, 0.9777]],

        [[0.5406, 0.3073],
         [0.0085, 0.0693]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0017137835707030447
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 4.85601903559462e-05
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.004212316740111065
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.012285862605987194
Global Adjusted Rand Index: -0.0006519339167301126
Average Adjusted Rand Index: -0.0029471671170078536
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -45262.5625
Iteration 100: Loss = -32475.71875
Iteration 200: Loss = -22727.900390625
Iteration 300: Loss = -16616.876953125
Iteration 400: Loss = -13896.2314453125
Iteration 500: Loss = -12990.14453125
Iteration 600: Loss = -12721.666015625
Iteration 700: Loss = -12615.025390625
Iteration 800: Loss = -12578.7763671875
Iteration 900: Loss = -12542.77734375
Iteration 1000: Loss = -12522.0087890625
Iteration 1100: Loss = -12509.9755859375
Iteration 1200: Loss = -12505.4150390625
Iteration 1300: Loss = -12500.951171875
Iteration 1400: Loss = -12495.814453125
Iteration 1500: Loss = -12489.5
Iteration 1600: Loss = -12488.0654296875
Iteration 1700: Loss = -12486.1083984375
Iteration 1800: Loss = -12481.201171875
Iteration 1900: Loss = -12480.255859375
Iteration 2000: Loss = -12479.392578125
Iteration 2100: Loss = -12478.5224609375
Iteration 2200: Loss = -12477.328125
Iteration 2300: Loss = -12476.3134765625
Iteration 2400: Loss = -12475.900390625
Iteration 2500: Loss = -12475.619140625
Iteration 2600: Loss = -12475.369140625
Iteration 2700: Loss = -12475.1552734375
Iteration 2800: Loss = -12474.94140625
Iteration 2900: Loss = -12474.763671875
Iteration 3000: Loss = -12474.6005859375
Iteration 3100: Loss = -12474.4365234375
Iteration 3200: Loss = -12474.2939453125
Iteration 3300: Loss = -12474.1513671875
Iteration 3400: Loss = -12474.029296875
Iteration 3500: Loss = -12473.8876953125
Iteration 3600: Loss = -12473.6962890625
Iteration 3700: Loss = -12473.5185546875
Iteration 3800: Loss = -12473.37109375
Iteration 3900: Loss = -12473.228515625
Iteration 4000: Loss = -12473.0810546875
Iteration 4100: Loss = -12472.900390625
Iteration 4200: Loss = -12472.7490234375
Iteration 4300: Loss = -12472.6513671875
Iteration 4400: Loss = -12472.572265625
Iteration 4500: Loss = -12472.517578125
Iteration 4600: Loss = -12472.47265625
Iteration 4700: Loss = -12472.435546875
Iteration 4800: Loss = -12472.4033203125
Iteration 4900: Loss = -12472.373046875
Iteration 5000: Loss = -12472.3447265625
Iteration 5100: Loss = -12472.3193359375
Iteration 5200: Loss = -12472.294921875
Iteration 5300: Loss = -12472.2685546875
Iteration 5400: Loss = -12472.2490234375
Iteration 5500: Loss = -12472.2275390625
Iteration 5600: Loss = -12472.2109375
Iteration 5700: Loss = -12472.193359375
Iteration 5800: Loss = -12472.177734375
Iteration 5900: Loss = -12472.162109375
Iteration 6000: Loss = -12472.150390625
Iteration 6100: Loss = -12472.1357421875
Iteration 6200: Loss = -12472.1240234375
Iteration 6300: Loss = -12472.1142578125
Iteration 6400: Loss = -12472.103515625
Iteration 6500: Loss = -12472.08984375
Iteration 6600: Loss = -12471.5341796875
Iteration 6700: Loss = -12471.5068359375
Iteration 6800: Loss = -12471.4931640625
Iteration 6900: Loss = -12471.482421875
Iteration 7000: Loss = -12471.47265625
Iteration 7100: Loss = -12471.46484375
Iteration 7200: Loss = -12471.4560546875
Iteration 7300: Loss = -12471.4482421875
Iteration 7400: Loss = -12471.439453125
Iteration 7500: Loss = -12471.427734375
Iteration 7600: Loss = -12471.419921875
Iteration 7700: Loss = -12471.416015625
Iteration 7800: Loss = -12471.408203125
Iteration 7900: Loss = -12471.4013671875
Iteration 8000: Loss = -12471.3828125
Iteration 8100: Loss = -12471.376953125
Iteration 8200: Loss = -12471.37109375
Iteration 8300: Loss = -12471.3642578125
Iteration 8400: Loss = -12471.357421875
Iteration 8500: Loss = -12471.3515625
Iteration 8600: Loss = -12471.3349609375
Iteration 8700: Loss = -12470.9677734375
Iteration 8800: Loss = -12470.2177734375
Iteration 8900: Loss = -12466.583984375
Iteration 9000: Loss = -12453.673828125
Iteration 9100: Loss = -12447.6181640625
Iteration 9200: Loss = -12445.8837890625
Iteration 9300: Loss = -12439.15625
Iteration 9400: Loss = -12370.443359375
Iteration 9500: Loss = -12275.6181640625
Iteration 9600: Loss = -12223.015625
Iteration 9700: Loss = -12182.33984375
Iteration 9800: Loss = -12160.474609375
Iteration 9900: Loss = -12120.9140625
Iteration 10000: Loss = -12116.9140625
Iteration 10100: Loss = -12116.5263671875
Iteration 10200: Loss = -12116.306640625
Iteration 10300: Loss = -12106.3740234375
Iteration 10400: Loss = -12096.3857421875
Iteration 10500: Loss = -12095.8876953125
Iteration 10600: Loss = -12093.1201171875
Iteration 10700: Loss = -12078.125
Iteration 10800: Loss = -12077.9443359375
Iteration 10900: Loss = -12077.8642578125
Iteration 11000: Loss = -12077.8076171875
Iteration 11100: Loss = -12077.76953125
Iteration 11200: Loss = -12077.7333984375
Iteration 11300: Loss = -12068.4130859375
Iteration 11400: Loss = -12067.6484375
Iteration 11500: Loss = -12060.9912109375
Iteration 11600: Loss = -12060.822265625
Iteration 11700: Loss = -12060.77734375
Iteration 11800: Loss = -12060.7509765625
Iteration 11900: Loss = -12060.7294921875
Iteration 12000: Loss = -12060.701171875
Iteration 12100: Loss = -12052.2470703125
Iteration 12200: Loss = -12052.1494140625
Iteration 12300: Loss = -12052.115234375
Iteration 12400: Loss = -12049.2646484375
Iteration 12500: Loss = -12041.66015625
Iteration 12600: Loss = -12041.58984375
Iteration 12700: Loss = -12041.5595703125
Iteration 12800: Loss = -12041.5380859375
Iteration 12900: Loss = -12041.5263671875
Iteration 13000: Loss = -12041.515625
Iteration 13100: Loss = -12041.505859375
Iteration 13200: Loss = -12041.498046875
Iteration 13300: Loss = -12041.4091796875
Iteration 13400: Loss = -12041.3798828125
Iteration 13500: Loss = -12041.3759765625
Iteration 13600: Loss = -12041.369140625
Iteration 13700: Loss = -12041.3623046875
Iteration 13800: Loss = -12041.19140625
Iteration 13900: Loss = -12036.76171875
Iteration 14000: Loss = -12036.740234375
Iteration 14100: Loss = -12036.7314453125
Iteration 14200: Loss = -12036.7255859375
Iteration 14300: Loss = -12036.7216796875
Iteration 14400: Loss = -12036.7177734375
Iteration 14500: Loss = -12035.0
Iteration 14600: Loss = -12034.99609375
Iteration 14700: Loss = -12034.9931640625
Iteration 14800: Loss = -12034.990234375
Iteration 14900: Loss = -12034.98828125
Iteration 15000: Loss = -12034.984375
Iteration 15100: Loss = -12034.9814453125
Iteration 15200: Loss = -12034.98046875
Iteration 15300: Loss = -12034.98046875
Iteration 15400: Loss = -12034.9794921875
Iteration 15500: Loss = -12034.9775390625
Iteration 15600: Loss = -12034.9765625
Iteration 15700: Loss = -12034.974609375
Iteration 15800: Loss = -12034.9765625
1
Iteration 15900: Loss = -12034.9755859375
2
Iteration 16000: Loss = -12034.97265625
Iteration 16100: Loss = -12034.96875
Iteration 16200: Loss = -12034.9697265625
1
Iteration 16300: Loss = -12034.9697265625
2
Iteration 16400: Loss = -12034.96875
Iteration 16500: Loss = -12034.966796875
Iteration 16600: Loss = -12034.966796875
Iteration 16700: Loss = -12034.96484375
Iteration 16800: Loss = -12034.9658203125
1
Iteration 16900: Loss = -12034.96484375
Iteration 17000: Loss = -12034.96484375
Iteration 17100: Loss = -12034.96484375
Iteration 17200: Loss = -12034.9638671875
Iteration 17300: Loss = -12034.9619140625
Iteration 17400: Loss = -12034.962890625
1
Iteration 17500: Loss = -12034.962890625
2
Iteration 17600: Loss = -12034.962890625
3
Iteration 17700: Loss = -12034.962890625
4
Iteration 17800: Loss = -12034.9609375
Iteration 17900: Loss = -12034.9619140625
1
Iteration 18000: Loss = -12034.9599609375
Iteration 18100: Loss = -12034.958984375
Iteration 18200: Loss = -12034.9609375
1
Iteration 18300: Loss = -12034.9599609375
2
Iteration 18400: Loss = -12034.9609375
3
Iteration 18500: Loss = -12034.958984375
Iteration 18600: Loss = -12034.9599609375
1
Iteration 18700: Loss = -12034.9599609375
2
Iteration 18800: Loss = -12034.958984375
Iteration 18900: Loss = -12034.9599609375
1
Iteration 19000: Loss = -12034.958984375
Iteration 19100: Loss = -12034.958984375
Iteration 19200: Loss = -12034.9599609375
1
Iteration 19300: Loss = -12034.9580078125
Iteration 19400: Loss = -12034.9599609375
1
Iteration 19500: Loss = -12034.958984375
2
Iteration 19600: Loss = -12034.958984375
3
Iteration 19700: Loss = -12034.95703125
Iteration 19800: Loss = -12034.9609375
1
Iteration 19900: Loss = -12034.9580078125
2
Iteration 20000: Loss = -12034.95703125
Iteration 20100: Loss = -12034.958984375
1
Iteration 20200: Loss = -12034.9580078125
2
Iteration 20300: Loss = -12034.958984375
3
Iteration 20400: Loss = -12034.9580078125
4
Iteration 20500: Loss = -12034.9580078125
5
Iteration 20600: Loss = -12034.9560546875
Iteration 20700: Loss = -12034.95703125
1
Iteration 20800: Loss = -12034.95703125
2
Iteration 20900: Loss = -12034.95703125
3
Iteration 21000: Loss = -12034.9580078125
4
Iteration 21100: Loss = -12034.95703125
5
Iteration 21200: Loss = -12034.9580078125
6
Iteration 21300: Loss = -12034.904296875
Iteration 21400: Loss = -12028.396484375
Iteration 21500: Loss = -12028.3515625
Iteration 21600: Loss = -12028.3369140625
Iteration 21700: Loss = -12028.33203125
Iteration 21800: Loss = -12028.3291015625
Iteration 21900: Loss = -12028.3251953125
Iteration 22000: Loss = -12028.3251953125
Iteration 22100: Loss = -12028.3232421875
Iteration 22200: Loss = -12028.3232421875
Iteration 22300: Loss = -12028.32421875
1
Iteration 22400: Loss = -12028.322265625
Iteration 22500: Loss = -12028.3212890625
Iteration 22600: Loss = -12028.3212890625
Iteration 22700: Loss = -12028.3203125
Iteration 22800: Loss = -12028.3203125
Iteration 22900: Loss = -12028.3212890625
1
Iteration 23000: Loss = -12028.3212890625
2
Iteration 23100: Loss = -12028.3193359375
Iteration 23200: Loss = -12028.3203125
1
Iteration 23300: Loss = -12028.3203125
2
Iteration 23400: Loss = -12028.3203125
3
Iteration 23500: Loss = -12028.3203125
4
Iteration 23600: Loss = -12028.107421875
Iteration 23700: Loss = -12028.10546875
Iteration 23800: Loss = -12028.1064453125
1
Iteration 23900: Loss = -12028.1044921875
Iteration 24000: Loss = -12023.37890625
Iteration 24100: Loss = -12023.33203125
Iteration 24200: Loss = -12023.3251953125
Iteration 24300: Loss = -12023.322265625
Iteration 24400: Loss = -12023.3193359375
Iteration 24500: Loss = -12023.3203125
1
Iteration 24600: Loss = -12023.31640625
Iteration 24700: Loss = -12023.3173828125
1
Iteration 24800: Loss = -12023.31640625
Iteration 24900: Loss = -12023.31640625
Iteration 25000: Loss = -12023.3173828125
1
Iteration 25100: Loss = -12023.31640625
Iteration 25200: Loss = -12023.31640625
Iteration 25300: Loss = -12023.3173828125
1
Iteration 25400: Loss = -12023.3154296875
Iteration 25500: Loss = -12023.3154296875
Iteration 25600: Loss = -12023.3173828125
1
Iteration 25700: Loss = -12023.3154296875
Iteration 25800: Loss = -12023.3154296875
Iteration 25900: Loss = -12023.3154296875
Iteration 26000: Loss = -12023.3154296875
Iteration 26100: Loss = -12023.3154296875
Iteration 26200: Loss = -12023.314453125
Iteration 26300: Loss = -12023.31640625
1
Iteration 26400: Loss = -12023.3154296875
2
Iteration 26500: Loss = -12023.3154296875
3
Iteration 26600: Loss = -12023.3154296875
4
Iteration 26700: Loss = -12023.3154296875
5
Iteration 26800: Loss = -12023.3154296875
6
Iteration 26900: Loss = -12023.31640625
7
Iteration 27000: Loss = -12023.31640625
8
Iteration 27100: Loss = -12023.3154296875
9
Iteration 27200: Loss = -12023.31640625
10
Iteration 27300: Loss = -12023.3154296875
11
Iteration 27400: Loss = -12023.3154296875
12
Iteration 27500: Loss = -12023.3154296875
13
Iteration 27600: Loss = -12023.3154296875
14
Iteration 27700: Loss = -12023.314453125
Iteration 27800: Loss = -12023.3154296875
1
Iteration 27900: Loss = -12023.31640625
2
Iteration 28000: Loss = -12023.3154296875
3
Iteration 28100: Loss = -12023.314453125
Iteration 28200: Loss = -12023.314453125
Iteration 28300: Loss = -12023.314453125
Iteration 28400: Loss = -12023.3154296875
1
Iteration 28500: Loss = -12023.3154296875
2
Iteration 28600: Loss = -12023.3154296875
3
Iteration 28700: Loss = -12023.3154296875
4
Iteration 28800: Loss = -12023.31640625
5
Iteration 28900: Loss = -12023.314453125
Iteration 29000: Loss = -12023.314453125
Iteration 29100: Loss = -12023.3134765625
Iteration 29200: Loss = -12023.3154296875
1
Iteration 29300: Loss = -12023.314453125
2
Iteration 29400: Loss = -12023.314453125
3
Iteration 29500: Loss = -12023.3154296875
4
Iteration 29600: Loss = -12023.3154296875
5
Iteration 29700: Loss = -12023.314453125
6
Iteration 29800: Loss = -12023.314453125
7
Iteration 29900: Loss = -12023.314453125
8
pi: tensor([[0.5131, 0.4869],
        [0.5208, 0.4792]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5400, 0.4600], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3050, 0.1110],
         [0.9793, 0.2987]],

        [[0.0439, 0.1024],
         [0.9901, 0.9912]],

        [[0.2536, 0.1005],
         [0.8716, 0.3746]],

        [[0.7953, 0.0956],
         [0.5965, 0.0246]],

        [[0.6530, 0.1103],
         [0.0651, 0.7742]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.03807625120079024
Average Adjusted Rand Index: 0.9681601267189862
[-0.0006519339167301126, 0.03807625120079024] [-0.0029471671170078536, 0.9681601267189862] [12463.4873046875, 12023.314453125]
-------------------------------------
This iteration is 99
True Objective function: Loss = -11966.208983127
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37221.67578125
Iteration 100: Loss = -22190.162109375
Iteration 200: Loss = -14551.7412109375
Iteration 300: Loss = -13279.1376953125
Iteration 400: Loss = -12984.1767578125
Iteration 500: Loss = -12851.4111328125
Iteration 600: Loss = -12784.34375
Iteration 700: Loss = -12738.15234375
Iteration 800: Loss = -12697.3955078125
Iteration 900: Loss = -12656.76953125
Iteration 1000: Loss = -12633.025390625
Iteration 1100: Loss = -12616.03515625
Iteration 1200: Loss = -12601.06640625
Iteration 1300: Loss = -12578.8271484375
Iteration 1400: Loss = -12562.8447265625
Iteration 1500: Loss = -12556.2548828125
Iteration 1600: Loss = -12551.4462890625
Iteration 1700: Loss = -12547.57421875
Iteration 1800: Loss = -12544.4189453125
Iteration 1900: Loss = -12539.8564453125
Iteration 2000: Loss = -12535.1455078125
Iteration 2100: Loss = -12531.8046875
Iteration 2200: Loss = -12528.8515625
Iteration 2300: Loss = -12524.966796875
Iteration 2400: Loss = -12520.7763671875
Iteration 2500: Loss = -12517.0625
Iteration 2600: Loss = -12513.8837890625
Iteration 2700: Loss = -12511.6357421875
Iteration 2800: Loss = -12509.673828125
Iteration 2900: Loss = -12507.466796875
Iteration 3000: Loss = -12504.2080078125
Iteration 3100: Loss = -12500.48046875
Iteration 3200: Loss = -12498.5380859375
Iteration 3300: Loss = -12496.7783203125
Iteration 3400: Loss = -12492.6552734375
Iteration 3500: Loss = -12491.3935546875
Iteration 3600: Loss = -12490.4267578125
Iteration 3700: Loss = -12489.0576171875
Iteration 3800: Loss = -12487.296875
Iteration 3900: Loss = -12486.60546875
Iteration 4000: Loss = -12486.0888671875
Iteration 4100: Loss = -12485.6337890625
Iteration 4200: Loss = -12485.1962890625
Iteration 4300: Loss = -12484.7919921875
Iteration 4400: Loss = -12484.2333984375
Iteration 4500: Loss = -12482.5986328125
Iteration 4600: Loss = -12482.244140625
Iteration 4700: Loss = -12481.900390625
Iteration 4800: Loss = -12481.619140625
Iteration 4900: Loss = -12481.40625
Iteration 5000: Loss = -12481.22265625
Iteration 5100: Loss = -12481.0517578125
Iteration 5200: Loss = -12480.873046875
Iteration 5300: Loss = -12480.6689453125
Iteration 5400: Loss = -12480.431640625
Iteration 5500: Loss = -12480.193359375
Iteration 5600: Loss = -12479.9716796875
Iteration 5700: Loss = -12479.6826171875
Iteration 5800: Loss = -12479.3798828125
Iteration 5900: Loss = -12479.099609375
Iteration 6000: Loss = -12478.767578125
Iteration 6100: Loss = -12478.3291015625
Iteration 6200: Loss = -12477.8720703125
Iteration 6300: Loss = -12477.6376953125
Iteration 6400: Loss = -12477.48828125
Iteration 6500: Loss = -12477.3486328125
Iteration 6600: Loss = -12477.18359375
Iteration 6700: Loss = -12476.8701171875
Iteration 6800: Loss = -12476.22265625
Iteration 6900: Loss = -12474.6259765625
Iteration 7000: Loss = -12473.41796875
Iteration 7100: Loss = -12467.2451171875
Iteration 7200: Loss = -12463.3037109375
Iteration 7300: Loss = -12461.38671875
Iteration 7400: Loss = -12455.875
Iteration 7500: Loss = -12451.658203125
Iteration 7600: Loss = -12449.9091796875
Iteration 7700: Loss = -12449.203125
Iteration 7800: Loss = -12448.87890625
Iteration 7900: Loss = -12447.826171875
Iteration 8000: Loss = -12446.6337890625
Iteration 8100: Loss = -12443.9931640625
Iteration 8200: Loss = -12443.830078125
Iteration 8300: Loss = -12443.6103515625
Iteration 8400: Loss = -12438.74609375
Iteration 8500: Loss = -12434.9140625
Iteration 8600: Loss = -12422.6083984375
Iteration 8700: Loss = -12421.9580078125
Iteration 8800: Loss = -12421.5625
Iteration 8900: Loss = -12421.3837890625
Iteration 9000: Loss = -12421.2998046875
Iteration 9100: Loss = -12421.2236328125
Iteration 9200: Loss = -12421.14453125
Iteration 9300: Loss = -12421.048828125
Iteration 9400: Loss = -12420.9609375
Iteration 9500: Loss = -12420.9140625
Iteration 9600: Loss = -12420.8681640625
Iteration 9700: Loss = -12420.5791015625
Iteration 9800: Loss = -12418.2900390625
Iteration 9900: Loss = -12418.2470703125
Iteration 10000: Loss = -12418.2236328125
Iteration 10100: Loss = -12418.2060546875
Iteration 10200: Loss = -12418.1923828125
Iteration 10300: Loss = -12418.1806640625
Iteration 10400: Loss = -12418.1650390625
Iteration 10500: Loss = -12418.150390625
Iteration 10600: Loss = -12418.1279296875
Iteration 10700: Loss = -12417.5546875
Iteration 10800: Loss = -12417.2548828125
Iteration 10900: Loss = -12416.1240234375
Iteration 11000: Loss = -12415.99609375
Iteration 11100: Loss = -12415.978515625
Iteration 11200: Loss = -12415.96875
Iteration 11300: Loss = -12415.9609375
Iteration 11400: Loss = -12415.9521484375
Iteration 11500: Loss = -12415.94140625
Iteration 11600: Loss = -12415.935546875
Iteration 11700: Loss = -12415.9267578125
Iteration 11800: Loss = -12415.9111328125
Iteration 11900: Loss = -12415.896484375
Iteration 12000: Loss = -12415.8583984375
Iteration 12100: Loss = -12415.849609375
Iteration 12200: Loss = -12415.841796875
Iteration 12300: Loss = -12415.8369140625
Iteration 12400: Loss = -12415.833984375
Iteration 12500: Loss = -12415.826171875
Iteration 12600: Loss = -12415.822265625
Iteration 12700: Loss = -12415.814453125
Iteration 12800: Loss = -12415.765625
Iteration 12900: Loss = -12414.552734375
Iteration 13000: Loss = -12414.533203125
Iteration 13100: Loss = -12414.5302734375
Iteration 13200: Loss = -12414.5283203125
Iteration 13300: Loss = -12414.52734375
Iteration 13400: Loss = -12414.52734375
Iteration 13500: Loss = -12414.5234375
Iteration 13600: Loss = -12414.5234375
Iteration 13700: Loss = -12414.521484375
Iteration 13800: Loss = -12414.521484375
Iteration 13900: Loss = -12414.51953125
Iteration 14000: Loss = -12414.517578125
Iteration 14100: Loss = -12414.517578125
Iteration 14200: Loss = -12414.5185546875
1
Iteration 14300: Loss = -12414.5166015625
Iteration 14400: Loss = -12414.515625
Iteration 14500: Loss = -12414.513671875
Iteration 14600: Loss = -12414.494140625
Iteration 14700: Loss = -12414.4853515625
Iteration 14800: Loss = -12414.4833984375
Iteration 14900: Loss = -12414.4814453125
Iteration 15000: Loss = -12414.4619140625
Iteration 15100: Loss = -12414.4306640625
Iteration 15200: Loss = -12414.4306640625
Iteration 15300: Loss = -12414.4052734375
Iteration 15400: Loss = -12414.3984375
Iteration 15500: Loss = -12414.3994140625
1
Iteration 15600: Loss = -12414.3974609375
Iteration 15700: Loss = -12414.396484375
Iteration 15800: Loss = -12414.3955078125
Iteration 15900: Loss = -12414.390625
Iteration 16000: Loss = -12414.3896484375
Iteration 16100: Loss = -12414.384765625
Iteration 16200: Loss = -12414.3818359375
Iteration 16300: Loss = -12414.3837890625
1
Iteration 16400: Loss = -12414.3818359375
Iteration 16500: Loss = -12414.380859375
Iteration 16600: Loss = -12414.3818359375
1
Iteration 16700: Loss = -12414.380859375
Iteration 16800: Loss = -12414.3798828125
Iteration 16900: Loss = -12414.3798828125
Iteration 17000: Loss = -12414.37890625
Iteration 17100: Loss = -12414.37890625
Iteration 17200: Loss = -12414.3779296875
Iteration 17300: Loss = -12414.3779296875
Iteration 17400: Loss = -12414.37109375
Iteration 17500: Loss = -12414.373046875
1
Iteration 17600: Loss = -12414.37109375
Iteration 17700: Loss = -12414.3583984375
Iteration 17800: Loss = -12414.359375
1
Iteration 17900: Loss = -12414.359375
2
Iteration 18000: Loss = -12414.3515625
Iteration 18100: Loss = -12414.3505859375
Iteration 18200: Loss = -12414.3408203125
Iteration 18300: Loss = -12414.341796875
1
Iteration 18400: Loss = -12414.33984375
Iteration 18500: Loss = -12414.341796875
1
Iteration 18600: Loss = -12414.3408203125
2
Iteration 18700: Loss = -12414.3408203125
3
Iteration 18800: Loss = -12414.3173828125
Iteration 18900: Loss = -12414.3125
Iteration 19000: Loss = -12414.314453125
1
Iteration 19100: Loss = -12414.2802734375
Iteration 19200: Loss = -12414.2802734375
Iteration 19300: Loss = -12414.275390625
Iteration 19400: Loss = -12414.2734375
Iteration 19500: Loss = -12414.2724609375
Iteration 19600: Loss = -12414.2724609375
Iteration 19700: Loss = -12414.2685546875
Iteration 19800: Loss = -12414.267578125
Iteration 19900: Loss = -12414.263671875
Iteration 20000: Loss = -12414.2626953125
Iteration 20100: Loss = -12414.2646484375
1
Iteration 20200: Loss = -12414.263671875
2
Iteration 20300: Loss = -12414.263671875
3
Iteration 20400: Loss = -12414.26171875
Iteration 20500: Loss = -12414.2626953125
1
Iteration 20600: Loss = -12414.26171875
Iteration 20700: Loss = -12414.2626953125
1
Iteration 20800: Loss = -12414.2626953125
2
Iteration 20900: Loss = -12414.2626953125
3
Iteration 21000: Loss = -12414.26171875
Iteration 21100: Loss = -12414.2626953125
1
Iteration 21200: Loss = -12414.26171875
Iteration 21300: Loss = -12414.259765625
Iteration 21400: Loss = -12414.26171875
1
Iteration 21500: Loss = -12414.2607421875
2
Iteration 21600: Loss = -12414.26171875
3
Iteration 21700: Loss = -12414.259765625
Iteration 21800: Loss = -12414.2626953125
1
Iteration 21900: Loss = -12414.26171875
2
Iteration 22000: Loss = -12414.2578125
Iteration 22100: Loss = -12414.2578125
Iteration 22200: Loss = -12414.2587890625
1
Iteration 22300: Loss = -12414.2587890625
2
Iteration 22400: Loss = -12414.2587890625
3
Iteration 22500: Loss = -12414.2568359375
Iteration 22600: Loss = -12414.255859375
Iteration 22700: Loss = -12414.2548828125
Iteration 22800: Loss = -12414.255859375
1
Iteration 22900: Loss = -12414.2568359375
2
Iteration 23000: Loss = -12414.2548828125
Iteration 23100: Loss = -12414.2548828125
Iteration 23200: Loss = -12414.25390625
Iteration 23300: Loss = -12414.25390625
Iteration 23400: Loss = -12414.255859375
1
Iteration 23500: Loss = -12414.2529296875
Iteration 23600: Loss = -12414.2548828125
1
Iteration 23700: Loss = -12414.25390625
2
Iteration 23800: Loss = -12414.2548828125
3
Iteration 23900: Loss = -12414.25390625
4
Iteration 24000: Loss = -12414.25390625
5
Iteration 24100: Loss = -12414.255859375
6
Iteration 24200: Loss = -12414.201171875
Iteration 24300: Loss = -12414.1982421875
Iteration 24400: Loss = -12414.2001953125
1
Iteration 24500: Loss = -12414.1982421875
Iteration 24600: Loss = -12414.1982421875
Iteration 24700: Loss = -12414.1982421875
Iteration 24800: Loss = -12414.1982421875
Iteration 24900: Loss = -12414.1982421875
Iteration 25000: Loss = -12414.2001953125
1
Iteration 25100: Loss = -12414.2001953125
2
Iteration 25200: Loss = -12414.19921875
3
Iteration 25300: Loss = -12414.1982421875
Iteration 25400: Loss = -12414.19921875
1
Iteration 25500: Loss = -12414.1982421875
Iteration 25600: Loss = -12414.19921875
1
Iteration 25700: Loss = -12414.201171875
2
Iteration 25800: Loss = -12414.1982421875
Iteration 25900: Loss = -12414.19921875
1
Iteration 26000: Loss = -12414.025390625
Iteration 26100: Loss = -12411.990234375
Iteration 26200: Loss = -12411.43359375
Iteration 26300: Loss = -12409.515625
Iteration 26400: Loss = -12406.6484375
Iteration 26500: Loss = -12309.814453125
Iteration 26600: Loss = -12307.6494140625
Iteration 26700: Loss = -12307.0302734375
Iteration 26800: Loss = -12298.9794921875
Iteration 26900: Loss = -12242.1044921875
Iteration 27000: Loss = -12240.259765625
Iteration 27100: Loss = -12234.8203125
Iteration 27200: Loss = -12234.498046875
Iteration 27300: Loss = -12232.5947265625
Iteration 27400: Loss = -12219.671875
Iteration 27500: Loss = -12217.767578125
Iteration 27600: Loss = -12208.96875
Iteration 27700: Loss = -12192.38671875
Iteration 27800: Loss = -12182.990234375
Iteration 27900: Loss = -12175.53515625
Iteration 28000: Loss = -12157.6533203125
Iteration 28100: Loss = -12129.9921875
Iteration 28200: Loss = -12115.232421875
Iteration 28300: Loss = -12109.779296875
Iteration 28400: Loss = -12108.1357421875
Iteration 28500: Loss = -12098.650390625
Iteration 28600: Loss = -12083.765625
Iteration 28700: Loss = -12079.251953125
Iteration 28800: Loss = -12078.9248046875
Iteration 28900: Loss = -12061.5859375
Iteration 29000: Loss = -12060.1904296875
Iteration 29100: Loss = -12053.0634765625
Iteration 29200: Loss = -12052.951171875
Iteration 29300: Loss = -12052.8935546875
Iteration 29400: Loss = -12052.85546875
Iteration 29500: Loss = -12052.8271484375
Iteration 29600: Loss = -12043.615234375
Iteration 29700: Loss = -12042.240234375
Iteration 29800: Loss = -12042.16796875
Iteration 29900: Loss = -12042.1240234375
pi: tensor([[0.3852, 0.6148],
        [0.6543, 0.3457]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5292, 0.4708], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2944, 0.0985],
         [0.0105, 0.3095]],

        [[0.9779, 0.1054],
         [0.0169, 0.7991]],

        [[0.0381, 0.1060],
         [0.8689, 0.7845]],

        [[0.9480, 0.1189],
         [0.6522, 0.0354]],

        [[0.0088, 0.0971],
         [0.2378, 0.1010]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 3
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080650161849864
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.02903402421291179
Average Adjusted Rand Index: 0.9536126584409926
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22042.17578125
Iteration 100: Loss = -15854.1416015625
Iteration 200: Loss = -13147.2705078125
Iteration 300: Loss = -12663.6513671875
Iteration 400: Loss = -12554.5380859375
Iteration 500: Loss = -12523.7138671875
Iteration 600: Loss = -12511.06640625
Iteration 700: Loss = -12503.8271484375
Iteration 800: Loss = -12496.73046875
Iteration 900: Loss = -12493.78515625
Iteration 1000: Loss = -12491.7587890625
Iteration 1100: Loss = -12490.2939453125
Iteration 1200: Loss = -12489.2109375
Iteration 1300: Loss = -12488.390625
Iteration 1400: Loss = -12487.748046875
Iteration 1500: Loss = -12487.234375
Iteration 1600: Loss = -12486.818359375
Iteration 1700: Loss = -12486.4775390625
Iteration 1800: Loss = -12486.1904296875
Iteration 1900: Loss = -12485.9453125
Iteration 2000: Loss = -12485.7392578125
Iteration 2100: Loss = -12485.560546875
Iteration 2200: Loss = -12485.408203125
Iteration 2300: Loss = -12485.2724609375
Iteration 2400: Loss = -12485.1591796875
Iteration 2500: Loss = -12485.0576171875
Iteration 2600: Loss = -12484.966796875
Iteration 2700: Loss = -12484.8857421875
Iteration 2800: Loss = -12484.810546875
Iteration 2900: Loss = -12484.73828125
Iteration 3000: Loss = -12484.67578125
Iteration 3100: Loss = -12484.62109375
Iteration 3200: Loss = -12484.5712890625
Iteration 3300: Loss = -12484.525390625
Iteration 3400: Loss = -12484.484375
Iteration 3500: Loss = -12484.447265625
Iteration 3600: Loss = -12484.41015625
Iteration 3700: Loss = -12484.3759765625
Iteration 3800: Loss = -12484.341796875
Iteration 3900: Loss = -12484.3095703125
Iteration 4000: Loss = -12484.2763671875
Iteration 4100: Loss = -12484.2421875
Iteration 4200: Loss = -12484.2021484375
Iteration 4300: Loss = -12484.150390625
Iteration 4400: Loss = -12484.0986328125
Iteration 4500: Loss = -12484.046875
Iteration 4600: Loss = -12484.00390625
Iteration 4700: Loss = -12483.96875
Iteration 4800: Loss = -12483.939453125
Iteration 4900: Loss = -12483.9111328125
Iteration 5000: Loss = -12483.8857421875
Iteration 5100: Loss = -12483.86328125
Iteration 5200: Loss = -12483.84375
Iteration 5300: Loss = -12483.8271484375
Iteration 5400: Loss = -12483.8125
Iteration 5500: Loss = -12483.798828125
Iteration 5600: Loss = -12483.7822265625
Iteration 5700: Loss = -12483.767578125
Iteration 5800: Loss = -12483.7548828125
Iteration 5900: Loss = -12483.73828125
Iteration 6000: Loss = -12483.7255859375
Iteration 6100: Loss = -12483.7080078125
Iteration 6200: Loss = -12483.6962890625
Iteration 6300: Loss = -12483.6787109375
Iteration 6400: Loss = -12483.6640625
Iteration 6500: Loss = -12483.6484375
Iteration 6600: Loss = -12483.6318359375
Iteration 6700: Loss = -12483.6171875
Iteration 6800: Loss = -12483.5986328125
Iteration 6900: Loss = -12483.5830078125
Iteration 7000: Loss = -12483.56640625
Iteration 7100: Loss = -12483.548828125
Iteration 7200: Loss = -12483.53515625
Iteration 7300: Loss = -12483.51953125
Iteration 7400: Loss = -12483.51171875
Iteration 7500: Loss = -12483.501953125
Iteration 7600: Loss = -12483.4951171875
Iteration 7700: Loss = -12483.48828125
Iteration 7800: Loss = -12483.482421875
Iteration 7900: Loss = -12483.48046875
Iteration 8000: Loss = -12483.4755859375
Iteration 8100: Loss = -12483.4716796875
Iteration 8200: Loss = -12483.4677734375
Iteration 8300: Loss = -12483.466796875
Iteration 8400: Loss = -12483.46484375
Iteration 8500: Loss = -12483.4609375
Iteration 8600: Loss = -12483.458984375
Iteration 8700: Loss = -12483.455078125
Iteration 8800: Loss = -12483.451171875
Iteration 8900: Loss = -12483.4482421875
Iteration 9000: Loss = -12483.4423828125
Iteration 9100: Loss = -12483.4345703125
Iteration 9200: Loss = -12483.4267578125
Iteration 9300: Loss = -12483.4150390625
Iteration 9400: Loss = -12483.40234375
Iteration 9500: Loss = -12483.388671875
Iteration 9600: Loss = -12483.3759765625
Iteration 9700: Loss = -12483.3671875
Iteration 9800: Loss = -12483.3603515625
Iteration 9900: Loss = -12483.3525390625
Iteration 10000: Loss = -12483.3505859375
Iteration 10100: Loss = -12483.349609375
Iteration 10200: Loss = -12483.345703125
Iteration 10300: Loss = -12483.3447265625
Iteration 10400: Loss = -12483.3427734375
Iteration 10500: Loss = -12483.34375
1
Iteration 10600: Loss = -12483.3427734375
Iteration 10700: Loss = -12483.341796875
Iteration 10800: Loss = -12483.33984375
Iteration 10900: Loss = -12483.33984375
Iteration 11000: Loss = -12483.3408203125
1
Iteration 11100: Loss = -12483.3408203125
2
Iteration 11200: Loss = -12483.33984375
Iteration 11300: Loss = -12483.3408203125
1
Iteration 11400: Loss = -12483.3388671875
Iteration 11500: Loss = -12483.33984375
1
Iteration 11600: Loss = -12483.33984375
2
Iteration 11700: Loss = -12483.3408203125
3
Iteration 11800: Loss = -12483.33984375
4
Iteration 11900: Loss = -12483.33984375
5
Iteration 12000: Loss = -12483.3408203125
6
Iteration 12100: Loss = -12483.33984375
7
Iteration 12200: Loss = -12483.33984375
8
Iteration 12300: Loss = -12483.3388671875
Iteration 12400: Loss = -12483.3388671875
Iteration 12500: Loss = -12483.33984375
1
Iteration 12600: Loss = -12483.33984375
2
Iteration 12700: Loss = -12483.33984375
3
Iteration 12800: Loss = -12483.3388671875
Iteration 12900: Loss = -12483.3388671875
Iteration 13000: Loss = -12483.3388671875
Iteration 13100: Loss = -12483.33984375
1
Iteration 13200: Loss = -12483.3388671875
Iteration 13300: Loss = -12483.3408203125
1
Iteration 13400: Loss = -12483.3388671875
Iteration 13500: Loss = -12483.33984375
1
Iteration 13600: Loss = -12483.33984375
2
Iteration 13700: Loss = -12483.337890625
Iteration 13800: Loss = -12483.33984375
1
Iteration 13900: Loss = -12483.337890625
Iteration 14000: Loss = -12483.33984375
1
Iteration 14100: Loss = -12483.33984375
2
Iteration 14200: Loss = -12483.337890625
Iteration 14300: Loss = -12483.3388671875
1
Iteration 14400: Loss = -12483.3388671875
2
Iteration 14500: Loss = -12483.3388671875
3
Iteration 14600: Loss = -12483.33984375
4
Iteration 14700: Loss = -12483.3388671875
5
Iteration 14800: Loss = -12483.3388671875
6
Iteration 14900: Loss = -12483.3369140625
Iteration 15000: Loss = -12483.337890625
1
Iteration 15100: Loss = -12483.33984375
2
Iteration 15200: Loss = -12483.337890625
3
Iteration 15300: Loss = -12483.3388671875
4
Iteration 15400: Loss = -12483.3388671875
5
Iteration 15500: Loss = -12483.337890625
6
Iteration 15600: Loss = -12483.337890625
7
Iteration 15700: Loss = -12483.337890625
8
Iteration 15800: Loss = -12483.337890625
9
Iteration 15900: Loss = -12483.3388671875
10
Iteration 16000: Loss = -12483.33984375
11
Iteration 16100: Loss = -12483.337890625
12
Iteration 16200: Loss = -12483.3388671875
13
Iteration 16300: Loss = -12483.3388671875
14
Iteration 16400: Loss = -12483.3388671875
15
Stopping early at iteration 16400 due to no improvement.
pi: tensor([[0.9819, 0.0181],
        [0.4795, 0.5205]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9971e-01, 2.9168e-04], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2038, 0.2162],
         [0.9931, 0.1477]],

        [[0.9101, 0.2807],
         [0.0256, 0.9862]],

        [[0.8765, 0.1834],
         [0.9867, 0.0492]],

        [[0.8745, 0.1725],
         [0.7637, 0.6932]],

        [[0.6411, 0.1467],
         [0.6045, 0.9752]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.02903402421291179, 0.0] [0.9536126584409926, 0.0] [12042.0908203125, 12483.3388671875]
