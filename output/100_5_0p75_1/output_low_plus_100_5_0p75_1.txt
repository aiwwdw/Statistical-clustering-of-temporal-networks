nohup: ignoring input
  0%|          | 0/100 [00:00<?, ?it/s]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/adj_generator.py:101: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)
  Y = torch.tensor(Y)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  1%|          | 1/100 [15:55<26:16:22, 955.38s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  2%|▏         | 2/100 [34:32<28:35:37, 1050.38s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  3%|▎         | 3/100 [50:23<27:04:45, 1005.00s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  4%|▍         | 4/100 [1:08:40<27:46:28, 1041.55s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  5%|▌         | 5/100 [1:23:10<25:51:12, 979.71s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  6%|▌         | 6/100 [1:39:37<25:38:38, 982.12s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  7%|▋         | 7/100 [1:57:32<26:09:07, 1012.33s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  8%|▊         | 8/100 [2:10:27<23:56:28, 936.83s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  9%|▉         | 9/100 [2:25:55<23:36:54, 934.22s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 10%|█         | 10/100 [2:41:46<23:29:08, 939.43s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 11%|█         | 11/100 [2:53:14<21:19:12, 862.39s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 12%|█▏        | 12/100 [3:10:05<22:11:15, 907.68s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 13%|█▎        | 13/100 [3:26:31<22:30:23, 931.30s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 14%|█▍        | 14/100 [3:41:53<22:10:47, 928.46s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 15%|█▌        | 15/100 [3:57:27<21:57:43, 930.16s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 16%|█▌        | 16/100 [4:13:53<22:05:33, 946.83s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 17%|█▋        | 17/100 [4:25:43<20:11:36, 875.87s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 18%|█▊        | 18/100 [4:43:49<21:23:07, 938.87s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 19%|█▉        | 19/100 [4:55:44<19:36:44, 871.66s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 20%|██        | 20/100 [5:12:28<20:15:03, 911.30s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 21%|██        | 21/100 [5:26:51<19:41:00, 896.97s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 22%|██▏       | 22/100 [5:39:21<18:28:40, 852.83s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 23%|██▎       | 23/100 [5:48:59<16:28:41, 770.41s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 24%|██▍       | 24/100 [6:04:13<17:10:23, 813.47s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 25%|██▌       | 25/100 [6:19:50<17:43:12, 850.56s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 26%|██▌       | 26/100 [6:34:29<17:39:19, 858.91s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 27%|██▋       | 27/100 [6:52:10<18:38:56, 919.68s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 28%|██▊       | 28/100 [7:07:59<18:34:06, 928.43s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 29%|██▉       | 29/100 [7:26:07<19:15:11, 976.22s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 30%|███       | 30/100 [7:43:03<19:12:51, 988.17s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 31%|███       | 31/100 [7:58:41<18:38:59, 973.04s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 32%|███▏      | 32/100 [8:16:55<19:04:00, 1009.42s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 33%|███▎      | 33/100 [8:31:33<18:03:03, 969.90s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 34%|███▍      | 34/100 [8:45:35<17:04:48, 931.64s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
-------------------------------------
This iteration is 0
True Objective function: Loss = -10697.83745837653
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22540.87109375
inf tensor(22540.8711, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10770.46875
tensor(22540.8711, grad_fn=<NegBackward0>) tensor(10770.4688, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10769.9453125
tensor(10770.4688, grad_fn=<NegBackward0>) tensor(10769.9453, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10769.322265625
tensor(10769.9453, grad_fn=<NegBackward0>) tensor(10769.3223, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10769.060546875
tensor(10769.3223, grad_fn=<NegBackward0>) tensor(10769.0605, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10768.953125
tensor(10769.0605, grad_fn=<NegBackward0>) tensor(10768.9531, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10768.884765625
tensor(10768.9531, grad_fn=<NegBackward0>) tensor(10768.8848, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10768.83203125
tensor(10768.8848, grad_fn=<NegBackward0>) tensor(10768.8320, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10768.7900390625
tensor(10768.8320, grad_fn=<NegBackward0>) tensor(10768.7900, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10768.755859375
tensor(10768.7900, grad_fn=<NegBackward0>) tensor(10768.7559, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10768.7294921875
tensor(10768.7559, grad_fn=<NegBackward0>) tensor(10768.7295, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10768.70703125
tensor(10768.7295, grad_fn=<NegBackward0>) tensor(10768.7070, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10768.6875
tensor(10768.7070, grad_fn=<NegBackward0>) tensor(10768.6875, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10768.6708984375
tensor(10768.6875, grad_fn=<NegBackward0>) tensor(10768.6709, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10768.6552734375
tensor(10768.6709, grad_fn=<NegBackward0>) tensor(10768.6553, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10768.640625
tensor(10768.6553, grad_fn=<NegBackward0>) tensor(10768.6406, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10768.6279296875
tensor(10768.6406, grad_fn=<NegBackward0>) tensor(10768.6279, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10768.6162109375
tensor(10768.6279, grad_fn=<NegBackward0>) tensor(10768.6162, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10768.603515625
tensor(10768.6162, grad_fn=<NegBackward0>) tensor(10768.6035, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10768.5927734375
tensor(10768.6035, grad_fn=<NegBackward0>) tensor(10768.5928, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10768.5830078125
tensor(10768.5928, grad_fn=<NegBackward0>) tensor(10768.5830, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10768.5732421875
tensor(10768.5830, grad_fn=<NegBackward0>) tensor(10768.5732, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10768.5634765625
tensor(10768.5732, grad_fn=<NegBackward0>) tensor(10768.5635, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10768.556640625
tensor(10768.5635, grad_fn=<NegBackward0>) tensor(10768.5566, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10768.548828125
tensor(10768.5566, grad_fn=<NegBackward0>) tensor(10768.5488, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10768.541015625
tensor(10768.5488, grad_fn=<NegBackward0>) tensor(10768.5410, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10768.5341796875
tensor(10768.5410, grad_fn=<NegBackward0>) tensor(10768.5342, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10768.5283203125
tensor(10768.5342, grad_fn=<NegBackward0>) tensor(10768.5283, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10768.5205078125
tensor(10768.5283, grad_fn=<NegBackward0>) tensor(10768.5205, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10768.5146484375
tensor(10768.5205, grad_fn=<NegBackward0>) tensor(10768.5146, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10768.509765625
tensor(10768.5146, grad_fn=<NegBackward0>) tensor(10768.5098, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10768.5048828125
tensor(10768.5098, grad_fn=<NegBackward0>) tensor(10768.5049, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10768.5
tensor(10768.5049, grad_fn=<NegBackward0>) tensor(10768.5000, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10768.49609375
tensor(10768.5000, grad_fn=<NegBackward0>) tensor(10768.4961, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10768.4912109375
tensor(10768.4961, grad_fn=<NegBackward0>) tensor(10768.4912, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10768.4873046875
tensor(10768.4912, grad_fn=<NegBackward0>) tensor(10768.4873, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10768.4853515625
tensor(10768.4873, grad_fn=<NegBackward0>) tensor(10768.4854, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10768.4814453125
tensor(10768.4854, grad_fn=<NegBackward0>) tensor(10768.4814, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10768.48046875
tensor(10768.4814, grad_fn=<NegBackward0>) tensor(10768.4805, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10768.4755859375
tensor(10768.4805, grad_fn=<NegBackward0>) tensor(10768.4756, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10768.474609375
tensor(10768.4756, grad_fn=<NegBackward0>) tensor(10768.4746, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10768.47265625
tensor(10768.4746, grad_fn=<NegBackward0>) tensor(10768.4727, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10768.470703125
tensor(10768.4727, grad_fn=<NegBackward0>) tensor(10768.4707, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10768.46875
tensor(10768.4707, grad_fn=<NegBackward0>) tensor(10768.4688, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10768.46875
tensor(10768.4688, grad_fn=<NegBackward0>) tensor(10768.4688, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10768.466796875
tensor(10768.4688, grad_fn=<NegBackward0>) tensor(10768.4668, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10768.4677734375
tensor(10768.4668, grad_fn=<NegBackward0>) tensor(10768.4678, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -10768.46484375
tensor(10768.4668, grad_fn=<NegBackward0>) tensor(10768.4648, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10768.462890625
tensor(10768.4648, grad_fn=<NegBackward0>) tensor(10768.4629, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10768.4638671875
tensor(10768.4629, grad_fn=<NegBackward0>) tensor(10768.4639, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10768.462890625
tensor(10768.4629, grad_fn=<NegBackward0>) tensor(10768.4629, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10768.4609375
tensor(10768.4629, grad_fn=<NegBackward0>) tensor(10768.4609, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10768.4619140625
tensor(10768.4609, grad_fn=<NegBackward0>) tensor(10768.4619, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10768.4599609375
tensor(10768.4609, grad_fn=<NegBackward0>) tensor(10768.4600, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10768.4619140625
tensor(10768.4600, grad_fn=<NegBackward0>) tensor(10768.4619, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10768.4599609375
tensor(10768.4600, grad_fn=<NegBackward0>) tensor(10768.4600, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10768.4609375
tensor(10768.4600, grad_fn=<NegBackward0>) tensor(10768.4609, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10768.4580078125
tensor(10768.4600, grad_fn=<NegBackward0>) tensor(10768.4580, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10768.458984375
tensor(10768.4580, grad_fn=<NegBackward0>) tensor(10768.4590, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10768.4580078125
tensor(10768.4580, grad_fn=<NegBackward0>) tensor(10768.4580, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10768.4580078125
tensor(10768.4580, grad_fn=<NegBackward0>) tensor(10768.4580, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10768.4580078125
tensor(10768.4580, grad_fn=<NegBackward0>) tensor(10768.4580, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10768.45703125
tensor(10768.4580, grad_fn=<NegBackward0>) tensor(10768.4570, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10768.4580078125
tensor(10768.4570, grad_fn=<NegBackward0>) tensor(10768.4580, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10768.45703125
tensor(10768.4570, grad_fn=<NegBackward0>) tensor(10768.4570, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10768.4560546875
tensor(10768.4570, grad_fn=<NegBackward0>) tensor(10768.4561, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10768.45703125
tensor(10768.4561, grad_fn=<NegBackward0>) tensor(10768.4570, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10768.455078125
tensor(10768.4561, grad_fn=<NegBackward0>) tensor(10768.4551, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10768.4560546875
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4561, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10768.4560546875
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4561, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -10768.455078125
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4551, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10768.4560546875
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4561, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10768.455078125
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4551, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10768.455078125
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4551, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10768.4580078125
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4580, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10768.455078125
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4551, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10768.4560546875
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4561, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10768.455078125
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4551, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10768.455078125
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4551, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10768.4560546875
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4561, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10768.59375
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.5938, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -10768.455078125
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4551, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10768.455078125
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4551, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10768.455078125
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4551, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10768.4541015625
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4541, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10768.4541015625
tensor(10768.4541, grad_fn=<NegBackward0>) tensor(10768.4541, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10768.453125
tensor(10768.4541, grad_fn=<NegBackward0>) tensor(10768.4531, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10768.4541015625
tensor(10768.4531, grad_fn=<NegBackward0>) tensor(10768.4541, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -10768.46875
tensor(10768.4531, grad_fn=<NegBackward0>) tensor(10768.4688, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -10768.453125
tensor(10768.4531, grad_fn=<NegBackward0>) tensor(10768.4531, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10768.4560546875
tensor(10768.4531, grad_fn=<NegBackward0>) tensor(10768.4561, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10768.4541015625
tensor(10768.4531, grad_fn=<NegBackward0>) tensor(10768.4541, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -10768.455078125
tensor(10768.4531, grad_fn=<NegBackward0>) tensor(10768.4551, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -10768.4541015625
tensor(10768.4531, grad_fn=<NegBackward0>) tensor(10768.4541, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -10768.453125
tensor(10768.4531, grad_fn=<NegBackward0>) tensor(10768.4531, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10768.8349609375
tensor(10768.4531, grad_fn=<NegBackward0>) tensor(10768.8350, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -10768.453125
tensor(10768.4531, grad_fn=<NegBackward0>) tensor(10768.4531, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10768.453125
tensor(10768.4531, grad_fn=<NegBackward0>) tensor(10768.4531, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10768.4580078125
tensor(10768.4531, grad_fn=<NegBackward0>) tensor(10768.4580, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -10768.453125
tensor(10768.4531, grad_fn=<NegBackward0>) tensor(10768.4531, grad_fn=<NegBackward0>)
pi: tensor([[2.2959e-04, 9.9977e-01],
        [5.7298e-03, 9.9427e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0438, 0.9562], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.5206, 0.2277],
         [0.6749, 0.1543]],

        [[0.7215, 0.2352],
         [0.5419, 0.5314]],

        [[0.5899, 0.1172],
         [0.7066, 0.5150]],

        [[0.6835, 0.1700],
         [0.6164, 0.6273]],

        [[0.6516, 0.1978],
         [0.6486, 0.7146]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0011993956145362663
Average Adjusted Rand Index: -0.0001522251028708703
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21840.1015625
inf tensor(21840.1016, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10770.8876953125
tensor(21840.1016, grad_fn=<NegBackward0>) tensor(10770.8877, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10769.98046875
tensor(10770.8877, grad_fn=<NegBackward0>) tensor(10769.9805, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10769.498046875
tensor(10769.9805, grad_fn=<NegBackward0>) tensor(10769.4980, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10769.349609375
tensor(10769.4980, grad_fn=<NegBackward0>) tensor(10769.3496, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10769.26171875
tensor(10769.3496, grad_fn=<NegBackward0>) tensor(10769.2617, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10769.193359375
tensor(10769.2617, grad_fn=<NegBackward0>) tensor(10769.1934, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10769.130859375
tensor(10769.1934, grad_fn=<NegBackward0>) tensor(10769.1309, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10769.064453125
tensor(10769.1309, grad_fn=<NegBackward0>) tensor(10769.0645, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10768.9990234375
tensor(10769.0645, grad_fn=<NegBackward0>) tensor(10768.9990, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10768.9287109375
tensor(10768.9990, grad_fn=<NegBackward0>) tensor(10768.9287, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10768.857421875
tensor(10768.9287, grad_fn=<NegBackward0>) tensor(10768.8574, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10768.7861328125
tensor(10768.8574, grad_fn=<NegBackward0>) tensor(10768.7861, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10768.7177734375
tensor(10768.7861, grad_fn=<NegBackward0>) tensor(10768.7178, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10768.66015625
tensor(10768.7178, grad_fn=<NegBackward0>) tensor(10768.6602, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10768.6181640625
tensor(10768.6602, grad_fn=<NegBackward0>) tensor(10768.6182, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10768.5888671875
tensor(10768.6182, grad_fn=<NegBackward0>) tensor(10768.5889, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10768.568359375
tensor(10768.5889, grad_fn=<NegBackward0>) tensor(10768.5684, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10768.552734375
tensor(10768.5684, grad_fn=<NegBackward0>) tensor(10768.5527, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10768.541015625
tensor(10768.5527, grad_fn=<NegBackward0>) tensor(10768.5410, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10768.5302734375
tensor(10768.5410, grad_fn=<NegBackward0>) tensor(10768.5303, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10768.5234375
tensor(10768.5303, grad_fn=<NegBackward0>) tensor(10768.5234, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10768.5146484375
tensor(10768.5234, grad_fn=<NegBackward0>) tensor(10768.5146, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10768.5078125
tensor(10768.5146, grad_fn=<NegBackward0>) tensor(10768.5078, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10768.501953125
tensor(10768.5078, grad_fn=<NegBackward0>) tensor(10768.5020, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10768.49609375
tensor(10768.5020, grad_fn=<NegBackward0>) tensor(10768.4961, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10768.4921875
tensor(10768.4961, grad_fn=<NegBackward0>) tensor(10768.4922, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10768.48828125
tensor(10768.4922, grad_fn=<NegBackward0>) tensor(10768.4883, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10768.4833984375
tensor(10768.4883, grad_fn=<NegBackward0>) tensor(10768.4834, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10768.4794921875
tensor(10768.4834, grad_fn=<NegBackward0>) tensor(10768.4795, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10768.4775390625
tensor(10768.4795, grad_fn=<NegBackward0>) tensor(10768.4775, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10768.47265625
tensor(10768.4775, grad_fn=<NegBackward0>) tensor(10768.4727, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10768.470703125
tensor(10768.4727, grad_fn=<NegBackward0>) tensor(10768.4707, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10768.4697265625
tensor(10768.4707, grad_fn=<NegBackward0>) tensor(10768.4697, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10768.4677734375
tensor(10768.4697, grad_fn=<NegBackward0>) tensor(10768.4678, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10768.4638671875
tensor(10768.4678, grad_fn=<NegBackward0>) tensor(10768.4639, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10768.46484375
tensor(10768.4639, grad_fn=<NegBackward0>) tensor(10768.4648, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -10768.46484375
tensor(10768.4639, grad_fn=<NegBackward0>) tensor(10768.4648, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -10768.462890625
tensor(10768.4639, grad_fn=<NegBackward0>) tensor(10768.4629, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10768.4599609375
tensor(10768.4629, grad_fn=<NegBackward0>) tensor(10768.4600, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10768.4609375
tensor(10768.4600, grad_fn=<NegBackward0>) tensor(10768.4609, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -10768.4599609375
tensor(10768.4600, grad_fn=<NegBackward0>) tensor(10768.4600, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10768.4599609375
tensor(10768.4600, grad_fn=<NegBackward0>) tensor(10768.4600, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10768.458984375
tensor(10768.4600, grad_fn=<NegBackward0>) tensor(10768.4590, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10768.45703125
tensor(10768.4590, grad_fn=<NegBackward0>) tensor(10768.4570, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10768.4580078125
tensor(10768.4570, grad_fn=<NegBackward0>) tensor(10768.4580, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10768.4580078125
tensor(10768.4570, grad_fn=<NegBackward0>) tensor(10768.4580, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -10768.45703125
tensor(10768.4570, grad_fn=<NegBackward0>) tensor(10768.4570, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10768.45703125
tensor(10768.4570, grad_fn=<NegBackward0>) tensor(10768.4570, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10768.45703125
tensor(10768.4570, grad_fn=<NegBackward0>) tensor(10768.4570, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10768.4560546875
tensor(10768.4570, grad_fn=<NegBackward0>) tensor(10768.4561, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10768.45703125
tensor(10768.4561, grad_fn=<NegBackward0>) tensor(10768.4570, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10768.4560546875
tensor(10768.4561, grad_fn=<NegBackward0>) tensor(10768.4561, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10768.455078125
tensor(10768.4561, grad_fn=<NegBackward0>) tensor(10768.4551, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10768.4560546875
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4561, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10768.4560546875
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4561, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -10768.455078125
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4551, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10768.455078125
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4551, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10768.4560546875
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4561, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10768.455078125
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4551, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10768.4541015625
tensor(10768.4551, grad_fn=<NegBackward0>) tensor(10768.4541, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10768.455078125
tensor(10768.4541, grad_fn=<NegBackward0>) tensor(10768.4551, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10768.4541015625
tensor(10768.4541, grad_fn=<NegBackward0>) tensor(10768.4541, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10768.455078125
tensor(10768.4541, grad_fn=<NegBackward0>) tensor(10768.4551, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10768.453125
tensor(10768.4541, grad_fn=<NegBackward0>) tensor(10768.4531, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10768.4541015625
tensor(10768.4531, grad_fn=<NegBackward0>) tensor(10768.4541, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10768.4541015625
tensor(10768.4531, grad_fn=<NegBackward0>) tensor(10768.4541, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -10768.4541015625
tensor(10768.4531, grad_fn=<NegBackward0>) tensor(10768.4541, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -10768.4541015625
tensor(10768.4531, grad_fn=<NegBackward0>) tensor(10768.4541, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -10768.4541015625
tensor(10768.4531, grad_fn=<NegBackward0>) tensor(10768.4541, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6900 due to no improvement.
pi: tensor([[4.8813e-04, 9.9951e-01],
        [5.6105e-03, 9.9439e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0429, 0.9571], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.5204, 0.2278],
         [0.6781, 0.1557]],

        [[0.5735, 0.2350],
         [0.5079, 0.7169]],

        [[0.7123, 0.1172],
         [0.5796, 0.5246]],

        [[0.5807, 0.1700],
         [0.6980, 0.6428]],

        [[0.5238, 0.1977],
         [0.6788, 0.5913]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0011993956145362663
Average Adjusted Rand Index: -0.0001522251028708703
[0.0011993956145362663, 0.0011993956145362663] [-0.0001522251028708703, -0.0001522251028708703] [10768.5693359375, 10768.4541015625]
-------------------------------------
This iteration is 1
True Objective function: Loss = -10687.744705462863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22733.66015625
inf tensor(22733.6602, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10705.1279296875
tensor(22733.6602, grad_fn=<NegBackward0>) tensor(10705.1279, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10704.5654296875
tensor(10705.1279, grad_fn=<NegBackward0>) tensor(10704.5654, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10704.4248046875
tensor(10704.5654, grad_fn=<NegBackward0>) tensor(10704.4248, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10704.333984375
tensor(10704.4248, grad_fn=<NegBackward0>) tensor(10704.3340, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10704.2412109375
tensor(10704.3340, grad_fn=<NegBackward0>) tensor(10704.2412, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10704.15234375
tensor(10704.2412, grad_fn=<NegBackward0>) tensor(10704.1523, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10704.1123046875
tensor(10704.1523, grad_fn=<NegBackward0>) tensor(10704.1123, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10704.0966796875
tensor(10704.1123, grad_fn=<NegBackward0>) tensor(10704.0967, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10704.08203125
tensor(10704.0967, grad_fn=<NegBackward0>) tensor(10704.0820, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10704.072265625
tensor(10704.0820, grad_fn=<NegBackward0>) tensor(10704.0723, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10704.0634765625
tensor(10704.0723, grad_fn=<NegBackward0>) tensor(10704.0635, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10704.0546875
tensor(10704.0635, grad_fn=<NegBackward0>) tensor(10704.0547, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10704.0478515625
tensor(10704.0547, grad_fn=<NegBackward0>) tensor(10704.0479, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10704.041015625
tensor(10704.0479, grad_fn=<NegBackward0>) tensor(10704.0410, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10704.03515625
tensor(10704.0410, grad_fn=<NegBackward0>) tensor(10704.0352, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10704.029296875
tensor(10704.0352, grad_fn=<NegBackward0>) tensor(10704.0293, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10704.0244140625
tensor(10704.0293, grad_fn=<NegBackward0>) tensor(10704.0244, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10704.0185546875
tensor(10704.0244, grad_fn=<NegBackward0>) tensor(10704.0186, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10704.013671875
tensor(10704.0186, grad_fn=<NegBackward0>) tensor(10704.0137, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10704.0087890625
tensor(10704.0137, grad_fn=<NegBackward0>) tensor(10704.0088, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10704.0048828125
tensor(10704.0088, grad_fn=<NegBackward0>) tensor(10704.0049, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10704.001953125
tensor(10704.0049, grad_fn=<NegBackward0>) tensor(10704.0020, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10703.9970703125
tensor(10704.0020, grad_fn=<NegBackward0>) tensor(10703.9971, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10703.9921875
tensor(10703.9971, grad_fn=<NegBackward0>) tensor(10703.9922, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10703.98828125
tensor(10703.9922, grad_fn=<NegBackward0>) tensor(10703.9883, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10703.986328125
tensor(10703.9883, grad_fn=<NegBackward0>) tensor(10703.9863, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10703.98046875
tensor(10703.9863, grad_fn=<NegBackward0>) tensor(10703.9805, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10703.978515625
tensor(10703.9805, grad_fn=<NegBackward0>) tensor(10703.9785, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10703.9755859375
tensor(10703.9785, grad_fn=<NegBackward0>) tensor(10703.9756, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10703.9716796875
tensor(10703.9756, grad_fn=<NegBackward0>) tensor(10703.9717, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10703.9677734375
tensor(10703.9717, grad_fn=<NegBackward0>) tensor(10703.9678, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10703.96484375
tensor(10703.9678, grad_fn=<NegBackward0>) tensor(10703.9648, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10703.9599609375
tensor(10703.9648, grad_fn=<NegBackward0>) tensor(10703.9600, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10703.9560546875
tensor(10703.9600, grad_fn=<NegBackward0>) tensor(10703.9561, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10703.9541015625
tensor(10703.9561, grad_fn=<NegBackward0>) tensor(10703.9541, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10703.9482421875
tensor(10703.9541, grad_fn=<NegBackward0>) tensor(10703.9482, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10703.943359375
tensor(10703.9482, grad_fn=<NegBackward0>) tensor(10703.9434, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10703.9423828125
tensor(10703.9434, grad_fn=<NegBackward0>) tensor(10703.9424, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10703.935546875
tensor(10703.9424, grad_fn=<NegBackward0>) tensor(10703.9355, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10703.9306640625
tensor(10703.9355, grad_fn=<NegBackward0>) tensor(10703.9307, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10703.9228515625
tensor(10703.9307, grad_fn=<NegBackward0>) tensor(10703.9229, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10703.9150390625
tensor(10703.9229, grad_fn=<NegBackward0>) tensor(10703.9150, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10703.908203125
tensor(10703.9150, grad_fn=<NegBackward0>) tensor(10703.9082, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10703.8984375
tensor(10703.9082, grad_fn=<NegBackward0>) tensor(10703.8984, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10703.88671875
tensor(10703.8984, grad_fn=<NegBackward0>) tensor(10703.8867, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10703.875
tensor(10703.8867, grad_fn=<NegBackward0>) tensor(10703.8750, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10703.865234375
tensor(10703.8750, grad_fn=<NegBackward0>) tensor(10703.8652, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10703.8515625
tensor(10703.8652, grad_fn=<NegBackward0>) tensor(10703.8516, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10703.837890625
tensor(10703.8516, grad_fn=<NegBackward0>) tensor(10703.8379, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10703.8203125
tensor(10703.8379, grad_fn=<NegBackward0>) tensor(10703.8203, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10703.794921875
tensor(10703.8203, grad_fn=<NegBackward0>) tensor(10703.7949, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10703.7509765625
tensor(10703.7949, grad_fn=<NegBackward0>) tensor(10703.7510, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10703.6484375
tensor(10703.7510, grad_fn=<NegBackward0>) tensor(10703.6484, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10703.5068359375
tensor(10703.6484, grad_fn=<NegBackward0>) tensor(10703.5068, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10703.4794921875
tensor(10703.5068, grad_fn=<NegBackward0>) tensor(10703.4795, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10703.466796875
tensor(10703.4795, grad_fn=<NegBackward0>) tensor(10703.4668, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10703.4580078125
tensor(10703.4668, grad_fn=<NegBackward0>) tensor(10703.4580, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10703.4541015625
tensor(10703.4580, grad_fn=<NegBackward0>) tensor(10703.4541, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10703.443359375
tensor(10703.4541, grad_fn=<NegBackward0>) tensor(10703.4434, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10703.4365234375
tensor(10703.4434, grad_fn=<NegBackward0>) tensor(10703.4365, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10703.4306640625
tensor(10703.4365, grad_fn=<NegBackward0>) tensor(10703.4307, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10703.4267578125
tensor(10703.4307, grad_fn=<NegBackward0>) tensor(10703.4268, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10703.421875
tensor(10703.4268, grad_fn=<NegBackward0>) tensor(10703.4219, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10703.4169921875
tensor(10703.4219, grad_fn=<NegBackward0>) tensor(10703.4170, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10703.4130859375
tensor(10703.4170, grad_fn=<NegBackward0>) tensor(10703.4131, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10703.41015625
tensor(10703.4131, grad_fn=<NegBackward0>) tensor(10703.4102, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10703.4052734375
tensor(10703.4102, grad_fn=<NegBackward0>) tensor(10703.4053, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10703.4033203125
tensor(10703.4053, grad_fn=<NegBackward0>) tensor(10703.4033, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10703.3994140625
tensor(10703.4033, grad_fn=<NegBackward0>) tensor(10703.3994, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10703.3974609375
tensor(10703.3994, grad_fn=<NegBackward0>) tensor(10703.3975, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10703.392578125
tensor(10703.3975, grad_fn=<NegBackward0>) tensor(10703.3926, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10703.38671875
tensor(10703.3926, grad_fn=<NegBackward0>) tensor(10703.3867, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10703.2685546875
tensor(10703.3867, grad_fn=<NegBackward0>) tensor(10703.2686, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10702.5078125
tensor(10703.2686, grad_fn=<NegBackward0>) tensor(10702.5078, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10702.4501953125
tensor(10702.5078, grad_fn=<NegBackward0>) tensor(10702.4502, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10702.4296875
tensor(10702.4502, grad_fn=<NegBackward0>) tensor(10702.4297, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10702.421875
tensor(10702.4297, grad_fn=<NegBackward0>) tensor(10702.4219, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10702.4580078125
tensor(10702.4219, grad_fn=<NegBackward0>) tensor(10702.4580, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10702.41015625
tensor(10702.4219, grad_fn=<NegBackward0>) tensor(10702.4102, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10702.40625
tensor(10702.4102, grad_fn=<NegBackward0>) tensor(10702.4062, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10702.404296875
tensor(10702.4062, grad_fn=<NegBackward0>) tensor(10702.4043, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10702.40234375
tensor(10702.4043, grad_fn=<NegBackward0>) tensor(10702.4023, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10702.4990234375
tensor(10702.4023, grad_fn=<NegBackward0>) tensor(10702.4990, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10702.4013671875
tensor(10702.4023, grad_fn=<NegBackward0>) tensor(10702.4014, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10702.3984375
tensor(10702.4014, grad_fn=<NegBackward0>) tensor(10702.3984, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10702.3994140625
tensor(10702.3984, grad_fn=<NegBackward0>) tensor(10702.3994, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10702.3984375
tensor(10702.3984, grad_fn=<NegBackward0>) tensor(10702.3984, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10702.3974609375
tensor(10702.3984, grad_fn=<NegBackward0>) tensor(10702.3975, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10702.3974609375
tensor(10702.3975, grad_fn=<NegBackward0>) tensor(10702.3975, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10702.4072265625
tensor(10702.3975, grad_fn=<NegBackward0>) tensor(10702.4072, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10702.396484375
tensor(10702.3975, grad_fn=<NegBackward0>) tensor(10702.3965, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10702.3955078125
tensor(10702.3965, grad_fn=<NegBackward0>) tensor(10702.3955, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10702.396484375
tensor(10702.3955, grad_fn=<NegBackward0>) tensor(10702.3965, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -10702.39453125
tensor(10702.3955, grad_fn=<NegBackward0>) tensor(10702.3945, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10702.3935546875
tensor(10702.3945, grad_fn=<NegBackward0>) tensor(10702.3936, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10702.396484375
tensor(10702.3936, grad_fn=<NegBackward0>) tensor(10702.3965, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -10702.3935546875
tensor(10702.3936, grad_fn=<NegBackward0>) tensor(10702.3936, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10702.39453125
tensor(10702.3936, grad_fn=<NegBackward0>) tensor(10702.3945, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -10702.396484375
tensor(10702.3936, grad_fn=<NegBackward0>) tensor(10702.3965, grad_fn=<NegBackward0>)
2
pi: tensor([[9.9999e-01, 1.2656e-05],
        [1.8818e-04, 9.9981e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9540, 0.0460], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1532, 0.1863],
         [0.5905, 0.1587]],

        [[0.6300, 0.2095],
         [0.6611, 0.6716]],

        [[0.6180, 0.1805],
         [0.6022, 0.5679]],

        [[0.7146, 0.2074],
         [0.6973, 0.5943]],

        [[0.6766, 0.1268],
         [0.6594, 0.6095]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: -0.003937327268695544
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: -0.004294123202807246
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.002056465888134684
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0031985518644306096
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 63
Adjusted Rand Index: 0.016452174998197153
Global Adjusted Rand Index: 0.003079007735216603
Average Adjusted Rand Index: 0.0026951484558519314
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21741.626953125
inf tensor(21741.6270, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10704.21484375
tensor(21741.6270, grad_fn=<NegBackward0>) tensor(10704.2148, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10703.955078125
tensor(10704.2148, grad_fn=<NegBackward0>) tensor(10703.9551, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10703.9384765625
tensor(10703.9551, grad_fn=<NegBackward0>) tensor(10703.9385, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10703.92578125
tensor(10703.9385, grad_fn=<NegBackward0>) tensor(10703.9258, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10703.9150390625
tensor(10703.9258, grad_fn=<NegBackward0>) tensor(10703.9150, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10703.9052734375
tensor(10703.9150, grad_fn=<NegBackward0>) tensor(10703.9053, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10703.8984375
tensor(10703.9053, grad_fn=<NegBackward0>) tensor(10703.8984, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10703.890625
tensor(10703.8984, grad_fn=<NegBackward0>) tensor(10703.8906, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10703.8857421875
tensor(10703.8906, grad_fn=<NegBackward0>) tensor(10703.8857, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10703.880859375
tensor(10703.8857, grad_fn=<NegBackward0>) tensor(10703.8809, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10703.8759765625
tensor(10703.8809, grad_fn=<NegBackward0>) tensor(10703.8760, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10703.869140625
tensor(10703.8760, grad_fn=<NegBackward0>) tensor(10703.8691, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10703.8642578125
tensor(10703.8691, grad_fn=<NegBackward0>) tensor(10703.8643, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10703.857421875
tensor(10703.8643, grad_fn=<NegBackward0>) tensor(10703.8574, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10703.8505859375
tensor(10703.8574, grad_fn=<NegBackward0>) tensor(10703.8506, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10703.8408203125
tensor(10703.8506, grad_fn=<NegBackward0>) tensor(10703.8408, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10703.83203125
tensor(10703.8408, grad_fn=<NegBackward0>) tensor(10703.8320, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10703.8193359375
tensor(10703.8320, grad_fn=<NegBackward0>) tensor(10703.8193, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10703.8046875
tensor(10703.8193, grad_fn=<NegBackward0>) tensor(10703.8047, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10703.78515625
tensor(10703.8047, grad_fn=<NegBackward0>) tensor(10703.7852, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10703.7548828125
tensor(10703.7852, grad_fn=<NegBackward0>) tensor(10703.7549, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10703.6982421875
tensor(10703.7549, grad_fn=<NegBackward0>) tensor(10703.6982, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10703.5693359375
tensor(10703.6982, grad_fn=<NegBackward0>) tensor(10703.5693, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10703.515625
tensor(10703.5693, grad_fn=<NegBackward0>) tensor(10703.5156, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10703.4970703125
tensor(10703.5156, grad_fn=<NegBackward0>) tensor(10703.4971, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10703.4814453125
tensor(10703.4971, grad_fn=<NegBackward0>) tensor(10703.4814, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10703.46875
tensor(10703.4814, grad_fn=<NegBackward0>) tensor(10703.4688, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10703.4580078125
tensor(10703.4688, grad_fn=<NegBackward0>) tensor(10703.4580, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10703.4482421875
tensor(10703.4580, grad_fn=<NegBackward0>) tensor(10703.4482, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10703.44140625
tensor(10703.4482, grad_fn=<NegBackward0>) tensor(10703.4414, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10703.4384765625
tensor(10703.4414, grad_fn=<NegBackward0>) tensor(10703.4385, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10703.427734375
tensor(10703.4385, grad_fn=<NegBackward0>) tensor(10703.4277, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10703.423828125
tensor(10703.4277, grad_fn=<NegBackward0>) tensor(10703.4238, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10703.4169921875
tensor(10703.4238, grad_fn=<NegBackward0>) tensor(10703.4170, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10703.412109375
tensor(10703.4170, grad_fn=<NegBackward0>) tensor(10703.4121, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10703.4091796875
tensor(10703.4121, grad_fn=<NegBackward0>) tensor(10703.4092, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10703.40625
tensor(10703.4092, grad_fn=<NegBackward0>) tensor(10703.4062, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10703.40234375
tensor(10703.4062, grad_fn=<NegBackward0>) tensor(10703.4023, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10703.3984375
tensor(10703.4023, grad_fn=<NegBackward0>) tensor(10703.3984, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10703.3984375
tensor(10703.3984, grad_fn=<NegBackward0>) tensor(10703.3984, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10703.3896484375
tensor(10703.3984, grad_fn=<NegBackward0>) tensor(10703.3896, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10703.3818359375
tensor(10703.3896, grad_fn=<NegBackward0>) tensor(10703.3818, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10702.9560546875
tensor(10703.3818, grad_fn=<NegBackward0>) tensor(10702.9561, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10702.4765625
tensor(10702.9561, grad_fn=<NegBackward0>) tensor(10702.4766, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10702.4365234375
tensor(10702.4766, grad_fn=<NegBackward0>) tensor(10702.4365, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10702.4228515625
tensor(10702.4365, grad_fn=<NegBackward0>) tensor(10702.4229, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10702.4150390625
tensor(10702.4229, grad_fn=<NegBackward0>) tensor(10702.4150, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10702.41015625
tensor(10702.4150, grad_fn=<NegBackward0>) tensor(10702.4102, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10702.40625
tensor(10702.4102, grad_fn=<NegBackward0>) tensor(10702.4062, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10702.4052734375
tensor(10702.4062, grad_fn=<NegBackward0>) tensor(10702.4053, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10702.4033203125
tensor(10702.4053, grad_fn=<NegBackward0>) tensor(10702.4033, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10702.400390625
tensor(10702.4033, grad_fn=<NegBackward0>) tensor(10702.4004, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10702.3994140625
tensor(10702.4004, grad_fn=<NegBackward0>) tensor(10702.3994, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10702.3984375
tensor(10702.3994, grad_fn=<NegBackward0>) tensor(10702.3984, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10702.3974609375
tensor(10702.3984, grad_fn=<NegBackward0>) tensor(10702.3975, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10702.3974609375
tensor(10702.3975, grad_fn=<NegBackward0>) tensor(10702.3975, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10702.3974609375
tensor(10702.3975, grad_fn=<NegBackward0>) tensor(10702.3975, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10702.3955078125
tensor(10702.3975, grad_fn=<NegBackward0>) tensor(10702.3955, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10702.3974609375
tensor(10702.3955, grad_fn=<NegBackward0>) tensor(10702.3975, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10702.3955078125
tensor(10702.3955, grad_fn=<NegBackward0>) tensor(10702.3955, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10702.3935546875
tensor(10702.3955, grad_fn=<NegBackward0>) tensor(10702.3936, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10702.39453125
tensor(10702.3936, grad_fn=<NegBackward0>) tensor(10702.3945, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10702.39453125
tensor(10702.3936, grad_fn=<NegBackward0>) tensor(10702.3945, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -10702.3935546875
tensor(10702.3936, grad_fn=<NegBackward0>) tensor(10702.3936, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10702.39453125
tensor(10702.3936, grad_fn=<NegBackward0>) tensor(10702.3945, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10702.3935546875
tensor(10702.3936, grad_fn=<NegBackward0>) tensor(10702.3936, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10702.392578125
tensor(10702.3936, grad_fn=<NegBackward0>) tensor(10702.3926, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10702.3935546875
tensor(10702.3926, grad_fn=<NegBackward0>) tensor(10702.3936, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10702.392578125
tensor(10702.3926, grad_fn=<NegBackward0>) tensor(10702.3926, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10702.392578125
tensor(10702.3926, grad_fn=<NegBackward0>) tensor(10702.3926, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10702.3994140625
tensor(10702.3926, grad_fn=<NegBackward0>) tensor(10702.3994, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10702.4013671875
tensor(10702.3926, grad_fn=<NegBackward0>) tensor(10702.4014, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -10702.4013671875
tensor(10702.3926, grad_fn=<NegBackward0>) tensor(10702.4014, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -10702.3916015625
tensor(10702.3926, grad_fn=<NegBackward0>) tensor(10702.3916, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10702.3935546875
tensor(10702.3916, grad_fn=<NegBackward0>) tensor(10702.3936, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10702.400390625
tensor(10702.3916, grad_fn=<NegBackward0>) tensor(10702.4004, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -10702.3916015625
tensor(10702.3916, grad_fn=<NegBackward0>) tensor(10702.3916, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10702.392578125
tensor(10702.3916, grad_fn=<NegBackward0>) tensor(10702.3926, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10702.3916015625
tensor(10702.3916, grad_fn=<NegBackward0>) tensor(10702.3916, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10702.455078125
tensor(10702.3916, grad_fn=<NegBackward0>) tensor(10702.4551, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10702.3916015625
tensor(10702.3916, grad_fn=<NegBackward0>) tensor(10702.3916, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10702.3916015625
tensor(10702.3916, grad_fn=<NegBackward0>) tensor(10702.3916, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10702.4013671875
tensor(10702.3916, grad_fn=<NegBackward0>) tensor(10702.4014, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10702.390625
tensor(10702.3916, grad_fn=<NegBackward0>) tensor(10702.3906, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10702.390625
tensor(10702.3906, grad_fn=<NegBackward0>) tensor(10702.3906, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10702.4130859375
tensor(10702.3906, grad_fn=<NegBackward0>) tensor(10702.4131, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10702.390625
tensor(10702.3906, grad_fn=<NegBackward0>) tensor(10702.3906, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10702.390625
tensor(10702.3906, grad_fn=<NegBackward0>) tensor(10702.3906, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10702.3916015625
tensor(10702.3906, grad_fn=<NegBackward0>) tensor(10702.3916, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -10702.390625
tensor(10702.3906, grad_fn=<NegBackward0>) tensor(10702.3906, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10702.4052734375
tensor(10702.3906, grad_fn=<NegBackward0>) tensor(10702.4053, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10702.390625
tensor(10702.3906, grad_fn=<NegBackward0>) tensor(10702.3906, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10702.390625
tensor(10702.3906, grad_fn=<NegBackward0>) tensor(10702.3906, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10702.392578125
tensor(10702.3906, grad_fn=<NegBackward0>) tensor(10702.3926, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -10702.3916015625
tensor(10702.3906, grad_fn=<NegBackward0>) tensor(10702.3916, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -10702.7158203125
tensor(10702.3906, grad_fn=<NegBackward0>) tensor(10702.7158, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -10702.390625
tensor(10702.3906, grad_fn=<NegBackward0>) tensor(10702.3906, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10702.3916015625
tensor(10702.3906, grad_fn=<NegBackward0>) tensor(10702.3916, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -10702.3916015625
tensor(10702.3906, grad_fn=<NegBackward0>) tensor(10702.3916, grad_fn=<NegBackward0>)
2
pi: tensor([[9.9999e-01, 7.2798e-06],
        [3.1154e-05, 9.9997e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9540, 0.0460], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1531, 0.1863],
         [0.6939, 0.1587]],

        [[0.7300, 0.2095],
         [0.6462, 0.6572]],

        [[0.6153, 0.1805],
         [0.5228, 0.5045]],

        [[0.5351, 0.2074],
         [0.5288, 0.5060]],

        [[0.5526, 0.1268],
         [0.5392, 0.5830]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: -0.003937327268695544
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: -0.004294123202807246
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.002056465888134684
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0031985518644306096
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 63
Adjusted Rand Index: 0.016452174998197153
Global Adjusted Rand Index: 0.003079007735216603
Average Adjusted Rand Index: 0.0026951484558519314
[0.003079007735216603, 0.003079007735216603] [0.0026951484558519314, 0.0026951484558519314] [10702.3916015625, 10702.3916015625]
-------------------------------------
This iteration is 2
True Objective function: Loss = -10968.114010278518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20809.755859375
inf tensor(20809.7559, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11056.5087890625
tensor(20809.7559, grad_fn=<NegBackward0>) tensor(11056.5088, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11056.1142578125
tensor(11056.5088, grad_fn=<NegBackward0>) tensor(11056.1143, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11056.001953125
tensor(11056.1143, grad_fn=<NegBackward0>) tensor(11056.0020, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11055.9052734375
tensor(11056.0020, grad_fn=<NegBackward0>) tensor(11055.9053, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11055.7890625
tensor(11055.9053, grad_fn=<NegBackward0>) tensor(11055.7891, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11055.685546875
tensor(11055.7891, grad_fn=<NegBackward0>) tensor(11055.6855, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11055.63671875
tensor(11055.6855, grad_fn=<NegBackward0>) tensor(11055.6367, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11055.580078125
tensor(11055.6367, grad_fn=<NegBackward0>) tensor(11055.5801, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11055.4765625
tensor(11055.5801, grad_fn=<NegBackward0>) tensor(11055.4766, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11055.21484375
tensor(11055.4766, grad_fn=<NegBackward0>) tensor(11055.2148, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11053.537109375
tensor(11055.2148, grad_fn=<NegBackward0>) tensor(11053.5371, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11052.4951171875
tensor(11053.5371, grad_fn=<NegBackward0>) tensor(11052.4951, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11052.35546875
tensor(11052.4951, grad_fn=<NegBackward0>) tensor(11052.3555, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11052.2939453125
tensor(11052.3555, grad_fn=<NegBackward0>) tensor(11052.2939, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11052.2607421875
tensor(11052.2939, grad_fn=<NegBackward0>) tensor(11052.2607, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11052.240234375
tensor(11052.2607, grad_fn=<NegBackward0>) tensor(11052.2402, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11052.2255859375
tensor(11052.2402, grad_fn=<NegBackward0>) tensor(11052.2256, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11052.21875
tensor(11052.2256, grad_fn=<NegBackward0>) tensor(11052.2188, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11052.2109375
tensor(11052.2188, grad_fn=<NegBackward0>) tensor(11052.2109, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11052.2060546875
tensor(11052.2109, grad_fn=<NegBackward0>) tensor(11052.2061, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11052.201171875
tensor(11052.2061, grad_fn=<NegBackward0>) tensor(11052.2012, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11052.197265625
tensor(11052.2012, grad_fn=<NegBackward0>) tensor(11052.1973, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11052.1943359375
tensor(11052.1973, grad_fn=<NegBackward0>) tensor(11052.1943, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11052.189453125
tensor(11052.1943, grad_fn=<NegBackward0>) tensor(11052.1895, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11052.1884765625
tensor(11052.1895, grad_fn=<NegBackward0>) tensor(11052.1885, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11052.1865234375
tensor(11052.1885, grad_fn=<NegBackward0>) tensor(11052.1865, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11052.1845703125
tensor(11052.1865, grad_fn=<NegBackward0>) tensor(11052.1846, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11052.1826171875
tensor(11052.1846, grad_fn=<NegBackward0>) tensor(11052.1826, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11052.1796875
tensor(11052.1826, grad_fn=<NegBackward0>) tensor(11052.1797, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11052.1787109375
tensor(11052.1797, grad_fn=<NegBackward0>) tensor(11052.1787, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11052.177734375
tensor(11052.1787, grad_fn=<NegBackward0>) tensor(11052.1777, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11052.17578125
tensor(11052.1777, grad_fn=<NegBackward0>) tensor(11052.1758, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11052.17578125
tensor(11052.1758, grad_fn=<NegBackward0>) tensor(11052.1758, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11052.1748046875
tensor(11052.1758, grad_fn=<NegBackward0>) tensor(11052.1748, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11052.173828125
tensor(11052.1748, grad_fn=<NegBackward0>) tensor(11052.1738, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11052.1728515625
tensor(11052.1738, grad_fn=<NegBackward0>) tensor(11052.1729, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11052.169921875
tensor(11052.1729, grad_fn=<NegBackward0>) tensor(11052.1699, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11052.1708984375
tensor(11052.1699, grad_fn=<NegBackward0>) tensor(11052.1709, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11052.16796875
tensor(11052.1699, grad_fn=<NegBackward0>) tensor(11052.1680, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11052.1669921875
tensor(11052.1680, grad_fn=<NegBackward0>) tensor(11052.1670, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11052.16796875
tensor(11052.1670, grad_fn=<NegBackward0>) tensor(11052.1680, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11052.166015625
tensor(11052.1670, grad_fn=<NegBackward0>) tensor(11052.1660, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11052.1650390625
tensor(11052.1660, grad_fn=<NegBackward0>) tensor(11052.1650, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11052.1650390625
tensor(11052.1650, grad_fn=<NegBackward0>) tensor(11052.1650, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11052.1650390625
tensor(11052.1650, grad_fn=<NegBackward0>) tensor(11052.1650, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11052.1650390625
tensor(11052.1650, grad_fn=<NegBackward0>) tensor(11052.1650, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11052.1630859375
tensor(11052.1650, grad_fn=<NegBackward0>) tensor(11052.1631, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11052.1630859375
tensor(11052.1631, grad_fn=<NegBackward0>) tensor(11052.1631, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11052.1640625
tensor(11052.1631, grad_fn=<NegBackward0>) tensor(11052.1641, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11052.1630859375
tensor(11052.1631, grad_fn=<NegBackward0>) tensor(11052.1631, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11052.162109375
tensor(11052.1631, grad_fn=<NegBackward0>) tensor(11052.1621, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11052.1611328125
tensor(11052.1621, grad_fn=<NegBackward0>) tensor(11052.1611, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11052.1611328125
tensor(11052.1611, grad_fn=<NegBackward0>) tensor(11052.1611, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11052.162109375
tensor(11052.1611, grad_fn=<NegBackward0>) tensor(11052.1621, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11052.162109375
tensor(11052.1611, grad_fn=<NegBackward0>) tensor(11052.1621, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11052.1591796875
tensor(11052.1611, grad_fn=<NegBackward0>) tensor(11052.1592, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11052.16015625
tensor(11052.1592, grad_fn=<NegBackward0>) tensor(11052.1602, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11052.16015625
tensor(11052.1592, grad_fn=<NegBackward0>) tensor(11052.1602, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11052.1611328125
tensor(11052.1592, grad_fn=<NegBackward0>) tensor(11052.1611, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -11052.1591796875
tensor(11052.1592, grad_fn=<NegBackward0>) tensor(11052.1592, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11052.1591796875
tensor(11052.1592, grad_fn=<NegBackward0>) tensor(11052.1592, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11052.16015625
tensor(11052.1592, grad_fn=<NegBackward0>) tensor(11052.1602, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11052.1591796875
tensor(11052.1592, grad_fn=<NegBackward0>) tensor(11052.1592, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11052.158203125
tensor(11052.1592, grad_fn=<NegBackward0>) tensor(11052.1582, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11052.158203125
tensor(11052.1582, grad_fn=<NegBackward0>) tensor(11052.1582, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11052.1591796875
tensor(11052.1582, grad_fn=<NegBackward0>) tensor(11052.1592, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11052.158203125
tensor(11052.1582, grad_fn=<NegBackward0>) tensor(11052.1582, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11052.1591796875
tensor(11052.1582, grad_fn=<NegBackward0>) tensor(11052.1592, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11052.158203125
tensor(11052.1582, grad_fn=<NegBackward0>) tensor(11052.1582, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11052.1572265625
tensor(11052.1582, grad_fn=<NegBackward0>) tensor(11052.1572, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11052.158203125
tensor(11052.1572, grad_fn=<NegBackward0>) tensor(11052.1582, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11052.1572265625
tensor(11052.1572, grad_fn=<NegBackward0>) tensor(11052.1572, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11052.17578125
tensor(11052.1572, grad_fn=<NegBackward0>) tensor(11052.1758, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11052.3173828125
tensor(11052.1572, grad_fn=<NegBackward0>) tensor(11052.3174, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11052.158203125
tensor(11052.1572, grad_fn=<NegBackward0>) tensor(11052.1582, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11052.158203125
tensor(11052.1572, grad_fn=<NegBackward0>) tensor(11052.1582, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -11052.1591796875
tensor(11052.1572, grad_fn=<NegBackward0>) tensor(11052.1592, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[9.6157e-01, 3.8434e-02],
        [9.9940e-01, 5.9843e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9555, 0.0445], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1645, 0.1979],
         [0.6998, 0.2513]],

        [[0.6990, 0.1221],
         [0.5335, 0.5857]],

        [[0.7254, 0.1974],
         [0.5489, 0.5485]],

        [[0.6038, 0.2081],
         [0.5116, 0.6357]],

        [[0.6892, 0.0726],
         [0.5848, 0.5337]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 39
Adjusted Rand Index: 0.02523388575915466
Global Adjusted Rand Index: 0.0009924746682510983
Average Adjusted Rand Index: 0.005046777151830932
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20379.25
inf tensor(20379.2500, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11056.2138671875
tensor(20379.2500, grad_fn=<NegBackward0>) tensor(11056.2139, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11055.9111328125
tensor(11056.2139, grad_fn=<NegBackward0>) tensor(11055.9111, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11055.8076171875
tensor(11055.9111, grad_fn=<NegBackward0>) tensor(11055.8076, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11055.6884765625
tensor(11055.8076, grad_fn=<NegBackward0>) tensor(11055.6885, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11055.5244140625
tensor(11055.6885, grad_fn=<NegBackward0>) tensor(11055.5244, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11055.33203125
tensor(11055.5244, grad_fn=<NegBackward0>) tensor(11055.3320, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11055.0810546875
tensor(11055.3320, grad_fn=<NegBackward0>) tensor(11055.0811, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11053.5537109375
tensor(11055.0811, grad_fn=<NegBackward0>) tensor(11053.5537, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11052.498046875
tensor(11053.5537, grad_fn=<NegBackward0>) tensor(11052.4980, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11052.3525390625
tensor(11052.4980, grad_fn=<NegBackward0>) tensor(11052.3525, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11052.296875
tensor(11052.3525, grad_fn=<NegBackward0>) tensor(11052.2969, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11052.2646484375
tensor(11052.2969, grad_fn=<NegBackward0>) tensor(11052.2646, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11052.24609375
tensor(11052.2646, grad_fn=<NegBackward0>) tensor(11052.2461, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11052.2314453125
tensor(11052.2461, grad_fn=<NegBackward0>) tensor(11052.2314, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11052.22265625
tensor(11052.2314, grad_fn=<NegBackward0>) tensor(11052.2227, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11052.21484375
tensor(11052.2227, grad_fn=<NegBackward0>) tensor(11052.2148, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11052.208984375
tensor(11052.2148, grad_fn=<NegBackward0>) tensor(11052.2090, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11052.2041015625
tensor(11052.2090, grad_fn=<NegBackward0>) tensor(11052.2041, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11052.2001953125
tensor(11052.2041, grad_fn=<NegBackward0>) tensor(11052.2002, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11052.1962890625
tensor(11052.2002, grad_fn=<NegBackward0>) tensor(11052.1963, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11052.1943359375
tensor(11052.1963, grad_fn=<NegBackward0>) tensor(11052.1943, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11052.19140625
tensor(11052.1943, grad_fn=<NegBackward0>) tensor(11052.1914, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11052.189453125
tensor(11052.1914, grad_fn=<NegBackward0>) tensor(11052.1895, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11052.185546875
tensor(11052.1895, grad_fn=<NegBackward0>) tensor(11052.1855, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11052.1845703125
tensor(11052.1855, grad_fn=<NegBackward0>) tensor(11052.1846, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11052.1826171875
tensor(11052.1846, grad_fn=<NegBackward0>) tensor(11052.1826, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11052.1787109375
tensor(11052.1826, grad_fn=<NegBackward0>) tensor(11052.1787, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11052.1796875
tensor(11052.1787, grad_fn=<NegBackward0>) tensor(11052.1797, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11052.1787109375
tensor(11052.1787, grad_fn=<NegBackward0>) tensor(11052.1787, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11052.1796875
tensor(11052.1787, grad_fn=<NegBackward0>) tensor(11052.1797, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11052.17578125
tensor(11052.1787, grad_fn=<NegBackward0>) tensor(11052.1758, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11052.1748046875
tensor(11052.1758, grad_fn=<NegBackward0>) tensor(11052.1748, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11052.173828125
tensor(11052.1748, grad_fn=<NegBackward0>) tensor(11052.1738, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11052.173828125
tensor(11052.1738, grad_fn=<NegBackward0>) tensor(11052.1738, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11052.171875
tensor(11052.1738, grad_fn=<NegBackward0>) tensor(11052.1719, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11052.169921875
tensor(11052.1719, grad_fn=<NegBackward0>) tensor(11052.1699, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11052.169921875
tensor(11052.1699, grad_fn=<NegBackward0>) tensor(11052.1699, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11052.1689453125
tensor(11052.1699, grad_fn=<NegBackward0>) tensor(11052.1689, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11052.1689453125
tensor(11052.1689, grad_fn=<NegBackward0>) tensor(11052.1689, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11052.16796875
tensor(11052.1689, grad_fn=<NegBackward0>) tensor(11052.1680, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11052.1669921875
tensor(11052.1680, grad_fn=<NegBackward0>) tensor(11052.1670, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11052.166015625
tensor(11052.1670, grad_fn=<NegBackward0>) tensor(11052.1660, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11052.166015625
tensor(11052.1660, grad_fn=<NegBackward0>) tensor(11052.1660, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11052.1640625
tensor(11052.1660, grad_fn=<NegBackward0>) tensor(11052.1641, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11052.1640625
tensor(11052.1641, grad_fn=<NegBackward0>) tensor(11052.1641, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11052.1650390625
tensor(11052.1641, grad_fn=<NegBackward0>) tensor(11052.1650, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11052.1630859375
tensor(11052.1641, grad_fn=<NegBackward0>) tensor(11052.1631, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11052.162109375
tensor(11052.1631, grad_fn=<NegBackward0>) tensor(11052.1621, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11052.1611328125
tensor(11052.1621, grad_fn=<NegBackward0>) tensor(11052.1611, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11052.162109375
tensor(11052.1611, grad_fn=<NegBackward0>) tensor(11052.1621, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11052.1630859375
tensor(11052.1611, grad_fn=<NegBackward0>) tensor(11052.1631, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11052.16015625
tensor(11052.1611, grad_fn=<NegBackward0>) tensor(11052.1602, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11052.1611328125
tensor(11052.1602, grad_fn=<NegBackward0>) tensor(11052.1611, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11052.162109375
tensor(11052.1602, grad_fn=<NegBackward0>) tensor(11052.1621, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11052.16015625
tensor(11052.1602, grad_fn=<NegBackward0>) tensor(11052.1602, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11052.16015625
tensor(11052.1602, grad_fn=<NegBackward0>) tensor(11052.1602, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11052.16015625
tensor(11052.1602, grad_fn=<NegBackward0>) tensor(11052.1602, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11052.1611328125
tensor(11052.1602, grad_fn=<NegBackward0>) tensor(11052.1611, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11052.16015625
tensor(11052.1602, grad_fn=<NegBackward0>) tensor(11052.1602, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11052.1611328125
tensor(11052.1602, grad_fn=<NegBackward0>) tensor(11052.1611, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11052.1591796875
tensor(11052.1602, grad_fn=<NegBackward0>) tensor(11052.1592, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11052.158203125
tensor(11052.1592, grad_fn=<NegBackward0>) tensor(11052.1582, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11052.16015625
tensor(11052.1582, grad_fn=<NegBackward0>) tensor(11052.1602, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11052.1591796875
tensor(11052.1582, grad_fn=<NegBackward0>) tensor(11052.1592, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11052.1591796875
tensor(11052.1582, grad_fn=<NegBackward0>) tensor(11052.1592, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11052.158203125
tensor(11052.1582, grad_fn=<NegBackward0>) tensor(11052.1582, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11052.158203125
tensor(11052.1582, grad_fn=<NegBackward0>) tensor(11052.1582, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11052.1591796875
tensor(11052.1582, grad_fn=<NegBackward0>) tensor(11052.1592, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11052.158203125
tensor(11052.1582, grad_fn=<NegBackward0>) tensor(11052.1582, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11052.158203125
tensor(11052.1582, grad_fn=<NegBackward0>) tensor(11052.1582, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11052.158203125
tensor(11052.1582, grad_fn=<NegBackward0>) tensor(11052.1582, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11052.158203125
tensor(11052.1582, grad_fn=<NegBackward0>) tensor(11052.1582, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11052.158203125
tensor(11052.1582, grad_fn=<NegBackward0>) tensor(11052.1582, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11052.158203125
tensor(11052.1582, grad_fn=<NegBackward0>) tensor(11052.1582, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11052.1572265625
tensor(11052.1582, grad_fn=<NegBackward0>) tensor(11052.1572, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11052.244140625
tensor(11052.1572, grad_fn=<NegBackward0>) tensor(11052.2441, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11052.1572265625
tensor(11052.1572, grad_fn=<NegBackward0>) tensor(11052.1572, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11052.2333984375
tensor(11052.1572, grad_fn=<NegBackward0>) tensor(11052.2334, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11052.1572265625
tensor(11052.1572, grad_fn=<NegBackward0>) tensor(11052.1572, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11052.1630859375
tensor(11052.1572, grad_fn=<NegBackward0>) tensor(11052.1631, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11052.1572265625
tensor(11052.1572, grad_fn=<NegBackward0>) tensor(11052.1572, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11052.158203125
tensor(11052.1572, grad_fn=<NegBackward0>) tensor(11052.1582, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11052.158203125
tensor(11052.1572, grad_fn=<NegBackward0>) tensor(11052.1582, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11052.1572265625
tensor(11052.1572, grad_fn=<NegBackward0>) tensor(11052.1572, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11052.158203125
tensor(11052.1572, grad_fn=<NegBackward0>) tensor(11052.1582, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11052.1572265625
tensor(11052.1572, grad_fn=<NegBackward0>) tensor(11052.1572, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11052.158203125
tensor(11052.1572, grad_fn=<NegBackward0>) tensor(11052.1582, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11052.1572265625
tensor(11052.1572, grad_fn=<NegBackward0>) tensor(11052.1572, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11052.15625
tensor(11052.1572, grad_fn=<NegBackward0>) tensor(11052.1562, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11052.1572265625
tensor(11052.1562, grad_fn=<NegBackward0>) tensor(11052.1572, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11052.1572265625
tensor(11052.1562, grad_fn=<NegBackward0>) tensor(11052.1572, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11052.1904296875
tensor(11052.1562, grad_fn=<NegBackward0>) tensor(11052.1904, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -11052.162109375
tensor(11052.1562, grad_fn=<NegBackward0>) tensor(11052.1621, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -11052.1572265625
tensor(11052.1562, grad_fn=<NegBackward0>) tensor(11052.1572, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9400 due to no improvement.
pi: tensor([[2.4055e-04, 9.9976e-01],
        [3.8772e-02, 9.6123e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0445, 0.9555], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2514, 0.1979],
         [0.6915, 0.1645]],

        [[0.5613, 0.1220],
         [0.6968, 0.6543]],

        [[0.5500, 0.1975],
         [0.6514, 0.7155]],

        [[0.7079, 0.2081],
         [0.6928, 0.7005]],

        [[0.6629, 0.0726],
         [0.5169, 0.5781]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 61
Adjusted Rand Index: 0.02523388575915466
Global Adjusted Rand Index: 0.0009924746682510983
Average Adjusted Rand Index: 0.005046777151830932
[0.0009924746682510983, 0.0009924746682510983] [0.005046777151830932, 0.005046777151830932] [11052.1591796875, 11052.1572265625]
-------------------------------------
This iteration is 3
True Objective function: Loss = -10773.008623623846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21850.419921875
inf tensor(21850.4199, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10845.6953125
tensor(21850.4199, grad_fn=<NegBackward0>) tensor(10845.6953, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10845.384765625
tensor(10845.6953, grad_fn=<NegBackward0>) tensor(10845.3848, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10845.3251953125
tensor(10845.3848, grad_fn=<NegBackward0>) tensor(10845.3252, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10845.2880859375
tensor(10845.3252, grad_fn=<NegBackward0>) tensor(10845.2881, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10845.25390625
tensor(10845.2881, grad_fn=<NegBackward0>) tensor(10845.2539, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10845.2177734375
tensor(10845.2539, grad_fn=<NegBackward0>) tensor(10845.2178, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10845.189453125
tensor(10845.2178, grad_fn=<NegBackward0>) tensor(10845.1895, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10845.1650390625
tensor(10845.1895, grad_fn=<NegBackward0>) tensor(10845.1650, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10845.1484375
tensor(10845.1650, grad_fn=<NegBackward0>) tensor(10845.1484, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10845.1318359375
tensor(10845.1484, grad_fn=<NegBackward0>) tensor(10845.1318, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10845.1142578125
tensor(10845.1318, grad_fn=<NegBackward0>) tensor(10845.1143, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10845.09375
tensor(10845.1143, grad_fn=<NegBackward0>) tensor(10845.0938, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10845.0732421875
tensor(10845.0938, grad_fn=<NegBackward0>) tensor(10845.0732, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10845.0576171875
tensor(10845.0732, grad_fn=<NegBackward0>) tensor(10845.0576, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10845.0458984375
tensor(10845.0576, grad_fn=<NegBackward0>) tensor(10845.0459, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10845.0361328125
tensor(10845.0459, grad_fn=<NegBackward0>) tensor(10845.0361, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10845.0263671875
tensor(10845.0361, grad_fn=<NegBackward0>) tensor(10845.0264, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10845.0205078125
tensor(10845.0264, grad_fn=<NegBackward0>) tensor(10845.0205, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10845.0078125
tensor(10845.0205, grad_fn=<NegBackward0>) tensor(10845.0078, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10844.998046875
tensor(10845.0078, grad_fn=<NegBackward0>) tensor(10844.9980, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10844.9775390625
tensor(10844.9980, grad_fn=<NegBackward0>) tensor(10844.9775, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10844.931640625
tensor(10844.9775, grad_fn=<NegBackward0>) tensor(10844.9316, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10843.9833984375
tensor(10844.9316, grad_fn=<NegBackward0>) tensor(10843.9834, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10842.6552734375
tensor(10843.9834, grad_fn=<NegBackward0>) tensor(10842.6553, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10842.353515625
tensor(10842.6553, grad_fn=<NegBackward0>) tensor(10842.3535, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10842.25
tensor(10842.3535, grad_fn=<NegBackward0>) tensor(10842.2500, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10842.2001953125
tensor(10842.2500, grad_fn=<NegBackward0>) tensor(10842.2002, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10842.171875
tensor(10842.2002, grad_fn=<NegBackward0>) tensor(10842.1719, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10842.1533203125
tensor(10842.1719, grad_fn=<NegBackward0>) tensor(10842.1533, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10842.140625
tensor(10842.1533, grad_fn=<NegBackward0>) tensor(10842.1406, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10842.130859375
tensor(10842.1406, grad_fn=<NegBackward0>) tensor(10842.1309, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10842.123046875
tensor(10842.1309, grad_fn=<NegBackward0>) tensor(10842.1230, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10842.1181640625
tensor(10842.1230, grad_fn=<NegBackward0>) tensor(10842.1182, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10842.11328125
tensor(10842.1182, grad_fn=<NegBackward0>) tensor(10842.1133, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10842.111328125
tensor(10842.1133, grad_fn=<NegBackward0>) tensor(10842.1113, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10842.1064453125
tensor(10842.1113, grad_fn=<NegBackward0>) tensor(10842.1064, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10842.103515625
tensor(10842.1064, grad_fn=<NegBackward0>) tensor(10842.1035, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10842.1015625
tensor(10842.1035, grad_fn=<NegBackward0>) tensor(10842.1016, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10842.099609375
tensor(10842.1016, grad_fn=<NegBackward0>) tensor(10842.0996, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10842.09765625
tensor(10842.0996, grad_fn=<NegBackward0>) tensor(10842.0977, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10842.0966796875
tensor(10842.0977, grad_fn=<NegBackward0>) tensor(10842.0967, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10842.0947265625
tensor(10842.0967, grad_fn=<NegBackward0>) tensor(10842.0947, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10842.09375
tensor(10842.0947, grad_fn=<NegBackward0>) tensor(10842.0938, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10842.0927734375
tensor(10842.0938, grad_fn=<NegBackward0>) tensor(10842.0928, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10842.091796875
tensor(10842.0928, grad_fn=<NegBackward0>) tensor(10842.0918, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10842.0908203125
tensor(10842.0918, grad_fn=<NegBackward0>) tensor(10842.0908, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10842.087890625
tensor(10842.0908, grad_fn=<NegBackward0>) tensor(10842.0879, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10842.0888671875
tensor(10842.0879, grad_fn=<NegBackward0>) tensor(10842.0889, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10842.0869140625
tensor(10842.0879, grad_fn=<NegBackward0>) tensor(10842.0869, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10842.0859375
tensor(10842.0869, grad_fn=<NegBackward0>) tensor(10842.0859, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10842.0869140625
tensor(10842.0859, grad_fn=<NegBackward0>) tensor(10842.0869, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10842.0849609375
tensor(10842.0859, grad_fn=<NegBackward0>) tensor(10842.0850, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10842.0849609375
tensor(10842.0850, grad_fn=<NegBackward0>) tensor(10842.0850, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10842.0830078125
tensor(10842.0850, grad_fn=<NegBackward0>) tensor(10842.0830, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10842.08203125
tensor(10842.0830, grad_fn=<NegBackward0>) tensor(10842.0820, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10842.08203125
tensor(10842.0820, grad_fn=<NegBackward0>) tensor(10842.0820, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10842.08203125
tensor(10842.0820, grad_fn=<NegBackward0>) tensor(10842.0820, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10842.0810546875
tensor(10842.0820, grad_fn=<NegBackward0>) tensor(10842.0811, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10842.0810546875
tensor(10842.0811, grad_fn=<NegBackward0>) tensor(10842.0811, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10842.080078125
tensor(10842.0811, grad_fn=<NegBackward0>) tensor(10842.0801, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10842.078125
tensor(10842.0801, grad_fn=<NegBackward0>) tensor(10842.0781, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10842.0791015625
tensor(10842.0781, grad_fn=<NegBackward0>) tensor(10842.0791, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10842.078125
tensor(10842.0781, grad_fn=<NegBackward0>) tensor(10842.0781, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10842.0771484375
tensor(10842.0781, grad_fn=<NegBackward0>) tensor(10842.0771, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10842.076171875
tensor(10842.0771, grad_fn=<NegBackward0>) tensor(10842.0762, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10842.076171875
tensor(10842.0762, grad_fn=<NegBackward0>) tensor(10842.0762, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10842.0751953125
tensor(10842.0762, grad_fn=<NegBackward0>) tensor(10842.0752, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10842.07421875
tensor(10842.0752, grad_fn=<NegBackward0>) tensor(10842.0742, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10842.0732421875
tensor(10842.0742, grad_fn=<NegBackward0>) tensor(10842.0732, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10842.0751953125
tensor(10842.0732, grad_fn=<NegBackward0>) tensor(10842.0752, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10842.072265625
tensor(10842.0732, grad_fn=<NegBackward0>) tensor(10842.0723, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10842.0732421875
tensor(10842.0723, grad_fn=<NegBackward0>) tensor(10842.0732, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10842.0712890625
tensor(10842.0723, grad_fn=<NegBackward0>) tensor(10842.0713, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10842.0712890625
tensor(10842.0713, grad_fn=<NegBackward0>) tensor(10842.0713, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10842.0703125
tensor(10842.0713, grad_fn=<NegBackward0>) tensor(10842.0703, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10842.0703125
tensor(10842.0703, grad_fn=<NegBackward0>) tensor(10842.0703, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10842.0693359375
tensor(10842.0703, grad_fn=<NegBackward0>) tensor(10842.0693, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10842.0693359375
tensor(10842.0693, grad_fn=<NegBackward0>) tensor(10842.0693, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10842.0673828125
tensor(10842.0693, grad_fn=<NegBackward0>) tensor(10842.0674, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10842.06640625
tensor(10842.0674, grad_fn=<NegBackward0>) tensor(10842.0664, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10842.064453125
tensor(10842.0664, grad_fn=<NegBackward0>) tensor(10842.0645, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10842.0654296875
tensor(10842.0645, grad_fn=<NegBackward0>) tensor(10842.0654, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10842.064453125
tensor(10842.0645, grad_fn=<NegBackward0>) tensor(10842.0645, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10842.064453125
tensor(10842.0645, grad_fn=<NegBackward0>) tensor(10842.0645, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10842.0625
tensor(10842.0645, grad_fn=<NegBackward0>) tensor(10842.0625, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10842.0615234375
tensor(10842.0625, grad_fn=<NegBackward0>) tensor(10842.0615, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10842.0595703125
tensor(10842.0615, grad_fn=<NegBackward0>) tensor(10842.0596, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10842.068359375
tensor(10842.0596, grad_fn=<NegBackward0>) tensor(10842.0684, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10842.0595703125
tensor(10842.0596, grad_fn=<NegBackward0>) tensor(10842.0596, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10842.0888671875
tensor(10842.0596, grad_fn=<NegBackward0>) tensor(10842.0889, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10842.05859375
tensor(10842.0596, grad_fn=<NegBackward0>) tensor(10842.0586, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10842.18359375
tensor(10842.0586, grad_fn=<NegBackward0>) tensor(10842.1836, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -10842.0576171875
tensor(10842.0586, grad_fn=<NegBackward0>) tensor(10842.0576, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10842.0712890625
tensor(10842.0576, grad_fn=<NegBackward0>) tensor(10842.0713, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -10842.0576171875
tensor(10842.0576, grad_fn=<NegBackward0>) tensor(10842.0576, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10842.0673828125
tensor(10842.0576, grad_fn=<NegBackward0>) tensor(10842.0674, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -10842.0546875
tensor(10842.0576, grad_fn=<NegBackward0>) tensor(10842.0547, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10842.0537109375
tensor(10842.0547, grad_fn=<NegBackward0>) tensor(10842.0537, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10842.0546875
tensor(10842.0537, grad_fn=<NegBackward0>) tensor(10842.0547, grad_fn=<NegBackward0>)
1
pi: tensor([[9.9969e-01, 3.1388e-04],
        [3.9925e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0104, 0.9896], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0261, 0.0940],
         [0.7277, 0.1594]],

        [[0.5765, 0.0925],
         [0.6411, 0.5378]],

        [[0.6139, 0.2702],
         [0.7002, 0.5438]],

        [[0.5184, 0.1227],
         [0.7256, 0.5498]],

        [[0.6662, 0.1400],
         [0.7227, 0.5408]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 41
Adjusted Rand Index: -0.006658343736995423
Global Adjusted Rand Index: -0.0017532594045644332
Average Adjusted Rand Index: -0.0027660039695742364
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22232.150390625
inf tensor(22232.1504, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10846.2353515625
tensor(22232.1504, grad_fn=<NegBackward0>) tensor(10846.2354, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10845.63671875
tensor(10846.2354, grad_fn=<NegBackward0>) tensor(10845.6367, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10845.494140625
tensor(10845.6367, grad_fn=<NegBackward0>) tensor(10845.4941, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10845.41796875
tensor(10845.4941, grad_fn=<NegBackward0>) tensor(10845.4180, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10845.3681640625
tensor(10845.4180, grad_fn=<NegBackward0>) tensor(10845.3682, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10845.3291015625
tensor(10845.3682, grad_fn=<NegBackward0>) tensor(10845.3291, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10845.298828125
tensor(10845.3291, grad_fn=<NegBackward0>) tensor(10845.2988, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10845.2724609375
tensor(10845.2988, grad_fn=<NegBackward0>) tensor(10845.2725, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10845.2490234375
tensor(10845.2725, grad_fn=<NegBackward0>) tensor(10845.2490, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10845.2275390625
tensor(10845.2490, grad_fn=<NegBackward0>) tensor(10845.2275, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10845.2060546875
tensor(10845.2275, grad_fn=<NegBackward0>) tensor(10845.2061, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10845.1845703125
tensor(10845.2061, grad_fn=<NegBackward0>) tensor(10845.1846, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10845.16796875
tensor(10845.1846, grad_fn=<NegBackward0>) tensor(10845.1680, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10845.15234375
tensor(10845.1680, grad_fn=<NegBackward0>) tensor(10845.1523, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10845.1376953125
tensor(10845.1523, grad_fn=<NegBackward0>) tensor(10845.1377, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10845.1279296875
tensor(10845.1377, grad_fn=<NegBackward0>) tensor(10845.1279, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10845.1201171875
tensor(10845.1279, grad_fn=<NegBackward0>) tensor(10845.1201, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10845.1123046875
tensor(10845.1201, grad_fn=<NegBackward0>) tensor(10845.1123, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10845.10546875
tensor(10845.1123, grad_fn=<NegBackward0>) tensor(10845.1055, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10845.0986328125
tensor(10845.1055, grad_fn=<NegBackward0>) tensor(10845.0986, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10845.0947265625
tensor(10845.0986, grad_fn=<NegBackward0>) tensor(10845.0947, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10845.0888671875
tensor(10845.0947, grad_fn=<NegBackward0>) tensor(10845.0889, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10845.0869140625
tensor(10845.0889, grad_fn=<NegBackward0>) tensor(10845.0869, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10845.0810546875
tensor(10845.0869, grad_fn=<NegBackward0>) tensor(10845.0811, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10845.0771484375
tensor(10845.0811, grad_fn=<NegBackward0>) tensor(10845.0771, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10845.0732421875
tensor(10845.0771, grad_fn=<NegBackward0>) tensor(10845.0732, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10845.0703125
tensor(10845.0732, grad_fn=<NegBackward0>) tensor(10845.0703, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10845.06640625
tensor(10845.0703, grad_fn=<NegBackward0>) tensor(10845.0664, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10845.064453125
tensor(10845.0664, grad_fn=<NegBackward0>) tensor(10845.0645, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10845.060546875
tensor(10845.0645, grad_fn=<NegBackward0>) tensor(10845.0605, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10845.0556640625
tensor(10845.0605, grad_fn=<NegBackward0>) tensor(10845.0557, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10845.0517578125
tensor(10845.0557, grad_fn=<NegBackward0>) tensor(10845.0518, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10845.0478515625
tensor(10845.0518, grad_fn=<NegBackward0>) tensor(10845.0479, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10845.0419921875
tensor(10845.0479, grad_fn=<NegBackward0>) tensor(10845.0420, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10845.0341796875
tensor(10845.0420, grad_fn=<NegBackward0>) tensor(10845.0342, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10845.02734375
tensor(10845.0342, grad_fn=<NegBackward0>) tensor(10845.0273, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10845.017578125
tensor(10845.0273, grad_fn=<NegBackward0>) tensor(10845.0176, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10845.0078125
tensor(10845.0176, grad_fn=<NegBackward0>) tensor(10845.0078, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10844.994140625
tensor(10845.0078, grad_fn=<NegBackward0>) tensor(10844.9941, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10844.9833984375
tensor(10844.9941, grad_fn=<NegBackward0>) tensor(10844.9834, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10844.966796875
tensor(10844.9834, grad_fn=<NegBackward0>) tensor(10844.9668, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10844.9560546875
tensor(10844.9668, grad_fn=<NegBackward0>) tensor(10844.9561, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10844.9443359375
tensor(10844.9561, grad_fn=<NegBackward0>) tensor(10844.9443, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10844.9345703125
tensor(10844.9443, grad_fn=<NegBackward0>) tensor(10844.9346, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10844.9287109375
tensor(10844.9346, grad_fn=<NegBackward0>) tensor(10844.9287, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10844.9228515625
tensor(10844.9287, grad_fn=<NegBackward0>) tensor(10844.9229, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10844.919921875
tensor(10844.9229, grad_fn=<NegBackward0>) tensor(10844.9199, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10844.9140625
tensor(10844.9199, grad_fn=<NegBackward0>) tensor(10844.9141, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10844.912109375
tensor(10844.9141, grad_fn=<NegBackward0>) tensor(10844.9121, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10844.91015625
tensor(10844.9121, grad_fn=<NegBackward0>) tensor(10844.9102, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10844.91015625
tensor(10844.9102, grad_fn=<NegBackward0>) tensor(10844.9102, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10844.91015625
tensor(10844.9102, grad_fn=<NegBackward0>) tensor(10844.9102, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10844.908203125
tensor(10844.9102, grad_fn=<NegBackward0>) tensor(10844.9082, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10844.91015625
tensor(10844.9082, grad_fn=<NegBackward0>) tensor(10844.9102, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10844.9091796875
tensor(10844.9082, grad_fn=<NegBackward0>) tensor(10844.9092, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -10844.908203125
tensor(10844.9082, grad_fn=<NegBackward0>) tensor(10844.9082, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10844.9091796875
tensor(10844.9082, grad_fn=<NegBackward0>) tensor(10844.9092, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10844.9072265625
tensor(10844.9082, grad_fn=<NegBackward0>) tensor(10844.9072, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10844.908203125
tensor(10844.9072, grad_fn=<NegBackward0>) tensor(10844.9082, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10844.908203125
tensor(10844.9072, grad_fn=<NegBackward0>) tensor(10844.9082, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -10844.90625
tensor(10844.9072, grad_fn=<NegBackward0>) tensor(10844.9062, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10844.9052734375
tensor(10844.9062, grad_fn=<NegBackward0>) tensor(10844.9053, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10844.9052734375
tensor(10844.9053, grad_fn=<NegBackward0>) tensor(10844.9053, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10844.90234375
tensor(10844.9053, grad_fn=<NegBackward0>) tensor(10844.9023, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10844.900390625
tensor(10844.9023, grad_fn=<NegBackward0>) tensor(10844.9004, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10844.8974609375
tensor(10844.9004, grad_fn=<NegBackward0>) tensor(10844.8975, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10844.9033203125
tensor(10844.8975, grad_fn=<NegBackward0>) tensor(10844.9033, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10844.8837890625
tensor(10844.8975, grad_fn=<NegBackward0>) tensor(10844.8838, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10844.8544921875
tensor(10844.8838, grad_fn=<NegBackward0>) tensor(10844.8545, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10844.482421875
tensor(10844.8545, grad_fn=<NegBackward0>) tensor(10844.4824, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10727.6318359375
tensor(10844.4824, grad_fn=<NegBackward0>) tensor(10727.6318, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10726.2626953125
tensor(10727.6318, grad_fn=<NegBackward0>) tensor(10726.2627, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10725.71875
tensor(10726.2627, grad_fn=<NegBackward0>) tensor(10725.7188, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10724.654296875
tensor(10725.7188, grad_fn=<NegBackward0>) tensor(10724.6543, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10724.59375
tensor(10724.6543, grad_fn=<NegBackward0>) tensor(10724.5938, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10724.2890625
tensor(10724.5938, grad_fn=<NegBackward0>) tensor(10724.2891, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10724.251953125
tensor(10724.2891, grad_fn=<NegBackward0>) tensor(10724.2520, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10724.216796875
tensor(10724.2520, grad_fn=<NegBackward0>) tensor(10724.2168, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10724.17578125
tensor(10724.2168, grad_fn=<NegBackward0>) tensor(10724.1758, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10724.1474609375
tensor(10724.1758, grad_fn=<NegBackward0>) tensor(10724.1475, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10724.140625
tensor(10724.1475, grad_fn=<NegBackward0>) tensor(10724.1406, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10724.1357421875
tensor(10724.1406, grad_fn=<NegBackward0>) tensor(10724.1357, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10724.1357421875
tensor(10724.1357, grad_fn=<NegBackward0>) tensor(10724.1357, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10724.0908203125
tensor(10724.1357, grad_fn=<NegBackward0>) tensor(10724.0908, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10724.087890625
tensor(10724.0908, grad_fn=<NegBackward0>) tensor(10724.0879, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10724.0830078125
tensor(10724.0879, grad_fn=<NegBackward0>) tensor(10724.0830, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10724.0703125
tensor(10724.0830, grad_fn=<NegBackward0>) tensor(10724.0703, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10724.138671875
tensor(10724.0703, grad_fn=<NegBackward0>) tensor(10724.1387, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10724.06640625
tensor(10724.0703, grad_fn=<NegBackward0>) tensor(10724.0664, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10724.0751953125
tensor(10724.0664, grad_fn=<NegBackward0>) tensor(10724.0752, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10724.05859375
tensor(10724.0664, grad_fn=<NegBackward0>) tensor(10724.0586, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10724.05859375
tensor(10724.0586, grad_fn=<NegBackward0>) tensor(10724.0586, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10724.0458984375
tensor(10724.0586, grad_fn=<NegBackward0>) tensor(10724.0459, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10724.0419921875
tensor(10724.0459, grad_fn=<NegBackward0>) tensor(10724.0420, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10724.0419921875
tensor(10724.0420, grad_fn=<NegBackward0>) tensor(10724.0420, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10724.041015625
tensor(10724.0420, grad_fn=<NegBackward0>) tensor(10724.0410, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10724.009765625
tensor(10724.0410, grad_fn=<NegBackward0>) tensor(10724.0098, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10724.0927734375
tensor(10724.0098, grad_fn=<NegBackward0>) tensor(10724.0928, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -10724.009765625
tensor(10724.0098, grad_fn=<NegBackward0>) tensor(10724.0098, grad_fn=<NegBackward0>)
pi: tensor([[0.7764, 0.2236],
        [0.3081, 0.6919]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5318, 0.4682], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1865, 0.0993],
         [0.5489, 0.2653]],

        [[0.6893, 0.0960],
         [0.5817, 0.5694]],

        [[0.7141, 0.1015],
         [0.6545, 0.7287]],

        [[0.5066, 0.1006],
         [0.5390, 0.6341]],

        [[0.6766, 0.1037],
         [0.5997, 0.5407]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
time is 1
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 2
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 93
Adjusted Rand Index: 0.7369480537608971
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369294996039799
time is 4
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8823435719624108
Global Adjusted Rand Index: 0.8387260263396155
Average Adjusted Rand Index: 0.8395658283668241
[-0.0017532594045644332, 0.8387260263396155] [-0.0027660039695742364, 0.8395658283668241] [10842.052734375, 10724.00390625]
-------------------------------------
This iteration is 4
True Objective function: Loss = -10648.960874680504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25004.1328125
inf tensor(25004.1328, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10752.8359375
tensor(25004.1328, grad_fn=<NegBackward0>) tensor(10752.8359, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10752.025390625
tensor(10752.8359, grad_fn=<NegBackward0>) tensor(10752.0254, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10751.7861328125
tensor(10752.0254, grad_fn=<NegBackward0>) tensor(10751.7861, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10751.6328125
tensor(10751.7861, grad_fn=<NegBackward0>) tensor(10751.6328, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10751.51953125
tensor(10751.6328, grad_fn=<NegBackward0>) tensor(10751.5195, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10751.4365234375
tensor(10751.5195, grad_fn=<NegBackward0>) tensor(10751.4365, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10751.3779296875
tensor(10751.4365, grad_fn=<NegBackward0>) tensor(10751.3779, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10751.3359375
tensor(10751.3779, grad_fn=<NegBackward0>) tensor(10751.3359, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10751.3037109375
tensor(10751.3359, grad_fn=<NegBackward0>) tensor(10751.3037, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10751.275390625
tensor(10751.3037, grad_fn=<NegBackward0>) tensor(10751.2754, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10751.2509765625
tensor(10751.2754, grad_fn=<NegBackward0>) tensor(10751.2510, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10751.2275390625
tensor(10751.2510, grad_fn=<NegBackward0>) tensor(10751.2275, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10751.2041015625
tensor(10751.2275, grad_fn=<NegBackward0>) tensor(10751.2041, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10751.18359375
tensor(10751.2041, grad_fn=<NegBackward0>) tensor(10751.1836, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10751.16015625
tensor(10751.1836, grad_fn=<NegBackward0>) tensor(10751.1602, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10751.1376953125
tensor(10751.1602, grad_fn=<NegBackward0>) tensor(10751.1377, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10751.111328125
tensor(10751.1377, grad_fn=<NegBackward0>) tensor(10751.1113, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10751.0830078125
tensor(10751.1113, grad_fn=<NegBackward0>) tensor(10751.0830, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10751.0498046875
tensor(10751.0830, grad_fn=<NegBackward0>) tensor(10751.0498, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10751.0146484375
tensor(10751.0498, grad_fn=<NegBackward0>) tensor(10751.0146, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10750.9794921875
tensor(10751.0146, grad_fn=<NegBackward0>) tensor(10750.9795, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10750.947265625
tensor(10750.9795, grad_fn=<NegBackward0>) tensor(10750.9473, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10750.9130859375
tensor(10750.9473, grad_fn=<NegBackward0>) tensor(10750.9131, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10750.8779296875
tensor(10750.9131, grad_fn=<NegBackward0>) tensor(10750.8779, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10750.83203125
tensor(10750.8779, grad_fn=<NegBackward0>) tensor(10750.8320, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10750.734375
tensor(10750.8320, grad_fn=<NegBackward0>) tensor(10750.7344, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10750.0615234375
tensor(10750.7344, grad_fn=<NegBackward0>) tensor(10750.0615, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10745.173828125
tensor(10750.0615, grad_fn=<NegBackward0>) tensor(10745.1738, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10744.8359375
tensor(10745.1738, grad_fn=<NegBackward0>) tensor(10744.8359, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10744.7783203125
tensor(10744.8359, grad_fn=<NegBackward0>) tensor(10744.7783, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10744.759765625
tensor(10744.7783, grad_fn=<NegBackward0>) tensor(10744.7598, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10744.751953125
tensor(10744.7598, grad_fn=<NegBackward0>) tensor(10744.7520, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10744.744140625
tensor(10744.7520, grad_fn=<NegBackward0>) tensor(10744.7441, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10744.7392578125
tensor(10744.7441, grad_fn=<NegBackward0>) tensor(10744.7393, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10744.7353515625
tensor(10744.7393, grad_fn=<NegBackward0>) tensor(10744.7354, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10744.7333984375
tensor(10744.7354, grad_fn=<NegBackward0>) tensor(10744.7334, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10744.734375
tensor(10744.7334, grad_fn=<NegBackward0>) tensor(10744.7344, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -10744.732421875
tensor(10744.7334, grad_fn=<NegBackward0>) tensor(10744.7324, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10744.732421875
tensor(10744.7324, grad_fn=<NegBackward0>) tensor(10744.7324, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10744.732421875
tensor(10744.7324, grad_fn=<NegBackward0>) tensor(10744.7324, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10744.7314453125
tensor(10744.7324, grad_fn=<NegBackward0>) tensor(10744.7314, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10744.73046875
tensor(10744.7314, grad_fn=<NegBackward0>) tensor(10744.7305, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10744.7314453125
tensor(10744.7305, grad_fn=<NegBackward0>) tensor(10744.7314, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -10744.728515625
tensor(10744.7305, grad_fn=<NegBackward0>) tensor(10744.7285, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10744.728515625
tensor(10744.7285, grad_fn=<NegBackward0>) tensor(10744.7285, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10744.728515625
tensor(10744.7285, grad_fn=<NegBackward0>) tensor(10744.7285, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10744.7275390625
tensor(10744.7285, grad_fn=<NegBackward0>) tensor(10744.7275, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10744.728515625
tensor(10744.7275, grad_fn=<NegBackward0>) tensor(10744.7285, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10744.7275390625
tensor(10744.7275, grad_fn=<NegBackward0>) tensor(10744.7275, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10744.728515625
tensor(10744.7275, grad_fn=<NegBackward0>) tensor(10744.7285, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10744.728515625
tensor(10744.7275, grad_fn=<NegBackward0>) tensor(10744.7285, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -10744.7265625
tensor(10744.7275, grad_fn=<NegBackward0>) tensor(10744.7266, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10744.7265625
tensor(10744.7266, grad_fn=<NegBackward0>) tensor(10744.7266, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10744.728515625
tensor(10744.7266, grad_fn=<NegBackward0>) tensor(10744.7285, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10744.7275390625
tensor(10744.7266, grad_fn=<NegBackward0>) tensor(10744.7275, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -10744.7265625
tensor(10744.7266, grad_fn=<NegBackward0>) tensor(10744.7266, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10744.7275390625
tensor(10744.7266, grad_fn=<NegBackward0>) tensor(10744.7275, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10744.7275390625
tensor(10744.7266, grad_fn=<NegBackward0>) tensor(10744.7275, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -10744.7265625
tensor(10744.7266, grad_fn=<NegBackward0>) tensor(10744.7266, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10744.7275390625
tensor(10744.7266, grad_fn=<NegBackward0>) tensor(10744.7275, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10744.7265625
tensor(10744.7266, grad_fn=<NegBackward0>) tensor(10744.7266, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10744.7275390625
tensor(10744.7266, grad_fn=<NegBackward0>) tensor(10744.7275, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10744.7265625
tensor(10744.7266, grad_fn=<NegBackward0>) tensor(10744.7266, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10744.724609375
tensor(10744.7266, grad_fn=<NegBackward0>) tensor(10744.7246, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10744.7255859375
tensor(10744.7246, grad_fn=<NegBackward0>) tensor(10744.7256, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10744.724609375
tensor(10744.7246, grad_fn=<NegBackward0>) tensor(10744.7246, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10744.7236328125
tensor(10744.7246, grad_fn=<NegBackward0>) tensor(10744.7236, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10744.7265625
tensor(10744.7236, grad_fn=<NegBackward0>) tensor(10744.7266, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10744.724609375
tensor(10744.7236, grad_fn=<NegBackward0>) tensor(10744.7246, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -10744.724609375
tensor(10744.7236, grad_fn=<NegBackward0>) tensor(10744.7246, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -10744.724609375
tensor(10744.7236, grad_fn=<NegBackward0>) tensor(10744.7246, grad_fn=<NegBackward0>)
4
Iteration 7200: Loss = -10744.7255859375
tensor(10744.7236, grad_fn=<NegBackward0>) tensor(10744.7256, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7200 due to no improvement.
pi: tensor([[0.4123, 0.5877],
        [0.0062, 0.9938]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1138, 0.8862], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0689, 0.1166],
         [0.5939, 0.1607]],

        [[0.5153, 0.1011],
         [0.6143, 0.6086]],

        [[0.6927, 0.0958],
         [0.7287, 0.5957]],

        [[0.7153, 0.0607],
         [0.6080, 0.6108]],

        [[0.5790, 0.2868],
         [0.5783, 0.7156]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.003243945514707375
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0013454550112703889
Average Adjusted Rand Index: -0.0027024608506621742
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21588.09765625
inf tensor(21588.0977, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10754.2392578125
tensor(21588.0977, grad_fn=<NegBackward0>) tensor(10754.2393, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10752.7490234375
tensor(10754.2393, grad_fn=<NegBackward0>) tensor(10752.7490, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10752.3056640625
tensor(10752.7490, grad_fn=<NegBackward0>) tensor(10752.3057, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10752.0419921875
tensor(10752.3057, grad_fn=<NegBackward0>) tensor(10752.0420, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10751.8525390625
tensor(10752.0420, grad_fn=<NegBackward0>) tensor(10751.8525, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10751.7138671875
tensor(10751.8525, grad_fn=<NegBackward0>) tensor(10751.7139, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10751.6123046875
tensor(10751.7139, grad_fn=<NegBackward0>) tensor(10751.6123, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10751.5419921875
tensor(10751.6123, grad_fn=<NegBackward0>) tensor(10751.5420, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10751.486328125
tensor(10751.5420, grad_fn=<NegBackward0>) tensor(10751.4863, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10751.4384765625
tensor(10751.4863, grad_fn=<NegBackward0>) tensor(10751.4385, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10751.3720703125
tensor(10751.4385, grad_fn=<NegBackward0>) tensor(10751.3721, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10751.212890625
tensor(10751.3721, grad_fn=<NegBackward0>) tensor(10751.2129, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10751.0908203125
tensor(10751.2129, grad_fn=<NegBackward0>) tensor(10751.0908, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10751.060546875
tensor(10751.0908, grad_fn=<NegBackward0>) tensor(10751.0605, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10751.0439453125
tensor(10751.0605, grad_fn=<NegBackward0>) tensor(10751.0439, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10751.0322265625
tensor(10751.0439, grad_fn=<NegBackward0>) tensor(10751.0322, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10751.0234375
tensor(10751.0322, grad_fn=<NegBackward0>) tensor(10751.0234, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10751.0166015625
tensor(10751.0234, grad_fn=<NegBackward0>) tensor(10751.0166, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10751.0107421875
tensor(10751.0166, grad_fn=<NegBackward0>) tensor(10751.0107, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10751.005859375
tensor(10751.0107, grad_fn=<NegBackward0>) tensor(10751.0059, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10751.00390625
tensor(10751.0059, grad_fn=<NegBackward0>) tensor(10751.0039, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10751.0
tensor(10751.0039, grad_fn=<NegBackward0>) tensor(10751., grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10750.998046875
tensor(10751., grad_fn=<NegBackward0>) tensor(10750.9980, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10750.9990234375
tensor(10750.9980, grad_fn=<NegBackward0>) tensor(10750.9990, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -10750.9951171875
tensor(10750.9980, grad_fn=<NegBackward0>) tensor(10750.9951, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10750.99609375
tensor(10750.9951, grad_fn=<NegBackward0>) tensor(10750.9961, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -10750.994140625
tensor(10750.9951, grad_fn=<NegBackward0>) tensor(10750.9941, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10750.9931640625
tensor(10750.9941, grad_fn=<NegBackward0>) tensor(10750.9932, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10750.9921875
tensor(10750.9932, grad_fn=<NegBackward0>) tensor(10750.9922, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10750.9921875
tensor(10750.9922, grad_fn=<NegBackward0>) tensor(10750.9922, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10750.9921875
tensor(10750.9922, grad_fn=<NegBackward0>) tensor(10750.9922, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10750.9921875
tensor(10750.9922, grad_fn=<NegBackward0>) tensor(10750.9922, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10750.990234375
tensor(10750.9922, grad_fn=<NegBackward0>) tensor(10750.9902, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10750.990234375
tensor(10750.9902, grad_fn=<NegBackward0>) tensor(10750.9902, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10750.990234375
tensor(10750.9902, grad_fn=<NegBackward0>) tensor(10750.9902, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10750.9892578125
tensor(10750.9902, grad_fn=<NegBackward0>) tensor(10750.9893, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10750.9892578125
tensor(10750.9893, grad_fn=<NegBackward0>) tensor(10750.9893, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10750.9892578125
tensor(10750.9893, grad_fn=<NegBackward0>) tensor(10750.9893, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10750.98828125
tensor(10750.9893, grad_fn=<NegBackward0>) tensor(10750.9883, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10750.9873046875
tensor(10750.9883, grad_fn=<NegBackward0>) tensor(10750.9873, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10750.98828125
tensor(10750.9873, grad_fn=<NegBackward0>) tensor(10750.9883, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -10750.986328125
tensor(10750.9873, grad_fn=<NegBackward0>) tensor(10750.9863, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10750.9892578125
tensor(10750.9863, grad_fn=<NegBackward0>) tensor(10750.9893, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -10750.986328125
tensor(10750.9863, grad_fn=<NegBackward0>) tensor(10750.9863, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10750.9873046875
tensor(10750.9863, grad_fn=<NegBackward0>) tensor(10750.9873, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10750.986328125
tensor(10750.9863, grad_fn=<NegBackward0>) tensor(10750.9863, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10750.986328125
tensor(10750.9863, grad_fn=<NegBackward0>) tensor(10750.9863, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10750.9853515625
tensor(10750.9863, grad_fn=<NegBackward0>) tensor(10750.9854, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10750.986328125
tensor(10750.9854, grad_fn=<NegBackward0>) tensor(10750.9863, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10750.9853515625
tensor(10750.9854, grad_fn=<NegBackward0>) tensor(10750.9854, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10750.9833984375
tensor(10750.9854, grad_fn=<NegBackward0>) tensor(10750.9834, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10750.9833984375
tensor(10750.9834, grad_fn=<NegBackward0>) tensor(10750.9834, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10750.9873046875
tensor(10750.9834, grad_fn=<NegBackward0>) tensor(10750.9873, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10750.9833984375
tensor(10750.9834, grad_fn=<NegBackward0>) tensor(10750.9834, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10750.984375
tensor(10750.9834, grad_fn=<NegBackward0>) tensor(10750.9844, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10750.9833984375
tensor(10750.9834, grad_fn=<NegBackward0>) tensor(10750.9834, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10750.9833984375
tensor(10750.9834, grad_fn=<NegBackward0>) tensor(10750.9834, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10750.982421875
tensor(10750.9834, grad_fn=<NegBackward0>) tensor(10750.9824, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10750.9814453125
tensor(10750.9824, grad_fn=<NegBackward0>) tensor(10750.9814, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10750.982421875
tensor(10750.9814, grad_fn=<NegBackward0>) tensor(10750.9824, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10750.9814453125
tensor(10750.9814, grad_fn=<NegBackward0>) tensor(10750.9814, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10750.982421875
tensor(10750.9814, grad_fn=<NegBackward0>) tensor(10750.9824, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10750.9833984375
tensor(10750.9814, grad_fn=<NegBackward0>) tensor(10750.9834, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -10750.9814453125
tensor(10750.9814, grad_fn=<NegBackward0>) tensor(10750.9814, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10750.9814453125
tensor(10750.9814, grad_fn=<NegBackward0>) tensor(10750.9814, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10750.9814453125
tensor(10750.9814, grad_fn=<NegBackward0>) tensor(10750.9814, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10750.9814453125
tensor(10750.9814, grad_fn=<NegBackward0>) tensor(10750.9814, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10750.9814453125
tensor(10750.9814, grad_fn=<NegBackward0>) tensor(10750.9814, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10750.9814453125
tensor(10750.9814, grad_fn=<NegBackward0>) tensor(10750.9814, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10750.98046875
tensor(10750.9814, grad_fn=<NegBackward0>) tensor(10750.9805, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10750.9794921875
tensor(10750.9805, grad_fn=<NegBackward0>) tensor(10750.9795, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10750.98046875
tensor(10750.9795, grad_fn=<NegBackward0>) tensor(10750.9805, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10750.98046875
tensor(10750.9795, grad_fn=<NegBackward0>) tensor(10750.9805, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -10750.9794921875
tensor(10750.9795, grad_fn=<NegBackward0>) tensor(10750.9795, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10750.9794921875
tensor(10750.9795, grad_fn=<NegBackward0>) tensor(10750.9795, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10750.9814453125
tensor(10750.9795, grad_fn=<NegBackward0>) tensor(10750.9814, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10750.9814453125
tensor(10750.9795, grad_fn=<NegBackward0>) tensor(10750.9814, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -10750.9794921875
tensor(10750.9795, grad_fn=<NegBackward0>) tensor(10750.9795, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10750.9814453125
tensor(10750.9795, grad_fn=<NegBackward0>) tensor(10750.9814, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10750.9775390625
tensor(10750.9795, grad_fn=<NegBackward0>) tensor(10750.9775, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10750.9794921875
tensor(10750.9775, grad_fn=<NegBackward0>) tensor(10750.9795, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10750.9794921875
tensor(10750.9775, grad_fn=<NegBackward0>) tensor(10750.9795, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -10750.978515625
tensor(10750.9775, grad_fn=<NegBackward0>) tensor(10750.9785, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -10750.978515625
tensor(10750.9775, grad_fn=<NegBackward0>) tensor(10750.9785, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -10750.98046875
tensor(10750.9775, grad_fn=<NegBackward0>) tensor(10750.9805, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[0.0023, 0.9977],
        [0.0378, 0.9622]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0551, 0.9449], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2479, 0.1949],
         [0.5480, 0.1532]],

        [[0.5262, 0.1717],
         [0.6773, 0.5045]],

        [[0.5024, 0.1803],
         [0.5633, 0.5206]],

        [[0.5260, 0.1924],
         [0.7190, 0.5156]],

        [[0.6288, 0.2435],
         [0.5289, 0.6599]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: 0.0009096910051066224
Average Adjusted Rand Index: 0.0001617442499919128
[-0.0013454550112703889, 0.0009096910051066224] [-0.0027024608506621742, 0.0001617442499919128] [10744.7255859375, 10750.98046875]
-------------------------------------
This iteration is 5
True Objective function: Loss = -11022.92164798985
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22066.857421875
inf tensor(22066.8574, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11151.6142578125
tensor(22066.8574, grad_fn=<NegBackward0>) tensor(11151.6143, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11151.2099609375
tensor(11151.6143, grad_fn=<NegBackward0>) tensor(11151.2100, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11151.0087890625
tensor(11151.2100, grad_fn=<NegBackward0>) tensor(11151.0088, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11150.8037109375
tensor(11151.0088, grad_fn=<NegBackward0>) tensor(11150.8037, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11150.60546875
tensor(11150.8037, grad_fn=<NegBackward0>) tensor(11150.6055, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11150.4541015625
tensor(11150.6055, grad_fn=<NegBackward0>) tensor(11150.4541, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11150.3564453125
tensor(11150.4541, grad_fn=<NegBackward0>) tensor(11150.3564, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11150.2900390625
tensor(11150.3564, grad_fn=<NegBackward0>) tensor(11150.2900, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11150.23828125
tensor(11150.2900, grad_fn=<NegBackward0>) tensor(11150.2383, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11150.185546875
tensor(11150.2383, grad_fn=<NegBackward0>) tensor(11150.1855, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11150.1259765625
tensor(11150.1855, grad_fn=<NegBackward0>) tensor(11150.1260, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11150.0654296875
tensor(11150.1260, grad_fn=<NegBackward0>) tensor(11150.0654, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11150.0068359375
tensor(11150.0654, grad_fn=<NegBackward0>) tensor(11150.0068, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11149.947265625
tensor(11150.0068, grad_fn=<NegBackward0>) tensor(11149.9473, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11149.884765625
tensor(11149.9473, grad_fn=<NegBackward0>) tensor(11149.8848, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11149.8115234375
tensor(11149.8848, grad_fn=<NegBackward0>) tensor(11149.8115, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11149.7236328125
tensor(11149.8115, grad_fn=<NegBackward0>) tensor(11149.7236, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11149.6240234375
tensor(11149.7236, grad_fn=<NegBackward0>) tensor(11149.6240, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11149.513671875
tensor(11149.6240, grad_fn=<NegBackward0>) tensor(11149.5137, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11149.3916015625
tensor(11149.5137, grad_fn=<NegBackward0>) tensor(11149.3916, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11149.2587890625
tensor(11149.3916, grad_fn=<NegBackward0>) tensor(11149.2588, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11149.107421875
tensor(11149.2588, grad_fn=<NegBackward0>) tensor(11149.1074, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11148.9365234375
tensor(11149.1074, grad_fn=<NegBackward0>) tensor(11148.9365, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11148.7578125
tensor(11148.9365, grad_fn=<NegBackward0>) tensor(11148.7578, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11148.5849609375
tensor(11148.7578, grad_fn=<NegBackward0>) tensor(11148.5850, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11148.4365234375
tensor(11148.5850, grad_fn=<NegBackward0>) tensor(11148.4365, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11148.34765625
tensor(11148.4365, grad_fn=<NegBackward0>) tensor(11148.3477, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11148.302734375
tensor(11148.3477, grad_fn=<NegBackward0>) tensor(11148.3027, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11148.2822265625
tensor(11148.3027, grad_fn=<NegBackward0>) tensor(11148.2822, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11148.275390625
tensor(11148.2822, grad_fn=<NegBackward0>) tensor(11148.2754, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11148.2724609375
tensor(11148.2754, grad_fn=<NegBackward0>) tensor(11148.2725, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11148.271484375
tensor(11148.2725, grad_fn=<NegBackward0>) tensor(11148.2715, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11148.2705078125
tensor(11148.2715, grad_fn=<NegBackward0>) tensor(11148.2705, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11148.26953125
tensor(11148.2705, grad_fn=<NegBackward0>) tensor(11148.2695, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11148.2705078125
tensor(11148.2695, grad_fn=<NegBackward0>) tensor(11148.2705, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11148.2705078125
tensor(11148.2695, grad_fn=<NegBackward0>) tensor(11148.2705, grad_fn=<NegBackward0>)
2
Iteration 3700: Loss = -11148.26953125
tensor(11148.2695, grad_fn=<NegBackward0>) tensor(11148.2695, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11148.2705078125
tensor(11148.2695, grad_fn=<NegBackward0>) tensor(11148.2705, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11148.26953125
tensor(11148.2695, grad_fn=<NegBackward0>) tensor(11148.2695, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11148.26953125
tensor(11148.2695, grad_fn=<NegBackward0>) tensor(11148.2695, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11148.2685546875
tensor(11148.2695, grad_fn=<NegBackward0>) tensor(11148.2686, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11148.2705078125
tensor(11148.2686, grad_fn=<NegBackward0>) tensor(11148.2705, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11148.2685546875
tensor(11148.2686, grad_fn=<NegBackward0>) tensor(11148.2686, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11148.26953125
tensor(11148.2686, grad_fn=<NegBackward0>) tensor(11148.2695, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11148.2685546875
tensor(11148.2686, grad_fn=<NegBackward0>) tensor(11148.2686, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11148.2685546875
tensor(11148.2686, grad_fn=<NegBackward0>) tensor(11148.2686, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11148.2685546875
tensor(11148.2686, grad_fn=<NegBackward0>) tensor(11148.2686, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11148.267578125
tensor(11148.2686, grad_fn=<NegBackward0>) tensor(11148.2676, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11148.267578125
tensor(11148.2676, grad_fn=<NegBackward0>) tensor(11148.2676, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11148.267578125
tensor(11148.2676, grad_fn=<NegBackward0>) tensor(11148.2676, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11148.2685546875
tensor(11148.2676, grad_fn=<NegBackward0>) tensor(11148.2686, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11148.267578125
tensor(11148.2676, grad_fn=<NegBackward0>) tensor(11148.2676, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11148.2705078125
tensor(11148.2676, grad_fn=<NegBackward0>) tensor(11148.2705, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11148.267578125
tensor(11148.2676, grad_fn=<NegBackward0>) tensor(11148.2676, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11148.267578125
tensor(11148.2676, grad_fn=<NegBackward0>) tensor(11148.2676, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11148.2666015625
tensor(11148.2676, grad_fn=<NegBackward0>) tensor(11148.2666, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11148.2841796875
tensor(11148.2666, grad_fn=<NegBackward0>) tensor(11148.2842, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11148.267578125
tensor(11148.2666, grad_fn=<NegBackward0>) tensor(11148.2676, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11148.267578125
tensor(11148.2666, grad_fn=<NegBackward0>) tensor(11148.2676, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -11148.2666015625
tensor(11148.2666, grad_fn=<NegBackward0>) tensor(11148.2666, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11148.267578125
tensor(11148.2666, grad_fn=<NegBackward0>) tensor(11148.2676, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11148.2822265625
tensor(11148.2666, grad_fn=<NegBackward0>) tensor(11148.2822, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11148.2666015625
tensor(11148.2666, grad_fn=<NegBackward0>) tensor(11148.2666, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11148.265625
tensor(11148.2666, grad_fn=<NegBackward0>) tensor(11148.2656, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11148.267578125
tensor(11148.2656, grad_fn=<NegBackward0>) tensor(11148.2676, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11148.265625
tensor(11148.2656, grad_fn=<NegBackward0>) tensor(11148.2656, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11148.265625
tensor(11148.2656, grad_fn=<NegBackward0>) tensor(11148.2656, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11148.2666015625
tensor(11148.2656, grad_fn=<NegBackward0>) tensor(11148.2666, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11148.2646484375
tensor(11148.2656, grad_fn=<NegBackward0>) tensor(11148.2646, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11148.2666015625
tensor(11148.2646, grad_fn=<NegBackward0>) tensor(11148.2666, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11148.265625
tensor(11148.2646, grad_fn=<NegBackward0>) tensor(11148.2656, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11148.2646484375
tensor(11148.2646, grad_fn=<NegBackward0>) tensor(11148.2646, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11148.265625
tensor(11148.2646, grad_fn=<NegBackward0>) tensor(11148.2656, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11148.265625
tensor(11148.2646, grad_fn=<NegBackward0>) tensor(11148.2656, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11148.2666015625
tensor(11148.2646, grad_fn=<NegBackward0>) tensor(11148.2666, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11148.2646484375
tensor(11148.2646, grad_fn=<NegBackward0>) tensor(11148.2646, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11148.2685546875
tensor(11148.2646, grad_fn=<NegBackward0>) tensor(11148.2686, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11148.2646484375
tensor(11148.2646, grad_fn=<NegBackward0>) tensor(11148.2646, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11148.28125
tensor(11148.2646, grad_fn=<NegBackward0>) tensor(11148.2812, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11148.2646484375
tensor(11148.2646, grad_fn=<NegBackward0>) tensor(11148.2646, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11148.2890625
tensor(11148.2646, grad_fn=<NegBackward0>) tensor(11148.2891, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11148.263671875
tensor(11148.2646, grad_fn=<NegBackward0>) tensor(11148.2637, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11148.2646484375
tensor(11148.2637, grad_fn=<NegBackward0>) tensor(11148.2646, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11148.263671875
tensor(11148.2637, grad_fn=<NegBackward0>) tensor(11148.2637, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11148.263671875
tensor(11148.2637, grad_fn=<NegBackward0>) tensor(11148.2637, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11148.2646484375
tensor(11148.2637, grad_fn=<NegBackward0>) tensor(11148.2646, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11148.263671875
tensor(11148.2637, grad_fn=<NegBackward0>) tensor(11148.2637, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11148.291015625
tensor(11148.2637, grad_fn=<NegBackward0>) tensor(11148.2910, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11148.263671875
tensor(11148.2637, grad_fn=<NegBackward0>) tensor(11148.2637, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11148.2646484375
tensor(11148.2637, grad_fn=<NegBackward0>) tensor(11148.2646, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11148.263671875
tensor(11148.2637, grad_fn=<NegBackward0>) tensor(11148.2637, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11148.265625
tensor(11148.2637, grad_fn=<NegBackward0>) tensor(11148.2656, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11148.2958984375
tensor(11148.2637, grad_fn=<NegBackward0>) tensor(11148.2959, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11148.2646484375
tensor(11148.2637, grad_fn=<NegBackward0>) tensor(11148.2646, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -11148.26953125
tensor(11148.2637, grad_fn=<NegBackward0>) tensor(11148.2695, grad_fn=<NegBackward0>)
4
Iteration 9600: Loss = -11148.330078125
tensor(11148.2637, grad_fn=<NegBackward0>) tensor(11148.3301, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9600 due to no improvement.
pi: tensor([[4.5612e-01, 5.4388e-01],
        [9.9900e-01, 9.9540e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4961, 0.5039], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1459, 0.1700],
         [0.5628, 0.1982]],

        [[0.5001, 0.1910],
         [0.5727, 0.6543]],

        [[0.5317, 0.1739],
         [0.6985, 0.6182]],

        [[0.5281, 0.1730],
         [0.6128, 0.5845]],

        [[0.6697, 0.1681],
         [0.5821, 0.7048]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.006381844064987857
time is 1
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0038987895312232452
time is 2
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.03115331411624301
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.004323639008651342
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.00416963576765682
Global Adjusted Rand Index: 0.009168198566541767
Average Adjusted Rand Index: 0.005764852564694585
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22716.849609375
inf tensor(22716.8496, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11151.994140625
tensor(22716.8496, grad_fn=<NegBackward0>) tensor(11151.9941, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11151.4306640625
tensor(11151.9941, grad_fn=<NegBackward0>) tensor(11151.4307, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11151.2080078125
tensor(11151.4307, grad_fn=<NegBackward0>) tensor(11151.2080, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11150.94140625
tensor(11151.2080, grad_fn=<NegBackward0>) tensor(11150.9414, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11150.6396484375
tensor(11150.9414, grad_fn=<NegBackward0>) tensor(11150.6396, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11150.4228515625
tensor(11150.6396, grad_fn=<NegBackward0>) tensor(11150.4229, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11150.3349609375
tensor(11150.4229, grad_fn=<NegBackward0>) tensor(11150.3350, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11150.28515625
tensor(11150.3350, grad_fn=<NegBackward0>) tensor(11150.2852, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11150.2470703125
tensor(11150.2852, grad_fn=<NegBackward0>) tensor(11150.2471, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11150.2119140625
tensor(11150.2471, grad_fn=<NegBackward0>) tensor(11150.2119, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11150.1767578125
tensor(11150.2119, grad_fn=<NegBackward0>) tensor(11150.1768, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11150.13671875
tensor(11150.1768, grad_fn=<NegBackward0>) tensor(11150.1367, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11150.0751953125
tensor(11150.1367, grad_fn=<NegBackward0>) tensor(11150.0752, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11149.9521484375
tensor(11150.0752, grad_fn=<NegBackward0>) tensor(11149.9521, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11149.7763671875
tensor(11149.9521, grad_fn=<NegBackward0>) tensor(11149.7764, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11149.599609375
tensor(11149.7764, grad_fn=<NegBackward0>) tensor(11149.5996, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11149.421875
tensor(11149.5996, grad_fn=<NegBackward0>) tensor(11149.4219, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11149.2216796875
tensor(11149.4219, grad_fn=<NegBackward0>) tensor(11149.2217, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11148.97265625
tensor(11149.2217, grad_fn=<NegBackward0>) tensor(11148.9727, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11148.703125
tensor(11148.9727, grad_fn=<NegBackward0>) tensor(11148.7031, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11148.4580078125
tensor(11148.7031, grad_fn=<NegBackward0>) tensor(11148.4580, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11148.3291015625
tensor(11148.4580, grad_fn=<NegBackward0>) tensor(11148.3291, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11148.28515625
tensor(11148.3291, grad_fn=<NegBackward0>) tensor(11148.2852, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11148.2734375
tensor(11148.2852, grad_fn=<NegBackward0>) tensor(11148.2734, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11148.2724609375
tensor(11148.2734, grad_fn=<NegBackward0>) tensor(11148.2725, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11148.26953125
tensor(11148.2725, grad_fn=<NegBackward0>) tensor(11148.2695, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11148.2705078125
tensor(11148.2695, grad_fn=<NegBackward0>) tensor(11148.2705, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11148.26953125
tensor(11148.2695, grad_fn=<NegBackward0>) tensor(11148.2695, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11148.26953125
tensor(11148.2695, grad_fn=<NegBackward0>) tensor(11148.2695, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11148.2822265625
tensor(11148.2695, grad_fn=<NegBackward0>) tensor(11148.2822, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11148.2685546875
tensor(11148.2695, grad_fn=<NegBackward0>) tensor(11148.2686, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11148.2685546875
tensor(11148.2686, grad_fn=<NegBackward0>) tensor(11148.2686, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11148.2685546875
tensor(11148.2686, grad_fn=<NegBackward0>) tensor(11148.2686, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11148.2685546875
tensor(11148.2686, grad_fn=<NegBackward0>) tensor(11148.2686, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11148.2685546875
tensor(11148.2686, grad_fn=<NegBackward0>) tensor(11148.2686, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11148.267578125
tensor(11148.2686, grad_fn=<NegBackward0>) tensor(11148.2676, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11148.2744140625
tensor(11148.2676, grad_fn=<NegBackward0>) tensor(11148.2744, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11148.2685546875
tensor(11148.2676, grad_fn=<NegBackward0>) tensor(11148.2686, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -11148.2685546875
tensor(11148.2676, grad_fn=<NegBackward0>) tensor(11148.2686, grad_fn=<NegBackward0>)
3
Iteration 4000: Loss = -11148.2685546875
tensor(11148.2676, grad_fn=<NegBackward0>) tensor(11148.2686, grad_fn=<NegBackward0>)
4
Iteration 4100: Loss = -11148.267578125
tensor(11148.2676, grad_fn=<NegBackward0>) tensor(11148.2676, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11148.2705078125
tensor(11148.2676, grad_fn=<NegBackward0>) tensor(11148.2705, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11148.2734375
tensor(11148.2676, grad_fn=<NegBackward0>) tensor(11148.2734, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -11148.26953125
tensor(11148.2676, grad_fn=<NegBackward0>) tensor(11148.2695, grad_fn=<NegBackward0>)
3
Iteration 4500: Loss = -11148.267578125
tensor(11148.2676, grad_fn=<NegBackward0>) tensor(11148.2676, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11148.2666015625
tensor(11148.2676, grad_fn=<NegBackward0>) tensor(11148.2666, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11148.2666015625
tensor(11148.2666, grad_fn=<NegBackward0>) tensor(11148.2666, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11148.2666015625
tensor(11148.2666, grad_fn=<NegBackward0>) tensor(11148.2666, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11148.265625
tensor(11148.2666, grad_fn=<NegBackward0>) tensor(11148.2656, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11148.2666015625
tensor(11148.2656, grad_fn=<NegBackward0>) tensor(11148.2666, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11148.265625
tensor(11148.2656, grad_fn=<NegBackward0>) tensor(11148.2656, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11148.265625
tensor(11148.2656, grad_fn=<NegBackward0>) tensor(11148.2656, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11148.265625
tensor(11148.2656, grad_fn=<NegBackward0>) tensor(11148.2656, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11148.2666015625
tensor(11148.2656, grad_fn=<NegBackward0>) tensor(11148.2666, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11148.265625
tensor(11148.2656, grad_fn=<NegBackward0>) tensor(11148.2656, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11148.265625
tensor(11148.2656, grad_fn=<NegBackward0>) tensor(11148.2656, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11148.265625
tensor(11148.2656, grad_fn=<NegBackward0>) tensor(11148.2656, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11148.2666015625
tensor(11148.2656, grad_fn=<NegBackward0>) tensor(11148.2666, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11148.265625
tensor(11148.2656, grad_fn=<NegBackward0>) tensor(11148.2656, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11148.3037109375
tensor(11148.2656, grad_fn=<NegBackward0>) tensor(11148.3037, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11148.2666015625
tensor(11148.2656, grad_fn=<NegBackward0>) tensor(11148.2666, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11148.2666015625
tensor(11148.2656, grad_fn=<NegBackward0>) tensor(11148.2666, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -11148.28125
tensor(11148.2656, grad_fn=<NegBackward0>) tensor(11148.2812, grad_fn=<NegBackward0>)
4
Iteration 6400: Loss = -11148.2646484375
tensor(11148.2656, grad_fn=<NegBackward0>) tensor(11148.2646, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11148.2958984375
tensor(11148.2646, grad_fn=<NegBackward0>) tensor(11148.2959, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11148.265625
tensor(11148.2646, grad_fn=<NegBackward0>) tensor(11148.2656, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11148.2646484375
tensor(11148.2646, grad_fn=<NegBackward0>) tensor(11148.2646, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11148.2744140625
tensor(11148.2646, grad_fn=<NegBackward0>) tensor(11148.2744, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11148.263671875
tensor(11148.2646, grad_fn=<NegBackward0>) tensor(11148.2637, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11148.265625
tensor(11148.2637, grad_fn=<NegBackward0>) tensor(11148.2656, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11148.2626953125
tensor(11148.2637, grad_fn=<NegBackward0>) tensor(11148.2627, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11148.265625
tensor(11148.2627, grad_fn=<NegBackward0>) tensor(11148.2656, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11148.263671875
tensor(11148.2627, grad_fn=<NegBackward0>) tensor(11148.2637, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11148.265625
tensor(11148.2627, grad_fn=<NegBackward0>) tensor(11148.2656, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11148.2646484375
tensor(11148.2627, grad_fn=<NegBackward0>) tensor(11148.2646, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -11148.2626953125
tensor(11148.2627, grad_fn=<NegBackward0>) tensor(11148.2627, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11148.263671875
tensor(11148.2627, grad_fn=<NegBackward0>) tensor(11148.2637, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11148.2626953125
tensor(11148.2627, grad_fn=<NegBackward0>) tensor(11148.2627, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11148.2646484375
tensor(11148.2627, grad_fn=<NegBackward0>) tensor(11148.2646, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11148.263671875
tensor(11148.2627, grad_fn=<NegBackward0>) tensor(11148.2637, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11148.263671875
tensor(11148.2627, grad_fn=<NegBackward0>) tensor(11148.2637, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -11148.2646484375
tensor(11148.2627, grad_fn=<NegBackward0>) tensor(11148.2646, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -11148.263671875
tensor(11148.2627, grad_fn=<NegBackward0>) tensor(11148.2637, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8300 due to no improvement.
pi: tensor([[0.4585, 0.5415],
        [0.9990, 0.0010]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4952, 0.5048], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1468, 0.1698],
         [0.5620, 0.1973]],

        [[0.5756, 0.1911],
         [0.6021, 0.5830]],

        [[0.6195, 0.1735],
         [0.7073, 0.7148]],

        [[0.6873, 0.1725],
         [0.6285, 0.6684]],

        [[0.7166, 0.1678],
         [0.6677, 0.5897]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.006381844064987857
time is 1
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 3.0741259601154114e-05
time is 2
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 61
Adjusted Rand Index: 0.03953798188668978
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.004323639008651342
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.00416963576765682
Global Adjusted Rand Index: 0.009155676916858988
Average Adjusted Rand Index: 0.006668176464459519
[0.009168198566541767, 0.009155676916858988] [0.005764852564694585, 0.006668176464459519] [11148.330078125, 11148.263671875]
-------------------------------------
This iteration is 6
True Objective function: Loss = -11071.357215388682
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22440.49609375
inf tensor(22440.4961, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11185.1240234375
tensor(22440.4961, grad_fn=<NegBackward0>) tensor(11185.1240, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11183.916015625
tensor(11185.1240, grad_fn=<NegBackward0>) tensor(11183.9160, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11183.533203125
tensor(11183.9160, grad_fn=<NegBackward0>) tensor(11183.5332, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11183.2822265625
tensor(11183.5332, grad_fn=<NegBackward0>) tensor(11183.2822, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11183.1220703125
tensor(11183.2822, grad_fn=<NegBackward0>) tensor(11183.1221, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11183.0087890625
tensor(11183.1221, grad_fn=<NegBackward0>) tensor(11183.0088, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11182.8671875
tensor(11183.0088, grad_fn=<NegBackward0>) tensor(11182.8672, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11182.7236328125
tensor(11182.8672, grad_fn=<NegBackward0>) tensor(11182.7236, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11182.4326171875
tensor(11182.7236, grad_fn=<NegBackward0>) tensor(11182.4326, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11180.630859375
tensor(11182.4326, grad_fn=<NegBackward0>) tensor(11180.6309, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11167.6591796875
tensor(11180.6309, grad_fn=<NegBackward0>) tensor(11167.6592, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11136.1240234375
tensor(11167.6592, grad_fn=<NegBackward0>) tensor(11136.1240, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11075.8583984375
tensor(11136.1240, grad_fn=<NegBackward0>) tensor(11075.8584, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11073.0517578125
tensor(11075.8584, grad_fn=<NegBackward0>) tensor(11073.0518, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11067.8291015625
tensor(11073.0518, grad_fn=<NegBackward0>) tensor(11067.8291, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11066.5244140625
tensor(11067.8291, grad_fn=<NegBackward0>) tensor(11066.5244, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11066.5078125
tensor(11066.5244, grad_fn=<NegBackward0>) tensor(11066.5078, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11066.49609375
tensor(11066.5078, grad_fn=<NegBackward0>) tensor(11066.4961, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11066.4853515625
tensor(11066.4961, grad_fn=<NegBackward0>) tensor(11066.4854, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11066.4541015625
tensor(11066.4854, grad_fn=<NegBackward0>) tensor(11066.4541, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11066.41015625
tensor(11066.4541, grad_fn=<NegBackward0>) tensor(11066.4102, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11066.130859375
tensor(11066.4102, grad_fn=<NegBackward0>) tensor(11066.1309, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11065.4912109375
tensor(11066.1309, grad_fn=<NegBackward0>) tensor(11065.4912, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11065.3046875
tensor(11065.4912, grad_fn=<NegBackward0>) tensor(11065.3047, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11065.2353515625
tensor(11065.3047, grad_fn=<NegBackward0>) tensor(11065.2354, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11032.3564453125
tensor(11065.2354, grad_fn=<NegBackward0>) tensor(11032.3564, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11031.5361328125
tensor(11032.3564, grad_fn=<NegBackward0>) tensor(11031.5361, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11031.5078125
tensor(11031.5361, grad_fn=<NegBackward0>) tensor(11031.5078, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11031.4990234375
tensor(11031.5078, grad_fn=<NegBackward0>) tensor(11031.4990, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11031.498046875
tensor(11031.4990, grad_fn=<NegBackward0>) tensor(11031.4980, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11031.498046875
tensor(11031.4980, grad_fn=<NegBackward0>) tensor(11031.4980, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11031.4970703125
tensor(11031.4980, grad_fn=<NegBackward0>) tensor(11031.4971, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11031.4970703125
tensor(11031.4971, grad_fn=<NegBackward0>) tensor(11031.4971, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11031.49609375
tensor(11031.4971, grad_fn=<NegBackward0>) tensor(11031.4961, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11031.49609375
tensor(11031.4961, grad_fn=<NegBackward0>) tensor(11031.4961, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11031.451171875
tensor(11031.4961, grad_fn=<NegBackward0>) tensor(11031.4512, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11031.4541015625
tensor(11031.4512, grad_fn=<NegBackward0>) tensor(11031.4541, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11031.451171875
tensor(11031.4512, grad_fn=<NegBackward0>) tensor(11031.4512, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11031.447265625
tensor(11031.4512, grad_fn=<NegBackward0>) tensor(11031.4473, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11031.4482421875
tensor(11031.4473, grad_fn=<NegBackward0>) tensor(11031.4482, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11031.4482421875
tensor(11031.4473, grad_fn=<NegBackward0>) tensor(11031.4482, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -11031.4140625
tensor(11031.4473, grad_fn=<NegBackward0>) tensor(11031.4141, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11029.5966796875
tensor(11031.4141, grad_fn=<NegBackward0>) tensor(11029.5967, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11029.5908203125
tensor(11029.5967, grad_fn=<NegBackward0>) tensor(11029.5908, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11029.591796875
tensor(11029.5908, grad_fn=<NegBackward0>) tensor(11029.5918, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11029.58984375
tensor(11029.5908, grad_fn=<NegBackward0>) tensor(11029.5898, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11029.58984375
tensor(11029.5898, grad_fn=<NegBackward0>) tensor(11029.5898, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11029.58984375
tensor(11029.5898, grad_fn=<NegBackward0>) tensor(11029.5898, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11029.5859375
tensor(11029.5898, grad_fn=<NegBackward0>) tensor(11029.5859, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11029.5859375
tensor(11029.5859, grad_fn=<NegBackward0>) tensor(11029.5859, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11025.0712890625
tensor(11029.5859, grad_fn=<NegBackward0>) tensor(11025.0713, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11025.060546875
tensor(11025.0713, grad_fn=<NegBackward0>) tensor(11025.0605, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11025.0556640625
tensor(11025.0605, grad_fn=<NegBackward0>) tensor(11025.0557, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11024.9541015625
tensor(11025.0557, grad_fn=<NegBackward0>) tensor(11024.9541, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11024.953125
tensor(11024.9541, grad_fn=<NegBackward0>) tensor(11024.9531, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11024.953125
tensor(11024.9531, grad_fn=<NegBackward0>) tensor(11024.9531, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11024.953125
tensor(11024.9531, grad_fn=<NegBackward0>) tensor(11024.9531, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11024.9541015625
tensor(11024.9531, grad_fn=<NegBackward0>) tensor(11024.9541, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11024.953125
tensor(11024.9531, grad_fn=<NegBackward0>) tensor(11024.9531, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11024.9521484375
tensor(11024.9531, grad_fn=<NegBackward0>) tensor(11024.9521, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11024.955078125
tensor(11024.9521, grad_fn=<NegBackward0>) tensor(11024.9551, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11024.9541015625
tensor(11024.9521, grad_fn=<NegBackward0>) tensor(11024.9541, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11024.94921875
tensor(11024.9521, grad_fn=<NegBackward0>) tensor(11024.9492, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11024.94921875
tensor(11024.9492, grad_fn=<NegBackward0>) tensor(11024.9492, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11024.94921875
tensor(11024.9492, grad_fn=<NegBackward0>) tensor(11024.9492, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11024.9521484375
tensor(11024.9492, grad_fn=<NegBackward0>) tensor(11024.9521, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11024.94921875
tensor(11024.9492, grad_fn=<NegBackward0>) tensor(11024.9492, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11024.95703125
tensor(11024.9492, grad_fn=<NegBackward0>) tensor(11024.9570, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11024.94921875
tensor(11024.9492, grad_fn=<NegBackward0>) tensor(11024.9492, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11024.94921875
tensor(11024.9492, grad_fn=<NegBackward0>) tensor(11024.9492, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11024.94921875
tensor(11024.9492, grad_fn=<NegBackward0>) tensor(11024.9492, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11024.94921875
tensor(11024.9492, grad_fn=<NegBackward0>) tensor(11024.9492, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11024.94921875
tensor(11024.9492, grad_fn=<NegBackward0>) tensor(11024.9492, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11024.94921875
tensor(11024.9492, grad_fn=<NegBackward0>) tensor(11024.9492, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11024.96484375
tensor(11024.9492, grad_fn=<NegBackward0>) tensor(11024.9648, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11024.998046875
tensor(11024.9492, grad_fn=<NegBackward0>) tensor(11024.9980, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11024.9482421875
tensor(11024.9492, grad_fn=<NegBackward0>) tensor(11024.9482, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11024.94921875
tensor(11024.9482, grad_fn=<NegBackward0>) tensor(11024.9492, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11024.9482421875
tensor(11024.9482, grad_fn=<NegBackward0>) tensor(11024.9482, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11024.94921875
tensor(11024.9482, grad_fn=<NegBackward0>) tensor(11024.9492, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11024.9482421875
tensor(11024.9482, grad_fn=<NegBackward0>) tensor(11024.9482, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11025.0634765625
tensor(11024.9482, grad_fn=<NegBackward0>) tensor(11025.0635, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11024.947265625
tensor(11024.9482, grad_fn=<NegBackward0>) tensor(11024.9473, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11024.947265625
tensor(11024.9473, grad_fn=<NegBackward0>) tensor(11024.9473, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11024.951171875
tensor(11024.9473, grad_fn=<NegBackward0>) tensor(11024.9512, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11024.9482421875
tensor(11024.9473, grad_fn=<NegBackward0>) tensor(11024.9482, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11024.9921875
tensor(11024.9473, grad_fn=<NegBackward0>) tensor(11024.9922, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -11024.9443359375
tensor(11024.9473, grad_fn=<NegBackward0>) tensor(11024.9443, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11024.9501953125
tensor(11024.9443, grad_fn=<NegBackward0>) tensor(11024.9502, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11024.9453125
tensor(11024.9443, grad_fn=<NegBackward0>) tensor(11024.9453, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11024.9443359375
tensor(11024.9443, grad_fn=<NegBackward0>) tensor(11024.9443, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11024.9453125
tensor(11024.9443, grad_fn=<NegBackward0>) tensor(11024.9453, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11024.9482421875
tensor(11024.9443, grad_fn=<NegBackward0>) tensor(11024.9482, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11024.9453125
tensor(11024.9443, grad_fn=<NegBackward0>) tensor(11024.9453, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -11024.9453125
tensor(11024.9443, grad_fn=<NegBackward0>) tensor(11024.9453, grad_fn=<NegBackward0>)
4
Iteration 9600: Loss = -11025.00390625
tensor(11024.9443, grad_fn=<NegBackward0>) tensor(11025.0039, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9600 due to no improvement.
pi: tensor([[0.7881, 0.2119],
        [0.3002, 0.6998]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5298, 0.4702], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2493, 0.1029],
         [0.5606, 0.2081]],

        [[0.6468, 0.1001],
         [0.6987, 0.5034]],

        [[0.6478, 0.0993],
         [0.6085, 0.7292]],

        [[0.6272, 0.0964],
         [0.6148, 0.7183]],

        [[0.6870, 0.1034],
         [0.6139, 0.6300]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 8
Adjusted Rand Index: 0.7026223303595567
time is 1
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448373142749671
time is 3
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8447122004349823
time is 4
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8444889800545335
Global Adjusted Rand Index: 0.8313883073155675
Average Adjusted Rand Index: 0.8314922917437941
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23942.607421875
inf tensor(23942.6074, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11186.4873046875
tensor(23942.6074, grad_fn=<NegBackward0>) tensor(11186.4873, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11185.9990234375
tensor(11186.4873, grad_fn=<NegBackward0>) tensor(11185.9990, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11185.841796875
tensor(11185.9990, grad_fn=<NegBackward0>) tensor(11185.8418, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11185.6962890625
tensor(11185.8418, grad_fn=<NegBackward0>) tensor(11185.6963, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11185.4267578125
tensor(11185.6963, grad_fn=<NegBackward0>) tensor(11185.4268, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11184.8037109375
tensor(11185.4268, grad_fn=<NegBackward0>) tensor(11184.8037, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11183.962890625
tensor(11184.8037, grad_fn=<NegBackward0>) tensor(11183.9629, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11179.408203125
tensor(11183.9629, grad_fn=<NegBackward0>) tensor(11179.4082, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11027.3173828125
tensor(11179.4082, grad_fn=<NegBackward0>) tensor(11027.3174, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11025.7744140625
tensor(11027.3174, grad_fn=<NegBackward0>) tensor(11025.7744, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11025.3388671875
tensor(11025.7744, grad_fn=<NegBackward0>) tensor(11025.3389, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11025.2939453125
tensor(11025.3389, grad_fn=<NegBackward0>) tensor(11025.2939, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11025.28125
tensor(11025.2939, grad_fn=<NegBackward0>) tensor(11025.2812, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11025.267578125
tensor(11025.2812, grad_fn=<NegBackward0>) tensor(11025.2676, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11025.25
tensor(11025.2676, grad_fn=<NegBackward0>) tensor(11025.2500, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11025.24609375
tensor(11025.2500, grad_fn=<NegBackward0>) tensor(11025.2461, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11025.25390625
tensor(11025.2461, grad_fn=<NegBackward0>) tensor(11025.2539, grad_fn=<NegBackward0>)
1
Iteration 1800: Loss = -11025.23828125
tensor(11025.2461, grad_fn=<NegBackward0>) tensor(11025.2383, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11025.234375
tensor(11025.2383, grad_fn=<NegBackward0>) tensor(11025.2344, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11025.232421875
tensor(11025.2344, grad_fn=<NegBackward0>) tensor(11025.2324, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11025.232421875
tensor(11025.2324, grad_fn=<NegBackward0>) tensor(11025.2324, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11025.2431640625
tensor(11025.2324, grad_fn=<NegBackward0>) tensor(11025.2432, grad_fn=<NegBackward0>)
1
Iteration 2300: Loss = -11025.2294921875
tensor(11025.2324, grad_fn=<NegBackward0>) tensor(11025.2295, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11025.2109375
tensor(11025.2295, grad_fn=<NegBackward0>) tensor(11025.2109, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11025.201171875
tensor(11025.2109, grad_fn=<NegBackward0>) tensor(11025.2012, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11025.2001953125
tensor(11025.2012, grad_fn=<NegBackward0>) tensor(11025.2002, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11025.2001953125
tensor(11025.2002, grad_fn=<NegBackward0>) tensor(11025.2002, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11025.2001953125
tensor(11025.2002, grad_fn=<NegBackward0>) tensor(11025.2002, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11025.201171875
tensor(11025.2002, grad_fn=<NegBackward0>) tensor(11025.2012, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11025.2001953125
tensor(11025.2002, grad_fn=<NegBackward0>) tensor(11025.2002, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11025.201171875
tensor(11025.2002, grad_fn=<NegBackward0>) tensor(11025.2012, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11025.1982421875
tensor(11025.2002, grad_fn=<NegBackward0>) tensor(11025.1982, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11025.197265625
tensor(11025.1982, grad_fn=<NegBackward0>) tensor(11025.1973, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11025.1982421875
tensor(11025.1973, grad_fn=<NegBackward0>) tensor(11025.1982, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11025.197265625
tensor(11025.1973, grad_fn=<NegBackward0>) tensor(11025.1973, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11025.1962890625
tensor(11025.1973, grad_fn=<NegBackward0>) tensor(11025.1963, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11025.1962890625
tensor(11025.1963, grad_fn=<NegBackward0>) tensor(11025.1963, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11025.1943359375
tensor(11025.1963, grad_fn=<NegBackward0>) tensor(11025.1943, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11025.16015625
tensor(11025.1943, grad_fn=<NegBackward0>) tensor(11025.1602, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11025.158203125
tensor(11025.1602, grad_fn=<NegBackward0>) tensor(11025.1582, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11025.1572265625
tensor(11025.1582, grad_fn=<NegBackward0>) tensor(11025.1572, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11025.1435546875
tensor(11025.1572, grad_fn=<NegBackward0>) tensor(11025.1436, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11025.1416015625
tensor(11025.1436, grad_fn=<NegBackward0>) tensor(11025.1416, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11025.1494140625
tensor(11025.1416, grad_fn=<NegBackward0>) tensor(11025.1494, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11025.142578125
tensor(11025.1416, grad_fn=<NegBackward0>) tensor(11025.1426, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -11025.1494140625
tensor(11025.1416, grad_fn=<NegBackward0>) tensor(11025.1494, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -11025.1416015625
tensor(11025.1416, grad_fn=<NegBackward0>) tensor(11025.1416, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11025.1494140625
tensor(11025.1416, grad_fn=<NegBackward0>) tensor(11025.1494, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11025.1416015625
tensor(11025.1416, grad_fn=<NegBackward0>) tensor(11025.1416, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11025.1416015625
tensor(11025.1416, grad_fn=<NegBackward0>) tensor(11025.1416, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11025.1416015625
tensor(11025.1416, grad_fn=<NegBackward0>) tensor(11025.1416, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11025.140625
tensor(11025.1416, grad_fn=<NegBackward0>) tensor(11025.1406, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11025.1435546875
tensor(11025.1406, grad_fn=<NegBackward0>) tensor(11025.1436, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11025.140625
tensor(11025.1406, grad_fn=<NegBackward0>) tensor(11025.1406, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11025.017578125
tensor(11025.1406, grad_fn=<NegBackward0>) tensor(11025.0176, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11024.935546875
tensor(11025.0176, grad_fn=<NegBackward0>) tensor(11024.9355, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11024.935546875
tensor(11024.9355, grad_fn=<NegBackward0>) tensor(11024.9355, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11024.939453125
tensor(11024.9355, grad_fn=<NegBackward0>) tensor(11024.9395, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11024.9365234375
tensor(11024.9355, grad_fn=<NegBackward0>) tensor(11024.9365, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11024.935546875
tensor(11024.9355, grad_fn=<NegBackward0>) tensor(11024.9355, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11024.935546875
tensor(11024.9355, grad_fn=<NegBackward0>) tensor(11024.9355, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11024.9375
tensor(11024.9355, grad_fn=<NegBackward0>) tensor(11024.9375, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11024.9365234375
tensor(11024.9355, grad_fn=<NegBackward0>) tensor(11024.9365, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11024.9580078125
tensor(11024.9355, grad_fn=<NegBackward0>) tensor(11024.9580, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -11024.9384765625
tensor(11024.9355, grad_fn=<NegBackward0>) tensor(11024.9385, grad_fn=<NegBackward0>)
4
Iteration 6600: Loss = -11024.9345703125
tensor(11024.9355, grad_fn=<NegBackward0>) tensor(11024.9346, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11024.935546875
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9355, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11024.9375
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9375, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11024.94921875
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9492, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11024.935546875
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9355, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -11024.9345703125
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9346, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11024.935546875
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9355, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11024.935546875
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9355, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11024.9462890625
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9463, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11024.9345703125
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9346, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11024.935546875
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9355, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11024.9345703125
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9346, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11024.935546875
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9355, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11024.9345703125
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9346, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11024.935546875
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9355, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11024.935546875
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9355, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11024.9345703125
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9346, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11024.9345703125
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9346, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11024.9345703125
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9346, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11024.935546875
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9355, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11024.9345703125
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9346, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11024.935546875
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9355, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11024.935546875
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9355, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11024.9443359375
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9443, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -11024.9482421875
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9482, grad_fn=<NegBackward0>)
4
Iteration 9100: Loss = -11024.9345703125
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9346, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11024.935546875
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9355, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11024.9345703125
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9346, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11024.9345703125
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9346, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11024.93359375
tensor(11024.9346, grad_fn=<NegBackward0>) tensor(11024.9336, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11024.9345703125
tensor(11024.9336, grad_fn=<NegBackward0>) tensor(11024.9346, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11024.9453125
tensor(11024.9336, grad_fn=<NegBackward0>) tensor(11024.9453, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -11024.9345703125
tensor(11024.9336, grad_fn=<NegBackward0>) tensor(11024.9346, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -11024.93359375
tensor(11024.9336, grad_fn=<NegBackward0>) tensor(11024.9336, grad_fn=<NegBackward0>)
pi: tensor([[0.7904, 0.2096],
        [0.3022, 0.6978]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5334, 0.4666], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2480, 0.1028],
         [0.5454, 0.2083]],

        [[0.5618, 0.0998],
         [0.6220, 0.6620]],

        [[0.6639, 0.0986],
         [0.5075, 0.5555]],

        [[0.5399, 0.0963],
         [0.6676, 0.7282]],

        [[0.5453, 0.1033],
         [0.7089, 0.6118]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 8
Adjusted Rand Index: 0.7026223303595567
time is 1
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448373142749671
time is 3
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8447122004349823
time is 4
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8444889800545335
Global Adjusted Rand Index: 0.8313883073155675
Average Adjusted Rand Index: 0.8314922917437941
[0.8313883073155675, 0.8313883073155675] [0.8314922917437941, 0.8314922917437941] [11025.00390625, 11024.93359375]
-------------------------------------
This iteration is 7
True Objective function: Loss = -10817.553627594172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22861.501953125
inf tensor(22861.5020, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10890.65625
tensor(22861.5020, grad_fn=<NegBackward0>) tensor(10890.6562, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10889.015625
tensor(10890.6562, grad_fn=<NegBackward0>) tensor(10889.0156, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10888.3505859375
tensor(10889.0156, grad_fn=<NegBackward0>) tensor(10888.3506, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10888.1435546875
tensor(10888.3506, grad_fn=<NegBackward0>) tensor(10888.1436, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10888.02734375
tensor(10888.1436, grad_fn=<NegBackward0>) tensor(10888.0273, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10887.939453125
tensor(10888.0273, grad_fn=<NegBackward0>) tensor(10887.9395, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10887.8681640625
tensor(10887.9395, grad_fn=<NegBackward0>) tensor(10887.8682, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10887.8046875
tensor(10887.8682, grad_fn=<NegBackward0>) tensor(10887.8047, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10887.7431640625
tensor(10887.8047, grad_fn=<NegBackward0>) tensor(10887.7432, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10887.685546875
tensor(10887.7432, grad_fn=<NegBackward0>) tensor(10887.6855, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10887.62890625
tensor(10887.6855, grad_fn=<NegBackward0>) tensor(10887.6289, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10887.57421875
tensor(10887.6289, grad_fn=<NegBackward0>) tensor(10887.5742, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10887.5234375
tensor(10887.5742, grad_fn=<NegBackward0>) tensor(10887.5234, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10887.474609375
tensor(10887.5234, grad_fn=<NegBackward0>) tensor(10887.4746, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10887.4296875
tensor(10887.4746, grad_fn=<NegBackward0>) tensor(10887.4297, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10887.384765625
tensor(10887.4297, grad_fn=<NegBackward0>) tensor(10887.3848, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10887.3466796875
tensor(10887.3848, grad_fn=<NegBackward0>) tensor(10887.3467, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10887.3095703125
tensor(10887.3467, grad_fn=<NegBackward0>) tensor(10887.3096, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10887.2744140625
tensor(10887.3096, grad_fn=<NegBackward0>) tensor(10887.2744, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10887.240234375
tensor(10887.2744, grad_fn=<NegBackward0>) tensor(10887.2402, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10887.205078125
tensor(10887.2402, grad_fn=<NegBackward0>) tensor(10887.2051, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10887.16796875
tensor(10887.2051, grad_fn=<NegBackward0>) tensor(10887.1680, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10887.1318359375
tensor(10887.1680, grad_fn=<NegBackward0>) tensor(10887.1318, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10887.0927734375
tensor(10887.1318, grad_fn=<NegBackward0>) tensor(10887.0928, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10887.0517578125
tensor(10887.0928, grad_fn=<NegBackward0>) tensor(10887.0518, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10887.009765625
tensor(10887.0518, grad_fn=<NegBackward0>) tensor(10887.0098, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10886.9677734375
tensor(10887.0098, grad_fn=<NegBackward0>) tensor(10886.9678, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10886.923828125
tensor(10886.9678, grad_fn=<NegBackward0>) tensor(10886.9238, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10886.8818359375
tensor(10886.9238, grad_fn=<NegBackward0>) tensor(10886.8818, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10886.841796875
tensor(10886.8818, grad_fn=<NegBackward0>) tensor(10886.8418, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10886.8017578125
tensor(10886.8418, grad_fn=<NegBackward0>) tensor(10886.8018, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10886.763671875
tensor(10886.8018, grad_fn=<NegBackward0>) tensor(10886.7637, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10886.724609375
tensor(10886.7637, grad_fn=<NegBackward0>) tensor(10886.7246, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10886.716796875
tensor(10886.7246, grad_fn=<NegBackward0>) tensor(10886.7168, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10886.6484375
tensor(10886.7168, grad_fn=<NegBackward0>) tensor(10886.6484, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10886.61328125
tensor(10886.6484, grad_fn=<NegBackward0>) tensor(10886.6133, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10886.5810546875
tensor(10886.6133, grad_fn=<NegBackward0>) tensor(10886.5811, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10886.556640625
tensor(10886.5811, grad_fn=<NegBackward0>) tensor(10886.5566, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10886.5380859375
tensor(10886.5566, grad_fn=<NegBackward0>) tensor(10886.5381, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10886.521484375
tensor(10886.5381, grad_fn=<NegBackward0>) tensor(10886.5215, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10886.5107421875
tensor(10886.5215, grad_fn=<NegBackward0>) tensor(10886.5107, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10886.5029296875
tensor(10886.5107, grad_fn=<NegBackward0>) tensor(10886.5029, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10886.4951171875
tensor(10886.5029, grad_fn=<NegBackward0>) tensor(10886.4951, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10886.48828125
tensor(10886.4951, grad_fn=<NegBackward0>) tensor(10886.4883, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10886.484375
tensor(10886.4883, grad_fn=<NegBackward0>) tensor(10886.4844, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10886.4794921875
tensor(10886.4844, grad_fn=<NegBackward0>) tensor(10886.4795, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10886.4765625
tensor(10886.4795, grad_fn=<NegBackward0>) tensor(10886.4766, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10886.474609375
tensor(10886.4766, grad_fn=<NegBackward0>) tensor(10886.4746, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10886.4716796875
tensor(10886.4746, grad_fn=<NegBackward0>) tensor(10886.4717, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10886.4697265625
tensor(10886.4717, grad_fn=<NegBackward0>) tensor(10886.4697, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10886.4658203125
tensor(10886.4697, grad_fn=<NegBackward0>) tensor(10886.4658, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10886.462890625
tensor(10886.4658, grad_fn=<NegBackward0>) tensor(10886.4629, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10886.4638671875
tensor(10886.4629, grad_fn=<NegBackward0>) tensor(10886.4639, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10886.4599609375
tensor(10886.4629, grad_fn=<NegBackward0>) tensor(10886.4600, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10886.45703125
tensor(10886.4600, grad_fn=<NegBackward0>) tensor(10886.4570, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10886.484375
tensor(10886.4570, grad_fn=<NegBackward0>) tensor(10886.4844, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10886.453125
tensor(10886.4570, grad_fn=<NegBackward0>) tensor(10886.4531, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10886.4501953125
tensor(10886.4531, grad_fn=<NegBackward0>) tensor(10886.4502, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10886.447265625
tensor(10886.4502, grad_fn=<NegBackward0>) tensor(10886.4473, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10886.4423828125
tensor(10886.4473, grad_fn=<NegBackward0>) tensor(10886.4424, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10886.4365234375
tensor(10886.4424, grad_fn=<NegBackward0>) tensor(10886.4365, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10886.4287109375
tensor(10886.4365, grad_fn=<NegBackward0>) tensor(10886.4287, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10886.4208984375
tensor(10886.4287, grad_fn=<NegBackward0>) tensor(10886.4209, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10886.4150390625
tensor(10886.4209, grad_fn=<NegBackward0>) tensor(10886.4150, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10886.41015625
tensor(10886.4150, grad_fn=<NegBackward0>) tensor(10886.4102, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10886.408203125
tensor(10886.4102, grad_fn=<NegBackward0>) tensor(10886.4082, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10886.4072265625
tensor(10886.4082, grad_fn=<NegBackward0>) tensor(10886.4072, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10886.4072265625
tensor(10886.4072, grad_fn=<NegBackward0>) tensor(10886.4072, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10886.4072265625
tensor(10886.4072, grad_fn=<NegBackward0>) tensor(10886.4072, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10886.40625
tensor(10886.4072, grad_fn=<NegBackward0>) tensor(10886.4062, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10886.4052734375
tensor(10886.4062, grad_fn=<NegBackward0>) tensor(10886.4053, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10886.4091796875
tensor(10886.4053, grad_fn=<NegBackward0>) tensor(10886.4092, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10886.4072265625
tensor(10886.4053, grad_fn=<NegBackward0>) tensor(10886.4072, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -10886.40625
tensor(10886.4053, grad_fn=<NegBackward0>) tensor(10886.4062, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -10886.40625
tensor(10886.4053, grad_fn=<NegBackward0>) tensor(10886.4062, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -10886.4072265625
tensor(10886.4053, grad_fn=<NegBackward0>) tensor(10886.4072, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7600 due to no improvement.
pi: tensor([[0.0359, 0.9641],
        [0.8728, 0.1272]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0121, 0.9879], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1575, 0.3072],
         [0.7110, 0.1607]],

        [[0.5781, 0.1695],
         [0.6250, 0.6664]],

        [[0.6416, 0.1576],
         [0.6032, 0.5534]],

        [[0.7117, 0.1493],
         [0.5493, 0.5424]],

        [[0.6949, 0.1645],
         [0.5622, 0.6120]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
Global Adjusted Rand Index: 0.0003490532159265987
Average Adjusted Rand Index: -0.0014983791324328355
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21906.44921875
inf tensor(21906.4492, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10889.21875
tensor(21906.4492, grad_fn=<NegBackward0>) tensor(10889.2188, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10888.0244140625
tensor(10889.2188, grad_fn=<NegBackward0>) tensor(10888.0244, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10887.7841796875
tensor(10888.0244, grad_fn=<NegBackward0>) tensor(10887.7842, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10887.611328125
tensor(10887.7842, grad_fn=<NegBackward0>) tensor(10887.6113, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10887.4794921875
tensor(10887.6113, grad_fn=<NegBackward0>) tensor(10887.4795, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10887.3837890625
tensor(10887.4795, grad_fn=<NegBackward0>) tensor(10887.3838, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10887.3154296875
tensor(10887.3838, grad_fn=<NegBackward0>) tensor(10887.3154, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10887.2666015625
tensor(10887.3154, grad_fn=<NegBackward0>) tensor(10887.2666, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10887.2294921875
tensor(10887.2666, grad_fn=<NegBackward0>) tensor(10887.2295, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10887.1953125
tensor(10887.2295, grad_fn=<NegBackward0>) tensor(10887.1953, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10887.1630859375
tensor(10887.1953, grad_fn=<NegBackward0>) tensor(10887.1631, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10887.1298828125
tensor(10887.1631, grad_fn=<NegBackward0>) tensor(10887.1299, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10887.09375
tensor(10887.1299, grad_fn=<NegBackward0>) tensor(10887.0938, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10887.052734375
tensor(10887.0938, grad_fn=<NegBackward0>) tensor(10887.0527, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10887.009765625
tensor(10887.0527, grad_fn=<NegBackward0>) tensor(10887.0098, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10886.962890625
tensor(10887.0098, grad_fn=<NegBackward0>) tensor(10886.9629, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10886.912109375
tensor(10886.9629, grad_fn=<NegBackward0>) tensor(10886.9121, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10886.8603515625
tensor(10886.9121, grad_fn=<NegBackward0>) tensor(10886.8604, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10886.80859375
tensor(10886.8604, grad_fn=<NegBackward0>) tensor(10886.8086, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10886.7568359375
tensor(10886.8086, grad_fn=<NegBackward0>) tensor(10886.7568, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10886.705078125
tensor(10886.7568, grad_fn=<NegBackward0>) tensor(10886.7051, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10886.646484375
tensor(10886.7051, grad_fn=<NegBackward0>) tensor(10886.6465, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10886.5908203125
tensor(10886.6465, grad_fn=<NegBackward0>) tensor(10886.5908, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10886.5673828125
tensor(10886.5908, grad_fn=<NegBackward0>) tensor(10886.5674, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10886.521484375
tensor(10886.5674, grad_fn=<NegBackward0>) tensor(10886.5215, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10886.5166015625
tensor(10886.5215, grad_fn=<NegBackward0>) tensor(10886.5166, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10886.4921875
tensor(10886.5166, grad_fn=<NegBackward0>) tensor(10886.4922, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10886.484375
tensor(10886.4922, grad_fn=<NegBackward0>) tensor(10886.4844, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10886.478515625
tensor(10886.4844, grad_fn=<NegBackward0>) tensor(10886.4785, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10886.474609375
tensor(10886.4785, grad_fn=<NegBackward0>) tensor(10886.4746, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10886.470703125
tensor(10886.4746, grad_fn=<NegBackward0>) tensor(10886.4707, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10886.466796875
tensor(10886.4707, grad_fn=<NegBackward0>) tensor(10886.4668, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10886.4638671875
tensor(10886.4668, grad_fn=<NegBackward0>) tensor(10886.4639, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10886.4609375
tensor(10886.4639, grad_fn=<NegBackward0>) tensor(10886.4609, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10886.458984375
tensor(10886.4609, grad_fn=<NegBackward0>) tensor(10886.4590, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10886.4560546875
tensor(10886.4590, grad_fn=<NegBackward0>) tensor(10886.4561, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10886.455078125
tensor(10886.4561, grad_fn=<NegBackward0>) tensor(10886.4551, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10886.44921875
tensor(10886.4551, grad_fn=<NegBackward0>) tensor(10886.4492, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10886.443359375
tensor(10886.4492, grad_fn=<NegBackward0>) tensor(10886.4434, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10886.4365234375
tensor(10886.4434, grad_fn=<NegBackward0>) tensor(10886.4365, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10886.427734375
tensor(10886.4365, grad_fn=<NegBackward0>) tensor(10886.4277, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10886.4189453125
tensor(10886.4277, grad_fn=<NegBackward0>) tensor(10886.4189, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10886.4130859375
tensor(10886.4189, grad_fn=<NegBackward0>) tensor(10886.4131, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10886.4091796875
tensor(10886.4131, grad_fn=<NegBackward0>) tensor(10886.4092, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10886.408203125
tensor(10886.4092, grad_fn=<NegBackward0>) tensor(10886.4082, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10886.4072265625
tensor(10886.4082, grad_fn=<NegBackward0>) tensor(10886.4072, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10886.4072265625
tensor(10886.4072, grad_fn=<NegBackward0>) tensor(10886.4072, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10886.4072265625
tensor(10886.4072, grad_fn=<NegBackward0>) tensor(10886.4072, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10886.4072265625
tensor(10886.4072, grad_fn=<NegBackward0>) tensor(10886.4072, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10886.4072265625
tensor(10886.4072, grad_fn=<NegBackward0>) tensor(10886.4072, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10886.40625
tensor(10886.4072, grad_fn=<NegBackward0>) tensor(10886.4062, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10886.4072265625
tensor(10886.4062, grad_fn=<NegBackward0>) tensor(10886.4072, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10886.4072265625
tensor(10886.4062, grad_fn=<NegBackward0>) tensor(10886.4072, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -10886.40625
tensor(10886.4062, grad_fn=<NegBackward0>) tensor(10886.4062, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10886.40625
tensor(10886.4062, grad_fn=<NegBackward0>) tensor(10886.4062, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10886.40625
tensor(10886.4062, grad_fn=<NegBackward0>) tensor(10886.4062, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10886.40625
tensor(10886.4062, grad_fn=<NegBackward0>) tensor(10886.4062, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10886.40625
tensor(10886.4062, grad_fn=<NegBackward0>) tensor(10886.4062, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10886.40625
tensor(10886.4062, grad_fn=<NegBackward0>) tensor(10886.4062, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10886.4052734375
tensor(10886.4062, grad_fn=<NegBackward0>) tensor(10886.4053, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10886.40625
tensor(10886.4053, grad_fn=<NegBackward0>) tensor(10886.4062, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10886.40625
tensor(10886.4053, grad_fn=<NegBackward0>) tensor(10886.4062, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -10886.4111328125
tensor(10886.4053, grad_fn=<NegBackward0>) tensor(10886.4111, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -10886.40625
tensor(10886.4053, grad_fn=<NegBackward0>) tensor(10886.4062, grad_fn=<NegBackward0>)
4
Iteration 6500: Loss = -10886.4111328125
tensor(10886.4053, grad_fn=<NegBackward0>) tensor(10886.4111, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6500 due to no improvement.
pi: tensor([[0.0422, 0.9578],
        [0.8750, 0.1250]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0121, 0.9879], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1575, 0.3073],
         [0.6385, 0.1607]],

        [[0.6054, 0.1696],
         [0.6495, 0.6309]],

        [[0.6950, 0.1575],
         [0.5864, 0.7199]],

        [[0.6943, 0.1493],
         [0.6931, 0.6949]],

        [[0.6062, 0.1645],
         [0.6439, 0.7241]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
Global Adjusted Rand Index: 0.0003490532159265987
Average Adjusted Rand Index: -0.0014983791324328355
[0.0003490532159265987, 0.0003490532159265987] [-0.0014983791324328355, -0.0014983791324328355] [10886.4072265625, 10886.4111328125]
-------------------------------------
This iteration is 8
True Objective function: Loss = -10726.131773118004
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21291.4765625
inf tensor(21291.4766, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10820.53125
tensor(21291.4766, grad_fn=<NegBackward0>) tensor(10820.5312, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10820.2705078125
tensor(10820.5312, grad_fn=<NegBackward0>) tensor(10820.2705, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10820.2333984375
tensor(10820.2705, grad_fn=<NegBackward0>) tensor(10820.2334, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10820.2119140625
tensor(10820.2334, grad_fn=<NegBackward0>) tensor(10820.2119, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10820.1865234375
tensor(10820.2119, grad_fn=<NegBackward0>) tensor(10820.1865, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10820.1494140625
tensor(10820.1865, grad_fn=<NegBackward0>) tensor(10820.1494, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10820.087890625
tensor(10820.1494, grad_fn=<NegBackward0>) tensor(10820.0879, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10819.99609375
tensor(10820.0879, grad_fn=<NegBackward0>) tensor(10819.9961, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10819.9267578125
tensor(10819.9961, grad_fn=<NegBackward0>) tensor(10819.9268, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10819.8974609375
tensor(10819.9268, grad_fn=<NegBackward0>) tensor(10819.8975, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10819.884765625
tensor(10819.8975, grad_fn=<NegBackward0>) tensor(10819.8848, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10819.8740234375
tensor(10819.8848, grad_fn=<NegBackward0>) tensor(10819.8740, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10819.86328125
tensor(10819.8740, grad_fn=<NegBackward0>) tensor(10819.8633, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10819.857421875
tensor(10819.8633, grad_fn=<NegBackward0>) tensor(10819.8574, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10819.8505859375
tensor(10819.8574, grad_fn=<NegBackward0>) tensor(10819.8506, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10819.845703125
tensor(10819.8506, grad_fn=<NegBackward0>) tensor(10819.8457, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10819.8408203125
tensor(10819.8457, grad_fn=<NegBackward0>) tensor(10819.8408, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10819.8359375
tensor(10819.8408, grad_fn=<NegBackward0>) tensor(10819.8359, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10819.830078125
tensor(10819.8359, grad_fn=<NegBackward0>) tensor(10819.8301, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10819.8173828125
tensor(10819.8301, grad_fn=<NegBackward0>) tensor(10819.8174, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10819.8017578125
tensor(10819.8174, grad_fn=<NegBackward0>) tensor(10819.8018, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10819.794921875
tensor(10819.8018, grad_fn=<NegBackward0>) tensor(10819.7949, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10819.7919921875
tensor(10819.7949, grad_fn=<NegBackward0>) tensor(10819.7920, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10819.7890625
tensor(10819.7920, grad_fn=<NegBackward0>) tensor(10819.7891, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10819.787109375
tensor(10819.7891, grad_fn=<NegBackward0>) tensor(10819.7871, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10819.78515625
tensor(10819.7871, grad_fn=<NegBackward0>) tensor(10819.7852, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10819.783203125
tensor(10819.7852, grad_fn=<NegBackward0>) tensor(10819.7832, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10819.78125
tensor(10819.7832, grad_fn=<NegBackward0>) tensor(10819.7812, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10819.7783203125
tensor(10819.7812, grad_fn=<NegBackward0>) tensor(10819.7783, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10819.77734375
tensor(10819.7783, grad_fn=<NegBackward0>) tensor(10819.7773, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10819.77734375
tensor(10819.7773, grad_fn=<NegBackward0>) tensor(10819.7773, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10819.7763671875
tensor(10819.7773, grad_fn=<NegBackward0>) tensor(10819.7764, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10819.7734375
tensor(10819.7764, grad_fn=<NegBackward0>) tensor(10819.7734, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10819.7724609375
tensor(10819.7734, grad_fn=<NegBackward0>) tensor(10819.7725, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10819.7705078125
tensor(10819.7725, grad_fn=<NegBackward0>) tensor(10819.7705, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10819.76953125
tensor(10819.7705, grad_fn=<NegBackward0>) tensor(10819.7695, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10819.76953125
tensor(10819.7695, grad_fn=<NegBackward0>) tensor(10819.7695, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10819.767578125
tensor(10819.7695, grad_fn=<NegBackward0>) tensor(10819.7676, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10819.7666015625
tensor(10819.7676, grad_fn=<NegBackward0>) tensor(10819.7666, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10819.7666015625
tensor(10819.7666, grad_fn=<NegBackward0>) tensor(10819.7666, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10819.765625
tensor(10819.7666, grad_fn=<NegBackward0>) tensor(10819.7656, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10819.763671875
tensor(10819.7656, grad_fn=<NegBackward0>) tensor(10819.7637, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10819.763671875
tensor(10819.7637, grad_fn=<NegBackward0>) tensor(10819.7637, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10819.7626953125
tensor(10819.7637, grad_fn=<NegBackward0>) tensor(10819.7627, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10819.7607421875
tensor(10819.7627, grad_fn=<NegBackward0>) tensor(10819.7607, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10819.7587890625
tensor(10819.7607, grad_fn=<NegBackward0>) tensor(10819.7588, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10819.7568359375
tensor(10819.7588, grad_fn=<NegBackward0>) tensor(10819.7568, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10819.7529296875
tensor(10819.7568, grad_fn=<NegBackward0>) tensor(10819.7529, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10819.7412109375
tensor(10819.7529, grad_fn=<NegBackward0>) tensor(10819.7412, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10819.7041015625
tensor(10819.7412, grad_fn=<NegBackward0>) tensor(10819.7041, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10819.4384765625
tensor(10819.7041, grad_fn=<NegBackward0>) tensor(10819.4385, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10818.4970703125
tensor(10819.4385, grad_fn=<NegBackward0>) tensor(10818.4971, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10818.49609375
tensor(10818.4971, grad_fn=<NegBackward0>) tensor(10818.4961, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10818.49609375
tensor(10818.4961, grad_fn=<NegBackward0>) tensor(10818.4961, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10818.49609375
tensor(10818.4961, grad_fn=<NegBackward0>) tensor(10818.4961, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10818.4970703125
tensor(10818.4961, grad_fn=<NegBackward0>) tensor(10818.4971, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10818.49609375
tensor(10818.4961, grad_fn=<NegBackward0>) tensor(10818.4961, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10818.4990234375
tensor(10818.4961, grad_fn=<NegBackward0>) tensor(10818.4990, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10818.49609375
tensor(10818.4961, grad_fn=<NegBackward0>) tensor(10818.4961, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10818.50390625
tensor(10818.4961, grad_fn=<NegBackward0>) tensor(10818.5039, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10818.4990234375
tensor(10818.4961, grad_fn=<NegBackward0>) tensor(10818.4990, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -10818.498046875
tensor(10818.4961, grad_fn=<NegBackward0>) tensor(10818.4980, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -10818.4970703125
tensor(10818.4961, grad_fn=<NegBackward0>) tensor(10818.4971, grad_fn=<NegBackward0>)
4
Iteration 6400: Loss = -10818.49609375
tensor(10818.4961, grad_fn=<NegBackward0>) tensor(10818.4961, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10818.5
tensor(10818.4961, grad_fn=<NegBackward0>) tensor(10818.5000, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10818.4951171875
tensor(10818.4961, grad_fn=<NegBackward0>) tensor(10818.4951, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10818.501953125
tensor(10818.4951, grad_fn=<NegBackward0>) tensor(10818.5020, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10818.49609375
tensor(10818.4951, grad_fn=<NegBackward0>) tensor(10818.4961, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -10818.505859375
tensor(10818.4951, grad_fn=<NegBackward0>) tensor(10818.5059, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -10818.5029296875
tensor(10818.4951, grad_fn=<NegBackward0>) tensor(10818.5029, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -10818.498046875
tensor(10818.4951, grad_fn=<NegBackward0>) tensor(10818.4980, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[2.9684e-01, 7.0316e-01],
        [9.9953e-01, 4.6732e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5523, 0.4477], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1400, 0.1583],
         [0.5632, 0.1847]],

        [[0.6642, 0.1645],
         [0.6409, 0.6556]],

        [[0.7127, 0.1642],
         [0.5329, 0.6569]],

        [[0.6239, 0.1591],
         [0.7006, 0.6180]],

        [[0.5550, 0.1637],
         [0.5988, 0.5627]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 63
Adjusted Rand Index: 0.05836628269944057
time is 1
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.001076281447598547
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.009400359857525796
time is 3
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 4
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.005775042517527009
Global Adjusted Rand Index: 0.01057042398154016
Average Adjusted Rand Index: 0.009742321243286152
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23402.83203125
inf tensor(23402.8320, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10821.26953125
tensor(23402.8320, grad_fn=<NegBackward0>) tensor(10821.2695, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10820.50390625
tensor(10821.2695, grad_fn=<NegBackward0>) tensor(10820.5039, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10820.33984375
tensor(10820.5039, grad_fn=<NegBackward0>) tensor(10820.3398, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10820.2705078125
tensor(10820.3398, grad_fn=<NegBackward0>) tensor(10820.2705, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10820.22265625
tensor(10820.2705, grad_fn=<NegBackward0>) tensor(10820.2227, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10820.1748046875
tensor(10820.2227, grad_fn=<NegBackward0>) tensor(10820.1748, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10820.123046875
tensor(10820.1748, grad_fn=<NegBackward0>) tensor(10820.1230, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10820.0712890625
tensor(10820.1230, grad_fn=<NegBackward0>) tensor(10820.0713, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10820.017578125
tensor(10820.0713, grad_fn=<NegBackward0>) tensor(10820.0176, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10819.962890625
tensor(10820.0176, grad_fn=<NegBackward0>) tensor(10819.9629, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10819.91015625
tensor(10819.9629, grad_fn=<NegBackward0>) tensor(10819.9102, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10819.857421875
tensor(10819.9102, grad_fn=<NegBackward0>) tensor(10819.8574, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10819.8154296875
tensor(10819.8574, grad_fn=<NegBackward0>) tensor(10819.8154, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10819.7822265625
tensor(10819.8154, grad_fn=<NegBackward0>) tensor(10819.7822, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10819.7578125
tensor(10819.7822, grad_fn=<NegBackward0>) tensor(10819.7578, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10819.7412109375
tensor(10819.7578, grad_fn=<NegBackward0>) tensor(10819.7412, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10819.728515625
tensor(10819.7412, grad_fn=<NegBackward0>) tensor(10819.7285, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10819.7177734375
tensor(10819.7285, grad_fn=<NegBackward0>) tensor(10819.7178, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10819.7099609375
tensor(10819.7178, grad_fn=<NegBackward0>) tensor(10819.7100, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10819.7041015625
tensor(10819.7100, grad_fn=<NegBackward0>) tensor(10819.7041, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10819.6982421875
tensor(10819.7041, grad_fn=<NegBackward0>) tensor(10819.6982, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10819.693359375
tensor(10819.6982, grad_fn=<NegBackward0>) tensor(10819.6934, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10819.69140625
tensor(10819.6934, grad_fn=<NegBackward0>) tensor(10819.6914, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10819.6865234375
tensor(10819.6914, grad_fn=<NegBackward0>) tensor(10819.6865, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10819.685546875
tensor(10819.6865, grad_fn=<NegBackward0>) tensor(10819.6855, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10819.681640625
tensor(10819.6855, grad_fn=<NegBackward0>) tensor(10819.6816, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10819.6796875
tensor(10819.6816, grad_fn=<NegBackward0>) tensor(10819.6797, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10819.6787109375
tensor(10819.6797, grad_fn=<NegBackward0>) tensor(10819.6787, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10819.67578125
tensor(10819.6787, grad_fn=<NegBackward0>) tensor(10819.6758, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10819.67578125
tensor(10819.6758, grad_fn=<NegBackward0>) tensor(10819.6758, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10819.6748046875
tensor(10819.6758, grad_fn=<NegBackward0>) tensor(10819.6748, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10819.6728515625
tensor(10819.6748, grad_fn=<NegBackward0>) tensor(10819.6729, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10819.6708984375
tensor(10819.6729, grad_fn=<NegBackward0>) tensor(10819.6709, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10819.6689453125
tensor(10819.6709, grad_fn=<NegBackward0>) tensor(10819.6689, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10819.6689453125
tensor(10819.6689, grad_fn=<NegBackward0>) tensor(10819.6689, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10819.66796875
tensor(10819.6689, grad_fn=<NegBackward0>) tensor(10819.6680, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10819.6669921875
tensor(10819.6680, grad_fn=<NegBackward0>) tensor(10819.6670, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10819.66796875
tensor(10819.6670, grad_fn=<NegBackward0>) tensor(10819.6680, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -10819.6669921875
tensor(10819.6670, grad_fn=<NegBackward0>) tensor(10819.6670, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10819.666015625
tensor(10819.6670, grad_fn=<NegBackward0>) tensor(10819.6660, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10819.6650390625
tensor(10819.6660, grad_fn=<NegBackward0>) tensor(10819.6650, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10819.6640625
tensor(10819.6650, grad_fn=<NegBackward0>) tensor(10819.6641, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10819.6630859375
tensor(10819.6641, grad_fn=<NegBackward0>) tensor(10819.6631, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10819.6630859375
tensor(10819.6631, grad_fn=<NegBackward0>) tensor(10819.6631, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10819.662109375
tensor(10819.6631, grad_fn=<NegBackward0>) tensor(10819.6621, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10819.6611328125
tensor(10819.6621, grad_fn=<NegBackward0>) tensor(10819.6611, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10819.66015625
tensor(10819.6611, grad_fn=<NegBackward0>) tensor(10819.6602, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10819.6591796875
tensor(10819.6602, grad_fn=<NegBackward0>) tensor(10819.6592, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10819.6572265625
tensor(10819.6592, grad_fn=<NegBackward0>) tensor(10819.6572, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10819.65625
tensor(10819.6572, grad_fn=<NegBackward0>) tensor(10819.6562, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10819.65234375
tensor(10819.6562, grad_fn=<NegBackward0>) tensor(10819.6523, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10819.6484375
tensor(10819.6523, grad_fn=<NegBackward0>) tensor(10819.6484, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10819.646484375
tensor(10819.6484, grad_fn=<NegBackward0>) tensor(10819.6465, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10819.6435546875
tensor(10819.6465, grad_fn=<NegBackward0>) tensor(10819.6436, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10819.642578125
tensor(10819.6436, grad_fn=<NegBackward0>) tensor(10819.6426, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10819.6416015625
tensor(10819.6426, grad_fn=<NegBackward0>) tensor(10819.6416, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10819.642578125
tensor(10819.6416, grad_fn=<NegBackward0>) tensor(10819.6426, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10819.6416015625
tensor(10819.6416, grad_fn=<NegBackward0>) tensor(10819.6416, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10819.640625
tensor(10819.6416, grad_fn=<NegBackward0>) tensor(10819.6406, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10819.6396484375
tensor(10819.6406, grad_fn=<NegBackward0>) tensor(10819.6396, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10819.638671875
tensor(10819.6396, grad_fn=<NegBackward0>) tensor(10819.6387, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10819.6396484375
tensor(10819.6387, grad_fn=<NegBackward0>) tensor(10819.6396, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10819.6396484375
tensor(10819.6387, grad_fn=<NegBackward0>) tensor(10819.6396, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -10819.6376953125
tensor(10819.6387, grad_fn=<NegBackward0>) tensor(10819.6377, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10819.6396484375
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6396, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10819.6396484375
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6396, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -10819.6396484375
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6396, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -10819.6376953125
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6377, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10819.638671875
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6387, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10819.638671875
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6387, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -10819.6376953125
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6377, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10819.638671875
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6387, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10819.6376953125
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6377, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10819.640625
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6406, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10819.638671875
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6387, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -10819.6396484375
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6396, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -10819.638671875
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6387, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -10819.6376953125
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6377, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10819.66796875
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6680, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10819.6376953125
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6377, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10819.6396484375
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6396, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10819.6591796875
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6592, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -10819.638671875
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6387, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -10819.6474609375
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6475, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -10819.6376953125
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6377, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10819.65234375
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6523, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10819.6376953125
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6377, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10819.77734375
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.7773, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10819.638671875
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6387, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -10819.63671875
tensor(10819.6377, grad_fn=<NegBackward0>) tensor(10819.6367, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10819.6376953125
tensor(10819.6367, grad_fn=<NegBackward0>) tensor(10819.6377, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10819.6376953125
tensor(10819.6367, grad_fn=<NegBackward0>) tensor(10819.6377, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -10819.6474609375
tensor(10819.6367, grad_fn=<NegBackward0>) tensor(10819.6475, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -10819.6376953125
tensor(10819.6367, grad_fn=<NegBackward0>) tensor(10819.6377, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -10819.67578125
tensor(10819.6367, grad_fn=<NegBackward0>) tensor(10819.6758, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9500 due to no improvement.
pi: tensor([[5.4114e-04, 9.9946e-01],
        [1.6130e-02, 9.8387e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0051, 0.9949], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2111, 0.0999],
         [0.7118, 0.1587]],

        [[0.7266, 0.2318],
         [0.5461, 0.6786]],

        [[0.5052, 0.2195],
         [0.6846, 0.5837]],

        [[0.5913, 0.0957],
         [0.6027, 0.5800]],

        [[0.7280, 0.1920],
         [0.6526, 0.6126]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.01057042398154016, 0.0] [0.009742321243286152, 0.0] [10818.498046875, 10819.67578125]
-------------------------------------
This iteration is 9
True Objective function: Loss = -10991.067278945342
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19785.02734375
inf tensor(19785.0273, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11092.619140625
tensor(19785.0273, grad_fn=<NegBackward0>) tensor(11092.6191, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11091.0302734375
tensor(11092.6191, grad_fn=<NegBackward0>) tensor(11091.0303, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11090.01171875
tensor(11091.0303, grad_fn=<NegBackward0>) tensor(11090.0117, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11089.8447265625
tensor(11090.0117, grad_fn=<NegBackward0>) tensor(11089.8447, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11089.73828125
tensor(11089.8447, grad_fn=<NegBackward0>) tensor(11089.7383, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11089.619140625
tensor(11089.7383, grad_fn=<NegBackward0>) tensor(11089.6191, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11089.3994140625
tensor(11089.6191, grad_fn=<NegBackward0>) tensor(11089.3994, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11088.7724609375
tensor(11089.3994, grad_fn=<NegBackward0>) tensor(11088.7725, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11085.681640625
tensor(11088.7725, grad_fn=<NegBackward0>) tensor(11085.6816, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11008.7294921875
tensor(11085.6816, grad_fn=<NegBackward0>) tensor(11008.7295, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11005.9140625
tensor(11008.7295, grad_fn=<NegBackward0>) tensor(11005.9141, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11005.5390625
tensor(11005.9141, grad_fn=<NegBackward0>) tensor(11005.5391, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11005.4599609375
tensor(11005.5391, grad_fn=<NegBackward0>) tensor(11005.4600, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11005.416015625
tensor(11005.4600, grad_fn=<NegBackward0>) tensor(11005.4160, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11005.306640625
tensor(11005.4160, grad_fn=<NegBackward0>) tensor(11005.3066, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11003.5546875
tensor(11005.3066, grad_fn=<NegBackward0>) tensor(11003.5547, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11003.146484375
tensor(11003.5547, grad_fn=<NegBackward0>) tensor(11003.1465, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11003.099609375
tensor(11003.1465, grad_fn=<NegBackward0>) tensor(11003.0996, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11003.07421875
tensor(11003.0996, grad_fn=<NegBackward0>) tensor(11003.0742, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11003.0244140625
tensor(11003.0742, grad_fn=<NegBackward0>) tensor(11003.0244, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11003.001953125
tensor(11003.0244, grad_fn=<NegBackward0>) tensor(11003.0020, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11002.99609375
tensor(11003.0020, grad_fn=<NegBackward0>) tensor(11002.9961, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11002.9921875
tensor(11002.9961, grad_fn=<NegBackward0>) tensor(11002.9922, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11002.9794921875
tensor(11002.9922, grad_fn=<NegBackward0>) tensor(11002.9795, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11002.931640625
tensor(11002.9795, grad_fn=<NegBackward0>) tensor(11002.9316, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11002.9296875
tensor(11002.9316, grad_fn=<NegBackward0>) tensor(11002.9297, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11002.9267578125
tensor(11002.9297, grad_fn=<NegBackward0>) tensor(11002.9268, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11002.9248046875
tensor(11002.9268, grad_fn=<NegBackward0>) tensor(11002.9248, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11002.923828125
tensor(11002.9248, grad_fn=<NegBackward0>) tensor(11002.9238, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11002.9208984375
tensor(11002.9238, grad_fn=<NegBackward0>) tensor(11002.9209, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11002.9169921875
tensor(11002.9209, grad_fn=<NegBackward0>) tensor(11002.9170, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11002.91015625
tensor(11002.9170, grad_fn=<NegBackward0>) tensor(11002.9102, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11002.8828125
tensor(11002.9102, grad_fn=<NegBackward0>) tensor(11002.8828, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11002.8798828125
tensor(11002.8828, grad_fn=<NegBackward0>) tensor(11002.8799, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11002.8798828125
tensor(11002.8799, grad_fn=<NegBackward0>) tensor(11002.8799, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11002.87890625
tensor(11002.8799, grad_fn=<NegBackward0>) tensor(11002.8789, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11002.87890625
tensor(11002.8789, grad_fn=<NegBackward0>) tensor(11002.8789, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11002.8759765625
tensor(11002.8789, grad_fn=<NegBackward0>) tensor(11002.8760, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11002.8740234375
tensor(11002.8760, grad_fn=<NegBackward0>) tensor(11002.8740, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11002.875
tensor(11002.8740, grad_fn=<NegBackward0>) tensor(11002.8750, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11002.8720703125
tensor(11002.8740, grad_fn=<NegBackward0>) tensor(11002.8721, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11002.818359375
tensor(11002.8721, grad_fn=<NegBackward0>) tensor(11002.8184, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11002.818359375
tensor(11002.8184, grad_fn=<NegBackward0>) tensor(11002.8184, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11002.818359375
tensor(11002.8184, grad_fn=<NegBackward0>) tensor(11002.8184, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11002.8173828125
tensor(11002.8184, grad_fn=<NegBackward0>) tensor(11002.8174, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11002.8173828125
tensor(11002.8174, grad_fn=<NegBackward0>) tensor(11002.8174, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11002.8173828125
tensor(11002.8174, grad_fn=<NegBackward0>) tensor(11002.8174, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11002.8154296875
tensor(11002.8174, grad_fn=<NegBackward0>) tensor(11002.8154, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11002.8173828125
tensor(11002.8154, grad_fn=<NegBackward0>) tensor(11002.8174, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11002.81640625
tensor(11002.8154, grad_fn=<NegBackward0>) tensor(11002.8164, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11002.8154296875
tensor(11002.8154, grad_fn=<NegBackward0>) tensor(11002.8154, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11002.826171875
tensor(11002.8154, grad_fn=<NegBackward0>) tensor(11002.8262, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11002.8154296875
tensor(11002.8154, grad_fn=<NegBackward0>) tensor(11002.8154, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11002.8154296875
tensor(11002.8154, grad_fn=<NegBackward0>) tensor(11002.8154, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11002.81640625
tensor(11002.8154, grad_fn=<NegBackward0>) tensor(11002.8164, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11002.8154296875
tensor(11002.8154, grad_fn=<NegBackward0>) tensor(11002.8154, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11002.814453125
tensor(11002.8154, grad_fn=<NegBackward0>) tensor(11002.8145, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11002.8134765625
tensor(11002.8145, grad_fn=<NegBackward0>) tensor(11002.8135, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11002.81640625
tensor(11002.8135, grad_fn=<NegBackward0>) tensor(11002.8164, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11002.8134765625
tensor(11002.8135, grad_fn=<NegBackward0>) tensor(11002.8135, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11002.8134765625
tensor(11002.8135, grad_fn=<NegBackward0>) tensor(11002.8135, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11002.8134765625
tensor(11002.8135, grad_fn=<NegBackward0>) tensor(11002.8135, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11002.8125
tensor(11002.8135, grad_fn=<NegBackward0>) tensor(11002.8125, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11002.818359375
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8184, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11002.8134765625
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8135, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11002.8134765625
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8135, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11002.8125
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8125, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11002.8134765625
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8135, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11002.8134765625
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8135, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11002.8125
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8125, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11002.8125
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8125, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11002.814453125
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8145, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11002.8125
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8125, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11002.8134765625
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8135, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11002.814453125
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8145, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11002.8125
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8125, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11002.830078125
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8301, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11002.8154296875
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8154, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11002.8173828125
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8174, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11002.8125
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8125, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11002.8115234375
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8115, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11002.8125
tensor(11002.8115, grad_fn=<NegBackward0>) tensor(11002.8125, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11002.8125
tensor(11002.8115, grad_fn=<NegBackward0>) tensor(11002.8125, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11002.8134765625
tensor(11002.8115, grad_fn=<NegBackward0>) tensor(11002.8135, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -11002.8125
tensor(11002.8115, grad_fn=<NegBackward0>) tensor(11002.8125, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -11002.8134765625
tensor(11002.8115, grad_fn=<NegBackward0>) tensor(11002.8135, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[0.7435, 0.2565],
        [0.1551, 0.8449]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9648, 0.0352], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1733, 0.0916],
         [0.5435, 0.2690]],

        [[0.6390, 0.1500],
         [0.5259, 0.5991]],

        [[0.6127, 0.1013],
         [0.6135, 0.6144]],

        [[0.6623, 0.1028],
         [0.5271, 0.5745]],

        [[0.6589, 0.1109],
         [0.6391, 0.6745]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
time is 1
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 77
Adjusted Rand Index: 0.2847975141013949
time is 2
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 92
Adjusted Rand Index: 0.7026396121386246
time is 3
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.7369913366172994
Global Adjusted Rand Index: 0.4083272048279664
Average Adjusted Rand Index: 0.5364462465297621
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22388.669921875
inf tensor(22388.6699, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11094.0537109375
tensor(22388.6699, grad_fn=<NegBackward0>) tensor(11094.0537, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11092.56640625
tensor(11094.0537, grad_fn=<NegBackward0>) tensor(11092.5664, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11091.7021484375
tensor(11092.5664, grad_fn=<NegBackward0>) tensor(11091.7021, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11090.861328125
tensor(11091.7021, grad_fn=<NegBackward0>) tensor(11090.8613, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11090.4755859375
tensor(11090.8613, grad_fn=<NegBackward0>) tensor(11090.4756, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11090.20703125
tensor(11090.4756, grad_fn=<NegBackward0>) tensor(11090.2070, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11089.9599609375
tensor(11090.2070, grad_fn=<NegBackward0>) tensor(11089.9600, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11089.7470703125
tensor(11089.9600, grad_fn=<NegBackward0>) tensor(11089.7471, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11089.5556640625
tensor(11089.7471, grad_fn=<NegBackward0>) tensor(11089.5557, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11089.318359375
tensor(11089.5557, grad_fn=<NegBackward0>) tensor(11089.3184, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11088.900390625
tensor(11089.3184, grad_fn=<NegBackward0>) tensor(11088.9004, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11087.9755859375
tensor(11088.9004, grad_fn=<NegBackward0>) tensor(11087.9756, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11082.625
tensor(11087.9756, grad_fn=<NegBackward0>) tensor(11082.6250, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11013.7236328125
tensor(11082.6250, grad_fn=<NegBackward0>) tensor(11013.7236, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11008.7998046875
tensor(11013.7236, grad_fn=<NegBackward0>) tensor(11008.7998, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11008.154296875
tensor(11008.7998, grad_fn=<NegBackward0>) tensor(11008.1543, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11007.474609375
tensor(11008.1543, grad_fn=<NegBackward0>) tensor(11007.4746, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11007.3623046875
tensor(11007.4746, grad_fn=<NegBackward0>) tensor(11007.3623, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11007.2724609375
tensor(11007.3623, grad_fn=<NegBackward0>) tensor(11007.2725, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11005.84765625
tensor(11007.2725, grad_fn=<NegBackward0>) tensor(11005.8477, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11005.6083984375
tensor(11005.8477, grad_fn=<NegBackward0>) tensor(11005.6084, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11005.58203125
tensor(11005.6084, grad_fn=<NegBackward0>) tensor(11005.5820, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11005.5546875
tensor(11005.5820, grad_fn=<NegBackward0>) tensor(11005.5547, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11005.431640625
tensor(11005.5547, grad_fn=<NegBackward0>) tensor(11005.4316, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11005.4013671875
tensor(11005.4316, grad_fn=<NegBackward0>) tensor(11005.4014, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11005.3876953125
tensor(11005.4014, grad_fn=<NegBackward0>) tensor(11005.3877, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11005.375
tensor(11005.3877, grad_fn=<NegBackward0>) tensor(11005.3750, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11005.3603515625
tensor(11005.3750, grad_fn=<NegBackward0>) tensor(11005.3604, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11005.3369140625
tensor(11005.3604, grad_fn=<NegBackward0>) tensor(11005.3369, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11005.2119140625
tensor(11005.3369, grad_fn=<NegBackward0>) tensor(11005.2119, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11004.0439453125
tensor(11005.2119, grad_fn=<NegBackward0>) tensor(11004.0439, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11003.7353515625
tensor(11004.0439, grad_fn=<NegBackward0>) tensor(11003.7354, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11003.5849609375
tensor(11003.7354, grad_fn=<NegBackward0>) tensor(11003.5850, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11003.3466796875
tensor(11003.5850, grad_fn=<NegBackward0>) tensor(11003.3467, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11003.046875
tensor(11003.3467, grad_fn=<NegBackward0>) tensor(11003.0469, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11003.0166015625
tensor(11003.0469, grad_fn=<NegBackward0>) tensor(11003.0166, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11002.99609375
tensor(11003.0166, grad_fn=<NegBackward0>) tensor(11002.9961, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11002.955078125
tensor(11002.9961, grad_fn=<NegBackward0>) tensor(11002.9551, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11002.9345703125
tensor(11002.9551, grad_fn=<NegBackward0>) tensor(11002.9346, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11002.9296875
tensor(11002.9346, grad_fn=<NegBackward0>) tensor(11002.9297, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11002.92578125
tensor(11002.9297, grad_fn=<NegBackward0>) tensor(11002.9258, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11002.923828125
tensor(11002.9258, grad_fn=<NegBackward0>) tensor(11002.9238, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11002.921875
tensor(11002.9238, grad_fn=<NegBackward0>) tensor(11002.9219, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11002.9267578125
tensor(11002.9219, grad_fn=<NegBackward0>) tensor(11002.9268, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11002.9150390625
tensor(11002.9219, grad_fn=<NegBackward0>) tensor(11002.9150, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11002.888671875
tensor(11002.9150, grad_fn=<NegBackward0>) tensor(11002.8887, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11002.8876953125
tensor(11002.8887, grad_fn=<NegBackward0>) tensor(11002.8877, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11002.8857421875
tensor(11002.8877, grad_fn=<NegBackward0>) tensor(11002.8857, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11002.8857421875
tensor(11002.8857, grad_fn=<NegBackward0>) tensor(11002.8857, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11002.8837890625
tensor(11002.8857, grad_fn=<NegBackward0>) tensor(11002.8838, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11002.8837890625
tensor(11002.8838, grad_fn=<NegBackward0>) tensor(11002.8838, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11002.8837890625
tensor(11002.8838, grad_fn=<NegBackward0>) tensor(11002.8838, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11002.8828125
tensor(11002.8838, grad_fn=<NegBackward0>) tensor(11002.8828, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11002.8828125
tensor(11002.8828, grad_fn=<NegBackward0>) tensor(11002.8828, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11002.8837890625
tensor(11002.8828, grad_fn=<NegBackward0>) tensor(11002.8838, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11002.8828125
tensor(11002.8828, grad_fn=<NegBackward0>) tensor(11002.8828, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11002.88671875
tensor(11002.8828, grad_fn=<NegBackward0>) tensor(11002.8867, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11002.8818359375
tensor(11002.8828, grad_fn=<NegBackward0>) tensor(11002.8818, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11002.884765625
tensor(11002.8818, grad_fn=<NegBackward0>) tensor(11002.8848, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11002.880859375
tensor(11002.8818, grad_fn=<NegBackward0>) tensor(11002.8809, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11002.900390625
tensor(11002.8809, grad_fn=<NegBackward0>) tensor(11002.9004, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11002.87890625
tensor(11002.8809, grad_fn=<NegBackward0>) tensor(11002.8789, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11002.876953125
tensor(11002.8789, grad_fn=<NegBackward0>) tensor(11002.8770, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11002.8740234375
tensor(11002.8770, grad_fn=<NegBackward0>) tensor(11002.8740, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11002.8720703125
tensor(11002.8740, grad_fn=<NegBackward0>) tensor(11002.8721, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11002.8720703125
tensor(11002.8721, grad_fn=<NegBackward0>) tensor(11002.8721, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11002.8720703125
tensor(11002.8721, grad_fn=<NegBackward0>) tensor(11002.8721, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11002.87109375
tensor(11002.8721, grad_fn=<NegBackward0>) tensor(11002.8711, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11002.8701171875
tensor(11002.8711, grad_fn=<NegBackward0>) tensor(11002.8701, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11002.8701171875
tensor(11002.8701, grad_fn=<NegBackward0>) tensor(11002.8701, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11002.818359375
tensor(11002.8701, grad_fn=<NegBackward0>) tensor(11002.8184, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11002.8173828125
tensor(11002.8184, grad_fn=<NegBackward0>) tensor(11002.8174, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11002.81640625
tensor(11002.8174, grad_fn=<NegBackward0>) tensor(11002.8164, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11002.8154296875
tensor(11002.8164, grad_fn=<NegBackward0>) tensor(11002.8154, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11002.8154296875
tensor(11002.8154, grad_fn=<NegBackward0>) tensor(11002.8154, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11002.814453125
tensor(11002.8154, grad_fn=<NegBackward0>) tensor(11002.8145, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11002.8154296875
tensor(11002.8145, grad_fn=<NegBackward0>) tensor(11002.8154, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11002.8134765625
tensor(11002.8145, grad_fn=<NegBackward0>) tensor(11002.8135, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11002.8134765625
tensor(11002.8135, grad_fn=<NegBackward0>) tensor(11002.8135, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11002.814453125
tensor(11002.8135, grad_fn=<NegBackward0>) tensor(11002.8145, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11002.8125
tensor(11002.8135, grad_fn=<NegBackward0>) tensor(11002.8125, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11002.8134765625
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8135, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11002.8134765625
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8135, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11002.8134765625
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8135, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -11002.837890625
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8379, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -11002.8134765625
tensor(11002.8125, grad_fn=<NegBackward0>) tensor(11002.8135, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[0.7431, 0.2569],
        [0.1550, 0.8450]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9648, 0.0352], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1734, 0.0916],
         [0.6027, 0.2688]],

        [[0.5741, 0.1499],
         [0.5376, 0.6266]],

        [[0.5040, 0.1013],
         [0.6662, 0.7011]],

        [[0.5164, 0.1028],
         [0.5178, 0.7126]],

        [[0.5811, 0.1109],
         [0.5126, 0.5990]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
time is 1
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 77
Adjusted Rand Index: 0.2847975141013949
time is 2
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 92
Adjusted Rand Index: 0.7026396121386246
time is 3
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.7369913366172994
Global Adjusted Rand Index: 0.4083272048279664
Average Adjusted Rand Index: 0.5364462465297621
[0.4083272048279664, 0.4083272048279664] [0.5364462465297621, 0.5364462465297621] [11002.8134765625, 11002.8134765625]
-------------------------------------
This iteration is 10
True Objective function: Loss = -10793.296586418353
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21867.78125
inf tensor(21867.7812, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10849.4052734375
tensor(21867.7812, grad_fn=<NegBackward0>) tensor(10849.4053, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10848.91015625
tensor(10849.4053, grad_fn=<NegBackward0>) tensor(10848.9102, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10848.7783203125
tensor(10848.9102, grad_fn=<NegBackward0>) tensor(10848.7783, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10848.7021484375
tensor(10848.7783, grad_fn=<NegBackward0>) tensor(10848.7021, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10848.634765625
tensor(10848.7021, grad_fn=<NegBackward0>) tensor(10848.6348, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10848.541015625
tensor(10848.6348, grad_fn=<NegBackward0>) tensor(10848.5410, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10848.3916015625
tensor(10848.5410, grad_fn=<NegBackward0>) tensor(10848.3916, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10848.244140625
tensor(10848.3916, grad_fn=<NegBackward0>) tensor(10848.2441, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10848.13671875
tensor(10848.2441, grad_fn=<NegBackward0>) tensor(10848.1367, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10848.029296875
tensor(10848.1367, grad_fn=<NegBackward0>) tensor(10848.0293, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10847.9189453125
tensor(10848.0293, grad_fn=<NegBackward0>) tensor(10847.9189, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10847.810546875
tensor(10847.9189, grad_fn=<NegBackward0>) tensor(10847.8105, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10847.703125
tensor(10847.8105, grad_fn=<NegBackward0>) tensor(10847.7031, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10847.595703125
tensor(10847.7031, grad_fn=<NegBackward0>) tensor(10847.5957, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10847.501953125
tensor(10847.5957, grad_fn=<NegBackward0>) tensor(10847.5020, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10847.4375
tensor(10847.5020, grad_fn=<NegBackward0>) tensor(10847.4375, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10847.404296875
tensor(10847.4375, grad_fn=<NegBackward0>) tensor(10847.4043, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10847.3896484375
tensor(10847.4043, grad_fn=<NegBackward0>) tensor(10847.3896, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10847.3837890625
tensor(10847.3896, grad_fn=<NegBackward0>) tensor(10847.3838, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10847.380859375
tensor(10847.3838, grad_fn=<NegBackward0>) tensor(10847.3809, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10847.3759765625
tensor(10847.3809, grad_fn=<NegBackward0>) tensor(10847.3760, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10847.375
tensor(10847.3760, grad_fn=<NegBackward0>) tensor(10847.3750, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10847.3740234375
tensor(10847.3750, grad_fn=<NegBackward0>) tensor(10847.3740, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10847.3720703125
tensor(10847.3740, grad_fn=<NegBackward0>) tensor(10847.3721, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10847.369140625
tensor(10847.3721, grad_fn=<NegBackward0>) tensor(10847.3691, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10847.3671875
tensor(10847.3691, grad_fn=<NegBackward0>) tensor(10847.3672, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10847.3662109375
tensor(10847.3672, grad_fn=<NegBackward0>) tensor(10847.3662, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10847.365234375
tensor(10847.3662, grad_fn=<NegBackward0>) tensor(10847.3652, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10847.36328125
tensor(10847.3652, grad_fn=<NegBackward0>) tensor(10847.3633, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10847.3623046875
tensor(10847.3633, grad_fn=<NegBackward0>) tensor(10847.3623, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10847.3623046875
tensor(10847.3623, grad_fn=<NegBackward0>) tensor(10847.3623, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10847.361328125
tensor(10847.3623, grad_fn=<NegBackward0>) tensor(10847.3613, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10847.361328125
tensor(10847.3613, grad_fn=<NegBackward0>) tensor(10847.3613, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10847.361328125
tensor(10847.3613, grad_fn=<NegBackward0>) tensor(10847.3613, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10847.3603515625
tensor(10847.3613, grad_fn=<NegBackward0>) tensor(10847.3604, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10847.359375
tensor(10847.3604, grad_fn=<NegBackward0>) tensor(10847.3594, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10847.359375
tensor(10847.3594, grad_fn=<NegBackward0>) tensor(10847.3594, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10847.357421875
tensor(10847.3594, grad_fn=<NegBackward0>) tensor(10847.3574, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10847.357421875
tensor(10847.3574, grad_fn=<NegBackward0>) tensor(10847.3574, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10847.35546875
tensor(10847.3574, grad_fn=<NegBackward0>) tensor(10847.3555, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10847.3525390625
tensor(10847.3555, grad_fn=<NegBackward0>) tensor(10847.3525, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10847.349609375
tensor(10847.3525, grad_fn=<NegBackward0>) tensor(10847.3496, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10847.34765625
tensor(10847.3496, grad_fn=<NegBackward0>) tensor(10847.3477, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10847.341796875
tensor(10847.3477, grad_fn=<NegBackward0>) tensor(10847.3418, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10847.33203125
tensor(10847.3418, grad_fn=<NegBackward0>) tensor(10847.3320, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10847.3154296875
tensor(10847.3320, grad_fn=<NegBackward0>) tensor(10847.3154, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10847.287109375
tensor(10847.3154, grad_fn=<NegBackward0>) tensor(10847.2871, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10847.2373046875
tensor(10847.2871, grad_fn=<NegBackward0>) tensor(10847.2373, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10847.15234375
tensor(10847.2373, grad_fn=<NegBackward0>) tensor(10847.1523, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10847.0341796875
tensor(10847.1523, grad_fn=<NegBackward0>) tensor(10847.0342, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10846.8798828125
tensor(10847.0342, grad_fn=<NegBackward0>) tensor(10846.8799, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10846.7333984375
tensor(10846.8799, grad_fn=<NegBackward0>) tensor(10846.7334, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10846.6669921875
tensor(10846.7334, grad_fn=<NegBackward0>) tensor(10846.6670, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10846.6376953125
tensor(10846.6670, grad_fn=<NegBackward0>) tensor(10846.6377, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10846.62109375
tensor(10846.6377, grad_fn=<NegBackward0>) tensor(10846.6211, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10846.61328125
tensor(10846.6211, grad_fn=<NegBackward0>) tensor(10846.6133, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10846.6044921875
tensor(10846.6133, grad_fn=<NegBackward0>) tensor(10846.6045, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10846.6015625
tensor(10846.6045, grad_fn=<NegBackward0>) tensor(10846.6016, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10846.5966796875
tensor(10846.6016, grad_fn=<NegBackward0>) tensor(10846.5967, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10846.595703125
tensor(10846.5967, grad_fn=<NegBackward0>) tensor(10846.5957, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10846.5927734375
tensor(10846.5957, grad_fn=<NegBackward0>) tensor(10846.5928, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10846.5888671875
tensor(10846.5928, grad_fn=<NegBackward0>) tensor(10846.5889, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10846.58984375
tensor(10846.5889, grad_fn=<NegBackward0>) tensor(10846.5898, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10846.587890625
tensor(10846.5889, grad_fn=<NegBackward0>) tensor(10846.5879, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10846.587890625
tensor(10846.5879, grad_fn=<NegBackward0>) tensor(10846.5879, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10846.5869140625
tensor(10846.5879, grad_fn=<NegBackward0>) tensor(10846.5869, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10846.5859375
tensor(10846.5869, grad_fn=<NegBackward0>) tensor(10846.5859, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10846.5849609375
tensor(10846.5859, grad_fn=<NegBackward0>) tensor(10846.5850, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10846.5849609375
tensor(10846.5850, grad_fn=<NegBackward0>) tensor(10846.5850, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10846.583984375
tensor(10846.5850, grad_fn=<NegBackward0>) tensor(10846.5840, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10846.58203125
tensor(10846.5840, grad_fn=<NegBackward0>) tensor(10846.5820, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10846.583984375
tensor(10846.5820, grad_fn=<NegBackward0>) tensor(10846.5840, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10846.5830078125
tensor(10846.5820, grad_fn=<NegBackward0>) tensor(10846.5830, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -10846.5830078125
tensor(10846.5820, grad_fn=<NegBackward0>) tensor(10846.5830, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -10846.5810546875
tensor(10846.5820, grad_fn=<NegBackward0>) tensor(10846.5811, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10846.5810546875
tensor(10846.5811, grad_fn=<NegBackward0>) tensor(10846.5811, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10846.58203125
tensor(10846.5811, grad_fn=<NegBackward0>) tensor(10846.5820, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10846.58203125
tensor(10846.5811, grad_fn=<NegBackward0>) tensor(10846.5820, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -10846.5830078125
tensor(10846.5811, grad_fn=<NegBackward0>) tensor(10846.5830, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -10846.5849609375
tensor(10846.5811, grad_fn=<NegBackward0>) tensor(10846.5850, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -10846.6025390625
tensor(10846.5811, grad_fn=<NegBackward0>) tensor(10846.6025, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8100 due to no improvement.
pi: tensor([[9.9947e-01, 5.3267e-04],
        [5.6366e-05, 9.9994e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0452, 0.9548], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1707, 0.1197],
         [0.6246, 0.1584]],

        [[0.5096, 0.1844],
         [0.5522, 0.6666]],

        [[0.5798, 0.2182],
         [0.6197, 0.5831]],

        [[0.6912, 0.1876],
         [0.5504, 0.6293]],

        [[0.6902, 0.1453],
         [0.5484, 0.6834]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 39
Adjusted Rand Index: 0.0031854590296234485
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 39
Adjusted Rand Index: -0.015623423336712405
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 37
Adjusted Rand Index: 0.004868348130358715
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
Global Adjusted Rand Index: -0.0004897857772189348
Average Adjusted Rand Index: -0.0016731088188083516
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21230.1875
inf tensor(21230.1875, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10849.6875
tensor(21230.1875, grad_fn=<NegBackward0>) tensor(10849.6875, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10849.05859375
tensor(10849.6875, grad_fn=<NegBackward0>) tensor(10849.0586, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10848.853515625
tensor(10849.0586, grad_fn=<NegBackward0>) tensor(10848.8535, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10848.6845703125
tensor(10848.8535, grad_fn=<NegBackward0>) tensor(10848.6846, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10848.484375
tensor(10848.6846, grad_fn=<NegBackward0>) tensor(10848.4844, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10848.298828125
tensor(10848.4844, grad_fn=<NegBackward0>) tensor(10848.2988, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10848.18359375
tensor(10848.2988, grad_fn=<NegBackward0>) tensor(10848.1836, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10848.1005859375
tensor(10848.1836, grad_fn=<NegBackward0>) tensor(10848.1006, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10848.0283203125
tensor(10848.1006, grad_fn=<NegBackward0>) tensor(10848.0283, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10847.962890625
tensor(10848.0283, grad_fn=<NegBackward0>) tensor(10847.9629, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10847.8955078125
tensor(10847.9629, grad_fn=<NegBackward0>) tensor(10847.8955, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10847.8232421875
tensor(10847.8955, grad_fn=<NegBackward0>) tensor(10847.8232, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10847.748046875
tensor(10847.8232, grad_fn=<NegBackward0>) tensor(10847.7480, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10847.669921875
tensor(10847.7480, grad_fn=<NegBackward0>) tensor(10847.6699, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10847.5947265625
tensor(10847.6699, grad_fn=<NegBackward0>) tensor(10847.5947, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10847.52734375
tensor(10847.5947, grad_fn=<NegBackward0>) tensor(10847.5273, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10847.47265625
tensor(10847.5273, grad_fn=<NegBackward0>) tensor(10847.4727, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10847.435546875
tensor(10847.4727, grad_fn=<NegBackward0>) tensor(10847.4355, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10847.4091796875
tensor(10847.4355, grad_fn=<NegBackward0>) tensor(10847.4092, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10847.392578125
tensor(10847.4092, grad_fn=<NegBackward0>) tensor(10847.3926, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10847.3818359375
tensor(10847.3926, grad_fn=<NegBackward0>) tensor(10847.3818, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10847.375
tensor(10847.3818, grad_fn=<NegBackward0>) tensor(10847.3750, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10847.3701171875
tensor(10847.3750, grad_fn=<NegBackward0>) tensor(10847.3701, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10847.3671875
tensor(10847.3701, grad_fn=<NegBackward0>) tensor(10847.3672, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10847.365234375
tensor(10847.3672, grad_fn=<NegBackward0>) tensor(10847.3652, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10847.36328125
tensor(10847.3652, grad_fn=<NegBackward0>) tensor(10847.3633, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10847.36328125
tensor(10847.3633, grad_fn=<NegBackward0>) tensor(10847.3633, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10847.361328125
tensor(10847.3633, grad_fn=<NegBackward0>) tensor(10847.3613, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10847.361328125
tensor(10847.3613, grad_fn=<NegBackward0>) tensor(10847.3613, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10847.361328125
tensor(10847.3613, grad_fn=<NegBackward0>) tensor(10847.3613, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10847.359375
tensor(10847.3613, grad_fn=<NegBackward0>) tensor(10847.3594, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10847.361328125
tensor(10847.3594, grad_fn=<NegBackward0>) tensor(10847.3613, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -10847.3603515625
tensor(10847.3594, grad_fn=<NegBackward0>) tensor(10847.3604, grad_fn=<NegBackward0>)
2
Iteration 3400: Loss = -10847.3603515625
tensor(10847.3594, grad_fn=<NegBackward0>) tensor(10847.3604, grad_fn=<NegBackward0>)
3
Iteration 3500: Loss = -10847.3583984375
tensor(10847.3594, grad_fn=<NegBackward0>) tensor(10847.3584, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10847.357421875
tensor(10847.3584, grad_fn=<NegBackward0>) tensor(10847.3574, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10847.3583984375
tensor(10847.3574, grad_fn=<NegBackward0>) tensor(10847.3584, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -10847.3583984375
tensor(10847.3574, grad_fn=<NegBackward0>) tensor(10847.3584, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -10847.3583984375
tensor(10847.3574, grad_fn=<NegBackward0>) tensor(10847.3584, grad_fn=<NegBackward0>)
3
Iteration 4000: Loss = -10847.3583984375
tensor(10847.3574, grad_fn=<NegBackward0>) tensor(10847.3584, grad_fn=<NegBackward0>)
4
Iteration 4100: Loss = -10847.3583984375
tensor(10847.3574, grad_fn=<NegBackward0>) tensor(10847.3584, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4100 due to no improvement.
pi: tensor([[0.9271, 0.0729],
        [0.9134, 0.0866]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9862, 0.0138], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1572, 0.1278],
         [0.7253, 0.1921]],

        [[0.5585, 0.1673],
         [0.6720, 0.7234]],

        [[0.6358, 0.2063],
         [0.5224, 0.5106]],

        [[0.6986, 0.1628],
         [0.6439, 0.7291]],

        [[0.5283, 0.1652],
         [0.5339, 0.5458]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 61
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 63
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 63
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[-0.0004897857772189348, 0.0] [-0.0016731088188083516, 0.0] [10846.6025390625, 10847.3583984375]
-------------------------------------
This iteration is 11
True Objective function: Loss = -10806.92067142735
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21529.611328125
inf tensor(21529.6113, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10903.931640625
tensor(21529.6113, grad_fn=<NegBackward0>) tensor(10903.9316, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10903.458984375
tensor(10903.9316, grad_fn=<NegBackward0>) tensor(10903.4590, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10903.3291015625
tensor(10903.4590, grad_fn=<NegBackward0>) tensor(10903.3291, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10903.251953125
tensor(10903.3291, grad_fn=<NegBackward0>) tensor(10903.2520, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10903.1943359375
tensor(10903.2520, grad_fn=<NegBackward0>) tensor(10903.1943, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10903.1455078125
tensor(10903.1943, grad_fn=<NegBackward0>) tensor(10903.1455, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10903.1015625
tensor(10903.1455, grad_fn=<NegBackward0>) tensor(10903.1016, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10903.05859375
tensor(10903.1016, grad_fn=<NegBackward0>) tensor(10903.0586, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10903.015625
tensor(10903.0586, grad_fn=<NegBackward0>) tensor(10903.0156, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10902.9697265625
tensor(10903.0156, grad_fn=<NegBackward0>) tensor(10902.9697, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10902.9169921875
tensor(10902.9697, grad_fn=<NegBackward0>) tensor(10902.9170, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10902.8564453125
tensor(10902.9170, grad_fn=<NegBackward0>) tensor(10902.8564, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10902.7890625
tensor(10902.8564, grad_fn=<NegBackward0>) tensor(10902.7891, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10902.7158203125
tensor(10902.7891, grad_fn=<NegBackward0>) tensor(10902.7158, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10902.6416015625
tensor(10902.7158, grad_fn=<NegBackward0>) tensor(10902.6416, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10902.5673828125
tensor(10902.6416, grad_fn=<NegBackward0>) tensor(10902.5674, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10902.4970703125
tensor(10902.5674, grad_fn=<NegBackward0>) tensor(10902.4971, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10902.4296875
tensor(10902.4971, grad_fn=<NegBackward0>) tensor(10902.4297, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10902.3681640625
tensor(10902.4297, grad_fn=<NegBackward0>) tensor(10902.3682, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10902.3134765625
tensor(10902.3682, grad_fn=<NegBackward0>) tensor(10902.3135, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10902.2646484375
tensor(10902.3135, grad_fn=<NegBackward0>) tensor(10902.2646, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10902.2177734375
tensor(10902.2646, grad_fn=<NegBackward0>) tensor(10902.2178, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10902.173828125
tensor(10902.2178, grad_fn=<NegBackward0>) tensor(10902.1738, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10902.1328125
tensor(10902.1738, grad_fn=<NegBackward0>) tensor(10902.1328, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10902.0966796875
tensor(10902.1328, grad_fn=<NegBackward0>) tensor(10902.0967, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10902.0654296875
tensor(10902.0967, grad_fn=<NegBackward0>) tensor(10902.0654, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10902.0390625
tensor(10902.0654, grad_fn=<NegBackward0>) tensor(10902.0391, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10902.0205078125
tensor(10902.0391, grad_fn=<NegBackward0>) tensor(10902.0205, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10902.001953125
tensor(10902.0205, grad_fn=<NegBackward0>) tensor(10902.0020, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10901.990234375
tensor(10902.0020, grad_fn=<NegBackward0>) tensor(10901.9902, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10901.9814453125
tensor(10901.9902, grad_fn=<NegBackward0>) tensor(10901.9814, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10901.97265625
tensor(10901.9814, grad_fn=<NegBackward0>) tensor(10901.9727, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10901.9677734375
tensor(10901.9727, grad_fn=<NegBackward0>) tensor(10901.9678, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10901.9638671875
tensor(10901.9678, grad_fn=<NegBackward0>) tensor(10901.9639, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10901.9609375
tensor(10901.9639, grad_fn=<NegBackward0>) tensor(10901.9609, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10901.9560546875
tensor(10901.9609, grad_fn=<NegBackward0>) tensor(10901.9561, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10901.9541015625
tensor(10901.9561, grad_fn=<NegBackward0>) tensor(10901.9541, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10901.9521484375
tensor(10901.9541, grad_fn=<NegBackward0>) tensor(10901.9521, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10901.9501953125
tensor(10901.9521, grad_fn=<NegBackward0>) tensor(10901.9502, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10901.9482421875
tensor(10901.9502, grad_fn=<NegBackward0>) tensor(10901.9482, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10901.9443359375
tensor(10901.9482, grad_fn=<NegBackward0>) tensor(10901.9443, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10901.9404296875
tensor(10901.9443, grad_fn=<NegBackward0>) tensor(10901.9404, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10901.935546875
tensor(10901.9404, grad_fn=<NegBackward0>) tensor(10901.9355, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10901.9306640625
tensor(10901.9355, grad_fn=<NegBackward0>) tensor(10901.9307, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10901.923828125
tensor(10901.9307, grad_fn=<NegBackward0>) tensor(10901.9238, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10901.9130859375
tensor(10901.9238, grad_fn=<NegBackward0>) tensor(10901.9131, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10901.896484375
tensor(10901.9131, grad_fn=<NegBackward0>) tensor(10901.8965, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10901.8759765625
tensor(10901.8965, grad_fn=<NegBackward0>) tensor(10901.8760, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10901.853515625
tensor(10901.8760, grad_fn=<NegBackward0>) tensor(10901.8535, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10901.83203125
tensor(10901.8535, grad_fn=<NegBackward0>) tensor(10901.8320, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10901.8154296875
tensor(10901.8320, grad_fn=<NegBackward0>) tensor(10901.8154, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10901.8017578125
tensor(10901.8154, grad_fn=<NegBackward0>) tensor(10901.8018, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10901.7958984375
tensor(10901.8018, grad_fn=<NegBackward0>) tensor(10901.7959, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10901.78515625
tensor(10901.7959, grad_fn=<NegBackward0>) tensor(10901.7852, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10901.7783203125
tensor(10901.7852, grad_fn=<NegBackward0>) tensor(10901.7783, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10901.775390625
tensor(10901.7783, grad_fn=<NegBackward0>) tensor(10901.7754, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10901.7705078125
tensor(10901.7754, grad_fn=<NegBackward0>) tensor(10901.7705, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10901.7685546875
tensor(10901.7705, grad_fn=<NegBackward0>) tensor(10901.7686, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10901.763671875
tensor(10901.7686, grad_fn=<NegBackward0>) tensor(10901.7637, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10901.76171875
tensor(10901.7637, grad_fn=<NegBackward0>) tensor(10901.7617, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10901.759765625
tensor(10901.7617, grad_fn=<NegBackward0>) tensor(10901.7598, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10901.7578125
tensor(10901.7598, grad_fn=<NegBackward0>) tensor(10901.7578, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10901.7587890625
tensor(10901.7578, grad_fn=<NegBackward0>) tensor(10901.7588, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10901.7568359375
tensor(10901.7578, grad_fn=<NegBackward0>) tensor(10901.7568, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10901.755859375
tensor(10901.7568, grad_fn=<NegBackward0>) tensor(10901.7559, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10901.7568359375
tensor(10901.7559, grad_fn=<NegBackward0>) tensor(10901.7568, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10901.7548828125
tensor(10901.7559, grad_fn=<NegBackward0>) tensor(10901.7549, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10901.7548828125
tensor(10901.7549, grad_fn=<NegBackward0>) tensor(10901.7549, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10901.75390625
tensor(10901.7549, grad_fn=<NegBackward0>) tensor(10901.7539, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10901.7548828125
tensor(10901.7539, grad_fn=<NegBackward0>) tensor(10901.7549, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10901.759765625
tensor(10901.7539, grad_fn=<NegBackward0>) tensor(10901.7598, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -10901.765625
tensor(10901.7539, grad_fn=<NegBackward0>) tensor(10901.7656, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -10901.75390625
tensor(10901.7539, grad_fn=<NegBackward0>) tensor(10901.7539, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10901.75390625
tensor(10901.7539, grad_fn=<NegBackward0>) tensor(10901.7539, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10901.7548828125
tensor(10901.7539, grad_fn=<NegBackward0>) tensor(10901.7549, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10901.75390625
tensor(10901.7539, grad_fn=<NegBackward0>) tensor(10901.7539, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10901.75390625
tensor(10901.7539, grad_fn=<NegBackward0>) tensor(10901.7539, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10901.763671875
tensor(10901.7539, grad_fn=<NegBackward0>) tensor(10901.7637, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10901.7529296875
tensor(10901.7539, grad_fn=<NegBackward0>) tensor(10901.7529, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10901.75390625
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.7539, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10901.75390625
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.7539, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -10901.7666015625
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.7666, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -10901.75390625
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.7539, grad_fn=<NegBackward0>)
4
Iteration 8400: Loss = -10901.7841796875
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.7842, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8400 due to no improvement.
pi: tensor([[0.8661, 0.1339],
        [0.6820, 0.3180]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0056, 0.9944], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1640, 0.0823],
         [0.5104, 0.1557]],

        [[0.7181, 0.1579],
         [0.5315, 0.6134]],

        [[0.6133, 0.1725],
         [0.5823, 0.6717]],

        [[0.6699, 0.1582],
         [0.7151, 0.6110]],

        [[0.5955, 0.1526],
         [0.5259, 0.6134]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00021449335873650538
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21539.607421875
inf tensor(21539.6074, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10904.3388671875
tensor(21539.6074, grad_fn=<NegBackward0>) tensor(10904.3389, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10903.6494140625
tensor(10904.3389, grad_fn=<NegBackward0>) tensor(10903.6494, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10903.435546875
tensor(10903.6494, grad_fn=<NegBackward0>) tensor(10903.4355, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10903.3310546875
tensor(10903.4355, grad_fn=<NegBackward0>) tensor(10903.3311, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10903.259765625
tensor(10903.3311, grad_fn=<NegBackward0>) tensor(10903.2598, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10903.2041015625
tensor(10903.2598, grad_fn=<NegBackward0>) tensor(10903.2041, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10903.158203125
tensor(10903.2041, grad_fn=<NegBackward0>) tensor(10903.1582, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10903.119140625
tensor(10903.1582, grad_fn=<NegBackward0>) tensor(10903.1191, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10903.083984375
tensor(10903.1191, grad_fn=<NegBackward0>) tensor(10903.0840, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10903.0498046875
tensor(10903.0840, grad_fn=<NegBackward0>) tensor(10903.0498, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10903.0146484375
tensor(10903.0498, grad_fn=<NegBackward0>) tensor(10903.0146, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10902.9765625
tensor(10903.0146, grad_fn=<NegBackward0>) tensor(10902.9766, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10902.9306640625
tensor(10902.9766, grad_fn=<NegBackward0>) tensor(10902.9307, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10902.8759765625
tensor(10902.9307, grad_fn=<NegBackward0>) tensor(10902.8760, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10902.810546875
tensor(10902.8760, grad_fn=<NegBackward0>) tensor(10902.8105, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10902.73046875
tensor(10902.8105, grad_fn=<NegBackward0>) tensor(10902.7305, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10902.6435546875
tensor(10902.7305, grad_fn=<NegBackward0>) tensor(10902.6436, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10902.55859375
tensor(10902.6436, grad_fn=<NegBackward0>) tensor(10902.5586, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10902.478515625
tensor(10902.5586, grad_fn=<NegBackward0>) tensor(10902.4785, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10902.40625
tensor(10902.4785, grad_fn=<NegBackward0>) tensor(10902.4062, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10902.3408203125
tensor(10902.4062, grad_fn=<NegBackward0>) tensor(10902.3408, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10902.2822265625
tensor(10902.3408, grad_fn=<NegBackward0>) tensor(10902.2822, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10902.2275390625
tensor(10902.2822, grad_fn=<NegBackward0>) tensor(10902.2275, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10902.177734375
tensor(10902.2275, grad_fn=<NegBackward0>) tensor(10902.1777, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10902.1357421875
tensor(10902.1777, grad_fn=<NegBackward0>) tensor(10902.1357, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10902.0966796875
tensor(10902.1357, grad_fn=<NegBackward0>) tensor(10902.0967, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10902.0634765625
tensor(10902.0967, grad_fn=<NegBackward0>) tensor(10902.0635, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10902.0390625
tensor(10902.0635, grad_fn=<NegBackward0>) tensor(10902.0391, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10902.0205078125
tensor(10902.0391, grad_fn=<NegBackward0>) tensor(10902.0205, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10902.0048828125
tensor(10902.0205, grad_fn=<NegBackward0>) tensor(10902.0049, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10901.9921875
tensor(10902.0049, grad_fn=<NegBackward0>) tensor(10901.9922, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10901.9833984375
tensor(10901.9922, grad_fn=<NegBackward0>) tensor(10901.9834, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10901.978515625
tensor(10901.9834, grad_fn=<NegBackward0>) tensor(10901.9785, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10901.9736328125
tensor(10901.9785, grad_fn=<NegBackward0>) tensor(10901.9736, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10901.96875
tensor(10901.9736, grad_fn=<NegBackward0>) tensor(10901.9688, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10901.96484375
tensor(10901.9688, grad_fn=<NegBackward0>) tensor(10901.9648, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10901.962890625
tensor(10901.9648, grad_fn=<NegBackward0>) tensor(10901.9629, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10901.9599609375
tensor(10901.9629, grad_fn=<NegBackward0>) tensor(10901.9600, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10901.95703125
tensor(10901.9600, grad_fn=<NegBackward0>) tensor(10901.9570, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10901.9560546875
tensor(10901.9570, grad_fn=<NegBackward0>) tensor(10901.9561, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10901.9521484375
tensor(10901.9561, grad_fn=<NegBackward0>) tensor(10901.9521, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10901.951171875
tensor(10901.9521, grad_fn=<NegBackward0>) tensor(10901.9512, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10901.947265625
tensor(10901.9512, grad_fn=<NegBackward0>) tensor(10901.9473, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10901.9443359375
tensor(10901.9473, grad_fn=<NegBackward0>) tensor(10901.9443, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10901.939453125
tensor(10901.9443, grad_fn=<NegBackward0>) tensor(10901.9395, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10901.9326171875
tensor(10901.9395, grad_fn=<NegBackward0>) tensor(10901.9326, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10901.921875
tensor(10901.9326, grad_fn=<NegBackward0>) tensor(10901.9219, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10901.9111328125
tensor(10901.9219, grad_fn=<NegBackward0>) tensor(10901.9111, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10901.8935546875
tensor(10901.9111, grad_fn=<NegBackward0>) tensor(10901.8936, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10901.8701171875
tensor(10901.8936, grad_fn=<NegBackward0>) tensor(10901.8701, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10901.84765625
tensor(10901.8701, grad_fn=<NegBackward0>) tensor(10901.8477, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10901.8251953125
tensor(10901.8477, grad_fn=<NegBackward0>) tensor(10901.8252, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10901.8115234375
tensor(10901.8252, grad_fn=<NegBackward0>) tensor(10901.8115, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10901.7978515625
tensor(10901.8115, grad_fn=<NegBackward0>) tensor(10901.7979, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10901.7890625
tensor(10901.7979, grad_fn=<NegBackward0>) tensor(10901.7891, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10901.7841796875
tensor(10901.7891, grad_fn=<NegBackward0>) tensor(10901.7842, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10901.775390625
tensor(10901.7842, grad_fn=<NegBackward0>) tensor(10901.7754, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10901.7763671875
tensor(10901.7754, grad_fn=<NegBackward0>) tensor(10901.7764, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10901.767578125
tensor(10901.7754, grad_fn=<NegBackward0>) tensor(10901.7676, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10901.7802734375
tensor(10901.7676, grad_fn=<NegBackward0>) tensor(10901.7803, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10901.763671875
tensor(10901.7676, grad_fn=<NegBackward0>) tensor(10901.7637, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10901.7607421875
tensor(10901.7637, grad_fn=<NegBackward0>) tensor(10901.7607, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10901.7646484375
tensor(10901.7607, grad_fn=<NegBackward0>) tensor(10901.7646, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10901.7578125
tensor(10901.7607, grad_fn=<NegBackward0>) tensor(10901.7578, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10901.7548828125
tensor(10901.7578, grad_fn=<NegBackward0>) tensor(10901.7549, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10901.7548828125
tensor(10901.7549, grad_fn=<NegBackward0>) tensor(10901.7549, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10901.7548828125
tensor(10901.7549, grad_fn=<NegBackward0>) tensor(10901.7549, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10901.7548828125
tensor(10901.7549, grad_fn=<NegBackward0>) tensor(10901.7549, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10901.759765625
tensor(10901.7549, grad_fn=<NegBackward0>) tensor(10901.7598, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10901.75390625
tensor(10901.7549, grad_fn=<NegBackward0>) tensor(10901.7539, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10901.7548828125
tensor(10901.7539, grad_fn=<NegBackward0>) tensor(10901.7549, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10901.7548828125
tensor(10901.7539, grad_fn=<NegBackward0>) tensor(10901.7549, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -10901.76953125
tensor(10901.7539, grad_fn=<NegBackward0>) tensor(10901.7695, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -10901.75390625
tensor(10901.7539, grad_fn=<NegBackward0>) tensor(10901.7539, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10901.8505859375
tensor(10901.7539, grad_fn=<NegBackward0>) tensor(10901.8506, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10901.75390625
tensor(10901.7539, grad_fn=<NegBackward0>) tensor(10901.7539, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10901.7568359375
tensor(10901.7539, grad_fn=<NegBackward0>) tensor(10901.7568, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10901.7587890625
tensor(10901.7539, grad_fn=<NegBackward0>) tensor(10901.7588, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -10901.763671875
tensor(10901.7539, grad_fn=<NegBackward0>) tensor(10901.7637, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -10901.8017578125
tensor(10901.7539, grad_fn=<NegBackward0>) tensor(10901.8018, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -10901.7529296875
tensor(10901.7539, grad_fn=<NegBackward0>) tensor(10901.7529, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10901.7529296875
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.7529, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10901.7529296875
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.7529, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10901.7724609375
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.7725, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -10901.7529296875
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.7529, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10901.7548828125
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.7549, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10901.7568359375
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.7568, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -10901.7568359375
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.7568, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -10901.7568359375
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.7568, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -10901.7529296875
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.7529, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10901.75390625
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.7539, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10901.8349609375
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.8350, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -10901.7529296875
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.7529, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10901.75390625
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.7539, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -10901.7578125
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.7578, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -10901.8310546875
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.8311, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -10901.7529296875
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.7529, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10901.75390625
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.7539, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -10901.7568359375
tensor(10901.7529, grad_fn=<NegBackward0>) tensor(10901.7568, grad_fn=<NegBackward0>)
2
pi: tensor([[0.8734, 0.1266],
        [0.6890, 0.3110]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0057, 0.9943], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1630, 0.0826],
         [0.6381, 0.1558]],

        [[0.6546, 0.1580],
         [0.6432, 0.6284]],

        [[0.5814, 0.1732],
         [0.6471, 0.5192]],

        [[0.6614, 0.1583],
         [0.6564, 0.5708]],

        [[0.5932, 0.1525],
         [0.6310, 0.6468]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00021449335873650538
Average Adjusted Rand Index: 0.0
[-0.00021449335873650538, -0.00021449335873650538] [0.0, 0.0] [10901.7841796875, 10901.7548828125]
-------------------------------------
This iteration is 12
True Objective function: Loss = -10766.036861841018
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22192.619140625
inf tensor(22192.6191, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10836.908203125
tensor(22192.6191, grad_fn=<NegBackward0>) tensor(10836.9082, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10835.81640625
tensor(10836.9082, grad_fn=<NegBackward0>) tensor(10835.8164, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10835.525390625
tensor(10835.8164, grad_fn=<NegBackward0>) tensor(10835.5254, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10835.4150390625
tensor(10835.5254, grad_fn=<NegBackward0>) tensor(10835.4150, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10835.3603515625
tensor(10835.4150, grad_fn=<NegBackward0>) tensor(10835.3604, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10835.3310546875
tensor(10835.3604, grad_fn=<NegBackward0>) tensor(10835.3311, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10835.3076171875
tensor(10835.3311, grad_fn=<NegBackward0>) tensor(10835.3076, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10835.291015625
tensor(10835.3076, grad_fn=<NegBackward0>) tensor(10835.2910, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10835.2783203125
tensor(10835.2910, grad_fn=<NegBackward0>) tensor(10835.2783, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10835.2666015625
tensor(10835.2783, grad_fn=<NegBackward0>) tensor(10835.2666, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10835.2578125
tensor(10835.2666, grad_fn=<NegBackward0>) tensor(10835.2578, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10835.2490234375
tensor(10835.2578, grad_fn=<NegBackward0>) tensor(10835.2490, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10835.2421875
tensor(10835.2490, grad_fn=<NegBackward0>) tensor(10835.2422, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10835.2314453125
tensor(10835.2422, grad_fn=<NegBackward0>) tensor(10835.2314, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10835.224609375
tensor(10835.2314, grad_fn=<NegBackward0>) tensor(10835.2246, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10835.2177734375
tensor(10835.2246, grad_fn=<NegBackward0>) tensor(10835.2178, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10835.208984375
tensor(10835.2178, grad_fn=<NegBackward0>) tensor(10835.2090, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10835.19921875
tensor(10835.2090, grad_fn=<NegBackward0>) tensor(10835.1992, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10835.1875
tensor(10835.1992, grad_fn=<NegBackward0>) tensor(10835.1875, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10835.1748046875
tensor(10835.1875, grad_fn=<NegBackward0>) tensor(10835.1748, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10835.158203125
tensor(10835.1748, grad_fn=<NegBackward0>) tensor(10835.1582, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10835.140625
tensor(10835.1582, grad_fn=<NegBackward0>) tensor(10835.1406, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10835.1181640625
tensor(10835.1406, grad_fn=<NegBackward0>) tensor(10835.1182, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10835.0927734375
tensor(10835.1182, grad_fn=<NegBackward0>) tensor(10835.0928, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10835.06640625
tensor(10835.0928, grad_fn=<NegBackward0>) tensor(10835.0664, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10835.0419921875
tensor(10835.0664, grad_fn=<NegBackward0>) tensor(10835.0420, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10835.0185546875
tensor(10835.0420, grad_fn=<NegBackward0>) tensor(10835.0186, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10834.9951171875
tensor(10835.0186, grad_fn=<NegBackward0>) tensor(10834.9951, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10834.9697265625
tensor(10834.9951, grad_fn=<NegBackward0>) tensor(10834.9697, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10834.943359375
tensor(10834.9697, grad_fn=<NegBackward0>) tensor(10834.9434, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10834.916015625
tensor(10834.9434, grad_fn=<NegBackward0>) tensor(10834.9160, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10834.88671875
tensor(10834.9160, grad_fn=<NegBackward0>) tensor(10834.8867, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10834.8515625
tensor(10834.8867, grad_fn=<NegBackward0>) tensor(10834.8516, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10834.802734375
tensor(10834.8516, grad_fn=<NegBackward0>) tensor(10834.8027, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10834.6865234375
tensor(10834.8027, grad_fn=<NegBackward0>) tensor(10834.6865, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10833.939453125
tensor(10834.6865, grad_fn=<NegBackward0>) tensor(10833.9395, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10833.6328125
tensor(10833.9395, grad_fn=<NegBackward0>) tensor(10833.6328, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10833.5908203125
tensor(10833.6328, grad_fn=<NegBackward0>) tensor(10833.5908, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10833.5771484375
tensor(10833.5908, grad_fn=<NegBackward0>) tensor(10833.5771, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10833.5712890625
tensor(10833.5771, grad_fn=<NegBackward0>) tensor(10833.5713, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10833.56640625
tensor(10833.5713, grad_fn=<NegBackward0>) tensor(10833.5664, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10833.5634765625
tensor(10833.5664, grad_fn=<NegBackward0>) tensor(10833.5635, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10833.5625
tensor(10833.5635, grad_fn=<NegBackward0>) tensor(10833.5625, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10833.5615234375
tensor(10833.5625, grad_fn=<NegBackward0>) tensor(10833.5615, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10833.560546875
tensor(10833.5615, grad_fn=<NegBackward0>) tensor(10833.5605, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10833.55859375
tensor(10833.5605, grad_fn=<NegBackward0>) tensor(10833.5586, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10833.5576171875
tensor(10833.5586, grad_fn=<NegBackward0>) tensor(10833.5576, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10833.5576171875
tensor(10833.5576, grad_fn=<NegBackward0>) tensor(10833.5576, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10833.5576171875
tensor(10833.5576, grad_fn=<NegBackward0>) tensor(10833.5576, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10833.556640625
tensor(10833.5576, grad_fn=<NegBackward0>) tensor(10833.5566, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10833.556640625
tensor(10833.5566, grad_fn=<NegBackward0>) tensor(10833.5566, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10833.556640625
tensor(10833.5566, grad_fn=<NegBackward0>) tensor(10833.5566, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10833.5537109375
tensor(10833.5566, grad_fn=<NegBackward0>) tensor(10833.5537, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10833.552734375
tensor(10833.5537, grad_fn=<NegBackward0>) tensor(10833.5527, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10833.552734375
tensor(10833.5527, grad_fn=<NegBackward0>) tensor(10833.5527, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10833.552734375
tensor(10833.5527, grad_fn=<NegBackward0>) tensor(10833.5527, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10833.552734375
tensor(10833.5527, grad_fn=<NegBackward0>) tensor(10833.5527, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10833.552734375
tensor(10833.5527, grad_fn=<NegBackward0>) tensor(10833.5527, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10833.5595703125
tensor(10833.5527, grad_fn=<NegBackward0>) tensor(10833.5596, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10833.552734375
tensor(10833.5527, grad_fn=<NegBackward0>) tensor(10833.5527, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10833.5517578125
tensor(10833.5527, grad_fn=<NegBackward0>) tensor(10833.5518, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10833.5517578125
tensor(10833.5518, grad_fn=<NegBackward0>) tensor(10833.5518, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10833.5517578125
tensor(10833.5518, grad_fn=<NegBackward0>) tensor(10833.5518, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10833.5517578125
tensor(10833.5518, grad_fn=<NegBackward0>) tensor(10833.5518, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10833.552734375
tensor(10833.5518, grad_fn=<NegBackward0>) tensor(10833.5527, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10833.55078125
tensor(10833.5518, grad_fn=<NegBackward0>) tensor(10833.5508, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10833.5517578125
tensor(10833.5508, grad_fn=<NegBackward0>) tensor(10833.5518, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10833.55078125
tensor(10833.5508, grad_fn=<NegBackward0>) tensor(10833.5508, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10833.55078125
tensor(10833.5508, grad_fn=<NegBackward0>) tensor(10833.5508, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10833.55078125
tensor(10833.5508, grad_fn=<NegBackward0>) tensor(10833.5508, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10833.5498046875
tensor(10833.5508, grad_fn=<NegBackward0>) tensor(10833.5498, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10833.552734375
tensor(10833.5498, grad_fn=<NegBackward0>) tensor(10833.5527, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10833.55078125
tensor(10833.5498, grad_fn=<NegBackward0>) tensor(10833.5508, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -10833.548828125
tensor(10833.5498, grad_fn=<NegBackward0>) tensor(10833.5488, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10833.55078125
tensor(10833.5488, grad_fn=<NegBackward0>) tensor(10833.5508, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10833.5498046875
tensor(10833.5488, grad_fn=<NegBackward0>) tensor(10833.5498, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -10833.5498046875
tensor(10833.5488, grad_fn=<NegBackward0>) tensor(10833.5498, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -10833.5498046875
tensor(10833.5488, grad_fn=<NegBackward0>) tensor(10833.5498, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -10833.5556640625
tensor(10833.5488, grad_fn=<NegBackward0>) tensor(10833.5557, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7900 due to no improvement.
pi: tensor([[0.9594, 0.0406],
        [0.9986, 0.0014]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8465, 0.1535], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1620, 0.1421],
         [0.7213, 0.1218]],

        [[0.6458, 0.1616],
         [0.6636, 0.5182]],

        [[0.7090, 0.0838],
         [0.5384, 0.5683]],

        [[0.7248, 0.1119],
         [0.6762, 0.5944]],

        [[0.6676, 0.1504],
         [0.6485, 0.6732]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 62
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0013157842494549874
Average Adjusted Rand Index: -0.0001522251028708703
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22546.3359375
inf tensor(22546.3359, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10836.3642578125
tensor(22546.3359, grad_fn=<NegBackward0>) tensor(10836.3643, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10835.5546875
tensor(10836.3643, grad_fn=<NegBackward0>) tensor(10835.5547, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10835.40234375
tensor(10835.5547, grad_fn=<NegBackward0>) tensor(10835.4023, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10835.2880859375
tensor(10835.4023, grad_fn=<NegBackward0>) tensor(10835.2881, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10835.02734375
tensor(10835.2881, grad_fn=<NegBackward0>) tensor(10835.0273, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10834.82421875
tensor(10835.0273, grad_fn=<NegBackward0>) tensor(10834.8242, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10834.7607421875
tensor(10834.8242, grad_fn=<NegBackward0>) tensor(10834.7607, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10834.7099609375
tensor(10834.7607, grad_fn=<NegBackward0>) tensor(10834.7100, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10834.6669921875
tensor(10834.7100, grad_fn=<NegBackward0>) tensor(10834.6670, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10834.6298828125
tensor(10834.6670, grad_fn=<NegBackward0>) tensor(10834.6299, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10834.599609375
tensor(10834.6299, grad_fn=<NegBackward0>) tensor(10834.5996, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10834.5703125
tensor(10834.5996, grad_fn=<NegBackward0>) tensor(10834.5703, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10834.5498046875
tensor(10834.5703, grad_fn=<NegBackward0>) tensor(10834.5498, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10834.5283203125
tensor(10834.5498, grad_fn=<NegBackward0>) tensor(10834.5283, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10834.51171875
tensor(10834.5283, grad_fn=<NegBackward0>) tensor(10834.5117, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10834.49609375
tensor(10834.5117, grad_fn=<NegBackward0>) tensor(10834.4961, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10834.482421875
tensor(10834.4961, grad_fn=<NegBackward0>) tensor(10834.4824, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10834.47265625
tensor(10834.4824, grad_fn=<NegBackward0>) tensor(10834.4727, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10834.4619140625
tensor(10834.4727, grad_fn=<NegBackward0>) tensor(10834.4619, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10834.453125
tensor(10834.4619, grad_fn=<NegBackward0>) tensor(10834.4531, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10834.4443359375
tensor(10834.4531, grad_fn=<NegBackward0>) tensor(10834.4443, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10834.4384765625
tensor(10834.4443, grad_fn=<NegBackward0>) tensor(10834.4385, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10834.431640625
tensor(10834.4385, grad_fn=<NegBackward0>) tensor(10834.4316, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10834.42578125
tensor(10834.4316, grad_fn=<NegBackward0>) tensor(10834.4258, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10834.419921875
tensor(10834.4258, grad_fn=<NegBackward0>) tensor(10834.4199, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10834.416015625
tensor(10834.4199, grad_fn=<NegBackward0>) tensor(10834.4160, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10834.412109375
tensor(10834.4160, grad_fn=<NegBackward0>) tensor(10834.4121, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10834.40625
tensor(10834.4121, grad_fn=<NegBackward0>) tensor(10834.4062, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10834.400390625
tensor(10834.4062, grad_fn=<NegBackward0>) tensor(10834.4004, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10834.3955078125
tensor(10834.4004, grad_fn=<NegBackward0>) tensor(10834.3955, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10834.388671875
tensor(10834.3955, grad_fn=<NegBackward0>) tensor(10834.3887, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10834.3837890625
tensor(10834.3887, grad_fn=<NegBackward0>) tensor(10834.3838, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10834.375
tensor(10834.3838, grad_fn=<NegBackward0>) tensor(10834.3750, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10834.3662109375
tensor(10834.3750, grad_fn=<NegBackward0>) tensor(10834.3662, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10834.3583984375
tensor(10834.3662, grad_fn=<NegBackward0>) tensor(10834.3584, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10834.34765625
tensor(10834.3584, grad_fn=<NegBackward0>) tensor(10834.3477, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10834.3369140625
tensor(10834.3477, grad_fn=<NegBackward0>) tensor(10834.3369, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10834.326171875
tensor(10834.3369, grad_fn=<NegBackward0>) tensor(10834.3262, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10834.31640625
tensor(10834.3262, grad_fn=<NegBackward0>) tensor(10834.3164, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10834.3076171875
tensor(10834.3164, grad_fn=<NegBackward0>) tensor(10834.3076, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10834.2998046875
tensor(10834.3076, grad_fn=<NegBackward0>) tensor(10834.2998, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10834.29296875
tensor(10834.2998, grad_fn=<NegBackward0>) tensor(10834.2930, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10834.2861328125
tensor(10834.2930, grad_fn=<NegBackward0>) tensor(10834.2861, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10834.279296875
tensor(10834.2861, grad_fn=<NegBackward0>) tensor(10834.2793, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10834.2724609375
tensor(10834.2793, grad_fn=<NegBackward0>) tensor(10834.2725, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10834.263671875
tensor(10834.2725, grad_fn=<NegBackward0>) tensor(10834.2637, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10834.251953125
tensor(10834.2637, grad_fn=<NegBackward0>) tensor(10834.2520, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10834.228515625
tensor(10834.2520, grad_fn=<NegBackward0>) tensor(10834.2285, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10834.173828125
tensor(10834.2285, grad_fn=<NegBackward0>) tensor(10834.1738, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10834.056640625
tensor(10834.1738, grad_fn=<NegBackward0>) tensor(10834.0566, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10833.873046875
tensor(10834.0566, grad_fn=<NegBackward0>) tensor(10833.8730, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10833.748046875
tensor(10833.8730, grad_fn=<NegBackward0>) tensor(10833.7480, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10833.69921875
tensor(10833.7480, grad_fn=<NegBackward0>) tensor(10833.6992, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10833.669921875
tensor(10833.6992, grad_fn=<NegBackward0>) tensor(10833.6699, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10833.6455078125
tensor(10833.6699, grad_fn=<NegBackward0>) tensor(10833.6455, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10833.62109375
tensor(10833.6455, grad_fn=<NegBackward0>) tensor(10833.6211, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10833.60546875
tensor(10833.6211, grad_fn=<NegBackward0>) tensor(10833.6055, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10833.59375
tensor(10833.6055, grad_fn=<NegBackward0>) tensor(10833.5938, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10833.583984375
tensor(10833.5938, grad_fn=<NegBackward0>) tensor(10833.5840, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10833.576171875
tensor(10833.5840, grad_fn=<NegBackward0>) tensor(10833.5762, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10833.603515625
tensor(10833.5762, grad_fn=<NegBackward0>) tensor(10833.6035, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10833.564453125
tensor(10833.5762, grad_fn=<NegBackward0>) tensor(10833.5645, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10833.560546875
tensor(10833.5645, grad_fn=<NegBackward0>) tensor(10833.5605, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10833.5576171875
tensor(10833.5605, grad_fn=<NegBackward0>) tensor(10833.5576, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10833.5537109375
tensor(10833.5576, grad_fn=<NegBackward0>) tensor(10833.5537, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10833.5556640625
tensor(10833.5537, grad_fn=<NegBackward0>) tensor(10833.5557, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10833.548828125
tensor(10833.5537, grad_fn=<NegBackward0>) tensor(10833.5488, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10833.5478515625
tensor(10833.5488, grad_fn=<NegBackward0>) tensor(10833.5479, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10833.544921875
tensor(10833.5479, grad_fn=<NegBackward0>) tensor(10833.5449, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10833.5439453125
tensor(10833.5449, grad_fn=<NegBackward0>) tensor(10833.5439, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10833.5458984375
tensor(10833.5439, grad_fn=<NegBackward0>) tensor(10833.5459, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10833.5498046875
tensor(10833.5439, grad_fn=<NegBackward0>) tensor(10833.5498, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -10833.541015625
tensor(10833.5439, grad_fn=<NegBackward0>) tensor(10833.5410, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10833.5419921875
tensor(10833.5410, grad_fn=<NegBackward0>) tensor(10833.5420, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10833.541015625
tensor(10833.5410, grad_fn=<NegBackward0>) tensor(10833.5410, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10833.541015625
tensor(10833.5410, grad_fn=<NegBackward0>) tensor(10833.5410, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10833.603515625
tensor(10833.5410, grad_fn=<NegBackward0>) tensor(10833.6035, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10833.5380859375
tensor(10833.5410, grad_fn=<NegBackward0>) tensor(10833.5381, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10833.5654296875
tensor(10833.5381, grad_fn=<NegBackward0>) tensor(10833.5654, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10833.5478515625
tensor(10833.5381, grad_fn=<NegBackward0>) tensor(10833.5479, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -10833.5625
tensor(10833.5381, grad_fn=<NegBackward0>) tensor(10833.5625, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -10833.5380859375
tensor(10833.5381, grad_fn=<NegBackward0>) tensor(10833.5381, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10833.5419921875
tensor(10833.5381, grad_fn=<NegBackward0>) tensor(10833.5420, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10833.583984375
tensor(10833.5381, grad_fn=<NegBackward0>) tensor(10833.5840, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -10833.5380859375
tensor(10833.5381, grad_fn=<NegBackward0>) tensor(10833.5381, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10833.5546875
tensor(10833.5381, grad_fn=<NegBackward0>) tensor(10833.5547, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10833.54296875
tensor(10833.5381, grad_fn=<NegBackward0>) tensor(10833.5430, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -10833.537109375
tensor(10833.5381, grad_fn=<NegBackward0>) tensor(10833.5371, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10833.5361328125
tensor(10833.5371, grad_fn=<NegBackward0>) tensor(10833.5361, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10833.5380859375
tensor(10833.5361, grad_fn=<NegBackward0>) tensor(10833.5381, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10833.537109375
tensor(10833.5361, grad_fn=<NegBackward0>) tensor(10833.5371, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -10833.5380859375
tensor(10833.5361, grad_fn=<NegBackward0>) tensor(10833.5381, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -10833.53515625
tensor(10833.5361, grad_fn=<NegBackward0>) tensor(10833.5352, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10833.5400390625
tensor(10833.5352, grad_fn=<NegBackward0>) tensor(10833.5400, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -10833.640625
tensor(10833.5352, grad_fn=<NegBackward0>) tensor(10833.6406, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -10833.5341796875
tensor(10833.5352, grad_fn=<NegBackward0>) tensor(10833.5342, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10833.5498046875
tensor(10833.5342, grad_fn=<NegBackward0>) tensor(10833.5498, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -10833.541015625
tensor(10833.5342, grad_fn=<NegBackward0>) tensor(10833.5410, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -10833.533203125
tensor(10833.5342, grad_fn=<NegBackward0>) tensor(10833.5332, grad_fn=<NegBackward0>)
pi: tensor([[0.6401, 0.3599],
        [0.9920, 0.0080]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0243, 0.9757], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1619, 0.2471],
         [0.7097, 0.1519]],

        [[0.6178, 0.1746],
         [0.5732, 0.5949]],

        [[0.6542, 0.1568],
         [0.5191, 0.6071]],

        [[0.5041, 0.1488],
         [0.6480, 0.7084]],

        [[0.5241, 0.1598],
         [0.6862, 0.5062]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.005453571602473584
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 62
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0027619568625826584
Average Adjusted Rand Index: 0.0010907143204947167
[-0.0013157842494549874, -0.0027619568625826584] [-0.0001522251028708703, 0.0010907143204947167] [10833.5556640625, 10833.5830078125]
-------------------------------------
This iteration is 13
True Objective function: Loss = -10830.309322778518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24107.16015625
inf tensor(24107.1602, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10931.49609375
tensor(24107.1602, grad_fn=<NegBackward0>) tensor(10931.4961, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10930.2177734375
tensor(10931.4961, grad_fn=<NegBackward0>) tensor(10930.2178, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10929.8505859375
tensor(10930.2178, grad_fn=<NegBackward0>) tensor(10929.8506, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10929.7236328125
tensor(10929.8506, grad_fn=<NegBackward0>) tensor(10929.7236, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10929.65234375
tensor(10929.7236, grad_fn=<NegBackward0>) tensor(10929.6523, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10929.5966796875
tensor(10929.6523, grad_fn=<NegBackward0>) tensor(10929.5967, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10929.5478515625
tensor(10929.5967, grad_fn=<NegBackward0>) tensor(10929.5479, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10929.5
tensor(10929.5479, grad_fn=<NegBackward0>) tensor(10929.5000, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10929.4560546875
tensor(10929.5000, grad_fn=<NegBackward0>) tensor(10929.4561, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10929.41015625
tensor(10929.4561, grad_fn=<NegBackward0>) tensor(10929.4102, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10929.36328125
tensor(10929.4102, grad_fn=<NegBackward0>) tensor(10929.3633, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10929.314453125
tensor(10929.3633, grad_fn=<NegBackward0>) tensor(10929.3145, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10929.259765625
tensor(10929.3145, grad_fn=<NegBackward0>) tensor(10929.2598, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10929.201171875
tensor(10929.2598, grad_fn=<NegBackward0>) tensor(10929.2012, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10929.134765625
tensor(10929.2012, grad_fn=<NegBackward0>) tensor(10929.1348, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10929.0576171875
tensor(10929.1348, grad_fn=<NegBackward0>) tensor(10929.0576, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10928.96484375
tensor(10929.0576, grad_fn=<NegBackward0>) tensor(10928.9648, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10928.859375
tensor(10928.9648, grad_fn=<NegBackward0>) tensor(10928.8594, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10928.73046875
tensor(10928.8594, grad_fn=<NegBackward0>) tensor(10928.7305, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10928.5732421875
tensor(10928.7305, grad_fn=<NegBackward0>) tensor(10928.5732, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10928.3828125
tensor(10928.5732, grad_fn=<NegBackward0>) tensor(10928.3828, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10928.146484375
tensor(10928.3828, grad_fn=<NegBackward0>) tensor(10928.1465, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10927.8486328125
tensor(10928.1465, grad_fn=<NegBackward0>) tensor(10927.8486, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10926.791015625
tensor(10927.8486, grad_fn=<NegBackward0>) tensor(10926.7910, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10919.6806640625
tensor(10926.7910, grad_fn=<NegBackward0>) tensor(10919.6807, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10919.1650390625
tensor(10919.6807, grad_fn=<NegBackward0>) tensor(10919.1650, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10918.98828125
tensor(10919.1650, grad_fn=<NegBackward0>) tensor(10918.9883, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10918.9384765625
tensor(10918.9883, grad_fn=<NegBackward0>) tensor(10918.9385, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10918.908203125
tensor(10918.9385, grad_fn=<NegBackward0>) tensor(10918.9082, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10918.5283203125
tensor(10918.9082, grad_fn=<NegBackward0>) tensor(10918.5283, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10917.6142578125
tensor(10918.5283, grad_fn=<NegBackward0>) tensor(10917.6143, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10917.5791015625
tensor(10917.6143, grad_fn=<NegBackward0>) tensor(10917.5791, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10917.5654296875
tensor(10917.5791, grad_fn=<NegBackward0>) tensor(10917.5654, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10917.556640625
tensor(10917.5654, grad_fn=<NegBackward0>) tensor(10917.5566, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10917.55078125
tensor(10917.5566, grad_fn=<NegBackward0>) tensor(10917.5508, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10917.5458984375
tensor(10917.5508, grad_fn=<NegBackward0>) tensor(10917.5459, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10917.5439453125
tensor(10917.5459, grad_fn=<NegBackward0>) tensor(10917.5439, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10917.5400390625
tensor(10917.5439, grad_fn=<NegBackward0>) tensor(10917.5400, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10917.5390625
tensor(10917.5400, grad_fn=<NegBackward0>) tensor(10917.5391, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10917.5361328125
tensor(10917.5391, grad_fn=<NegBackward0>) tensor(10917.5361, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10917.5341796875
tensor(10917.5361, grad_fn=<NegBackward0>) tensor(10917.5342, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10917.533203125
tensor(10917.5342, grad_fn=<NegBackward0>) tensor(10917.5332, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10917.53125
tensor(10917.5332, grad_fn=<NegBackward0>) tensor(10917.5312, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10917.5263671875
tensor(10917.5312, grad_fn=<NegBackward0>) tensor(10917.5264, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10917.5263671875
tensor(10917.5264, grad_fn=<NegBackward0>) tensor(10917.5264, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10917.5244140625
tensor(10917.5264, grad_fn=<NegBackward0>) tensor(10917.5244, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10917.525390625
tensor(10917.5244, grad_fn=<NegBackward0>) tensor(10917.5254, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10917.5234375
tensor(10917.5244, grad_fn=<NegBackward0>) tensor(10917.5234, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10917.5234375
tensor(10917.5234, grad_fn=<NegBackward0>) tensor(10917.5234, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10917.5234375
tensor(10917.5234, grad_fn=<NegBackward0>) tensor(10917.5234, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10917.521484375
tensor(10917.5234, grad_fn=<NegBackward0>) tensor(10917.5215, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10917.521484375
tensor(10917.5215, grad_fn=<NegBackward0>) tensor(10917.5215, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10917.521484375
tensor(10917.5215, grad_fn=<NegBackward0>) tensor(10917.5215, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10917.5224609375
tensor(10917.5215, grad_fn=<NegBackward0>) tensor(10917.5225, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10917.51953125
tensor(10917.5215, grad_fn=<NegBackward0>) tensor(10917.5195, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10917.51953125
tensor(10917.5195, grad_fn=<NegBackward0>) tensor(10917.5195, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10917.5185546875
tensor(10917.5195, grad_fn=<NegBackward0>) tensor(10917.5186, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10917.5166015625
tensor(10917.5186, grad_fn=<NegBackward0>) tensor(10917.5166, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10917.5185546875
tensor(10917.5166, grad_fn=<NegBackward0>) tensor(10917.5186, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10917.5185546875
tensor(10917.5166, grad_fn=<NegBackward0>) tensor(10917.5186, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -10917.521484375
tensor(10917.5166, grad_fn=<NegBackward0>) tensor(10917.5215, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -10917.517578125
tensor(10917.5166, grad_fn=<NegBackward0>) tensor(10917.5176, grad_fn=<NegBackward0>)
4
Iteration 6300: Loss = -10917.5166015625
tensor(10917.5166, grad_fn=<NegBackward0>) tensor(10917.5166, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10917.517578125
tensor(10917.5166, grad_fn=<NegBackward0>) tensor(10917.5176, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10917.517578125
tensor(10917.5166, grad_fn=<NegBackward0>) tensor(10917.5176, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -10917.5166015625
tensor(10917.5166, grad_fn=<NegBackward0>) tensor(10917.5166, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10917.5166015625
tensor(10917.5166, grad_fn=<NegBackward0>) tensor(10917.5166, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10917.5146484375
tensor(10917.5166, grad_fn=<NegBackward0>) tensor(10917.5146, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10917.515625
tensor(10917.5146, grad_fn=<NegBackward0>) tensor(10917.5156, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10917.525390625
tensor(10917.5146, grad_fn=<NegBackward0>) tensor(10917.5254, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -10917.5146484375
tensor(10917.5146, grad_fn=<NegBackward0>) tensor(10917.5146, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10917.5146484375
tensor(10917.5146, grad_fn=<NegBackward0>) tensor(10917.5146, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10917.5146484375
tensor(10917.5146, grad_fn=<NegBackward0>) tensor(10917.5146, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10917.515625
tensor(10917.5146, grad_fn=<NegBackward0>) tensor(10917.5156, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10917.5146484375
tensor(10917.5146, grad_fn=<NegBackward0>) tensor(10917.5146, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10917.5146484375
tensor(10917.5146, grad_fn=<NegBackward0>) tensor(10917.5146, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10917.513671875
tensor(10917.5146, grad_fn=<NegBackward0>) tensor(10917.5137, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10917.51953125
tensor(10917.5137, grad_fn=<NegBackward0>) tensor(10917.5195, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10917.5478515625
tensor(10917.5137, grad_fn=<NegBackward0>) tensor(10917.5479, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -10917.5126953125
tensor(10917.5137, grad_fn=<NegBackward0>) tensor(10917.5127, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10917.5126953125
tensor(10917.5127, grad_fn=<NegBackward0>) tensor(10917.5127, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10917.517578125
tensor(10917.5127, grad_fn=<NegBackward0>) tensor(10917.5176, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10917.5126953125
tensor(10917.5127, grad_fn=<NegBackward0>) tensor(10917.5127, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10917.513671875
tensor(10917.5127, grad_fn=<NegBackward0>) tensor(10917.5137, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -10917.5126953125
tensor(10917.5127, grad_fn=<NegBackward0>) tensor(10917.5127, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10917.5126953125
tensor(10917.5127, grad_fn=<NegBackward0>) tensor(10917.5127, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10917.513671875
tensor(10917.5127, grad_fn=<NegBackward0>) tensor(10917.5137, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -10917.5126953125
tensor(10917.5127, grad_fn=<NegBackward0>) tensor(10917.5127, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10917.513671875
tensor(10917.5127, grad_fn=<NegBackward0>) tensor(10917.5137, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -10917.513671875
tensor(10917.5127, grad_fn=<NegBackward0>) tensor(10917.5137, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -10917.5126953125
tensor(10917.5127, grad_fn=<NegBackward0>) tensor(10917.5127, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10917.5126953125
tensor(10917.5127, grad_fn=<NegBackward0>) tensor(10917.5127, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10917.5126953125
tensor(10917.5127, grad_fn=<NegBackward0>) tensor(10917.5127, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10917.5126953125
tensor(10917.5127, grad_fn=<NegBackward0>) tensor(10917.5127, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10917.5380859375
tensor(10917.5127, grad_fn=<NegBackward0>) tensor(10917.5381, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -10917.51171875
tensor(10917.5127, grad_fn=<NegBackward0>) tensor(10917.5117, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10917.61328125
tensor(10917.5117, grad_fn=<NegBackward0>) tensor(10917.6133, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -10917.51171875
tensor(10917.5117, grad_fn=<NegBackward0>) tensor(10917.5117, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10917.5126953125
tensor(10917.5117, grad_fn=<NegBackward0>) tensor(10917.5127, grad_fn=<NegBackward0>)
1
pi: tensor([[2.6498e-04, 9.9974e-01],
        [2.4131e-01, 7.5869e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0381, 0.9619], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1912, 0.1177],
         [0.5287, 0.1653]],

        [[0.6889, 0.1619],
         [0.6991, 0.6139]],

        [[0.5724, 0.1803],
         [0.5907, 0.6432]],

        [[0.6505, 0.0805],
         [0.6605, 0.6163]],

        [[0.6924, 0.1744],
         [0.5309, 0.5392]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 37
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 73
Adjusted Rand Index: 0.20473002416511507
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.003681595691538529
Average Adjusted Rand Index: 0.04094600483302301
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23117.34765625
inf tensor(23117.3477, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10930.8037109375
tensor(23117.3477, grad_fn=<NegBackward0>) tensor(10930.8037, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10929.984375
tensor(10930.8037, grad_fn=<NegBackward0>) tensor(10929.9844, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10929.7412109375
tensor(10929.9844, grad_fn=<NegBackward0>) tensor(10929.7412, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10929.57421875
tensor(10929.7412, grad_fn=<NegBackward0>) tensor(10929.5742, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10929.39453125
tensor(10929.5742, grad_fn=<NegBackward0>) tensor(10929.3945, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10929.0537109375
tensor(10929.3945, grad_fn=<NegBackward0>) tensor(10929.0537, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10926.978515625
tensor(10929.0537, grad_fn=<NegBackward0>) tensor(10926.9785, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10922.6572265625
tensor(10926.9785, grad_fn=<NegBackward0>) tensor(10922.6572, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10921.3701171875
tensor(10922.6572, grad_fn=<NegBackward0>) tensor(10921.3701, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10920.7275390625
tensor(10921.3701, grad_fn=<NegBackward0>) tensor(10920.7275, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10920.265625
tensor(10920.7275, grad_fn=<NegBackward0>) tensor(10920.2656, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10919.916015625
tensor(10920.2656, grad_fn=<NegBackward0>) tensor(10919.9160, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10919.6591796875
tensor(10919.9160, grad_fn=<NegBackward0>) tensor(10919.6592, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10919.4716796875
tensor(10919.6592, grad_fn=<NegBackward0>) tensor(10919.4717, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10919.3310546875
tensor(10919.4717, grad_fn=<NegBackward0>) tensor(10919.3311, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10919.216796875
tensor(10919.3311, grad_fn=<NegBackward0>) tensor(10919.2168, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10919.087890625
tensor(10919.2168, grad_fn=<NegBackward0>) tensor(10919.0879, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10918.9892578125
tensor(10919.0879, grad_fn=<NegBackward0>) tensor(10918.9893, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10918.9462890625
tensor(10918.9893, grad_fn=<NegBackward0>) tensor(10918.9463, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10918.9228515625
tensor(10918.9463, grad_fn=<NegBackward0>) tensor(10918.9229, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10918.9072265625
tensor(10918.9229, grad_fn=<NegBackward0>) tensor(10918.9072, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10918.896484375
tensor(10918.9072, grad_fn=<NegBackward0>) tensor(10918.8965, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10918.88671875
tensor(10918.8965, grad_fn=<NegBackward0>) tensor(10918.8867, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10918.8828125
tensor(10918.8867, grad_fn=<NegBackward0>) tensor(10918.8828, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10918.869140625
tensor(10918.8828, grad_fn=<NegBackward0>) tensor(10918.8691, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10917.830078125
tensor(10918.8691, grad_fn=<NegBackward0>) tensor(10917.8301, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10917.5859375
tensor(10917.8301, grad_fn=<NegBackward0>) tensor(10917.5859, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10917.5693359375
tensor(10917.5859, grad_fn=<NegBackward0>) tensor(10917.5693, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10917.560546875
tensor(10917.5693, grad_fn=<NegBackward0>) tensor(10917.5605, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10917.5537109375
tensor(10917.5605, grad_fn=<NegBackward0>) tensor(10917.5537, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10917.5498046875
tensor(10917.5537, grad_fn=<NegBackward0>) tensor(10917.5498, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10917.5458984375
tensor(10917.5498, grad_fn=<NegBackward0>) tensor(10917.5459, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10917.5419921875
tensor(10917.5459, grad_fn=<NegBackward0>) tensor(10917.5420, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10917.5380859375
tensor(10917.5420, grad_fn=<NegBackward0>) tensor(10917.5381, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10917.537109375
tensor(10917.5381, grad_fn=<NegBackward0>) tensor(10917.5371, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10917.5341796875
tensor(10917.5371, grad_fn=<NegBackward0>) tensor(10917.5342, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10917.5322265625
tensor(10917.5342, grad_fn=<NegBackward0>) tensor(10917.5322, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10917.5341796875
tensor(10917.5322, grad_fn=<NegBackward0>) tensor(10917.5342, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -10917.529296875
tensor(10917.5322, grad_fn=<NegBackward0>) tensor(10917.5293, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10917.5283203125
tensor(10917.5293, grad_fn=<NegBackward0>) tensor(10917.5283, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10917.529296875
tensor(10917.5283, grad_fn=<NegBackward0>) tensor(10917.5293, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -10917.5263671875
tensor(10917.5283, grad_fn=<NegBackward0>) tensor(10917.5264, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10917.5263671875
tensor(10917.5264, grad_fn=<NegBackward0>) tensor(10917.5264, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10917.5244140625
tensor(10917.5264, grad_fn=<NegBackward0>) tensor(10917.5244, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10917.525390625
tensor(10917.5244, grad_fn=<NegBackward0>) tensor(10917.5254, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10917.5244140625
tensor(10917.5244, grad_fn=<NegBackward0>) tensor(10917.5244, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10917.5234375
tensor(10917.5244, grad_fn=<NegBackward0>) tensor(10917.5234, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10917.5244140625
tensor(10917.5234, grad_fn=<NegBackward0>) tensor(10917.5244, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10917.5234375
tensor(10917.5234, grad_fn=<NegBackward0>) tensor(10917.5234, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10917.521484375
tensor(10917.5234, grad_fn=<NegBackward0>) tensor(10917.5215, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10917.51953125
tensor(10917.5215, grad_fn=<NegBackward0>) tensor(10917.5195, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10917.51953125
tensor(10917.5195, grad_fn=<NegBackward0>) tensor(10917.5195, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10917.51953125
tensor(10917.5195, grad_fn=<NegBackward0>) tensor(10917.5195, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10917.51953125
tensor(10917.5195, grad_fn=<NegBackward0>) tensor(10917.5195, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10917.51953125
tensor(10917.5195, grad_fn=<NegBackward0>) tensor(10917.5195, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10917.51953125
tensor(10917.5195, grad_fn=<NegBackward0>) tensor(10917.5195, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10917.5185546875
tensor(10917.5195, grad_fn=<NegBackward0>) tensor(10917.5186, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10917.5205078125
tensor(10917.5186, grad_fn=<NegBackward0>) tensor(10917.5205, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10917.5185546875
tensor(10917.5186, grad_fn=<NegBackward0>) tensor(10917.5186, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10917.5498046875
tensor(10917.5186, grad_fn=<NegBackward0>) tensor(10917.5498, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10917.517578125
tensor(10917.5186, grad_fn=<NegBackward0>) tensor(10917.5176, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10917.5146484375
tensor(10917.5176, grad_fn=<NegBackward0>) tensor(10917.5146, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10917.515625
tensor(10917.5146, grad_fn=<NegBackward0>) tensor(10917.5156, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10917.515625
tensor(10917.5146, grad_fn=<NegBackward0>) tensor(10917.5156, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -10917.56640625
tensor(10917.5146, grad_fn=<NegBackward0>) tensor(10917.5664, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -10917.515625
tensor(10917.5146, grad_fn=<NegBackward0>) tensor(10917.5156, grad_fn=<NegBackward0>)
4
Iteration 6700: Loss = -10917.515625
tensor(10917.5146, grad_fn=<NegBackward0>) tensor(10917.5156, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6700 due to no improvement.
pi: tensor([[0.7587, 0.2413],
        [0.9988, 0.0012]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9618, 0.0382], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1653, 0.1176],
         [0.6242, 0.1911]],

        [[0.6972, 0.1619],
         [0.7012, 0.7041]],

        [[0.7070, 0.1803],
         [0.5312, 0.6066]],

        [[0.7096, 0.0805],
         [0.5120, 0.5866]],

        [[0.5983, 0.1744],
         [0.6214, 0.5604]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 63
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 27
Adjusted Rand Index: 0.20473002416511507
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.003681595691538529
Average Adjusted Rand Index: 0.04094600483302301
[0.003681595691538529, 0.003681595691538529] [0.04094600483302301, 0.04094600483302301] [10917.5126953125, 10917.515625]
-------------------------------------
This iteration is 14
True Objective function: Loss = -10864.281084561346
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23849.8671875
inf tensor(23849.8672, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10921.3427734375
tensor(23849.8672, grad_fn=<NegBackward0>) tensor(10921.3428, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10919.986328125
tensor(10921.3428, grad_fn=<NegBackward0>) tensor(10919.9863, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10919.708984375
tensor(10919.9863, grad_fn=<NegBackward0>) tensor(10919.7090, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10919.4755859375
tensor(10919.7090, grad_fn=<NegBackward0>) tensor(10919.4756, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10919.2490234375
tensor(10919.4756, grad_fn=<NegBackward0>) tensor(10919.2490, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10919.0439453125
tensor(10919.2490, grad_fn=<NegBackward0>) tensor(10919.0439, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10918.88671875
tensor(10919.0439, grad_fn=<NegBackward0>) tensor(10918.8867, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10918.7861328125
tensor(10918.8867, grad_fn=<NegBackward0>) tensor(10918.7861, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10918.712890625
tensor(10918.7861, grad_fn=<NegBackward0>) tensor(10918.7129, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10918.673828125
tensor(10918.7129, grad_fn=<NegBackward0>) tensor(10918.6738, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10918.6533203125
tensor(10918.6738, grad_fn=<NegBackward0>) tensor(10918.6533, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10918.6376953125
tensor(10918.6533, grad_fn=<NegBackward0>) tensor(10918.6377, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10918.6259765625
tensor(10918.6377, grad_fn=<NegBackward0>) tensor(10918.6260, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10918.61328125
tensor(10918.6260, grad_fn=<NegBackward0>) tensor(10918.6133, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10918.6015625
tensor(10918.6133, grad_fn=<NegBackward0>) tensor(10918.6016, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10918.5908203125
tensor(10918.6016, grad_fn=<NegBackward0>) tensor(10918.5908, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10918.5771484375
tensor(10918.5908, grad_fn=<NegBackward0>) tensor(10918.5771, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10918.556640625
tensor(10918.5771, grad_fn=<NegBackward0>) tensor(10918.5566, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10918.541015625
tensor(10918.5566, grad_fn=<NegBackward0>) tensor(10918.5410, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10918.521484375
tensor(10918.5410, grad_fn=<NegBackward0>) tensor(10918.5215, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10918.4853515625
tensor(10918.5215, grad_fn=<NegBackward0>) tensor(10918.4854, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10918.4248046875
tensor(10918.4854, grad_fn=<NegBackward0>) tensor(10918.4248, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10918.3369140625
tensor(10918.4248, grad_fn=<NegBackward0>) tensor(10918.3369, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10918.2333984375
tensor(10918.3369, grad_fn=<NegBackward0>) tensor(10918.2334, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10915.25
tensor(10918.2334, grad_fn=<NegBackward0>) tensor(10915.2500, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10886.6884765625
tensor(10915.2500, grad_fn=<NegBackward0>) tensor(10886.6885, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10826.8662109375
tensor(10886.6885, grad_fn=<NegBackward0>) tensor(10826.8662, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10815.7197265625
tensor(10826.8662, grad_fn=<NegBackward0>) tensor(10815.7197, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10815.568359375
tensor(10815.7197, grad_fn=<NegBackward0>) tensor(10815.5684, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10815.53515625
tensor(10815.5684, grad_fn=<NegBackward0>) tensor(10815.5352, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10815.4423828125
tensor(10815.5352, grad_fn=<NegBackward0>) tensor(10815.4424, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10815.4375
tensor(10815.4424, grad_fn=<NegBackward0>) tensor(10815.4375, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10815.400390625
tensor(10815.4375, grad_fn=<NegBackward0>) tensor(10815.4004, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10815.3955078125
tensor(10815.4004, grad_fn=<NegBackward0>) tensor(10815.3955, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10815.3671875
tensor(10815.3955, grad_fn=<NegBackward0>) tensor(10815.3672, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10815.3623046875
tensor(10815.3672, grad_fn=<NegBackward0>) tensor(10815.3623, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10815.3603515625
tensor(10815.3623, grad_fn=<NegBackward0>) tensor(10815.3604, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10815.3173828125
tensor(10815.3604, grad_fn=<NegBackward0>) tensor(10815.3174, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10815.279296875
tensor(10815.3174, grad_fn=<NegBackward0>) tensor(10815.2793, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10815.2587890625
tensor(10815.2793, grad_fn=<NegBackward0>) tensor(10815.2588, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10815.255859375
tensor(10815.2588, grad_fn=<NegBackward0>) tensor(10815.2559, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10815.2548828125
tensor(10815.2559, grad_fn=<NegBackward0>) tensor(10815.2549, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10815.255859375
tensor(10815.2549, grad_fn=<NegBackward0>) tensor(10815.2559, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -10815.26171875
tensor(10815.2549, grad_fn=<NegBackward0>) tensor(10815.2617, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -10815.25
tensor(10815.2549, grad_fn=<NegBackward0>) tensor(10815.2500, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10815.244140625
tensor(10815.2500, grad_fn=<NegBackward0>) tensor(10815.2441, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10815.2431640625
tensor(10815.2441, grad_fn=<NegBackward0>) tensor(10815.2432, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10815.2294921875
tensor(10815.2432, grad_fn=<NegBackward0>) tensor(10815.2295, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10815.2255859375
tensor(10815.2295, grad_fn=<NegBackward0>) tensor(10815.2256, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10815.2216796875
tensor(10815.2256, grad_fn=<NegBackward0>) tensor(10815.2217, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10815.224609375
tensor(10815.2217, grad_fn=<NegBackward0>) tensor(10815.2246, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10815.2236328125
tensor(10815.2217, grad_fn=<NegBackward0>) tensor(10815.2236, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -10815.21875
tensor(10815.2217, grad_fn=<NegBackward0>) tensor(10815.2188, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10815.2138671875
tensor(10815.2188, grad_fn=<NegBackward0>) tensor(10815.2139, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10815.171875
tensor(10815.2139, grad_fn=<NegBackward0>) tensor(10815.1719, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10815.158203125
tensor(10815.1719, grad_fn=<NegBackward0>) tensor(10815.1582, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10815.1669921875
tensor(10815.1582, grad_fn=<NegBackward0>) tensor(10815.1670, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10815.158203125
tensor(10815.1582, grad_fn=<NegBackward0>) tensor(10815.1582, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10815.1572265625
tensor(10815.1582, grad_fn=<NegBackward0>) tensor(10815.1572, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10815.1552734375
tensor(10815.1572, grad_fn=<NegBackward0>) tensor(10815.1553, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10815.15234375
tensor(10815.1553, grad_fn=<NegBackward0>) tensor(10815.1523, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10815.150390625
tensor(10815.1523, grad_fn=<NegBackward0>) tensor(10815.1504, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10815.150390625
tensor(10815.1504, grad_fn=<NegBackward0>) tensor(10815.1504, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10815.1494140625
tensor(10815.1504, grad_fn=<NegBackward0>) tensor(10815.1494, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10815.150390625
tensor(10815.1494, grad_fn=<NegBackward0>) tensor(10815.1504, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10815.1494140625
tensor(10815.1494, grad_fn=<NegBackward0>) tensor(10815.1494, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10815.14453125
tensor(10815.1494, grad_fn=<NegBackward0>) tensor(10815.1445, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10815.126953125
tensor(10815.1445, grad_fn=<NegBackward0>) tensor(10815.1270, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10815.123046875
tensor(10815.1270, grad_fn=<NegBackward0>) tensor(10815.1230, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10815.10546875
tensor(10815.1230, grad_fn=<NegBackward0>) tensor(10815.1055, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10815.1064453125
tensor(10815.1055, grad_fn=<NegBackward0>) tensor(10815.1064, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10815.10546875
tensor(10815.1055, grad_fn=<NegBackward0>) tensor(10815.1055, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10815.10546875
tensor(10815.1055, grad_fn=<NegBackward0>) tensor(10815.1055, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10815.1142578125
tensor(10815.1055, grad_fn=<NegBackward0>) tensor(10815.1143, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10815.103515625
tensor(10815.1055, grad_fn=<NegBackward0>) tensor(10815.1035, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10814.806640625
tensor(10815.1035, grad_fn=<NegBackward0>) tensor(10814.8066, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10814.7919921875
tensor(10814.8066, grad_fn=<NegBackward0>) tensor(10814.7920, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10814.794921875
tensor(10814.7920, grad_fn=<NegBackward0>) tensor(10814.7949, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10814.78515625
tensor(10814.7920, grad_fn=<NegBackward0>) tensor(10814.7852, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10814.7861328125
tensor(10814.7852, grad_fn=<NegBackward0>) tensor(10814.7861, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10814.794921875
tensor(10814.7852, grad_fn=<NegBackward0>) tensor(10814.7949, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -10814.78515625
tensor(10814.7852, grad_fn=<NegBackward0>) tensor(10814.7852, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10814.7919921875
tensor(10814.7852, grad_fn=<NegBackward0>) tensor(10814.7920, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10814.78515625
tensor(10814.7852, grad_fn=<NegBackward0>) tensor(10814.7852, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10814.7861328125
tensor(10814.7852, grad_fn=<NegBackward0>) tensor(10814.7861, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -10814.7841796875
tensor(10814.7852, grad_fn=<NegBackward0>) tensor(10814.7842, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10814.78125
tensor(10814.7842, grad_fn=<NegBackward0>) tensor(10814.7812, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10814.78125
tensor(10814.7812, grad_fn=<NegBackward0>) tensor(10814.7812, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10814.7802734375
tensor(10814.7812, grad_fn=<NegBackward0>) tensor(10814.7803, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10814.7802734375
tensor(10814.7803, grad_fn=<NegBackward0>) tensor(10814.7803, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10814.7802734375
tensor(10814.7803, grad_fn=<NegBackward0>) tensor(10814.7803, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10814.78125
tensor(10814.7803, grad_fn=<NegBackward0>) tensor(10814.7812, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -10814.7685546875
tensor(10814.7803, grad_fn=<NegBackward0>) tensor(10814.7686, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10814.783203125
tensor(10814.7686, grad_fn=<NegBackward0>) tensor(10814.7832, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -10814.7646484375
tensor(10814.7686, grad_fn=<NegBackward0>) tensor(10814.7646, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10814.7705078125
tensor(10814.7646, grad_fn=<NegBackward0>) tensor(10814.7705, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -10814.765625
tensor(10814.7646, grad_fn=<NegBackward0>) tensor(10814.7656, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -10814.7646484375
tensor(10814.7646, grad_fn=<NegBackward0>) tensor(10814.7646, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10814.7646484375
tensor(10814.7646, grad_fn=<NegBackward0>) tensor(10814.7646, grad_fn=<NegBackward0>)
pi: tensor([[0.7719, 0.2281],
        [0.2377, 0.7623]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4569, 0.5431], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1931, 0.0994],
         [0.6470, 0.2464]],

        [[0.5512, 0.1105],
         [0.7266, 0.5980]],

        [[0.7135, 0.1041],
         [0.6985, 0.6965]],

        [[0.6434, 0.1017],
         [0.6788, 0.6440]],

        [[0.6297, 0.0947],
         [0.6694, 0.6263]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 1
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448326530612245
time is 3
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 91
Adjusted Rand Index: 0.669100416419012
time is 4
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369439308410987
Global Adjusted Rand Index: 0.8024203631013856
Average Adjusted Rand Index: 0.8033067131955802
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21953.9921875
inf tensor(21953.9922, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10921.54296875
tensor(21953.9922, grad_fn=<NegBackward0>) tensor(10921.5430, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10920.4609375
tensor(10921.5430, grad_fn=<NegBackward0>) tensor(10920.4609, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10919.9423828125
tensor(10920.4609, grad_fn=<NegBackward0>) tensor(10919.9424, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10919.7548828125
tensor(10919.9424, grad_fn=<NegBackward0>) tensor(10919.7549, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10919.5556640625
tensor(10919.7549, grad_fn=<NegBackward0>) tensor(10919.5557, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10919.3056640625
tensor(10919.5557, grad_fn=<NegBackward0>) tensor(10919.3057, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10919.017578125
tensor(10919.3057, grad_fn=<NegBackward0>) tensor(10919.0176, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10918.7822265625
tensor(10919.0176, grad_fn=<NegBackward0>) tensor(10918.7822, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10918.65625
tensor(10918.7822, grad_fn=<NegBackward0>) tensor(10918.6562, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10918.603515625
tensor(10918.6562, grad_fn=<NegBackward0>) tensor(10918.6035, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10918.5830078125
tensor(10918.6035, grad_fn=<NegBackward0>) tensor(10918.5830, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10918.5771484375
tensor(10918.5830, grad_fn=<NegBackward0>) tensor(10918.5771, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10918.5751953125
tensor(10918.5771, grad_fn=<NegBackward0>) tensor(10918.5752, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10918.57421875
tensor(10918.5752, grad_fn=<NegBackward0>) tensor(10918.5742, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10918.5732421875
tensor(10918.5742, grad_fn=<NegBackward0>) tensor(10918.5732, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10918.57421875
tensor(10918.5732, grad_fn=<NegBackward0>) tensor(10918.5742, grad_fn=<NegBackward0>)
1
Iteration 1700: Loss = -10918.5712890625
tensor(10918.5732, grad_fn=<NegBackward0>) tensor(10918.5713, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10918.572265625
tensor(10918.5713, grad_fn=<NegBackward0>) tensor(10918.5723, grad_fn=<NegBackward0>)
1
Iteration 1900: Loss = -10918.5703125
tensor(10918.5713, grad_fn=<NegBackward0>) tensor(10918.5703, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10918.5712890625
tensor(10918.5703, grad_fn=<NegBackward0>) tensor(10918.5713, grad_fn=<NegBackward0>)
1
Iteration 2100: Loss = -10918.5703125
tensor(10918.5703, grad_fn=<NegBackward0>) tensor(10918.5703, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10918.5703125
tensor(10918.5703, grad_fn=<NegBackward0>) tensor(10918.5703, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10918.5712890625
tensor(10918.5703, grad_fn=<NegBackward0>) tensor(10918.5713, grad_fn=<NegBackward0>)
1
Iteration 2400: Loss = -10918.5703125
tensor(10918.5703, grad_fn=<NegBackward0>) tensor(10918.5703, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10918.5693359375
tensor(10918.5703, grad_fn=<NegBackward0>) tensor(10918.5693, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10918.5693359375
tensor(10918.5693, grad_fn=<NegBackward0>) tensor(10918.5693, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10918.568359375
tensor(10918.5693, grad_fn=<NegBackward0>) tensor(10918.5684, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10918.5693359375
tensor(10918.5684, grad_fn=<NegBackward0>) tensor(10918.5693, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -10918.568359375
tensor(10918.5684, grad_fn=<NegBackward0>) tensor(10918.5684, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10918.5693359375
tensor(10918.5684, grad_fn=<NegBackward0>) tensor(10918.5693, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -10918.568359375
tensor(10918.5684, grad_fn=<NegBackward0>) tensor(10918.5684, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10918.568359375
tensor(10918.5684, grad_fn=<NegBackward0>) tensor(10918.5684, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10918.5673828125
tensor(10918.5684, grad_fn=<NegBackward0>) tensor(10918.5674, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10918.5673828125
tensor(10918.5674, grad_fn=<NegBackward0>) tensor(10918.5674, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10918.56640625
tensor(10918.5674, grad_fn=<NegBackward0>) tensor(10918.5664, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10918.56640625
tensor(10918.5664, grad_fn=<NegBackward0>) tensor(10918.5664, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10918.56640625
tensor(10918.5664, grad_fn=<NegBackward0>) tensor(10918.5664, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10918.56640625
tensor(10918.5664, grad_fn=<NegBackward0>) tensor(10918.5664, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10918.56640625
tensor(10918.5664, grad_fn=<NegBackward0>) tensor(10918.5664, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10918.5654296875
tensor(10918.5664, grad_fn=<NegBackward0>) tensor(10918.5654, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10918.564453125
tensor(10918.5654, grad_fn=<NegBackward0>) tensor(10918.5645, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10918.56640625
tensor(10918.5645, grad_fn=<NegBackward0>) tensor(10918.5664, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10918.5654296875
tensor(10918.5645, grad_fn=<NegBackward0>) tensor(10918.5654, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -10918.564453125
tensor(10918.5645, grad_fn=<NegBackward0>) tensor(10918.5645, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10918.5654296875
tensor(10918.5645, grad_fn=<NegBackward0>) tensor(10918.5654, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10918.5654296875
tensor(10918.5645, grad_fn=<NegBackward0>) tensor(10918.5654, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -10918.564453125
tensor(10918.5645, grad_fn=<NegBackward0>) tensor(10918.5645, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10918.564453125
tensor(10918.5645, grad_fn=<NegBackward0>) tensor(10918.5645, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10918.5654296875
tensor(10918.5645, grad_fn=<NegBackward0>) tensor(10918.5654, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10918.564453125
tensor(10918.5645, grad_fn=<NegBackward0>) tensor(10918.5645, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10918.5654296875
tensor(10918.5645, grad_fn=<NegBackward0>) tensor(10918.5654, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10918.564453125
tensor(10918.5645, grad_fn=<NegBackward0>) tensor(10918.5645, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10918.5634765625
tensor(10918.5645, grad_fn=<NegBackward0>) tensor(10918.5635, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10918.5634765625
tensor(10918.5635, grad_fn=<NegBackward0>) tensor(10918.5635, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10918.5634765625
tensor(10918.5635, grad_fn=<NegBackward0>) tensor(10918.5635, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10918.5634765625
tensor(10918.5635, grad_fn=<NegBackward0>) tensor(10918.5635, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10918.564453125
tensor(10918.5635, grad_fn=<NegBackward0>) tensor(10918.5645, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10918.5634765625
tensor(10918.5635, grad_fn=<NegBackward0>) tensor(10918.5635, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10918.564453125
tensor(10918.5635, grad_fn=<NegBackward0>) tensor(10918.5645, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10918.564453125
tensor(10918.5635, grad_fn=<NegBackward0>) tensor(10918.5645, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -10918.564453125
tensor(10918.5635, grad_fn=<NegBackward0>) tensor(10918.5645, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -10918.5625
tensor(10918.5635, grad_fn=<NegBackward0>) tensor(10918.5625, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10918.5634765625
tensor(10918.5625, grad_fn=<NegBackward0>) tensor(10918.5635, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10918.5634765625
tensor(10918.5625, grad_fn=<NegBackward0>) tensor(10918.5635, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -10918.5634765625
tensor(10918.5625, grad_fn=<NegBackward0>) tensor(10918.5635, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -10918.5634765625
tensor(10918.5625, grad_fn=<NegBackward0>) tensor(10918.5635, grad_fn=<NegBackward0>)
4
Iteration 6700: Loss = -10918.5634765625
tensor(10918.5625, grad_fn=<NegBackward0>) tensor(10918.5635, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6700 due to no improvement.
pi: tensor([[0.0012, 0.9988],
        [0.0193, 0.9807]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1124, 0.8876], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3117, 0.2075],
         [0.6536, 0.1578]],

        [[0.5113, 0.2320],
         [0.6584, 0.7202]],

        [[0.7031, 0.1911],
         [0.6622, 0.5701]],

        [[0.7271, 0.1397],
         [0.6391, 0.6020]],

        [[0.5236, 0.1613],
         [0.7071, 0.6667]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.003243945514707375
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00048132174495227055
Average Adjusted Rand Index: -0.000648789102941475
[0.8024203631013856, 0.00048132174495227055] [0.8033067131955802, -0.000648789102941475] [10814.78515625, 10918.5634765625]
-------------------------------------
This iteration is 15
True Objective function: Loss = -11051.134702805504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21216.966796875
inf tensor(21216.9668, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11141.7646484375
tensor(21216.9668, grad_fn=<NegBackward0>) tensor(11141.7646, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11141.5126953125
tensor(11141.7646, grad_fn=<NegBackward0>) tensor(11141.5127, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11141.3466796875
tensor(11141.5127, grad_fn=<NegBackward0>) tensor(11141.3467, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11141.0908203125
tensor(11141.3467, grad_fn=<NegBackward0>) tensor(11141.0908, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11140.6044921875
tensor(11141.0908, grad_fn=<NegBackward0>) tensor(11140.6045, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11140.2724609375
tensor(11140.6045, grad_fn=<NegBackward0>) tensor(11140.2725, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11140.0205078125
tensor(11140.2725, grad_fn=<NegBackward0>) tensor(11140.0205, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11139.7001953125
tensor(11140.0205, grad_fn=<NegBackward0>) tensor(11139.7002, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11139.5478515625
tensor(11139.7002, grad_fn=<NegBackward0>) tensor(11139.5479, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11139.462890625
tensor(11139.5479, grad_fn=<NegBackward0>) tensor(11139.4629, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11139.3837890625
tensor(11139.4629, grad_fn=<NegBackward0>) tensor(11139.3838, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11139.2841796875
tensor(11139.3838, grad_fn=<NegBackward0>) tensor(11139.2842, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11139.1552734375
tensor(11139.2842, grad_fn=<NegBackward0>) tensor(11139.1553, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11138.9833984375
tensor(11139.1553, grad_fn=<NegBackward0>) tensor(11138.9834, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11138.7705078125
tensor(11138.9834, grad_fn=<NegBackward0>) tensor(11138.7705, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11138.537109375
tensor(11138.7705, grad_fn=<NegBackward0>) tensor(11138.5371, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11138.3310546875
tensor(11138.5371, grad_fn=<NegBackward0>) tensor(11138.3311, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11138.1708984375
tensor(11138.3311, grad_fn=<NegBackward0>) tensor(11138.1709, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11138.0537109375
tensor(11138.1709, grad_fn=<NegBackward0>) tensor(11138.0537, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11137.9765625
tensor(11138.0537, grad_fn=<NegBackward0>) tensor(11137.9766, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11137.9365234375
tensor(11137.9766, grad_fn=<NegBackward0>) tensor(11137.9365, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11137.9169921875
tensor(11137.9365, grad_fn=<NegBackward0>) tensor(11137.9170, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11137.9052734375
tensor(11137.9170, grad_fn=<NegBackward0>) tensor(11137.9053, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11137.89453125
tensor(11137.9053, grad_fn=<NegBackward0>) tensor(11137.8945, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11137.87890625
tensor(11137.8945, grad_fn=<NegBackward0>) tensor(11137.8789, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11137.8515625
tensor(11137.8789, grad_fn=<NegBackward0>) tensor(11137.8516, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11137.79296875
tensor(11137.8516, grad_fn=<NegBackward0>) tensor(11137.7930, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11137.681640625
tensor(11137.7930, grad_fn=<NegBackward0>) tensor(11137.6816, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11137.556640625
tensor(11137.6816, grad_fn=<NegBackward0>) tensor(11137.5566, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11137.478515625
tensor(11137.5566, grad_fn=<NegBackward0>) tensor(11137.4785, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11137.4150390625
tensor(11137.4785, grad_fn=<NegBackward0>) tensor(11137.4150, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11137.20703125
tensor(11137.4150, grad_fn=<NegBackward0>) tensor(11137.2070, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11136.8251953125
tensor(11137.2070, grad_fn=<NegBackward0>) tensor(11136.8252, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11132.49609375
tensor(11136.8252, grad_fn=<NegBackward0>) tensor(11132.4961, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11132.021484375
tensor(11132.4961, grad_fn=<NegBackward0>) tensor(11132.0215, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11131.9658203125
tensor(11132.0215, grad_fn=<NegBackward0>) tensor(11131.9658, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11131.9443359375
tensor(11131.9658, grad_fn=<NegBackward0>) tensor(11131.9443, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11131.935546875
tensor(11131.9443, grad_fn=<NegBackward0>) tensor(11131.9355, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11131.9267578125
tensor(11131.9355, grad_fn=<NegBackward0>) tensor(11131.9268, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11131.923828125
tensor(11131.9268, grad_fn=<NegBackward0>) tensor(11131.9238, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11131.9208984375
tensor(11131.9238, grad_fn=<NegBackward0>) tensor(11131.9209, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11131.9169921875
tensor(11131.9209, grad_fn=<NegBackward0>) tensor(11131.9170, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11131.916015625
tensor(11131.9170, grad_fn=<NegBackward0>) tensor(11131.9160, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11131.9150390625
tensor(11131.9160, grad_fn=<NegBackward0>) tensor(11131.9150, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11131.9130859375
tensor(11131.9150, grad_fn=<NegBackward0>) tensor(11131.9131, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11131.9130859375
tensor(11131.9131, grad_fn=<NegBackward0>) tensor(11131.9131, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11131.9111328125
tensor(11131.9131, grad_fn=<NegBackward0>) tensor(11131.9111, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11131.91015625
tensor(11131.9111, grad_fn=<NegBackward0>) tensor(11131.9102, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11131.9091796875
tensor(11131.9102, grad_fn=<NegBackward0>) tensor(11131.9092, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11131.9091796875
tensor(11131.9092, grad_fn=<NegBackward0>) tensor(11131.9092, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11131.9072265625
tensor(11131.9092, grad_fn=<NegBackward0>) tensor(11131.9072, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11131.908203125
tensor(11131.9072, grad_fn=<NegBackward0>) tensor(11131.9082, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11131.9072265625
tensor(11131.9072, grad_fn=<NegBackward0>) tensor(11131.9072, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11131.90625
tensor(11131.9072, grad_fn=<NegBackward0>) tensor(11131.9062, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11131.90625
tensor(11131.9062, grad_fn=<NegBackward0>) tensor(11131.9062, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11131.9072265625
tensor(11131.9062, grad_fn=<NegBackward0>) tensor(11131.9072, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11131.9052734375
tensor(11131.9062, grad_fn=<NegBackward0>) tensor(11131.9053, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11131.9052734375
tensor(11131.9053, grad_fn=<NegBackward0>) tensor(11131.9053, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11131.9052734375
tensor(11131.9053, grad_fn=<NegBackward0>) tensor(11131.9053, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11131.904296875
tensor(11131.9053, grad_fn=<NegBackward0>) tensor(11131.9043, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11131.904296875
tensor(11131.9043, grad_fn=<NegBackward0>) tensor(11131.9043, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11131.90625
tensor(11131.9043, grad_fn=<NegBackward0>) tensor(11131.9062, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11131.9033203125
tensor(11131.9043, grad_fn=<NegBackward0>) tensor(11131.9033, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11131.9052734375
tensor(11131.9033, grad_fn=<NegBackward0>) tensor(11131.9053, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11131.92578125
tensor(11131.9033, grad_fn=<NegBackward0>) tensor(11131.9258, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11131.9033203125
tensor(11131.9033, grad_fn=<NegBackward0>) tensor(11131.9033, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11131.904296875
tensor(11131.9033, grad_fn=<NegBackward0>) tensor(11131.9043, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11131.90234375
tensor(11131.9033, grad_fn=<NegBackward0>) tensor(11131.9023, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11131.90234375
tensor(11131.9023, grad_fn=<NegBackward0>) tensor(11131.9023, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11131.9033203125
tensor(11131.9023, grad_fn=<NegBackward0>) tensor(11131.9033, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11131.90234375
tensor(11131.9023, grad_fn=<NegBackward0>) tensor(11131.9023, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11132.064453125
tensor(11131.9023, grad_fn=<NegBackward0>) tensor(11132.0645, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11131.904296875
tensor(11131.9023, grad_fn=<NegBackward0>) tensor(11131.9043, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11131.904296875
tensor(11131.9023, grad_fn=<NegBackward0>) tensor(11131.9043, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11131.904296875
tensor(11131.9023, grad_fn=<NegBackward0>) tensor(11131.9043, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -11131.90234375
tensor(11131.9023, grad_fn=<NegBackward0>) tensor(11131.9023, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11131.9736328125
tensor(11131.9023, grad_fn=<NegBackward0>) tensor(11131.9736, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11131.90234375
tensor(11131.9023, grad_fn=<NegBackward0>) tensor(11131.9023, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11131.900390625
tensor(11131.9023, grad_fn=<NegBackward0>) tensor(11131.9004, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11131.91015625
tensor(11131.9004, grad_fn=<NegBackward0>) tensor(11131.9102, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11131.900390625
tensor(11131.9004, grad_fn=<NegBackward0>) tensor(11131.9004, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11131.90234375
tensor(11131.9004, grad_fn=<NegBackward0>) tensor(11131.9023, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11131.90234375
tensor(11131.9004, grad_fn=<NegBackward0>) tensor(11131.9023, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11131.9013671875
tensor(11131.9004, grad_fn=<NegBackward0>) tensor(11131.9014, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -11131.90234375
tensor(11131.9004, grad_fn=<NegBackward0>) tensor(11131.9023, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -11131.9033203125
tensor(11131.9004, grad_fn=<NegBackward0>) tensor(11131.9033, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[9.9999e-01, 6.5928e-06],
        [5.6260e-01, 4.3740e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0134, 0.9866], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1797, 0.0914],
         [0.5543, 0.1564]],

        [[0.5777, 0.1711],
         [0.6993, 0.6857]],

        [[0.5330, 0.1311],
         [0.5213, 0.6939]],

        [[0.7025, 0.1028],
         [0.6290, 0.7046]],

        [[0.5743, 0.0964],
         [0.6186, 0.5700]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.01836565232257119
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.021492879445994654
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.007272876449007818
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 38
Adjusted Rand Index: 0.02868762140155424
Global Adjusted Rand Index: 0.014764533516511117
Average Adjusted Rand Index: 0.012254655344222454
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23440.83984375
inf tensor(23440.8398, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11142.642578125
tensor(23440.8398, grad_fn=<NegBackward0>) tensor(11142.6426, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11141.677734375
tensor(11142.6426, grad_fn=<NegBackward0>) tensor(11141.6777, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11141.455078125
tensor(11141.6777, grad_fn=<NegBackward0>) tensor(11141.4551, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11141.2822265625
tensor(11141.4551, grad_fn=<NegBackward0>) tensor(11141.2822, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11141.0703125
tensor(11141.2822, grad_fn=<NegBackward0>) tensor(11141.0703, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11140.779296875
tensor(11141.0703, grad_fn=<NegBackward0>) tensor(11140.7793, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11140.466796875
tensor(11140.7793, grad_fn=<NegBackward0>) tensor(11140.4668, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11140.2021484375
tensor(11140.4668, grad_fn=<NegBackward0>) tensor(11140.2021, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11140.0
tensor(11140.2021, grad_fn=<NegBackward0>) tensor(11140., grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11139.841796875
tensor(11140., grad_fn=<NegBackward0>) tensor(11139.8418, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11139.7099609375
tensor(11139.8418, grad_fn=<NegBackward0>) tensor(11139.7100, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11139.6005859375
tensor(11139.7100, grad_fn=<NegBackward0>) tensor(11139.6006, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11139.5048828125
tensor(11139.6006, grad_fn=<NegBackward0>) tensor(11139.5049, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11139.4169921875
tensor(11139.5049, grad_fn=<NegBackward0>) tensor(11139.4170, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11139.3291015625
tensor(11139.4170, grad_fn=<NegBackward0>) tensor(11139.3291, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11139.2412109375
tensor(11139.3291, grad_fn=<NegBackward0>) tensor(11139.2412, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11139.1484375
tensor(11139.2412, grad_fn=<NegBackward0>) tensor(11139.1484, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11139.0478515625
tensor(11139.1484, grad_fn=<NegBackward0>) tensor(11139.0479, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11138.8955078125
tensor(11139.0479, grad_fn=<NegBackward0>) tensor(11138.8955, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11138.6396484375
tensor(11138.8955, grad_fn=<NegBackward0>) tensor(11138.6396, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11138.3857421875
tensor(11138.6396, grad_fn=<NegBackward0>) tensor(11138.3857, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11138.2587890625
tensor(11138.3857, grad_fn=<NegBackward0>) tensor(11138.2588, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11138.224609375
tensor(11138.2588, grad_fn=<NegBackward0>) tensor(11138.2246, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11138.142578125
tensor(11138.2246, grad_fn=<NegBackward0>) tensor(11138.1426, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11138.0107421875
tensor(11138.1426, grad_fn=<NegBackward0>) tensor(11138.0107, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11137.7861328125
tensor(11138.0107, grad_fn=<NegBackward0>) tensor(11137.7861, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11137.3193359375
tensor(11137.7861, grad_fn=<NegBackward0>) tensor(11137.3193, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11136.654296875
tensor(11137.3193, grad_fn=<NegBackward0>) tensor(11136.6543, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11136.0673828125
tensor(11136.6543, grad_fn=<NegBackward0>) tensor(11136.0674, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11135.7451171875
tensor(11136.0674, grad_fn=<NegBackward0>) tensor(11135.7451, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11135.5947265625
tensor(11135.7451, grad_fn=<NegBackward0>) tensor(11135.5947, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11135.5224609375
tensor(11135.5947, grad_fn=<NegBackward0>) tensor(11135.5225, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11135.4853515625
tensor(11135.5225, grad_fn=<NegBackward0>) tensor(11135.4854, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11135.4619140625
tensor(11135.4854, grad_fn=<NegBackward0>) tensor(11135.4619, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11135.4482421875
tensor(11135.4619, grad_fn=<NegBackward0>) tensor(11135.4482, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11135.435546875
tensor(11135.4482, grad_fn=<NegBackward0>) tensor(11135.4355, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11135.427734375
tensor(11135.4355, grad_fn=<NegBackward0>) tensor(11135.4277, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11135.421875
tensor(11135.4277, grad_fn=<NegBackward0>) tensor(11135.4219, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11135.4169921875
tensor(11135.4219, grad_fn=<NegBackward0>) tensor(11135.4170, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11135.4130859375
tensor(11135.4170, grad_fn=<NegBackward0>) tensor(11135.4131, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11135.41015625
tensor(11135.4131, grad_fn=<NegBackward0>) tensor(11135.4102, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11135.4072265625
tensor(11135.4102, grad_fn=<NegBackward0>) tensor(11135.4072, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11135.404296875
tensor(11135.4072, grad_fn=<NegBackward0>) tensor(11135.4043, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11135.40234375
tensor(11135.4043, grad_fn=<NegBackward0>) tensor(11135.4023, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11135.400390625
tensor(11135.4023, grad_fn=<NegBackward0>) tensor(11135.4004, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11135.3984375
tensor(11135.4004, grad_fn=<NegBackward0>) tensor(11135.3984, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11135.3974609375
tensor(11135.3984, grad_fn=<NegBackward0>) tensor(11135.3975, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11135.3974609375
tensor(11135.3975, grad_fn=<NegBackward0>) tensor(11135.3975, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11135.3955078125
tensor(11135.3975, grad_fn=<NegBackward0>) tensor(11135.3955, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11135.3935546875
tensor(11135.3955, grad_fn=<NegBackward0>) tensor(11135.3936, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11135.392578125
tensor(11135.3936, grad_fn=<NegBackward0>) tensor(11135.3926, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11135.3935546875
tensor(11135.3926, grad_fn=<NegBackward0>) tensor(11135.3936, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11135.3916015625
tensor(11135.3926, grad_fn=<NegBackward0>) tensor(11135.3916, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11135.390625
tensor(11135.3916, grad_fn=<NegBackward0>) tensor(11135.3906, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11135.390625
tensor(11135.3906, grad_fn=<NegBackward0>) tensor(11135.3906, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11135.3896484375
tensor(11135.3906, grad_fn=<NegBackward0>) tensor(11135.3896, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11135.3896484375
tensor(11135.3896, grad_fn=<NegBackward0>) tensor(11135.3896, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11135.3896484375
tensor(11135.3896, grad_fn=<NegBackward0>) tensor(11135.3896, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11135.3876953125
tensor(11135.3896, grad_fn=<NegBackward0>) tensor(11135.3877, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11135.3876953125
tensor(11135.3877, grad_fn=<NegBackward0>) tensor(11135.3877, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11135.3876953125
tensor(11135.3877, grad_fn=<NegBackward0>) tensor(11135.3877, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11135.38671875
tensor(11135.3877, grad_fn=<NegBackward0>) tensor(11135.3867, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11135.38671875
tensor(11135.3867, grad_fn=<NegBackward0>) tensor(11135.3867, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11135.38671875
tensor(11135.3867, grad_fn=<NegBackward0>) tensor(11135.3867, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11135.38671875
tensor(11135.3867, grad_fn=<NegBackward0>) tensor(11135.3867, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11135.384765625
tensor(11135.3867, grad_fn=<NegBackward0>) tensor(11135.3848, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11135.384765625
tensor(11135.3848, grad_fn=<NegBackward0>) tensor(11135.3848, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11135.384765625
tensor(11135.3848, grad_fn=<NegBackward0>) tensor(11135.3848, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11135.384765625
tensor(11135.3848, grad_fn=<NegBackward0>) tensor(11135.3848, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11135.3857421875
tensor(11135.3848, grad_fn=<NegBackward0>) tensor(11135.3857, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11135.3857421875
tensor(11135.3848, grad_fn=<NegBackward0>) tensor(11135.3857, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11135.384765625
tensor(11135.3848, grad_fn=<NegBackward0>) tensor(11135.3848, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11135.3837890625
tensor(11135.3848, grad_fn=<NegBackward0>) tensor(11135.3838, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11135.384765625
tensor(11135.3838, grad_fn=<NegBackward0>) tensor(11135.3848, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11135.384765625
tensor(11135.3838, grad_fn=<NegBackward0>) tensor(11135.3848, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11135.3837890625
tensor(11135.3838, grad_fn=<NegBackward0>) tensor(11135.3838, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11135.3837890625
tensor(11135.3838, grad_fn=<NegBackward0>) tensor(11135.3838, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11135.3837890625
tensor(11135.3838, grad_fn=<NegBackward0>) tensor(11135.3838, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11135.3828125
tensor(11135.3838, grad_fn=<NegBackward0>) tensor(11135.3828, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11135.3828125
tensor(11135.3828, grad_fn=<NegBackward0>) tensor(11135.3828, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11135.384765625
tensor(11135.3828, grad_fn=<NegBackward0>) tensor(11135.3848, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11135.384765625
tensor(11135.3828, grad_fn=<NegBackward0>) tensor(11135.3848, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11135.3828125
tensor(11135.3828, grad_fn=<NegBackward0>) tensor(11135.3828, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11135.3828125
tensor(11135.3828, grad_fn=<NegBackward0>) tensor(11135.3828, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11135.3828125
tensor(11135.3828, grad_fn=<NegBackward0>) tensor(11135.3828, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11135.3828125
tensor(11135.3828, grad_fn=<NegBackward0>) tensor(11135.3828, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11135.380859375
tensor(11135.3828, grad_fn=<NegBackward0>) tensor(11135.3809, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11135.384765625
tensor(11135.3809, grad_fn=<NegBackward0>) tensor(11135.3848, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11135.3837890625
tensor(11135.3809, grad_fn=<NegBackward0>) tensor(11135.3838, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -11135.3837890625
tensor(11135.3809, grad_fn=<NegBackward0>) tensor(11135.3838, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -11135.3818359375
tensor(11135.3809, grad_fn=<NegBackward0>) tensor(11135.3818, grad_fn=<NegBackward0>)
4
Iteration 9200: Loss = -11135.412109375
tensor(11135.3809, grad_fn=<NegBackward0>) tensor(11135.4121, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9200 due to no improvement.
pi: tensor([[9.5403e-01, 4.5967e-02],
        [9.9987e-01, 1.2826e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0040, 0.9960], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1727, 0.1557],
         [0.6589, 0.1548]],

        [[0.5149, 0.1743],
         [0.6769, 0.6247]],

        [[0.5271, 0.1055],
         [0.6312, 0.6965]],

        [[0.5814, 0.0780],
         [0.7175, 0.6333]],

        [[0.5722, 0.1843],
         [0.6085, 0.6378]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.0032454170932134756
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0002798404793684107
Average Adjusted Rand Index: -0.0008060064462963757
[0.014764533516511117, 0.0002798404793684107] [0.012254655344222454, -0.0008060064462963757] [11131.9033203125, 11135.412109375]
-------------------------------------
This iteration is 16
True Objective function: Loss = -10692.69215580235
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20387.4296875
inf tensor(20387.4297, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10769.509765625
tensor(20387.4297, grad_fn=<NegBackward0>) tensor(10769.5098, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10769.1201171875
tensor(10769.5098, grad_fn=<NegBackward0>) tensor(10769.1201, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10769.015625
tensor(10769.1201, grad_fn=<NegBackward0>) tensor(10769.0156, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10768.904296875
tensor(10769.0156, grad_fn=<NegBackward0>) tensor(10768.9043, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10768.7333984375
tensor(10768.9043, grad_fn=<NegBackward0>) tensor(10768.7334, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10768.2568359375
tensor(10768.7334, grad_fn=<NegBackward0>) tensor(10768.2568, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10767.5517578125
tensor(10768.2568, grad_fn=<NegBackward0>) tensor(10767.5518, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10766.6826171875
tensor(10767.5518, grad_fn=<NegBackward0>) tensor(10766.6826, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10766.4326171875
tensor(10766.6826, grad_fn=<NegBackward0>) tensor(10766.4326, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10766.37890625
tensor(10766.4326, grad_fn=<NegBackward0>) tensor(10766.3789, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10766.3544921875
tensor(10766.3789, grad_fn=<NegBackward0>) tensor(10766.3545, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10766.3408203125
tensor(10766.3545, grad_fn=<NegBackward0>) tensor(10766.3408, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10766.3310546875
tensor(10766.3408, grad_fn=<NegBackward0>) tensor(10766.3311, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10766.322265625
tensor(10766.3311, grad_fn=<NegBackward0>) tensor(10766.3223, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10766.3193359375
tensor(10766.3223, grad_fn=<NegBackward0>) tensor(10766.3193, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10766.314453125
tensor(10766.3193, grad_fn=<NegBackward0>) tensor(10766.3145, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10766.3115234375
tensor(10766.3145, grad_fn=<NegBackward0>) tensor(10766.3115, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10766.3095703125
tensor(10766.3115, grad_fn=<NegBackward0>) tensor(10766.3096, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10766.306640625
tensor(10766.3096, grad_fn=<NegBackward0>) tensor(10766.3066, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10766.3037109375
tensor(10766.3066, grad_fn=<NegBackward0>) tensor(10766.3037, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10766.3037109375
tensor(10766.3037, grad_fn=<NegBackward0>) tensor(10766.3037, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10766.30078125
tensor(10766.3037, grad_fn=<NegBackward0>) tensor(10766.3008, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10766.2998046875
tensor(10766.3008, grad_fn=<NegBackward0>) tensor(10766.2998, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10766.298828125
tensor(10766.2998, grad_fn=<NegBackward0>) tensor(10766.2988, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10766.2978515625
tensor(10766.2988, grad_fn=<NegBackward0>) tensor(10766.2979, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10766.296875
tensor(10766.2979, grad_fn=<NegBackward0>) tensor(10766.2969, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10766.294921875
tensor(10766.2969, grad_fn=<NegBackward0>) tensor(10766.2949, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10766.2958984375
tensor(10766.2949, grad_fn=<NegBackward0>) tensor(10766.2959, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -10766.2939453125
tensor(10766.2949, grad_fn=<NegBackward0>) tensor(10766.2939, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10766.2939453125
tensor(10766.2939, grad_fn=<NegBackward0>) tensor(10766.2939, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10766.2939453125
tensor(10766.2939, grad_fn=<NegBackward0>) tensor(10766.2939, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10766.29296875
tensor(10766.2939, grad_fn=<NegBackward0>) tensor(10766.2930, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10766.2919921875
tensor(10766.2930, grad_fn=<NegBackward0>) tensor(10766.2920, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10766.29296875
tensor(10766.2920, grad_fn=<NegBackward0>) tensor(10766.2930, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -10766.2939453125
tensor(10766.2920, grad_fn=<NegBackward0>) tensor(10766.2939, grad_fn=<NegBackward0>)
2
Iteration 3600: Loss = -10766.291015625
tensor(10766.2920, grad_fn=<NegBackward0>) tensor(10766.2910, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10766.29296875
tensor(10766.2910, grad_fn=<NegBackward0>) tensor(10766.2930, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -10766.291015625
tensor(10766.2910, grad_fn=<NegBackward0>) tensor(10766.2910, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10766.2900390625
tensor(10766.2910, grad_fn=<NegBackward0>) tensor(10766.2900, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10766.3056640625
tensor(10766.2900, grad_fn=<NegBackward0>) tensor(10766.3057, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -10766.2890625
tensor(10766.2900, grad_fn=<NegBackward0>) tensor(10766.2891, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10766.291015625
tensor(10766.2891, grad_fn=<NegBackward0>) tensor(10766.2910, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10766.2890625
tensor(10766.2891, grad_fn=<NegBackward0>) tensor(10766.2891, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10766.2900390625
tensor(10766.2891, grad_fn=<NegBackward0>) tensor(10766.2900, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10766.2890625
tensor(10766.2891, grad_fn=<NegBackward0>) tensor(10766.2891, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10766.2890625
tensor(10766.2891, grad_fn=<NegBackward0>) tensor(10766.2891, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10766.2900390625
tensor(10766.2891, grad_fn=<NegBackward0>) tensor(10766.2900, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10766.2880859375
tensor(10766.2891, grad_fn=<NegBackward0>) tensor(10766.2881, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10766.2890625
tensor(10766.2881, grad_fn=<NegBackward0>) tensor(10766.2891, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10766.2880859375
tensor(10766.2881, grad_fn=<NegBackward0>) tensor(10766.2881, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10766.2880859375
tensor(10766.2881, grad_fn=<NegBackward0>) tensor(10766.2881, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10766.2880859375
tensor(10766.2881, grad_fn=<NegBackward0>) tensor(10766.2881, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10766.2880859375
tensor(10766.2881, grad_fn=<NegBackward0>) tensor(10766.2881, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10766.2880859375
tensor(10766.2881, grad_fn=<NegBackward0>) tensor(10766.2881, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10766.287109375
tensor(10766.2881, grad_fn=<NegBackward0>) tensor(10766.2871, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10766.2880859375
tensor(10766.2871, grad_fn=<NegBackward0>) tensor(10766.2881, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10766.287109375
tensor(10766.2871, grad_fn=<NegBackward0>) tensor(10766.2871, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10766.2880859375
tensor(10766.2871, grad_fn=<NegBackward0>) tensor(10766.2881, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10766.287109375
tensor(10766.2871, grad_fn=<NegBackward0>) tensor(10766.2871, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10766.2880859375
tensor(10766.2871, grad_fn=<NegBackward0>) tensor(10766.2881, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10766.2880859375
tensor(10766.2871, grad_fn=<NegBackward0>) tensor(10766.2881, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -10766.287109375
tensor(10766.2871, grad_fn=<NegBackward0>) tensor(10766.2871, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10766.2880859375
tensor(10766.2871, grad_fn=<NegBackward0>) tensor(10766.2881, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10766.2880859375
tensor(10766.2871, grad_fn=<NegBackward0>) tensor(10766.2881, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -10766.294921875
tensor(10766.2871, grad_fn=<NegBackward0>) tensor(10766.2949, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -10766.2880859375
tensor(10766.2871, grad_fn=<NegBackward0>) tensor(10766.2881, grad_fn=<NegBackward0>)
4
Iteration 6700: Loss = -10766.2900390625
tensor(10766.2871, grad_fn=<NegBackward0>) tensor(10766.2900, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6700 due to no improvement.
pi: tensor([[9.3768e-01, 6.2319e-02],
        [3.0531e-04, 9.9969e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9983e-01, 1.7424e-04], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1509, 0.1431],
         [0.5565, 0.2103]],

        [[0.5447, 0.1871],
         [0.5953, 0.6019]],

        [[0.7024, 0.1846],
         [0.6707, 0.6727]],

        [[0.7238, 0.1640],
         [0.6787, 0.5619]],

        [[0.6743, 0.1836],
         [0.6572, 0.5903]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.009009494263563287
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0014922741295917668
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.01851330918808678
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: 0.005065160483418353
Average Adjusted Rand Index: 0.0024911970478752343
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23840.787109375
inf tensor(23840.7871, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10770.599609375
tensor(23840.7871, grad_fn=<NegBackward0>) tensor(10770.5996, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10770.2958984375
tensor(10770.5996, grad_fn=<NegBackward0>) tensor(10770.2959, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10770.1875
tensor(10770.2959, grad_fn=<NegBackward0>) tensor(10770.1875, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10770.1123046875
tensor(10770.1875, grad_fn=<NegBackward0>) tensor(10770.1123, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10770.0439453125
tensor(10770.1123, grad_fn=<NegBackward0>) tensor(10770.0439, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10769.9560546875
tensor(10770.0439, grad_fn=<NegBackward0>) tensor(10769.9561, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10769.8046875
tensor(10769.9561, grad_fn=<NegBackward0>) tensor(10769.8047, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10769.69140625
tensor(10769.8047, grad_fn=<NegBackward0>) tensor(10769.6914, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10769.544921875
tensor(10769.6914, grad_fn=<NegBackward0>) tensor(10769.5449, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10769.2744140625
tensor(10769.5449, grad_fn=<NegBackward0>) tensor(10769.2744, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10768.9921875
tensor(10769.2744, grad_fn=<NegBackward0>) tensor(10768.9922, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10768.796875
tensor(10768.9922, grad_fn=<NegBackward0>) tensor(10768.7969, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10768.583984375
tensor(10768.7969, grad_fn=<NegBackward0>) tensor(10768.5840, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10768.27734375
tensor(10768.5840, grad_fn=<NegBackward0>) tensor(10768.2773, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10767.9638671875
tensor(10768.2773, grad_fn=<NegBackward0>) tensor(10767.9639, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10767.6708984375
tensor(10767.9639, grad_fn=<NegBackward0>) tensor(10767.6709, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10767.53515625
tensor(10767.6709, grad_fn=<NegBackward0>) tensor(10767.5352, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10767.509765625
tensor(10767.5352, grad_fn=<NegBackward0>) tensor(10767.5098, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10767.501953125
tensor(10767.5098, grad_fn=<NegBackward0>) tensor(10767.5020, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10767.4990234375
tensor(10767.5020, grad_fn=<NegBackward0>) tensor(10767.4990, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10767.4990234375
tensor(10767.4990, grad_fn=<NegBackward0>) tensor(10767.4990, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10767.4970703125
tensor(10767.4990, grad_fn=<NegBackward0>) tensor(10767.4971, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10767.49609375
tensor(10767.4971, grad_fn=<NegBackward0>) tensor(10767.4961, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10767.4970703125
tensor(10767.4961, grad_fn=<NegBackward0>) tensor(10767.4971, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -10767.49609375
tensor(10767.4961, grad_fn=<NegBackward0>) tensor(10767.4961, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10767.494140625
tensor(10767.4961, grad_fn=<NegBackward0>) tensor(10767.4941, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10767.4921875
tensor(10767.4941, grad_fn=<NegBackward0>) tensor(10767.4922, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10767.4892578125
tensor(10767.4922, grad_fn=<NegBackward0>) tensor(10767.4893, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10767.4873046875
tensor(10767.4893, grad_fn=<NegBackward0>) tensor(10767.4873, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10767.4794921875
tensor(10767.4873, grad_fn=<NegBackward0>) tensor(10767.4795, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10767.466796875
tensor(10767.4795, grad_fn=<NegBackward0>) tensor(10767.4668, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10767.4208984375
tensor(10767.4668, grad_fn=<NegBackward0>) tensor(10767.4209, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10766.8359375
tensor(10767.4209, grad_fn=<NegBackward0>) tensor(10766.8359, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10766.4072265625
tensor(10766.8359, grad_fn=<NegBackward0>) tensor(10766.4072, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10766.3486328125
tensor(10766.4072, grad_fn=<NegBackward0>) tensor(10766.3486, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10766.3271484375
tensor(10766.3486, grad_fn=<NegBackward0>) tensor(10766.3271, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10766.3154296875
tensor(10766.3271, grad_fn=<NegBackward0>) tensor(10766.3154, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10766.3076171875
tensor(10766.3154, grad_fn=<NegBackward0>) tensor(10766.3076, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10766.302734375
tensor(10766.3076, grad_fn=<NegBackward0>) tensor(10766.3027, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10766.298828125
tensor(10766.3027, grad_fn=<NegBackward0>) tensor(10766.2988, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10766.296875
tensor(10766.2988, grad_fn=<NegBackward0>) tensor(10766.2969, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10766.2939453125
tensor(10766.2969, grad_fn=<NegBackward0>) tensor(10766.2939, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10766.2939453125
tensor(10766.2939, grad_fn=<NegBackward0>) tensor(10766.2939, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10766.2939453125
tensor(10766.2939, grad_fn=<NegBackward0>) tensor(10766.2939, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10766.2919921875
tensor(10766.2939, grad_fn=<NegBackward0>) tensor(10766.2920, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10766.291015625
tensor(10766.2920, grad_fn=<NegBackward0>) tensor(10766.2910, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10766.291015625
tensor(10766.2910, grad_fn=<NegBackward0>) tensor(10766.2910, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10766.291015625
tensor(10766.2910, grad_fn=<NegBackward0>) tensor(10766.2910, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10766.2900390625
tensor(10766.2910, grad_fn=<NegBackward0>) tensor(10766.2900, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10766.2900390625
tensor(10766.2900, grad_fn=<NegBackward0>) tensor(10766.2900, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10766.2890625
tensor(10766.2900, grad_fn=<NegBackward0>) tensor(10766.2891, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10766.2890625
tensor(10766.2891, grad_fn=<NegBackward0>) tensor(10766.2891, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10766.2890625
tensor(10766.2891, grad_fn=<NegBackward0>) tensor(10766.2891, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10766.2880859375
tensor(10766.2891, grad_fn=<NegBackward0>) tensor(10766.2881, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10766.287109375
tensor(10766.2881, grad_fn=<NegBackward0>) tensor(10766.2871, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10766.287109375
tensor(10766.2871, grad_fn=<NegBackward0>) tensor(10766.2871, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10766.2890625
tensor(10766.2871, grad_fn=<NegBackward0>) tensor(10766.2891, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10766.2880859375
tensor(10766.2871, grad_fn=<NegBackward0>) tensor(10766.2881, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -10766.2880859375
tensor(10766.2871, grad_fn=<NegBackward0>) tensor(10766.2881, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -10766.2880859375
tensor(10766.2871, grad_fn=<NegBackward0>) tensor(10766.2881, grad_fn=<NegBackward0>)
4
Iteration 6100: Loss = -10766.2890625
tensor(10766.2871, grad_fn=<NegBackward0>) tensor(10766.2891, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6100 due to no improvement.
pi: tensor([[0.9989, 0.0011],
        [0.0627, 0.9373]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.2449e-04, 9.9988e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2099, 0.1437],
         [0.5627, 0.1509]],

        [[0.6058, 0.1869],
         [0.6547, 0.5458]],

        [[0.5297, 0.1843],
         [0.6856, 0.6722]],

        [[0.7113, 0.1639],
         [0.5515, 0.6437]],

        [[0.5456, 0.1835],
         [0.6526, 0.7220]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.009009494263563287
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0014922741295917668
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.01851330918808678
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: 0.005065160483418353
Average Adjusted Rand Index: 0.0024911970478752343
[0.005065160483418353, 0.005065160483418353] [0.0024911970478752343, 0.0024911970478752343] [10766.2900390625, 10766.2890625]
-------------------------------------
This iteration is 17
True Objective function: Loss = -10788.84167248401
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22875.302734375
inf tensor(22875.3027, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10851.6171875
tensor(22875.3027, grad_fn=<NegBackward0>) tensor(10851.6172, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10850.708984375
tensor(10851.6172, grad_fn=<NegBackward0>) tensor(10850.7090, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10850.4619140625
tensor(10850.7090, grad_fn=<NegBackward0>) tensor(10850.4619, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10850.353515625
tensor(10850.4619, grad_fn=<NegBackward0>) tensor(10850.3535, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10850.283203125
tensor(10850.3535, grad_fn=<NegBackward0>) tensor(10850.2832, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10850.1484375
tensor(10850.2832, grad_fn=<NegBackward0>) tensor(10850.1484, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10846.7236328125
tensor(10850.1484, grad_fn=<NegBackward0>) tensor(10846.7236, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10846.310546875
tensor(10846.7236, grad_fn=<NegBackward0>) tensor(10846.3105, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10846.2255859375
tensor(10846.3105, grad_fn=<NegBackward0>) tensor(10846.2256, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10846.1728515625
tensor(10846.2256, grad_fn=<NegBackward0>) tensor(10846.1729, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10846.11328125
tensor(10846.1729, grad_fn=<NegBackward0>) tensor(10846.1133, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10846.0185546875
tensor(10846.1133, grad_fn=<NegBackward0>) tensor(10846.0186, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10845.7900390625
tensor(10846.0186, grad_fn=<NegBackward0>) tensor(10845.7900, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10844.951171875
tensor(10845.7900, grad_fn=<NegBackward0>) tensor(10844.9512, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10844.7333984375
tensor(10844.9512, grad_fn=<NegBackward0>) tensor(10844.7334, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10844.6689453125
tensor(10844.7334, grad_fn=<NegBackward0>) tensor(10844.6689, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10844.640625
tensor(10844.6689, grad_fn=<NegBackward0>) tensor(10844.6406, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10844.6240234375
tensor(10844.6406, grad_fn=<NegBackward0>) tensor(10844.6240, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10844.6162109375
tensor(10844.6240, grad_fn=<NegBackward0>) tensor(10844.6162, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10844.61328125
tensor(10844.6162, grad_fn=<NegBackward0>) tensor(10844.6133, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10844.6103515625
tensor(10844.6133, grad_fn=<NegBackward0>) tensor(10844.6104, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10844.6083984375
tensor(10844.6104, grad_fn=<NegBackward0>) tensor(10844.6084, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10844.609375
tensor(10844.6084, grad_fn=<NegBackward0>) tensor(10844.6094, grad_fn=<NegBackward0>)
1
Iteration 2400: Loss = -10844.6064453125
tensor(10844.6084, grad_fn=<NegBackward0>) tensor(10844.6064, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10844.607421875
tensor(10844.6064, grad_fn=<NegBackward0>) tensor(10844.6074, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -10844.607421875
tensor(10844.6064, grad_fn=<NegBackward0>) tensor(10844.6074, grad_fn=<NegBackward0>)
2
Iteration 2700: Loss = -10844.6064453125
tensor(10844.6064, grad_fn=<NegBackward0>) tensor(10844.6064, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10844.607421875
tensor(10844.6064, grad_fn=<NegBackward0>) tensor(10844.6074, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -10844.6064453125
tensor(10844.6064, grad_fn=<NegBackward0>) tensor(10844.6064, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10844.60546875
tensor(10844.6064, grad_fn=<NegBackward0>) tensor(10844.6055, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10844.603515625
tensor(10844.6055, grad_fn=<NegBackward0>) tensor(10844.6035, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10844.60546875
tensor(10844.6035, grad_fn=<NegBackward0>) tensor(10844.6055, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -10844.603515625
tensor(10844.6035, grad_fn=<NegBackward0>) tensor(10844.6035, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10844.6044921875
tensor(10844.6035, grad_fn=<NegBackward0>) tensor(10844.6045, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -10844.599609375
tensor(10844.6035, grad_fn=<NegBackward0>) tensor(10844.5996, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10844.59765625
tensor(10844.5996, grad_fn=<NegBackward0>) tensor(10844.5977, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10844.59375
tensor(10844.5977, grad_fn=<NegBackward0>) tensor(10844.5938, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10844.583984375
tensor(10844.5938, grad_fn=<NegBackward0>) tensor(10844.5840, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10844.5625
tensor(10844.5840, grad_fn=<NegBackward0>) tensor(10844.5625, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10844.466796875
tensor(10844.5625, grad_fn=<NegBackward0>) tensor(10844.4668, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10841.6171875
tensor(10844.4668, grad_fn=<NegBackward0>) tensor(10841.6172, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10840.123046875
tensor(10841.6172, grad_fn=<NegBackward0>) tensor(10840.1230, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10839.72265625
tensor(10840.1230, grad_fn=<NegBackward0>) tensor(10839.7227, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10839.484375
tensor(10839.7227, grad_fn=<NegBackward0>) tensor(10839.4844, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10839.2978515625
tensor(10839.4844, grad_fn=<NegBackward0>) tensor(10839.2979, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10839.1474609375
tensor(10839.2979, grad_fn=<NegBackward0>) tensor(10839.1475, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10839.0302734375
tensor(10839.1475, grad_fn=<NegBackward0>) tensor(10839.0303, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10838.939453125
tensor(10839.0303, grad_fn=<NegBackward0>) tensor(10838.9395, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10838.869140625
tensor(10838.9395, grad_fn=<NegBackward0>) tensor(10838.8691, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10838.814453125
tensor(10838.8691, grad_fn=<NegBackward0>) tensor(10838.8145, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10838.7724609375
tensor(10838.8145, grad_fn=<NegBackward0>) tensor(10838.7725, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10838.740234375
tensor(10838.7725, grad_fn=<NegBackward0>) tensor(10838.7402, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10838.712890625
tensor(10838.7402, grad_fn=<NegBackward0>) tensor(10838.7129, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10838.6904296875
tensor(10838.7129, grad_fn=<NegBackward0>) tensor(10838.6904, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10838.671875
tensor(10838.6904, grad_fn=<NegBackward0>) tensor(10838.6719, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10838.6552734375
tensor(10838.6719, grad_fn=<NegBackward0>) tensor(10838.6553, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10838.6416015625
tensor(10838.6553, grad_fn=<NegBackward0>) tensor(10838.6416, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10838.630859375
tensor(10838.6416, grad_fn=<NegBackward0>) tensor(10838.6309, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10838.6220703125
tensor(10838.6309, grad_fn=<NegBackward0>) tensor(10838.6221, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10838.611328125
tensor(10838.6221, grad_fn=<NegBackward0>) tensor(10838.6113, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10838.6025390625
tensor(10838.6113, grad_fn=<NegBackward0>) tensor(10838.6025, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10838.595703125
tensor(10838.6025, grad_fn=<NegBackward0>) tensor(10838.5957, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10838.58984375
tensor(10838.5957, grad_fn=<NegBackward0>) tensor(10838.5898, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10838.583984375
tensor(10838.5898, grad_fn=<NegBackward0>) tensor(10838.5840, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10838.580078125
tensor(10838.5840, grad_fn=<NegBackward0>) tensor(10838.5801, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10838.5732421875
tensor(10838.5801, grad_fn=<NegBackward0>) tensor(10838.5732, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10838.5703125
tensor(10838.5732, grad_fn=<NegBackward0>) tensor(10838.5703, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10838.5654296875
tensor(10838.5703, grad_fn=<NegBackward0>) tensor(10838.5654, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10838.5625
tensor(10838.5654, grad_fn=<NegBackward0>) tensor(10838.5625, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10838.5615234375
tensor(10838.5625, grad_fn=<NegBackward0>) tensor(10838.5615, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10838.5576171875
tensor(10838.5615, grad_fn=<NegBackward0>) tensor(10838.5576, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10838.5556640625
tensor(10838.5576, grad_fn=<NegBackward0>) tensor(10838.5557, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10838.552734375
tensor(10838.5557, grad_fn=<NegBackward0>) tensor(10838.5527, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10838.5498046875
tensor(10838.5527, grad_fn=<NegBackward0>) tensor(10838.5498, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10838.5478515625
tensor(10838.5498, grad_fn=<NegBackward0>) tensor(10838.5479, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10838.546875
tensor(10838.5479, grad_fn=<NegBackward0>) tensor(10838.5469, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10838.544921875
tensor(10838.5469, grad_fn=<NegBackward0>) tensor(10838.5449, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10838.5439453125
tensor(10838.5449, grad_fn=<NegBackward0>) tensor(10838.5439, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10838.541015625
tensor(10838.5439, grad_fn=<NegBackward0>) tensor(10838.5410, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10838.5400390625
tensor(10838.5410, grad_fn=<NegBackward0>) tensor(10838.5400, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10838.5390625
tensor(10838.5400, grad_fn=<NegBackward0>) tensor(10838.5391, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10838.54296875
tensor(10838.5391, grad_fn=<NegBackward0>) tensor(10838.5430, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10838.5361328125
tensor(10838.5391, grad_fn=<NegBackward0>) tensor(10838.5361, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10838.5791015625
tensor(10838.5361, grad_fn=<NegBackward0>) tensor(10838.5791, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -10838.5341796875
tensor(10838.5361, grad_fn=<NegBackward0>) tensor(10838.5342, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10838.703125
tensor(10838.5342, grad_fn=<NegBackward0>) tensor(10838.7031, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10838.5322265625
tensor(10838.5342, grad_fn=<NegBackward0>) tensor(10838.5322, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10838.5419921875
tensor(10838.5322, grad_fn=<NegBackward0>) tensor(10838.5420, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10838.53125
tensor(10838.5322, grad_fn=<NegBackward0>) tensor(10838.5312, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10838.53125
tensor(10838.5312, grad_fn=<NegBackward0>) tensor(10838.5312, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10838.53125
tensor(10838.5312, grad_fn=<NegBackward0>) tensor(10838.5312, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10838.529296875
tensor(10838.5312, grad_fn=<NegBackward0>) tensor(10838.5293, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10838.529296875
tensor(10838.5293, grad_fn=<NegBackward0>) tensor(10838.5293, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10838.5283203125
tensor(10838.5293, grad_fn=<NegBackward0>) tensor(10838.5283, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10838.529296875
tensor(10838.5283, grad_fn=<NegBackward0>) tensor(10838.5293, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -10838.5263671875
tensor(10838.5283, grad_fn=<NegBackward0>) tensor(10838.5264, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10838.52734375
tensor(10838.5264, grad_fn=<NegBackward0>) tensor(10838.5273, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -10838.5263671875
tensor(10838.5264, grad_fn=<NegBackward0>) tensor(10838.5264, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10838.5244140625
tensor(10838.5264, grad_fn=<NegBackward0>) tensor(10838.5244, grad_fn=<NegBackward0>)
pi: tensor([[9.9996e-01, 4.3940e-05],
        [1.2048e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0298, 0.9702], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[4.9647e-04, 2.8530e-01],
         [5.9326e-01, 1.5608e-01]],

        [[5.8677e-01, 1.7515e-01],
         [5.2537e-01, 5.8605e-01]],

        [[6.4449e-01, 1.7850e-01],
         [6.7961e-01, 5.3500e-01]],

        [[5.3373e-01, 1.8214e-01],
         [6.5202e-01, 6.0110e-01]],

        [[7.2791e-01, 2.3711e-01],
         [5.8892e-01, 6.9755e-01]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007766707522784365
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.002764517368444376
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
Global Adjusted Rand Index: -0.00014584426996972635
Average Adjusted Rand Index: -0.0015530782992861313
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20720.220703125
inf tensor(20720.2207, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10849.701171875
tensor(20720.2207, grad_fn=<NegBackward0>) tensor(10849.7012, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10846.3388671875
tensor(10849.7012, grad_fn=<NegBackward0>) tensor(10846.3389, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10845.912109375
tensor(10846.3389, grad_fn=<NegBackward0>) tensor(10845.9121, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10845.552734375
tensor(10845.9121, grad_fn=<NegBackward0>) tensor(10845.5527, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10845.2431640625
tensor(10845.5527, grad_fn=<NegBackward0>) tensor(10845.2432, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10845.0234375
tensor(10845.2432, grad_fn=<NegBackward0>) tensor(10845.0234, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10844.9033203125
tensor(10845.0234, grad_fn=<NegBackward0>) tensor(10844.9033, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10844.8369140625
tensor(10844.9033, grad_fn=<NegBackward0>) tensor(10844.8369, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10844.791015625
tensor(10844.8369, grad_fn=<NegBackward0>) tensor(10844.7910, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10844.75390625
tensor(10844.7910, grad_fn=<NegBackward0>) tensor(10844.7539, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10844.7216796875
tensor(10844.7539, grad_fn=<NegBackward0>) tensor(10844.7217, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10844.697265625
tensor(10844.7217, grad_fn=<NegBackward0>) tensor(10844.6973, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10844.67578125
tensor(10844.6973, grad_fn=<NegBackward0>) tensor(10844.6758, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10844.6572265625
tensor(10844.6758, grad_fn=<NegBackward0>) tensor(10844.6572, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10844.64453125
tensor(10844.6572, grad_fn=<NegBackward0>) tensor(10844.6445, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10844.634765625
tensor(10844.6445, grad_fn=<NegBackward0>) tensor(10844.6348, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10844.6240234375
tensor(10844.6348, grad_fn=<NegBackward0>) tensor(10844.6240, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10844.6201171875
tensor(10844.6240, grad_fn=<NegBackward0>) tensor(10844.6201, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10844.6162109375
tensor(10844.6201, grad_fn=<NegBackward0>) tensor(10844.6162, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10844.61328125
tensor(10844.6162, grad_fn=<NegBackward0>) tensor(10844.6133, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10844.6103515625
tensor(10844.6133, grad_fn=<NegBackward0>) tensor(10844.6104, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10844.609375
tensor(10844.6104, grad_fn=<NegBackward0>) tensor(10844.6094, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10844.609375
tensor(10844.6094, grad_fn=<NegBackward0>) tensor(10844.6094, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10844.6083984375
tensor(10844.6094, grad_fn=<NegBackward0>) tensor(10844.6084, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10844.6064453125
tensor(10844.6084, grad_fn=<NegBackward0>) tensor(10844.6064, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10844.6064453125
tensor(10844.6064, grad_fn=<NegBackward0>) tensor(10844.6064, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10844.6064453125
tensor(10844.6064, grad_fn=<NegBackward0>) tensor(10844.6064, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10844.60546875
tensor(10844.6064, grad_fn=<NegBackward0>) tensor(10844.6055, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10844.60546875
tensor(10844.6055, grad_fn=<NegBackward0>) tensor(10844.6055, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10844.6044921875
tensor(10844.6055, grad_fn=<NegBackward0>) tensor(10844.6045, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10844.6064453125
tensor(10844.6045, grad_fn=<NegBackward0>) tensor(10844.6064, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -10844.6044921875
tensor(10844.6045, grad_fn=<NegBackward0>) tensor(10844.6045, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10844.60546875
tensor(10844.6045, grad_fn=<NegBackward0>) tensor(10844.6055, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -10844.6025390625
tensor(10844.6045, grad_fn=<NegBackward0>) tensor(10844.6025, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10844.603515625
tensor(10844.6025, grad_fn=<NegBackward0>) tensor(10844.6035, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -10844.6015625
tensor(10844.6025, grad_fn=<NegBackward0>) tensor(10844.6016, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10844.6005859375
tensor(10844.6016, grad_fn=<NegBackward0>) tensor(10844.6006, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10844.5986328125
tensor(10844.6006, grad_fn=<NegBackward0>) tensor(10844.5986, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10844.595703125
tensor(10844.5986, grad_fn=<NegBackward0>) tensor(10844.5957, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10844.59375
tensor(10844.5957, grad_fn=<NegBackward0>) tensor(10844.5938, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10844.587890625
tensor(10844.5938, grad_fn=<NegBackward0>) tensor(10844.5879, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10844.5732421875
tensor(10844.5879, grad_fn=<NegBackward0>) tensor(10844.5732, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10844.5341796875
tensor(10844.5732, grad_fn=<NegBackward0>) tensor(10844.5342, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10844.2529296875
tensor(10844.5342, grad_fn=<NegBackward0>) tensor(10844.2529, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10841.17578125
tensor(10844.2529, grad_fn=<NegBackward0>) tensor(10841.1758, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10840.0390625
tensor(10841.1758, grad_fn=<NegBackward0>) tensor(10840.0391, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10839.5771484375
tensor(10840.0391, grad_fn=<NegBackward0>) tensor(10839.5771, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10839.2744140625
tensor(10839.5771, grad_fn=<NegBackward0>) tensor(10839.2744, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10839.0673828125
tensor(10839.2744, grad_fn=<NegBackward0>) tensor(10839.0674, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10838.931640625
tensor(10839.0674, grad_fn=<NegBackward0>) tensor(10838.9316, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10838.8427734375
tensor(10838.9316, grad_fn=<NegBackward0>) tensor(10838.8428, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10838.7802734375
tensor(10838.8428, grad_fn=<NegBackward0>) tensor(10838.7803, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10838.736328125
tensor(10838.7803, grad_fn=<NegBackward0>) tensor(10838.7363, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10838.701171875
tensor(10838.7363, grad_fn=<NegBackward0>) tensor(10838.7012, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10838.67578125
tensor(10838.7012, grad_fn=<NegBackward0>) tensor(10838.6758, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10838.6552734375
tensor(10838.6758, grad_fn=<NegBackward0>) tensor(10838.6553, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10838.6396484375
tensor(10838.6553, grad_fn=<NegBackward0>) tensor(10838.6396, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10838.625
tensor(10838.6396, grad_fn=<NegBackward0>) tensor(10838.6250, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10838.615234375
tensor(10838.6250, grad_fn=<NegBackward0>) tensor(10838.6152, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10838.6044921875
tensor(10838.6152, grad_fn=<NegBackward0>) tensor(10838.6045, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10838.5966796875
tensor(10838.6045, grad_fn=<NegBackward0>) tensor(10838.5967, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10838.587890625
tensor(10838.5967, grad_fn=<NegBackward0>) tensor(10838.5879, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10838.5810546875
tensor(10838.5879, grad_fn=<NegBackward0>) tensor(10838.5811, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10838.578125
tensor(10838.5811, grad_fn=<NegBackward0>) tensor(10838.5781, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10838.572265625
tensor(10838.5781, grad_fn=<NegBackward0>) tensor(10838.5723, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10838.568359375
tensor(10838.5723, grad_fn=<NegBackward0>) tensor(10838.5684, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10838.564453125
tensor(10838.5684, grad_fn=<NegBackward0>) tensor(10838.5645, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10838.560546875
tensor(10838.5645, grad_fn=<NegBackward0>) tensor(10838.5605, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10838.5576171875
tensor(10838.5605, grad_fn=<NegBackward0>) tensor(10838.5576, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10838.5546875
tensor(10838.5576, grad_fn=<NegBackward0>) tensor(10838.5547, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10838.5517578125
tensor(10838.5547, grad_fn=<NegBackward0>) tensor(10838.5518, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10838.5498046875
tensor(10838.5518, grad_fn=<NegBackward0>) tensor(10838.5498, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10838.5478515625
tensor(10838.5498, grad_fn=<NegBackward0>) tensor(10838.5479, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10838.546875
tensor(10838.5479, grad_fn=<NegBackward0>) tensor(10838.5469, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10838.5537109375
tensor(10838.5469, grad_fn=<NegBackward0>) tensor(10838.5537, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10838.54296875
tensor(10838.5469, grad_fn=<NegBackward0>) tensor(10838.5430, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10838.5439453125
tensor(10838.5430, grad_fn=<NegBackward0>) tensor(10838.5439, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10838.5390625
tensor(10838.5430, grad_fn=<NegBackward0>) tensor(10838.5391, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10838.5390625
tensor(10838.5391, grad_fn=<NegBackward0>) tensor(10838.5391, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10838.5361328125
tensor(10838.5391, grad_fn=<NegBackward0>) tensor(10838.5361, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10838.5361328125
tensor(10838.5361, grad_fn=<NegBackward0>) tensor(10838.5361, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10838.53515625
tensor(10838.5361, grad_fn=<NegBackward0>) tensor(10838.5352, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10838.5322265625
tensor(10838.5352, grad_fn=<NegBackward0>) tensor(10838.5322, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10838.5322265625
tensor(10838.5322, grad_fn=<NegBackward0>) tensor(10838.5322, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10838.5322265625
tensor(10838.5322, grad_fn=<NegBackward0>) tensor(10838.5322, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10838.53125
tensor(10838.5322, grad_fn=<NegBackward0>) tensor(10838.5312, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10838.533203125
tensor(10838.5312, grad_fn=<NegBackward0>) tensor(10838.5332, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -10838.529296875
tensor(10838.5312, grad_fn=<NegBackward0>) tensor(10838.5293, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10838.53515625
tensor(10838.5293, grad_fn=<NegBackward0>) tensor(10838.5352, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -10838.5283203125
tensor(10838.5293, grad_fn=<NegBackward0>) tensor(10838.5283, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10838.5283203125
tensor(10838.5283, grad_fn=<NegBackward0>) tensor(10838.5283, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10838.52734375
tensor(10838.5283, grad_fn=<NegBackward0>) tensor(10838.5273, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10838.6220703125
tensor(10838.5273, grad_fn=<NegBackward0>) tensor(10838.6221, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -10838.5263671875
tensor(10838.5273, grad_fn=<NegBackward0>) tensor(10838.5264, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10838.7314453125
tensor(10838.5264, grad_fn=<NegBackward0>) tensor(10838.7314, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -10838.5244140625
tensor(10838.5264, grad_fn=<NegBackward0>) tensor(10838.5244, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10838.53125
tensor(10838.5244, grad_fn=<NegBackward0>) tensor(10838.5312, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -10838.525390625
tensor(10838.5244, grad_fn=<NegBackward0>) tensor(10838.5254, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -10838.5341796875
tensor(10838.5244, grad_fn=<NegBackward0>) tensor(10838.5342, grad_fn=<NegBackward0>)
3
pi: tensor([[1.0000e+00, 1.4374e-06],
        [6.2219e-05, 9.9994e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9701, 0.0299], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.5609e-01, 2.8530e-01],
         [6.3599e-01, 3.4483e-04]],

        [[5.0374e-01, 1.7515e-01],
         [5.2205e-01, 6.5150e-01]],

        [[7.0898e-01, 1.7850e-01],
         [5.5011e-01, 5.7849e-01]],

        [[6.1446e-01, 1.8214e-01],
         [5.9257e-01, 7.2520e-01]],

        [[7.0792e-01, 2.3711e-01],
         [6.9690e-01, 6.4689e-01]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0007766707522784365
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.002764517368444376
Global Adjusted Rand Index: -0.00014584426996972635
Average Adjusted Rand Index: -0.0015530782992861313
[-0.00014584426996972635, -0.00014584426996972635] [-0.0015530782992861313, -0.0015530782992861313] [10838.5234375, 10838.5244140625]
-------------------------------------
This iteration is 18
True Objective function: Loss = -10970.931249424195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21455.455078125
inf tensor(21455.4551, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10971.8955078125
tensor(21455.4551, grad_fn=<NegBackward0>) tensor(10971.8955, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10971.119140625
tensor(10971.8955, grad_fn=<NegBackward0>) tensor(10971.1191, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10970.8916015625
tensor(10971.1191, grad_fn=<NegBackward0>) tensor(10970.8916, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10970.5322265625
tensor(10970.8916, grad_fn=<NegBackward0>) tensor(10970.5322, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10969.50390625
tensor(10970.5322, grad_fn=<NegBackward0>) tensor(10969.5039, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10968.591796875
tensor(10969.5039, grad_fn=<NegBackward0>) tensor(10968.5918, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10967.9619140625
tensor(10968.5918, grad_fn=<NegBackward0>) tensor(10967.9619, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10967.5908203125
tensor(10967.9619, grad_fn=<NegBackward0>) tensor(10967.5908, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10967.404296875
tensor(10967.5908, grad_fn=<NegBackward0>) tensor(10967.4043, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10967.30859375
tensor(10967.4043, grad_fn=<NegBackward0>) tensor(10967.3086, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10967.259765625
tensor(10967.3086, grad_fn=<NegBackward0>) tensor(10967.2598, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10967.2265625
tensor(10967.2598, grad_fn=<NegBackward0>) tensor(10967.2266, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10967.205078125
tensor(10967.2266, grad_fn=<NegBackward0>) tensor(10967.2051, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10967.1904296875
tensor(10967.2051, grad_fn=<NegBackward0>) tensor(10967.1904, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10967.1787109375
tensor(10967.1904, grad_fn=<NegBackward0>) tensor(10967.1787, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10967.169921875
tensor(10967.1787, grad_fn=<NegBackward0>) tensor(10967.1699, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10967.162109375
tensor(10967.1699, grad_fn=<NegBackward0>) tensor(10967.1621, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10967.1533203125
tensor(10967.1621, grad_fn=<NegBackward0>) tensor(10967.1533, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10967.1494140625
tensor(10967.1533, grad_fn=<NegBackward0>) tensor(10967.1494, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10967.14453125
tensor(10967.1494, grad_fn=<NegBackward0>) tensor(10967.1445, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10967.140625
tensor(10967.1445, grad_fn=<NegBackward0>) tensor(10967.1406, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10967.1376953125
tensor(10967.1406, grad_fn=<NegBackward0>) tensor(10967.1377, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10967.1357421875
tensor(10967.1377, grad_fn=<NegBackward0>) tensor(10967.1357, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10967.1328125
tensor(10967.1357, grad_fn=<NegBackward0>) tensor(10967.1328, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10967.130859375
tensor(10967.1328, grad_fn=<NegBackward0>) tensor(10967.1309, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10967.1298828125
tensor(10967.1309, grad_fn=<NegBackward0>) tensor(10967.1299, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10967.126953125
tensor(10967.1299, grad_fn=<NegBackward0>) tensor(10967.1270, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10967.125
tensor(10967.1270, grad_fn=<NegBackward0>) tensor(10967.1250, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10967.1240234375
tensor(10967.1250, grad_fn=<NegBackward0>) tensor(10967.1240, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10967.1240234375
tensor(10967.1240, grad_fn=<NegBackward0>) tensor(10967.1240, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10967.123046875
tensor(10967.1240, grad_fn=<NegBackward0>) tensor(10967.1230, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10967.12109375
tensor(10967.1230, grad_fn=<NegBackward0>) tensor(10967.1211, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10967.12109375
tensor(10967.1211, grad_fn=<NegBackward0>) tensor(10967.1211, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10967.119140625
tensor(10967.1211, grad_fn=<NegBackward0>) tensor(10967.1191, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10967.1201171875
tensor(10967.1191, grad_fn=<NegBackward0>) tensor(10967.1201, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -10967.1181640625
tensor(10967.1191, grad_fn=<NegBackward0>) tensor(10967.1182, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10967.1181640625
tensor(10967.1182, grad_fn=<NegBackward0>) tensor(10967.1182, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10967.119140625
tensor(10967.1182, grad_fn=<NegBackward0>) tensor(10967.1191, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -10967.1162109375
tensor(10967.1182, grad_fn=<NegBackward0>) tensor(10967.1162, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10967.1162109375
tensor(10967.1162, grad_fn=<NegBackward0>) tensor(10967.1162, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10967.115234375
tensor(10967.1162, grad_fn=<NegBackward0>) tensor(10967.1152, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10967.115234375
tensor(10967.1152, grad_fn=<NegBackward0>) tensor(10967.1152, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10967.115234375
tensor(10967.1152, grad_fn=<NegBackward0>) tensor(10967.1152, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10967.115234375
tensor(10967.1152, grad_fn=<NegBackward0>) tensor(10967.1152, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10967.11328125
tensor(10967.1152, grad_fn=<NegBackward0>) tensor(10967.1133, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10967.11328125
tensor(10967.1133, grad_fn=<NegBackward0>) tensor(10967.1133, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10967.11328125
tensor(10967.1133, grad_fn=<NegBackward0>) tensor(10967.1133, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10967.11328125
tensor(10967.1133, grad_fn=<NegBackward0>) tensor(10967.1133, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10967.11328125
tensor(10967.1133, grad_fn=<NegBackward0>) tensor(10967.1133, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10967.1123046875
tensor(10967.1133, grad_fn=<NegBackward0>) tensor(10967.1123, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10967.1142578125
tensor(10967.1123, grad_fn=<NegBackward0>) tensor(10967.1143, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10967.11328125
tensor(10967.1123, grad_fn=<NegBackward0>) tensor(10967.1133, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -10967.111328125
tensor(10967.1123, grad_fn=<NegBackward0>) tensor(10967.1113, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10967.11328125
tensor(10967.1113, grad_fn=<NegBackward0>) tensor(10967.1133, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10967.111328125
tensor(10967.1113, grad_fn=<NegBackward0>) tensor(10967.1113, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10967.1123046875
tensor(10967.1113, grad_fn=<NegBackward0>) tensor(10967.1123, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10967.111328125
tensor(10967.1113, grad_fn=<NegBackward0>) tensor(10967.1113, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10967.111328125
tensor(10967.1113, grad_fn=<NegBackward0>) tensor(10967.1113, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10967.1083984375
tensor(10967.1113, grad_fn=<NegBackward0>) tensor(10967.1084, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10967.10546875
tensor(10967.1084, grad_fn=<NegBackward0>) tensor(10967.1055, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10967.09765625
tensor(10967.1055, grad_fn=<NegBackward0>) tensor(10967.0977, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10967.087890625
tensor(10967.0977, grad_fn=<NegBackward0>) tensor(10967.0879, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10967.0859375
tensor(10967.0879, grad_fn=<NegBackward0>) tensor(10967.0859, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10967.0849609375
tensor(10967.0859, grad_fn=<NegBackward0>) tensor(10967.0850, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10967.0859375
tensor(10967.0850, grad_fn=<NegBackward0>) tensor(10967.0859, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10967.08203125
tensor(10967.0850, grad_fn=<NegBackward0>) tensor(10967.0820, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10967.0849609375
tensor(10967.0820, grad_fn=<NegBackward0>) tensor(10967.0850, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10967.083984375
tensor(10967.0820, grad_fn=<NegBackward0>) tensor(10967.0840, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -10967.083984375
tensor(10967.0820, grad_fn=<NegBackward0>) tensor(10967.0840, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -10967.0830078125
tensor(10967.0820, grad_fn=<NegBackward0>) tensor(10967.0830, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -10967.0849609375
tensor(10967.0820, grad_fn=<NegBackward0>) tensor(10967.0850, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[2.2170e-04, 9.9978e-01],
        [1.2201e-02, 9.8780e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0104, 0.9896], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2040, 0.1428],
         [0.5411, 0.1625]],

        [[0.5689, 0.2313],
         [0.5186, 0.5940]],

        [[0.7023, 0.1982],
         [0.6571, 0.7242]],

        [[0.7147, 0.0305],
         [0.7274, 0.6805]],

        [[0.6697, 0.1084],
         [0.6757, 0.6634]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0006199009473308488
Average Adjusted Rand Index: -0.00015692302765368048
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20836.24609375
inf tensor(20836.2461, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10971.5439453125
tensor(20836.2461, grad_fn=<NegBackward0>) tensor(10971.5439, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10971.1005859375
tensor(10971.5439, grad_fn=<NegBackward0>) tensor(10971.1006, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10970.958984375
tensor(10971.1006, grad_fn=<NegBackward0>) tensor(10970.9590, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10970.8046875
tensor(10970.9590, grad_fn=<NegBackward0>) tensor(10970.8047, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10970.5556640625
tensor(10970.8047, grad_fn=<NegBackward0>) tensor(10970.5557, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10970.228515625
tensor(10970.5557, grad_fn=<NegBackward0>) tensor(10970.2285, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10970.0712890625
tensor(10970.2285, grad_fn=<NegBackward0>) tensor(10970.0713, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10970.021484375
tensor(10970.0713, grad_fn=<NegBackward0>) tensor(10970.0215, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10969.9951171875
tensor(10970.0215, grad_fn=<NegBackward0>) tensor(10969.9951, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10969.9775390625
tensor(10969.9951, grad_fn=<NegBackward0>) tensor(10969.9775, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10969.9658203125
tensor(10969.9775, grad_fn=<NegBackward0>) tensor(10969.9658, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10969.95703125
tensor(10969.9658, grad_fn=<NegBackward0>) tensor(10969.9570, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10969.9482421875
tensor(10969.9570, grad_fn=<NegBackward0>) tensor(10969.9482, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10969.94140625
tensor(10969.9482, grad_fn=<NegBackward0>) tensor(10969.9414, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10969.9365234375
tensor(10969.9414, grad_fn=<NegBackward0>) tensor(10969.9365, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10969.9326171875
tensor(10969.9365, grad_fn=<NegBackward0>) tensor(10969.9326, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10969.927734375
tensor(10969.9326, grad_fn=<NegBackward0>) tensor(10969.9277, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10969.923828125
tensor(10969.9277, grad_fn=<NegBackward0>) tensor(10969.9238, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10969.919921875
tensor(10969.9238, grad_fn=<NegBackward0>) tensor(10969.9199, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10969.916015625
tensor(10969.9199, grad_fn=<NegBackward0>) tensor(10969.9160, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10969.9140625
tensor(10969.9160, grad_fn=<NegBackward0>) tensor(10969.9141, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10969.9111328125
tensor(10969.9141, grad_fn=<NegBackward0>) tensor(10969.9111, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10969.91015625
tensor(10969.9111, grad_fn=<NegBackward0>) tensor(10969.9102, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10969.9072265625
tensor(10969.9102, grad_fn=<NegBackward0>) tensor(10969.9072, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10969.9072265625
tensor(10969.9072, grad_fn=<NegBackward0>) tensor(10969.9072, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10969.9033203125
tensor(10969.9072, grad_fn=<NegBackward0>) tensor(10969.9033, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10969.90234375
tensor(10969.9033, grad_fn=<NegBackward0>) tensor(10969.9023, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10969.900390625
tensor(10969.9023, grad_fn=<NegBackward0>) tensor(10969.9004, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10969.900390625
tensor(10969.9004, grad_fn=<NegBackward0>) tensor(10969.9004, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10969.8984375
tensor(10969.9004, grad_fn=<NegBackward0>) tensor(10969.8984, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10969.8974609375
tensor(10969.8984, grad_fn=<NegBackward0>) tensor(10969.8975, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10969.896484375
tensor(10969.8975, grad_fn=<NegBackward0>) tensor(10969.8965, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10969.8955078125
tensor(10969.8965, grad_fn=<NegBackward0>) tensor(10969.8955, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10969.8955078125
tensor(10969.8955, grad_fn=<NegBackward0>) tensor(10969.8955, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10969.89453125
tensor(10969.8955, grad_fn=<NegBackward0>) tensor(10969.8945, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10969.8935546875
tensor(10969.8945, grad_fn=<NegBackward0>) tensor(10969.8936, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10969.892578125
tensor(10969.8936, grad_fn=<NegBackward0>) tensor(10969.8926, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10969.8916015625
tensor(10969.8926, grad_fn=<NegBackward0>) tensor(10969.8916, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10969.890625
tensor(10969.8916, grad_fn=<NegBackward0>) tensor(10969.8906, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10969.890625
tensor(10969.8906, grad_fn=<NegBackward0>) tensor(10969.8906, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10969.8896484375
tensor(10969.8906, grad_fn=<NegBackward0>) tensor(10969.8896, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10969.8896484375
tensor(10969.8896, grad_fn=<NegBackward0>) tensor(10969.8896, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10969.888671875
tensor(10969.8896, grad_fn=<NegBackward0>) tensor(10969.8887, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10969.888671875
tensor(10969.8887, grad_fn=<NegBackward0>) tensor(10969.8887, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10969.8896484375
tensor(10969.8887, grad_fn=<NegBackward0>) tensor(10969.8896, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10969.88671875
tensor(10969.8887, grad_fn=<NegBackward0>) tensor(10969.8867, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10969.8876953125
tensor(10969.8867, grad_fn=<NegBackward0>) tensor(10969.8877, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10969.88671875
tensor(10969.8867, grad_fn=<NegBackward0>) tensor(10969.8867, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10969.8876953125
tensor(10969.8867, grad_fn=<NegBackward0>) tensor(10969.8877, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10969.8857421875
tensor(10969.8867, grad_fn=<NegBackward0>) tensor(10969.8857, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10969.8857421875
tensor(10969.8857, grad_fn=<NegBackward0>) tensor(10969.8857, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10969.88671875
tensor(10969.8857, grad_fn=<NegBackward0>) tensor(10969.8867, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10969.8837890625
tensor(10969.8857, grad_fn=<NegBackward0>) tensor(10969.8838, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10969.8857421875
tensor(10969.8838, grad_fn=<NegBackward0>) tensor(10969.8857, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10969.8857421875
tensor(10969.8838, grad_fn=<NegBackward0>) tensor(10969.8857, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -10969.8857421875
tensor(10969.8838, grad_fn=<NegBackward0>) tensor(10969.8857, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -10969.884765625
tensor(10969.8838, grad_fn=<NegBackward0>) tensor(10969.8848, grad_fn=<NegBackward0>)
4
Iteration 5800: Loss = -10969.8857421875
tensor(10969.8838, grad_fn=<NegBackward0>) tensor(10969.8857, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5800 due to no improvement.
pi: tensor([[8.1512e-04, 9.9918e-01],
        [4.4896e-02, 9.5510e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0013, 0.9987], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2519, 0.1681],
         [0.5187, 0.1589]],

        [[0.5667, 0.2170],
         [0.5342, 0.5622]],

        [[0.5414, 0.2000],
         [0.6462, 0.7125]],

        [[0.6370, 0.2295],
         [0.7089, 0.5077]],

        [[0.5648, 0.1813],
         [0.6093, 0.5389]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.000814506368914951
Average Adjusted Rand Index: -0.00023875441244733224
[-0.0006199009473308488, 0.000814506368914951] [-0.00015692302765368048, -0.00023875441244733224] [10967.0849609375, 10969.8857421875]
-------------------------------------
This iteration is 19
True Objective function: Loss = -10873.278967245691
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21981.2734375
inf tensor(21981.2734, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10987.587890625
tensor(21981.2734, grad_fn=<NegBackward0>) tensor(10987.5879, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10986.6953125
tensor(10987.5879, grad_fn=<NegBackward0>) tensor(10986.6953, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10985.5224609375
tensor(10986.6953, grad_fn=<NegBackward0>) tensor(10985.5225, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10984.7080078125
tensor(10985.5225, grad_fn=<NegBackward0>) tensor(10984.7080, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10984.4560546875
tensor(10984.7080, grad_fn=<NegBackward0>) tensor(10984.4561, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10984.3525390625
tensor(10984.4561, grad_fn=<NegBackward0>) tensor(10984.3525, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10984.3017578125
tensor(10984.3525, grad_fn=<NegBackward0>) tensor(10984.3018, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10984.2734375
tensor(10984.3018, grad_fn=<NegBackward0>) tensor(10984.2734, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10984.2568359375
tensor(10984.2734, grad_fn=<NegBackward0>) tensor(10984.2568, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10984.244140625
tensor(10984.2568, grad_fn=<NegBackward0>) tensor(10984.2441, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10984.234375
tensor(10984.2441, grad_fn=<NegBackward0>) tensor(10984.2344, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10984.228515625
tensor(10984.2344, grad_fn=<NegBackward0>) tensor(10984.2285, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10984.22265625
tensor(10984.2285, grad_fn=<NegBackward0>) tensor(10984.2227, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10984.21875
tensor(10984.2227, grad_fn=<NegBackward0>) tensor(10984.2188, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10984.21484375
tensor(10984.2188, grad_fn=<NegBackward0>) tensor(10984.2148, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10984.212890625
tensor(10984.2148, grad_fn=<NegBackward0>) tensor(10984.2129, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10984.2109375
tensor(10984.2129, grad_fn=<NegBackward0>) tensor(10984.2109, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10984.2109375
tensor(10984.2109, grad_fn=<NegBackward0>) tensor(10984.2109, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10984.208984375
tensor(10984.2109, grad_fn=<NegBackward0>) tensor(10984.2090, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10984.20703125
tensor(10984.2090, grad_fn=<NegBackward0>) tensor(10984.2070, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10984.2080078125
tensor(10984.2070, grad_fn=<NegBackward0>) tensor(10984.2080, grad_fn=<NegBackward0>)
1
Iteration 2200: Loss = -10984.20703125
tensor(10984.2070, grad_fn=<NegBackward0>) tensor(10984.2070, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10984.2060546875
tensor(10984.2070, grad_fn=<NegBackward0>) tensor(10984.2061, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10984.205078125
tensor(10984.2061, grad_fn=<NegBackward0>) tensor(10984.2051, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10984.205078125
tensor(10984.2051, grad_fn=<NegBackward0>) tensor(10984.2051, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10984.2060546875
tensor(10984.2051, grad_fn=<NegBackward0>) tensor(10984.2061, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -10984.2060546875
tensor(10984.2051, grad_fn=<NegBackward0>) tensor(10984.2061, grad_fn=<NegBackward0>)
2
Iteration 2800: Loss = -10984.2041015625
tensor(10984.2051, grad_fn=<NegBackward0>) tensor(10984.2041, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10984.203125
tensor(10984.2041, grad_fn=<NegBackward0>) tensor(10984.2031, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10984.2041015625
tensor(10984.2031, grad_fn=<NegBackward0>) tensor(10984.2041, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -10984.201171875
tensor(10984.2031, grad_fn=<NegBackward0>) tensor(10984.2012, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10984.201171875
tensor(10984.2012, grad_fn=<NegBackward0>) tensor(10984.2012, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10984.2001953125
tensor(10984.2012, grad_fn=<NegBackward0>) tensor(10984.2002, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10984.2001953125
tensor(10984.2002, grad_fn=<NegBackward0>) tensor(10984.2002, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10984.197265625
tensor(10984.2002, grad_fn=<NegBackward0>) tensor(10984.1973, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10984.1923828125
tensor(10984.1973, grad_fn=<NegBackward0>) tensor(10984.1924, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10984.1796875
tensor(10984.1924, grad_fn=<NegBackward0>) tensor(10984.1797, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10984.083984375
tensor(10984.1797, grad_fn=<NegBackward0>) tensor(10984.0840, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10981.0283203125
tensor(10984.0840, grad_fn=<NegBackward0>) tensor(10981.0283, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10979.876953125
tensor(10981.0283, grad_fn=<NegBackward0>) tensor(10979.8770, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10979.6533203125
tensor(10979.8770, grad_fn=<NegBackward0>) tensor(10979.6533, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10979.6005859375
tensor(10979.6533, grad_fn=<NegBackward0>) tensor(10979.6006, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10979.5791015625
tensor(10979.6006, grad_fn=<NegBackward0>) tensor(10979.5791, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10979.5654296875
tensor(10979.5791, grad_fn=<NegBackward0>) tensor(10979.5654, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10979.55859375
tensor(10979.5654, grad_fn=<NegBackward0>) tensor(10979.5586, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10979.5537109375
tensor(10979.5586, grad_fn=<NegBackward0>) tensor(10979.5537, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10979.546875
tensor(10979.5537, grad_fn=<NegBackward0>) tensor(10979.5469, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10979.544921875
tensor(10979.5469, grad_fn=<NegBackward0>) tensor(10979.5449, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10979.54296875
tensor(10979.5449, grad_fn=<NegBackward0>) tensor(10979.5430, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10979.5400390625
tensor(10979.5430, grad_fn=<NegBackward0>) tensor(10979.5400, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10979.5390625
tensor(10979.5400, grad_fn=<NegBackward0>) tensor(10979.5391, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10979.5380859375
tensor(10979.5391, grad_fn=<NegBackward0>) tensor(10979.5381, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10979.5361328125
tensor(10979.5381, grad_fn=<NegBackward0>) tensor(10979.5361, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10979.5361328125
tensor(10979.5361, grad_fn=<NegBackward0>) tensor(10979.5361, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10979.53515625
tensor(10979.5361, grad_fn=<NegBackward0>) tensor(10979.5352, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10979.533203125
tensor(10979.5352, grad_fn=<NegBackward0>) tensor(10979.5332, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10979.533203125
tensor(10979.5332, grad_fn=<NegBackward0>) tensor(10979.5332, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10979.53125
tensor(10979.5332, grad_fn=<NegBackward0>) tensor(10979.5312, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10979.533203125
tensor(10979.5312, grad_fn=<NegBackward0>) tensor(10979.5332, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10979.53125
tensor(10979.5312, grad_fn=<NegBackward0>) tensor(10979.5312, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10979.53125
tensor(10979.5312, grad_fn=<NegBackward0>) tensor(10979.5312, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10979.53125
tensor(10979.5312, grad_fn=<NegBackward0>) tensor(10979.5312, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10979.529296875
tensor(10979.5312, grad_fn=<NegBackward0>) tensor(10979.5293, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10979.529296875
tensor(10979.5293, grad_fn=<NegBackward0>) tensor(10979.5293, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10979.529296875
tensor(10979.5293, grad_fn=<NegBackward0>) tensor(10979.5293, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10979.5283203125
tensor(10979.5293, grad_fn=<NegBackward0>) tensor(10979.5283, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10979.5283203125
tensor(10979.5283, grad_fn=<NegBackward0>) tensor(10979.5283, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10979.5283203125
tensor(10979.5283, grad_fn=<NegBackward0>) tensor(10979.5283, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10979.5283203125
tensor(10979.5283, grad_fn=<NegBackward0>) tensor(10979.5283, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10979.52734375
tensor(10979.5283, grad_fn=<NegBackward0>) tensor(10979.5273, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10979.5283203125
tensor(10979.5273, grad_fn=<NegBackward0>) tensor(10979.5283, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10979.5283203125
tensor(10979.5273, grad_fn=<NegBackward0>) tensor(10979.5283, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -10979.5263671875
tensor(10979.5273, grad_fn=<NegBackward0>) tensor(10979.5264, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10979.52734375
tensor(10979.5264, grad_fn=<NegBackward0>) tensor(10979.5273, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10979.52734375
tensor(10979.5264, grad_fn=<NegBackward0>) tensor(10979.5273, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -10979.525390625
tensor(10979.5264, grad_fn=<NegBackward0>) tensor(10979.5254, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10979.5263671875
tensor(10979.5254, grad_fn=<NegBackward0>) tensor(10979.5264, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10979.52734375
tensor(10979.5254, grad_fn=<NegBackward0>) tensor(10979.5273, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -10979.52734375
tensor(10979.5254, grad_fn=<NegBackward0>) tensor(10979.5273, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -10979.5263671875
tensor(10979.5254, grad_fn=<NegBackward0>) tensor(10979.5264, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -10979.52734375
tensor(10979.5254, grad_fn=<NegBackward0>) tensor(10979.5273, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8100 due to no improvement.
pi: tensor([[1.0000e+00, 1.2309e-06],
        [1.5911e-04, 9.9984e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9900, 0.0100], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1639, 0.0910],
         [0.6831, 0.1014]],

        [[0.7264, 0.0507],
         [0.5908, 0.5635]],

        [[0.7199, 0.1614],
         [0.5670, 0.6504]],

        [[0.6990, 0.1314],
         [0.6765, 0.5802]],

        [[0.6457, 0.0608],
         [0.7107, 0.5068]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: -0.005513710431743218
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
Global Adjusted Rand Index: 0.00015159247594385454
Average Adjusted Rand Index: 0.0003033213513743884
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20050.1796875
inf tensor(20050.1797, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10987.4091796875
tensor(20050.1797, grad_fn=<NegBackward0>) tensor(10987.4092, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10987.16015625
tensor(10987.4092, grad_fn=<NegBackward0>) tensor(10987.1602, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10986.8623046875
tensor(10987.1602, grad_fn=<NegBackward0>) tensor(10986.8623, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10985.4853515625
tensor(10986.8623, grad_fn=<NegBackward0>) tensor(10985.4854, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10984.6845703125
tensor(10985.4854, grad_fn=<NegBackward0>) tensor(10984.6846, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10984.4873046875
tensor(10984.6846, grad_fn=<NegBackward0>) tensor(10984.4873, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10984.400390625
tensor(10984.4873, grad_fn=<NegBackward0>) tensor(10984.4004, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10984.3212890625
tensor(10984.4004, grad_fn=<NegBackward0>) tensor(10984.3213, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10984.2431640625
tensor(10984.3213, grad_fn=<NegBackward0>) tensor(10984.2432, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10984.2080078125
tensor(10984.2432, grad_fn=<NegBackward0>) tensor(10984.2080, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10984.1865234375
tensor(10984.2080, grad_fn=<NegBackward0>) tensor(10984.1865, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10984.162109375
tensor(10984.1865, grad_fn=<NegBackward0>) tensor(10984.1621, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10984.123046875
tensor(10984.1621, grad_fn=<NegBackward0>) tensor(10984.1230, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10984.09375
tensor(10984.1230, grad_fn=<NegBackward0>) tensor(10984.0938, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10984.0712890625
tensor(10984.0938, grad_fn=<NegBackward0>) tensor(10984.0713, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10984.0
tensor(10984.0713, grad_fn=<NegBackward0>) tensor(10984., grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10983.4775390625
tensor(10984., grad_fn=<NegBackward0>) tensor(10983.4775, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10982.9453125
tensor(10983.4775, grad_fn=<NegBackward0>) tensor(10982.9453, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10982.4599609375
tensor(10982.9453, grad_fn=<NegBackward0>) tensor(10982.4600, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10975.0126953125
tensor(10982.4600, grad_fn=<NegBackward0>) tensor(10975.0127, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10839.853515625
tensor(10975.0127, grad_fn=<NegBackward0>) tensor(10839.8535, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10835.5546875
tensor(10839.8535, grad_fn=<NegBackward0>) tensor(10835.5547, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10835.484375
tensor(10835.5547, grad_fn=<NegBackward0>) tensor(10835.4844, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10835.44140625
tensor(10835.4844, grad_fn=<NegBackward0>) tensor(10835.4414, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10835.4326171875
tensor(10835.4414, grad_fn=<NegBackward0>) tensor(10835.4326, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10835.4306640625
tensor(10835.4326, grad_fn=<NegBackward0>) tensor(10835.4307, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10835.3681640625
tensor(10835.4307, grad_fn=<NegBackward0>) tensor(10835.3682, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10835.3642578125
tensor(10835.3682, grad_fn=<NegBackward0>) tensor(10835.3643, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10835.0107421875
tensor(10835.3643, grad_fn=<NegBackward0>) tensor(10835.0107, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10834.9990234375
tensor(10835.0107, grad_fn=<NegBackward0>) tensor(10834.9990, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10834.994140625
tensor(10834.9990, grad_fn=<NegBackward0>) tensor(10834.9941, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10834.9736328125
tensor(10834.9941, grad_fn=<NegBackward0>) tensor(10834.9736, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10834.9892578125
tensor(10834.9736, grad_fn=<NegBackward0>) tensor(10834.9893, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -10834.9677734375
tensor(10834.9736, grad_fn=<NegBackward0>) tensor(10834.9678, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10834.90234375
tensor(10834.9678, grad_fn=<NegBackward0>) tensor(10834.9023, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10834.8505859375
tensor(10834.9023, grad_fn=<NegBackward0>) tensor(10834.8506, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10834.841796875
tensor(10834.8506, grad_fn=<NegBackward0>) tensor(10834.8418, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10834.1396484375
tensor(10834.8418, grad_fn=<NegBackward0>) tensor(10834.1396, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10834.1181640625
tensor(10834.1396, grad_fn=<NegBackward0>) tensor(10834.1182, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10834.119140625
tensor(10834.1182, grad_fn=<NegBackward0>) tensor(10834.1191, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -10834.1201171875
tensor(10834.1182, grad_fn=<NegBackward0>) tensor(10834.1201, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -10834.1142578125
tensor(10834.1182, grad_fn=<NegBackward0>) tensor(10834.1143, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10834.0146484375
tensor(10834.1143, grad_fn=<NegBackward0>) tensor(10834.0146, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10834.01171875
tensor(10834.0146, grad_fn=<NegBackward0>) tensor(10834.0117, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10834.0107421875
tensor(10834.0117, grad_fn=<NegBackward0>) tensor(10834.0107, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10833.9755859375
tensor(10834.0107, grad_fn=<NegBackward0>) tensor(10833.9756, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10833.9736328125
tensor(10833.9756, grad_fn=<NegBackward0>) tensor(10833.9736, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10833.9736328125
tensor(10833.9736, grad_fn=<NegBackward0>) tensor(10833.9736, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10833.9736328125
tensor(10833.9736, grad_fn=<NegBackward0>) tensor(10833.9736, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10833.96484375
tensor(10833.9736, grad_fn=<NegBackward0>) tensor(10833.9648, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10833.9638671875
tensor(10833.9648, grad_fn=<NegBackward0>) tensor(10833.9639, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10833.962890625
tensor(10833.9639, grad_fn=<NegBackward0>) tensor(10833.9629, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10833.958984375
tensor(10833.9629, grad_fn=<NegBackward0>) tensor(10833.9590, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10833.6796875
tensor(10833.9590, grad_fn=<NegBackward0>) tensor(10833.6797, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10833.681640625
tensor(10833.6797, grad_fn=<NegBackward0>) tensor(10833.6816, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10833.677734375
tensor(10833.6797, grad_fn=<NegBackward0>) tensor(10833.6777, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10833.6767578125
tensor(10833.6777, grad_fn=<NegBackward0>) tensor(10833.6768, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10833.6748046875
tensor(10833.6768, grad_fn=<NegBackward0>) tensor(10833.6748, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10833.67578125
tensor(10833.6748, grad_fn=<NegBackward0>) tensor(10833.6758, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10833.6728515625
tensor(10833.6748, grad_fn=<NegBackward0>) tensor(10833.6729, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10833.671875
tensor(10833.6729, grad_fn=<NegBackward0>) tensor(10833.6719, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10833.6630859375
tensor(10833.6719, grad_fn=<NegBackward0>) tensor(10833.6631, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10833.6630859375
tensor(10833.6631, grad_fn=<NegBackward0>) tensor(10833.6631, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10833.6640625
tensor(10833.6631, grad_fn=<NegBackward0>) tensor(10833.6641, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10833.6630859375
tensor(10833.6631, grad_fn=<NegBackward0>) tensor(10833.6631, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10833.662109375
tensor(10833.6631, grad_fn=<NegBackward0>) tensor(10833.6621, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10833.6630859375
tensor(10833.6621, grad_fn=<NegBackward0>) tensor(10833.6631, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10833.6669921875
tensor(10833.6621, grad_fn=<NegBackward0>) tensor(10833.6670, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -10833.6708984375
tensor(10833.6621, grad_fn=<NegBackward0>) tensor(10833.6709, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -10833.662109375
tensor(10833.6621, grad_fn=<NegBackward0>) tensor(10833.6621, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10833.6650390625
tensor(10833.6621, grad_fn=<NegBackward0>) tensor(10833.6650, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10833.6669921875
tensor(10833.6621, grad_fn=<NegBackward0>) tensor(10833.6670, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -10833.669921875
tensor(10833.6621, grad_fn=<NegBackward0>) tensor(10833.6699, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -10833.6572265625
tensor(10833.6621, grad_fn=<NegBackward0>) tensor(10833.6572, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10833.658203125
tensor(10833.6572, grad_fn=<NegBackward0>) tensor(10833.6582, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10833.658203125
tensor(10833.6572, grad_fn=<NegBackward0>) tensor(10833.6582, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -10833.65625
tensor(10833.6572, grad_fn=<NegBackward0>) tensor(10833.6562, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10833.6552734375
tensor(10833.6562, grad_fn=<NegBackward0>) tensor(10833.6553, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10833.6552734375
tensor(10833.6553, grad_fn=<NegBackward0>) tensor(10833.6553, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10833.6552734375
tensor(10833.6553, grad_fn=<NegBackward0>) tensor(10833.6553, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10833.6552734375
tensor(10833.6553, grad_fn=<NegBackward0>) tensor(10833.6553, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10831.5068359375
tensor(10833.6553, grad_fn=<NegBackward0>) tensor(10831.5068, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10831.37890625
tensor(10831.5068, grad_fn=<NegBackward0>) tensor(10831.3789, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10831.3828125
tensor(10831.3789, grad_fn=<NegBackward0>) tensor(10831.3828, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -10831.3779296875
tensor(10831.3789, grad_fn=<NegBackward0>) tensor(10831.3779, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10831.3837890625
tensor(10831.3779, grad_fn=<NegBackward0>) tensor(10831.3838, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10831.376953125
tensor(10831.3779, grad_fn=<NegBackward0>) tensor(10831.3770, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10831.4287109375
tensor(10831.3770, grad_fn=<NegBackward0>) tensor(10831.4287, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10831.3779296875
tensor(10831.3770, grad_fn=<NegBackward0>) tensor(10831.3779, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -10831.35546875
tensor(10831.3770, grad_fn=<NegBackward0>) tensor(10831.3555, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10831.3525390625
tensor(10831.3555, grad_fn=<NegBackward0>) tensor(10831.3525, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10831.453125
tensor(10831.3525, grad_fn=<NegBackward0>) tensor(10831.4531, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -10831.3525390625
tensor(10831.3525, grad_fn=<NegBackward0>) tensor(10831.3525, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10831.3525390625
tensor(10831.3525, grad_fn=<NegBackward0>) tensor(10831.3525, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10831.3544921875
tensor(10831.3525, grad_fn=<NegBackward0>) tensor(10831.3545, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -10831.35546875
tensor(10831.3525, grad_fn=<NegBackward0>) tensor(10831.3555, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -10831.30078125
tensor(10831.3525, grad_fn=<NegBackward0>) tensor(10831.3008, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10831.3583984375
tensor(10831.3008, grad_fn=<NegBackward0>) tensor(10831.3584, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -10831.298828125
tensor(10831.3008, grad_fn=<NegBackward0>) tensor(10831.2988, grad_fn=<NegBackward0>)
pi: tensor([[0.7556, 0.2444],
        [0.2944, 0.7056]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4272, 0.5728], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2482, 0.0939],
         [0.5712, 0.2069]],

        [[0.5335, 0.0937],
         [0.5162, 0.5228]],

        [[0.7038, 0.0951],
         [0.6331, 0.5629]],

        [[0.6862, 0.0974],
         [0.7190, 0.6935]],

        [[0.5514, 0.1045],
         [0.6628, 0.5183]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7720997823854144
time is 2
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448099509740526
time is 3
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448420005390695
time is 4
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 6
Adjusted Rand Index: 0.7720997823854144
Global Adjusted Rand Index: 0.8241117577210788
Average Adjusted Rand Index: 0.8232551517416387
[0.00015159247594385454, 0.8241117577210788] [0.0003033213513743884, 0.8232551517416387] [10979.52734375, 10831.302734375]
-------------------------------------
This iteration is 20
True Objective function: Loss = -10946.009702805504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21719.78125
inf tensor(21719.7812, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11050.1669921875
tensor(21719.7812, grad_fn=<NegBackward0>) tensor(11050.1670, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11049.4853515625
tensor(11050.1670, grad_fn=<NegBackward0>) tensor(11049.4854, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11049.345703125
tensor(11049.4854, grad_fn=<NegBackward0>) tensor(11049.3457, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11049.263671875
tensor(11049.3457, grad_fn=<NegBackward0>) tensor(11049.2637, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11049.197265625
tensor(11049.2637, grad_fn=<NegBackward0>) tensor(11049.1973, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11049.1435546875
tensor(11049.1973, grad_fn=<NegBackward0>) tensor(11049.1436, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11049.0966796875
tensor(11049.1436, grad_fn=<NegBackward0>) tensor(11049.0967, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11049.052734375
tensor(11049.0967, grad_fn=<NegBackward0>) tensor(11049.0527, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11049.015625
tensor(11049.0527, grad_fn=<NegBackward0>) tensor(11049.0156, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11048.98046875
tensor(11049.0156, grad_fn=<NegBackward0>) tensor(11048.9805, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11048.951171875
tensor(11048.9805, grad_fn=<NegBackward0>) tensor(11048.9512, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11048.9296875
tensor(11048.9512, grad_fn=<NegBackward0>) tensor(11048.9297, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11048.91015625
tensor(11048.9297, grad_fn=<NegBackward0>) tensor(11048.9102, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11048.89453125
tensor(11048.9102, grad_fn=<NegBackward0>) tensor(11048.8945, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11048.8828125
tensor(11048.8945, grad_fn=<NegBackward0>) tensor(11048.8828, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11048.873046875
tensor(11048.8828, grad_fn=<NegBackward0>) tensor(11048.8730, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11048.8623046875
tensor(11048.8730, grad_fn=<NegBackward0>) tensor(11048.8623, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11048.8525390625
tensor(11048.8623, grad_fn=<NegBackward0>) tensor(11048.8525, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11048.8447265625
tensor(11048.8525, grad_fn=<NegBackward0>) tensor(11048.8447, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11048.8369140625
tensor(11048.8447, grad_fn=<NegBackward0>) tensor(11048.8369, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11048.826171875
tensor(11048.8369, grad_fn=<NegBackward0>) tensor(11048.8262, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11048.8154296875
tensor(11048.8262, grad_fn=<NegBackward0>) tensor(11048.8154, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11048.8056640625
tensor(11048.8154, grad_fn=<NegBackward0>) tensor(11048.8057, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11048.794921875
tensor(11048.8057, grad_fn=<NegBackward0>) tensor(11048.7949, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11048.7841796875
tensor(11048.7949, grad_fn=<NegBackward0>) tensor(11048.7842, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11048.7705078125
tensor(11048.7842, grad_fn=<NegBackward0>) tensor(11048.7705, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11048.7578125
tensor(11048.7705, grad_fn=<NegBackward0>) tensor(11048.7578, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11048.7451171875
tensor(11048.7578, grad_fn=<NegBackward0>) tensor(11048.7451, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11048.732421875
tensor(11048.7451, grad_fn=<NegBackward0>) tensor(11048.7324, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11048.7197265625
tensor(11048.7324, grad_fn=<NegBackward0>) tensor(11048.7197, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11048.70703125
tensor(11048.7197, grad_fn=<NegBackward0>) tensor(11048.7070, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11048.6943359375
tensor(11048.7070, grad_fn=<NegBackward0>) tensor(11048.6943, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11048.6826171875
tensor(11048.6943, grad_fn=<NegBackward0>) tensor(11048.6826, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11048.6728515625
tensor(11048.6826, grad_fn=<NegBackward0>) tensor(11048.6729, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11048.6611328125
tensor(11048.6729, grad_fn=<NegBackward0>) tensor(11048.6611, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11048.6484375
tensor(11048.6611, grad_fn=<NegBackward0>) tensor(11048.6484, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11048.634765625
tensor(11048.6484, grad_fn=<NegBackward0>) tensor(11048.6348, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11048.62109375
tensor(11048.6348, grad_fn=<NegBackward0>) tensor(11048.6211, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11048.6015625
tensor(11048.6211, grad_fn=<NegBackward0>) tensor(11048.6016, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11048.57421875
tensor(11048.6016, grad_fn=<NegBackward0>) tensor(11048.5742, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11048.529296875
tensor(11048.5742, grad_fn=<NegBackward0>) tensor(11048.5293, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11048.45703125
tensor(11048.5293, grad_fn=<NegBackward0>) tensor(11048.4570, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11048.333984375
tensor(11048.4570, grad_fn=<NegBackward0>) tensor(11048.3340, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11048.1611328125
tensor(11048.3340, grad_fn=<NegBackward0>) tensor(11048.1611, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11047.990234375
tensor(11048.1611, grad_fn=<NegBackward0>) tensor(11047.9902, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11047.8896484375
tensor(11047.9902, grad_fn=<NegBackward0>) tensor(11047.8896, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11047.8310546875
tensor(11047.8896, grad_fn=<NegBackward0>) tensor(11047.8311, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11047.794921875
tensor(11047.8311, grad_fn=<NegBackward0>) tensor(11047.7949, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11047.7685546875
tensor(11047.7949, grad_fn=<NegBackward0>) tensor(11047.7686, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11047.751953125
tensor(11047.7686, grad_fn=<NegBackward0>) tensor(11047.7520, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11047.73828125
tensor(11047.7520, grad_fn=<NegBackward0>) tensor(11047.7383, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11047.7275390625
tensor(11047.7383, grad_fn=<NegBackward0>) tensor(11047.7275, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11047.7177734375
tensor(11047.7275, grad_fn=<NegBackward0>) tensor(11047.7178, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11047.7119140625
tensor(11047.7178, grad_fn=<NegBackward0>) tensor(11047.7119, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11047.70703125
tensor(11047.7119, grad_fn=<NegBackward0>) tensor(11047.7070, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11047.7001953125
tensor(11047.7070, grad_fn=<NegBackward0>) tensor(11047.7002, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11047.697265625
tensor(11047.7002, grad_fn=<NegBackward0>) tensor(11047.6973, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11047.693359375
tensor(11047.6973, grad_fn=<NegBackward0>) tensor(11047.6934, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11047.689453125
tensor(11047.6934, grad_fn=<NegBackward0>) tensor(11047.6895, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11047.6884765625
tensor(11047.6895, grad_fn=<NegBackward0>) tensor(11047.6885, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11047.685546875
tensor(11047.6885, grad_fn=<NegBackward0>) tensor(11047.6855, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11047.6845703125
tensor(11047.6855, grad_fn=<NegBackward0>) tensor(11047.6846, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11047.681640625
tensor(11047.6846, grad_fn=<NegBackward0>) tensor(11047.6816, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11047.681640625
tensor(11047.6816, grad_fn=<NegBackward0>) tensor(11047.6816, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11047.6796875
tensor(11047.6816, grad_fn=<NegBackward0>) tensor(11047.6797, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11047.6767578125
tensor(11047.6797, grad_fn=<NegBackward0>) tensor(11047.6768, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11047.677734375
tensor(11047.6768, grad_fn=<NegBackward0>) tensor(11047.6777, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11047.67578125
tensor(11047.6768, grad_fn=<NegBackward0>) tensor(11047.6758, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11047.6728515625
tensor(11047.6758, grad_fn=<NegBackward0>) tensor(11047.6729, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11047.673828125
tensor(11047.6729, grad_fn=<NegBackward0>) tensor(11047.6738, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11047.6728515625
tensor(11047.6729, grad_fn=<NegBackward0>) tensor(11047.6729, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11047.6728515625
tensor(11047.6729, grad_fn=<NegBackward0>) tensor(11047.6729, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11047.671875
tensor(11047.6729, grad_fn=<NegBackward0>) tensor(11047.6719, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11047.6708984375
tensor(11047.6719, grad_fn=<NegBackward0>) tensor(11047.6709, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11047.669921875
tensor(11047.6709, grad_fn=<NegBackward0>) tensor(11047.6699, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11047.71484375
tensor(11047.6699, grad_fn=<NegBackward0>) tensor(11047.7148, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11047.669921875
tensor(11047.6699, grad_fn=<NegBackward0>) tensor(11047.6699, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11047.66796875
tensor(11047.6699, grad_fn=<NegBackward0>) tensor(11047.6680, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11047.66796875
tensor(11047.6680, grad_fn=<NegBackward0>) tensor(11047.6680, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11047.6669921875
tensor(11047.6680, grad_fn=<NegBackward0>) tensor(11047.6670, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11047.6669921875
tensor(11047.6670, grad_fn=<NegBackward0>) tensor(11047.6670, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11047.6669921875
tensor(11047.6670, grad_fn=<NegBackward0>) tensor(11047.6670, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11047.666015625
tensor(11047.6670, grad_fn=<NegBackward0>) tensor(11047.6660, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11047.71875
tensor(11047.6660, grad_fn=<NegBackward0>) tensor(11047.7188, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11047.666015625
tensor(11047.6660, grad_fn=<NegBackward0>) tensor(11047.6660, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11047.666015625
tensor(11047.6660, grad_fn=<NegBackward0>) tensor(11047.6660, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11047.67578125
tensor(11047.6660, grad_fn=<NegBackward0>) tensor(11047.6758, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11047.6640625
tensor(11047.6660, grad_fn=<NegBackward0>) tensor(11047.6641, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11047.6630859375
tensor(11047.6641, grad_fn=<NegBackward0>) tensor(11047.6631, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11047.6650390625
tensor(11047.6631, grad_fn=<NegBackward0>) tensor(11047.6650, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11047.6650390625
tensor(11047.6631, grad_fn=<NegBackward0>) tensor(11047.6650, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11047.6767578125
tensor(11047.6631, grad_fn=<NegBackward0>) tensor(11047.6768, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -11047.662109375
tensor(11047.6631, grad_fn=<NegBackward0>) tensor(11047.6621, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11047.6630859375
tensor(11047.6621, grad_fn=<NegBackward0>) tensor(11047.6631, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11047.662109375
tensor(11047.6621, grad_fn=<NegBackward0>) tensor(11047.6621, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11047.662109375
tensor(11047.6621, grad_fn=<NegBackward0>) tensor(11047.6621, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11047.69140625
tensor(11047.6621, grad_fn=<NegBackward0>) tensor(11047.6914, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11047.6630859375
tensor(11047.6621, grad_fn=<NegBackward0>) tensor(11047.6631, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -11047.6630859375
tensor(11047.6621, grad_fn=<NegBackward0>) tensor(11047.6631, grad_fn=<NegBackward0>)
3
pi: tensor([[9.9937e-01, 6.2790e-04],
        [9.3898e-05, 9.9991e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0363, 0.9637], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1704, 0.1363],
         [0.5702, 0.1636]],

        [[0.6713, 0.2357],
         [0.5112, 0.6129]],

        [[0.6170, 0.1762],
         [0.7170, 0.7087]],

        [[0.5255, 0.1271],
         [0.6512, 0.5169]],

        [[0.7135, 0.1639],
         [0.6087, 0.5246]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.00035341041046094813
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.001829943009345947
Global Adjusted Rand Index: 0.001257599769092383
Average Adjusted Rand Index: -0.00015074478516432054
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19752.802734375
inf tensor(19752.8027, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11049.7060546875
tensor(19752.8027, grad_fn=<NegBackward0>) tensor(11049.7061, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11049.3759765625
tensor(11049.7061, grad_fn=<NegBackward0>) tensor(11049.3760, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11049.2333984375
tensor(11049.3760, grad_fn=<NegBackward0>) tensor(11049.2334, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11049.0771484375
tensor(11049.2334, grad_fn=<NegBackward0>) tensor(11049.0771, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11048.8818359375
tensor(11049.0771, grad_fn=<NegBackward0>) tensor(11048.8818, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11048.6826171875
tensor(11048.8818, grad_fn=<NegBackward0>) tensor(11048.6826, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11048.546875
tensor(11048.6826, grad_fn=<NegBackward0>) tensor(11048.5469, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11048.4697265625
tensor(11048.5469, grad_fn=<NegBackward0>) tensor(11048.4697, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11048.41796875
tensor(11048.4697, grad_fn=<NegBackward0>) tensor(11048.4180, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11048.40234375
tensor(11048.4180, grad_fn=<NegBackward0>) tensor(11048.4023, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11048.3896484375
tensor(11048.4023, grad_fn=<NegBackward0>) tensor(11048.3896, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11048.37890625
tensor(11048.3896, grad_fn=<NegBackward0>) tensor(11048.3789, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11048.369140625
tensor(11048.3789, grad_fn=<NegBackward0>) tensor(11048.3691, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11048.359375
tensor(11048.3691, grad_fn=<NegBackward0>) tensor(11048.3594, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11048.3515625
tensor(11048.3594, grad_fn=<NegBackward0>) tensor(11048.3516, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11048.3369140625
tensor(11048.3516, grad_fn=<NegBackward0>) tensor(11048.3369, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11048.3125
tensor(11048.3369, grad_fn=<NegBackward0>) tensor(11048.3125, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11048.259765625
tensor(11048.3125, grad_fn=<NegBackward0>) tensor(11048.2598, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11048.1826171875
tensor(11048.2598, grad_fn=<NegBackward0>) tensor(11048.1826, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11048.1533203125
tensor(11048.1826, grad_fn=<NegBackward0>) tensor(11048.1533, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11048.146484375
tensor(11048.1533, grad_fn=<NegBackward0>) tensor(11048.1465, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11048.1416015625
tensor(11048.1465, grad_fn=<NegBackward0>) tensor(11048.1416, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11048.140625
tensor(11048.1416, grad_fn=<NegBackward0>) tensor(11048.1406, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11048.13671875
tensor(11048.1406, grad_fn=<NegBackward0>) tensor(11048.1367, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11048.134765625
tensor(11048.1367, grad_fn=<NegBackward0>) tensor(11048.1348, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11048.1318359375
tensor(11048.1348, grad_fn=<NegBackward0>) tensor(11048.1318, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11048.1318359375
tensor(11048.1318, grad_fn=<NegBackward0>) tensor(11048.1318, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11048.1328125
tensor(11048.1318, grad_fn=<NegBackward0>) tensor(11048.1328, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11048.1298828125
tensor(11048.1318, grad_fn=<NegBackward0>) tensor(11048.1299, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11048.12890625
tensor(11048.1299, grad_fn=<NegBackward0>) tensor(11048.1289, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11048.126953125
tensor(11048.1289, grad_fn=<NegBackward0>) tensor(11048.1270, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11048.125
tensor(11048.1270, grad_fn=<NegBackward0>) tensor(11048.1250, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11048.1259765625
tensor(11048.1250, grad_fn=<NegBackward0>) tensor(11048.1260, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11048.1259765625
tensor(11048.1250, grad_fn=<NegBackward0>) tensor(11048.1260, grad_fn=<NegBackward0>)
2
Iteration 3500: Loss = -11048.1240234375
tensor(11048.1250, grad_fn=<NegBackward0>) tensor(11048.1240, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11048.123046875
tensor(11048.1240, grad_fn=<NegBackward0>) tensor(11048.1230, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11048.1279296875
tensor(11048.1230, grad_fn=<NegBackward0>) tensor(11048.1279, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11048.12109375
tensor(11048.1230, grad_fn=<NegBackward0>) tensor(11048.1211, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11048.1220703125
tensor(11048.1211, grad_fn=<NegBackward0>) tensor(11048.1221, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11048.1220703125
tensor(11048.1211, grad_fn=<NegBackward0>) tensor(11048.1221, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -11048.12109375
tensor(11048.1211, grad_fn=<NegBackward0>) tensor(11048.1211, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11048.1201171875
tensor(11048.1211, grad_fn=<NegBackward0>) tensor(11048.1201, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11048.12109375
tensor(11048.1201, grad_fn=<NegBackward0>) tensor(11048.1211, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11048.1220703125
tensor(11048.1201, grad_fn=<NegBackward0>) tensor(11048.1221, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -11048.1201171875
tensor(11048.1201, grad_fn=<NegBackward0>) tensor(11048.1201, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11048.12109375
tensor(11048.1201, grad_fn=<NegBackward0>) tensor(11048.1211, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11048.1201171875
tensor(11048.1201, grad_fn=<NegBackward0>) tensor(11048.1201, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11048.1201171875
tensor(11048.1201, grad_fn=<NegBackward0>) tensor(11048.1201, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11048.119140625
tensor(11048.1201, grad_fn=<NegBackward0>) tensor(11048.1191, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11048.1201171875
tensor(11048.1191, grad_fn=<NegBackward0>) tensor(11048.1201, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11048.119140625
tensor(11048.1191, grad_fn=<NegBackward0>) tensor(11048.1191, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11048.1201171875
tensor(11048.1191, grad_fn=<NegBackward0>) tensor(11048.1201, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11048.1201171875
tensor(11048.1191, grad_fn=<NegBackward0>) tensor(11048.1201, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -11048.1201171875
tensor(11048.1191, grad_fn=<NegBackward0>) tensor(11048.1201, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -11048.1201171875
tensor(11048.1191, grad_fn=<NegBackward0>) tensor(11048.1201, grad_fn=<NegBackward0>)
4
Iteration 5600: Loss = -11048.1201171875
tensor(11048.1191, grad_fn=<NegBackward0>) tensor(11048.1201, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5600 due to no improvement.
pi: tensor([[0.0278, 0.9722],
        [0.0203, 0.9797]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8544, 0.1456], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1602, 0.1624],
         [0.7180, 0.1654]],

        [[0.5871, 0.2111],
         [0.5573, 0.6368]],

        [[0.7310, 0.1460],
         [0.6022, 0.6677]],

        [[0.6954, 0.0842],
         [0.5303, 0.5060]],

        [[0.6748, 0.1647],
         [0.5367, 0.6216]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.005453571602473584
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00432603684855123
Average Adjusted Rand Index: 0.0010907143204947167
[0.001257599769092383, 0.00432603684855123] [-0.00015074478516432054, 0.0010907143204947167] [11047.6669921875, 11048.1201171875]
-------------------------------------
This iteration is 21
True Objective function: Loss = -10786.458613698027
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20288.453125
inf tensor(20288.4531, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10782.1328125
tensor(20288.4531, grad_fn=<NegBackward0>) tensor(10782.1328, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10781.8193359375
tensor(10782.1328, grad_fn=<NegBackward0>) tensor(10781.8193, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10781.6787109375
tensor(10781.8193, grad_fn=<NegBackward0>) tensor(10781.6787, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10781.5576171875
tensor(10781.6787, grad_fn=<NegBackward0>) tensor(10781.5576, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10781.4501953125
tensor(10781.5576, grad_fn=<NegBackward0>) tensor(10781.4502, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10781.365234375
tensor(10781.4502, grad_fn=<NegBackward0>) tensor(10781.3652, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10781.29296875
tensor(10781.3652, grad_fn=<NegBackward0>) tensor(10781.2930, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10781.2216796875
tensor(10781.2930, grad_fn=<NegBackward0>) tensor(10781.2217, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10781.1357421875
tensor(10781.2217, grad_fn=<NegBackward0>) tensor(10781.1357, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10781.0146484375
tensor(10781.1357, grad_fn=<NegBackward0>) tensor(10781.0146, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10780.837890625
tensor(10781.0146, grad_fn=<NegBackward0>) tensor(10780.8379, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10780.591796875
tensor(10780.8379, grad_fn=<NegBackward0>) tensor(10780.5918, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10780.310546875
tensor(10780.5918, grad_fn=<NegBackward0>) tensor(10780.3105, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10780.0830078125
tensor(10780.3105, grad_fn=<NegBackward0>) tensor(10780.0830, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10779.94921875
tensor(10780.0830, grad_fn=<NegBackward0>) tensor(10779.9492, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10779.8779296875
tensor(10779.9492, grad_fn=<NegBackward0>) tensor(10779.8779, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10779.8349609375
tensor(10779.8779, grad_fn=<NegBackward0>) tensor(10779.8350, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10779.8095703125
tensor(10779.8350, grad_fn=<NegBackward0>) tensor(10779.8096, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10779.7880859375
tensor(10779.8096, grad_fn=<NegBackward0>) tensor(10779.7881, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10779.7666015625
tensor(10779.7881, grad_fn=<NegBackward0>) tensor(10779.7666, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10779.7275390625
tensor(10779.7666, grad_fn=<NegBackward0>) tensor(10779.7275, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10779.6630859375
tensor(10779.7275, grad_fn=<NegBackward0>) tensor(10779.6631, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10779.4970703125
tensor(10779.6631, grad_fn=<NegBackward0>) tensor(10779.4971, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10778.7958984375
tensor(10779.4971, grad_fn=<NegBackward0>) tensor(10778.7959, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10777.6875
tensor(10778.7959, grad_fn=<NegBackward0>) tensor(10777.6875, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10777.400390625
tensor(10777.6875, grad_fn=<NegBackward0>) tensor(10777.4004, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10777.3193359375
tensor(10777.4004, grad_fn=<NegBackward0>) tensor(10777.3193, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10777.275390625
tensor(10777.3193, grad_fn=<NegBackward0>) tensor(10777.2754, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10777.1162109375
tensor(10777.2754, grad_fn=<NegBackward0>) tensor(10777.1162, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10777.1025390625
tensor(10777.1162, grad_fn=<NegBackward0>) tensor(10777.1025, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10777.0966796875
tensor(10777.1025, grad_fn=<NegBackward0>) tensor(10777.0967, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10777.0927734375
tensor(10777.0967, grad_fn=<NegBackward0>) tensor(10777.0928, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10777.0869140625
tensor(10777.0928, grad_fn=<NegBackward0>) tensor(10777.0869, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10777.0830078125
tensor(10777.0869, grad_fn=<NegBackward0>) tensor(10777.0830, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10777.0791015625
tensor(10777.0830, grad_fn=<NegBackward0>) tensor(10777.0791, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10777.072265625
tensor(10777.0791, grad_fn=<NegBackward0>) tensor(10777.0723, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10777.0693359375
tensor(10777.0723, grad_fn=<NegBackward0>) tensor(10777.0693, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10777.064453125
tensor(10777.0693, grad_fn=<NegBackward0>) tensor(10777.0645, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10777.060546875
tensor(10777.0645, grad_fn=<NegBackward0>) tensor(10777.0605, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10777.0546875
tensor(10777.0605, grad_fn=<NegBackward0>) tensor(10777.0547, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10777.0498046875
tensor(10777.0547, grad_fn=<NegBackward0>) tensor(10777.0498, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10777.0380859375
tensor(10777.0498, grad_fn=<NegBackward0>) tensor(10777.0381, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10777.0205078125
tensor(10777.0381, grad_fn=<NegBackward0>) tensor(10777.0205, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10777.0087890625
tensor(10777.0205, grad_fn=<NegBackward0>) tensor(10777.0088, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10776.9853515625
tensor(10777.0088, grad_fn=<NegBackward0>) tensor(10776.9854, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10776.984375
tensor(10776.9854, grad_fn=<NegBackward0>) tensor(10776.9844, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10776.982421875
tensor(10776.9844, grad_fn=<NegBackward0>) tensor(10776.9824, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10776.984375
tensor(10776.9824, grad_fn=<NegBackward0>) tensor(10776.9844, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10776.9833984375
tensor(10776.9824, grad_fn=<NegBackward0>) tensor(10776.9834, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -10776.9833984375
tensor(10776.9824, grad_fn=<NegBackward0>) tensor(10776.9834, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -10776.9833984375
tensor(10776.9824, grad_fn=<NegBackward0>) tensor(10776.9834, grad_fn=<NegBackward0>)
4
Iteration 5200: Loss = -10776.9833984375
tensor(10776.9824, grad_fn=<NegBackward0>) tensor(10776.9834, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5200 due to no improvement.
pi: tensor([[0.9903, 0.0097],
        [0.6082, 0.3918]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8651, 0.1349], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1587, 0.1625],
         [0.5812, 0.1694]],

        [[0.5834, 0.1928],
         [0.6992, 0.6769]],

        [[0.6488, 0.0879],
         [0.6919, 0.5008]],

        [[0.5649, 0.0610],
         [0.6682, 0.5609]],

        [[0.7013, 0.0977],
         [0.5930, 0.5137]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0012662455124815629
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: -0.0016991907422381144
Average Adjusted Rand Index: -0.000932107873554967
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23011.392578125
inf tensor(23011.3926, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10783.13671875
tensor(23011.3926, grad_fn=<NegBackward0>) tensor(10783.1367, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10782.265625
tensor(10783.1367, grad_fn=<NegBackward0>) tensor(10782.2656, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10782.01171875
tensor(10782.2656, grad_fn=<NegBackward0>) tensor(10782.0117, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10781.8935546875
tensor(10782.0117, grad_fn=<NegBackward0>) tensor(10781.8936, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10781.8173828125
tensor(10781.8936, grad_fn=<NegBackward0>) tensor(10781.8174, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10781.7509765625
tensor(10781.8174, grad_fn=<NegBackward0>) tensor(10781.7510, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10781.6650390625
tensor(10781.7510, grad_fn=<NegBackward0>) tensor(10781.6650, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10781.5107421875
tensor(10781.6650, grad_fn=<NegBackward0>) tensor(10781.5107, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10781.33203125
tensor(10781.5107, grad_fn=<NegBackward0>) tensor(10781.3320, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10781.212890625
tensor(10781.3320, grad_fn=<NegBackward0>) tensor(10781.2129, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10781.09375
tensor(10781.2129, grad_fn=<NegBackward0>) tensor(10781.0938, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10780.9345703125
tensor(10781.0938, grad_fn=<NegBackward0>) tensor(10780.9346, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10780.6904296875
tensor(10780.9346, grad_fn=<NegBackward0>) tensor(10780.6904, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10780.3642578125
tensor(10780.6904, grad_fn=<NegBackward0>) tensor(10780.3643, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10780.1005859375
tensor(10780.3643, grad_fn=<NegBackward0>) tensor(10780.1006, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10779.8505859375
tensor(10780.1006, grad_fn=<NegBackward0>) tensor(10779.8506, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10779.5791015625
tensor(10779.8506, grad_fn=<NegBackward0>) tensor(10779.5791, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10779.466796875
tensor(10779.5791, grad_fn=<NegBackward0>) tensor(10779.4668, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10779.3935546875
tensor(10779.4668, grad_fn=<NegBackward0>) tensor(10779.3936, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10778.5986328125
tensor(10779.3936, grad_fn=<NegBackward0>) tensor(10778.5986, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10777.521484375
tensor(10778.5986, grad_fn=<NegBackward0>) tensor(10777.5215, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10777.3798828125
tensor(10777.5215, grad_fn=<NegBackward0>) tensor(10777.3799, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10777.333984375
tensor(10777.3799, grad_fn=<NegBackward0>) tensor(10777.3340, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10777.30859375
tensor(10777.3340, grad_fn=<NegBackward0>) tensor(10777.3086, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10777.291015625
tensor(10777.3086, grad_fn=<NegBackward0>) tensor(10777.2910, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10777.27734375
tensor(10777.2910, grad_fn=<NegBackward0>) tensor(10777.2773, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10777.2666015625
tensor(10777.2773, grad_fn=<NegBackward0>) tensor(10777.2666, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10777.2578125
tensor(10777.2666, grad_fn=<NegBackward0>) tensor(10777.2578, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10777.251953125
tensor(10777.2578, grad_fn=<NegBackward0>) tensor(10777.2520, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10777.248046875
tensor(10777.2520, grad_fn=<NegBackward0>) tensor(10777.2480, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10777.244140625
tensor(10777.2480, grad_fn=<NegBackward0>) tensor(10777.2441, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10777.2421875
tensor(10777.2441, grad_fn=<NegBackward0>) tensor(10777.2422, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10777.2392578125
tensor(10777.2422, grad_fn=<NegBackward0>) tensor(10777.2393, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10777.2373046875
tensor(10777.2393, grad_fn=<NegBackward0>) tensor(10777.2373, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10777.236328125
tensor(10777.2373, grad_fn=<NegBackward0>) tensor(10777.2363, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10777.23046875
tensor(10777.2363, grad_fn=<NegBackward0>) tensor(10777.2305, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10777.11328125
tensor(10777.2305, grad_fn=<NegBackward0>) tensor(10777.1133, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10777.10546875
tensor(10777.1133, grad_fn=<NegBackward0>) tensor(10777.1055, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10777.103515625
tensor(10777.1055, grad_fn=<NegBackward0>) tensor(10777.1035, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10777.0966796875
tensor(10777.1035, grad_fn=<NegBackward0>) tensor(10777.0967, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10777.09375
tensor(10777.0967, grad_fn=<NegBackward0>) tensor(10777.0938, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10777.0927734375
tensor(10777.0938, grad_fn=<NegBackward0>) tensor(10777.0928, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10777.0927734375
tensor(10777.0928, grad_fn=<NegBackward0>) tensor(10777.0928, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10777.0888671875
tensor(10777.0928, grad_fn=<NegBackward0>) tensor(10777.0889, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10777.0888671875
tensor(10777.0889, grad_fn=<NegBackward0>) tensor(10777.0889, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10777.087890625
tensor(10777.0889, grad_fn=<NegBackward0>) tensor(10777.0879, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10777.087890625
tensor(10777.0879, grad_fn=<NegBackward0>) tensor(10777.0879, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10777.0869140625
tensor(10777.0879, grad_fn=<NegBackward0>) tensor(10777.0869, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10777.0859375
tensor(10777.0869, grad_fn=<NegBackward0>) tensor(10777.0859, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10777.0859375
tensor(10777.0859, grad_fn=<NegBackward0>) tensor(10777.0859, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10777.0830078125
tensor(10777.0859, grad_fn=<NegBackward0>) tensor(10777.0830, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10777.083984375
tensor(10777.0830, grad_fn=<NegBackward0>) tensor(10777.0840, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10777.0810546875
tensor(10777.0830, grad_fn=<NegBackward0>) tensor(10777.0811, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10777.08203125
tensor(10777.0811, grad_fn=<NegBackward0>) tensor(10777.0820, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10777.0791015625
tensor(10777.0811, grad_fn=<NegBackward0>) tensor(10777.0791, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10777.07421875
tensor(10777.0791, grad_fn=<NegBackward0>) tensor(10777.0742, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10777.072265625
tensor(10777.0742, grad_fn=<NegBackward0>) tensor(10777.0723, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10777.0654296875
tensor(10777.0723, grad_fn=<NegBackward0>) tensor(10777.0654, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10777.0634765625
tensor(10777.0654, grad_fn=<NegBackward0>) tensor(10777.0635, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10777.05859375
tensor(10777.0635, grad_fn=<NegBackward0>) tensor(10777.0586, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10777.052734375
tensor(10777.0586, grad_fn=<NegBackward0>) tensor(10777.0527, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10777.0439453125
tensor(10777.0527, grad_fn=<NegBackward0>) tensor(10777.0439, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10777.033203125
tensor(10777.0439, grad_fn=<NegBackward0>) tensor(10777.0332, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10777.0029296875
tensor(10777.0332, grad_fn=<NegBackward0>) tensor(10777.0029, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10776.984375
tensor(10777.0029, grad_fn=<NegBackward0>) tensor(10776.9844, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10776.98046875
tensor(10776.9844, grad_fn=<NegBackward0>) tensor(10776.9805, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10776.98046875
tensor(10776.9805, grad_fn=<NegBackward0>) tensor(10776.9805, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10776.98046875
tensor(10776.9805, grad_fn=<NegBackward0>) tensor(10776.9805, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10776.9794921875
tensor(10776.9805, grad_fn=<NegBackward0>) tensor(10776.9795, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10776.9814453125
tensor(10776.9795, grad_fn=<NegBackward0>) tensor(10776.9814, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10776.98046875
tensor(10776.9795, grad_fn=<NegBackward0>) tensor(10776.9805, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -10776.9794921875
tensor(10776.9795, grad_fn=<NegBackward0>) tensor(10776.9795, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10776.9794921875
tensor(10776.9795, grad_fn=<NegBackward0>) tensor(10776.9795, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10776.9794921875
tensor(10776.9795, grad_fn=<NegBackward0>) tensor(10776.9795, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10776.9853515625
tensor(10776.9795, grad_fn=<NegBackward0>) tensor(10776.9854, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10776.98046875
tensor(10776.9795, grad_fn=<NegBackward0>) tensor(10776.9805, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -10776.9814453125
tensor(10776.9795, grad_fn=<NegBackward0>) tensor(10776.9814, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -10776.9794921875
tensor(10776.9795, grad_fn=<NegBackward0>) tensor(10776.9795, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10776.9814453125
tensor(10776.9795, grad_fn=<NegBackward0>) tensor(10776.9814, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10776.98046875
tensor(10776.9795, grad_fn=<NegBackward0>) tensor(10776.9805, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -10777.0341796875
tensor(10776.9795, grad_fn=<NegBackward0>) tensor(10777.0342, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -10776.9814453125
tensor(10776.9795, grad_fn=<NegBackward0>) tensor(10776.9814, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -10776.98046875
tensor(10776.9795, grad_fn=<NegBackward0>) tensor(10776.9805, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8300 due to no improvement.
pi: tensor([[0.3919, 0.6081],
        [0.0097, 0.9903]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1350, 0.8650], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1694, 0.1625],
         [0.5268, 0.1587]],

        [[0.6460, 0.1928],
         [0.6718, 0.7249]],

        [[0.7075, 0.0879],
         [0.5375, 0.6621]],

        [[0.7125, 0.0610],
         [0.5962, 0.5164]],

        [[0.7139, 0.0977],
         [0.7309, 0.6922]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0012662455124815629
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: -0.0016991907422381144
Average Adjusted Rand Index: -0.000932107873554967
[-0.0016991907422381144, -0.0016991907422381144] [-0.000932107873554967, -0.000932107873554967] [10776.9833984375, 10776.98046875]
-------------------------------------
This iteration is 22
True Objective function: Loss = -11001.193050269521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23931.79296875
inf tensor(23931.7930, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11073.873046875
tensor(23931.7930, grad_fn=<NegBackward0>) tensor(11073.8730, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11073.201171875
tensor(11073.8730, grad_fn=<NegBackward0>) tensor(11073.2012, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11072.9814453125
tensor(11073.2012, grad_fn=<NegBackward0>) tensor(11072.9814, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11072.8447265625
tensor(11072.9814, grad_fn=<NegBackward0>) tensor(11072.8447, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11072.7373046875
tensor(11072.8447, grad_fn=<NegBackward0>) tensor(11072.7373, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11072.634765625
tensor(11072.7373, grad_fn=<NegBackward0>) tensor(11072.6348, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11072.533203125
tensor(11072.6348, grad_fn=<NegBackward0>) tensor(11072.5332, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11072.4306640625
tensor(11072.5332, grad_fn=<NegBackward0>) tensor(11072.4307, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11072.32421875
tensor(11072.4307, grad_fn=<NegBackward0>) tensor(11072.3242, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11072.21484375
tensor(11072.3242, grad_fn=<NegBackward0>) tensor(11072.2148, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11072.1044921875
tensor(11072.2148, grad_fn=<NegBackward0>) tensor(11072.1045, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11071.998046875
tensor(11072.1045, grad_fn=<NegBackward0>) tensor(11071.9980, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11071.89453125
tensor(11071.9980, grad_fn=<NegBackward0>) tensor(11071.8945, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11071.806640625
tensor(11071.8945, grad_fn=<NegBackward0>) tensor(11071.8066, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11071.7353515625
tensor(11071.8066, grad_fn=<NegBackward0>) tensor(11071.7354, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11071.6904296875
tensor(11071.7354, grad_fn=<NegBackward0>) tensor(11071.6904, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11071.662109375
tensor(11071.6904, grad_fn=<NegBackward0>) tensor(11071.6621, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11071.6494140625
tensor(11071.6621, grad_fn=<NegBackward0>) tensor(11071.6494, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11071.6416015625
tensor(11071.6494, grad_fn=<NegBackward0>) tensor(11071.6416, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11071.638671875
tensor(11071.6416, grad_fn=<NegBackward0>) tensor(11071.6387, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11071.63671875
tensor(11071.6387, grad_fn=<NegBackward0>) tensor(11071.6367, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11071.634765625
tensor(11071.6367, grad_fn=<NegBackward0>) tensor(11071.6348, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11071.634765625
tensor(11071.6348, grad_fn=<NegBackward0>) tensor(11071.6348, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11071.6357421875
tensor(11071.6348, grad_fn=<NegBackward0>) tensor(11071.6357, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -11071.634765625
tensor(11071.6348, grad_fn=<NegBackward0>) tensor(11071.6348, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11071.6328125
tensor(11071.6348, grad_fn=<NegBackward0>) tensor(11071.6328, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11071.6337890625
tensor(11071.6328, grad_fn=<NegBackward0>) tensor(11071.6338, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11071.6337890625
tensor(11071.6328, grad_fn=<NegBackward0>) tensor(11071.6338, grad_fn=<NegBackward0>)
2
Iteration 2900: Loss = -11071.6318359375
tensor(11071.6328, grad_fn=<NegBackward0>) tensor(11071.6318, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11071.6328125
tensor(11071.6318, grad_fn=<NegBackward0>) tensor(11071.6328, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11071.6337890625
tensor(11071.6318, grad_fn=<NegBackward0>) tensor(11071.6338, grad_fn=<NegBackward0>)
2
Iteration 3200: Loss = -11071.6337890625
tensor(11071.6318, grad_fn=<NegBackward0>) tensor(11071.6338, grad_fn=<NegBackward0>)
3
Iteration 3300: Loss = -11071.6328125
tensor(11071.6318, grad_fn=<NegBackward0>) tensor(11071.6328, grad_fn=<NegBackward0>)
4
Iteration 3400: Loss = -11071.6337890625
tensor(11071.6318, grad_fn=<NegBackward0>) tensor(11071.6338, grad_fn=<NegBackward0>)
5
Stopping early at iteration 3400 due to no improvement.
pi: tensor([[0.9433, 0.0567],
        [0.9477, 0.0523]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9930, 0.0070], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1647, 0.1746],
         [0.6434, 0.1557]],

        [[0.6654, 0.2370],
         [0.6731, 0.6361]],

        [[0.6016, 0.1493],
         [0.6492, 0.5459]],

        [[0.7122, 0.1484],
         [0.7274, 0.5752]],

        [[0.6944, 0.1254],
         [0.5214, 0.6949]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: -0.012494332208171696
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0015272641989018702
Average Adjusted Rand Index: -0.002498866441634339
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24161.490234375
inf tensor(24161.4902, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11073.130859375
tensor(24161.4902, grad_fn=<NegBackward0>) tensor(11073.1309, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11070.626953125
tensor(11073.1309, grad_fn=<NegBackward0>) tensor(11070.6270, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11069.8935546875
tensor(11070.6270, grad_fn=<NegBackward0>) tensor(11069.8936, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11069.509765625
tensor(11069.8936, grad_fn=<NegBackward0>) tensor(11069.5098, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11069.2548828125
tensor(11069.5098, grad_fn=<NegBackward0>) tensor(11069.2549, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11068.962890625
tensor(11069.2549, grad_fn=<NegBackward0>) tensor(11068.9629, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11068.6025390625
tensor(11068.9629, grad_fn=<NegBackward0>) tensor(11068.6025, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11068.373046875
tensor(11068.6025, grad_fn=<NegBackward0>) tensor(11068.3730, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11068.193359375
tensor(11068.3730, grad_fn=<NegBackward0>) tensor(11068.1934, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11068.0556640625
tensor(11068.1934, grad_fn=<NegBackward0>) tensor(11068.0557, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11067.958984375
tensor(11068.0557, grad_fn=<NegBackward0>) tensor(11067.9590, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11067.89453125
tensor(11067.9590, grad_fn=<NegBackward0>) tensor(11067.8945, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11067.8525390625
tensor(11067.8945, grad_fn=<NegBackward0>) tensor(11067.8525, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11067.82421875
tensor(11067.8525, grad_fn=<NegBackward0>) tensor(11067.8242, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11067.802734375
tensor(11067.8242, grad_fn=<NegBackward0>) tensor(11067.8027, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11067.787109375
tensor(11067.8027, grad_fn=<NegBackward0>) tensor(11067.7871, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11067.7744140625
tensor(11067.7871, grad_fn=<NegBackward0>) tensor(11067.7744, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11067.7626953125
tensor(11067.7744, grad_fn=<NegBackward0>) tensor(11067.7627, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11067.751953125
tensor(11067.7627, grad_fn=<NegBackward0>) tensor(11067.7520, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11067.744140625
tensor(11067.7520, grad_fn=<NegBackward0>) tensor(11067.7441, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11067.7333984375
tensor(11067.7441, grad_fn=<NegBackward0>) tensor(11067.7334, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11067.71875
tensor(11067.7334, grad_fn=<NegBackward0>) tensor(11067.7188, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11067.6953125
tensor(11067.7188, grad_fn=<NegBackward0>) tensor(11067.6953, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11067.6142578125
tensor(11067.6953, grad_fn=<NegBackward0>) tensor(11067.6143, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11067.1474609375
tensor(11067.6143, grad_fn=<NegBackward0>) tensor(11067.1475, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11066.5498046875
tensor(11067.1475, grad_fn=<NegBackward0>) tensor(11066.5498, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11066.3544921875
tensor(11066.5498, grad_fn=<NegBackward0>) tensor(11066.3545, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11066.31640625
tensor(11066.3545, grad_fn=<NegBackward0>) tensor(11066.3164, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11066.30078125
tensor(11066.3164, grad_fn=<NegBackward0>) tensor(11066.3008, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11066.2939453125
tensor(11066.3008, grad_fn=<NegBackward0>) tensor(11066.2939, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11066.2890625
tensor(11066.2939, grad_fn=<NegBackward0>) tensor(11066.2891, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11066.2861328125
tensor(11066.2891, grad_fn=<NegBackward0>) tensor(11066.2861, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11066.283203125
tensor(11066.2861, grad_fn=<NegBackward0>) tensor(11066.2832, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11066.2822265625
tensor(11066.2832, grad_fn=<NegBackward0>) tensor(11066.2822, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11066.2802734375
tensor(11066.2822, grad_fn=<NegBackward0>) tensor(11066.2803, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11066.28125
tensor(11066.2803, grad_fn=<NegBackward0>) tensor(11066.2812, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11066.28125
tensor(11066.2803, grad_fn=<NegBackward0>) tensor(11066.2812, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -11066.28125
tensor(11066.2803, grad_fn=<NegBackward0>) tensor(11066.2812, grad_fn=<NegBackward0>)
3
Iteration 3900: Loss = -11066.2783203125
tensor(11066.2803, grad_fn=<NegBackward0>) tensor(11066.2783, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11066.2783203125
tensor(11066.2783, grad_fn=<NegBackward0>) tensor(11066.2783, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11066.2783203125
tensor(11066.2783, grad_fn=<NegBackward0>) tensor(11066.2783, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11066.2783203125
tensor(11066.2783, grad_fn=<NegBackward0>) tensor(11066.2783, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11066.27734375
tensor(11066.2783, grad_fn=<NegBackward0>) tensor(11066.2773, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11066.27734375
tensor(11066.2773, grad_fn=<NegBackward0>) tensor(11066.2773, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11066.2763671875
tensor(11066.2773, grad_fn=<NegBackward0>) tensor(11066.2764, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11066.2783203125
tensor(11066.2764, grad_fn=<NegBackward0>) tensor(11066.2783, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11066.2763671875
tensor(11066.2764, grad_fn=<NegBackward0>) tensor(11066.2764, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11066.275390625
tensor(11066.2764, grad_fn=<NegBackward0>) tensor(11066.2754, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11066.2763671875
tensor(11066.2754, grad_fn=<NegBackward0>) tensor(11066.2764, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11066.275390625
tensor(11066.2754, grad_fn=<NegBackward0>) tensor(11066.2754, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11066.2763671875
tensor(11066.2754, grad_fn=<NegBackward0>) tensor(11066.2764, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11066.27734375
tensor(11066.2754, grad_fn=<NegBackward0>) tensor(11066.2773, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11066.275390625
tensor(11066.2754, grad_fn=<NegBackward0>) tensor(11066.2754, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11066.275390625
tensor(11066.2754, grad_fn=<NegBackward0>) tensor(11066.2754, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11066.2744140625
tensor(11066.2754, grad_fn=<NegBackward0>) tensor(11066.2744, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11066.27734375
tensor(11066.2744, grad_fn=<NegBackward0>) tensor(11066.2773, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11066.2763671875
tensor(11066.2744, grad_fn=<NegBackward0>) tensor(11066.2764, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11066.275390625
tensor(11066.2744, grad_fn=<NegBackward0>) tensor(11066.2754, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -11066.275390625
tensor(11066.2744, grad_fn=<NegBackward0>) tensor(11066.2754, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -11066.2744140625
tensor(11066.2744, grad_fn=<NegBackward0>) tensor(11066.2744, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11066.275390625
tensor(11066.2744, grad_fn=<NegBackward0>) tensor(11066.2754, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11066.2763671875
tensor(11066.2744, grad_fn=<NegBackward0>) tensor(11066.2764, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11066.2744140625
tensor(11066.2744, grad_fn=<NegBackward0>) tensor(11066.2744, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11066.2763671875
tensor(11066.2744, grad_fn=<NegBackward0>) tensor(11066.2764, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11066.2763671875
tensor(11066.2744, grad_fn=<NegBackward0>) tensor(11066.2764, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11066.2763671875
tensor(11066.2744, grad_fn=<NegBackward0>) tensor(11066.2764, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11066.275390625
tensor(11066.2744, grad_fn=<NegBackward0>) tensor(11066.2754, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -11066.275390625
tensor(11066.2744, grad_fn=<NegBackward0>) tensor(11066.2754, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6800 due to no improvement.
pi: tensor([[0.1029, 0.8971],
        [0.0131, 0.9869]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7771, 0.2229], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1692, 0.1617],
         [0.6861, 0.1594]],

        [[0.7285, 0.2354],
         [0.5521, 0.5874]],

        [[0.6318, 0.1566],
         [0.7161, 0.6193]],

        [[0.5271, 0.2997],
         [0.6732, 0.5854]],

        [[0.6759, 0.3065],
         [0.5527, 0.6031]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.02768166089965398
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
Global Adjusted Rand Index: -0.004342889006947893
Average Adjusted Rand Index: -0.006971350227542255
[-0.0015272641989018702, -0.004342889006947893] [-0.002498866441634339, -0.006971350227542255] [11071.6337890625, 11066.275390625]
-------------------------------------
This iteration is 23
True Objective function: Loss = -10954.605200692185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23299.681640625
inf tensor(23299.6816, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11053.7138671875
tensor(23299.6816, grad_fn=<NegBackward0>) tensor(11053.7139, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11052.783203125
tensor(11053.7139, grad_fn=<NegBackward0>) tensor(11052.7832, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11037.8662109375
tensor(11052.7832, grad_fn=<NegBackward0>) tensor(11037.8662, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11037.412109375
tensor(11037.8662, grad_fn=<NegBackward0>) tensor(11037.4121, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11037.3056640625
tensor(11037.4121, grad_fn=<NegBackward0>) tensor(11037.3057, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11037.2060546875
tensor(11037.3057, grad_fn=<NegBackward0>) tensor(11037.2061, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11037.0859375
tensor(11037.2061, grad_fn=<NegBackward0>) tensor(11037.0859, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11036.931640625
tensor(11037.0859, grad_fn=<NegBackward0>) tensor(11036.9316, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11036.7294921875
tensor(11036.9316, grad_fn=<NegBackward0>) tensor(11036.7295, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11036.4970703125
tensor(11036.7295, grad_fn=<NegBackward0>) tensor(11036.4971, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11036.3310546875
tensor(11036.4971, grad_fn=<NegBackward0>) tensor(11036.3311, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11036.18359375
tensor(11036.3311, grad_fn=<NegBackward0>) tensor(11036.1836, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11036.037109375
tensor(11036.1836, grad_fn=<NegBackward0>) tensor(11036.0371, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11036.0107421875
tensor(11036.0371, grad_fn=<NegBackward0>) tensor(11036.0107, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11036.005859375
tensor(11036.0107, grad_fn=<NegBackward0>) tensor(11036.0059, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11036.001953125
tensor(11036.0059, grad_fn=<NegBackward0>) tensor(11036.0020, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11035.998046875
tensor(11036.0020, grad_fn=<NegBackward0>) tensor(11035.9980, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11035.9931640625
tensor(11035.9980, grad_fn=<NegBackward0>) tensor(11035.9932, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11035.9921875
tensor(11035.9932, grad_fn=<NegBackward0>) tensor(11035.9922, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11035.9912109375
tensor(11035.9922, grad_fn=<NegBackward0>) tensor(11035.9912, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11035.9892578125
tensor(11035.9912, grad_fn=<NegBackward0>) tensor(11035.9893, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11035.98828125
tensor(11035.9893, grad_fn=<NegBackward0>) tensor(11035.9883, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11035.9873046875
tensor(11035.9883, grad_fn=<NegBackward0>) tensor(11035.9873, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11035.9853515625
tensor(11035.9873, grad_fn=<NegBackward0>) tensor(11035.9854, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11035.9833984375
tensor(11035.9854, grad_fn=<NegBackward0>) tensor(11035.9834, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11035.984375
tensor(11035.9834, grad_fn=<NegBackward0>) tensor(11035.9844, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -11035.9833984375
tensor(11035.9834, grad_fn=<NegBackward0>) tensor(11035.9834, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11035.9814453125
tensor(11035.9834, grad_fn=<NegBackward0>) tensor(11035.9814, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11035.9794921875
tensor(11035.9814, grad_fn=<NegBackward0>) tensor(11035.9795, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11035.98046875
tensor(11035.9795, grad_fn=<NegBackward0>) tensor(11035.9805, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11035.9794921875
tensor(11035.9795, grad_fn=<NegBackward0>) tensor(11035.9795, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11035.9765625
tensor(11035.9795, grad_fn=<NegBackward0>) tensor(11035.9766, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11035.9775390625
tensor(11035.9766, grad_fn=<NegBackward0>) tensor(11035.9775, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11035.9775390625
tensor(11035.9766, grad_fn=<NegBackward0>) tensor(11035.9775, grad_fn=<NegBackward0>)
2
Iteration 3500: Loss = -11035.9765625
tensor(11035.9766, grad_fn=<NegBackward0>) tensor(11035.9766, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11035.9765625
tensor(11035.9766, grad_fn=<NegBackward0>) tensor(11035.9766, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11035.9736328125
tensor(11035.9766, grad_fn=<NegBackward0>) tensor(11035.9736, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11035.9755859375
tensor(11035.9736, grad_fn=<NegBackward0>) tensor(11035.9756, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11035.9755859375
tensor(11035.9736, grad_fn=<NegBackward0>) tensor(11035.9756, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -11035.97265625
tensor(11035.9736, grad_fn=<NegBackward0>) tensor(11035.9727, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11035.9755859375
tensor(11035.9727, grad_fn=<NegBackward0>) tensor(11035.9756, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11035.9736328125
tensor(11035.9727, grad_fn=<NegBackward0>) tensor(11035.9736, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -11035.9736328125
tensor(11035.9727, grad_fn=<NegBackward0>) tensor(11035.9736, grad_fn=<NegBackward0>)
3
Iteration 4400: Loss = -11035.97265625
tensor(11035.9727, grad_fn=<NegBackward0>) tensor(11035.9727, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11035.9794921875
tensor(11035.9727, grad_fn=<NegBackward0>) tensor(11035.9795, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11035.9716796875
tensor(11035.9727, grad_fn=<NegBackward0>) tensor(11035.9717, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11035.97265625
tensor(11035.9717, grad_fn=<NegBackward0>) tensor(11035.9727, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11035.97265625
tensor(11035.9717, grad_fn=<NegBackward0>) tensor(11035.9727, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -11035.9716796875
tensor(11035.9717, grad_fn=<NegBackward0>) tensor(11035.9717, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11035.97265625
tensor(11035.9717, grad_fn=<NegBackward0>) tensor(11035.9727, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11035.97265625
tensor(11035.9717, grad_fn=<NegBackward0>) tensor(11035.9727, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11035.9716796875
tensor(11035.9717, grad_fn=<NegBackward0>) tensor(11035.9717, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11035.9716796875
tensor(11035.9717, grad_fn=<NegBackward0>) tensor(11035.9717, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11035.9716796875
tensor(11035.9717, grad_fn=<NegBackward0>) tensor(11035.9717, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11035.9716796875
tensor(11035.9717, grad_fn=<NegBackward0>) tensor(11035.9717, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11035.9716796875
tensor(11035.9717, grad_fn=<NegBackward0>) tensor(11035.9717, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11035.97265625
tensor(11035.9717, grad_fn=<NegBackward0>) tensor(11035.9727, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11035.9697265625
tensor(11035.9717, grad_fn=<NegBackward0>) tensor(11035.9697, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11035.970703125
tensor(11035.9697, grad_fn=<NegBackward0>) tensor(11035.9707, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11035.9697265625
tensor(11035.9697, grad_fn=<NegBackward0>) tensor(11035.9697, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11035.9697265625
tensor(11035.9697, grad_fn=<NegBackward0>) tensor(11035.9697, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11035.970703125
tensor(11035.9697, grad_fn=<NegBackward0>) tensor(11035.9707, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11035.96875
tensor(11035.9697, grad_fn=<NegBackward0>) tensor(11035.9688, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11035.970703125
tensor(11035.9688, grad_fn=<NegBackward0>) tensor(11035.9707, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11035.970703125
tensor(11035.9688, grad_fn=<NegBackward0>) tensor(11035.9707, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11035.9755859375
tensor(11035.9688, grad_fn=<NegBackward0>) tensor(11035.9756, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11035.9697265625
tensor(11035.9688, grad_fn=<NegBackward0>) tensor(11035.9697, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -11035.9697265625
tensor(11035.9688, grad_fn=<NegBackward0>) tensor(11035.9697, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6800 due to no improvement.
pi: tensor([[1.0180e-04, 9.9990e-01],
        [4.5017e-02, 9.5498e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5377, 0.4623], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2710, 0.1141],
         [0.7103, 0.1613]],

        [[0.5357, 0.1543],
         [0.5874, 0.6845]],

        [[0.5463, 0.2242],
         [0.5166, 0.6495]],

        [[0.7298, 0.1964],
         [0.5776, 0.5523]],

        [[0.7220, 0.1292],
         [0.6776, 0.7193]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448509923071951
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.014262894825058084
Average Adjusted Rand Index: 0.16798812697577042
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23561.037109375
inf tensor(23561.0371, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11057.9111328125
tensor(23561.0371, grad_fn=<NegBackward0>) tensor(11057.9111, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11055.4970703125
tensor(11057.9111, grad_fn=<NegBackward0>) tensor(11055.4971, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11053.2353515625
tensor(11055.4971, grad_fn=<NegBackward0>) tensor(11053.2354, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11052.224609375
tensor(11053.2354, grad_fn=<NegBackward0>) tensor(11052.2246, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11050.466796875
tensor(11052.2246, grad_fn=<NegBackward0>) tensor(11050.4668, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11048.16796875
tensor(11050.4668, grad_fn=<NegBackward0>) tensor(11048.1680, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11044.939453125
tensor(11048.1680, grad_fn=<NegBackward0>) tensor(11044.9395, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11037.158203125
tensor(11044.9395, grad_fn=<NegBackward0>) tensor(11037.1582, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11036.330078125
tensor(11037.1582, grad_fn=<NegBackward0>) tensor(11036.3301, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11036.2001953125
tensor(11036.3301, grad_fn=<NegBackward0>) tensor(11036.2002, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11036.1513671875
tensor(11036.2002, grad_fn=<NegBackward0>) tensor(11036.1514, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11036.12890625
tensor(11036.1514, grad_fn=<NegBackward0>) tensor(11036.1289, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11036.1103515625
tensor(11036.1289, grad_fn=<NegBackward0>) tensor(11036.1104, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11036.09765625
tensor(11036.1104, grad_fn=<NegBackward0>) tensor(11036.0977, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11036.0849609375
tensor(11036.0977, grad_fn=<NegBackward0>) tensor(11036.0850, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11036.0751953125
tensor(11036.0850, grad_fn=<NegBackward0>) tensor(11036.0752, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11036.0673828125
tensor(11036.0752, grad_fn=<NegBackward0>) tensor(11036.0674, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11036.05859375
tensor(11036.0674, grad_fn=<NegBackward0>) tensor(11036.0586, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11036.052734375
tensor(11036.0586, grad_fn=<NegBackward0>) tensor(11036.0527, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11036.0458984375
tensor(11036.0527, grad_fn=<NegBackward0>) tensor(11036.0459, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11036.0400390625
tensor(11036.0459, grad_fn=<NegBackward0>) tensor(11036.0400, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11036.033203125
tensor(11036.0400, grad_fn=<NegBackward0>) tensor(11036.0332, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11036.0302734375
tensor(11036.0332, grad_fn=<NegBackward0>) tensor(11036.0303, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11036.0244140625
tensor(11036.0303, grad_fn=<NegBackward0>) tensor(11036.0244, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11036.01953125
tensor(11036.0244, grad_fn=<NegBackward0>) tensor(11036.0195, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11036.0166015625
tensor(11036.0195, grad_fn=<NegBackward0>) tensor(11036.0166, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11036.0126953125
tensor(11036.0166, grad_fn=<NegBackward0>) tensor(11036.0127, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11036.0087890625
tensor(11036.0127, grad_fn=<NegBackward0>) tensor(11036.0088, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11036.005859375
tensor(11036.0088, grad_fn=<NegBackward0>) tensor(11036.0059, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11036.0029296875
tensor(11036.0059, grad_fn=<NegBackward0>) tensor(11036.0029, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11036.0
tensor(11036.0029, grad_fn=<NegBackward0>) tensor(11036., grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11035.998046875
tensor(11036., grad_fn=<NegBackward0>) tensor(11035.9980, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11035.9970703125
tensor(11035.9980, grad_fn=<NegBackward0>) tensor(11035.9971, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11035.9931640625
tensor(11035.9971, grad_fn=<NegBackward0>) tensor(11035.9932, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11035.9931640625
tensor(11035.9932, grad_fn=<NegBackward0>) tensor(11035.9932, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11035.990234375
tensor(11035.9932, grad_fn=<NegBackward0>) tensor(11035.9902, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11035.9892578125
tensor(11035.9902, grad_fn=<NegBackward0>) tensor(11035.9893, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11035.9873046875
tensor(11035.9893, grad_fn=<NegBackward0>) tensor(11035.9873, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11035.9853515625
tensor(11035.9873, grad_fn=<NegBackward0>) tensor(11035.9854, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11035.986328125
tensor(11035.9854, grad_fn=<NegBackward0>) tensor(11035.9863, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11035.9833984375
tensor(11035.9854, grad_fn=<NegBackward0>) tensor(11035.9834, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11035.9921875
tensor(11035.9834, grad_fn=<NegBackward0>) tensor(11035.9922, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11035.982421875
tensor(11035.9834, grad_fn=<NegBackward0>) tensor(11035.9824, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11035.98046875
tensor(11035.9824, grad_fn=<NegBackward0>) tensor(11035.9805, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11035.9794921875
tensor(11035.9805, grad_fn=<NegBackward0>) tensor(11035.9795, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11035.9794921875
tensor(11035.9795, grad_fn=<NegBackward0>) tensor(11035.9795, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11035.98046875
tensor(11035.9795, grad_fn=<NegBackward0>) tensor(11035.9805, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11035.978515625
tensor(11035.9795, grad_fn=<NegBackward0>) tensor(11035.9785, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11035.978515625
tensor(11035.9785, grad_fn=<NegBackward0>) tensor(11035.9785, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11035.978515625
tensor(11035.9785, grad_fn=<NegBackward0>) tensor(11035.9785, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11035.978515625
tensor(11035.9785, grad_fn=<NegBackward0>) tensor(11035.9785, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11035.9775390625
tensor(11035.9785, grad_fn=<NegBackward0>) tensor(11035.9775, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11035.9814453125
tensor(11035.9775, grad_fn=<NegBackward0>) tensor(11035.9814, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11035.9755859375
tensor(11035.9775, grad_fn=<NegBackward0>) tensor(11035.9756, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11035.974609375
tensor(11035.9756, grad_fn=<NegBackward0>) tensor(11035.9746, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11035.9755859375
tensor(11035.9746, grad_fn=<NegBackward0>) tensor(11035.9756, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11035.974609375
tensor(11035.9746, grad_fn=<NegBackward0>) tensor(11035.9746, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11035.974609375
tensor(11035.9746, grad_fn=<NegBackward0>) tensor(11035.9746, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11035.974609375
tensor(11035.9746, grad_fn=<NegBackward0>) tensor(11035.9746, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11035.9736328125
tensor(11035.9746, grad_fn=<NegBackward0>) tensor(11035.9736, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11035.974609375
tensor(11035.9736, grad_fn=<NegBackward0>) tensor(11035.9746, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11035.9736328125
tensor(11035.9736, grad_fn=<NegBackward0>) tensor(11035.9736, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11035.974609375
tensor(11035.9736, grad_fn=<NegBackward0>) tensor(11035.9746, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11035.9736328125
tensor(11035.9736, grad_fn=<NegBackward0>) tensor(11035.9736, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11035.97265625
tensor(11035.9736, grad_fn=<NegBackward0>) tensor(11035.9727, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11035.9736328125
tensor(11035.9727, grad_fn=<NegBackward0>) tensor(11035.9736, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11035.9716796875
tensor(11035.9727, grad_fn=<NegBackward0>) tensor(11035.9717, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11035.97265625
tensor(11035.9717, grad_fn=<NegBackward0>) tensor(11035.9727, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11035.970703125
tensor(11035.9717, grad_fn=<NegBackward0>) tensor(11035.9707, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11035.9716796875
tensor(11035.9707, grad_fn=<NegBackward0>) tensor(11035.9717, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11035.970703125
tensor(11035.9707, grad_fn=<NegBackward0>) tensor(11035.9707, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11035.970703125
tensor(11035.9707, grad_fn=<NegBackward0>) tensor(11035.9707, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11035.970703125
tensor(11035.9707, grad_fn=<NegBackward0>) tensor(11035.9707, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11035.9716796875
tensor(11035.9707, grad_fn=<NegBackward0>) tensor(11035.9717, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11035.9716796875
tensor(11035.9707, grad_fn=<NegBackward0>) tensor(11035.9717, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11035.9716796875
tensor(11035.9707, grad_fn=<NegBackward0>) tensor(11035.9717, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11035.970703125
tensor(11035.9707, grad_fn=<NegBackward0>) tensor(11035.9707, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11035.970703125
tensor(11035.9707, grad_fn=<NegBackward0>) tensor(11035.9707, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11035.970703125
tensor(11035.9707, grad_fn=<NegBackward0>) tensor(11035.9707, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11035.970703125
tensor(11035.9707, grad_fn=<NegBackward0>) tensor(11035.9707, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11035.970703125
tensor(11035.9707, grad_fn=<NegBackward0>) tensor(11035.9707, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11035.970703125
tensor(11035.9707, grad_fn=<NegBackward0>) tensor(11035.9707, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11035.970703125
tensor(11035.9707, grad_fn=<NegBackward0>) tensor(11035.9707, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11035.9697265625
tensor(11035.9707, grad_fn=<NegBackward0>) tensor(11035.9697, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11035.9697265625
tensor(11035.9697, grad_fn=<NegBackward0>) tensor(11035.9697, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11035.9697265625
tensor(11035.9697, grad_fn=<NegBackward0>) tensor(11035.9697, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11035.970703125
tensor(11035.9697, grad_fn=<NegBackward0>) tensor(11035.9707, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11035.970703125
tensor(11035.9697, grad_fn=<NegBackward0>) tensor(11035.9707, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11035.96875
tensor(11035.9697, grad_fn=<NegBackward0>) tensor(11035.9688, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11035.9765625
tensor(11035.9688, grad_fn=<NegBackward0>) tensor(11035.9766, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11035.96875
tensor(11035.9688, grad_fn=<NegBackward0>) tensor(11035.9688, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11035.970703125
tensor(11035.9688, grad_fn=<NegBackward0>) tensor(11035.9707, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11035.970703125
tensor(11035.9688, grad_fn=<NegBackward0>) tensor(11035.9707, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11035.970703125
tensor(11035.9688, grad_fn=<NegBackward0>) tensor(11035.9707, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -11036.125
tensor(11035.9688, grad_fn=<NegBackward0>) tensor(11036.1250, grad_fn=<NegBackward0>)
4
Iteration 9600: Loss = -11035.970703125
tensor(11035.9688, grad_fn=<NegBackward0>) tensor(11035.9707, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9600 due to no improvement.
pi: tensor([[8.3399e-05, 9.9992e-01],
        [4.4974e-02, 9.5503e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5379, 0.4621], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2710, 0.1141],
         [0.5803, 0.1613]],

        [[0.5042, 0.1543],
         [0.6214, 0.6981]],

        [[0.7011, 0.2242],
         [0.5417, 0.5893]],

        [[0.6326, 0.1964],
         [0.6307, 0.5212]],

        [[0.5179, 0.1292],
         [0.6601, 0.5017]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448509923071951
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.014262894825058084
Average Adjusted Rand Index: 0.16798812697577042
[0.014262894825058084, 0.014262894825058084] [0.16798812697577042, 0.16798812697577042] [11035.9697265625, 11035.970703125]
-------------------------------------
This iteration is 24
True Objective function: Loss = -11141.456217630155
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21466.232421875
inf tensor(21466.2324, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11222.478515625
tensor(21466.2324, grad_fn=<NegBackward0>) tensor(11222.4785, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11222.05078125
tensor(11222.4785, grad_fn=<NegBackward0>) tensor(11222.0508, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11221.853515625
tensor(11222.0508, grad_fn=<NegBackward0>) tensor(11221.8535, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11221.6201171875
tensor(11221.8535, grad_fn=<NegBackward0>) tensor(11221.6201, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11221.34765625
tensor(11221.6201, grad_fn=<NegBackward0>) tensor(11221.3477, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11220.791015625
tensor(11221.3477, grad_fn=<NegBackward0>) tensor(11220.7910, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11220.2734375
tensor(11220.7910, grad_fn=<NegBackward0>) tensor(11220.2734, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11220.1318359375
tensor(11220.2734, grad_fn=<NegBackward0>) tensor(11220.1318, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11220.056640625
tensor(11220.1318, grad_fn=<NegBackward0>) tensor(11220.0566, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11220.0068359375
tensor(11220.0566, grad_fn=<NegBackward0>) tensor(11220.0068, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11219.966796875
tensor(11220.0068, grad_fn=<NegBackward0>) tensor(11219.9668, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11219.931640625
tensor(11219.9668, grad_fn=<NegBackward0>) tensor(11219.9316, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11219.8974609375
tensor(11219.9316, grad_fn=<NegBackward0>) tensor(11219.8975, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11219.8701171875
tensor(11219.8975, grad_fn=<NegBackward0>) tensor(11219.8701, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11219.8505859375
tensor(11219.8701, grad_fn=<NegBackward0>) tensor(11219.8506, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11219.837890625
tensor(11219.8506, grad_fn=<NegBackward0>) tensor(11219.8379, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11219.828125
tensor(11219.8379, grad_fn=<NegBackward0>) tensor(11219.8281, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11219.822265625
tensor(11219.8281, grad_fn=<NegBackward0>) tensor(11219.8223, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11219.81640625
tensor(11219.8223, grad_fn=<NegBackward0>) tensor(11219.8164, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11219.8115234375
tensor(11219.8164, grad_fn=<NegBackward0>) tensor(11219.8115, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11219.806640625
tensor(11219.8115, grad_fn=<NegBackward0>) tensor(11219.8066, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11219.802734375
tensor(11219.8066, grad_fn=<NegBackward0>) tensor(11219.8027, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11219.7998046875
tensor(11219.8027, grad_fn=<NegBackward0>) tensor(11219.7998, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11219.7939453125
tensor(11219.7998, grad_fn=<NegBackward0>) tensor(11219.7939, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11219.7919921875
tensor(11219.7939, grad_fn=<NegBackward0>) tensor(11219.7920, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11219.7880859375
tensor(11219.7920, grad_fn=<NegBackward0>) tensor(11219.7881, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11219.7841796875
tensor(11219.7881, grad_fn=<NegBackward0>) tensor(11219.7842, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11219.7822265625
tensor(11219.7842, grad_fn=<NegBackward0>) tensor(11219.7822, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11219.779296875
tensor(11219.7822, grad_fn=<NegBackward0>) tensor(11219.7793, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11219.7783203125
tensor(11219.7793, grad_fn=<NegBackward0>) tensor(11219.7783, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11219.7724609375
tensor(11219.7783, grad_fn=<NegBackward0>) tensor(11219.7725, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11219.7724609375
tensor(11219.7725, grad_fn=<NegBackward0>) tensor(11219.7725, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11219.7705078125
tensor(11219.7725, grad_fn=<NegBackward0>) tensor(11219.7705, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11219.7685546875
tensor(11219.7705, grad_fn=<NegBackward0>) tensor(11219.7686, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11219.7646484375
tensor(11219.7686, grad_fn=<NegBackward0>) tensor(11219.7646, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11219.763671875
tensor(11219.7646, grad_fn=<NegBackward0>) tensor(11219.7637, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11219.7626953125
tensor(11219.7637, grad_fn=<NegBackward0>) tensor(11219.7627, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11219.7607421875
tensor(11219.7627, grad_fn=<NegBackward0>) tensor(11219.7607, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11219.7607421875
tensor(11219.7607, grad_fn=<NegBackward0>) tensor(11219.7607, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11219.7587890625
tensor(11219.7607, grad_fn=<NegBackward0>) tensor(11219.7588, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11219.7587890625
tensor(11219.7588, grad_fn=<NegBackward0>) tensor(11219.7588, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11219.7578125
tensor(11219.7588, grad_fn=<NegBackward0>) tensor(11219.7578, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11219.7568359375
tensor(11219.7578, grad_fn=<NegBackward0>) tensor(11219.7568, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11219.755859375
tensor(11219.7568, grad_fn=<NegBackward0>) tensor(11219.7559, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11219.7568359375
tensor(11219.7559, grad_fn=<NegBackward0>) tensor(11219.7568, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11219.7548828125
tensor(11219.7559, grad_fn=<NegBackward0>) tensor(11219.7549, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11219.7548828125
tensor(11219.7549, grad_fn=<NegBackward0>) tensor(11219.7549, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11219.7529296875
tensor(11219.7549, grad_fn=<NegBackward0>) tensor(11219.7529, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11219.75390625
tensor(11219.7529, grad_fn=<NegBackward0>) tensor(11219.7539, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11219.751953125
tensor(11219.7529, grad_fn=<NegBackward0>) tensor(11219.7520, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11219.75390625
tensor(11219.7520, grad_fn=<NegBackward0>) tensor(11219.7539, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11219.751953125
tensor(11219.7520, grad_fn=<NegBackward0>) tensor(11219.7520, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11219.75390625
tensor(11219.7520, grad_fn=<NegBackward0>) tensor(11219.7539, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11219.75390625
tensor(11219.7520, grad_fn=<NegBackward0>) tensor(11219.7539, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11219.7529296875
tensor(11219.7520, grad_fn=<NegBackward0>) tensor(11219.7529, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -11219.7529296875
tensor(11219.7520, grad_fn=<NegBackward0>) tensor(11219.7529, grad_fn=<NegBackward0>)
4
Iteration 5700: Loss = -11219.751953125
tensor(11219.7520, grad_fn=<NegBackward0>) tensor(11219.7520, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11219.7509765625
tensor(11219.7520, grad_fn=<NegBackward0>) tensor(11219.7510, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11219.7529296875
tensor(11219.7510, grad_fn=<NegBackward0>) tensor(11219.7529, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11219.7509765625
tensor(11219.7510, grad_fn=<NegBackward0>) tensor(11219.7510, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11219.7509765625
tensor(11219.7510, grad_fn=<NegBackward0>) tensor(11219.7510, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11219.751953125
tensor(11219.7510, grad_fn=<NegBackward0>) tensor(11219.7520, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11219.7509765625
tensor(11219.7510, grad_fn=<NegBackward0>) tensor(11219.7510, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11219.7509765625
tensor(11219.7510, grad_fn=<NegBackward0>) tensor(11219.7510, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11219.7509765625
tensor(11219.7510, grad_fn=<NegBackward0>) tensor(11219.7510, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11219.7509765625
tensor(11219.7510, grad_fn=<NegBackward0>) tensor(11219.7510, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11219.75
tensor(11219.7510, grad_fn=<NegBackward0>) tensor(11219.7500, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11219.75
tensor(11219.7500, grad_fn=<NegBackward0>) tensor(11219.7500, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11219.75
tensor(11219.7500, grad_fn=<NegBackward0>) tensor(11219.7500, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11219.7490234375
tensor(11219.7500, grad_fn=<NegBackward0>) tensor(11219.7490, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11219.75
tensor(11219.7490, grad_fn=<NegBackward0>) tensor(11219.7500, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11219.7509765625
tensor(11219.7490, grad_fn=<NegBackward0>) tensor(11219.7510, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11219.75
tensor(11219.7490, grad_fn=<NegBackward0>) tensor(11219.7500, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11219.755859375
tensor(11219.7490, grad_fn=<NegBackward0>) tensor(11219.7559, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -11219.75390625
tensor(11219.7490, grad_fn=<NegBackward0>) tensor(11219.7539, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7500 due to no improvement.
pi: tensor([[9.0359e-04, 9.9910e-01],
        [2.5476e-02, 9.7452e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0058, 0.9942], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2788, 0.2453],
         [0.6294, 0.1659]],

        [[0.6726, 0.1576],
         [0.6195, 0.6125]],

        [[0.5608, 0.2669],
         [0.6436, 0.6222]],

        [[0.5306, 0.2427],
         [0.6921, 0.6199]],

        [[0.7152, 0.2096],
         [0.5685, 0.7009]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: -0.005513710431743218
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -1.1256141347984021e-05
Average Adjusted Rand Index: -0.0007692965357627092
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21184.20703125
inf tensor(21184.2070, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11222.34765625
tensor(21184.2070, grad_fn=<NegBackward0>) tensor(11222.3477, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11221.9384765625
tensor(11222.3477, grad_fn=<NegBackward0>) tensor(11221.9385, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11221.615234375
tensor(11221.9385, grad_fn=<NegBackward0>) tensor(11221.6152, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11220.921875
tensor(11221.6152, grad_fn=<NegBackward0>) tensor(11220.9219, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11220.296875
tensor(11220.9219, grad_fn=<NegBackward0>) tensor(11220.2969, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11220.0888671875
tensor(11220.2969, grad_fn=<NegBackward0>) tensor(11220.0889, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11220.0107421875
tensor(11220.0889, grad_fn=<NegBackward0>) tensor(11220.0107, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11219.9697265625
tensor(11220.0107, grad_fn=<NegBackward0>) tensor(11219.9697, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11219.9443359375
tensor(11219.9697, grad_fn=<NegBackward0>) tensor(11219.9443, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11219.9248046875
tensor(11219.9443, grad_fn=<NegBackward0>) tensor(11219.9248, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11219.9091796875
tensor(11219.9248, grad_fn=<NegBackward0>) tensor(11219.9092, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11219.8974609375
tensor(11219.9092, grad_fn=<NegBackward0>) tensor(11219.8975, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11219.8876953125
tensor(11219.8975, grad_fn=<NegBackward0>) tensor(11219.8877, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11219.87890625
tensor(11219.8877, grad_fn=<NegBackward0>) tensor(11219.8789, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11219.873046875
tensor(11219.8789, grad_fn=<NegBackward0>) tensor(11219.8730, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11219.865234375
tensor(11219.8730, grad_fn=<NegBackward0>) tensor(11219.8652, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11219.8583984375
tensor(11219.8652, grad_fn=<NegBackward0>) tensor(11219.8584, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11219.853515625
tensor(11219.8584, grad_fn=<NegBackward0>) tensor(11219.8535, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11219.84765625
tensor(11219.8535, grad_fn=<NegBackward0>) tensor(11219.8477, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11219.8427734375
tensor(11219.8477, grad_fn=<NegBackward0>) tensor(11219.8428, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11219.837890625
tensor(11219.8428, grad_fn=<NegBackward0>) tensor(11219.8379, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11219.8330078125
tensor(11219.8379, grad_fn=<NegBackward0>) tensor(11219.8330, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11219.830078125
tensor(11219.8330, grad_fn=<NegBackward0>) tensor(11219.8301, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11219.8251953125
tensor(11219.8301, grad_fn=<NegBackward0>) tensor(11219.8252, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11219.822265625
tensor(11219.8252, grad_fn=<NegBackward0>) tensor(11219.8223, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11219.818359375
tensor(11219.8223, grad_fn=<NegBackward0>) tensor(11219.8184, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11219.81640625
tensor(11219.8184, grad_fn=<NegBackward0>) tensor(11219.8164, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11219.8134765625
tensor(11219.8164, grad_fn=<NegBackward0>) tensor(11219.8135, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11219.8095703125
tensor(11219.8135, grad_fn=<NegBackward0>) tensor(11219.8096, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11219.8076171875
tensor(11219.8096, grad_fn=<NegBackward0>) tensor(11219.8076, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11219.8056640625
tensor(11219.8076, grad_fn=<NegBackward0>) tensor(11219.8057, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11219.8046875
tensor(11219.8057, grad_fn=<NegBackward0>) tensor(11219.8047, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11219.802734375
tensor(11219.8047, grad_fn=<NegBackward0>) tensor(11219.8027, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11219.7998046875
tensor(11219.8027, grad_fn=<NegBackward0>) tensor(11219.7998, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11219.798828125
tensor(11219.7998, grad_fn=<NegBackward0>) tensor(11219.7988, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11219.7978515625
tensor(11219.7988, grad_fn=<NegBackward0>) tensor(11219.7979, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11219.7958984375
tensor(11219.7979, grad_fn=<NegBackward0>) tensor(11219.7959, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11219.794921875
tensor(11219.7959, grad_fn=<NegBackward0>) tensor(11219.7949, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11219.7958984375
tensor(11219.7949, grad_fn=<NegBackward0>) tensor(11219.7959, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11219.7939453125
tensor(11219.7949, grad_fn=<NegBackward0>) tensor(11219.7939, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11219.7919921875
tensor(11219.7939, grad_fn=<NegBackward0>) tensor(11219.7920, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11219.7900390625
tensor(11219.7920, grad_fn=<NegBackward0>) tensor(11219.7900, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11219.7900390625
tensor(11219.7900, grad_fn=<NegBackward0>) tensor(11219.7900, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11219.7900390625
tensor(11219.7900, grad_fn=<NegBackward0>) tensor(11219.7900, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11219.7890625
tensor(11219.7900, grad_fn=<NegBackward0>) tensor(11219.7891, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11219.7880859375
tensor(11219.7891, grad_fn=<NegBackward0>) tensor(11219.7881, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11219.787109375
tensor(11219.7881, grad_fn=<NegBackward0>) tensor(11219.7871, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11219.787109375
tensor(11219.7871, grad_fn=<NegBackward0>) tensor(11219.7871, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11219.787109375
tensor(11219.7871, grad_fn=<NegBackward0>) tensor(11219.7871, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11219.78515625
tensor(11219.7871, grad_fn=<NegBackward0>) tensor(11219.7852, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11219.78515625
tensor(11219.7852, grad_fn=<NegBackward0>) tensor(11219.7852, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11219.78515625
tensor(11219.7852, grad_fn=<NegBackward0>) tensor(11219.7852, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11219.7841796875
tensor(11219.7852, grad_fn=<NegBackward0>) tensor(11219.7842, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11219.7822265625
tensor(11219.7842, grad_fn=<NegBackward0>) tensor(11219.7822, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11219.783203125
tensor(11219.7822, grad_fn=<NegBackward0>) tensor(11219.7832, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11219.783203125
tensor(11219.7822, grad_fn=<NegBackward0>) tensor(11219.7832, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11219.7822265625
tensor(11219.7822, grad_fn=<NegBackward0>) tensor(11219.7822, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11219.7802734375
tensor(11219.7822, grad_fn=<NegBackward0>) tensor(11219.7803, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11219.78125
tensor(11219.7803, grad_fn=<NegBackward0>) tensor(11219.7812, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11219.78125
tensor(11219.7803, grad_fn=<NegBackward0>) tensor(11219.7812, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11219.7802734375
tensor(11219.7803, grad_fn=<NegBackward0>) tensor(11219.7803, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11219.779296875
tensor(11219.7803, grad_fn=<NegBackward0>) tensor(11219.7793, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11219.779296875
tensor(11219.7793, grad_fn=<NegBackward0>) tensor(11219.7793, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11219.78125
tensor(11219.7793, grad_fn=<NegBackward0>) tensor(11219.7812, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11219.7802734375
tensor(11219.7793, grad_fn=<NegBackward0>) tensor(11219.7803, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11219.7802734375
tensor(11219.7793, grad_fn=<NegBackward0>) tensor(11219.7803, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11219.7802734375
tensor(11219.7793, grad_fn=<NegBackward0>) tensor(11219.7803, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -11219.779296875
tensor(11219.7793, grad_fn=<NegBackward0>) tensor(11219.7793, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11219.779296875
tensor(11219.7793, grad_fn=<NegBackward0>) tensor(11219.7793, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11219.7802734375
tensor(11219.7793, grad_fn=<NegBackward0>) tensor(11219.7803, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11219.78125
tensor(11219.7793, grad_fn=<NegBackward0>) tensor(11219.7812, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11219.7802734375
tensor(11219.7793, grad_fn=<NegBackward0>) tensor(11219.7803, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -11220.07421875
tensor(11219.7793, grad_fn=<NegBackward0>) tensor(11220.0742, grad_fn=<NegBackward0>)
4
Iteration 7400: Loss = -11219.7783203125
tensor(11219.7793, grad_fn=<NegBackward0>) tensor(11219.7783, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11219.779296875
tensor(11219.7783, grad_fn=<NegBackward0>) tensor(11219.7793, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11219.7802734375
tensor(11219.7783, grad_fn=<NegBackward0>) tensor(11219.7803, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11219.77734375
tensor(11219.7783, grad_fn=<NegBackward0>) tensor(11219.7773, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11219.77734375
tensor(11219.7773, grad_fn=<NegBackward0>) tensor(11219.7773, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11219.779296875
tensor(11219.7773, grad_fn=<NegBackward0>) tensor(11219.7793, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11219.77734375
tensor(11219.7773, grad_fn=<NegBackward0>) tensor(11219.7773, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11219.900390625
tensor(11219.7773, grad_fn=<NegBackward0>) tensor(11219.9004, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11219.77734375
tensor(11219.7773, grad_fn=<NegBackward0>) tensor(11219.7773, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11219.779296875
tensor(11219.7773, grad_fn=<NegBackward0>) tensor(11219.7793, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11219.7802734375
tensor(11219.7773, grad_fn=<NegBackward0>) tensor(11219.7803, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11219.77734375
tensor(11219.7773, grad_fn=<NegBackward0>) tensor(11219.7773, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11219.7763671875
tensor(11219.7773, grad_fn=<NegBackward0>) tensor(11219.7764, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11219.77734375
tensor(11219.7764, grad_fn=<NegBackward0>) tensor(11219.7773, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11219.7783203125
tensor(11219.7764, grad_fn=<NegBackward0>) tensor(11219.7783, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11219.7783203125
tensor(11219.7764, grad_fn=<NegBackward0>) tensor(11219.7783, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -11219.77734375
tensor(11219.7764, grad_fn=<NegBackward0>) tensor(11219.7773, grad_fn=<NegBackward0>)
4
Iteration 9100: Loss = -11219.77734375
tensor(11219.7764, grad_fn=<NegBackward0>) tensor(11219.7773, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9100 due to no improvement.
pi: tensor([[9.7648e-01, 2.3518e-02],
        [9.9932e-01, 6.8285e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9984, 0.0016], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1665, 0.1488],
         [0.6092, 0.2778]],

        [[0.6584, 0.1555],
         [0.6703, 0.5428]],

        [[0.5362, 0.2704],
         [0.6240, 0.5624]],

        [[0.7081, 0.2448],
         [0.6114, 0.5582]],

        [[0.5884, 0.2095],
         [0.6426, 0.7059]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: -0.005513710431743218
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -1.1256141347984021e-05
Average Adjusted Rand Index: -0.0007692965357627092
[-1.1256141347984021e-05, -1.1256141347984021e-05] [-0.0007692965357627092, -0.0007692965357627092] [11219.75390625, 11219.77734375]
-------------------------------------
This iteration is 25
True Objective function: Loss = -10997.229203605853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21885.373046875
inf tensor(21885.3730, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11090.7822265625
tensor(21885.3730, grad_fn=<NegBackward0>) tensor(11090.7822, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11090.0947265625
tensor(11090.7822, grad_fn=<NegBackward0>) tensor(11090.0947, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11089.9521484375
tensor(11090.0947, grad_fn=<NegBackward0>) tensor(11089.9521, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11089.892578125
tensor(11089.9521, grad_fn=<NegBackward0>) tensor(11089.8926, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11089.845703125
tensor(11089.8926, grad_fn=<NegBackward0>) tensor(11089.8457, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11089.80078125
tensor(11089.8457, grad_fn=<NegBackward0>) tensor(11089.8008, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11089.7548828125
tensor(11089.8008, grad_fn=<NegBackward0>) tensor(11089.7549, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11089.7109375
tensor(11089.7549, grad_fn=<NegBackward0>) tensor(11089.7109, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11089.66796875
tensor(11089.7109, grad_fn=<NegBackward0>) tensor(11089.6680, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11089.6279296875
tensor(11089.6680, grad_fn=<NegBackward0>) tensor(11089.6279, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11089.5869140625
tensor(11089.6279, grad_fn=<NegBackward0>) tensor(11089.5869, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11089.5458984375
tensor(11089.5869, grad_fn=<NegBackward0>) tensor(11089.5459, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11089.4970703125
tensor(11089.5459, grad_fn=<NegBackward0>) tensor(11089.4971, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11089.4462890625
tensor(11089.4971, grad_fn=<NegBackward0>) tensor(11089.4463, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11089.388671875
tensor(11089.4463, grad_fn=<NegBackward0>) tensor(11089.3887, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11089.322265625
tensor(11089.3887, grad_fn=<NegBackward0>) tensor(11089.3223, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11089.24609375
tensor(11089.3223, grad_fn=<NegBackward0>) tensor(11089.2461, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11089.1591796875
tensor(11089.2461, grad_fn=<NegBackward0>) tensor(11089.1592, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11089.0576171875
tensor(11089.1592, grad_fn=<NegBackward0>) tensor(11089.0576, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11088.9326171875
tensor(11089.0576, grad_fn=<NegBackward0>) tensor(11088.9326, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11088.76171875
tensor(11088.9326, grad_fn=<NegBackward0>) tensor(11088.7617, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11088.5146484375
tensor(11088.7617, grad_fn=<NegBackward0>) tensor(11088.5146, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11088.26953125
tensor(11088.5146, grad_fn=<NegBackward0>) tensor(11088.2695, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11088.095703125
tensor(11088.2695, grad_fn=<NegBackward0>) tensor(11088.0957, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11087.97265625
tensor(11088.0957, grad_fn=<NegBackward0>) tensor(11087.9727, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11087.8642578125
tensor(11087.9727, grad_fn=<NegBackward0>) tensor(11087.8643, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11087.7548828125
tensor(11087.8643, grad_fn=<NegBackward0>) tensor(11087.7549, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11087.6201171875
tensor(11087.7549, grad_fn=<NegBackward0>) tensor(11087.6201, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11087.40234375
tensor(11087.6201, grad_fn=<NegBackward0>) tensor(11087.4023, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11061.4970703125
tensor(11087.4023, grad_fn=<NegBackward0>) tensor(11061.4971, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10954.125
tensor(11061.4971, grad_fn=<NegBackward0>) tensor(10954.1250, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10953.9365234375
tensor(10954.1250, grad_fn=<NegBackward0>) tensor(10953.9365, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10953.8330078125
tensor(10953.9365, grad_fn=<NegBackward0>) tensor(10953.8330, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10953.73828125
tensor(10953.8330, grad_fn=<NegBackward0>) tensor(10953.7383, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10953.634765625
tensor(10953.7383, grad_fn=<NegBackward0>) tensor(10953.6348, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10953.6181640625
tensor(10953.6348, grad_fn=<NegBackward0>) tensor(10953.6182, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10953.603515625
tensor(10953.6182, grad_fn=<NegBackward0>) tensor(10953.6035, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10953.5908203125
tensor(10953.6035, grad_fn=<NegBackward0>) tensor(10953.5908, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10953.5732421875
tensor(10953.5908, grad_fn=<NegBackward0>) tensor(10953.5732, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10953.560546875
tensor(10953.5732, grad_fn=<NegBackward0>) tensor(10953.5605, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10953.5576171875
tensor(10953.5605, grad_fn=<NegBackward0>) tensor(10953.5576, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10953.5546875
tensor(10953.5576, grad_fn=<NegBackward0>) tensor(10953.5547, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10953.5517578125
tensor(10953.5547, grad_fn=<NegBackward0>) tensor(10953.5518, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10953.5498046875
tensor(10953.5518, grad_fn=<NegBackward0>) tensor(10953.5498, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10953.5498046875
tensor(10953.5498, grad_fn=<NegBackward0>) tensor(10953.5498, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10953.5478515625
tensor(10953.5498, grad_fn=<NegBackward0>) tensor(10953.5479, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10953.546875
tensor(10953.5479, grad_fn=<NegBackward0>) tensor(10953.5469, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10953.5458984375
tensor(10953.5469, grad_fn=<NegBackward0>) tensor(10953.5459, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10953.5439453125
tensor(10953.5459, grad_fn=<NegBackward0>) tensor(10953.5439, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10953.5146484375
tensor(10953.5439, grad_fn=<NegBackward0>) tensor(10953.5146, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10953.5078125
tensor(10953.5146, grad_fn=<NegBackward0>) tensor(10953.5078, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10953.51953125
tensor(10953.5078, grad_fn=<NegBackward0>) tensor(10953.5195, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10953.5087890625
tensor(10953.5078, grad_fn=<NegBackward0>) tensor(10953.5088, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -10953.5107421875
tensor(10953.5078, grad_fn=<NegBackward0>) tensor(10953.5107, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -10953.50390625
tensor(10953.5078, grad_fn=<NegBackward0>) tensor(10953.5039, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10953.5048828125
tensor(10953.5039, grad_fn=<NegBackward0>) tensor(10953.5049, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10953.5029296875
tensor(10953.5039, grad_fn=<NegBackward0>) tensor(10953.5029, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10953.5078125
tensor(10953.5029, grad_fn=<NegBackward0>) tensor(10953.5078, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10953.50390625
tensor(10953.5029, grad_fn=<NegBackward0>) tensor(10953.5039, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -10953.5029296875
tensor(10953.5029, grad_fn=<NegBackward0>) tensor(10953.5029, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10953.501953125
tensor(10953.5029, grad_fn=<NegBackward0>) tensor(10953.5020, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10953.501953125
tensor(10953.5020, grad_fn=<NegBackward0>) tensor(10953.5020, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10953.501953125
tensor(10953.5020, grad_fn=<NegBackward0>) tensor(10953.5020, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10953.501953125
tensor(10953.5020, grad_fn=<NegBackward0>) tensor(10953.5020, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10953.501953125
tensor(10953.5020, grad_fn=<NegBackward0>) tensor(10953.5020, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10953.501953125
tensor(10953.5020, grad_fn=<NegBackward0>) tensor(10953.5020, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10953.5
tensor(10953.5020, grad_fn=<NegBackward0>) tensor(10953.5000, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10953.5
tensor(10953.5000, grad_fn=<NegBackward0>) tensor(10953.5000, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10953.5029296875
tensor(10953.5000, grad_fn=<NegBackward0>) tensor(10953.5029, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10953.501953125
tensor(10953.5000, grad_fn=<NegBackward0>) tensor(10953.5020, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -10953.5029296875
tensor(10953.5000, grad_fn=<NegBackward0>) tensor(10953.5029, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -10953.5009765625
tensor(10953.5000, grad_fn=<NegBackward0>) tensor(10953.5010, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -10953.5
tensor(10953.5000, grad_fn=<NegBackward0>) tensor(10953.5000, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10953.5
tensor(10953.5000, grad_fn=<NegBackward0>) tensor(10953.5000, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10953.5
tensor(10953.5000, grad_fn=<NegBackward0>) tensor(10953.5000, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10953.5
tensor(10953.5000, grad_fn=<NegBackward0>) tensor(10953.5000, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10953.5
tensor(10953.5000, grad_fn=<NegBackward0>) tensor(10953.5000, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10953.5
tensor(10953.5000, grad_fn=<NegBackward0>) tensor(10953.5000, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10953.5
tensor(10953.5000, grad_fn=<NegBackward0>) tensor(10953.5000, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10953.4990234375
tensor(10953.5000, grad_fn=<NegBackward0>) tensor(10953.4990, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10953.5048828125
tensor(10953.4990, grad_fn=<NegBackward0>) tensor(10953.5049, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10953.5087890625
tensor(10953.4990, grad_fn=<NegBackward0>) tensor(10953.5088, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -10953.5
tensor(10953.4990, grad_fn=<NegBackward0>) tensor(10953.5000, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -10953.5
tensor(10953.4990, grad_fn=<NegBackward0>) tensor(10953.5000, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -10953.4990234375
tensor(10953.4990, grad_fn=<NegBackward0>) tensor(10953.4990, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10953.5
tensor(10953.4990, grad_fn=<NegBackward0>) tensor(10953.5000, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10953.4990234375
tensor(10953.4990, grad_fn=<NegBackward0>) tensor(10953.4990, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10953.498046875
tensor(10953.4990, grad_fn=<NegBackward0>) tensor(10953.4980, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10953.5
tensor(10953.4980, grad_fn=<NegBackward0>) tensor(10953.5000, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -10953.4990234375
tensor(10953.4980, grad_fn=<NegBackward0>) tensor(10953.4990, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -10953.498046875
tensor(10953.4980, grad_fn=<NegBackward0>) tensor(10953.4980, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10953.498046875
tensor(10953.4980, grad_fn=<NegBackward0>) tensor(10953.4980, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10953.4990234375
tensor(10953.4980, grad_fn=<NegBackward0>) tensor(10953.4990, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -10953.505859375
tensor(10953.4980, grad_fn=<NegBackward0>) tensor(10953.5059, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -10953.4990234375
tensor(10953.4980, grad_fn=<NegBackward0>) tensor(10953.4990, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -10953.5244140625
tensor(10953.4980, grad_fn=<NegBackward0>) tensor(10953.5244, grad_fn=<NegBackward0>)
4
Iteration 9700: Loss = -10953.498046875
tensor(10953.4980, grad_fn=<NegBackward0>) tensor(10953.4980, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10953.498046875
tensor(10953.4980, grad_fn=<NegBackward0>) tensor(10953.4980, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10953.498046875
tensor(10953.4980, grad_fn=<NegBackward0>) tensor(10953.4980, grad_fn=<NegBackward0>)
pi: tensor([[0.7417, 0.2583],
        [0.2239, 0.7761]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5391, 0.4609], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2056, 0.1019],
         [0.6622, 0.2504]],

        [[0.6345, 0.0971],
         [0.6483, 0.6085]],

        [[0.7194, 0.1028],
         [0.5567, 0.7254]],

        [[0.5111, 0.1019],
         [0.7114, 0.6303]],

        [[0.6698, 0.1054],
         [0.6932, 0.7200]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208047711084835
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448373142749671
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080650161849864
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080808080808081
time is 4
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
Global Adjusted Rand Index: 0.8460916938420658
Average Adjusted Rand Index: 0.8453243608078628
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24210.486328125
inf tensor(24210.4863, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11090.4833984375
tensor(24210.4863, grad_fn=<NegBackward0>) tensor(11090.4834, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11089.9189453125
tensor(11090.4834, grad_fn=<NegBackward0>) tensor(11089.9189, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11089.734375
tensor(11089.9189, grad_fn=<NegBackward0>) tensor(11089.7344, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11089.5888671875
tensor(11089.7344, grad_fn=<NegBackward0>) tensor(11089.5889, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11089.5068359375
tensor(11089.5889, grad_fn=<NegBackward0>) tensor(11089.5068, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11089.4169921875
tensor(11089.5068, grad_fn=<NegBackward0>) tensor(11089.4170, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11089.3115234375
tensor(11089.4170, grad_fn=<NegBackward0>) tensor(11089.3115, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11089.1982421875
tensor(11089.3115, grad_fn=<NegBackward0>) tensor(11089.1982, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11089.08984375
tensor(11089.1982, grad_fn=<NegBackward0>) tensor(11089.0898, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11088.990234375
tensor(11089.0898, grad_fn=<NegBackward0>) tensor(11088.9902, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11088.8994140625
tensor(11088.9902, grad_fn=<NegBackward0>) tensor(11088.8994, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11088.818359375
tensor(11088.8994, grad_fn=<NegBackward0>) tensor(11088.8184, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11088.75390625
tensor(11088.8184, grad_fn=<NegBackward0>) tensor(11088.7539, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11088.703125
tensor(11088.7539, grad_fn=<NegBackward0>) tensor(11088.7031, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11088.6650390625
tensor(11088.7031, grad_fn=<NegBackward0>) tensor(11088.6650, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11088.6416015625
tensor(11088.6650, grad_fn=<NegBackward0>) tensor(11088.6416, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11088.6220703125
tensor(11088.6416, grad_fn=<NegBackward0>) tensor(11088.6221, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11088.6103515625
tensor(11088.6221, grad_fn=<NegBackward0>) tensor(11088.6104, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11088.5986328125
tensor(11088.6104, grad_fn=<NegBackward0>) tensor(11088.5986, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11088.5908203125
tensor(11088.5986, grad_fn=<NegBackward0>) tensor(11088.5908, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11088.58203125
tensor(11088.5908, grad_fn=<NegBackward0>) tensor(11088.5820, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11088.57421875
tensor(11088.5820, grad_fn=<NegBackward0>) tensor(11088.5742, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11088.56640625
tensor(11088.5742, grad_fn=<NegBackward0>) tensor(11088.5664, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11088.5576171875
tensor(11088.5664, grad_fn=<NegBackward0>) tensor(11088.5576, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11088.5478515625
tensor(11088.5576, grad_fn=<NegBackward0>) tensor(11088.5479, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11088.5400390625
tensor(11088.5479, grad_fn=<NegBackward0>) tensor(11088.5400, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11088.53515625
tensor(11088.5400, grad_fn=<NegBackward0>) tensor(11088.5352, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11088.5322265625
tensor(11088.5352, grad_fn=<NegBackward0>) tensor(11088.5322, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11088.52734375
tensor(11088.5322, grad_fn=<NegBackward0>) tensor(11088.5273, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11088.5263671875
tensor(11088.5273, grad_fn=<NegBackward0>) tensor(11088.5264, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11088.5263671875
tensor(11088.5264, grad_fn=<NegBackward0>) tensor(11088.5264, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11088.5244140625
tensor(11088.5264, grad_fn=<NegBackward0>) tensor(11088.5244, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11088.5234375
tensor(11088.5244, grad_fn=<NegBackward0>) tensor(11088.5234, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11088.521484375
tensor(11088.5234, grad_fn=<NegBackward0>) tensor(11088.5215, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11088.521484375
tensor(11088.5215, grad_fn=<NegBackward0>) tensor(11088.5215, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11088.51953125
tensor(11088.5215, grad_fn=<NegBackward0>) tensor(11088.5195, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11088.5185546875
tensor(11088.5195, grad_fn=<NegBackward0>) tensor(11088.5186, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11088.51953125
tensor(11088.5186, grad_fn=<NegBackward0>) tensor(11088.5195, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11088.51953125
tensor(11088.5186, grad_fn=<NegBackward0>) tensor(11088.5195, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -11088.517578125
tensor(11088.5186, grad_fn=<NegBackward0>) tensor(11088.5176, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11088.517578125
tensor(11088.5176, grad_fn=<NegBackward0>) tensor(11088.5176, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11088.5185546875
tensor(11088.5176, grad_fn=<NegBackward0>) tensor(11088.5186, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11088.5166015625
tensor(11088.5176, grad_fn=<NegBackward0>) tensor(11088.5166, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11088.5166015625
tensor(11088.5166, grad_fn=<NegBackward0>) tensor(11088.5166, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11088.517578125
tensor(11088.5166, grad_fn=<NegBackward0>) tensor(11088.5176, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11088.5166015625
tensor(11088.5166, grad_fn=<NegBackward0>) tensor(11088.5166, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11088.5166015625
tensor(11088.5166, grad_fn=<NegBackward0>) tensor(11088.5166, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11088.515625
tensor(11088.5166, grad_fn=<NegBackward0>) tensor(11088.5156, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11088.515625
tensor(11088.5156, grad_fn=<NegBackward0>) tensor(11088.5156, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11088.5166015625
tensor(11088.5156, grad_fn=<NegBackward0>) tensor(11088.5166, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11088.513671875
tensor(11088.5156, grad_fn=<NegBackward0>) tensor(11088.5137, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11088.5166015625
tensor(11088.5137, grad_fn=<NegBackward0>) tensor(11088.5166, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11088.515625
tensor(11088.5137, grad_fn=<NegBackward0>) tensor(11088.5156, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -11088.5166015625
tensor(11088.5137, grad_fn=<NegBackward0>) tensor(11088.5166, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -11088.515625
tensor(11088.5137, grad_fn=<NegBackward0>) tensor(11088.5156, grad_fn=<NegBackward0>)
4
Iteration 5600: Loss = -11088.513671875
tensor(11088.5137, grad_fn=<NegBackward0>) tensor(11088.5137, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11088.515625
tensor(11088.5137, grad_fn=<NegBackward0>) tensor(11088.5156, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11088.515625
tensor(11088.5137, grad_fn=<NegBackward0>) tensor(11088.5156, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11088.5146484375
tensor(11088.5137, grad_fn=<NegBackward0>) tensor(11088.5146, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -11088.5146484375
tensor(11088.5137, grad_fn=<NegBackward0>) tensor(11088.5146, grad_fn=<NegBackward0>)
4
Iteration 6100: Loss = -11088.5146484375
tensor(11088.5137, grad_fn=<NegBackward0>) tensor(11088.5146, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6100 due to no improvement.
pi: tensor([[0.0079, 0.9921],
        [0.0235, 0.9765]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9648, 0.0352], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1692, 0.1682],
         [0.5267, 0.1638]],

        [[0.6192, 0.0546],
         [0.5079, 0.6445]],

        [[0.5582, 0.1402],
         [0.5912, 0.7113]],

        [[0.6907, 0.2279],
         [0.5057, 0.7246]],

        [[0.6335, 0.1949],
         [0.6056, 0.5075]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0002741709093146428
Average Adjusted Rand Index: 0.00033344555058593435
[0.8460916938420658, -0.0002741709093146428] [0.8453243608078628, 0.00033344555058593435] [10953.498046875, 11088.5146484375]
-------------------------------------
This iteration is 26
True Objective function: Loss = -10816.294530674195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21686.423828125
inf tensor(21686.4238, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10988.7861328125
tensor(21686.4238, grad_fn=<NegBackward0>) tensor(10988.7861, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10982.078125
tensor(10988.7861, grad_fn=<NegBackward0>) tensor(10982.0781, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10977.4140625
tensor(10982.0781, grad_fn=<NegBackward0>) tensor(10977.4141, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10974.80859375
tensor(10977.4141, grad_fn=<NegBackward0>) tensor(10974.8086, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10973.1005859375
tensor(10974.8086, grad_fn=<NegBackward0>) tensor(10973.1006, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10970.2802734375
tensor(10973.1006, grad_fn=<NegBackward0>) tensor(10970.2803, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10949.076171875
tensor(10970.2803, grad_fn=<NegBackward0>) tensor(10949.0762, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10873.87890625
tensor(10949.0762, grad_fn=<NegBackward0>) tensor(10873.8789, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10845.314453125
tensor(10873.8789, grad_fn=<NegBackward0>) tensor(10845.3145, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10843.853515625
tensor(10845.3145, grad_fn=<NegBackward0>) tensor(10843.8535, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10837.7080078125
tensor(10843.8535, grad_fn=<NegBackward0>) tensor(10837.7080, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10837.58203125
tensor(10837.7080, grad_fn=<NegBackward0>) tensor(10837.5820, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10837.501953125
tensor(10837.5820, grad_fn=<NegBackward0>) tensor(10837.5020, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10837.3427734375
tensor(10837.5020, grad_fn=<NegBackward0>) tensor(10837.3428, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10837.138671875
tensor(10837.3428, grad_fn=<NegBackward0>) tensor(10837.1387, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10837.0693359375
tensor(10837.1387, grad_fn=<NegBackward0>) tensor(10837.0693, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10833.505859375
tensor(10837.0693, grad_fn=<NegBackward0>) tensor(10833.5059, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10833.48046875
tensor(10833.5059, grad_fn=<NegBackward0>) tensor(10833.4805, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10833.4619140625
tensor(10833.4805, grad_fn=<NegBackward0>) tensor(10833.4619, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10833.4228515625
tensor(10833.4619, grad_fn=<NegBackward0>) tensor(10833.4229, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10832.9248046875
tensor(10833.4229, grad_fn=<NegBackward0>) tensor(10832.9248, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10832.9091796875
tensor(10832.9248, grad_fn=<NegBackward0>) tensor(10832.9092, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10832.896484375
tensor(10832.9092, grad_fn=<NegBackward0>) tensor(10832.8965, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10832.8896484375
tensor(10832.8965, grad_fn=<NegBackward0>) tensor(10832.8896, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10832.8818359375
tensor(10832.8896, grad_fn=<NegBackward0>) tensor(10832.8818, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10832.865234375
tensor(10832.8818, grad_fn=<NegBackward0>) tensor(10832.8652, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10832.8330078125
tensor(10832.8652, grad_fn=<NegBackward0>) tensor(10832.8330, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10832.8291015625
tensor(10832.8330, grad_fn=<NegBackward0>) tensor(10832.8291, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10832.8251953125
tensor(10832.8291, grad_fn=<NegBackward0>) tensor(10832.8252, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10832.759765625
tensor(10832.8252, grad_fn=<NegBackward0>) tensor(10832.7598, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10832.7568359375
tensor(10832.7598, grad_fn=<NegBackward0>) tensor(10832.7568, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10832.7529296875
tensor(10832.7568, grad_fn=<NegBackward0>) tensor(10832.7529, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10831.998046875
tensor(10832.7529, grad_fn=<NegBackward0>) tensor(10831.9980, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10831.2578125
tensor(10831.9980, grad_fn=<NegBackward0>) tensor(10831.2578, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10831.25390625
tensor(10831.2578, grad_fn=<NegBackward0>) tensor(10831.2539, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10831.251953125
tensor(10831.2539, grad_fn=<NegBackward0>) tensor(10831.2520, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10831.25
tensor(10831.2520, grad_fn=<NegBackward0>) tensor(10831.2500, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10831.2470703125
tensor(10831.2500, grad_fn=<NegBackward0>) tensor(10831.2471, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10831.2421875
tensor(10831.2471, grad_fn=<NegBackward0>) tensor(10831.2422, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10831.1884765625
tensor(10831.2422, grad_fn=<NegBackward0>) tensor(10831.1885, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10831.18359375
tensor(10831.1885, grad_fn=<NegBackward0>) tensor(10831.1836, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10831.1826171875
tensor(10831.1836, grad_fn=<NegBackward0>) tensor(10831.1826, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10831.181640625
tensor(10831.1826, grad_fn=<NegBackward0>) tensor(10831.1816, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10831.181640625
tensor(10831.1816, grad_fn=<NegBackward0>) tensor(10831.1816, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10831.1806640625
tensor(10831.1816, grad_fn=<NegBackward0>) tensor(10831.1807, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10831.1796875
tensor(10831.1807, grad_fn=<NegBackward0>) tensor(10831.1797, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10831.1787109375
tensor(10831.1797, grad_fn=<NegBackward0>) tensor(10831.1787, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10831.1767578125
tensor(10831.1787, grad_fn=<NegBackward0>) tensor(10831.1768, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10831.177734375
tensor(10831.1768, grad_fn=<NegBackward0>) tensor(10831.1777, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10831.1748046875
tensor(10831.1768, grad_fn=<NegBackward0>) tensor(10831.1748, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10831.171875
tensor(10831.1748, grad_fn=<NegBackward0>) tensor(10831.1719, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10831.1708984375
tensor(10831.1719, grad_fn=<NegBackward0>) tensor(10831.1709, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10831.17578125
tensor(10831.1709, grad_fn=<NegBackward0>) tensor(10831.1758, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10831.166015625
tensor(10831.1709, grad_fn=<NegBackward0>) tensor(10831.1660, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10831.140625
tensor(10831.1660, grad_fn=<NegBackward0>) tensor(10831.1406, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10830.783203125
tensor(10831.1406, grad_fn=<NegBackward0>) tensor(10830.7832, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10830.375
tensor(10830.7832, grad_fn=<NegBackward0>) tensor(10830.3750, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10830.296875
tensor(10830.3750, grad_fn=<NegBackward0>) tensor(10830.2969, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10830.2822265625
tensor(10830.2969, grad_fn=<NegBackward0>) tensor(10830.2822, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10830.1708984375
tensor(10830.2822, grad_fn=<NegBackward0>) tensor(10830.1709, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10830.1103515625
tensor(10830.1709, grad_fn=<NegBackward0>) tensor(10830.1104, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10830.1044921875
tensor(10830.1104, grad_fn=<NegBackward0>) tensor(10830.1045, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10830.1005859375
tensor(10830.1045, grad_fn=<NegBackward0>) tensor(10830.1006, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10830.09765625
tensor(10830.1006, grad_fn=<NegBackward0>) tensor(10830.0977, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10829.98828125
tensor(10830.0977, grad_fn=<NegBackward0>) tensor(10829.9883, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10829.90625
tensor(10829.9883, grad_fn=<NegBackward0>) tensor(10829.9062, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10829.90234375
tensor(10829.9062, grad_fn=<NegBackward0>) tensor(10829.9023, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10829.357421875
tensor(10829.9023, grad_fn=<NegBackward0>) tensor(10829.3574, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10829.2822265625
tensor(10829.3574, grad_fn=<NegBackward0>) tensor(10829.2822, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10829.259765625
tensor(10829.2822, grad_fn=<NegBackward0>) tensor(10829.2598, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10829.1962890625
tensor(10829.2598, grad_fn=<NegBackward0>) tensor(10829.1963, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10829.1552734375
tensor(10829.1963, grad_fn=<NegBackward0>) tensor(10829.1553, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10828.8505859375
tensor(10829.1553, grad_fn=<NegBackward0>) tensor(10828.8506, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10828.8046875
tensor(10828.8506, grad_fn=<NegBackward0>) tensor(10828.8047, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10828.78125
tensor(10828.8047, grad_fn=<NegBackward0>) tensor(10828.7812, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10826.0771484375
tensor(10828.7812, grad_fn=<NegBackward0>) tensor(10826.0771, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10825.927734375
tensor(10826.0771, grad_fn=<NegBackward0>) tensor(10825.9277, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10825.9189453125
tensor(10825.9277, grad_fn=<NegBackward0>) tensor(10825.9189, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10825.9296875
tensor(10825.9189, grad_fn=<NegBackward0>) tensor(10825.9297, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10825.9169921875
tensor(10825.9189, grad_fn=<NegBackward0>) tensor(10825.9170, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10825.916015625
tensor(10825.9170, grad_fn=<NegBackward0>) tensor(10825.9160, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10824.2841796875
tensor(10825.9160, grad_fn=<NegBackward0>) tensor(10824.2842, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10824.2021484375
tensor(10824.2842, grad_fn=<NegBackward0>) tensor(10824.2021, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10824.0859375
tensor(10824.2021, grad_fn=<NegBackward0>) tensor(10824.0859, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10824.0849609375
tensor(10824.0859, grad_fn=<NegBackward0>) tensor(10824.0850, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10824.0732421875
tensor(10824.0850, grad_fn=<NegBackward0>) tensor(10824.0732, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10824.0712890625
tensor(10824.0732, grad_fn=<NegBackward0>) tensor(10824.0713, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10824.0703125
tensor(10824.0713, grad_fn=<NegBackward0>) tensor(10824.0703, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10823.5712890625
tensor(10824.0703, grad_fn=<NegBackward0>) tensor(10823.5713, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10823.044921875
tensor(10823.5713, grad_fn=<NegBackward0>) tensor(10823.0449, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10823.0478515625
tensor(10823.0449, grad_fn=<NegBackward0>) tensor(10823.0479, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10812.49609375
tensor(10823.0449, grad_fn=<NegBackward0>) tensor(10812.4961, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10795.109375
tensor(10812.4961, grad_fn=<NegBackward0>) tensor(10795.1094, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10787.158203125
tensor(10795.1094, grad_fn=<NegBackward0>) tensor(10787.1582, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10787.1494140625
tensor(10787.1582, grad_fn=<NegBackward0>) tensor(10787.1494, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10781.3037109375
tensor(10787.1494, grad_fn=<NegBackward0>) tensor(10781.3037, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10781.3125
tensor(10781.3037, grad_fn=<NegBackward0>) tensor(10781.3125, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -10781.2587890625
tensor(10781.3037, grad_fn=<NegBackward0>) tensor(10781.2588, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10781.2578125
tensor(10781.2588, grad_fn=<NegBackward0>) tensor(10781.2578, grad_fn=<NegBackward0>)
pi: tensor([[0.7133, 0.2867],
        [0.2288, 0.7712]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5116, 0.4884], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1934, 0.0878],
         [0.6040, 0.2582]],

        [[0.5440, 0.0946],
         [0.5709, 0.6090]],

        [[0.5319, 0.0848],
         [0.5958, 0.6504]],

        [[0.5152, 0.0956],
         [0.5408, 0.5397]],

        [[0.5183, 0.1048],
         [0.7048, 0.5791]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 89
Adjusted Rand Index: 0.6044118965160035
time is 1
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208047711084835
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8823435719624108
time is 4
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
Global Adjusted Rand Index: 0.8534786936372262
Average Adjusted Rand Index: 0.8579945928865838
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23583.2265625
inf tensor(23583.2266, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10951.3017578125
tensor(23583.2266, grad_fn=<NegBackward0>) tensor(10951.3018, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10926.1689453125
tensor(10951.3018, grad_fn=<NegBackward0>) tensor(10926.1689, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10806.5673828125
tensor(10926.1689, grad_fn=<NegBackward0>) tensor(10806.5674, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10795.75
tensor(10806.5674, grad_fn=<NegBackward0>) tensor(10795.7500, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10795.505859375
tensor(10795.7500, grad_fn=<NegBackward0>) tensor(10795.5059, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10795.3095703125
tensor(10795.5059, grad_fn=<NegBackward0>) tensor(10795.3096, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10795.224609375
tensor(10795.3096, grad_fn=<NegBackward0>) tensor(10795.2246, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10795.208984375
tensor(10795.2246, grad_fn=<NegBackward0>) tensor(10795.2090, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10794.923828125
tensor(10795.2090, grad_fn=<NegBackward0>) tensor(10794.9238, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10794.349609375
tensor(10794.9238, grad_fn=<NegBackward0>) tensor(10794.3496, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10794.3427734375
tensor(10794.3496, grad_fn=<NegBackward0>) tensor(10794.3428, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10794.333984375
tensor(10794.3428, grad_fn=<NegBackward0>) tensor(10794.3340, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10794.3232421875
tensor(10794.3340, grad_fn=<NegBackward0>) tensor(10794.3232, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10794.302734375
tensor(10794.3232, grad_fn=<NegBackward0>) tensor(10794.3027, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10794.2998046875
tensor(10794.3027, grad_fn=<NegBackward0>) tensor(10794.2998, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10794.298828125
tensor(10794.2998, grad_fn=<NegBackward0>) tensor(10794.2988, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10794.296875
tensor(10794.2988, grad_fn=<NegBackward0>) tensor(10794.2969, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10794.294921875
tensor(10794.2969, grad_fn=<NegBackward0>) tensor(10794.2949, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10794.29296875
tensor(10794.2949, grad_fn=<NegBackward0>) tensor(10794.2930, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10794.2626953125
tensor(10794.2930, grad_fn=<NegBackward0>) tensor(10794.2627, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10794.26171875
tensor(10794.2627, grad_fn=<NegBackward0>) tensor(10794.2617, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10794.2607421875
tensor(10794.2617, grad_fn=<NegBackward0>) tensor(10794.2607, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10794.26171875
tensor(10794.2607, grad_fn=<NegBackward0>) tensor(10794.2617, grad_fn=<NegBackward0>)
1
Iteration 2400: Loss = -10794.2607421875
tensor(10794.2607, grad_fn=<NegBackward0>) tensor(10794.2607, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10794.2587890625
tensor(10794.2607, grad_fn=<NegBackward0>) tensor(10794.2588, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10794.2568359375
tensor(10794.2588, grad_fn=<NegBackward0>) tensor(10794.2568, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10794.255859375
tensor(10794.2568, grad_fn=<NegBackward0>) tensor(10794.2559, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10794.255859375
tensor(10794.2559, grad_fn=<NegBackward0>) tensor(10794.2559, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10794.2548828125
tensor(10794.2559, grad_fn=<NegBackward0>) tensor(10794.2549, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10794.2626953125
tensor(10794.2549, grad_fn=<NegBackward0>) tensor(10794.2627, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -10794.2529296875
tensor(10794.2549, grad_fn=<NegBackward0>) tensor(10794.2529, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10794.25390625
tensor(10794.2529, grad_fn=<NegBackward0>) tensor(10794.2539, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -10794.25390625
tensor(10794.2529, grad_fn=<NegBackward0>) tensor(10794.2539, grad_fn=<NegBackward0>)
2
Iteration 3400: Loss = -10794.25390625
tensor(10794.2529, grad_fn=<NegBackward0>) tensor(10794.2539, grad_fn=<NegBackward0>)
3
Iteration 3500: Loss = -10794.26171875
tensor(10794.2529, grad_fn=<NegBackward0>) tensor(10794.2617, grad_fn=<NegBackward0>)
4
Iteration 3600: Loss = -10794.251953125
tensor(10794.2529, grad_fn=<NegBackward0>) tensor(10794.2520, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10794.251953125
tensor(10794.2520, grad_fn=<NegBackward0>) tensor(10794.2520, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10794.251953125
tensor(10794.2520, grad_fn=<NegBackward0>) tensor(10794.2520, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10794.26171875
tensor(10794.2520, grad_fn=<NegBackward0>) tensor(10794.2617, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -10794.251953125
tensor(10794.2520, grad_fn=<NegBackward0>) tensor(10794.2520, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10794.25
tensor(10794.2520, grad_fn=<NegBackward0>) tensor(10794.2500, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10794.2587890625
tensor(10794.2500, grad_fn=<NegBackward0>) tensor(10794.2588, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10794.251953125
tensor(10794.2500, grad_fn=<NegBackward0>) tensor(10794.2520, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -10794.25
tensor(10794.2500, grad_fn=<NegBackward0>) tensor(10794.2500, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10794.251953125
tensor(10794.2500, grad_fn=<NegBackward0>) tensor(10794.2520, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10794.25
tensor(10794.2500, grad_fn=<NegBackward0>) tensor(10794.2500, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10794.2509765625
tensor(10794.2500, grad_fn=<NegBackward0>) tensor(10794.2510, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10794.2490234375
tensor(10794.2500, grad_fn=<NegBackward0>) tensor(10794.2490, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10794.25
tensor(10794.2490, grad_fn=<NegBackward0>) tensor(10794.2500, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10794.25
tensor(10794.2490, grad_fn=<NegBackward0>) tensor(10794.2500, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -10794.2490234375
tensor(10794.2490, grad_fn=<NegBackward0>) tensor(10794.2490, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10794.248046875
tensor(10794.2490, grad_fn=<NegBackward0>) tensor(10794.2480, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10794.24609375
tensor(10794.2480, grad_fn=<NegBackward0>) tensor(10794.2461, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10794.2470703125
tensor(10794.2461, grad_fn=<NegBackward0>) tensor(10794.2471, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10794.24609375
tensor(10794.2461, grad_fn=<NegBackward0>) tensor(10794.2461, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10794.24609375
tensor(10794.2461, grad_fn=<NegBackward0>) tensor(10794.2461, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10794.24609375
tensor(10794.2461, grad_fn=<NegBackward0>) tensor(10794.2461, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10794.24609375
tensor(10794.2461, grad_fn=<NegBackward0>) tensor(10794.2461, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10794.2470703125
tensor(10794.2461, grad_fn=<NegBackward0>) tensor(10794.2471, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10794.24609375
tensor(10794.2461, grad_fn=<NegBackward0>) tensor(10794.2461, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10794.244140625
tensor(10794.2461, grad_fn=<NegBackward0>) tensor(10794.2441, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10794.244140625
tensor(10794.2441, grad_fn=<NegBackward0>) tensor(10794.2441, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10794.244140625
tensor(10794.2441, grad_fn=<NegBackward0>) tensor(10794.2441, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10794.2451171875
tensor(10794.2441, grad_fn=<NegBackward0>) tensor(10794.2451, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10794.2431640625
tensor(10794.2441, grad_fn=<NegBackward0>) tensor(10794.2432, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10794.2626953125
tensor(10794.2432, grad_fn=<NegBackward0>) tensor(10794.2627, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10794.2431640625
tensor(10794.2432, grad_fn=<NegBackward0>) tensor(10794.2432, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10794.244140625
tensor(10794.2432, grad_fn=<NegBackward0>) tensor(10794.2441, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10794.244140625
tensor(10794.2432, grad_fn=<NegBackward0>) tensor(10794.2441, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -10794.2421875
tensor(10794.2432, grad_fn=<NegBackward0>) tensor(10794.2422, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10794.2431640625
tensor(10794.2422, grad_fn=<NegBackward0>) tensor(10794.2432, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10794.2421875
tensor(10794.2422, grad_fn=<NegBackward0>) tensor(10794.2422, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10794.2431640625
tensor(10794.2422, grad_fn=<NegBackward0>) tensor(10794.2432, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -10794.2421875
tensor(10794.2422, grad_fn=<NegBackward0>) tensor(10794.2422, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10794.2412109375
tensor(10794.2422, grad_fn=<NegBackward0>) tensor(10794.2412, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10794.240234375
tensor(10794.2412, grad_fn=<NegBackward0>) tensor(10794.2402, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10794.240234375
tensor(10794.2402, grad_fn=<NegBackward0>) tensor(10794.2402, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10794.2392578125
tensor(10794.2402, grad_fn=<NegBackward0>) tensor(10794.2393, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10794.240234375
tensor(10794.2393, grad_fn=<NegBackward0>) tensor(10794.2402, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10794.2412109375
tensor(10794.2393, grad_fn=<NegBackward0>) tensor(10794.2412, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -10794.2392578125
tensor(10794.2393, grad_fn=<NegBackward0>) tensor(10794.2393, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10794.2421875
tensor(10794.2393, grad_fn=<NegBackward0>) tensor(10794.2422, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10794.2392578125
tensor(10794.2393, grad_fn=<NegBackward0>) tensor(10794.2393, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10794.2392578125
tensor(10794.2393, grad_fn=<NegBackward0>) tensor(10794.2393, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10794.2392578125
tensor(10794.2393, grad_fn=<NegBackward0>) tensor(10794.2393, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10794.23828125
tensor(10794.2393, grad_fn=<NegBackward0>) tensor(10794.2383, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10794.23828125
tensor(10794.2383, grad_fn=<NegBackward0>) tensor(10794.2383, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10794.2373046875
tensor(10794.2383, grad_fn=<NegBackward0>) tensor(10794.2373, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10794.240234375
tensor(10794.2373, grad_fn=<NegBackward0>) tensor(10794.2402, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -10794.23828125
tensor(10794.2373, grad_fn=<NegBackward0>) tensor(10794.2383, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -10794.23828125
tensor(10794.2373, grad_fn=<NegBackward0>) tensor(10794.2383, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -10794.2392578125
tensor(10794.2373, grad_fn=<NegBackward0>) tensor(10794.2393, grad_fn=<NegBackward0>)
4
Iteration 9300: Loss = -10794.2421875
tensor(10794.2373, grad_fn=<NegBackward0>) tensor(10794.2422, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[0.2726, 0.7274],
        [0.7722, 0.2278]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4515, 0.5485], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2233, 0.0827],
         [0.5701, 0.2398]],

        [[0.5259, 0.0922],
         [0.6055, 0.6937]],

        [[0.5250, 0.0847],
         [0.5634, 0.6386]],

        [[0.6166, 0.0939],
         [0.6366, 0.7074]],

        [[0.6876, 0.1031],
         [0.6573, 0.6367]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 92
Adjusted Rand Index: 0.7026262626262626
time is 1
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 6
Adjusted Rand Index: 0.7721545392564556
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599529290626264
time is 3
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8822858823962049
time is 4
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824165642894751
Global Adjusted Rand Index: 0.0334218810537257
Average Adjusted Rand Index: 0.839887235526205
[0.8534786936372262, 0.0334218810537257] [0.8579945928865838, 0.839887235526205] [10781.25390625, 10794.2421875]
-------------------------------------
This iteration is 27
True Objective function: Loss = -10863.73057012116
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23014.67578125
inf tensor(23014.6758, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11003.833984375
tensor(23014.6758, grad_fn=<NegBackward0>) tensor(11003.8340, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11002.78125
tensor(11003.8340, grad_fn=<NegBackward0>) tensor(11002.7812, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11002.09765625
tensor(11002.7812, grad_fn=<NegBackward0>) tensor(11002.0977, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11001.6689453125
tensor(11002.0977, grad_fn=<NegBackward0>) tensor(11001.6689, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11001.22265625
tensor(11001.6689, grad_fn=<NegBackward0>) tensor(11001.2227, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11000.9404296875
tensor(11001.2227, grad_fn=<NegBackward0>) tensor(11000.9404, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11000.7861328125
tensor(11000.9404, grad_fn=<NegBackward0>) tensor(11000.7861, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11000.626953125
tensor(11000.7861, grad_fn=<NegBackward0>) tensor(11000.6270, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11000.361328125
tensor(11000.6270, grad_fn=<NegBackward0>) tensor(11000.3613, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10999.91015625
tensor(11000.3613, grad_fn=<NegBackward0>) tensor(10999.9102, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10997.66015625
tensor(10999.9102, grad_fn=<NegBackward0>) tensor(10997.6602, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10945.900390625
tensor(10997.6602, grad_fn=<NegBackward0>) tensor(10945.9004, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10834.1376953125
tensor(10945.9004, grad_fn=<NegBackward0>) tensor(10834.1377, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10833.330078125
tensor(10834.1377, grad_fn=<NegBackward0>) tensor(10833.3301, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10832.5751953125
tensor(10833.3301, grad_fn=<NegBackward0>) tensor(10832.5752, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10831.890625
tensor(10832.5752, grad_fn=<NegBackward0>) tensor(10831.8906, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10831.83203125
tensor(10831.8906, grad_fn=<NegBackward0>) tensor(10831.8320, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10831.6298828125
tensor(10831.8320, grad_fn=<NegBackward0>) tensor(10831.6299, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10831.5693359375
tensor(10831.6299, grad_fn=<NegBackward0>) tensor(10831.5693, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10831.5517578125
tensor(10831.5693, grad_fn=<NegBackward0>) tensor(10831.5518, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10831.5361328125
tensor(10831.5518, grad_fn=<NegBackward0>) tensor(10831.5361, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10831.525390625
tensor(10831.5361, grad_fn=<NegBackward0>) tensor(10831.5254, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10831.5087890625
tensor(10831.5254, grad_fn=<NegBackward0>) tensor(10831.5088, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10831.4912109375
tensor(10831.5088, grad_fn=<NegBackward0>) tensor(10831.4912, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10831.439453125
tensor(10831.4912, grad_fn=<NegBackward0>) tensor(10831.4395, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10831.42578125
tensor(10831.4395, grad_fn=<NegBackward0>) tensor(10831.4258, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10831.4111328125
tensor(10831.4258, grad_fn=<NegBackward0>) tensor(10831.4111, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10831.4072265625
tensor(10831.4111, grad_fn=<NegBackward0>) tensor(10831.4072, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10831.40625
tensor(10831.4072, grad_fn=<NegBackward0>) tensor(10831.4062, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10831.4033203125
tensor(10831.4062, grad_fn=<NegBackward0>) tensor(10831.4033, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10831.3984375
tensor(10831.4033, grad_fn=<NegBackward0>) tensor(10831.3984, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10831.3779296875
tensor(10831.3984, grad_fn=<NegBackward0>) tensor(10831.3779, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10831.3701171875
tensor(10831.3779, grad_fn=<NegBackward0>) tensor(10831.3701, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10831.369140625
tensor(10831.3701, grad_fn=<NegBackward0>) tensor(10831.3691, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10831.369140625
tensor(10831.3691, grad_fn=<NegBackward0>) tensor(10831.3691, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10831.3662109375
tensor(10831.3691, grad_fn=<NegBackward0>) tensor(10831.3662, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10831.37890625
tensor(10831.3662, grad_fn=<NegBackward0>) tensor(10831.3789, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -10831.36328125
tensor(10831.3662, grad_fn=<NegBackward0>) tensor(10831.3633, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10831.3603515625
tensor(10831.3633, grad_fn=<NegBackward0>) tensor(10831.3604, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10831.3544921875
tensor(10831.3604, grad_fn=<NegBackward0>) tensor(10831.3545, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10831.3515625
tensor(10831.3545, grad_fn=<NegBackward0>) tensor(10831.3516, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10831.3515625
tensor(10831.3516, grad_fn=<NegBackward0>) tensor(10831.3516, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10831.349609375
tensor(10831.3516, grad_fn=<NegBackward0>) tensor(10831.3496, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10831.3486328125
tensor(10831.3496, grad_fn=<NegBackward0>) tensor(10831.3486, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10831.34765625
tensor(10831.3486, grad_fn=<NegBackward0>) tensor(10831.3477, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10831.34765625
tensor(10831.3477, grad_fn=<NegBackward0>) tensor(10831.3477, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10831.3466796875
tensor(10831.3477, grad_fn=<NegBackward0>) tensor(10831.3467, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10831.34765625
tensor(10831.3467, grad_fn=<NegBackward0>) tensor(10831.3477, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10831.3408203125
tensor(10831.3467, grad_fn=<NegBackward0>) tensor(10831.3408, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10831.294921875
tensor(10831.3408, grad_fn=<NegBackward0>) tensor(10831.2949, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10831.2958984375
tensor(10831.2949, grad_fn=<NegBackward0>) tensor(10831.2959, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10831.294921875
tensor(10831.2949, grad_fn=<NegBackward0>) tensor(10831.2949, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10831.2958984375
tensor(10831.2949, grad_fn=<NegBackward0>) tensor(10831.2959, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10831.2958984375
tensor(10831.2949, grad_fn=<NegBackward0>) tensor(10831.2959, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -10831.29296875
tensor(10831.2949, grad_fn=<NegBackward0>) tensor(10831.2930, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10831.2919921875
tensor(10831.2930, grad_fn=<NegBackward0>) tensor(10831.2920, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10831.2919921875
tensor(10831.2920, grad_fn=<NegBackward0>) tensor(10831.2920, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10831.2919921875
tensor(10831.2920, grad_fn=<NegBackward0>) tensor(10831.2920, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10831.2978515625
tensor(10831.2920, grad_fn=<NegBackward0>) tensor(10831.2979, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10831.291015625
tensor(10831.2920, grad_fn=<NegBackward0>) tensor(10831.2910, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10831.2900390625
tensor(10831.2910, grad_fn=<NegBackward0>) tensor(10831.2900, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10831.3017578125
tensor(10831.2900, grad_fn=<NegBackward0>) tensor(10831.3018, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10831.2900390625
tensor(10831.2900, grad_fn=<NegBackward0>) tensor(10831.2900, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10831.2919921875
tensor(10831.2900, grad_fn=<NegBackward0>) tensor(10831.2920, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10831.2900390625
tensor(10831.2900, grad_fn=<NegBackward0>) tensor(10831.2900, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10831.3203125
tensor(10831.2900, grad_fn=<NegBackward0>) tensor(10831.3203, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10831.2900390625
tensor(10831.2900, grad_fn=<NegBackward0>) tensor(10831.2900, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10831.28515625
tensor(10831.2900, grad_fn=<NegBackward0>) tensor(10831.2852, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10831.28515625
tensor(10831.2852, grad_fn=<NegBackward0>) tensor(10831.2852, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10831.287109375
tensor(10831.2852, grad_fn=<NegBackward0>) tensor(10831.2871, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10831.28515625
tensor(10831.2852, grad_fn=<NegBackward0>) tensor(10831.2852, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10831.2841796875
tensor(10831.2852, grad_fn=<NegBackward0>) tensor(10831.2842, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10831.28515625
tensor(10831.2842, grad_fn=<NegBackward0>) tensor(10831.2852, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -10831.2900390625
tensor(10831.2842, grad_fn=<NegBackward0>) tensor(10831.2900, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -10831.28515625
tensor(10831.2842, grad_fn=<NegBackward0>) tensor(10831.2852, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -10831.28515625
tensor(10831.2842, grad_fn=<NegBackward0>) tensor(10831.2852, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -10831.2841796875
tensor(10831.2842, grad_fn=<NegBackward0>) tensor(10831.2842, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10831.283203125
tensor(10831.2842, grad_fn=<NegBackward0>) tensor(10831.2832, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10831.2822265625
tensor(10831.2832, grad_fn=<NegBackward0>) tensor(10831.2822, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10831.279296875
tensor(10831.2822, grad_fn=<NegBackward0>) tensor(10831.2793, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10831.2802734375
tensor(10831.2793, grad_fn=<NegBackward0>) tensor(10831.2803, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10831.28125
tensor(10831.2793, grad_fn=<NegBackward0>) tensor(10831.2812, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -10831.279296875
tensor(10831.2793, grad_fn=<NegBackward0>) tensor(10831.2793, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10831.2646484375
tensor(10831.2793, grad_fn=<NegBackward0>) tensor(10831.2646, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10831.30859375
tensor(10831.2646, grad_fn=<NegBackward0>) tensor(10831.3086, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -10831.2646484375
tensor(10831.2646, grad_fn=<NegBackward0>) tensor(10831.2646, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10831.26953125
tensor(10831.2646, grad_fn=<NegBackward0>) tensor(10831.2695, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -10831.263671875
tensor(10831.2646, grad_fn=<NegBackward0>) tensor(10831.2637, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10831.271484375
tensor(10831.2637, grad_fn=<NegBackward0>) tensor(10831.2715, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -10831.263671875
tensor(10831.2637, grad_fn=<NegBackward0>) tensor(10831.2637, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10831.2646484375
tensor(10831.2637, grad_fn=<NegBackward0>) tensor(10831.2646, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10831.265625
tensor(10831.2637, grad_fn=<NegBackward0>) tensor(10831.2656, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -10831.265625
tensor(10831.2637, grad_fn=<NegBackward0>) tensor(10831.2656, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -10831.263671875
tensor(10831.2637, grad_fn=<NegBackward0>) tensor(10831.2637, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10831.265625
tensor(10831.2637, grad_fn=<NegBackward0>) tensor(10831.2656, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -10831.265625
tensor(10831.2637, grad_fn=<NegBackward0>) tensor(10831.2656, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -10831.26953125
tensor(10831.2637, grad_fn=<NegBackward0>) tensor(10831.2695, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -10831.263671875
tensor(10831.2637, grad_fn=<NegBackward0>) tensor(10831.2637, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10831.36328125
tensor(10831.2637, grad_fn=<NegBackward0>) tensor(10831.3633, grad_fn=<NegBackward0>)
1
pi: tensor([[0.7977, 0.2023],
        [0.1911, 0.8089]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4292, 0.5708], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2053, 0.0973],
         [0.5432, 0.2441]],

        [[0.6909, 0.0935],
         [0.7086, 0.6080]],

        [[0.6879, 0.1054],
         [0.5070, 0.6702]],

        [[0.5065, 0.0939],
         [0.7150, 0.6225]],

        [[0.6037, 0.0976],
         [0.5183, 0.6556]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 94
Adjusted Rand Index: 0.7721212121212121
time is 3
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
Global Adjusted Rand Index: 0.8984962484450622
Average Adjusted Rand Index: 0.8990649654390547
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20432.322265625
inf tensor(20432.3223, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11003.9228515625
tensor(20432.3223, grad_fn=<NegBackward0>) tensor(11003.9229, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11003.451171875
tensor(11003.9229, grad_fn=<NegBackward0>) tensor(11003.4512, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11002.79296875
tensor(11003.4512, grad_fn=<NegBackward0>) tensor(11002.7930, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11001.52734375
tensor(11002.7930, grad_fn=<NegBackward0>) tensor(11001.5273, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11000.3994140625
tensor(11001.5273, grad_fn=<NegBackward0>) tensor(11000.3994, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10999.40625
tensor(11000.3994, grad_fn=<NegBackward0>) tensor(10999.4062, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10995.9423828125
tensor(10999.4062, grad_fn=<NegBackward0>) tensor(10995.9424, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10867.5966796875
tensor(10995.9424, grad_fn=<NegBackward0>) tensor(10867.5967, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10833.57421875
tensor(10867.5967, grad_fn=<NegBackward0>) tensor(10833.5742, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10832.0244140625
tensor(10833.5742, grad_fn=<NegBackward0>) tensor(10832.0244, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10831.908203125
tensor(10832.0244, grad_fn=<NegBackward0>) tensor(10831.9082, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10831.8486328125
tensor(10831.9082, grad_fn=<NegBackward0>) tensor(10831.8486, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10831.7861328125
tensor(10831.8486, grad_fn=<NegBackward0>) tensor(10831.7861, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10831.7255859375
tensor(10831.7861, grad_fn=<NegBackward0>) tensor(10831.7256, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10831.6923828125
tensor(10831.7256, grad_fn=<NegBackward0>) tensor(10831.6924, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10831.6748046875
tensor(10831.6924, grad_fn=<NegBackward0>) tensor(10831.6748, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10831.529296875
tensor(10831.6748, grad_fn=<NegBackward0>) tensor(10831.5293, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10831.4814453125
tensor(10831.5293, grad_fn=<NegBackward0>) tensor(10831.4814, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10831.4677734375
tensor(10831.4814, grad_fn=<NegBackward0>) tensor(10831.4678, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10831.4375
tensor(10831.4678, grad_fn=<NegBackward0>) tensor(10831.4375, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10831.4306640625
tensor(10831.4375, grad_fn=<NegBackward0>) tensor(10831.4307, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10831.423828125
tensor(10831.4307, grad_fn=<NegBackward0>) tensor(10831.4238, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10831.4150390625
tensor(10831.4238, grad_fn=<NegBackward0>) tensor(10831.4150, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10831.41015625
tensor(10831.4150, grad_fn=<NegBackward0>) tensor(10831.4102, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10831.40234375
tensor(10831.4102, grad_fn=<NegBackward0>) tensor(10831.4023, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10831.38671875
tensor(10831.4023, grad_fn=<NegBackward0>) tensor(10831.3867, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10831.3828125
tensor(10831.3867, grad_fn=<NegBackward0>) tensor(10831.3828, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10831.3798828125
tensor(10831.3828, grad_fn=<NegBackward0>) tensor(10831.3799, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10831.37890625
tensor(10831.3799, grad_fn=<NegBackward0>) tensor(10831.3789, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10831.376953125
tensor(10831.3789, grad_fn=<NegBackward0>) tensor(10831.3770, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10831.373046875
tensor(10831.3770, grad_fn=<NegBackward0>) tensor(10831.3730, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10831.361328125
tensor(10831.3730, grad_fn=<NegBackward0>) tensor(10831.3613, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10831.361328125
tensor(10831.3613, grad_fn=<NegBackward0>) tensor(10831.3613, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10831.361328125
tensor(10831.3613, grad_fn=<NegBackward0>) tensor(10831.3613, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10831.3603515625
tensor(10831.3613, grad_fn=<NegBackward0>) tensor(10831.3604, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10831.359375
tensor(10831.3604, grad_fn=<NegBackward0>) tensor(10831.3594, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10831.3642578125
tensor(10831.3594, grad_fn=<NegBackward0>) tensor(10831.3643, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -10831.359375
tensor(10831.3594, grad_fn=<NegBackward0>) tensor(10831.3594, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10831.357421875
tensor(10831.3594, grad_fn=<NegBackward0>) tensor(10831.3574, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10831.3544921875
tensor(10831.3574, grad_fn=<NegBackward0>) tensor(10831.3545, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10831.341796875
tensor(10831.3545, grad_fn=<NegBackward0>) tensor(10831.3418, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10831.3408203125
tensor(10831.3418, grad_fn=<NegBackward0>) tensor(10831.3408, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10831.341796875
tensor(10831.3408, grad_fn=<NegBackward0>) tensor(10831.3418, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -10831.33984375
tensor(10831.3408, grad_fn=<NegBackward0>) tensor(10831.3398, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10831.3369140625
tensor(10831.3398, grad_fn=<NegBackward0>) tensor(10831.3369, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10831.333984375
tensor(10831.3369, grad_fn=<NegBackward0>) tensor(10831.3340, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10831.3193359375
tensor(10831.3340, grad_fn=<NegBackward0>) tensor(10831.3193, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10831.3076171875
tensor(10831.3193, grad_fn=<NegBackward0>) tensor(10831.3076, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10831.306640625
tensor(10831.3076, grad_fn=<NegBackward0>) tensor(10831.3066, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10831.3046875
tensor(10831.3066, grad_fn=<NegBackward0>) tensor(10831.3047, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10831.3056640625
tensor(10831.3047, grad_fn=<NegBackward0>) tensor(10831.3057, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10831.3046875
tensor(10831.3047, grad_fn=<NegBackward0>) tensor(10831.3047, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10831.3037109375
tensor(10831.3047, grad_fn=<NegBackward0>) tensor(10831.3037, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10831.3037109375
tensor(10831.3037, grad_fn=<NegBackward0>) tensor(10831.3037, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10831.302734375
tensor(10831.3037, grad_fn=<NegBackward0>) tensor(10831.3027, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10831.302734375
tensor(10831.3027, grad_fn=<NegBackward0>) tensor(10831.3027, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10831.302734375
tensor(10831.3027, grad_fn=<NegBackward0>) tensor(10831.3027, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10831.3037109375
tensor(10831.3027, grad_fn=<NegBackward0>) tensor(10831.3037, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10831.3017578125
tensor(10831.3027, grad_fn=<NegBackward0>) tensor(10831.3018, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10831.3017578125
tensor(10831.3018, grad_fn=<NegBackward0>) tensor(10831.3018, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10831.3017578125
tensor(10831.3018, grad_fn=<NegBackward0>) tensor(10831.3018, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10831.3017578125
tensor(10831.3018, grad_fn=<NegBackward0>) tensor(10831.3018, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10831.30078125
tensor(10831.3018, grad_fn=<NegBackward0>) tensor(10831.3008, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10831.302734375
tensor(10831.3008, grad_fn=<NegBackward0>) tensor(10831.3027, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10831.3056640625
tensor(10831.3008, grad_fn=<NegBackward0>) tensor(10831.3057, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -10831.3017578125
tensor(10831.3008, grad_fn=<NegBackward0>) tensor(10831.3018, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -10831.310546875
tensor(10831.3008, grad_fn=<NegBackward0>) tensor(10831.3105, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -10831.30078125
tensor(10831.3008, grad_fn=<NegBackward0>) tensor(10831.3008, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10831.3017578125
tensor(10831.3008, grad_fn=<NegBackward0>) tensor(10831.3018, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10831.3017578125
tensor(10831.3008, grad_fn=<NegBackward0>) tensor(10831.3018, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -10831.3037109375
tensor(10831.3008, grad_fn=<NegBackward0>) tensor(10831.3037, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -10831.3056640625
tensor(10831.3008, grad_fn=<NegBackward0>) tensor(10831.3057, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -10831.3017578125
tensor(10831.3008, grad_fn=<NegBackward0>) tensor(10831.3018, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7300 due to no improvement.
pi: tensor([[0.8084, 0.1916],
        [0.2011, 0.7989]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5692, 0.4308], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2445, 0.0973],
         [0.6122, 0.2053]],

        [[0.5336, 0.0936],
         [0.5857, 0.7049]],

        [[0.6853, 0.1054],
         [0.5334, 0.5242]],

        [[0.5313, 0.0939],
         [0.6943, 0.5094]],

        [[0.5121, 0.0976],
         [0.5222, 0.6834]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 6
Adjusted Rand Index: 0.7721212121212121
time is 3
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
Global Adjusted Rand Index: 0.8984962484450622
Average Adjusted Rand Index: 0.8990649654390547
[0.8984962484450622, 0.8984962484450622] [0.8990649654390547, 0.8990649654390547] [10831.2646484375, 10831.3017578125]
-------------------------------------
This iteration is 28
True Objective function: Loss = -11008.779923284359
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23091.83203125
inf tensor(23091.8320, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11112.92578125
tensor(23091.8320, grad_fn=<NegBackward0>) tensor(11112.9258, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11108.9931640625
tensor(11112.9258, grad_fn=<NegBackward0>) tensor(11108.9932, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11108.3046875
tensor(11108.9932, grad_fn=<NegBackward0>) tensor(11108.3047, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11107.763671875
tensor(11108.3047, grad_fn=<NegBackward0>) tensor(11107.7637, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11107.1298828125
tensor(11107.7637, grad_fn=<NegBackward0>) tensor(11107.1299, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11106.5595703125
tensor(11107.1299, grad_fn=<NegBackward0>) tensor(11106.5596, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11106.0009765625
tensor(11106.5596, grad_fn=<NegBackward0>) tensor(11106.0010, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11105.060546875
tensor(11106.0010, grad_fn=<NegBackward0>) tensor(11105.0605, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11104.6923828125
tensor(11105.0605, grad_fn=<NegBackward0>) tensor(11104.6924, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11104.5673828125
tensor(11104.6924, grad_fn=<NegBackward0>) tensor(11104.5674, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11104.5048828125
tensor(11104.5674, grad_fn=<NegBackward0>) tensor(11104.5049, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11104.4697265625
tensor(11104.5049, grad_fn=<NegBackward0>) tensor(11104.4697, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11104.4423828125
tensor(11104.4697, grad_fn=<NegBackward0>) tensor(11104.4424, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11104.4189453125
tensor(11104.4424, grad_fn=<NegBackward0>) tensor(11104.4189, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11104.3857421875
tensor(11104.4189, grad_fn=<NegBackward0>) tensor(11104.3857, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11104.283203125
tensor(11104.3857, grad_fn=<NegBackward0>) tensor(11104.2832, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11103.380859375
tensor(11104.2832, grad_fn=<NegBackward0>) tensor(11103.3809, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11102.8056640625
tensor(11103.3809, grad_fn=<NegBackward0>) tensor(11102.8057, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11102.5888671875
tensor(11102.8057, grad_fn=<NegBackward0>) tensor(11102.5889, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11102.3876953125
tensor(11102.5889, grad_fn=<NegBackward0>) tensor(11102.3877, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11102.19140625
tensor(11102.3877, grad_fn=<NegBackward0>) tensor(11102.1914, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11102.0498046875
tensor(11102.1914, grad_fn=<NegBackward0>) tensor(11102.0498, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11101.9638671875
tensor(11102.0498, grad_fn=<NegBackward0>) tensor(11101.9639, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11101.91015625
tensor(11101.9639, grad_fn=<NegBackward0>) tensor(11101.9102, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11101.8759765625
tensor(11101.9102, grad_fn=<NegBackward0>) tensor(11101.8760, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11101.849609375
tensor(11101.8760, grad_fn=<NegBackward0>) tensor(11101.8496, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11101.830078125
tensor(11101.8496, grad_fn=<NegBackward0>) tensor(11101.8301, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11101.8154296875
tensor(11101.8301, grad_fn=<NegBackward0>) tensor(11101.8154, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11101.8037109375
tensor(11101.8154, grad_fn=<NegBackward0>) tensor(11101.8037, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11101.794921875
tensor(11101.8037, grad_fn=<NegBackward0>) tensor(11101.7949, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11101.787109375
tensor(11101.7949, grad_fn=<NegBackward0>) tensor(11101.7871, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11101.78125
tensor(11101.7871, grad_fn=<NegBackward0>) tensor(11101.7812, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11101.775390625
tensor(11101.7812, grad_fn=<NegBackward0>) tensor(11101.7754, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11101.76953125
tensor(11101.7754, grad_fn=<NegBackward0>) tensor(11101.7695, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11101.765625
tensor(11101.7695, grad_fn=<NegBackward0>) tensor(11101.7656, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11101.7626953125
tensor(11101.7656, grad_fn=<NegBackward0>) tensor(11101.7627, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11101.759765625
tensor(11101.7627, grad_fn=<NegBackward0>) tensor(11101.7598, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11101.7578125
tensor(11101.7598, grad_fn=<NegBackward0>) tensor(11101.7578, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11101.75390625
tensor(11101.7578, grad_fn=<NegBackward0>) tensor(11101.7539, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11101.7529296875
tensor(11101.7539, grad_fn=<NegBackward0>) tensor(11101.7529, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11101.7490234375
tensor(11101.7529, grad_fn=<NegBackward0>) tensor(11101.7490, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11101.75
tensor(11101.7490, grad_fn=<NegBackward0>) tensor(11101.7500, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11101.7451171875
tensor(11101.7490, grad_fn=<NegBackward0>) tensor(11101.7451, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11101.744140625
tensor(11101.7451, grad_fn=<NegBackward0>) tensor(11101.7441, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11101.7431640625
tensor(11101.7441, grad_fn=<NegBackward0>) tensor(11101.7432, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11101.7421875
tensor(11101.7432, grad_fn=<NegBackward0>) tensor(11101.7422, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11101.7412109375
tensor(11101.7422, grad_fn=<NegBackward0>) tensor(11101.7412, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11101.7412109375
tensor(11101.7412, grad_fn=<NegBackward0>) tensor(11101.7412, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11101.740234375
tensor(11101.7412, grad_fn=<NegBackward0>) tensor(11101.7402, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11101.7392578125
tensor(11101.7402, grad_fn=<NegBackward0>) tensor(11101.7393, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11101.736328125
tensor(11101.7393, grad_fn=<NegBackward0>) tensor(11101.7363, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11101.7373046875
tensor(11101.7363, grad_fn=<NegBackward0>) tensor(11101.7373, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11101.734375
tensor(11101.7363, grad_fn=<NegBackward0>) tensor(11101.7344, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11101.7353515625
tensor(11101.7344, grad_fn=<NegBackward0>) tensor(11101.7354, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11101.732421875
tensor(11101.7344, grad_fn=<NegBackward0>) tensor(11101.7324, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11101.734375
tensor(11101.7324, grad_fn=<NegBackward0>) tensor(11101.7344, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11101.7333984375
tensor(11101.7324, grad_fn=<NegBackward0>) tensor(11101.7334, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11101.7421875
tensor(11101.7324, grad_fn=<NegBackward0>) tensor(11101.7422, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -11101.73046875
tensor(11101.7324, grad_fn=<NegBackward0>) tensor(11101.7305, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11101.7314453125
tensor(11101.7305, grad_fn=<NegBackward0>) tensor(11101.7314, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11101.73046875
tensor(11101.7305, grad_fn=<NegBackward0>) tensor(11101.7305, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11101.73046875
tensor(11101.7305, grad_fn=<NegBackward0>) tensor(11101.7305, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11101.7314453125
tensor(11101.7305, grad_fn=<NegBackward0>) tensor(11101.7314, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11101.7294921875
tensor(11101.7305, grad_fn=<NegBackward0>) tensor(11101.7295, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11101.728515625
tensor(11101.7295, grad_fn=<NegBackward0>) tensor(11101.7285, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11101.7294921875
tensor(11101.7285, grad_fn=<NegBackward0>) tensor(11101.7295, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11101.73046875
tensor(11101.7285, grad_fn=<NegBackward0>) tensor(11101.7305, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11101.728515625
tensor(11101.7285, grad_fn=<NegBackward0>) tensor(11101.7285, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11101.728515625
tensor(11101.7285, grad_fn=<NegBackward0>) tensor(11101.7285, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11101.728515625
tensor(11101.7285, grad_fn=<NegBackward0>) tensor(11101.7285, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11101.7275390625
tensor(11101.7285, grad_fn=<NegBackward0>) tensor(11101.7275, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11101.734375
tensor(11101.7275, grad_fn=<NegBackward0>) tensor(11101.7344, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11101.7275390625
tensor(11101.7275, grad_fn=<NegBackward0>) tensor(11101.7275, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11101.7275390625
tensor(11101.7275, grad_fn=<NegBackward0>) tensor(11101.7275, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11101.7265625
tensor(11101.7275, grad_fn=<NegBackward0>) tensor(11101.7266, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11101.7275390625
tensor(11101.7266, grad_fn=<NegBackward0>) tensor(11101.7275, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11101.7265625
tensor(11101.7266, grad_fn=<NegBackward0>) tensor(11101.7266, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11101.7255859375
tensor(11101.7266, grad_fn=<NegBackward0>) tensor(11101.7256, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11101.7265625
tensor(11101.7256, grad_fn=<NegBackward0>) tensor(11101.7266, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11101.728515625
tensor(11101.7256, grad_fn=<NegBackward0>) tensor(11101.7285, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11101.7626953125
tensor(11101.7256, grad_fn=<NegBackward0>) tensor(11101.7627, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -11101.7255859375
tensor(11101.7256, grad_fn=<NegBackward0>) tensor(11101.7256, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11101.7265625
tensor(11101.7256, grad_fn=<NegBackward0>) tensor(11101.7266, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11101.7265625
tensor(11101.7256, grad_fn=<NegBackward0>) tensor(11101.7266, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11101.7255859375
tensor(11101.7256, grad_fn=<NegBackward0>) tensor(11101.7256, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11101.7265625
tensor(11101.7256, grad_fn=<NegBackward0>) tensor(11101.7266, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11101.724609375
tensor(11101.7256, grad_fn=<NegBackward0>) tensor(11101.7246, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11101.724609375
tensor(11101.7246, grad_fn=<NegBackward0>) tensor(11101.7246, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11101.724609375
tensor(11101.7246, grad_fn=<NegBackward0>) tensor(11101.7246, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11101.7255859375
tensor(11101.7246, grad_fn=<NegBackward0>) tensor(11101.7256, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11101.7255859375
tensor(11101.7246, grad_fn=<NegBackward0>) tensor(11101.7256, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11101.724609375
tensor(11101.7246, grad_fn=<NegBackward0>) tensor(11101.7246, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11101.7255859375
tensor(11101.7246, grad_fn=<NegBackward0>) tensor(11101.7256, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11101.7314453125
tensor(11101.7246, grad_fn=<NegBackward0>) tensor(11101.7314, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -11101.7255859375
tensor(11101.7246, grad_fn=<NegBackward0>) tensor(11101.7256, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -11101.83203125
tensor(11101.7246, grad_fn=<NegBackward0>) tensor(11101.8320, grad_fn=<NegBackward0>)
4
Iteration 9700: Loss = -11101.7255859375
tensor(11101.7246, grad_fn=<NegBackward0>) tensor(11101.7256, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9700 due to no improvement.
pi: tensor([[4.6907e-01, 5.3093e-01],
        [5.5128e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1961, 0.8039], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2615, 0.2065],
         [0.5528, 0.1592]],

        [[0.6566, 0.2044],
         [0.5101, 0.5226]],

        [[0.6210, 0.2299],
         [0.5281, 0.5825]],

        [[0.5732, 0.2026],
         [0.5830, 0.6290]],

        [[0.7077, 0.0522],
         [0.5445, 0.6820]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.010101010101010102
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.004130624939255516
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: -0.010213452814961178
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.005453571602473584
Global Adjusted Rand Index: -0.004660560315638867
Average Adjusted Rand Index: -0.0022982783777193063
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22020.51953125
inf tensor(22020.5195, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11112.4326171875
tensor(22020.5195, grad_fn=<NegBackward0>) tensor(11112.4326, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11108.4013671875
tensor(11112.4326, grad_fn=<NegBackward0>) tensor(11108.4014, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11108.17578125
tensor(11108.4014, grad_fn=<NegBackward0>) tensor(11108.1758, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11107.8544921875
tensor(11108.1758, grad_fn=<NegBackward0>) tensor(11107.8545, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11107.015625
tensor(11107.8545, grad_fn=<NegBackward0>) tensor(11107.0156, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11105.7158203125
tensor(11107.0156, grad_fn=<NegBackward0>) tensor(11105.7158, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11104.859375
tensor(11105.7158, grad_fn=<NegBackward0>) tensor(11104.8594, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11104.62109375
tensor(11104.8594, grad_fn=<NegBackward0>) tensor(11104.6211, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11104.533203125
tensor(11104.6211, grad_fn=<NegBackward0>) tensor(11104.5332, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11104.4951171875
tensor(11104.5332, grad_fn=<NegBackward0>) tensor(11104.4951, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11104.455078125
tensor(11104.4951, grad_fn=<NegBackward0>) tensor(11104.4551, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11104.4287109375
tensor(11104.4551, grad_fn=<NegBackward0>) tensor(11104.4287, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11104.3974609375
tensor(11104.4287, grad_fn=<NegBackward0>) tensor(11104.3975, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11104.33203125
tensor(11104.3975, grad_fn=<NegBackward0>) tensor(11104.3320, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11103.76171875
tensor(11104.3320, grad_fn=<NegBackward0>) tensor(11103.7617, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11102.8134765625
tensor(11103.7617, grad_fn=<NegBackward0>) tensor(11102.8135, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11102.5595703125
tensor(11102.8135, grad_fn=<NegBackward0>) tensor(11102.5596, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11102.3251953125
tensor(11102.5596, grad_fn=<NegBackward0>) tensor(11102.3252, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11102.1220703125
tensor(11102.3252, grad_fn=<NegBackward0>) tensor(11102.1221, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11101.994140625
tensor(11102.1221, grad_fn=<NegBackward0>) tensor(11101.9941, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11101.921875
tensor(11101.9941, grad_fn=<NegBackward0>) tensor(11101.9219, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11101.876953125
tensor(11101.9219, grad_fn=<NegBackward0>) tensor(11101.8770, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11101.84765625
tensor(11101.8770, grad_fn=<NegBackward0>) tensor(11101.8477, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11101.8271484375
tensor(11101.8477, grad_fn=<NegBackward0>) tensor(11101.8271, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11101.8125
tensor(11101.8271, grad_fn=<NegBackward0>) tensor(11101.8125, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11101.80078125
tensor(11101.8125, grad_fn=<NegBackward0>) tensor(11101.8008, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11101.791015625
tensor(11101.8008, grad_fn=<NegBackward0>) tensor(11101.7910, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11101.783203125
tensor(11101.7910, grad_fn=<NegBackward0>) tensor(11101.7832, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11101.77734375
tensor(11101.7832, grad_fn=<NegBackward0>) tensor(11101.7773, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11101.7724609375
tensor(11101.7773, grad_fn=<NegBackward0>) tensor(11101.7725, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11101.7666015625
tensor(11101.7725, grad_fn=<NegBackward0>) tensor(11101.7666, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11101.7626953125
tensor(11101.7666, grad_fn=<NegBackward0>) tensor(11101.7627, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11101.7578125
tensor(11101.7627, grad_fn=<NegBackward0>) tensor(11101.7578, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11101.755859375
tensor(11101.7578, grad_fn=<NegBackward0>) tensor(11101.7559, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11101.75390625
tensor(11101.7559, grad_fn=<NegBackward0>) tensor(11101.7539, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11101.7509765625
tensor(11101.7539, grad_fn=<NegBackward0>) tensor(11101.7510, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11101.7490234375
tensor(11101.7510, grad_fn=<NegBackward0>) tensor(11101.7490, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11101.7470703125
tensor(11101.7490, grad_fn=<NegBackward0>) tensor(11101.7471, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11101.7451171875
tensor(11101.7471, grad_fn=<NegBackward0>) tensor(11101.7451, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11101.744140625
tensor(11101.7451, grad_fn=<NegBackward0>) tensor(11101.7441, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11101.7421875
tensor(11101.7441, grad_fn=<NegBackward0>) tensor(11101.7422, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11101.740234375
tensor(11101.7422, grad_fn=<NegBackward0>) tensor(11101.7402, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11101.7412109375
tensor(11101.7402, grad_fn=<NegBackward0>) tensor(11101.7412, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11101.7392578125
tensor(11101.7402, grad_fn=<NegBackward0>) tensor(11101.7393, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11101.73828125
tensor(11101.7393, grad_fn=<NegBackward0>) tensor(11101.7383, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11101.7373046875
tensor(11101.7383, grad_fn=<NegBackward0>) tensor(11101.7373, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11101.736328125
tensor(11101.7373, grad_fn=<NegBackward0>) tensor(11101.7363, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11101.7353515625
tensor(11101.7363, grad_fn=<NegBackward0>) tensor(11101.7354, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11101.734375
tensor(11101.7354, grad_fn=<NegBackward0>) tensor(11101.7344, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11101.732421875
tensor(11101.7344, grad_fn=<NegBackward0>) tensor(11101.7324, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11101.732421875
tensor(11101.7324, grad_fn=<NegBackward0>) tensor(11101.7324, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11101.7314453125
tensor(11101.7324, grad_fn=<NegBackward0>) tensor(11101.7314, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11101.7314453125
tensor(11101.7314, grad_fn=<NegBackward0>) tensor(11101.7314, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11101.7353515625
tensor(11101.7314, grad_fn=<NegBackward0>) tensor(11101.7354, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11101.7314453125
tensor(11101.7314, grad_fn=<NegBackward0>) tensor(11101.7314, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11101.7314453125
tensor(11101.7314, grad_fn=<NegBackward0>) tensor(11101.7314, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11101.732421875
tensor(11101.7314, grad_fn=<NegBackward0>) tensor(11101.7324, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11101.7294921875
tensor(11101.7314, grad_fn=<NegBackward0>) tensor(11101.7295, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11101.7314453125
tensor(11101.7295, grad_fn=<NegBackward0>) tensor(11101.7314, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11101.73046875
tensor(11101.7295, grad_fn=<NegBackward0>) tensor(11101.7305, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11101.73046875
tensor(11101.7295, grad_fn=<NegBackward0>) tensor(11101.7305, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -11101.7294921875
tensor(11101.7295, grad_fn=<NegBackward0>) tensor(11101.7295, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11101.728515625
tensor(11101.7295, grad_fn=<NegBackward0>) tensor(11101.7285, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11101.728515625
tensor(11101.7285, grad_fn=<NegBackward0>) tensor(11101.7285, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11101.728515625
tensor(11101.7285, grad_fn=<NegBackward0>) tensor(11101.7285, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11101.728515625
tensor(11101.7285, grad_fn=<NegBackward0>) tensor(11101.7285, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11101.728515625
tensor(11101.7285, grad_fn=<NegBackward0>) tensor(11101.7285, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11101.7275390625
tensor(11101.7285, grad_fn=<NegBackward0>) tensor(11101.7275, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11101.7275390625
tensor(11101.7275, grad_fn=<NegBackward0>) tensor(11101.7275, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11101.7294921875
tensor(11101.7275, grad_fn=<NegBackward0>) tensor(11101.7295, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11101.7275390625
tensor(11101.7275, grad_fn=<NegBackward0>) tensor(11101.7275, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11101.7294921875
tensor(11101.7275, grad_fn=<NegBackward0>) tensor(11101.7295, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11101.7275390625
tensor(11101.7275, grad_fn=<NegBackward0>) tensor(11101.7275, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11101.7275390625
tensor(11101.7275, grad_fn=<NegBackward0>) tensor(11101.7275, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11101.7265625
tensor(11101.7275, grad_fn=<NegBackward0>) tensor(11101.7266, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11101.736328125
tensor(11101.7266, grad_fn=<NegBackward0>) tensor(11101.7363, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11101.7255859375
tensor(11101.7266, grad_fn=<NegBackward0>) tensor(11101.7256, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11101.7255859375
tensor(11101.7256, grad_fn=<NegBackward0>) tensor(11101.7256, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11101.7255859375
tensor(11101.7256, grad_fn=<NegBackward0>) tensor(11101.7256, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11101.7255859375
tensor(11101.7256, grad_fn=<NegBackward0>) tensor(11101.7256, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11101.7275390625
tensor(11101.7256, grad_fn=<NegBackward0>) tensor(11101.7275, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11101.724609375
tensor(11101.7256, grad_fn=<NegBackward0>) tensor(11101.7246, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11101.724609375
tensor(11101.7246, grad_fn=<NegBackward0>) tensor(11101.7246, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11101.7255859375
tensor(11101.7246, grad_fn=<NegBackward0>) tensor(11101.7256, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11101.7275390625
tensor(11101.7246, grad_fn=<NegBackward0>) tensor(11101.7275, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11101.7255859375
tensor(11101.7246, grad_fn=<NegBackward0>) tensor(11101.7256, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -11101.7265625
tensor(11101.7246, grad_fn=<NegBackward0>) tensor(11101.7266, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -11101.724609375
tensor(11101.7246, grad_fn=<NegBackward0>) tensor(11101.7246, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11101.732421875
tensor(11101.7246, grad_fn=<NegBackward0>) tensor(11101.7324, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11101.7255859375
tensor(11101.7246, grad_fn=<NegBackward0>) tensor(11101.7256, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11101.724609375
tensor(11101.7246, grad_fn=<NegBackward0>) tensor(11101.7246, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11101.7236328125
tensor(11101.7246, grad_fn=<NegBackward0>) tensor(11101.7236, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11101.73046875
tensor(11101.7236, grad_fn=<NegBackward0>) tensor(11101.7305, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11101.724609375
tensor(11101.7236, grad_fn=<NegBackward0>) tensor(11101.7246, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -11101.7236328125
tensor(11101.7236, grad_fn=<NegBackward0>) tensor(11101.7236, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11101.7646484375
tensor(11101.7236, grad_fn=<NegBackward0>) tensor(11101.7646, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11101.724609375
tensor(11101.7236, grad_fn=<NegBackward0>) tensor(11101.7246, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -11101.751953125
tensor(11101.7236, grad_fn=<NegBackward0>) tensor(11101.7520, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -11101.7236328125
tensor(11101.7236, grad_fn=<NegBackward0>) tensor(11101.7236, grad_fn=<NegBackward0>)
pi: tensor([[1.0000e+00, 4.6627e-06],
        [5.3096e-01, 4.6904e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8039, 0.1961], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1592, 0.2065],
         [0.7133, 0.2615]],

        [[0.6158, 0.2044],
         [0.5910, 0.5624]],

        [[0.5398, 0.2299],
         [0.5999, 0.6867]],

        [[0.6631, 0.2026],
         [0.5362, 0.5385]],

        [[0.5044, 0.0522],
         [0.7259, 0.6588]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.010101010101010102
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.004130624939255516
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: -0.010213452814961178
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.005453571602473584
Global Adjusted Rand Index: -0.004660560315638867
Average Adjusted Rand Index: -0.0022982783777193063
[-0.004660560315638867, -0.004660560315638867] [-0.0022982783777193063, -0.0022982783777193063] [11101.7255859375, 11101.724609375]
-------------------------------------
This iteration is 29
True Objective function: Loss = -11007.374834561346
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24276.548828125
inf tensor(24276.5488, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11134.529296875
tensor(24276.5488, grad_fn=<NegBackward0>) tensor(11134.5293, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11133.7421875
tensor(11134.5293, grad_fn=<NegBackward0>) tensor(11133.7422, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11133.4638671875
tensor(11133.7422, grad_fn=<NegBackward0>) tensor(11133.4639, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11133.1240234375
tensor(11133.4639, grad_fn=<NegBackward0>) tensor(11133.1240, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11132.623046875
tensor(11133.1240, grad_fn=<NegBackward0>) tensor(11132.6230, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11132.1630859375
tensor(11132.6230, grad_fn=<NegBackward0>) tensor(11132.1631, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11131.8017578125
tensor(11132.1631, grad_fn=<NegBackward0>) tensor(11131.8018, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11131.4775390625
tensor(11131.8018, grad_fn=<NegBackward0>) tensor(11131.4775, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11131.1767578125
tensor(11131.4775, grad_fn=<NegBackward0>) tensor(11131.1768, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11130.8779296875
tensor(11131.1768, grad_fn=<NegBackward0>) tensor(11130.8779, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11130.56640625
tensor(11130.8779, grad_fn=<NegBackward0>) tensor(11130.5664, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11130.21484375
tensor(11130.5664, grad_fn=<NegBackward0>) tensor(11130.2148, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11129.8095703125
tensor(11130.2148, grad_fn=<NegBackward0>) tensor(11129.8096, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11128.85546875
tensor(11129.8096, grad_fn=<NegBackward0>) tensor(11128.8555, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11115.6337890625
tensor(11128.8555, grad_fn=<NegBackward0>) tensor(11115.6338, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11085.224609375
tensor(11115.6338, grad_fn=<NegBackward0>) tensor(11085.2246, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11053.26171875
tensor(11085.2246, grad_fn=<NegBackward0>) tensor(11053.2617, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10986.6787109375
tensor(11053.2617, grad_fn=<NegBackward0>) tensor(10986.6787, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10981.84765625
tensor(10986.6787, grad_fn=<NegBackward0>) tensor(10981.8477, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10977.5595703125
tensor(10981.8477, grad_fn=<NegBackward0>) tensor(10977.5596, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10977.3779296875
tensor(10977.5596, grad_fn=<NegBackward0>) tensor(10977.3779, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10977.1796875
tensor(10977.3779, grad_fn=<NegBackward0>) tensor(10977.1797, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10977.1240234375
tensor(10977.1797, grad_fn=<NegBackward0>) tensor(10977.1240, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10977.08984375
tensor(10977.1240, grad_fn=<NegBackward0>) tensor(10977.0898, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10977.064453125
tensor(10977.0898, grad_fn=<NegBackward0>) tensor(10977.0645, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10977.0439453125
tensor(10977.0645, grad_fn=<NegBackward0>) tensor(10977.0439, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10977.02734375
tensor(10977.0439, grad_fn=<NegBackward0>) tensor(10977.0273, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10977.0126953125
tensor(10977.0273, grad_fn=<NegBackward0>) tensor(10977.0127, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10977.0009765625
tensor(10977.0127, grad_fn=<NegBackward0>) tensor(10977.0010, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10976.990234375
tensor(10977.0010, grad_fn=<NegBackward0>) tensor(10976.9902, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10976.9814453125
tensor(10976.9902, grad_fn=<NegBackward0>) tensor(10976.9814, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10976.947265625
tensor(10976.9814, grad_fn=<NegBackward0>) tensor(10976.9473, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10976.916015625
tensor(10976.9473, grad_fn=<NegBackward0>) tensor(10976.9160, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10976.9111328125
tensor(10976.9160, grad_fn=<NegBackward0>) tensor(10976.9111, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10976.9072265625
tensor(10976.9111, grad_fn=<NegBackward0>) tensor(10976.9072, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10976.8974609375
tensor(10976.9072, grad_fn=<NegBackward0>) tensor(10976.8975, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10976.78125
tensor(10976.8975, grad_fn=<NegBackward0>) tensor(10976.7812, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10976.7734375
tensor(10976.7812, grad_fn=<NegBackward0>) tensor(10976.7734, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10976.7705078125
tensor(10976.7734, grad_fn=<NegBackward0>) tensor(10976.7705, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10976.7666015625
tensor(10976.7705, grad_fn=<NegBackward0>) tensor(10976.7666, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10976.751953125
tensor(10976.7666, grad_fn=<NegBackward0>) tensor(10976.7520, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10976.572265625
tensor(10976.7520, grad_fn=<NegBackward0>) tensor(10976.5723, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10976.56640625
tensor(10976.5723, grad_fn=<NegBackward0>) tensor(10976.5664, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10976.5654296875
tensor(10976.5664, grad_fn=<NegBackward0>) tensor(10976.5654, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10976.5732421875
tensor(10976.5654, grad_fn=<NegBackward0>) tensor(10976.5732, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10976.5625
tensor(10976.5654, grad_fn=<NegBackward0>) tensor(10976.5625, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10976.5634765625
tensor(10976.5625, grad_fn=<NegBackward0>) tensor(10976.5635, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10976.5595703125
tensor(10976.5625, grad_fn=<NegBackward0>) tensor(10976.5596, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10976.55859375
tensor(10976.5596, grad_fn=<NegBackward0>) tensor(10976.5586, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10976.55859375
tensor(10976.5586, grad_fn=<NegBackward0>) tensor(10976.5586, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10976.548828125
tensor(10976.5586, grad_fn=<NegBackward0>) tensor(10976.5488, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10976.5517578125
tensor(10976.5488, grad_fn=<NegBackward0>) tensor(10976.5518, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10976.548828125
tensor(10976.5488, grad_fn=<NegBackward0>) tensor(10976.5488, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10976.544921875
tensor(10976.5488, grad_fn=<NegBackward0>) tensor(10976.5449, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10976.544921875
tensor(10976.5449, grad_fn=<NegBackward0>) tensor(10976.5449, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10976.544921875
tensor(10976.5449, grad_fn=<NegBackward0>) tensor(10976.5449, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10976.546875
tensor(10976.5449, grad_fn=<NegBackward0>) tensor(10976.5469, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10976.5419921875
tensor(10976.5449, grad_fn=<NegBackward0>) tensor(10976.5420, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10976.5400390625
tensor(10976.5420, grad_fn=<NegBackward0>) tensor(10976.5400, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10976.5390625
tensor(10976.5400, grad_fn=<NegBackward0>) tensor(10976.5391, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10976.5302734375
tensor(10976.5391, grad_fn=<NegBackward0>) tensor(10976.5303, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10976.525390625
tensor(10976.5303, grad_fn=<NegBackward0>) tensor(10976.5254, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10976.5234375
tensor(10976.5254, grad_fn=<NegBackward0>) tensor(10976.5234, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10976.5224609375
tensor(10976.5234, grad_fn=<NegBackward0>) tensor(10976.5225, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10976.51953125
tensor(10976.5225, grad_fn=<NegBackward0>) tensor(10976.5195, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10976.5205078125
tensor(10976.5195, grad_fn=<NegBackward0>) tensor(10976.5205, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10976.51953125
tensor(10976.5195, grad_fn=<NegBackward0>) tensor(10976.5195, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10976.5185546875
tensor(10976.5195, grad_fn=<NegBackward0>) tensor(10976.5186, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10976.51953125
tensor(10976.5186, grad_fn=<NegBackward0>) tensor(10976.5195, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10976.5185546875
tensor(10976.5186, grad_fn=<NegBackward0>) tensor(10976.5186, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10976.517578125
tensor(10976.5186, grad_fn=<NegBackward0>) tensor(10976.5176, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10976.5166015625
tensor(10976.5176, grad_fn=<NegBackward0>) tensor(10976.5166, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10976.5185546875
tensor(10976.5166, grad_fn=<NegBackward0>) tensor(10976.5186, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -10976.5166015625
tensor(10976.5166, grad_fn=<NegBackward0>) tensor(10976.5166, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10976.517578125
tensor(10976.5166, grad_fn=<NegBackward0>) tensor(10976.5176, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10976.5166015625
tensor(10976.5166, grad_fn=<NegBackward0>) tensor(10976.5166, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10976.521484375
tensor(10976.5166, grad_fn=<NegBackward0>) tensor(10976.5215, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10976.5166015625
tensor(10976.5166, grad_fn=<NegBackward0>) tensor(10976.5166, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10976.5556640625
tensor(10976.5166, grad_fn=<NegBackward0>) tensor(10976.5557, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10976.517578125
tensor(10976.5166, grad_fn=<NegBackward0>) tensor(10976.5176, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -10976.517578125
tensor(10976.5166, grad_fn=<NegBackward0>) tensor(10976.5176, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -10976.560546875
tensor(10976.5166, grad_fn=<NegBackward0>) tensor(10976.5605, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -10976.5146484375
tensor(10976.5166, grad_fn=<NegBackward0>) tensor(10976.5146, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10976.666015625
tensor(10976.5146, grad_fn=<NegBackward0>) tensor(10976.6660, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -10976.5166015625
tensor(10976.5146, grad_fn=<NegBackward0>) tensor(10976.5166, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -10976.515625
tensor(10976.5146, grad_fn=<NegBackward0>) tensor(10976.5156, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -10976.515625
tensor(10976.5146, grad_fn=<NegBackward0>) tensor(10976.5156, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -10976.515625
tensor(10976.5146, grad_fn=<NegBackward0>) tensor(10976.5156, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8800 due to no improvement.
pi: tensor([[0.7607, 0.2393],
        [0.2508, 0.7492]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4635, 0.5365], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2040, 0.0955],
         [0.5195, 0.2574]],

        [[0.6667, 0.1052],
         [0.6900, 0.7057]],

        [[0.5413, 0.1021],
         [0.6633, 0.5175]],

        [[0.7017, 0.1019],
         [0.5706, 0.5553]],

        [[0.6619, 0.0987],
         [0.5924, 0.5966]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8823435719624108
time is 2
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721141809334062
Global Adjusted Rand Index: 0.8909175104403587
Average Adjusted Rand Index: 0.8915358708668695
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21788.23828125
inf tensor(21788.2383, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11133.962890625
tensor(21788.2383, grad_fn=<NegBackward0>) tensor(11133.9629, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11133.3095703125
tensor(11133.9629, grad_fn=<NegBackward0>) tensor(11133.3096, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11132.8037109375
tensor(11133.3096, grad_fn=<NegBackward0>) tensor(11132.8037, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11132.0771484375
tensor(11132.8037, grad_fn=<NegBackward0>) tensor(11132.0771, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11131.494140625
tensor(11132.0771, grad_fn=<NegBackward0>) tensor(11131.4941, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11131.0869140625
tensor(11131.4941, grad_fn=<NegBackward0>) tensor(11131.0869, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11130.72265625
tensor(11131.0869, grad_fn=<NegBackward0>) tensor(11130.7227, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11130.3681640625
tensor(11130.7227, grad_fn=<NegBackward0>) tensor(11130.3682, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11130.01171875
tensor(11130.3682, grad_fn=<NegBackward0>) tensor(11130.0117, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11129.640625
tensor(11130.0117, grad_fn=<NegBackward0>) tensor(11129.6406, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11128.9111328125
tensor(11129.6406, grad_fn=<NegBackward0>) tensor(11128.9111, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11094.640625
tensor(11128.9111, grad_fn=<NegBackward0>) tensor(11094.6406, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11082.529296875
tensor(11094.6406, grad_fn=<NegBackward0>) tensor(11082.5293, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10989.2939453125
tensor(11082.5293, grad_fn=<NegBackward0>) tensor(10989.2939, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10981.7744140625
tensor(10989.2939, grad_fn=<NegBackward0>) tensor(10981.7744, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10977.6044921875
tensor(10981.7744, grad_fn=<NegBackward0>) tensor(10977.6045, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10977.4033203125
tensor(10977.6045, grad_fn=<NegBackward0>) tensor(10977.4033, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10977.1044921875
tensor(10977.4033, grad_fn=<NegBackward0>) tensor(10977.1045, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10976.962890625
tensor(10977.1045, grad_fn=<NegBackward0>) tensor(10976.9629, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10976.8828125
tensor(10976.9629, grad_fn=<NegBackward0>) tensor(10976.8828, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10976.869140625
tensor(10976.8828, grad_fn=<NegBackward0>) tensor(10976.8691, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10976.857421875
tensor(10976.8691, grad_fn=<NegBackward0>) tensor(10976.8574, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10976.8486328125
tensor(10976.8574, grad_fn=<NegBackward0>) tensor(10976.8486, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10976.8388671875
tensor(10976.8486, grad_fn=<NegBackward0>) tensor(10976.8389, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10976.82421875
tensor(10976.8389, grad_fn=<NegBackward0>) tensor(10976.8242, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10976.8203125
tensor(10976.8242, grad_fn=<NegBackward0>) tensor(10976.8203, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10976.8154296875
tensor(10976.8203, grad_fn=<NegBackward0>) tensor(10976.8154, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10976.8076171875
tensor(10976.8154, grad_fn=<NegBackward0>) tensor(10976.8076, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10976.8037109375
tensor(10976.8076, grad_fn=<NegBackward0>) tensor(10976.8037, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10976.7998046875
tensor(10976.8037, grad_fn=<NegBackward0>) tensor(10976.7998, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10976.796875
tensor(10976.7998, grad_fn=<NegBackward0>) tensor(10976.7969, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10976.78125
tensor(10976.7969, grad_fn=<NegBackward0>) tensor(10976.7812, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10976.7353515625
tensor(10976.7812, grad_fn=<NegBackward0>) tensor(10976.7354, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10976.74609375
tensor(10976.7354, grad_fn=<NegBackward0>) tensor(10976.7461, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -10976.7255859375
tensor(10976.7354, grad_fn=<NegBackward0>) tensor(10976.7256, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10976.7197265625
tensor(10976.7256, grad_fn=<NegBackward0>) tensor(10976.7197, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10976.7158203125
tensor(10976.7197, grad_fn=<NegBackward0>) tensor(10976.7158, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10976.7109375
tensor(10976.7158, grad_fn=<NegBackward0>) tensor(10976.7109, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10976.58984375
tensor(10976.7109, grad_fn=<NegBackward0>) tensor(10976.5898, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10976.5849609375
tensor(10976.5898, grad_fn=<NegBackward0>) tensor(10976.5850, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10976.5810546875
tensor(10976.5850, grad_fn=<NegBackward0>) tensor(10976.5811, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10976.576171875
tensor(10976.5811, grad_fn=<NegBackward0>) tensor(10976.5762, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10976.572265625
tensor(10976.5762, grad_fn=<NegBackward0>) tensor(10976.5723, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10976.5703125
tensor(10976.5723, grad_fn=<NegBackward0>) tensor(10976.5703, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10976.5673828125
tensor(10976.5703, grad_fn=<NegBackward0>) tensor(10976.5674, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10976.5791015625
tensor(10976.5674, grad_fn=<NegBackward0>) tensor(10976.5791, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -10976.5634765625
tensor(10976.5674, grad_fn=<NegBackward0>) tensor(10976.5635, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10976.5634765625
tensor(10976.5635, grad_fn=<NegBackward0>) tensor(10976.5635, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10976.5556640625
tensor(10976.5635, grad_fn=<NegBackward0>) tensor(10976.5557, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10976.556640625
tensor(10976.5557, grad_fn=<NegBackward0>) tensor(10976.5566, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10976.5546875
tensor(10976.5557, grad_fn=<NegBackward0>) tensor(10976.5547, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10976.572265625
tensor(10976.5547, grad_fn=<NegBackward0>) tensor(10976.5723, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10976.5546875
tensor(10976.5547, grad_fn=<NegBackward0>) tensor(10976.5547, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10976.5537109375
tensor(10976.5547, grad_fn=<NegBackward0>) tensor(10976.5537, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10976.5517578125
tensor(10976.5537, grad_fn=<NegBackward0>) tensor(10976.5518, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10976.5517578125
tensor(10976.5518, grad_fn=<NegBackward0>) tensor(10976.5518, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10976.5517578125
tensor(10976.5518, grad_fn=<NegBackward0>) tensor(10976.5518, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10976.5546875
tensor(10976.5518, grad_fn=<NegBackward0>) tensor(10976.5547, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10976.5517578125
tensor(10976.5518, grad_fn=<NegBackward0>) tensor(10976.5518, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10976.5478515625
tensor(10976.5518, grad_fn=<NegBackward0>) tensor(10976.5479, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10976.546875
tensor(10976.5479, grad_fn=<NegBackward0>) tensor(10976.5469, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10976.5458984375
tensor(10976.5469, grad_fn=<NegBackward0>) tensor(10976.5459, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10976.546875
tensor(10976.5459, grad_fn=<NegBackward0>) tensor(10976.5469, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10976.544921875
tensor(10976.5459, grad_fn=<NegBackward0>) tensor(10976.5449, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10976.54296875
tensor(10976.5449, grad_fn=<NegBackward0>) tensor(10976.5430, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10976.544921875
tensor(10976.5430, grad_fn=<NegBackward0>) tensor(10976.5449, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10976.54296875
tensor(10976.5430, grad_fn=<NegBackward0>) tensor(10976.5430, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10976.5439453125
tensor(10976.5430, grad_fn=<NegBackward0>) tensor(10976.5439, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10976.5439453125
tensor(10976.5430, grad_fn=<NegBackward0>) tensor(10976.5439, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -10976.5478515625
tensor(10976.5430, grad_fn=<NegBackward0>) tensor(10976.5479, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -10976.54296875
tensor(10976.5430, grad_fn=<NegBackward0>) tensor(10976.5430, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10976.544921875
tensor(10976.5430, grad_fn=<NegBackward0>) tensor(10976.5449, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10976.5419921875
tensor(10976.5430, grad_fn=<NegBackward0>) tensor(10976.5420, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10976.5400390625
tensor(10976.5420, grad_fn=<NegBackward0>) tensor(10976.5400, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10976.5400390625
tensor(10976.5400, grad_fn=<NegBackward0>) tensor(10976.5400, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10976.5390625
tensor(10976.5400, grad_fn=<NegBackward0>) tensor(10976.5391, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10976.5400390625
tensor(10976.5391, grad_fn=<NegBackward0>) tensor(10976.5400, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10976.5390625
tensor(10976.5391, grad_fn=<NegBackward0>) tensor(10976.5391, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10976.5400390625
tensor(10976.5391, grad_fn=<NegBackward0>) tensor(10976.5400, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10976.537109375
tensor(10976.5391, grad_fn=<NegBackward0>) tensor(10976.5371, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10976.529296875
tensor(10976.5371, grad_fn=<NegBackward0>) tensor(10976.5293, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10976.52734375
tensor(10976.5293, grad_fn=<NegBackward0>) tensor(10976.5273, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10976.53125
tensor(10976.5273, grad_fn=<NegBackward0>) tensor(10976.5312, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10976.52734375
tensor(10976.5273, grad_fn=<NegBackward0>) tensor(10976.5273, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10976.548828125
tensor(10976.5273, grad_fn=<NegBackward0>) tensor(10976.5488, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -10976.52734375
tensor(10976.5273, grad_fn=<NegBackward0>) tensor(10976.5273, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10976.5263671875
tensor(10976.5273, grad_fn=<NegBackward0>) tensor(10976.5264, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10976.787109375
tensor(10976.5264, grad_fn=<NegBackward0>) tensor(10976.7871, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10976.52734375
tensor(10976.5264, grad_fn=<NegBackward0>) tensor(10976.5273, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -10976.513671875
tensor(10976.5264, grad_fn=<NegBackward0>) tensor(10976.5137, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10976.5263671875
tensor(10976.5137, grad_fn=<NegBackward0>) tensor(10976.5264, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10976.51171875
tensor(10976.5137, grad_fn=<NegBackward0>) tensor(10976.5117, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10976.505859375
tensor(10976.5117, grad_fn=<NegBackward0>) tensor(10976.5059, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10976.509765625
tensor(10976.5059, grad_fn=<NegBackward0>) tensor(10976.5098, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -10976.5068359375
tensor(10976.5059, grad_fn=<NegBackward0>) tensor(10976.5068, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -10976.5263671875
tensor(10976.5059, grad_fn=<NegBackward0>) tensor(10976.5264, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -10976.5068359375
tensor(10976.5059, grad_fn=<NegBackward0>) tensor(10976.5068, grad_fn=<NegBackward0>)
4
Iteration 9800: Loss = -10976.6640625
tensor(10976.5059, grad_fn=<NegBackward0>) tensor(10976.6641, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9800 due to no improvement.
pi: tensor([[0.7443, 0.2557],
        [0.2368, 0.7632]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5307, 0.4693], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2595, 0.0960],
         [0.6875, 0.2024]],

        [[0.5688, 0.1059],
         [0.5426, 0.7080]],

        [[0.6525, 0.1022],
         [0.6385, 0.5681]],

        [[0.5598, 0.1019],
         [0.7222, 0.6380]],

        [[0.5124, 0.0990],
         [0.7040, 0.5427]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8823435719624108
time is 2
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721141809334062
Global Adjusted Rand Index: 0.8909175104403587
Average Adjusted Rand Index: 0.8915358708668695
[0.8909175104403587, 0.8909175104403587] [0.8915358708668695, 0.8915358708668695] [10976.515625, 10976.6640625]
-------------------------------------
This iteration is 30
True Objective function: Loss = -10867.437231942185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23038.78125
inf tensor(23038.7812, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11006.5791015625
tensor(23038.7812, grad_fn=<NegBackward0>) tensor(11006.5791, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11005.5439453125
tensor(11006.5791, grad_fn=<NegBackward0>) tensor(11005.5439, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11005.23046875
tensor(11005.5439, grad_fn=<NegBackward0>) tensor(11005.2305, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11004.9111328125
tensor(11005.2305, grad_fn=<NegBackward0>) tensor(11004.9111, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11004.2080078125
tensor(11004.9111, grad_fn=<NegBackward0>) tensor(11004.2080, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11003.609375
tensor(11004.2080, grad_fn=<NegBackward0>) tensor(11003.6094, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11003.0400390625
tensor(11003.6094, grad_fn=<NegBackward0>) tensor(11003.0400, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11002.48046875
tensor(11003.0400, grad_fn=<NegBackward0>) tensor(11002.4805, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11001.970703125
tensor(11002.4805, grad_fn=<NegBackward0>) tensor(11001.9707, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11001.515625
tensor(11001.9707, grad_fn=<NegBackward0>) tensor(11001.5156, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11001.107421875
tensor(11001.5156, grad_fn=<NegBackward0>) tensor(11001.1074, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11000.763671875
tensor(11001.1074, grad_fn=<NegBackward0>) tensor(11000.7637, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11000.5009765625
tensor(11000.7637, grad_fn=<NegBackward0>) tensor(11000.5010, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11000.31640625
tensor(11000.5010, grad_fn=<NegBackward0>) tensor(11000.3164, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11000.205078125
tensor(11000.3164, grad_fn=<NegBackward0>) tensor(11000.2051, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11000.140625
tensor(11000.2051, grad_fn=<NegBackward0>) tensor(11000.1406, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11000.0966796875
tensor(11000.1406, grad_fn=<NegBackward0>) tensor(11000.0967, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11000.060546875
tensor(11000.0967, grad_fn=<NegBackward0>) tensor(11000.0605, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11000.0126953125
tensor(11000.0605, grad_fn=<NegBackward0>) tensor(11000.0127, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10981.4912109375
tensor(11000.0127, grad_fn=<NegBackward0>) tensor(10981.4912, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10975.146484375
tensor(10981.4912, grad_fn=<NegBackward0>) tensor(10975.1465, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10970.578125
tensor(10975.1465, grad_fn=<NegBackward0>) tensor(10970.5781, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10969.5263671875
tensor(10970.5781, grad_fn=<NegBackward0>) tensor(10969.5264, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10969.404296875
tensor(10969.5264, grad_fn=<NegBackward0>) tensor(10969.4043, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10969.353515625
tensor(10969.4043, grad_fn=<NegBackward0>) tensor(10969.3535, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10969.3251953125
tensor(10969.3535, grad_fn=<NegBackward0>) tensor(10969.3252, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10969.306640625
tensor(10969.3252, grad_fn=<NegBackward0>) tensor(10969.3066, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10969.294921875
tensor(10969.3066, grad_fn=<NegBackward0>) tensor(10969.2949, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10969.28515625
tensor(10969.2949, grad_fn=<NegBackward0>) tensor(10969.2852, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10969.2763671875
tensor(10969.2852, grad_fn=<NegBackward0>) tensor(10969.2764, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10969.2724609375
tensor(10969.2764, grad_fn=<NegBackward0>) tensor(10969.2725, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10969.2666015625
tensor(10969.2725, grad_fn=<NegBackward0>) tensor(10969.2666, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10969.263671875
tensor(10969.2666, grad_fn=<NegBackward0>) tensor(10969.2637, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10969.2607421875
tensor(10969.2637, grad_fn=<NegBackward0>) tensor(10969.2607, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10969.2578125
tensor(10969.2607, grad_fn=<NegBackward0>) tensor(10969.2578, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10969.2548828125
tensor(10969.2578, grad_fn=<NegBackward0>) tensor(10969.2549, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10969.2548828125
tensor(10969.2549, grad_fn=<NegBackward0>) tensor(10969.2549, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10969.251953125
tensor(10969.2549, grad_fn=<NegBackward0>) tensor(10969.2520, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10969.2509765625
tensor(10969.2520, grad_fn=<NegBackward0>) tensor(10969.2510, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10969.2490234375
tensor(10969.2510, grad_fn=<NegBackward0>) tensor(10969.2490, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10969.248046875
tensor(10969.2490, grad_fn=<NegBackward0>) tensor(10969.2480, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10969.2470703125
tensor(10969.2480, grad_fn=<NegBackward0>) tensor(10969.2471, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10969.24609375
tensor(10969.2471, grad_fn=<NegBackward0>) tensor(10969.2461, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10969.2451171875
tensor(10969.2461, grad_fn=<NegBackward0>) tensor(10969.2451, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10969.2451171875
tensor(10969.2451, grad_fn=<NegBackward0>) tensor(10969.2451, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10969.244140625
tensor(10969.2451, grad_fn=<NegBackward0>) tensor(10969.2441, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10969.2431640625
tensor(10969.2441, grad_fn=<NegBackward0>) tensor(10969.2432, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10969.2421875
tensor(10969.2432, grad_fn=<NegBackward0>) tensor(10969.2422, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10969.240234375
tensor(10969.2422, grad_fn=<NegBackward0>) tensor(10969.2402, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10969.2412109375
tensor(10969.2402, grad_fn=<NegBackward0>) tensor(10969.2412, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10969.2392578125
tensor(10969.2402, grad_fn=<NegBackward0>) tensor(10969.2393, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10969.240234375
tensor(10969.2393, grad_fn=<NegBackward0>) tensor(10969.2402, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10969.244140625
tensor(10969.2393, grad_fn=<NegBackward0>) tensor(10969.2441, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -10969.240234375
tensor(10969.2393, grad_fn=<NegBackward0>) tensor(10969.2402, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -10969.23828125
tensor(10969.2393, grad_fn=<NegBackward0>) tensor(10969.2383, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10969.23828125
tensor(10969.2383, grad_fn=<NegBackward0>) tensor(10969.2383, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10969.23828125
tensor(10969.2383, grad_fn=<NegBackward0>) tensor(10969.2383, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10969.2373046875
tensor(10969.2383, grad_fn=<NegBackward0>) tensor(10969.2373, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10969.2373046875
tensor(10969.2373, grad_fn=<NegBackward0>) tensor(10969.2373, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10969.2373046875
tensor(10969.2373, grad_fn=<NegBackward0>) tensor(10969.2373, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10969.236328125
tensor(10969.2373, grad_fn=<NegBackward0>) tensor(10969.2363, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10969.236328125
tensor(10969.2363, grad_fn=<NegBackward0>) tensor(10969.2363, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10969.236328125
tensor(10969.2363, grad_fn=<NegBackward0>) tensor(10969.2363, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10969.2353515625
tensor(10969.2363, grad_fn=<NegBackward0>) tensor(10969.2354, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10969.236328125
tensor(10969.2354, grad_fn=<NegBackward0>) tensor(10969.2363, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10969.236328125
tensor(10969.2354, grad_fn=<NegBackward0>) tensor(10969.2363, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -10969.2353515625
tensor(10969.2354, grad_fn=<NegBackward0>) tensor(10969.2354, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10969.240234375
tensor(10969.2354, grad_fn=<NegBackward0>) tensor(10969.2402, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10969.2353515625
tensor(10969.2354, grad_fn=<NegBackward0>) tensor(10969.2354, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10969.240234375
tensor(10969.2354, grad_fn=<NegBackward0>) tensor(10969.2402, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10969.2353515625
tensor(10969.2354, grad_fn=<NegBackward0>) tensor(10969.2354, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10969.236328125
tensor(10969.2354, grad_fn=<NegBackward0>) tensor(10969.2363, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10969.234375
tensor(10969.2354, grad_fn=<NegBackward0>) tensor(10969.2344, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10969.2353515625
tensor(10969.2344, grad_fn=<NegBackward0>) tensor(10969.2354, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10969.2353515625
tensor(10969.2344, grad_fn=<NegBackward0>) tensor(10969.2354, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -10969.2490234375
tensor(10969.2344, grad_fn=<NegBackward0>) tensor(10969.2490, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -10969.2333984375
tensor(10969.2344, grad_fn=<NegBackward0>) tensor(10969.2334, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10969.2353515625
tensor(10969.2334, grad_fn=<NegBackward0>) tensor(10969.2354, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10969.2373046875
tensor(10969.2334, grad_fn=<NegBackward0>) tensor(10969.2373, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -10969.26953125
tensor(10969.2334, grad_fn=<NegBackward0>) tensor(10969.2695, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -10969.271484375
tensor(10969.2334, grad_fn=<NegBackward0>) tensor(10969.2715, grad_fn=<NegBackward0>)
4
Iteration 8200: Loss = -10969.234375
tensor(10969.2334, grad_fn=<NegBackward0>) tensor(10969.2344, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[1.1366e-05, 9.9999e-01],
        [1.0156e-01, 8.9844e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5151, 0.4849], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2692, 0.0993],
         [0.6987, 0.1608]],

        [[0.6515, 0.0747],
         [0.6465, 0.6756]],

        [[0.6207, 0.1773],
         [0.7231, 0.6265]],

        [[0.5416, 0.2140],
         [0.5785, 0.6050]],

        [[0.6046, 0.2041],
         [0.5331, 0.6485]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080650161849864
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.015076275178066338
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: -0.015837733814333767
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: 0.010909073640315427
Average Adjusted Rand Index: 0.16073343878247107
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19847.96875
inf tensor(19847.9688, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11005.3857421875
tensor(19847.9688, grad_fn=<NegBackward0>) tensor(11005.3857, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11004.9287109375
tensor(11005.3857, grad_fn=<NegBackward0>) tensor(11004.9287, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11004.2666015625
tensor(11004.9287, grad_fn=<NegBackward0>) tensor(11004.2666, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11003.1123046875
tensor(11004.2666, grad_fn=<NegBackward0>) tensor(11003.1123, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11002.26953125
tensor(11003.1123, grad_fn=<NegBackward0>) tensor(11002.2695, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11001.8505859375
tensor(11002.2695, grad_fn=<NegBackward0>) tensor(11001.8506, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11001.6279296875
tensor(11001.8506, grad_fn=<NegBackward0>) tensor(11001.6279, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11001.46875
tensor(11001.6279, grad_fn=<NegBackward0>) tensor(11001.4688, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11001.302734375
tensor(11001.4688, grad_fn=<NegBackward0>) tensor(11001.3027, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11001.0400390625
tensor(11001.3027, grad_fn=<NegBackward0>) tensor(11001.0400, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11000.640625
tensor(11001.0400, grad_fn=<NegBackward0>) tensor(11000.6406, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11000.3984375
tensor(11000.6406, grad_fn=<NegBackward0>) tensor(11000.3984, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11000.2578125
tensor(11000.3984, grad_fn=<NegBackward0>) tensor(11000.2578, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11000.1806640625
tensor(11000.2578, grad_fn=<NegBackward0>) tensor(11000.1807, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11000.142578125
tensor(11000.1807, grad_fn=<NegBackward0>) tensor(11000.1426, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11000.119140625
tensor(11000.1426, grad_fn=<NegBackward0>) tensor(11000.1191, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11000.091796875
tensor(11000.1191, grad_fn=<NegBackward0>) tensor(11000.0918, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11000.0244140625
tensor(11000.0918, grad_fn=<NegBackward0>) tensor(11000.0244, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10997.79296875
tensor(11000.0244, grad_fn=<NegBackward0>) tensor(10997.7930, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10833.03515625
tensor(10997.7930, grad_fn=<NegBackward0>) tensor(10833.0352, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10831.8974609375
tensor(10833.0352, grad_fn=<NegBackward0>) tensor(10831.8975, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10831.830078125
tensor(10831.8975, grad_fn=<NegBackward0>) tensor(10831.8301, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10831.7021484375
tensor(10831.8301, grad_fn=<NegBackward0>) tensor(10831.7021, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10831.6162109375
tensor(10831.7021, grad_fn=<NegBackward0>) tensor(10831.6162, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10830.7333984375
tensor(10831.6162, grad_fn=<NegBackward0>) tensor(10830.7334, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10830.72265625
tensor(10830.7334, grad_fn=<NegBackward0>) tensor(10830.7227, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10830.7158203125
tensor(10830.7227, grad_fn=<NegBackward0>) tensor(10830.7158, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10830.705078125
tensor(10830.7158, grad_fn=<NegBackward0>) tensor(10830.7051, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10830.62890625
tensor(10830.7051, grad_fn=<NegBackward0>) tensor(10830.6289, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10830.5830078125
tensor(10830.6289, grad_fn=<NegBackward0>) tensor(10830.5830, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10830.578125
tensor(10830.5830, grad_fn=<NegBackward0>) tensor(10830.5781, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10830.57421875
tensor(10830.5781, grad_fn=<NegBackward0>) tensor(10830.5742, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10830.5537109375
tensor(10830.5742, grad_fn=<NegBackward0>) tensor(10830.5537, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10830.5146484375
tensor(10830.5537, grad_fn=<NegBackward0>) tensor(10830.5146, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10830.501953125
tensor(10830.5146, grad_fn=<NegBackward0>) tensor(10830.5020, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10830.4970703125
tensor(10830.5020, grad_fn=<NegBackward0>) tensor(10830.4971, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10830.494140625
tensor(10830.4971, grad_fn=<NegBackward0>) tensor(10830.4941, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10830.4892578125
tensor(10830.4941, grad_fn=<NegBackward0>) tensor(10830.4893, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10830.4296875
tensor(10830.4893, grad_fn=<NegBackward0>) tensor(10830.4297, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10830.42578125
tensor(10830.4297, grad_fn=<NegBackward0>) tensor(10830.4258, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10830.4248046875
tensor(10830.4258, grad_fn=<NegBackward0>) tensor(10830.4248, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10830.419921875
tensor(10830.4248, grad_fn=<NegBackward0>) tensor(10830.4199, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10830.41796875
tensor(10830.4199, grad_fn=<NegBackward0>) tensor(10830.4180, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10830.416015625
tensor(10830.4180, grad_fn=<NegBackward0>) tensor(10830.4160, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10830.4208984375
tensor(10830.4160, grad_fn=<NegBackward0>) tensor(10830.4209, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10830.412109375
tensor(10830.4160, grad_fn=<NegBackward0>) tensor(10830.4121, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10830.4091796875
tensor(10830.4121, grad_fn=<NegBackward0>) tensor(10830.4092, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10830.427734375
tensor(10830.4092, grad_fn=<NegBackward0>) tensor(10830.4277, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10830.408203125
tensor(10830.4092, grad_fn=<NegBackward0>) tensor(10830.4082, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10830.4091796875
tensor(10830.4082, grad_fn=<NegBackward0>) tensor(10830.4092, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10830.4091796875
tensor(10830.4082, grad_fn=<NegBackward0>) tensor(10830.4092, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -10830.4033203125
tensor(10830.4082, grad_fn=<NegBackward0>) tensor(10830.4033, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10830.40234375
tensor(10830.4033, grad_fn=<NegBackward0>) tensor(10830.4023, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10830.4033203125
tensor(10830.4023, grad_fn=<NegBackward0>) tensor(10830.4033, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10830.40234375
tensor(10830.4023, grad_fn=<NegBackward0>) tensor(10830.4023, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10830.4013671875
tensor(10830.4023, grad_fn=<NegBackward0>) tensor(10830.4014, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10830.40234375
tensor(10830.4014, grad_fn=<NegBackward0>) tensor(10830.4023, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10830.3994140625
tensor(10830.4014, grad_fn=<NegBackward0>) tensor(10830.3994, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10830.4013671875
tensor(10830.3994, grad_fn=<NegBackward0>) tensor(10830.4014, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10830.3955078125
tensor(10830.3994, grad_fn=<NegBackward0>) tensor(10830.3955, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10830.3935546875
tensor(10830.3955, grad_fn=<NegBackward0>) tensor(10830.3936, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10830.392578125
tensor(10830.3936, grad_fn=<NegBackward0>) tensor(10830.3926, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10830.390625
tensor(10830.3926, grad_fn=<NegBackward0>) tensor(10830.3906, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10830.38671875
tensor(10830.3906, grad_fn=<NegBackward0>) tensor(10830.3867, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10830.38671875
tensor(10830.3867, grad_fn=<NegBackward0>) tensor(10830.3867, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10830.384765625
tensor(10830.3867, grad_fn=<NegBackward0>) tensor(10830.3848, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10830.39453125
tensor(10830.3848, grad_fn=<NegBackward0>) tensor(10830.3945, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10830.3818359375
tensor(10830.3848, grad_fn=<NegBackward0>) tensor(10830.3818, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10830.3818359375
tensor(10830.3818, grad_fn=<NegBackward0>) tensor(10830.3818, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10830.380859375
tensor(10830.3818, grad_fn=<NegBackward0>) tensor(10830.3809, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10830.4189453125
tensor(10830.3809, grad_fn=<NegBackward0>) tensor(10830.4189, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10830.380859375
tensor(10830.3809, grad_fn=<NegBackward0>) tensor(10830.3809, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10830.3818359375
tensor(10830.3809, grad_fn=<NegBackward0>) tensor(10830.3818, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -10830.3798828125
tensor(10830.3809, grad_fn=<NegBackward0>) tensor(10830.3799, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10830.3818359375
tensor(10830.3799, grad_fn=<NegBackward0>) tensor(10830.3818, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10830.380859375
tensor(10830.3799, grad_fn=<NegBackward0>) tensor(10830.3809, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -10830.3798828125
tensor(10830.3799, grad_fn=<NegBackward0>) tensor(10830.3799, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10830.37890625
tensor(10830.3799, grad_fn=<NegBackward0>) tensor(10830.3789, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10830.3798828125
tensor(10830.3789, grad_fn=<NegBackward0>) tensor(10830.3799, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10830.3984375
tensor(10830.3789, grad_fn=<NegBackward0>) tensor(10830.3984, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -10830.380859375
tensor(10830.3789, grad_fn=<NegBackward0>) tensor(10830.3809, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -10830.3798828125
tensor(10830.3789, grad_fn=<NegBackward0>) tensor(10830.3799, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -10830.37890625
tensor(10830.3789, grad_fn=<NegBackward0>) tensor(10830.3789, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10830.37890625
tensor(10830.3789, grad_fn=<NegBackward0>) tensor(10830.3789, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10830.37890625
tensor(10830.3789, grad_fn=<NegBackward0>) tensor(10830.3789, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10830.3798828125
tensor(10830.3789, grad_fn=<NegBackward0>) tensor(10830.3799, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10830.3798828125
tensor(10830.3789, grad_fn=<NegBackward0>) tensor(10830.3799, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -10830.3798828125
tensor(10830.3789, grad_fn=<NegBackward0>) tensor(10830.3799, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -10830.3798828125
tensor(10830.3789, grad_fn=<NegBackward0>) tensor(10830.3799, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -10830.3798828125
tensor(10830.3789, grad_fn=<NegBackward0>) tensor(10830.3799, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[0.7239, 0.2761],
        [0.2419, 0.7581]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4659, 0.5341], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1959, 0.0972],
         [0.6770, 0.2562]],

        [[0.6693, 0.0880],
         [0.5558, 0.5063]],

        [[0.6561, 0.0954],
         [0.5365, 0.5165]],

        [[0.6401, 0.1030],
         [0.7300, 0.6510]],

        [[0.6423, 0.0998],
         [0.5166, 0.6220]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 1
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824124176797128
time is 4
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 94
Adjusted Rand Index: 0.7721212121212121
Global Adjusted Rand Index: 0.8985004415265123
Average Adjusted Rand Index: 0.8990672265260148
[0.010909073640315427, 0.8985004415265123] [0.16073343878247107, 0.8990672265260148] [10969.234375, 10830.3798828125]
-------------------------------------
This iteration is 31
True Objective function: Loss = -11000.073158844172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20285.43359375
inf tensor(20285.4336, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11110.396484375
tensor(20285.4336, grad_fn=<NegBackward0>) tensor(11110.3965, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11108.5009765625
tensor(11110.3965, grad_fn=<NegBackward0>) tensor(11108.5010, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11106.2392578125
tensor(11108.5010, grad_fn=<NegBackward0>) tensor(11106.2393, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11105.4150390625
tensor(11106.2393, grad_fn=<NegBackward0>) tensor(11105.4150, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11104.685546875
tensor(11105.4150, grad_fn=<NegBackward0>) tensor(11104.6855, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11104.2060546875
tensor(11104.6855, grad_fn=<NegBackward0>) tensor(11104.2061, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11103.7529296875
tensor(11104.2061, grad_fn=<NegBackward0>) tensor(11103.7529, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11103.4609375
tensor(11103.7529, grad_fn=<NegBackward0>) tensor(11103.4609, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11103.2373046875
tensor(11103.4609, grad_fn=<NegBackward0>) tensor(11103.2373, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11103.052734375
tensor(11103.2373, grad_fn=<NegBackward0>) tensor(11103.0527, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11102.9169921875
tensor(11103.0527, grad_fn=<NegBackward0>) tensor(11102.9170, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11102.8017578125
tensor(11102.9170, grad_fn=<NegBackward0>) tensor(11102.8018, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11102.6953125
tensor(11102.8018, grad_fn=<NegBackward0>) tensor(11102.6953, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11102.5908203125
tensor(11102.6953, grad_fn=<NegBackward0>) tensor(11102.5908, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11102.5
tensor(11102.5908, grad_fn=<NegBackward0>) tensor(11102.5000, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11102.3935546875
tensor(11102.5000, grad_fn=<NegBackward0>) tensor(11102.3936, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11102.2626953125
tensor(11102.3936, grad_fn=<NegBackward0>) tensor(11102.2627, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11102.123046875
tensor(11102.2627, grad_fn=<NegBackward0>) tensor(11102.1230, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11102.0068359375
tensor(11102.1230, grad_fn=<NegBackward0>) tensor(11102.0068, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11101.921875
tensor(11102.0068, grad_fn=<NegBackward0>) tensor(11101.9219, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11101.865234375
tensor(11101.9219, grad_fn=<NegBackward0>) tensor(11101.8652, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11101.822265625
tensor(11101.8652, grad_fn=<NegBackward0>) tensor(11101.8223, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11101.79296875
tensor(11101.8223, grad_fn=<NegBackward0>) tensor(11101.7930, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11101.771484375
tensor(11101.7930, grad_fn=<NegBackward0>) tensor(11101.7715, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11101.7529296875
tensor(11101.7715, grad_fn=<NegBackward0>) tensor(11101.7529, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11101.73828125
tensor(11101.7529, grad_fn=<NegBackward0>) tensor(11101.7383, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11101.7275390625
tensor(11101.7383, grad_fn=<NegBackward0>) tensor(11101.7275, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11101.716796875
tensor(11101.7275, grad_fn=<NegBackward0>) tensor(11101.7168, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11101.7119140625
tensor(11101.7168, grad_fn=<NegBackward0>) tensor(11101.7119, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11101.703125
tensor(11101.7119, grad_fn=<NegBackward0>) tensor(11101.7031, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11101.697265625
tensor(11101.7031, grad_fn=<NegBackward0>) tensor(11101.6973, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11101.69140625
tensor(11101.6973, grad_fn=<NegBackward0>) tensor(11101.6914, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11101.6884765625
tensor(11101.6914, grad_fn=<NegBackward0>) tensor(11101.6885, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11101.68359375
tensor(11101.6885, grad_fn=<NegBackward0>) tensor(11101.6836, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11101.6806640625
tensor(11101.6836, grad_fn=<NegBackward0>) tensor(11101.6807, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11101.6767578125
tensor(11101.6807, grad_fn=<NegBackward0>) tensor(11101.6768, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11101.673828125
tensor(11101.6768, grad_fn=<NegBackward0>) tensor(11101.6738, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11101.6708984375
tensor(11101.6738, grad_fn=<NegBackward0>) tensor(11101.6709, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11101.66796875
tensor(11101.6709, grad_fn=<NegBackward0>) tensor(11101.6680, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11101.6669921875
tensor(11101.6680, grad_fn=<NegBackward0>) tensor(11101.6670, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11101.6650390625
tensor(11101.6670, grad_fn=<NegBackward0>) tensor(11101.6650, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11101.6630859375
tensor(11101.6650, grad_fn=<NegBackward0>) tensor(11101.6631, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11101.6611328125
tensor(11101.6631, grad_fn=<NegBackward0>) tensor(11101.6611, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11101.6591796875
tensor(11101.6611, grad_fn=<NegBackward0>) tensor(11101.6592, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11101.658203125
tensor(11101.6592, grad_fn=<NegBackward0>) tensor(11101.6582, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11101.658203125
tensor(11101.6582, grad_fn=<NegBackward0>) tensor(11101.6582, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11101.65625
tensor(11101.6582, grad_fn=<NegBackward0>) tensor(11101.6562, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11101.6552734375
tensor(11101.6562, grad_fn=<NegBackward0>) tensor(11101.6553, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11101.654296875
tensor(11101.6553, grad_fn=<NegBackward0>) tensor(11101.6543, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11101.6552734375
tensor(11101.6543, grad_fn=<NegBackward0>) tensor(11101.6553, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11101.65234375
tensor(11101.6543, grad_fn=<NegBackward0>) tensor(11101.6523, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11101.6513671875
tensor(11101.6523, grad_fn=<NegBackward0>) tensor(11101.6514, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11101.6513671875
tensor(11101.6514, grad_fn=<NegBackward0>) tensor(11101.6514, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11101.650390625
tensor(11101.6514, grad_fn=<NegBackward0>) tensor(11101.6504, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11101.650390625
tensor(11101.6504, grad_fn=<NegBackward0>) tensor(11101.6504, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11101.6494140625
tensor(11101.6504, grad_fn=<NegBackward0>) tensor(11101.6494, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11101.6484375
tensor(11101.6494, grad_fn=<NegBackward0>) tensor(11101.6484, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11101.6484375
tensor(11101.6484, grad_fn=<NegBackward0>) tensor(11101.6484, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11101.6484375
tensor(11101.6484, grad_fn=<NegBackward0>) tensor(11101.6484, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11101.646484375
tensor(11101.6484, grad_fn=<NegBackward0>) tensor(11101.6465, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11101.6474609375
tensor(11101.6465, grad_fn=<NegBackward0>) tensor(11101.6475, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11101.646484375
tensor(11101.6465, grad_fn=<NegBackward0>) tensor(11101.6465, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11101.6474609375
tensor(11101.6465, grad_fn=<NegBackward0>) tensor(11101.6475, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11101.6455078125
tensor(11101.6465, grad_fn=<NegBackward0>) tensor(11101.6455, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11101.64453125
tensor(11101.6455, grad_fn=<NegBackward0>) tensor(11101.6445, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11101.6455078125
tensor(11101.6445, grad_fn=<NegBackward0>) tensor(11101.6455, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11101.64453125
tensor(11101.6445, grad_fn=<NegBackward0>) tensor(11101.6445, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11101.6435546875
tensor(11101.6445, grad_fn=<NegBackward0>) tensor(11101.6436, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11101.64453125
tensor(11101.6436, grad_fn=<NegBackward0>) tensor(11101.6445, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11101.6435546875
tensor(11101.6436, grad_fn=<NegBackward0>) tensor(11101.6436, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11101.64453125
tensor(11101.6436, grad_fn=<NegBackward0>) tensor(11101.6445, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11101.64453125
tensor(11101.6436, grad_fn=<NegBackward0>) tensor(11101.6445, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11101.6533203125
tensor(11101.6436, grad_fn=<NegBackward0>) tensor(11101.6533, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11101.6435546875
tensor(11101.6436, grad_fn=<NegBackward0>) tensor(11101.6436, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11101.6435546875
tensor(11101.6436, grad_fn=<NegBackward0>) tensor(11101.6436, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11101.64453125
tensor(11101.6436, grad_fn=<NegBackward0>) tensor(11101.6445, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11101.642578125
tensor(11101.6436, grad_fn=<NegBackward0>) tensor(11101.6426, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11101.6416015625
tensor(11101.6426, grad_fn=<NegBackward0>) tensor(11101.6416, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11101.642578125
tensor(11101.6416, grad_fn=<NegBackward0>) tensor(11101.6426, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11101.640625
tensor(11101.6416, grad_fn=<NegBackward0>) tensor(11101.6406, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11101.646484375
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6465, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11101.640625
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6406, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11101.6416015625
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6416, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11101.67578125
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6758, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11101.640625
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6406, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11101.640625
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6406, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11101.640625
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6406, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11101.640625
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6406, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11101.640625
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6406, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11101.646484375
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6465, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11101.640625
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6406, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11101.6787109375
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6787, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11101.6396484375
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6396, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11101.6416015625
tensor(11101.6396, grad_fn=<NegBackward0>) tensor(11101.6416, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11101.6396484375
tensor(11101.6396, grad_fn=<NegBackward0>) tensor(11101.6396, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11101.6396484375
tensor(11101.6396, grad_fn=<NegBackward0>) tensor(11101.6396, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11101.6396484375
tensor(11101.6396, grad_fn=<NegBackward0>) tensor(11101.6396, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11101.6396484375
tensor(11101.6396, grad_fn=<NegBackward0>) tensor(11101.6396, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11101.640625
tensor(11101.6396, grad_fn=<NegBackward0>) tensor(11101.6406, grad_fn=<NegBackward0>)
1
pi: tensor([[9.9995e-01, 4.6729e-05],
        [7.8373e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0321, 0.9679], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3769, 0.2435],
         [0.5622, 0.1620]],

        [[0.6916, 0.2649],
         [0.6795, 0.5789]],

        [[0.7000, 0.1974],
         [0.6091, 0.6151]],

        [[0.6916, 0.1746],
         [0.6719, 0.7042]],

        [[0.5352, 0.2050],
         [0.6201, 0.5071]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.002764517368444376
Global Adjusted Rand Index: 0.0005154613025668345
Average Adjusted Rand Index: -0.0006832245135254071
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19912.47265625
inf tensor(19912.4727, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11110.8046875
tensor(19912.4727, grad_fn=<NegBackward0>) tensor(11110.8047, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11109.0615234375
tensor(11110.8047, grad_fn=<NegBackward0>) tensor(11109.0615, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11105.8505859375
tensor(11109.0615, grad_fn=<NegBackward0>) tensor(11105.8506, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11104.951171875
tensor(11105.8506, grad_fn=<NegBackward0>) tensor(11104.9512, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11104.4931640625
tensor(11104.9512, grad_fn=<NegBackward0>) tensor(11104.4932, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11104.2744140625
tensor(11104.4932, grad_fn=<NegBackward0>) tensor(11104.2744, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11104.1171875
tensor(11104.2744, grad_fn=<NegBackward0>) tensor(11104.1172, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11103.76171875
tensor(11104.1172, grad_fn=<NegBackward0>) tensor(11103.7617, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11103.3388671875
tensor(11103.7617, grad_fn=<NegBackward0>) tensor(11103.3389, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11103.107421875
tensor(11103.3389, grad_fn=<NegBackward0>) tensor(11103.1074, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11102.9453125
tensor(11103.1074, grad_fn=<NegBackward0>) tensor(11102.9453, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11102.802734375
tensor(11102.9453, grad_fn=<NegBackward0>) tensor(11102.8027, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11102.7138671875
tensor(11102.8027, grad_fn=<NegBackward0>) tensor(11102.7139, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11102.6455078125
tensor(11102.7139, grad_fn=<NegBackward0>) tensor(11102.6455, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11102.58984375
tensor(11102.6455, grad_fn=<NegBackward0>) tensor(11102.5898, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11102.5361328125
tensor(11102.5898, grad_fn=<NegBackward0>) tensor(11102.5361, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11102.4716796875
tensor(11102.5361, grad_fn=<NegBackward0>) tensor(11102.4717, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11102.3779296875
tensor(11102.4717, grad_fn=<NegBackward0>) tensor(11102.3779, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11102.2333984375
tensor(11102.3779, grad_fn=<NegBackward0>) tensor(11102.2334, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11102.0732421875
tensor(11102.2334, grad_fn=<NegBackward0>) tensor(11102.0732, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11101.9521484375
tensor(11102.0732, grad_fn=<NegBackward0>) tensor(11101.9521, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11101.8671875
tensor(11101.9521, grad_fn=<NegBackward0>) tensor(11101.8672, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11101.81640625
tensor(11101.8672, grad_fn=<NegBackward0>) tensor(11101.8164, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11101.78125
tensor(11101.8164, grad_fn=<NegBackward0>) tensor(11101.7812, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11101.7568359375
tensor(11101.7812, grad_fn=<NegBackward0>) tensor(11101.7568, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11101.73828125
tensor(11101.7568, grad_fn=<NegBackward0>) tensor(11101.7383, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11101.7236328125
tensor(11101.7383, grad_fn=<NegBackward0>) tensor(11101.7236, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11101.7109375
tensor(11101.7236, grad_fn=<NegBackward0>) tensor(11101.7109, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11101.705078125
tensor(11101.7109, grad_fn=<NegBackward0>) tensor(11101.7051, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11101.6962890625
tensor(11101.7051, grad_fn=<NegBackward0>) tensor(11101.6963, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11101.69140625
tensor(11101.6963, grad_fn=<NegBackward0>) tensor(11101.6914, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11101.685546875
tensor(11101.6914, grad_fn=<NegBackward0>) tensor(11101.6855, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11101.6806640625
tensor(11101.6855, grad_fn=<NegBackward0>) tensor(11101.6807, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11101.6767578125
tensor(11101.6807, grad_fn=<NegBackward0>) tensor(11101.6768, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11101.6748046875
tensor(11101.6768, grad_fn=<NegBackward0>) tensor(11101.6748, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11101.6708984375
tensor(11101.6748, grad_fn=<NegBackward0>) tensor(11101.6709, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11101.66796875
tensor(11101.6709, grad_fn=<NegBackward0>) tensor(11101.6680, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11101.6669921875
tensor(11101.6680, grad_fn=<NegBackward0>) tensor(11101.6670, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11101.6650390625
tensor(11101.6670, grad_fn=<NegBackward0>) tensor(11101.6650, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11101.662109375
tensor(11101.6650, grad_fn=<NegBackward0>) tensor(11101.6621, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11101.66015625
tensor(11101.6621, grad_fn=<NegBackward0>) tensor(11101.6602, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11101.6591796875
tensor(11101.6602, grad_fn=<NegBackward0>) tensor(11101.6592, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11101.65625
tensor(11101.6592, grad_fn=<NegBackward0>) tensor(11101.6562, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11101.6552734375
tensor(11101.6562, grad_fn=<NegBackward0>) tensor(11101.6553, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11101.6552734375
tensor(11101.6553, grad_fn=<NegBackward0>) tensor(11101.6553, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11101.6533203125
tensor(11101.6553, grad_fn=<NegBackward0>) tensor(11101.6533, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11101.6533203125
tensor(11101.6533, grad_fn=<NegBackward0>) tensor(11101.6533, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11101.6513671875
tensor(11101.6533, grad_fn=<NegBackward0>) tensor(11101.6514, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11101.6484375
tensor(11101.6514, grad_fn=<NegBackward0>) tensor(11101.6484, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11101.650390625
tensor(11101.6484, grad_fn=<NegBackward0>) tensor(11101.6504, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11101.6474609375
tensor(11101.6484, grad_fn=<NegBackward0>) tensor(11101.6475, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11101.6484375
tensor(11101.6475, grad_fn=<NegBackward0>) tensor(11101.6484, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11101.6484375
tensor(11101.6475, grad_fn=<NegBackward0>) tensor(11101.6484, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -11101.646484375
tensor(11101.6475, grad_fn=<NegBackward0>) tensor(11101.6465, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11101.6474609375
tensor(11101.6465, grad_fn=<NegBackward0>) tensor(11101.6475, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11101.646484375
tensor(11101.6465, grad_fn=<NegBackward0>) tensor(11101.6465, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11101.6455078125
tensor(11101.6465, grad_fn=<NegBackward0>) tensor(11101.6455, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11101.6455078125
tensor(11101.6455, grad_fn=<NegBackward0>) tensor(11101.6455, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11101.64453125
tensor(11101.6455, grad_fn=<NegBackward0>) tensor(11101.6445, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11101.6435546875
tensor(11101.6445, grad_fn=<NegBackward0>) tensor(11101.6436, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11101.6455078125
tensor(11101.6436, grad_fn=<NegBackward0>) tensor(11101.6455, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11101.642578125
tensor(11101.6436, grad_fn=<NegBackward0>) tensor(11101.6426, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11101.6435546875
tensor(11101.6426, grad_fn=<NegBackward0>) tensor(11101.6436, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11101.6435546875
tensor(11101.6426, grad_fn=<NegBackward0>) tensor(11101.6436, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11101.640625
tensor(11101.6426, grad_fn=<NegBackward0>) tensor(11101.6406, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11101.642578125
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6426, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11101.6416015625
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6416, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11101.642578125
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6426, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -11101.6416015625
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6416, grad_fn=<NegBackward0>)
4
Iteration 7000: Loss = -11101.640625
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6406, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11101.640625
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6406, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11101.642578125
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6426, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11101.6416015625
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6416, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11101.6416015625
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6416, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11101.6611328125
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6611, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -11101.640625
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6406, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11101.6455078125
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6455, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11101.638671875
tensor(11101.6406, grad_fn=<NegBackward0>) tensor(11101.6387, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11101.6455078125
tensor(11101.6387, grad_fn=<NegBackward0>) tensor(11101.6455, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11101.638671875
tensor(11101.6387, grad_fn=<NegBackward0>) tensor(11101.6387, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11101.669921875
tensor(11101.6387, grad_fn=<NegBackward0>) tensor(11101.6699, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11101.6396484375
tensor(11101.6387, grad_fn=<NegBackward0>) tensor(11101.6396, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11101.759765625
tensor(11101.6387, grad_fn=<NegBackward0>) tensor(11101.7598, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11101.6396484375
tensor(11101.6387, grad_fn=<NegBackward0>) tensor(11101.6396, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -11101.638671875
tensor(11101.6387, grad_fn=<NegBackward0>) tensor(11101.6387, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11101.6396484375
tensor(11101.6387, grad_fn=<NegBackward0>) tensor(11101.6396, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11101.6376953125
tensor(11101.6387, grad_fn=<NegBackward0>) tensor(11101.6377, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11101.8017578125
tensor(11101.6377, grad_fn=<NegBackward0>) tensor(11101.8018, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11101.638671875
tensor(11101.6377, grad_fn=<NegBackward0>) tensor(11101.6387, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -11101.6494140625
tensor(11101.6377, grad_fn=<NegBackward0>) tensor(11101.6494, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -11101.6376953125
tensor(11101.6377, grad_fn=<NegBackward0>) tensor(11101.6377, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11101.6396484375
tensor(11101.6377, grad_fn=<NegBackward0>) tensor(11101.6396, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11101.6396484375
tensor(11101.6377, grad_fn=<NegBackward0>) tensor(11101.6396, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11101.6376953125
tensor(11101.6377, grad_fn=<NegBackward0>) tensor(11101.6377, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11101.6376953125
tensor(11101.6377, grad_fn=<NegBackward0>) tensor(11101.6377, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11101.6376953125
tensor(11101.6377, grad_fn=<NegBackward0>) tensor(11101.6377, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11101.6376953125
tensor(11101.6377, grad_fn=<NegBackward0>) tensor(11101.6377, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11101.638671875
tensor(11101.6377, grad_fn=<NegBackward0>) tensor(11101.6387, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11101.6416015625
tensor(11101.6377, grad_fn=<NegBackward0>) tensor(11101.6416, grad_fn=<NegBackward0>)
2
pi: tensor([[9.9999e-01, 7.6517e-06],
        [4.5160e-05, 9.9995e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9679, 0.0321], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1620, 0.2434],
         [0.6623, 0.3771]],

        [[0.5880, 0.2648],
         [0.6419, 0.7273]],

        [[0.5313, 0.1974],
         [0.6276, 0.7127]],

        [[0.6583, 0.1746],
         [0.7196, 0.7109]],

        [[0.6845, 0.2050],
         [0.7109, 0.6535]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
Global Adjusted Rand Index: 0.0005154613025668345
Average Adjusted Rand Index: -0.0006832245135254071
[0.0005154613025668345, 0.0005154613025668345] [-0.0006832245135254071, -0.0006832245135254071] [11101.638671875, 11101.6376953125]
-------------------------------------
This iteration is 32
True Objective function: Loss = -10990.332944993004
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19851.28515625
inf tensor(19851.2852, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11069.2060546875
tensor(19851.2852, grad_fn=<NegBackward0>) tensor(11069.2061, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11068.6298828125
tensor(11069.2061, grad_fn=<NegBackward0>) tensor(11068.6299, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11068.1533203125
tensor(11068.6299, grad_fn=<NegBackward0>) tensor(11068.1533, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11067.4638671875
tensor(11068.1533, grad_fn=<NegBackward0>) tensor(11067.4639, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11065.2138671875
tensor(11067.4639, grad_fn=<NegBackward0>) tensor(11065.2139, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11061.650390625
tensor(11065.2139, grad_fn=<NegBackward0>) tensor(11061.6504, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11060.22265625
tensor(11061.6504, grad_fn=<NegBackward0>) tensor(11060.2227, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11059.7841796875
tensor(11060.2227, grad_fn=<NegBackward0>) tensor(11059.7842, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11059.6337890625
tensor(11059.7842, grad_fn=<NegBackward0>) tensor(11059.6338, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11059.548828125
tensor(11059.6338, grad_fn=<NegBackward0>) tensor(11059.5488, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11059.4580078125
tensor(11059.5488, grad_fn=<NegBackward0>) tensor(11059.4580, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11059.400390625
tensor(11059.4580, grad_fn=<NegBackward0>) tensor(11059.4004, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11059.3515625
tensor(11059.4004, grad_fn=<NegBackward0>) tensor(11059.3516, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11059.3037109375
tensor(11059.3516, grad_fn=<NegBackward0>) tensor(11059.3037, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11059.2626953125
tensor(11059.3037, grad_fn=<NegBackward0>) tensor(11059.2627, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11059.216796875
tensor(11059.2627, grad_fn=<NegBackward0>) tensor(11059.2168, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11059.16015625
tensor(11059.2168, grad_fn=<NegBackward0>) tensor(11059.1602, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11059.0751953125
tensor(11059.1602, grad_fn=<NegBackward0>) tensor(11059.0752, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11058.9736328125
tensor(11059.0752, grad_fn=<NegBackward0>) tensor(11058.9736, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11058.8583984375
tensor(11058.9736, grad_fn=<NegBackward0>) tensor(11058.8584, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11058.560546875
tensor(11058.8584, grad_fn=<NegBackward0>) tensor(11058.5605, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10954.869140625
tensor(11058.5605, grad_fn=<NegBackward0>) tensor(10954.8691, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10952.5390625
tensor(10954.8691, grad_fn=<NegBackward0>) tensor(10952.5391, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10951.7353515625
tensor(10952.5391, grad_fn=<NegBackward0>) tensor(10951.7354, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10951.6923828125
tensor(10951.7354, grad_fn=<NegBackward0>) tensor(10951.6924, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10951.669921875
tensor(10951.6924, grad_fn=<NegBackward0>) tensor(10951.6699, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10951.4814453125
tensor(10951.6699, grad_fn=<NegBackward0>) tensor(10951.4814, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10951.4560546875
tensor(10951.4814, grad_fn=<NegBackward0>) tensor(10951.4561, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10950.5087890625
tensor(10951.4561, grad_fn=<NegBackward0>) tensor(10950.5088, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10950.4892578125
tensor(10950.5088, grad_fn=<NegBackward0>) tensor(10950.4893, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10950.4873046875
tensor(10950.4893, grad_fn=<NegBackward0>) tensor(10950.4873, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10950.484375
tensor(10950.4873, grad_fn=<NegBackward0>) tensor(10950.4844, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10950.482421875
tensor(10950.4844, grad_fn=<NegBackward0>) tensor(10950.4824, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10950.4794921875
tensor(10950.4824, grad_fn=<NegBackward0>) tensor(10950.4795, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10950.4775390625
tensor(10950.4795, grad_fn=<NegBackward0>) tensor(10950.4775, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10950.4775390625
tensor(10950.4775, grad_fn=<NegBackward0>) tensor(10950.4775, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10950.4765625
tensor(10950.4775, grad_fn=<NegBackward0>) tensor(10950.4766, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10950.4755859375
tensor(10950.4766, grad_fn=<NegBackward0>) tensor(10950.4756, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10950.4755859375
tensor(10950.4756, grad_fn=<NegBackward0>) tensor(10950.4756, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10950.4755859375
tensor(10950.4756, grad_fn=<NegBackward0>) tensor(10950.4756, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10950.4736328125
tensor(10950.4756, grad_fn=<NegBackward0>) tensor(10950.4736, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10950.4765625
tensor(10950.4736, grad_fn=<NegBackward0>) tensor(10950.4766, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10950.47265625
tensor(10950.4736, grad_fn=<NegBackward0>) tensor(10950.4727, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10950.470703125
tensor(10950.4727, grad_fn=<NegBackward0>) tensor(10950.4707, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10950.46875
tensor(10950.4707, grad_fn=<NegBackward0>) tensor(10950.4688, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10950.4697265625
tensor(10950.4688, grad_fn=<NegBackward0>) tensor(10950.4697, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -10950.4677734375
tensor(10950.4688, grad_fn=<NegBackward0>) tensor(10950.4678, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10950.474609375
tensor(10950.4678, grad_fn=<NegBackward0>) tensor(10950.4746, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10950.46875
tensor(10950.4678, grad_fn=<NegBackward0>) tensor(10950.4688, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -10950.4677734375
tensor(10950.4678, grad_fn=<NegBackward0>) tensor(10950.4678, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10950.4658203125
tensor(10950.4678, grad_fn=<NegBackward0>) tensor(10950.4658, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10950.466796875
tensor(10950.4658, grad_fn=<NegBackward0>) tensor(10950.4668, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10950.4658203125
tensor(10950.4658, grad_fn=<NegBackward0>) tensor(10950.4658, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10950.4658203125
tensor(10950.4658, grad_fn=<NegBackward0>) tensor(10950.4658, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10950.4658203125
tensor(10950.4658, grad_fn=<NegBackward0>) tensor(10950.4658, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10950.466796875
tensor(10950.4658, grad_fn=<NegBackward0>) tensor(10950.4668, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10950.4658203125
tensor(10950.4658, grad_fn=<NegBackward0>) tensor(10950.4658, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10950.466796875
tensor(10950.4658, grad_fn=<NegBackward0>) tensor(10950.4668, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10950.4658203125
tensor(10950.4658, grad_fn=<NegBackward0>) tensor(10950.4658, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10950.4658203125
tensor(10950.4658, grad_fn=<NegBackward0>) tensor(10950.4658, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10950.466796875
tensor(10950.4658, grad_fn=<NegBackward0>) tensor(10950.4668, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10950.4658203125
tensor(10950.4658, grad_fn=<NegBackward0>) tensor(10950.4658, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10950.46484375
tensor(10950.4658, grad_fn=<NegBackward0>) tensor(10950.4648, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10950.46484375
tensor(10950.4648, grad_fn=<NegBackward0>) tensor(10950.4648, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10950.4658203125
tensor(10950.4648, grad_fn=<NegBackward0>) tensor(10950.4658, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10950.4658203125
tensor(10950.4648, grad_fn=<NegBackward0>) tensor(10950.4658, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -10950.4658203125
tensor(10950.4648, grad_fn=<NegBackward0>) tensor(10950.4658, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -10950.46484375
tensor(10950.4648, grad_fn=<NegBackward0>) tensor(10950.4648, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10950.4658203125
tensor(10950.4648, grad_fn=<NegBackward0>) tensor(10950.4658, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10950.5673828125
tensor(10950.4648, grad_fn=<NegBackward0>) tensor(10950.5674, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -10950.466796875
tensor(10950.4648, grad_fn=<NegBackward0>) tensor(10950.4668, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -10950.4697265625
tensor(10950.4648, grad_fn=<NegBackward0>) tensor(10950.4697, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -10950.4658203125
tensor(10950.4648, grad_fn=<NegBackward0>) tensor(10950.4658, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7300 due to no improvement.
pi: tensor([[0.7780, 0.2220],
        [0.2188, 0.7812]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4048, 0.5952], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1979, 0.1074],
         [0.5020, 0.2447]],

        [[0.5794, 0.0946],
         [0.5962, 0.5132]],

        [[0.6631, 0.1087],
         [0.5652, 0.5288]],

        [[0.5568, 0.1040],
         [0.6901, 0.6499]],

        [[0.5553, 0.1078],
         [0.7217, 0.6716]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8447743642510657
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824165642894751
time is 3
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080650161849864
time is 4
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080808080808081
Global Adjusted Rand Index: 0.8609044279038179
Average Adjusted Rand Index: 0.8606669117248302
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23682.833984375
inf tensor(23682.8340, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11069.79296875
tensor(23682.8340, grad_fn=<NegBackward0>) tensor(11069.7930, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11068.8701171875
tensor(11069.7930, grad_fn=<NegBackward0>) tensor(11068.8701, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11068.2861328125
tensor(11068.8701, grad_fn=<NegBackward0>) tensor(11068.2861, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11067.89453125
tensor(11068.2861, grad_fn=<NegBackward0>) tensor(11067.8945, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11067.029296875
tensor(11067.8945, grad_fn=<NegBackward0>) tensor(11067.0293, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11065.9716796875
tensor(11067.0293, grad_fn=<NegBackward0>) tensor(11065.9717, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11062.533203125
tensor(11065.9717, grad_fn=<NegBackward0>) tensor(11062.5332, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11061.380859375
tensor(11062.5332, grad_fn=<NegBackward0>) tensor(11061.3809, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11060.8251953125
tensor(11061.3809, grad_fn=<NegBackward0>) tensor(11060.8252, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11060.43359375
tensor(11060.8252, grad_fn=<NegBackward0>) tensor(11060.4336, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11060.162109375
tensor(11060.4336, grad_fn=<NegBackward0>) tensor(11060.1621, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11059.9521484375
tensor(11060.1621, grad_fn=<NegBackward0>) tensor(11059.9521, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11059.8017578125
tensor(11059.9521, grad_fn=<NegBackward0>) tensor(11059.8018, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11059.708984375
tensor(11059.8018, grad_fn=<NegBackward0>) tensor(11059.7090, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11059.6376953125
tensor(11059.7090, grad_fn=<NegBackward0>) tensor(11059.6377, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11059.568359375
tensor(11059.6377, grad_fn=<NegBackward0>) tensor(11059.5684, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11059.5166015625
tensor(11059.5684, grad_fn=<NegBackward0>) tensor(11059.5166, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11059.45703125
tensor(11059.5166, grad_fn=<NegBackward0>) tensor(11059.4570, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11059.4169921875
tensor(11059.4570, grad_fn=<NegBackward0>) tensor(11059.4170, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11059.3857421875
tensor(11059.4170, grad_fn=<NegBackward0>) tensor(11059.3857, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11059.3369140625
tensor(11059.3857, grad_fn=<NegBackward0>) tensor(11059.3369, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11059.2900390625
tensor(11059.3369, grad_fn=<NegBackward0>) tensor(11059.2900, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11059.244140625
tensor(11059.2900, grad_fn=<NegBackward0>) tensor(11059.2441, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11059.1669921875
tensor(11059.2441, grad_fn=<NegBackward0>) tensor(11059.1670, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11059.0830078125
tensor(11059.1670, grad_fn=<NegBackward0>) tensor(11059.0830, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11058.990234375
tensor(11059.0830, grad_fn=<NegBackward0>) tensor(11058.9902, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11058.6708984375
tensor(11058.9902, grad_fn=<NegBackward0>) tensor(11058.6709, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10955.3896484375
tensor(11058.6709, grad_fn=<NegBackward0>) tensor(10955.3896, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10952.9482421875
tensor(10955.3896, grad_fn=<NegBackward0>) tensor(10952.9482, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10952.6923828125
tensor(10952.9482, grad_fn=<NegBackward0>) tensor(10952.6924, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10952.6435546875
tensor(10952.6924, grad_fn=<NegBackward0>) tensor(10952.6436, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10951.958984375
tensor(10952.6436, grad_fn=<NegBackward0>) tensor(10951.9590, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10951.927734375
tensor(10951.9590, grad_fn=<NegBackward0>) tensor(10951.9277, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10951.91015625
tensor(10951.9277, grad_fn=<NegBackward0>) tensor(10951.9102, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10951.90234375
tensor(10951.9102, grad_fn=<NegBackward0>) tensor(10951.9023, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10951.896484375
tensor(10951.9023, grad_fn=<NegBackward0>) tensor(10951.8965, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10951.87890625
tensor(10951.8965, grad_fn=<NegBackward0>) tensor(10951.8789, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10951.62890625
tensor(10951.8789, grad_fn=<NegBackward0>) tensor(10951.6289, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10951.626953125
tensor(10951.6289, grad_fn=<NegBackward0>) tensor(10951.6270, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10951.623046875
tensor(10951.6270, grad_fn=<NegBackward0>) tensor(10951.6230, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10951.62109375
tensor(10951.6230, grad_fn=<NegBackward0>) tensor(10951.6211, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10951.619140625
tensor(10951.6211, grad_fn=<NegBackward0>) tensor(10951.6191, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10951.6171875
tensor(10951.6191, grad_fn=<NegBackward0>) tensor(10951.6172, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10951.6171875
tensor(10951.6172, grad_fn=<NegBackward0>) tensor(10951.6172, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10951.615234375
tensor(10951.6172, grad_fn=<NegBackward0>) tensor(10951.6152, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10951.6142578125
tensor(10951.6152, grad_fn=<NegBackward0>) tensor(10951.6143, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10951.6142578125
tensor(10951.6143, grad_fn=<NegBackward0>) tensor(10951.6143, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10951.61328125
tensor(10951.6143, grad_fn=<NegBackward0>) tensor(10951.6133, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10951.626953125
tensor(10951.6133, grad_fn=<NegBackward0>) tensor(10951.6270, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10951.61328125
tensor(10951.6133, grad_fn=<NegBackward0>) tensor(10951.6133, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10951.611328125
tensor(10951.6133, grad_fn=<NegBackward0>) tensor(10951.6113, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10951.6123046875
tensor(10951.6113, grad_fn=<NegBackward0>) tensor(10951.6123, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10951.607421875
tensor(10951.6113, grad_fn=<NegBackward0>) tensor(10951.6074, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10950.53515625
tensor(10951.6074, grad_fn=<NegBackward0>) tensor(10950.5352, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10950.474609375
tensor(10950.5352, grad_fn=<NegBackward0>) tensor(10950.4746, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10950.474609375
tensor(10950.4746, grad_fn=<NegBackward0>) tensor(10950.4746, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10950.4755859375
tensor(10950.4746, grad_fn=<NegBackward0>) tensor(10950.4756, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10950.47265625
tensor(10950.4746, grad_fn=<NegBackward0>) tensor(10950.4727, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10950.48046875
tensor(10950.4727, grad_fn=<NegBackward0>) tensor(10950.4805, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10950.474609375
tensor(10950.4727, grad_fn=<NegBackward0>) tensor(10950.4746, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -10950.4716796875
tensor(10950.4727, grad_fn=<NegBackward0>) tensor(10950.4717, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10950.470703125
tensor(10950.4717, grad_fn=<NegBackward0>) tensor(10950.4707, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10950.47265625
tensor(10950.4707, grad_fn=<NegBackward0>) tensor(10950.4727, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10950.4716796875
tensor(10950.4707, grad_fn=<NegBackward0>) tensor(10950.4717, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -10950.470703125
tensor(10950.4707, grad_fn=<NegBackward0>) tensor(10950.4707, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10950.4716796875
tensor(10950.4707, grad_fn=<NegBackward0>) tensor(10950.4717, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10950.4697265625
tensor(10950.4707, grad_fn=<NegBackward0>) tensor(10950.4697, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10950.4677734375
tensor(10950.4697, grad_fn=<NegBackward0>) tensor(10950.4678, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10950.4677734375
tensor(10950.4678, grad_fn=<NegBackward0>) tensor(10950.4678, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10950.4677734375
tensor(10950.4678, grad_fn=<NegBackward0>) tensor(10950.4678, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10950.4814453125
tensor(10950.4678, grad_fn=<NegBackward0>) tensor(10950.4814, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10950.466796875
tensor(10950.4678, grad_fn=<NegBackward0>) tensor(10950.4668, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10950.4658203125
tensor(10950.4668, grad_fn=<NegBackward0>) tensor(10950.4658, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10950.4658203125
tensor(10950.4658, grad_fn=<NegBackward0>) tensor(10950.4658, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10950.4736328125
tensor(10950.4658, grad_fn=<NegBackward0>) tensor(10950.4736, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10950.466796875
tensor(10950.4658, grad_fn=<NegBackward0>) tensor(10950.4668, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -10950.4677734375
tensor(10950.4658, grad_fn=<NegBackward0>) tensor(10950.4678, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -10950.466796875
tensor(10950.4658, grad_fn=<NegBackward0>) tensor(10950.4668, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -10950.4658203125
tensor(10950.4658, grad_fn=<NegBackward0>) tensor(10950.4658, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10950.4658203125
tensor(10950.4658, grad_fn=<NegBackward0>) tensor(10950.4658, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10950.4716796875
tensor(10950.4658, grad_fn=<NegBackward0>) tensor(10950.4717, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10950.46484375
tensor(10950.4658, grad_fn=<NegBackward0>) tensor(10950.4648, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10950.46484375
tensor(10950.4648, grad_fn=<NegBackward0>) tensor(10950.4648, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10950.4658203125
tensor(10950.4648, grad_fn=<NegBackward0>) tensor(10950.4658, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -10950.466796875
tensor(10950.4648, grad_fn=<NegBackward0>) tensor(10950.4668, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -10950.4658203125
tensor(10950.4648, grad_fn=<NegBackward0>) tensor(10950.4658, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -10950.484375
tensor(10950.4648, grad_fn=<NegBackward0>) tensor(10950.4844, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -10950.466796875
tensor(10950.4648, grad_fn=<NegBackward0>) tensor(10950.4668, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8800 due to no improvement.
pi: tensor([[0.7782, 0.2218],
        [0.2188, 0.7812]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4048, 0.5952], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1979, 0.1074],
         [0.5866, 0.2447]],

        [[0.6557, 0.0946],
         [0.7271, 0.5828]],

        [[0.5937, 0.1087],
         [0.5399, 0.7074]],

        [[0.5548, 0.1040],
         [0.6269, 0.6914]],

        [[0.7015, 0.1078],
         [0.6542, 0.7026]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8447743642510657
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824165642894751
time is 3
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080650161849864
time is 4
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080808080808081
Global Adjusted Rand Index: 0.8609044279038179
Average Adjusted Rand Index: 0.8606669117248302
[0.8609044279038179, 0.8609044279038179] [0.8606669117248302, 0.8606669117248302] [10950.4658203125, 10950.466796875]
-------------------------------------
This iteration is 33
True Objective function: Loss = -10925.987890049195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21551.30859375
inf tensor(21551.3086, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10999.076171875
tensor(21551.3086, grad_fn=<NegBackward0>) tensor(10999.0762, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10997.607421875
tensor(10999.0762, grad_fn=<NegBackward0>) tensor(10997.6074, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10996.98046875
tensor(10997.6074, grad_fn=<NegBackward0>) tensor(10996.9805, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10996.2802734375
tensor(10996.9805, grad_fn=<NegBackward0>) tensor(10996.2803, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10995.375
tensor(10996.2803, grad_fn=<NegBackward0>) tensor(10995.3750, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10994.9267578125
tensor(10995.3750, grad_fn=<NegBackward0>) tensor(10994.9268, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10994.74609375
tensor(10994.9268, grad_fn=<NegBackward0>) tensor(10994.7461, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10994.638671875
tensor(10994.7461, grad_fn=<NegBackward0>) tensor(10994.6387, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10994.56640625
tensor(10994.6387, grad_fn=<NegBackward0>) tensor(10994.5664, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10994.515625
tensor(10994.5664, grad_fn=<NegBackward0>) tensor(10994.5156, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10994.470703125
tensor(10994.5156, grad_fn=<NegBackward0>) tensor(10994.4707, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10994.4228515625
tensor(10994.4707, grad_fn=<NegBackward0>) tensor(10994.4229, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10994.2939453125
tensor(10994.4229, grad_fn=<NegBackward0>) tensor(10994.2939, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10994.0126953125
tensor(10994.2939, grad_fn=<NegBackward0>) tensor(10994.0127, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10993.9482421875
tensor(10994.0127, grad_fn=<NegBackward0>) tensor(10993.9482, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10993.91015625
tensor(10993.9482, grad_fn=<NegBackward0>) tensor(10993.9102, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10993.880859375
tensor(10993.9102, grad_fn=<NegBackward0>) tensor(10993.8809, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10993.8564453125
tensor(10993.8809, grad_fn=<NegBackward0>) tensor(10993.8564, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10993.8349609375
tensor(10993.8564, grad_fn=<NegBackward0>) tensor(10993.8350, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10993.8193359375
tensor(10993.8350, grad_fn=<NegBackward0>) tensor(10993.8193, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10993.8095703125
tensor(10993.8193, grad_fn=<NegBackward0>) tensor(10993.8096, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10993.8056640625
tensor(10993.8096, grad_fn=<NegBackward0>) tensor(10993.8057, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10993.80078125
tensor(10993.8057, grad_fn=<NegBackward0>) tensor(10993.8008, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10993.798828125
tensor(10993.8008, grad_fn=<NegBackward0>) tensor(10993.7988, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10993.7958984375
tensor(10993.7988, grad_fn=<NegBackward0>) tensor(10993.7959, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10993.7900390625
tensor(10993.7959, grad_fn=<NegBackward0>) tensor(10993.7900, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10993.7841796875
tensor(10993.7900, grad_fn=<NegBackward0>) tensor(10993.7842, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10993.77734375
tensor(10993.7842, grad_fn=<NegBackward0>) tensor(10993.7773, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10993.7626953125
tensor(10993.7773, grad_fn=<NegBackward0>) tensor(10993.7627, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10993.736328125
tensor(10993.7627, grad_fn=<NegBackward0>) tensor(10993.7363, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10993.66796875
tensor(10993.7363, grad_fn=<NegBackward0>) tensor(10993.6680, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10993.31640625
tensor(10993.6680, grad_fn=<NegBackward0>) tensor(10993.3164, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10915.3662109375
tensor(10993.3164, grad_fn=<NegBackward0>) tensor(10915.3662, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10873.671875
tensor(10915.3662, grad_fn=<NegBackward0>) tensor(10873.6719, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10873.5576171875
tensor(10873.6719, grad_fn=<NegBackward0>) tensor(10873.5576, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10873.5361328125
tensor(10873.5576, grad_fn=<NegBackward0>) tensor(10873.5361, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10873.5205078125
tensor(10873.5361, grad_fn=<NegBackward0>) tensor(10873.5205, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10873.5087890625
tensor(10873.5205, grad_fn=<NegBackward0>) tensor(10873.5088, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10873.50390625
tensor(10873.5088, grad_fn=<NegBackward0>) tensor(10873.5039, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10873.498046875
tensor(10873.5039, grad_fn=<NegBackward0>) tensor(10873.4980, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10873.494140625
tensor(10873.4980, grad_fn=<NegBackward0>) tensor(10873.4941, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10873.5
tensor(10873.4941, grad_fn=<NegBackward0>) tensor(10873.5000, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10873.4892578125
tensor(10873.4941, grad_fn=<NegBackward0>) tensor(10873.4893, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10873.48828125
tensor(10873.4893, grad_fn=<NegBackward0>) tensor(10873.4883, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10873.486328125
tensor(10873.4883, grad_fn=<NegBackward0>) tensor(10873.4863, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10873.4853515625
tensor(10873.4863, grad_fn=<NegBackward0>) tensor(10873.4854, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10873.486328125
tensor(10873.4854, grad_fn=<NegBackward0>) tensor(10873.4863, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10873.490234375
tensor(10873.4854, grad_fn=<NegBackward0>) tensor(10873.4902, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -10873.4833984375
tensor(10873.4854, grad_fn=<NegBackward0>) tensor(10873.4834, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10873.484375
tensor(10873.4834, grad_fn=<NegBackward0>) tensor(10873.4844, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10873.4833984375
tensor(10873.4834, grad_fn=<NegBackward0>) tensor(10873.4834, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10873.4814453125
tensor(10873.4834, grad_fn=<NegBackward0>) tensor(10873.4814, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10873.482421875
tensor(10873.4814, grad_fn=<NegBackward0>) tensor(10873.4824, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10873.48046875
tensor(10873.4814, grad_fn=<NegBackward0>) tensor(10873.4805, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10873.478515625
tensor(10873.4805, grad_fn=<NegBackward0>) tensor(10873.4785, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10873.4814453125
tensor(10873.4785, grad_fn=<NegBackward0>) tensor(10873.4814, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10873.48046875
tensor(10873.4785, grad_fn=<NegBackward0>) tensor(10873.4805, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -10873.49609375
tensor(10873.4785, grad_fn=<NegBackward0>) tensor(10873.4961, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -10873.4794921875
tensor(10873.4785, grad_fn=<NegBackward0>) tensor(10873.4795, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -10873.48046875
tensor(10873.4785, grad_fn=<NegBackward0>) tensor(10873.4805, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6000 due to no improvement.
pi: tensor([[0.7738, 0.2262],
        [0.2897, 0.7103]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4409, 0.5591], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2478, 0.0985],
         [0.5285, 0.2010]],

        [[0.5495, 0.0973],
         [0.7265, 0.6140]],

        [[0.6360, 0.1044],
         [0.6067, 0.5203]],

        [[0.5531, 0.1019],
         [0.7043, 0.6806]],

        [[0.7054, 0.0989],
         [0.6282, 0.6870]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 8
Adjusted Rand Index: 0.7026262626262626
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080620079101718
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7720123474937528
Global Adjusted Rand Index: 0.8096191846506263
Average Adjusted Rand Index: 0.8095085306625712
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20179.30859375
inf tensor(20179.3086, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10998.3447265625
tensor(20179.3086, grad_fn=<NegBackward0>) tensor(10998.3447, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10996.9169921875
tensor(10998.3447, grad_fn=<NegBackward0>) tensor(10996.9170, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10995.7607421875
tensor(10996.9170, grad_fn=<NegBackward0>) tensor(10995.7607, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10994.9912109375
tensor(10995.7607, grad_fn=<NegBackward0>) tensor(10994.9912, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10994.685546875
tensor(10994.9912, grad_fn=<NegBackward0>) tensor(10994.6855, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10994.564453125
tensor(10994.6855, grad_fn=<NegBackward0>) tensor(10994.5645, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10994.5
tensor(10994.5645, grad_fn=<NegBackward0>) tensor(10994.5000, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10994.4521484375
tensor(10994.5000, grad_fn=<NegBackward0>) tensor(10994.4521, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10994.37109375
tensor(10994.4521, grad_fn=<NegBackward0>) tensor(10994.3711, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10994.03515625
tensor(10994.3711, grad_fn=<NegBackward0>) tensor(10994.0352, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10993.9345703125
tensor(10994.0352, grad_fn=<NegBackward0>) tensor(10993.9346, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10993.8916015625
tensor(10993.9346, grad_fn=<NegBackward0>) tensor(10993.8916, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10993.859375
tensor(10993.8916, grad_fn=<NegBackward0>) tensor(10993.8594, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10993.837890625
tensor(10993.8594, grad_fn=<NegBackward0>) tensor(10993.8379, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10993.8232421875
tensor(10993.8379, grad_fn=<NegBackward0>) tensor(10993.8232, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10993.818359375
tensor(10993.8232, grad_fn=<NegBackward0>) tensor(10993.8184, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10993.8125
tensor(10993.8184, grad_fn=<NegBackward0>) tensor(10993.8125, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10993.810546875
tensor(10993.8125, grad_fn=<NegBackward0>) tensor(10993.8105, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10993.8095703125
tensor(10993.8105, grad_fn=<NegBackward0>) tensor(10993.8096, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10993.806640625
tensor(10993.8096, grad_fn=<NegBackward0>) tensor(10993.8066, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10993.8046875
tensor(10993.8066, grad_fn=<NegBackward0>) tensor(10993.8047, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10993.8017578125
tensor(10993.8047, grad_fn=<NegBackward0>) tensor(10993.8018, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10993.7978515625
tensor(10993.8018, grad_fn=<NegBackward0>) tensor(10993.7979, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10993.79296875
tensor(10993.7979, grad_fn=<NegBackward0>) tensor(10993.7930, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10993.7822265625
tensor(10993.7930, grad_fn=<NegBackward0>) tensor(10993.7822, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10993.7646484375
tensor(10993.7822, grad_fn=<NegBackward0>) tensor(10993.7646, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10993.7119140625
tensor(10993.7646, grad_fn=<NegBackward0>) tensor(10993.7119, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10993.3046875
tensor(10993.7119, grad_fn=<NegBackward0>) tensor(10993.3047, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10886.41796875
tensor(10993.3047, grad_fn=<NegBackward0>) tensor(10886.4180, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10873.6123046875
tensor(10886.4180, grad_fn=<NegBackward0>) tensor(10873.6123, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10873.56640625
tensor(10873.6123, grad_fn=<NegBackward0>) tensor(10873.5664, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10873.54296875
tensor(10873.5664, grad_fn=<NegBackward0>) tensor(10873.5430, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10873.5283203125
tensor(10873.5430, grad_fn=<NegBackward0>) tensor(10873.5283, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10873.51171875
tensor(10873.5283, grad_fn=<NegBackward0>) tensor(10873.5117, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10873.505859375
tensor(10873.5117, grad_fn=<NegBackward0>) tensor(10873.5059, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10873.4990234375
tensor(10873.5059, grad_fn=<NegBackward0>) tensor(10873.4990, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10873.4931640625
tensor(10873.4990, grad_fn=<NegBackward0>) tensor(10873.4932, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10873.4921875
tensor(10873.4932, grad_fn=<NegBackward0>) tensor(10873.4922, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10873.490234375
tensor(10873.4922, grad_fn=<NegBackward0>) tensor(10873.4902, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10873.48828125
tensor(10873.4902, grad_fn=<NegBackward0>) tensor(10873.4883, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10873.4873046875
tensor(10873.4883, grad_fn=<NegBackward0>) tensor(10873.4873, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10873.4873046875
tensor(10873.4873, grad_fn=<NegBackward0>) tensor(10873.4873, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10873.486328125
tensor(10873.4873, grad_fn=<NegBackward0>) tensor(10873.4863, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10873.4873046875
tensor(10873.4863, grad_fn=<NegBackward0>) tensor(10873.4873, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10873.4833984375
tensor(10873.4863, grad_fn=<NegBackward0>) tensor(10873.4834, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10873.4833984375
tensor(10873.4834, grad_fn=<NegBackward0>) tensor(10873.4834, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10873.4892578125
tensor(10873.4834, grad_fn=<NegBackward0>) tensor(10873.4893, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10873.4814453125
tensor(10873.4834, grad_fn=<NegBackward0>) tensor(10873.4814, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10873.482421875
tensor(10873.4814, grad_fn=<NegBackward0>) tensor(10873.4824, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10873.4814453125
tensor(10873.4814, grad_fn=<NegBackward0>) tensor(10873.4814, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10873.48046875
tensor(10873.4814, grad_fn=<NegBackward0>) tensor(10873.4805, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10873.48046875
tensor(10873.4805, grad_fn=<NegBackward0>) tensor(10873.4805, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10873.4814453125
tensor(10873.4805, grad_fn=<NegBackward0>) tensor(10873.4814, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10873.4794921875
tensor(10873.4805, grad_fn=<NegBackward0>) tensor(10873.4795, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10873.48046875
tensor(10873.4795, grad_fn=<NegBackward0>) tensor(10873.4805, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10873.4814453125
tensor(10873.4795, grad_fn=<NegBackward0>) tensor(10873.4814, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -10873.482421875
tensor(10873.4795, grad_fn=<NegBackward0>) tensor(10873.4824, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -10873.4794921875
tensor(10873.4795, grad_fn=<NegBackward0>) tensor(10873.4795, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10873.478515625
tensor(10873.4795, grad_fn=<NegBackward0>) tensor(10873.4785, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10873.4794921875
tensor(10873.4785, grad_fn=<NegBackward0>) tensor(10873.4795, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10873.4794921875
tensor(10873.4785, grad_fn=<NegBackward0>) tensor(10873.4795, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -10873.478515625
tensor(10873.4785, grad_fn=<NegBackward0>) tensor(10873.4785, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10873.48046875
tensor(10873.4785, grad_fn=<NegBackward0>) tensor(10873.4805, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10873.478515625
tensor(10873.4785, grad_fn=<NegBackward0>) tensor(10873.4785, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10873.478515625
tensor(10873.4785, grad_fn=<NegBackward0>) tensor(10873.4785, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10873.4912109375
tensor(10873.4785, grad_fn=<NegBackward0>) tensor(10873.4912, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10873.4775390625
tensor(10873.4785, grad_fn=<NegBackward0>) tensor(10873.4775, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10873.48046875
tensor(10873.4775, grad_fn=<NegBackward0>) tensor(10873.4805, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10873.4775390625
tensor(10873.4775, grad_fn=<NegBackward0>) tensor(10873.4775, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10873.4833984375
tensor(10873.4775, grad_fn=<NegBackward0>) tensor(10873.4834, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10873.4775390625
tensor(10873.4775, grad_fn=<NegBackward0>) tensor(10873.4775, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10873.478515625
tensor(10873.4775, grad_fn=<NegBackward0>) tensor(10873.4785, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10873.478515625
tensor(10873.4775, grad_fn=<NegBackward0>) tensor(10873.4785, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -10873.4775390625
tensor(10873.4775, grad_fn=<NegBackward0>) tensor(10873.4775, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10873.4775390625
tensor(10873.4775, grad_fn=<NegBackward0>) tensor(10873.4775, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10873.513671875
tensor(10873.4775, grad_fn=<NegBackward0>) tensor(10873.5137, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10873.4775390625
tensor(10873.4775, grad_fn=<NegBackward0>) tensor(10873.4775, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10873.482421875
tensor(10873.4775, grad_fn=<NegBackward0>) tensor(10873.4824, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10873.4765625
tensor(10873.4775, grad_fn=<NegBackward0>) tensor(10873.4766, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10873.5361328125
tensor(10873.4766, grad_fn=<NegBackward0>) tensor(10873.5361, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10873.46875
tensor(10873.4766, grad_fn=<NegBackward0>) tensor(10873.4688, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10873.4775390625
tensor(10873.4688, grad_fn=<NegBackward0>) tensor(10873.4775, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10873.46875
tensor(10873.4688, grad_fn=<NegBackward0>) tensor(10873.4688, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10873.509765625
tensor(10873.4688, grad_fn=<NegBackward0>) tensor(10873.5098, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -10873.46875
tensor(10873.4688, grad_fn=<NegBackward0>) tensor(10873.4688, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10873.48046875
tensor(10873.4688, grad_fn=<NegBackward0>) tensor(10873.4805, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10873.4677734375
tensor(10873.4688, grad_fn=<NegBackward0>) tensor(10873.4678, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10873.47265625
tensor(10873.4678, grad_fn=<NegBackward0>) tensor(10873.4727, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10873.484375
tensor(10873.4678, grad_fn=<NegBackward0>) tensor(10873.4844, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -10873.4677734375
tensor(10873.4678, grad_fn=<NegBackward0>) tensor(10873.4678, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10873.4697265625
tensor(10873.4678, grad_fn=<NegBackward0>) tensor(10873.4697, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10873.4697265625
tensor(10873.4678, grad_fn=<NegBackward0>) tensor(10873.4697, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -10873.470703125
tensor(10873.4678, grad_fn=<NegBackward0>) tensor(10873.4707, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -10873.4765625
tensor(10873.4678, grad_fn=<NegBackward0>) tensor(10873.4766, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -10873.46875
tensor(10873.4678, grad_fn=<NegBackward0>) tensor(10873.4688, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9500 due to no improvement.
pi: tensor([[0.7741, 0.2259],
        [0.2889, 0.7111]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4410, 0.5590], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2478, 0.0985],
         [0.5042, 0.2010]],

        [[0.6463, 0.0973],
         [0.5962, 0.7308]],

        [[0.5069, 0.1044],
         [0.6694, 0.5614]],

        [[0.6482, 0.1019],
         [0.5295, 0.5667]],

        [[0.5156, 0.0989],
         [0.6012, 0.5160]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 8
Adjusted Rand Index: 0.7026262626262626
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080620079101718
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7720123474937528
Global Adjusted Rand Index: 0.8096191846506263
Average Adjusted Rand Index: 0.8095085306625712
[0.8096191846506263, 0.8096191846506263] [0.8095085306625712, 0.8095085306625712] [10873.48046875, 10873.46875]
-------------------------------------
This iteration is 34
True Objective function: Loss = -10903.222798668823
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22866.357421875
inf tensor(22866.3574, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11001.1943359375
tensor(22866.3574, grad_fn=<NegBackward0>) tensor(11001.1943, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11000.7861328125
tensor(11001.1943, grad_fn=<NegBackward0>) tensor(11000.7861, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11000.640625
tensor(11000.7861, grad_fn=<NegBackward0>) tensor(11000.6406, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11000.5546875
tensor(11000.6406, grad_fn=<NegBackward0>) tensor(11000.5547, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11000.4912109375
tensor(11000.5547, grad_fn=<NegBackward0>) tensor(11000.4912, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11000.4423828125
tensor(11000.4912, grad_fn=<NegBackward0>) tensor(11000.4424, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11000.39453125
tensor(11000.4424, grad_fn=<NegBackward0>) tensor(11000.3945, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11000.3544921875
tensor(11000.3945, grad_fn=<NegBackward0>) tensor(11000.3545, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11000.3193359375
tensor(11000.3545, grad_fn=<NegBackward0>) tensor(11000.3193, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11000.294921875
tensor(11000.3193, grad_fn=<NegBackward0>) tensor(11000.2949, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11000.2783203125
tensor(11000.2949, grad_fn=<NegBackward0>) tensor(11000.2783, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11000.26171875
tensor(11000.2783, grad_fn=<NegBackward0>) tensor(11000.2617, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11000.2509765625
tensor(11000.2617, grad_fn=<NegBackward0>) tensor(11000.2510, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11000.2392578125
tensor(11000.2510, grad_fn=<NegBackward0>) tensor(11000.2393, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11000.23046875
tensor(11000.2393, grad_fn=<NegBackward0>) tensor(11000.2305, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11000.22265625
tensor(11000.2305, grad_fn=<NegBackward0>) tensor(11000.2227, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11000.2138671875
tensor(11000.2227, grad_fn=<NegBackward0>) tensor(11000.2139, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11000.2041015625
tensor(11000.2139, grad_fn=<NegBackward0>) tensor(11000.2041, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11000.1953125
tensor(11000.2041, grad_fn=<NegBackward0>) tensor(11000.1953, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11000.185546875
tensor(11000.1953, grad_fn=<NegBackward0>) tensor(11000.1855, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11000.17578125
tensor(11000.1855, grad_fn=<NegBackward0>) tensor(11000.1758, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11000.16796875
tensor(11000.1758, grad_fn=<NegBackward0>) tensor(11000.1680, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11000.158203125
tensor(11000.1680, grad_fn=<NegBackward0>) tensor(11000.1582, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11000.150390625
tensor(11000.1582, grad_fn=<NegBackward0>) tensor(11000.1504, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11000.1435546875
tensor(11000.1504, grad_fn=<NegBackward0>) tensor(11000.1436, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11000.13671875
tensor(11000.1436, grad_fn=<NegBackward0>) tensor(11000.1367, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11000.12890625
tensor(11000.1367, grad_fn=<NegBackward0>) tensor(11000.1289, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11000.1259765625
tensor(11000.1289, grad_fn=<NegBackward0>) tensor(11000.1260, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11000.1201171875
tensor(11000.1260, grad_fn=<NegBackward0>) tensor(11000.1201, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11000.115234375
tensor(11000.1201, grad_fn=<NegBackward0>) tensor(11000.1152, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11000.11328125
tensor(11000.1152, grad_fn=<NegBackward0>) tensor(11000.1133, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11000.111328125
tensor(11000.1133, grad_fn=<NegBackward0>) tensor(11000.1113, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11000.109375
tensor(11000.1113, grad_fn=<NegBackward0>) tensor(11000.1094, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11000.1083984375
tensor(11000.1094, grad_fn=<NegBackward0>) tensor(11000.1084, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11000.1064453125
tensor(11000.1084, grad_fn=<NegBackward0>) tensor(11000.1064, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11000.1064453125
tensor(11000.1064, grad_fn=<NegBackward0>) tensor(11000.1064, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11000.1044921875
tensor(11000.1064, grad_fn=<NegBackward0>) tensor(11000.1045, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11000.1044921875
tensor(11000.1045, grad_fn=<NegBackward0>) tensor(11000.1045, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11000.1025390625
tensor(11000.1045, grad_fn=<NegBackward0>) tensor(11000.1025, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11000.1025390625
tensor(11000.1025, grad_fn=<NegBackward0>) tensor(11000.1025, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11000.1015625
tensor(11000.1025, grad_fn=<NegBackward0>) tensor(11000.1016, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11000.1005859375
tensor(11000.1016, grad_fn=<NegBackward0>) tensor(11000.1006, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11000.099609375
tensor(11000.1006, grad_fn=<NegBackward0>) tensor(11000.0996, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11000.09765625
tensor(11000.0996, grad_fn=<NegBackward0>) tensor(11000.0977, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11000.095703125
tensor(11000.0977, grad_fn=<NegBackward0>) tensor(11000.0957, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11000.0947265625
tensor(11000.0957, grad_fn=<NegBackward0>) tensor(11000.0947, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11000.091796875
tensor(11000.0947, grad_fn=<NegBackward0>) tensor(11000.0918, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11000.0888671875
tensor(11000.0918, grad_fn=<NegBackward0>) tensor(11000.0889, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11000.08203125
tensor(11000.0889, grad_fn=<NegBackward0>) tensor(11000.0820, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11000.07421875
tensor(11000.0820, grad_fn=<NegBackward0>) tensor(11000.0742, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11000.0615234375
tensor(11000.0742, grad_fn=<NegBackward0>) tensor(11000.0615, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11000.046875
tensor(11000.0615, grad_fn=<NegBackward0>) tensor(11000.0469, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11000.0302734375
tensor(11000.0469, grad_fn=<NegBackward0>) tensor(11000.0303, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11000.0126953125
tensor(11000.0303, grad_fn=<NegBackward0>) tensor(11000.0127, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11000.00390625
tensor(11000.0127, grad_fn=<NegBackward0>) tensor(11000.0039, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10999.9814453125
tensor(11000.0039, grad_fn=<NegBackward0>) tensor(10999.9814, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10999.96875
tensor(10999.9814, grad_fn=<NegBackward0>) tensor(10999.9688, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10999.9580078125
tensor(10999.9688, grad_fn=<NegBackward0>) tensor(10999.9580, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10999.9501953125
tensor(10999.9580, grad_fn=<NegBackward0>) tensor(10999.9502, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10999.9453125
tensor(10999.9502, grad_fn=<NegBackward0>) tensor(10999.9453, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10999.9716796875
tensor(10999.9453, grad_fn=<NegBackward0>) tensor(10999.9717, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10999.9384765625
tensor(10999.9453, grad_fn=<NegBackward0>) tensor(10999.9385, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10999.9375
tensor(10999.9385, grad_fn=<NegBackward0>) tensor(10999.9375, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10999.935546875
tensor(10999.9375, grad_fn=<NegBackward0>) tensor(10999.9355, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10999.9365234375
tensor(10999.9355, grad_fn=<NegBackward0>) tensor(10999.9365, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10999.93359375
tensor(10999.9355, grad_fn=<NegBackward0>) tensor(10999.9336, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10999.9365234375
tensor(10999.9336, grad_fn=<NegBackward0>) tensor(10999.9365, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10999.9326171875
tensor(10999.9336, grad_fn=<NegBackward0>) tensor(10999.9326, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10999.93359375
tensor(10999.9326, grad_fn=<NegBackward0>) tensor(10999.9336, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10999.931640625
tensor(10999.9326, grad_fn=<NegBackward0>) tensor(10999.9316, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10999.931640625
tensor(10999.9316, grad_fn=<NegBackward0>) tensor(10999.9316, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10999.9306640625
tensor(10999.9316, grad_fn=<NegBackward0>) tensor(10999.9307, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10999.9306640625
tensor(10999.9307, grad_fn=<NegBackward0>) tensor(10999.9307, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10999.9287109375
tensor(10999.9307, grad_fn=<NegBackward0>) tensor(10999.9287, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10999.9296875
tensor(10999.9287, grad_fn=<NegBackward0>) tensor(10999.9297, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10999.9287109375
tensor(10999.9287, grad_fn=<NegBackward0>) tensor(10999.9287, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10999.9287109375
tensor(10999.9287, grad_fn=<NegBackward0>) tensor(10999.9287, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10999.9296875
tensor(10999.9287, grad_fn=<NegBackward0>) tensor(10999.9297, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10999.927734375
tensor(10999.9287, grad_fn=<NegBackward0>) tensor(10999.9277, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10999.9306640625
tensor(10999.9277, grad_fn=<NegBackward0>) tensor(10999.9307, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10999.9287109375
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 35%|███▌      | 35/100 [9:01:55<17:05:04, 946.23s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 36%|███▌      | 36/100 [9:12:42<15:13:28, 856.38s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 37%|███▋      | 37/100 [9:30:48<16:11:25, 925.16s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 38%|███▊      | 38/100 [9:45:18<15:38:58, 908.68s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 39%|███▉      | 39/100 [10:02:45<16:06:05, 950.26s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 40%|████      | 40/100 [10:17:40<15:33:45, 933.76s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 41%|████      | 41/100 [10:32:12<14:59:53, 915.14s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 42%|████▏     | 42/100 [10:44:35<13:54:47, 863.57s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 43%|████▎     | 43/100 [10:57:53<13:21:35, 843.78s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 44%|████▍     | 44/100 [11:12:37<13:18:50, 855.91s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 45%|████▌     | 45/100 [11:26:53<13:04:34, 855.90s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 46%|████▌     | 46/100 [11:43:52<13:34:28, 904.97s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 47%|████▋     | 47/100 [11:57:04<12:49:12, 870.81s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 48%|████▊     | 48/100 [12:15:57<13:42:52, 949.47s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 49%|████▉     | 49/100 [12:29:42<12:55:24, 912.24s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 50%|█████     | 50/100 [12:45:14<12:45:04, 918.09s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 51%|█████     | 51/100 [13:03:29<13:13:17, 971.37s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 52%|█████▏    | 52/100 [13:14:34<11:43:34, 879.48s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 53%|█████▎    | 53/100 [13:24:59<10:29:03, 803.04s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 54%|█████▍    | 54/100 [13:40:06<10:39:34, 834.23s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 55%|█████▌    | 55/100 [13:52:08<10:00:17, 800.38s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 56%|█████▌    | 56/100 [14:06:37<10:02:04, 821.02s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 57%|█████▋    | 57/100 [14:23:08<10:24:58, 872.05s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 58%|█████▊    | 58/100 [14:39:35<10:34:31, 906.45s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 59%|█████▉    | 59/100 [14:53:45<10:07:52, 889.57s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 60%|██████    | 60/100 [15:04:33<9:04:44, 817.11s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 61%|██████    | 61/100 [15:15:40<8:21:50, 772.07s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 62%|██████▏   | 62/100 [15:27:02<7:51:59, 745.24s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 63%|██████▎   | 63/100 [15:45:27<8:46:02, 853.05s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 64%|██████▍   | 64/100 [16:00:24<8:39:39, 866.09s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 65%|██████▌   | 65/100 [16:09:55<7:33:42, 777.78s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 66%|██████▌   | 66/100 [16:19:40<6:47:58, 719.96s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 67%|██████▋   | 67/100 [16:34:14<7:01:22, 766.14s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 68%|██████▊   | 68/100 [16:52:22<7:40:00, 862.51s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 69%|██████▉   | 69/100 [17:08:16<7:39:53, 890.11s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 70%|███████   | 70/100 [17:23:47<7:31:14, 902.48s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
tensor(10999.9277, grad_fn=<NegBackward0>) tensor(10999.9287, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -10999.9267578125
tensor(10999.9277, grad_fn=<NegBackward0>) tensor(10999.9268, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10999.927734375
tensor(10999.9268, grad_fn=<NegBackward0>) tensor(10999.9277, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10999.9287109375
tensor(10999.9268, grad_fn=<NegBackward0>) tensor(10999.9287, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -10999.9404296875
tensor(10999.9268, grad_fn=<NegBackward0>) tensor(10999.9404, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -10999.927734375
tensor(10999.9268, grad_fn=<NegBackward0>) tensor(10999.9277, grad_fn=<NegBackward0>)
4
Iteration 8700: Loss = -10999.927734375
tensor(10999.9268, grad_fn=<NegBackward0>) tensor(10999.9277, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8700 due to no improvement.
pi: tensor([[0.9951, 0.0049],
        [0.2138, 0.7862]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0306, 0.9694], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1682, 0.1901],
         [0.7041, 0.1595]],

        [[0.5802, 0.1636],
         [0.5648, 0.7151]],

        [[0.7191, 0.1620],
         [0.6802, 0.7165]],

        [[0.6203, 0.1613],
         [0.6258, 0.6175]],

        [[0.6116, 0.1678],
         [0.5222, 0.5962]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 3
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.009926899686712943
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0021615455684164207
Average Adjusted Rand Index: -0.00243832649928544
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21961.169921875
inf tensor(21961.1699, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11001.5302734375
tensor(21961.1699, grad_fn=<NegBackward0>) tensor(11001.5303, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11000.8466796875
tensor(11001.5303, grad_fn=<NegBackward0>) tensor(11000.8467, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11000.662109375
tensor(11000.8467, grad_fn=<NegBackward0>) tensor(11000.6621, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11000.5595703125
tensor(11000.6621, grad_fn=<NegBackward0>) tensor(11000.5596, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11000.48828125
tensor(11000.5596, grad_fn=<NegBackward0>) tensor(11000.4883, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11000.427734375
tensor(11000.4883, grad_fn=<NegBackward0>) tensor(11000.4277, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11000.3759765625
tensor(11000.4277, grad_fn=<NegBackward0>) tensor(11000.3760, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11000.3330078125
tensor(11000.3760, grad_fn=<NegBackward0>) tensor(11000.3330, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11000.30078125
tensor(11000.3330, grad_fn=<NegBackward0>) tensor(11000.3008, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11000.2763671875
tensor(11000.3008, grad_fn=<NegBackward0>) tensor(11000.2764, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11000.259765625
tensor(11000.2764, grad_fn=<NegBackward0>) tensor(11000.2598, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11000.2451171875
tensor(11000.2598, grad_fn=<NegBackward0>) tensor(11000.2451, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11000.232421875
tensor(11000.2451, grad_fn=<NegBackward0>) tensor(11000.2324, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11000.2177734375
tensor(11000.2324, grad_fn=<NegBackward0>) tensor(11000.2178, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11000.2060546875
tensor(11000.2178, grad_fn=<NegBackward0>) tensor(11000.2061, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11000.197265625
tensor(11000.2061, grad_fn=<NegBackward0>) tensor(11000.1973, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11000.1865234375
tensor(11000.1973, grad_fn=<NegBackward0>) tensor(11000.1865, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11000.1787109375
tensor(11000.1865, grad_fn=<NegBackward0>) tensor(11000.1787, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11000.173828125
tensor(11000.1787, grad_fn=<NegBackward0>) tensor(11000.1738, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11000.166015625
tensor(11000.1738, grad_fn=<NegBackward0>) tensor(11000.1660, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11000.158203125
tensor(11000.1660, grad_fn=<NegBackward0>) tensor(11000.1582, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11000.1513671875
tensor(11000.1582, grad_fn=<NegBackward0>) tensor(11000.1514, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11000.14453125
tensor(11000.1514, grad_fn=<NegBackward0>) tensor(11000.1445, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11000.13671875
tensor(11000.1445, grad_fn=<NegBackward0>) tensor(11000.1367, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11000.12890625
tensor(11000.1367, grad_fn=<NegBackward0>) tensor(11000.1289, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11000.1162109375
tensor(11000.1289, grad_fn=<NegBackward0>) tensor(11000.1162, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11000.103515625
tensor(11000.1162, grad_fn=<NegBackward0>) tensor(11000.1035, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11000.0849609375
tensor(11000.1035, grad_fn=<NegBackward0>) tensor(11000.0850, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11000.0390625
tensor(11000.0850, grad_fn=<NegBackward0>) tensor(11000.0391, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10999.8486328125
tensor(11000.0391, grad_fn=<NegBackward0>) tensor(10999.8486, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10999.662109375
tensor(10999.8486, grad_fn=<NegBackward0>) tensor(10999.6621, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10999.595703125
tensor(10999.6621, grad_fn=<NegBackward0>) tensor(10999.5957, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10999.5576171875
tensor(10999.5957, grad_fn=<NegBackward0>) tensor(10999.5576, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10999.5263671875
tensor(10999.5576, grad_fn=<NegBackward0>) tensor(10999.5264, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10999.4931640625
tensor(10999.5264, grad_fn=<NegBackward0>) tensor(10999.4932, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10999.4658203125
tensor(10999.4932, grad_fn=<NegBackward0>) tensor(10999.4658, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10999.408203125
tensor(10999.4658, grad_fn=<NegBackward0>) tensor(10999.4082, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10998.978515625
tensor(10999.4082, grad_fn=<NegBackward0>) tensor(10998.9785, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10998.83984375
tensor(10998.9785, grad_fn=<NegBackward0>) tensor(10998.8398, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10998.8203125
tensor(10998.8398, grad_fn=<NegBackward0>) tensor(10998.8203, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10998.8154296875
tensor(10998.8203, grad_fn=<NegBackward0>) tensor(10998.8154, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10998.8134765625
tensor(10998.8154, grad_fn=<NegBackward0>) tensor(10998.8135, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10998.810546875
tensor(10998.8135, grad_fn=<NegBackward0>) tensor(10998.8105, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10998.8125
tensor(10998.8105, grad_fn=<NegBackward0>) tensor(10998.8125, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10998.8125
tensor(10998.8105, grad_fn=<NegBackward0>) tensor(10998.8125, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -10998.8095703125
tensor(10998.8105, grad_fn=<NegBackward0>) tensor(10998.8096, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10998.80859375
tensor(10998.8096, grad_fn=<NegBackward0>) tensor(10998.8086, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10998.80859375
tensor(10998.8086, grad_fn=<NegBackward0>) tensor(10998.8086, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10998.8076171875
tensor(10998.8086, grad_fn=<NegBackward0>) tensor(10998.8076, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10998.806640625
tensor(10998.8076, grad_fn=<NegBackward0>) tensor(10998.8066, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10998.8056640625
tensor(10998.8066, grad_fn=<NegBackward0>) tensor(10998.8057, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10998.806640625
tensor(10998.8057, grad_fn=<NegBackward0>) tensor(10998.8066, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10998.8056640625
tensor(10998.8057, grad_fn=<NegBackward0>) tensor(10998.8057, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10998.8037109375
tensor(10998.8057, grad_fn=<NegBackward0>) tensor(10998.8037, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10998.8037109375
tensor(10998.8037, grad_fn=<NegBackward0>) tensor(10998.8037, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10998.8037109375
tensor(10998.8037, grad_fn=<NegBackward0>) tensor(10998.8037, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10998.8046875
tensor(10998.8037, grad_fn=<NegBackward0>) tensor(10998.8047, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10998.8017578125
tensor(10998.8037, grad_fn=<NegBackward0>) tensor(10998.8018, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10998.8017578125
tensor(10998.8018, grad_fn=<NegBackward0>) tensor(10998.8018, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10998.802734375
tensor(10998.8018, grad_fn=<NegBackward0>) tensor(10998.8027, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10998.802734375
tensor(10998.8018, grad_fn=<NegBackward0>) tensor(10998.8027, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -10998.8017578125
tensor(10998.8018, grad_fn=<NegBackward0>) tensor(10998.8018, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10998.802734375
tensor(10998.8018, grad_fn=<NegBackward0>) tensor(10998.8027, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10998.8017578125
tensor(10998.8018, grad_fn=<NegBackward0>) tensor(10998.8018, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10998.806640625
tensor(10998.8018, grad_fn=<NegBackward0>) tensor(10998.8066, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10998.8056640625
tensor(10998.8018, grad_fn=<NegBackward0>) tensor(10998.8057, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -10998.8017578125
tensor(10998.8018, grad_fn=<NegBackward0>) tensor(10998.8018, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10998.80078125
tensor(10998.8018, grad_fn=<NegBackward0>) tensor(10998.8008, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10998.802734375
tensor(10998.8008, grad_fn=<NegBackward0>) tensor(10998.8027, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10998.802734375
tensor(10998.8008, grad_fn=<NegBackward0>) tensor(10998.8027, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -10998.8017578125
tensor(10998.8008, grad_fn=<NegBackward0>) tensor(10998.8018, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -10998.80078125
tensor(10998.8008, grad_fn=<NegBackward0>) tensor(10998.8008, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10998.80078125
tensor(10998.8008, grad_fn=<NegBackward0>) tensor(10998.8008, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10998.8017578125
tensor(10998.8008, grad_fn=<NegBackward0>) tensor(10998.8018, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10998.7998046875
tensor(10998.8008, grad_fn=<NegBackward0>) tensor(10998.7998, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10998.80078125
tensor(10998.7998, grad_fn=<NegBackward0>) tensor(10998.8008, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10998.80078125
tensor(10998.7998, grad_fn=<NegBackward0>) tensor(10998.8008, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -10998.7998046875
tensor(10998.7998, grad_fn=<NegBackward0>) tensor(10998.7998, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10998.8388671875
tensor(10998.7998, grad_fn=<NegBackward0>) tensor(10998.8389, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10998.80078125
tensor(10998.7998, grad_fn=<NegBackward0>) tensor(10998.8008, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -10998.80859375
tensor(10998.7998, grad_fn=<NegBackward0>) tensor(10998.8086, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -10998.7998046875
tensor(10998.7998, grad_fn=<NegBackward0>) tensor(10998.7998, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10998.8017578125
tensor(10998.7998, grad_fn=<NegBackward0>) tensor(10998.8018, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10998.80078125
tensor(10998.7998, grad_fn=<NegBackward0>) tensor(10998.8008, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -10998.7998046875
tensor(10998.7998, grad_fn=<NegBackward0>) tensor(10998.7998, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10998.7998046875
tensor(10998.7998, grad_fn=<NegBackward0>) tensor(10998.7998, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10998.80078125
tensor(10998.7998, grad_fn=<NegBackward0>) tensor(10998.8008, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -10998.916015625
tensor(10998.7998, grad_fn=<NegBackward0>) tensor(10998.9160, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -10998.80078125
tensor(10998.7998, grad_fn=<NegBackward0>) tensor(10998.8008, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -10998.80078125
tensor(10998.7998, grad_fn=<NegBackward0>) tensor(10998.8008, grad_fn=<NegBackward0>)
4
Iteration 9100: Loss = -10998.80078125
tensor(10998.7998, grad_fn=<NegBackward0>) tensor(10998.8008, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9100 due to no improvement.
pi: tensor([[9.1894e-01, 8.1060e-02],
        [9.9961e-01, 3.9144e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8984, 0.1016], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1687, 0.1315],
         [0.5672, 0.1117]],

        [[0.6929, 0.1229],
         [0.7272, 0.5754]],

        [[0.6449, 0.1099],
         [0.6256, 0.7169]],

        [[0.5910, 0.1432],
         [0.5430, 0.6170]],

        [[0.5559, 0.1588],
         [0.6387, 0.5875]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008735738497905159
Average Adjusted Rand Index: 0.0005107695272972583
[-0.0021615455684164207, -0.0008735738497905159] [-0.00243832649928544, 0.0005107695272972583] [10999.927734375, 10998.80078125]
-------------------------------------
This iteration is 35
True Objective function: Loss = -10879.269140049195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23343.451171875
inf tensor(23343.4512, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10915.529296875
tensor(23343.4512, grad_fn=<NegBackward0>) tensor(10915.5293, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10914.205078125
tensor(10915.5293, grad_fn=<NegBackward0>) tensor(10914.2051, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10913.3623046875
tensor(10914.2051, grad_fn=<NegBackward0>) tensor(10913.3623, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10912.9111328125
tensor(10913.3623, grad_fn=<NegBackward0>) tensor(10912.9111, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10912.5869140625
tensor(10912.9111, grad_fn=<NegBackward0>) tensor(10912.5869, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10912.3251953125
tensor(10912.5869, grad_fn=<NegBackward0>) tensor(10912.3252, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10912.08984375
tensor(10912.3252, grad_fn=<NegBackward0>) tensor(10912.0898, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10911.8759765625
tensor(10912.0898, grad_fn=<NegBackward0>) tensor(10911.8760, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10911.689453125
tensor(10911.8760, grad_fn=<NegBackward0>) tensor(10911.6895, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10911.5439453125
tensor(10911.6895, grad_fn=<NegBackward0>) tensor(10911.5439, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10911.4365234375
tensor(10911.5439, grad_fn=<NegBackward0>) tensor(10911.4365, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10911.3662109375
tensor(10911.4365, grad_fn=<NegBackward0>) tensor(10911.3662, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10911.3212890625
tensor(10911.3662, grad_fn=<NegBackward0>) tensor(10911.3213, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10911.2958984375
tensor(10911.3213, grad_fn=<NegBackward0>) tensor(10911.2959, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10911.283203125
tensor(10911.2959, grad_fn=<NegBackward0>) tensor(10911.2832, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10911.2763671875
tensor(10911.2832, grad_fn=<NegBackward0>) tensor(10911.2764, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10911.2724609375
tensor(10911.2764, grad_fn=<NegBackward0>) tensor(10911.2725, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10911.2685546875
tensor(10911.2725, grad_fn=<NegBackward0>) tensor(10911.2686, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10911.267578125
tensor(10911.2686, grad_fn=<NegBackward0>) tensor(10911.2676, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10911.265625
tensor(10911.2676, grad_fn=<NegBackward0>) tensor(10911.2656, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10911.2646484375
tensor(10911.2656, grad_fn=<NegBackward0>) tensor(10911.2646, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10911.2646484375
tensor(10911.2646, grad_fn=<NegBackward0>) tensor(10911.2646, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10911.263671875
tensor(10911.2646, grad_fn=<NegBackward0>) tensor(10911.2637, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10911.26171875
tensor(10911.2637, grad_fn=<NegBackward0>) tensor(10911.2617, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10911.2626953125
tensor(10911.2617, grad_fn=<NegBackward0>) tensor(10911.2627, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -10911.2607421875
tensor(10911.2617, grad_fn=<NegBackward0>) tensor(10911.2607, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10911.26171875
tensor(10911.2607, grad_fn=<NegBackward0>) tensor(10911.2617, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -10911.2607421875
tensor(10911.2607, grad_fn=<NegBackward0>) tensor(10911.2607, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10911.259765625
tensor(10911.2607, grad_fn=<NegBackward0>) tensor(10911.2598, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10911.2587890625
tensor(10911.2598, grad_fn=<NegBackward0>) tensor(10911.2588, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10911.2587890625
tensor(10911.2588, grad_fn=<NegBackward0>) tensor(10911.2588, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10911.2587890625
tensor(10911.2588, grad_fn=<NegBackward0>) tensor(10911.2588, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10911.259765625
tensor(10911.2588, grad_fn=<NegBackward0>) tensor(10911.2598, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -10911.2587890625
tensor(10911.2588, grad_fn=<NegBackward0>) tensor(10911.2588, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10911.2587890625
tensor(10911.2588, grad_fn=<NegBackward0>) tensor(10911.2588, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10911.259765625
tensor(10911.2588, grad_fn=<NegBackward0>) tensor(10911.2598, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -10911.2587890625
tensor(10911.2588, grad_fn=<NegBackward0>) tensor(10911.2588, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10911.2578125
tensor(10911.2588, grad_fn=<NegBackward0>) tensor(10911.2578, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10911.2578125
tensor(10911.2578, grad_fn=<NegBackward0>) tensor(10911.2578, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10911.2568359375
tensor(10911.2578, grad_fn=<NegBackward0>) tensor(10911.2568, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10911.2587890625
tensor(10911.2568, grad_fn=<NegBackward0>) tensor(10911.2588, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -10911.2587890625
tensor(10911.2568, grad_fn=<NegBackward0>) tensor(10911.2588, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -10911.2568359375
tensor(10911.2568, grad_fn=<NegBackward0>) tensor(10911.2568, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10911.255859375
tensor(10911.2568, grad_fn=<NegBackward0>) tensor(10911.2559, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10911.2568359375
tensor(10911.2559, grad_fn=<NegBackward0>) tensor(10911.2568, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10911.255859375
tensor(10911.2559, grad_fn=<NegBackward0>) tensor(10911.2559, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10911.2578125
tensor(10911.2559, grad_fn=<NegBackward0>) tensor(10911.2578, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10911.2568359375
tensor(10911.2559, grad_fn=<NegBackward0>) tensor(10911.2568, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -10911.2578125
tensor(10911.2559, grad_fn=<NegBackward0>) tensor(10911.2578, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -10911.2568359375
tensor(10911.2559, grad_fn=<NegBackward0>) tensor(10911.2568, grad_fn=<NegBackward0>)
4
Iteration 5100: Loss = -10911.255859375
tensor(10911.2559, grad_fn=<NegBackward0>) tensor(10911.2559, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10911.2548828125
tensor(10911.2559, grad_fn=<NegBackward0>) tensor(10911.2549, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10911.2568359375
tensor(10911.2549, grad_fn=<NegBackward0>) tensor(10911.2568, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10911.255859375
tensor(10911.2549, grad_fn=<NegBackward0>) tensor(10911.2559, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -10911.255859375
tensor(10911.2549, grad_fn=<NegBackward0>) tensor(10911.2559, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -10911.255859375
tensor(10911.2549, grad_fn=<NegBackward0>) tensor(10911.2559, grad_fn=<NegBackward0>)
4
Iteration 5700: Loss = -10911.2548828125
tensor(10911.2549, grad_fn=<NegBackward0>) tensor(10911.2549, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10911.2548828125
tensor(10911.2549, grad_fn=<NegBackward0>) tensor(10911.2549, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10911.255859375
tensor(10911.2549, grad_fn=<NegBackward0>) tensor(10911.2559, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10911.255859375
tensor(10911.2549, grad_fn=<NegBackward0>) tensor(10911.2559, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -10911.2548828125
tensor(10911.2549, grad_fn=<NegBackward0>) tensor(10911.2549, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10911.25390625
tensor(10911.2549, grad_fn=<NegBackward0>) tensor(10911.2539, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10911.2548828125
tensor(10911.2539, grad_fn=<NegBackward0>) tensor(10911.2549, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10911.2548828125
tensor(10911.2539, grad_fn=<NegBackward0>) tensor(10911.2549, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -10911.2548828125
tensor(10911.2539, grad_fn=<NegBackward0>) tensor(10911.2549, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -10911.25390625
tensor(10911.2539, grad_fn=<NegBackward0>) tensor(10911.2539, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10911.25390625
tensor(10911.2539, grad_fn=<NegBackward0>) tensor(10911.2539, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10911.25390625
tensor(10911.2539, grad_fn=<NegBackward0>) tensor(10911.2539, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10911.25390625
tensor(10911.2539, grad_fn=<NegBackward0>) tensor(10911.2539, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10911.251953125
tensor(10911.2539, grad_fn=<NegBackward0>) tensor(10911.2520, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10911.2529296875
tensor(10911.2520, grad_fn=<NegBackward0>) tensor(10911.2529, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10911.2548828125
tensor(10911.2520, grad_fn=<NegBackward0>) tensor(10911.2549, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -10911.25390625
tensor(10911.2520, grad_fn=<NegBackward0>) tensor(10911.2539, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -10911.259765625
tensor(10911.2520, grad_fn=<NegBackward0>) tensor(10911.2598, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -10911.2529296875
tensor(10911.2520, grad_fn=<NegBackward0>) tensor(10911.2529, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7500 due to no improvement.
pi: tensor([[0.0084, 0.9916],
        [0.0535, 0.9465]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0130, 0.9870], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1959, 0.1901],
         [0.6595, 0.1583]],

        [[0.5918, 0.2534],
         [0.5919, 0.5284]],

        [[0.5860, 0.1480],
         [0.6898, 0.5848]],

        [[0.6663, 0.1797],
         [0.6739, 0.6361]],

        [[0.7006, 0.1661],
         [0.5070, 0.6757]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0070073061126442035
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0018058126460340001
Average Adjusted Rand Index: -0.0014014612225288407
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22051.888671875
inf tensor(22051.8887, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10915.5673828125
tensor(22051.8887, grad_fn=<NegBackward0>) tensor(10915.5674, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10914.3232421875
tensor(10915.5674, grad_fn=<NegBackward0>) tensor(10914.3232, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10912.94140625
tensor(10914.3232, grad_fn=<NegBackward0>) tensor(10912.9414, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10912.435546875
tensor(10912.9414, grad_fn=<NegBackward0>) tensor(10912.4355, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10912.0556640625
tensor(10912.4355, grad_fn=<NegBackward0>) tensor(10912.0557, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10911.7783203125
tensor(10912.0557, grad_fn=<NegBackward0>) tensor(10911.7783, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10911.6044921875
tensor(10911.7783, grad_fn=<NegBackward0>) tensor(10911.6045, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10911.474609375
tensor(10911.6045, grad_fn=<NegBackward0>) tensor(10911.4746, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10911.3798828125
tensor(10911.4746, grad_fn=<NegBackward0>) tensor(10911.3799, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10911.3154296875
tensor(10911.3799, grad_fn=<NegBackward0>) tensor(10911.3154, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10911.28125
tensor(10911.3154, grad_fn=<NegBackward0>) tensor(10911.2812, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10911.2666015625
tensor(10911.2812, grad_fn=<NegBackward0>) tensor(10911.2666, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10911.2607421875
tensor(10911.2666, grad_fn=<NegBackward0>) tensor(10911.2607, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10911.2578125
tensor(10911.2607, grad_fn=<NegBackward0>) tensor(10911.2578, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10911.255859375
tensor(10911.2578, grad_fn=<NegBackward0>) tensor(10911.2559, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10911.2548828125
tensor(10911.2559, grad_fn=<NegBackward0>) tensor(10911.2549, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10911.2548828125
tensor(10911.2549, grad_fn=<NegBackward0>) tensor(10911.2549, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10911.255859375
tensor(10911.2549, grad_fn=<NegBackward0>) tensor(10911.2559, grad_fn=<NegBackward0>)
1
Iteration 1900: Loss = -10911.255859375
tensor(10911.2549, grad_fn=<NegBackward0>) tensor(10911.2559, grad_fn=<NegBackward0>)
2
Iteration 2000: Loss = -10911.25390625
tensor(10911.2549, grad_fn=<NegBackward0>) tensor(10911.2539, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10911.2548828125
tensor(10911.2539, grad_fn=<NegBackward0>) tensor(10911.2549, grad_fn=<NegBackward0>)
1
Iteration 2200: Loss = -10911.25390625
tensor(10911.2539, grad_fn=<NegBackward0>) tensor(10911.2539, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10911.25390625
tensor(10911.2539, grad_fn=<NegBackward0>) tensor(10911.2539, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10911.2548828125
tensor(10911.2539, grad_fn=<NegBackward0>) tensor(10911.2549, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -10911.255859375
tensor(10911.2539, grad_fn=<NegBackward0>) tensor(10911.2559, grad_fn=<NegBackward0>)
2
Iteration 2600: Loss = -10911.2548828125
tensor(10911.2539, grad_fn=<NegBackward0>) tensor(10911.2549, grad_fn=<NegBackward0>)
3
Iteration 2700: Loss = -10911.255859375
tensor(10911.2539, grad_fn=<NegBackward0>) tensor(10911.2559, grad_fn=<NegBackward0>)
4
Iteration 2800: Loss = -10911.25390625
tensor(10911.2539, grad_fn=<NegBackward0>) tensor(10911.2539, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10911.2548828125
tensor(10911.2539, grad_fn=<NegBackward0>) tensor(10911.2549, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -10911.25390625
tensor(10911.2539, grad_fn=<NegBackward0>) tensor(10911.2539, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10911.2529296875
tensor(10911.2539, grad_fn=<NegBackward0>) tensor(10911.2529, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10911.2548828125
tensor(10911.2529, grad_fn=<NegBackward0>) tensor(10911.2549, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -10911.25390625
tensor(10911.2529, grad_fn=<NegBackward0>) tensor(10911.2539, grad_fn=<NegBackward0>)
2
Iteration 3400: Loss = -10911.251953125
tensor(10911.2529, grad_fn=<NegBackward0>) tensor(10911.2520, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10911.25390625
tensor(10911.2520, grad_fn=<NegBackward0>) tensor(10911.2539, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -10911.2529296875
tensor(10911.2520, grad_fn=<NegBackward0>) tensor(10911.2529, grad_fn=<NegBackward0>)
2
Iteration 3700: Loss = -10911.25390625
tensor(10911.2520, grad_fn=<NegBackward0>) tensor(10911.2539, grad_fn=<NegBackward0>)
3
Iteration 3800: Loss = -10911.2529296875
tensor(10911.2520, grad_fn=<NegBackward0>) tensor(10911.2529, grad_fn=<NegBackward0>)
4
Iteration 3900: Loss = -10911.2529296875
tensor(10911.2520, grad_fn=<NegBackward0>) tensor(10911.2529, grad_fn=<NegBackward0>)
5
Stopping early at iteration 3900 due to no improvement.
pi: tensor([[0.0082, 0.9918],
        [0.0532, 0.9468]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0101, 0.9899], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1956, 0.1943],
         [0.5198, 0.1583]],

        [[0.6349, 0.2535],
         [0.6223, 0.5604]],

        [[0.7089, 0.1479],
         [0.6402, 0.5241]],

        [[0.6967, 0.1796],
         [0.5270, 0.7211]],

        [[0.6531, 0.1660],
         [0.6407, 0.7271]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0070073061126442035
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0018058126460340001
Average Adjusted Rand Index: -0.0014014612225288407
[0.0018058126460340001, 0.0018058126460340001] [-0.0014014612225288407, -0.0014014612225288407] [10911.2529296875, 10911.2529296875]
-------------------------------------
This iteration is 36
True Objective function: Loss = -10812.241250737165
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20132.30859375
inf tensor(20132.3086, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10953.6171875
tensor(20132.3086, grad_fn=<NegBackward0>) tensor(10953.6172, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10952.96484375
tensor(10953.6172, grad_fn=<NegBackward0>) tensor(10952.9648, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10952.642578125
tensor(10952.9648, grad_fn=<NegBackward0>) tensor(10952.6426, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10952.3935546875
tensor(10952.6426, grad_fn=<NegBackward0>) tensor(10952.3936, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10952.2568359375
tensor(10952.3936, grad_fn=<NegBackward0>) tensor(10952.2568, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10952.126953125
tensor(10952.2568, grad_fn=<NegBackward0>) tensor(10952.1270, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10951.9892578125
tensor(10952.1270, grad_fn=<NegBackward0>) tensor(10951.9893, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10951.83984375
tensor(10951.9893, grad_fn=<NegBackward0>) tensor(10951.8398, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10951.67578125
tensor(10951.8398, grad_fn=<NegBackward0>) tensor(10951.6758, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10951.5166015625
tensor(10951.6758, grad_fn=<NegBackward0>) tensor(10951.5166, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10951.3798828125
tensor(10951.5166, grad_fn=<NegBackward0>) tensor(10951.3799, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10951.2763671875
tensor(10951.3799, grad_fn=<NegBackward0>) tensor(10951.2764, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10951.2041015625
tensor(10951.2764, grad_fn=<NegBackward0>) tensor(10951.2041, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10951.1513671875
tensor(10951.2041, grad_fn=<NegBackward0>) tensor(10951.1514, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10951.1123046875
tensor(10951.1514, grad_fn=<NegBackward0>) tensor(10951.1123, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10951.076171875
tensor(10951.1123, grad_fn=<NegBackward0>) tensor(10951.0762, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10951.044921875
tensor(10951.0762, grad_fn=<NegBackward0>) tensor(10951.0449, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10951.0166015625
tensor(10951.0449, grad_fn=<NegBackward0>) tensor(10951.0166, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10950.986328125
tensor(10951.0166, grad_fn=<NegBackward0>) tensor(10950.9863, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10950.9541015625
tensor(10950.9863, grad_fn=<NegBackward0>) tensor(10950.9541, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10950.919921875
tensor(10950.9541, grad_fn=<NegBackward0>) tensor(10950.9199, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10950.8857421875
tensor(10950.9199, grad_fn=<NegBackward0>) tensor(10950.8857, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10950.85546875
tensor(10950.8857, grad_fn=<NegBackward0>) tensor(10950.8555, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10950.826171875
tensor(10950.8555, grad_fn=<NegBackward0>) tensor(10950.8262, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10950.775390625
tensor(10950.8262, grad_fn=<NegBackward0>) tensor(10950.7754, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10950.333984375
tensor(10950.7754, grad_fn=<NegBackward0>) tensor(10950.3340, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10774.2373046875
tensor(10950.3340, grad_fn=<NegBackward0>) tensor(10774.2373, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10773.1181640625
tensor(10774.2373, grad_fn=<NegBackward0>) tensor(10773.1182, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10768.7822265625
tensor(10773.1182, grad_fn=<NegBackward0>) tensor(10768.7822, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10768.74609375
tensor(10768.7822, grad_fn=<NegBackward0>) tensor(10768.7461, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10768.703125
tensor(10768.7461, grad_fn=<NegBackward0>) tensor(10768.7031, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10768.6845703125
tensor(10768.7031, grad_fn=<NegBackward0>) tensor(10768.6846, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10768.666015625
tensor(10768.6846, grad_fn=<NegBackward0>) tensor(10768.6660, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10768.6005859375
tensor(10768.6660, grad_fn=<NegBackward0>) tensor(10768.6006, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10768.5810546875
tensor(10768.6006, grad_fn=<NegBackward0>) tensor(10768.5811, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10768.5712890625
tensor(10768.5811, grad_fn=<NegBackward0>) tensor(10768.5713, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10768.5693359375
tensor(10768.5713, grad_fn=<NegBackward0>) tensor(10768.5693, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10768.548828125
tensor(10768.5693, grad_fn=<NegBackward0>) tensor(10768.5488, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10768.4833984375
tensor(10768.5488, grad_fn=<NegBackward0>) tensor(10768.4834, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10768.484375
tensor(10768.4834, grad_fn=<NegBackward0>) tensor(10768.4844, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -10768.470703125
tensor(10768.4834, grad_fn=<NegBackward0>) tensor(10768.4707, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10768.44921875
tensor(10768.4707, grad_fn=<NegBackward0>) tensor(10768.4492, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10768.4482421875
tensor(10768.4492, grad_fn=<NegBackward0>) tensor(10768.4482, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10768.4453125
tensor(10768.4482, grad_fn=<NegBackward0>) tensor(10768.4453, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10768.4453125
tensor(10768.4453, grad_fn=<NegBackward0>) tensor(10768.4453, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10768.4404296875
tensor(10768.4453, grad_fn=<NegBackward0>) tensor(10768.4404, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10768.4375
tensor(10768.4404, grad_fn=<NegBackward0>) tensor(10768.4375, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10768.4375
tensor(10768.4375, grad_fn=<NegBackward0>) tensor(10768.4375, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10768.4345703125
tensor(10768.4375, grad_fn=<NegBackward0>) tensor(10768.4346, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10768.42578125
tensor(10768.4346, grad_fn=<NegBackward0>) tensor(10768.4258, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10768.421875
tensor(10768.4258, grad_fn=<NegBackward0>) tensor(10768.4219, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10768.421875
tensor(10768.4219, grad_fn=<NegBackward0>) tensor(10768.4219, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10768.416015625
tensor(10768.4219, grad_fn=<NegBackward0>) tensor(10768.4160, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10768.4140625
tensor(10768.4160, grad_fn=<NegBackward0>) tensor(10768.4141, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10768.42578125
tensor(10768.4141, grad_fn=<NegBackward0>) tensor(10768.4258, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10768.41015625
tensor(10768.4141, grad_fn=<NegBackward0>) tensor(10768.4102, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10768.41015625
tensor(10768.4102, grad_fn=<NegBackward0>) tensor(10768.4102, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10768.408203125
tensor(10768.4102, grad_fn=<NegBackward0>) tensor(10768.4082, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10768.4150390625
tensor(10768.4082, grad_fn=<NegBackward0>) tensor(10768.4150, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10768.4033203125
tensor(10768.4082, grad_fn=<NegBackward0>) tensor(10768.4033, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10768.39453125
tensor(10768.4033, grad_fn=<NegBackward0>) tensor(10768.3945, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10768.3544921875
tensor(10768.3945, grad_fn=<NegBackward0>) tensor(10768.3545, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10768.3544921875
tensor(10768.3545, grad_fn=<NegBackward0>) tensor(10768.3545, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10768.353515625
tensor(10768.3545, grad_fn=<NegBackward0>) tensor(10768.3535, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10768.341796875
tensor(10768.3535, grad_fn=<NegBackward0>) tensor(10768.3418, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10768.337890625
tensor(10768.3418, grad_fn=<NegBackward0>) tensor(10768.3379, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10768.3271484375
tensor(10768.3379, grad_fn=<NegBackward0>) tensor(10768.3271, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10768.326171875
tensor(10768.3271, grad_fn=<NegBackward0>) tensor(10768.3262, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10768.3017578125
tensor(10768.3262, grad_fn=<NegBackward0>) tensor(10768.3018, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10768.302734375
tensor(10768.3018, grad_fn=<NegBackward0>) tensor(10768.3027, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10768.3017578125
tensor(10768.3018, grad_fn=<NegBackward0>) tensor(10768.3018, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10768.296875
tensor(10768.3018, grad_fn=<NegBackward0>) tensor(10768.2969, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10768.296875
tensor(10768.2969, grad_fn=<NegBackward0>) tensor(10768.2969, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10768.2939453125
tensor(10768.2969, grad_fn=<NegBackward0>) tensor(10768.2939, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10768.294921875
tensor(10768.2939, grad_fn=<NegBackward0>) tensor(10768.2949, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10768.296875
tensor(10768.2939, grad_fn=<NegBackward0>) tensor(10768.2969, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -10768.2919921875
tensor(10768.2939, grad_fn=<NegBackward0>) tensor(10768.2920, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10768.3154296875
tensor(10768.2920, grad_fn=<NegBackward0>) tensor(10768.3154, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10768.3828125
tensor(10768.2920, grad_fn=<NegBackward0>) tensor(10768.3828, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -10768.2900390625
tensor(10768.2920, grad_fn=<NegBackward0>) tensor(10768.2900, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10768.3037109375
tensor(10768.2900, grad_fn=<NegBackward0>) tensor(10768.3037, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10768.2900390625
tensor(10768.2900, grad_fn=<NegBackward0>) tensor(10768.2900, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10768.287109375
tensor(10768.2900, grad_fn=<NegBackward0>) tensor(10768.2871, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10768.294921875
tensor(10768.2871, grad_fn=<NegBackward0>) tensor(10768.2949, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -10768.2880859375
tensor(10768.2871, grad_fn=<NegBackward0>) tensor(10768.2881, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -10768.4111328125
tensor(10768.2871, grad_fn=<NegBackward0>) tensor(10768.4111, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -10768.287109375
tensor(10768.2871, grad_fn=<NegBackward0>) tensor(10768.2871, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10768.2421875
tensor(10768.2871, grad_fn=<NegBackward0>) tensor(10768.2422, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10768.244140625
tensor(10768.2422, grad_fn=<NegBackward0>) tensor(10768.2441, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -10768.19921875
tensor(10768.2422, grad_fn=<NegBackward0>) tensor(10768.1992, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10768.19921875
tensor(10768.1992, grad_fn=<NegBackward0>) tensor(10768.1992, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10768.1923828125
tensor(10768.1992, grad_fn=<NegBackward0>) tensor(10768.1924, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10768.2099609375
tensor(10768.1924, grad_fn=<NegBackward0>) tensor(10768.2100, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -10768.1923828125
tensor(10768.1924, grad_fn=<NegBackward0>) tensor(10768.1924, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10768.2265625
tensor(10768.1924, grad_fn=<NegBackward0>) tensor(10768.2266, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -10768.1923828125
tensor(10768.1924, grad_fn=<NegBackward0>) tensor(10768.1924, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10768.26171875
tensor(10768.1924, grad_fn=<NegBackward0>) tensor(10768.2617, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -10768.1923828125
tensor(10768.1924, grad_fn=<NegBackward0>) tensor(10768.1924, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10768.216796875
tensor(10768.1924, grad_fn=<NegBackward0>) tensor(10768.2168, grad_fn=<NegBackward0>)
1
pi: tensor([[0.7907, 0.2093],
        [0.1785, 0.8215]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4817, 0.5183], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2613, 0.1053],
         [0.6846, 0.1961]],

        [[0.6974, 0.1005],
         [0.6672, 0.6530]],

        [[0.6458, 0.0945],
         [0.6867, 0.6768]],

        [[0.5348, 0.0924],
         [0.6234, 0.5404]],

        [[0.6092, 0.0952],
         [0.6329, 0.6603]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080702804390127
time is 1
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448338943900694
time is 2
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208
time is 3
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080514782056689
Global Adjusted Rand Index: 0.8683606233217204
Average Adjusted Rand Index: 0.8683503469366795
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23421.875
inf tensor(23421.8750, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10953.5322265625
tensor(23421.8750, grad_fn=<NegBackward0>) tensor(10953.5322, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10952.77734375
tensor(10953.5322, grad_fn=<NegBackward0>) tensor(10952.7773, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10952.4423828125
tensor(10952.7773, grad_fn=<NegBackward0>) tensor(10952.4424, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10952.2978515625
tensor(10952.4424, grad_fn=<NegBackward0>) tensor(10952.2979, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10952.189453125
tensor(10952.2979, grad_fn=<NegBackward0>) tensor(10952.1895, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10952.0654296875
tensor(10952.1895, grad_fn=<NegBackward0>) tensor(10952.0654, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10951.91796875
tensor(10952.0654, grad_fn=<NegBackward0>) tensor(10951.9180, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10951.7392578125
tensor(10951.9180, grad_fn=<NegBackward0>) tensor(10951.7393, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10951.544921875
tensor(10951.7393, grad_fn=<NegBackward0>) tensor(10951.5449, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10951.365234375
tensor(10951.5449, grad_fn=<NegBackward0>) tensor(10951.3652, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10951.2197265625
tensor(10951.3652, grad_fn=<NegBackward0>) tensor(10951.2197, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10951.09375
tensor(10951.2197, grad_fn=<NegBackward0>) tensor(10951.0938, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10950.9921875
tensor(10951.0938, grad_fn=<NegBackward0>) tensor(10950.9922, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10950.91015625
tensor(10950.9922, grad_fn=<NegBackward0>) tensor(10950.9102, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10950.8125
tensor(10950.9102, grad_fn=<NegBackward0>) tensor(10950.8125, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10950.603515625
tensor(10950.8125, grad_fn=<NegBackward0>) tensor(10950.6035, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10860.412109375
tensor(10950.6035, grad_fn=<NegBackward0>) tensor(10860.4121, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10773.5625
tensor(10860.4121, grad_fn=<NegBackward0>) tensor(10773.5625, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10773.208984375
tensor(10773.5625, grad_fn=<NegBackward0>) tensor(10773.2090, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10773.12109375
tensor(10773.2090, grad_fn=<NegBackward0>) tensor(10773.1211, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10768.8203125
tensor(10773.1211, grad_fn=<NegBackward0>) tensor(10768.8203, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10768.7841796875
tensor(10768.8203, grad_fn=<NegBackward0>) tensor(10768.7842, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10768.7431640625
tensor(10768.7842, grad_fn=<NegBackward0>) tensor(10768.7432, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10768.736328125
tensor(10768.7432, grad_fn=<NegBackward0>) tensor(10768.7363, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10768.7109375
tensor(10768.7363, grad_fn=<NegBackward0>) tensor(10768.7109, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10768.701171875
tensor(10768.7109, grad_fn=<NegBackward0>) tensor(10768.7012, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10768.65234375
tensor(10768.7012, grad_fn=<NegBackward0>) tensor(10768.6523, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10768.6513671875
tensor(10768.6523, grad_fn=<NegBackward0>) tensor(10768.6514, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10768.6474609375
tensor(10768.6514, grad_fn=<NegBackward0>) tensor(10768.6475, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10768.6455078125
tensor(10768.6475, grad_fn=<NegBackward0>) tensor(10768.6455, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10768.642578125
tensor(10768.6455, grad_fn=<NegBackward0>) tensor(10768.6426, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10768.640625
tensor(10768.6426, grad_fn=<NegBackward0>) tensor(10768.6406, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10768.6376953125
tensor(10768.6406, grad_fn=<NegBackward0>) tensor(10768.6377, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10768.6181640625
tensor(10768.6377, grad_fn=<NegBackward0>) tensor(10768.6182, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10768.6015625
tensor(10768.6182, grad_fn=<NegBackward0>) tensor(10768.6016, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10768.6015625
tensor(10768.6016, grad_fn=<NegBackward0>) tensor(10768.6016, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10768.599609375
tensor(10768.6016, grad_fn=<NegBackward0>) tensor(10768.5996, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10768.60546875
tensor(10768.5996, grad_fn=<NegBackward0>) tensor(10768.6055, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -10768.59765625
tensor(10768.5996, grad_fn=<NegBackward0>) tensor(10768.5977, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10768.5810546875
tensor(10768.5977, grad_fn=<NegBackward0>) tensor(10768.5811, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10768.5810546875
tensor(10768.5811, grad_fn=<NegBackward0>) tensor(10768.5811, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10768.58203125
tensor(10768.5811, grad_fn=<NegBackward0>) tensor(10768.5820, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10768.580078125
tensor(10768.5811, grad_fn=<NegBackward0>) tensor(10768.5801, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10768.576171875
tensor(10768.5801, grad_fn=<NegBackward0>) tensor(10768.5762, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10768.568359375
tensor(10768.5762, grad_fn=<NegBackward0>) tensor(10768.5684, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10768.5654296875
tensor(10768.5684, grad_fn=<NegBackward0>) tensor(10768.5654, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10768.5654296875
tensor(10768.5654, grad_fn=<NegBackward0>) tensor(10768.5654, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10768.56640625
tensor(10768.5654, grad_fn=<NegBackward0>) tensor(10768.5664, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10768.564453125
tensor(10768.5654, grad_fn=<NegBackward0>) tensor(10768.5645, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10768.5634765625
tensor(10768.5645, grad_fn=<NegBackward0>) tensor(10768.5635, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10768.5615234375
tensor(10768.5635, grad_fn=<NegBackward0>) tensor(10768.5615, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10768.5615234375
tensor(10768.5615, grad_fn=<NegBackward0>) tensor(10768.5615, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10768.560546875
tensor(10768.5615, grad_fn=<NegBackward0>) tensor(10768.5605, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10768.5595703125
tensor(10768.5605, grad_fn=<NegBackward0>) tensor(10768.5596, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10768.5498046875
tensor(10768.5596, grad_fn=<NegBackward0>) tensor(10768.5498, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10768.5400390625
tensor(10768.5498, grad_fn=<NegBackward0>) tensor(10768.5400, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10768.5478515625
tensor(10768.5400, grad_fn=<NegBackward0>) tensor(10768.5479, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10768.5390625
tensor(10768.5400, grad_fn=<NegBackward0>) tensor(10768.5391, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10768.5400390625
tensor(10768.5391, grad_fn=<NegBackward0>) tensor(10768.5400, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10768.5380859375
tensor(10768.5391, grad_fn=<NegBackward0>) tensor(10768.5381, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10768.5400390625
tensor(10768.5381, grad_fn=<NegBackward0>) tensor(10768.5400, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10768.541015625
tensor(10768.5381, grad_fn=<NegBackward0>) tensor(10768.5410, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -10768.5390625
tensor(10768.5381, grad_fn=<NegBackward0>) tensor(10768.5391, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -10768.5380859375
tensor(10768.5381, grad_fn=<NegBackward0>) tensor(10768.5381, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10768.541015625
tensor(10768.5381, grad_fn=<NegBackward0>) tensor(10768.5410, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10768.5380859375
tensor(10768.5381, grad_fn=<NegBackward0>) tensor(10768.5381, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10768.5234375
tensor(10768.5381, grad_fn=<NegBackward0>) tensor(10768.5234, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10768.46875
tensor(10768.5234, grad_fn=<NegBackward0>) tensor(10768.4688, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10768.466796875
tensor(10768.4688, grad_fn=<NegBackward0>) tensor(10768.4668, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10768.4697265625
tensor(10768.4668, grad_fn=<NegBackward0>) tensor(10768.4697, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10768.462890625
tensor(10768.4668, grad_fn=<NegBackward0>) tensor(10768.4629, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10768.4599609375
tensor(10768.4629, grad_fn=<NegBackward0>) tensor(10768.4600, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10768.458984375
tensor(10768.4600, grad_fn=<NegBackward0>) tensor(10768.4590, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10768.453125
tensor(10768.4590, grad_fn=<NegBackward0>) tensor(10768.4531, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10768.4501953125
tensor(10768.4531, grad_fn=<NegBackward0>) tensor(10768.4502, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10768.44921875
tensor(10768.4502, grad_fn=<NegBackward0>) tensor(10768.4492, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10768.451171875
tensor(10768.4492, grad_fn=<NegBackward0>) tensor(10768.4512, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10768.44921875
tensor(10768.4492, grad_fn=<NegBackward0>) tensor(10768.4492, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10768.44921875
tensor(10768.4492, grad_fn=<NegBackward0>) tensor(10768.4492, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10768.474609375
tensor(10768.4492, grad_fn=<NegBackward0>) tensor(10768.4746, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10768.4482421875
tensor(10768.4492, grad_fn=<NegBackward0>) tensor(10768.4482, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10768.44921875
tensor(10768.4482, grad_fn=<NegBackward0>) tensor(10768.4492, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10768.4482421875
tensor(10768.4482, grad_fn=<NegBackward0>) tensor(10768.4482, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10768.4482421875
tensor(10768.4482, grad_fn=<NegBackward0>) tensor(10768.4482, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10768.447265625
tensor(10768.4482, grad_fn=<NegBackward0>) tensor(10768.4473, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10768.43359375
tensor(10768.4473, grad_fn=<NegBackward0>) tensor(10768.4336, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10768.4326171875
tensor(10768.4336, grad_fn=<NegBackward0>) tensor(10768.4326, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10768.43359375
tensor(10768.4326, grad_fn=<NegBackward0>) tensor(10768.4336, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10768.4345703125
tensor(10768.4326, grad_fn=<NegBackward0>) tensor(10768.4346, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -10768.4267578125
tensor(10768.4326, grad_fn=<NegBackward0>) tensor(10768.4268, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10768.431640625
tensor(10768.4268, grad_fn=<NegBackward0>) tensor(10768.4316, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10768.427734375
tensor(10768.4268, grad_fn=<NegBackward0>) tensor(10768.4277, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -10768.42578125
tensor(10768.4268, grad_fn=<NegBackward0>) tensor(10768.4258, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10768.419921875
tensor(10768.4258, grad_fn=<NegBackward0>) tensor(10768.4199, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10768.41015625
tensor(10768.4199, grad_fn=<NegBackward0>) tensor(10768.4102, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10768.4091796875
tensor(10768.4102, grad_fn=<NegBackward0>) tensor(10768.4092, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10768.408203125
tensor(10768.4092, grad_fn=<NegBackward0>) tensor(10768.4082, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10768.4091796875
tensor(10768.4082, grad_fn=<NegBackward0>) tensor(10768.4092, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -10768.4111328125
tensor(10768.4082, grad_fn=<NegBackward0>) tensor(10768.4111, grad_fn=<NegBackward0>)
2
pi: tensor([[0.8219, 0.1781],
        [0.2093, 0.7907]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5185, 0.4815], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1961, 0.1058],
         [0.6671, 0.2612]],

        [[0.7074, 0.1005],
         [0.6045, 0.7310]],

        [[0.5244, 0.0947],
         [0.6749, 0.5855]],

        [[0.7171, 0.0924],
         [0.5084, 0.6938]],

        [[0.6497, 0.0959],
         [0.6437, 0.5816]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080702804390127
time is 1
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
time is 2
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208
time is 3
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080514782056689
Global Adjusted Rand Index: 0.8683606233217204
Average Adjusted Rand Index: 0.8683503469366795
[0.8683606233217204, 0.8683606233217204] [0.8683503469366795, 0.8683503469366795] [10768.1904296875, 10768.369140625]
-------------------------------------
This iteration is 37
True Objective function: Loss = -10990.694375737165
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23727.548828125
inf tensor(23727.5488, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11118.9833984375
tensor(23727.5488, grad_fn=<NegBackward0>) tensor(11118.9834, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11118.2802734375
tensor(11118.9834, grad_fn=<NegBackward0>) tensor(11118.2803, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11117.607421875
tensor(11118.2803, grad_fn=<NegBackward0>) tensor(11117.6074, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11117.1376953125
tensor(11117.6074, grad_fn=<NegBackward0>) tensor(11117.1377, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11116.828125
tensor(11117.1377, grad_fn=<NegBackward0>) tensor(11116.8281, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11116.607421875
tensor(11116.8281, grad_fn=<NegBackward0>) tensor(11116.6074, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11116.419921875
tensor(11116.6074, grad_fn=<NegBackward0>) tensor(11116.4199, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11116.2109375
tensor(11116.4199, grad_fn=<NegBackward0>) tensor(11116.2109, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11115.8720703125
tensor(11116.2109, grad_fn=<NegBackward0>) tensor(11115.8721, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11115.2734375
tensor(11115.8721, grad_fn=<NegBackward0>) tensor(11115.2734, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11114.8828125
tensor(11115.2734, grad_fn=<NegBackward0>) tensor(11114.8828, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11114.53125
tensor(11114.8828, grad_fn=<NegBackward0>) tensor(11114.5312, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11114.173828125
tensor(11114.5312, grad_fn=<NegBackward0>) tensor(11114.1738, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11114.0185546875
tensor(11114.1738, grad_fn=<NegBackward0>) tensor(11114.0186, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11113.9326171875
tensor(11114.0186, grad_fn=<NegBackward0>) tensor(11113.9326, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11113.4853515625
tensor(11113.9326, grad_fn=<NegBackward0>) tensor(11113.4854, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11018.935546875
tensor(11113.4854, grad_fn=<NegBackward0>) tensor(11018.9355, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11018.45703125
tensor(11018.9355, grad_fn=<NegBackward0>) tensor(11018.4570, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11018.38671875
tensor(11018.4570, grad_fn=<NegBackward0>) tensor(11018.3867, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11018.35546875
tensor(11018.3867, grad_fn=<NegBackward0>) tensor(11018.3555, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11018.326171875
tensor(11018.3555, grad_fn=<NegBackward0>) tensor(11018.3262, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11018.310546875
tensor(11018.3262, grad_fn=<NegBackward0>) tensor(11018.3105, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11018.2900390625
tensor(11018.3105, grad_fn=<NegBackward0>) tensor(11018.2900, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11018.25390625
tensor(11018.2900, grad_fn=<NegBackward0>) tensor(11018.2539, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11018.115234375
tensor(11018.2539, grad_fn=<NegBackward0>) tensor(11018.1152, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11017.427734375
tensor(11018.1152, grad_fn=<NegBackward0>) tensor(11017.4277, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10973.06640625
tensor(11017.4277, grad_fn=<NegBackward0>) tensor(10973.0664, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10964.7783203125
tensor(10973.0664, grad_fn=<NegBackward0>) tensor(10964.7783, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10956.18359375
tensor(10964.7783, grad_fn=<NegBackward0>) tensor(10956.1836, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10956.1748046875
tensor(10956.1836, grad_fn=<NegBackward0>) tensor(10956.1748, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10956.169921875
tensor(10956.1748, grad_fn=<NegBackward0>) tensor(10956.1699, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10956.1455078125
tensor(10956.1699, grad_fn=<NegBackward0>) tensor(10956.1455, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10956.1455078125
tensor(10956.1455, grad_fn=<NegBackward0>) tensor(10956.1455, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10956.146484375
tensor(10956.1455, grad_fn=<NegBackward0>) tensor(10956.1465, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -10956.1435546875
tensor(10956.1455, grad_fn=<NegBackward0>) tensor(10956.1436, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10956.1435546875
tensor(10956.1436, grad_fn=<NegBackward0>) tensor(10956.1436, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10956.1435546875
tensor(10956.1436, grad_fn=<NegBackward0>) tensor(10956.1436, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10956.146484375
tensor(10956.1436, grad_fn=<NegBackward0>) tensor(10956.1465, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -10956.142578125
tensor(10956.1436, grad_fn=<NegBackward0>) tensor(10956.1426, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10956.142578125
tensor(10956.1426, grad_fn=<NegBackward0>) tensor(10956.1426, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10956.1416015625
tensor(10956.1426, grad_fn=<NegBackward0>) tensor(10956.1416, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10956.140625
tensor(10956.1416, grad_fn=<NegBackward0>) tensor(10956.1406, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10956.142578125
tensor(10956.1406, grad_fn=<NegBackward0>) tensor(10956.1426, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -10956.14453125
tensor(10956.1406, grad_fn=<NegBackward0>) tensor(10956.1445, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -10956.1416015625
tensor(10956.1406, grad_fn=<NegBackward0>) tensor(10956.1416, grad_fn=<NegBackward0>)
3
Iteration 4600: Loss = -10956.142578125
tensor(10956.1406, grad_fn=<NegBackward0>) tensor(10956.1426, grad_fn=<NegBackward0>)
4
Iteration 4700: Loss = -10956.140625
tensor(10956.1406, grad_fn=<NegBackward0>) tensor(10956.1406, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10956.138671875
tensor(10956.1406, grad_fn=<NegBackward0>) tensor(10956.1387, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10956.134765625
tensor(10956.1387, grad_fn=<NegBackward0>) tensor(10956.1348, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10956.134765625
tensor(10956.1348, grad_fn=<NegBackward0>) tensor(10956.1348, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10956.1337890625
tensor(10956.1348, grad_fn=<NegBackward0>) tensor(10956.1338, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10956.1357421875
tensor(10956.1338, grad_fn=<NegBackward0>) tensor(10956.1357, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10956.1328125
tensor(10956.1338, grad_fn=<NegBackward0>) tensor(10956.1328, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10956.1318359375
tensor(10956.1328, grad_fn=<NegBackward0>) tensor(10956.1318, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10956.1337890625
tensor(10956.1318, grad_fn=<NegBackward0>) tensor(10956.1338, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10956.1328125
tensor(10956.1318, grad_fn=<NegBackward0>) tensor(10956.1328, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -10956.1328125
tensor(10956.1318, grad_fn=<NegBackward0>) tensor(10956.1328, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -10956.134765625
tensor(10956.1318, grad_fn=<NegBackward0>) tensor(10956.1348, grad_fn=<NegBackward0>)
4
Iteration 5900: Loss = -10956.134765625
tensor(10956.1318, grad_fn=<NegBackward0>) tensor(10956.1348, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5900 due to no improvement.
pi: tensor([[0.7628, 0.2372],
        [0.1741, 0.8259]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5016, 0.4984], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2078, 0.0968],
         [0.6436, 0.2490]],

        [[0.6910, 0.0992],
         [0.6919, 0.6408]],

        [[0.6209, 0.1089],
         [0.7258, 0.6490]],

        [[0.7171, 0.0933],
         [0.6183, 0.6350]],

        [[0.7081, 0.1034],
         [0.6908, 0.5019]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080650161849864
time is 1
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 2
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9206925302859384
Global Adjusted Rand Index: 0.8833667419806591
Average Adjusted Rand Index: 0.8830428095652485
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22468.6171875
inf tensor(22468.6172, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11119.873046875
tensor(22468.6172, grad_fn=<NegBackward0>) tensor(11119.8730, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11118.7919921875
tensor(11119.8730, grad_fn=<NegBackward0>) tensor(11118.7920, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11118.3271484375
tensor(11118.7920, grad_fn=<NegBackward0>) tensor(11118.3271, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11117.9189453125
tensor(11118.3271, grad_fn=<NegBackward0>) tensor(11117.9189, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11115.9013671875
tensor(11117.9189, grad_fn=<NegBackward0>) tensor(11115.9014, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11113.0537109375
tensor(11115.9014, grad_fn=<NegBackward0>) tensor(11113.0537, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11112.4716796875
tensor(11113.0537, grad_fn=<NegBackward0>) tensor(11112.4717, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11112.2568359375
tensor(11112.4717, grad_fn=<NegBackward0>) tensor(11112.2568, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11112.1455078125
tensor(11112.2568, grad_fn=<NegBackward0>) tensor(11112.1455, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11112.0771484375
tensor(11112.1455, grad_fn=<NegBackward0>) tensor(11112.0771, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11112.0322265625
tensor(11112.0771, grad_fn=<NegBackward0>) tensor(11112.0322, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11112.001953125
tensor(11112.0322, grad_fn=<NegBackward0>) tensor(11112.0020, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11111.98046875
tensor(11112.0020, grad_fn=<NegBackward0>) tensor(11111.9805, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11111.9619140625
tensor(11111.9805, grad_fn=<NegBackward0>) tensor(11111.9619, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11111.94921875
tensor(11111.9619, grad_fn=<NegBackward0>) tensor(11111.9492, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11111.9375
tensor(11111.9492, grad_fn=<NegBackward0>) tensor(11111.9375, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11111.9287109375
tensor(11111.9375, grad_fn=<NegBackward0>) tensor(11111.9287, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11111.919921875
tensor(11111.9287, grad_fn=<NegBackward0>) tensor(11111.9199, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11111.9130859375
tensor(11111.9199, grad_fn=<NegBackward0>) tensor(11111.9131, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11111.9072265625
tensor(11111.9131, grad_fn=<NegBackward0>) tensor(11111.9072, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11111.9013671875
tensor(11111.9072, grad_fn=<NegBackward0>) tensor(11111.9014, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11111.8955078125
tensor(11111.9014, grad_fn=<NegBackward0>) tensor(11111.8955, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11111.8896484375
tensor(11111.8955, grad_fn=<NegBackward0>) tensor(11111.8896, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11111.8857421875
tensor(11111.8896, grad_fn=<NegBackward0>) tensor(11111.8857, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11111.880859375
tensor(11111.8857, grad_fn=<NegBackward0>) tensor(11111.8809, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11111.8759765625
tensor(11111.8809, grad_fn=<NegBackward0>) tensor(11111.8760, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11111.8720703125
tensor(11111.8760, grad_fn=<NegBackward0>) tensor(11111.8721, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11111.8642578125
tensor(11111.8721, grad_fn=<NegBackward0>) tensor(11111.8643, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11111.861328125
tensor(11111.8643, grad_fn=<NegBackward0>) tensor(11111.8613, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11111.8564453125
tensor(11111.8613, grad_fn=<NegBackward0>) tensor(11111.8564, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11111.8525390625
tensor(11111.8564, grad_fn=<NegBackward0>) tensor(11111.8525, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11111.849609375
tensor(11111.8525, grad_fn=<NegBackward0>) tensor(11111.8496, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11111.8466796875
tensor(11111.8496, grad_fn=<NegBackward0>) tensor(11111.8467, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11111.84375
tensor(11111.8467, grad_fn=<NegBackward0>) tensor(11111.8438, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11111.8447265625
tensor(11111.8438, grad_fn=<NegBackward0>) tensor(11111.8447, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11111.8408203125
tensor(11111.8438, grad_fn=<NegBackward0>) tensor(11111.8408, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11111.837890625
tensor(11111.8408, grad_fn=<NegBackward0>) tensor(11111.8379, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11111.837890625
tensor(11111.8379, grad_fn=<NegBackward0>) tensor(11111.8379, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11111.837890625
tensor(11111.8379, grad_fn=<NegBackward0>) tensor(11111.8379, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11111.8359375
tensor(11111.8379, grad_fn=<NegBackward0>) tensor(11111.8359, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11111.8359375
tensor(11111.8359, grad_fn=<NegBackward0>) tensor(11111.8359, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11111.8349609375
tensor(11111.8359, grad_fn=<NegBackward0>) tensor(11111.8350, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11111.8330078125
tensor(11111.8350, grad_fn=<NegBackward0>) tensor(11111.8330, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11111.833984375
tensor(11111.8330, grad_fn=<NegBackward0>) tensor(11111.8340, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11111.8330078125
tensor(11111.8330, grad_fn=<NegBackward0>) tensor(11111.8330, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11111.83203125
tensor(11111.8330, grad_fn=<NegBackward0>) tensor(11111.8320, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11111.830078125
tensor(11111.8320, grad_fn=<NegBackward0>) tensor(11111.8301, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11111.8310546875
tensor(11111.8301, grad_fn=<NegBackward0>) tensor(11111.8311, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11111.8310546875
tensor(11111.8301, grad_fn=<NegBackward0>) tensor(11111.8311, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11111.8310546875
tensor(11111.8301, grad_fn=<NegBackward0>) tensor(11111.8311, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -11111.8291015625
tensor(11111.8301, grad_fn=<NegBackward0>) tensor(11111.8291, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11111.8291015625
tensor(11111.8291, grad_fn=<NegBackward0>) tensor(11111.8291, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11111.828125
tensor(11111.8291, grad_fn=<NegBackward0>) tensor(11111.8281, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11111.828125
tensor(11111.8281, grad_fn=<NegBackward0>) tensor(11111.8281, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11111.8291015625
tensor(11111.8281, grad_fn=<NegBackward0>) tensor(11111.8291, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11111.8271484375
tensor(11111.8281, grad_fn=<NegBackward0>) tensor(11111.8271, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11111.828125
tensor(11111.8271, grad_fn=<NegBackward0>) tensor(11111.8281, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11111.8271484375
tensor(11111.8271, grad_fn=<NegBackward0>) tensor(11111.8271, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11111.826171875
tensor(11111.8271, grad_fn=<NegBackward0>) tensor(11111.8262, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11111.8271484375
tensor(11111.8262, grad_fn=<NegBackward0>) tensor(11111.8271, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11111.8251953125
tensor(11111.8262, grad_fn=<NegBackward0>) tensor(11111.8252, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11111.8251953125
tensor(11111.8252, grad_fn=<NegBackward0>) tensor(11111.8252, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11111.8271484375
tensor(11111.8252, grad_fn=<NegBackward0>) tensor(11111.8271, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11111.8271484375
tensor(11111.8252, grad_fn=<NegBackward0>) tensor(11111.8271, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11111.8251953125
tensor(11111.8252, grad_fn=<NegBackward0>) tensor(11111.8252, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11111.8251953125
tensor(11111.8252, grad_fn=<NegBackward0>) tensor(11111.8252, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11111.8271484375
tensor(11111.8252, grad_fn=<NegBackward0>) tensor(11111.8271, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11111.826171875
tensor(11111.8252, grad_fn=<NegBackward0>) tensor(11111.8262, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11111.826171875
tensor(11111.8252, grad_fn=<NegBackward0>) tensor(11111.8262, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11111.8251953125
tensor(11111.8252, grad_fn=<NegBackward0>) tensor(11111.8252, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11111.8310546875
tensor(11111.8252, grad_fn=<NegBackward0>) tensor(11111.8311, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11111.8251953125
tensor(11111.8252, grad_fn=<NegBackward0>) tensor(11111.8252, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11111.8271484375
tensor(11111.8252, grad_fn=<NegBackward0>) tensor(11111.8271, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11111.8251953125
tensor(11111.8252, grad_fn=<NegBackward0>) tensor(11111.8252, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11111.826171875
tensor(11111.8252, grad_fn=<NegBackward0>) tensor(11111.8262, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11111.826171875
tensor(11111.8252, grad_fn=<NegBackward0>) tensor(11111.8262, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11111.89453125
tensor(11111.8252, grad_fn=<NegBackward0>) tensor(11111.8945, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11111.82421875
tensor(11111.8252, grad_fn=<NegBackward0>) tensor(11111.8242, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11111.8251953125
tensor(11111.8242, grad_fn=<NegBackward0>) tensor(11111.8252, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11111.8251953125
tensor(11111.8242, grad_fn=<NegBackward0>) tensor(11111.8252, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11111.82421875
tensor(11111.8242, grad_fn=<NegBackward0>) tensor(11111.8242, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11111.82421875
tensor(11111.8242, grad_fn=<NegBackward0>) tensor(11111.8242, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11111.8505859375
tensor(11111.8242, grad_fn=<NegBackward0>) tensor(11111.8506, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11111.82421875
tensor(11111.8242, grad_fn=<NegBackward0>) tensor(11111.8242, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11111.8251953125
tensor(11111.8242, grad_fn=<NegBackward0>) tensor(11111.8252, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11111.82421875
tensor(11111.8242, grad_fn=<NegBackward0>) tensor(11111.8242, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11111.8232421875
tensor(11111.8242, grad_fn=<NegBackward0>) tensor(11111.8232, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11111.8408203125
tensor(11111.8232, grad_fn=<NegBackward0>) tensor(11111.8408, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11111.82421875
tensor(11111.8232, grad_fn=<NegBackward0>) tensor(11111.8242, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -11111.8251953125
tensor(11111.8232, grad_fn=<NegBackward0>) tensor(11111.8252, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -11111.8251953125
tensor(11111.8232, grad_fn=<NegBackward0>) tensor(11111.8252, grad_fn=<NegBackward0>)
4
Iteration 9200: Loss = -11111.8232421875
tensor(11111.8232, grad_fn=<NegBackward0>) tensor(11111.8232, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11112.02734375
tensor(11111.8232, grad_fn=<NegBackward0>) tensor(11112.0273, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11111.8232421875
tensor(11111.8232, grad_fn=<NegBackward0>) tensor(11111.8232, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11111.8232421875
tensor(11111.8232, grad_fn=<NegBackward0>) tensor(11111.8232, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11112.166015625
tensor(11111.8232, grad_fn=<NegBackward0>) tensor(11112.1660, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11111.8232421875
tensor(11111.8232, grad_fn=<NegBackward0>) tensor(11111.8232, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11111.82421875
tensor(11111.8232, grad_fn=<NegBackward0>) tensor(11111.8242, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11111.8232421875
tensor(11111.8232, grad_fn=<NegBackward0>) tensor(11111.8232, grad_fn=<NegBackward0>)
pi: tensor([[1.7597e-04, 9.9982e-01],
        [3.8568e-02, 9.6143e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0163, 0.9837], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2118, 0.1378],
         [0.6180, 0.1660]],

        [[0.5178, 0.1702],
         [0.5145, 0.5068]],

        [[0.6153, 0.1792],
         [0.6126, 0.6103]],

        [[0.6268, 0.0570],
         [0.6690, 0.5539]],

        [[0.6241, 0.2302],
         [0.6492, 0.6477]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 59
Adjusted Rand Index: -0.006658343736995423
Global Adjusted Rand Index: 0.00036718158158499854
Average Adjusted Rand Index: 0.0006077251919948548
[0.8833667419806591, 0.00036718158158499854] [0.8830428095652485, 0.0006077251919948548] [10956.134765625, 11111.8232421875]
-------------------------------------
This iteration is 38
True Objective function: Loss = -10943.961810195342
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23684.984375
inf tensor(23684.9844, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11065.8583984375
tensor(23684.9844, grad_fn=<NegBackward0>) tensor(11065.8584, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11065.3603515625
tensor(11065.8584, grad_fn=<NegBackward0>) tensor(11065.3604, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11065.1650390625
tensor(11065.3604, grad_fn=<NegBackward0>) tensor(11065.1650, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11064.9404296875
tensor(11065.1650, grad_fn=<NegBackward0>) tensor(11064.9404, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11064.6708984375
tensor(11064.9404, grad_fn=<NegBackward0>) tensor(11064.6709, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11064.3564453125
tensor(11064.6709, grad_fn=<NegBackward0>) tensor(11064.3564, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11064.091796875
tensor(11064.3564, grad_fn=<NegBackward0>) tensor(11064.0918, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11063.8505859375
tensor(11064.0918, grad_fn=<NegBackward0>) tensor(11063.8506, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11063.501953125
tensor(11063.8506, grad_fn=<NegBackward0>) tensor(11063.5020, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11062.7958984375
tensor(11063.5020, grad_fn=<NegBackward0>) tensor(11062.7959, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11062.28125
tensor(11062.7959, grad_fn=<NegBackward0>) tensor(11062.2812, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11061.880859375
tensor(11062.2812, grad_fn=<NegBackward0>) tensor(11061.8809, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11061.55078125
tensor(11061.8809, grad_fn=<NegBackward0>) tensor(11061.5508, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11061.2626953125
tensor(11061.5508, grad_fn=<NegBackward0>) tensor(11061.2627, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11060.9970703125
tensor(11061.2627, grad_fn=<NegBackward0>) tensor(11060.9971, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11060.791015625
tensor(11060.9971, grad_fn=<NegBackward0>) tensor(11060.7910, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11060.6376953125
tensor(11060.7910, grad_fn=<NegBackward0>) tensor(11060.6377, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11060.5234375
tensor(11060.6377, grad_fn=<NegBackward0>) tensor(11060.5234, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11060.4521484375
tensor(11060.5234, grad_fn=<NegBackward0>) tensor(11060.4521, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11060.40234375
tensor(11060.4521, grad_fn=<NegBackward0>) tensor(11060.4023, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11060.3681640625
tensor(11060.4023, grad_fn=<NegBackward0>) tensor(11060.3682, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11060.34375
tensor(11060.3682, grad_fn=<NegBackward0>) tensor(11060.3438, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11060.32421875
tensor(11060.3438, grad_fn=<NegBackward0>) tensor(11060.3242, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11060.3115234375
tensor(11060.3242, grad_fn=<NegBackward0>) tensor(11060.3115, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11060.2998046875
tensor(11060.3115, grad_fn=<NegBackward0>) tensor(11060.2998, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11060.2900390625
tensor(11060.2998, grad_fn=<NegBackward0>) tensor(11060.2900, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11060.2841796875
tensor(11060.2900, grad_fn=<NegBackward0>) tensor(11060.2842, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11060.28125
tensor(11060.2842, grad_fn=<NegBackward0>) tensor(11060.2812, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11060.2744140625
tensor(11060.2812, grad_fn=<NegBackward0>) tensor(11060.2744, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11060.2685546875
tensor(11060.2744, grad_fn=<NegBackward0>) tensor(11060.2686, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11060.2646484375
tensor(11060.2686, grad_fn=<NegBackward0>) tensor(11060.2646, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11060.26171875
tensor(11060.2646, grad_fn=<NegBackward0>) tensor(11060.2617, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11060.259765625
tensor(11060.2617, grad_fn=<NegBackward0>) tensor(11060.2598, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11060.2568359375
tensor(11060.2598, grad_fn=<NegBackward0>) tensor(11060.2568, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11060.25390625
tensor(11060.2568, grad_fn=<NegBackward0>) tensor(11060.2539, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11060.251953125
tensor(11060.2539, grad_fn=<NegBackward0>) tensor(11060.2520, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11060.2490234375
tensor(11060.2520, grad_fn=<NegBackward0>) tensor(11060.2490, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11060.25
tensor(11060.2490, grad_fn=<NegBackward0>) tensor(11060.2500, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11060.2470703125
tensor(11060.2490, grad_fn=<NegBackward0>) tensor(11060.2471, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11060.2470703125
tensor(11060.2471, grad_fn=<NegBackward0>) tensor(11060.2471, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11060.2451171875
tensor(11060.2471, grad_fn=<NegBackward0>) tensor(11060.2451, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11060.244140625
tensor(11060.2451, grad_fn=<NegBackward0>) tensor(11060.2441, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11060.2431640625
tensor(11060.2441, grad_fn=<NegBackward0>) tensor(11060.2432, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11060.2412109375
tensor(11060.2432, grad_fn=<NegBackward0>) tensor(11060.2412, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11060.240234375
tensor(11060.2412, grad_fn=<NegBackward0>) tensor(11060.2402, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11060.2412109375
tensor(11060.2402, grad_fn=<NegBackward0>) tensor(11060.2412, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11060.240234375
tensor(11060.2402, grad_fn=<NegBackward0>) tensor(11060.2402, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11060.2392578125
tensor(11060.2402, grad_fn=<NegBackward0>) tensor(11060.2393, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11060.23828125
tensor(11060.2393, grad_fn=<NegBackward0>) tensor(11060.2383, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11060.2412109375
tensor(11060.2383, grad_fn=<NegBackward0>) tensor(11060.2412, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11060.236328125
tensor(11060.2383, grad_fn=<NegBackward0>) tensor(11060.2363, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11060.236328125
tensor(11060.2363, grad_fn=<NegBackward0>) tensor(11060.2363, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11060.2353515625
tensor(11060.2363, grad_fn=<NegBackward0>) tensor(11060.2354, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11060.2353515625
tensor(11060.2354, grad_fn=<NegBackward0>) tensor(11060.2354, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11060.236328125
tensor(11060.2354, grad_fn=<NegBackward0>) tensor(11060.2363, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11060.236328125
tensor(11060.2354, grad_fn=<NegBackward0>) tensor(11060.2363, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11060.234375
tensor(11060.2354, grad_fn=<NegBackward0>) tensor(11060.2344, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11060.234375
tensor(11060.2344, grad_fn=<NegBackward0>) tensor(11060.2344, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11060.234375
tensor(11060.2344, grad_fn=<NegBackward0>) tensor(11060.2344, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11060.234375
tensor(11060.2344, grad_fn=<NegBackward0>) tensor(11060.2344, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11060.234375
tensor(11060.2344, grad_fn=<NegBackward0>) tensor(11060.2344, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11060.232421875
tensor(11060.2344, grad_fn=<NegBackward0>) tensor(11060.2324, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11060.2333984375
tensor(11060.2324, grad_fn=<NegBackward0>) tensor(11060.2334, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11060.232421875
tensor(11060.2324, grad_fn=<NegBackward0>) tensor(11060.2324, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11060.232421875
tensor(11060.2324, grad_fn=<NegBackward0>) tensor(11060.2324, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11060.236328125
tensor(11060.2324, grad_fn=<NegBackward0>) tensor(11060.2363, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11060.232421875
tensor(11060.2324, grad_fn=<NegBackward0>) tensor(11060.2324, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11060.232421875
tensor(11060.2324, grad_fn=<NegBackward0>) tensor(11060.2324, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11060.23046875
tensor(11060.2324, grad_fn=<NegBackward0>) tensor(11060.2305, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11060.2314453125
tensor(11060.2305, grad_fn=<NegBackward0>) tensor(11060.2314, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11060.2314453125
tensor(11060.2305, grad_fn=<NegBackward0>) tensor(11060.2314, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11060.23046875
tensor(11060.2305, grad_fn=<NegBackward0>) tensor(11060.2305, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11060.23046875
tensor(11060.2305, grad_fn=<NegBackward0>) tensor(11060.2305, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11060.2314453125
tensor(11060.2305, grad_fn=<NegBackward0>) tensor(11060.2314, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11060.23046875
tensor(11060.2305, grad_fn=<NegBackward0>) tensor(11060.2305, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11060.23046875
tensor(11060.2305, grad_fn=<NegBackward0>) tensor(11060.2305, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11060.23046875
tensor(11060.2305, grad_fn=<NegBackward0>) tensor(11060.2305, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11060.2294921875
tensor(11060.2305, grad_fn=<NegBackward0>) tensor(11060.2295, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11060.23046875
tensor(11060.2295, grad_fn=<NegBackward0>) tensor(11060.2305, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11060.2294921875
tensor(11060.2295, grad_fn=<NegBackward0>) tensor(11060.2295, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11060.23046875
tensor(11060.2295, grad_fn=<NegBackward0>) tensor(11060.2305, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11060.2314453125
tensor(11060.2295, grad_fn=<NegBackward0>) tensor(11060.2314, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11060.2294921875
tensor(11060.2295, grad_fn=<NegBackward0>) tensor(11060.2295, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11060.2294921875
tensor(11060.2295, grad_fn=<NegBackward0>) tensor(11060.2295, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11060.23046875
tensor(11060.2295, grad_fn=<NegBackward0>) tensor(11060.2305, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11060.2431640625
tensor(11060.2295, grad_fn=<NegBackward0>) tensor(11060.2432, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11060.2294921875
tensor(11060.2295, grad_fn=<NegBackward0>) tensor(11060.2295, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11060.24609375
tensor(11060.2295, grad_fn=<NegBackward0>) tensor(11060.2461, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11060.2294921875
tensor(11060.2295, grad_fn=<NegBackward0>) tensor(11060.2295, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11060.5322265625
tensor(11060.2295, grad_fn=<NegBackward0>) tensor(11060.5322, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11060.228515625
tensor(11060.2295, grad_fn=<NegBackward0>) tensor(11060.2285, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11060.23046875
tensor(11060.2285, grad_fn=<NegBackward0>) tensor(11060.2305, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11060.23046875
tensor(11060.2285, grad_fn=<NegBackward0>) tensor(11060.2305, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11060.2294921875
tensor(11060.2285, grad_fn=<NegBackward0>) tensor(11060.2295, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -11060.228515625
tensor(11060.2285, grad_fn=<NegBackward0>) tensor(11060.2285, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11060.2294921875
tensor(11060.2285, grad_fn=<NegBackward0>) tensor(11060.2295, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11060.23046875
tensor(11060.2285, grad_fn=<NegBackward0>) tensor(11060.2305, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -11060.2353515625
tensor(11060.2285, grad_fn=<NegBackward0>) tensor(11060.2354, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -11060.2275390625
tensor(11060.2285, grad_fn=<NegBackward0>) tensor(11060.2275, grad_fn=<NegBackward0>)
pi: tensor([[9.9999e-01, 8.9106e-06],
        [1.2232e-01, 8.7768e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8957, 0.1043], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1629, 0.1937],
         [0.6644, 0.2622]],

        [[0.5976, 0.1672],
         [0.6837, 0.5882]],

        [[0.6948, 0.1721],
         [0.5475, 0.5277]],

        [[0.5233, 0.2083],
         [0.6965, 0.6234]],

        [[0.6997, 0.0882],
         [0.5759, 0.7190]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.0037746410354473374
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.003422492374587702
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.002283992613470697
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
Global Adjusted Rand Index: -0.0004163923331970167
Average Adjusted Rand Index: -0.003627816869061387
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22186.40625
inf tensor(22186.4062, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11065.33203125
tensor(22186.4062, grad_fn=<NegBackward0>) tensor(11065.3320, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11064.8662109375
tensor(11065.3320, grad_fn=<NegBackward0>) tensor(11064.8662, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11064.7568359375
tensor(11064.8662, grad_fn=<NegBackward0>) tensor(11064.7568, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11064.697265625
tensor(11064.7568, grad_fn=<NegBackward0>) tensor(11064.6973, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11064.65625
tensor(11064.6973, grad_fn=<NegBackward0>) tensor(11064.6562, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11064.619140625
tensor(11064.6562, grad_fn=<NegBackward0>) tensor(11064.6191, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11064.58203125
tensor(11064.6191, grad_fn=<NegBackward0>) tensor(11064.5820, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11064.5361328125
tensor(11064.5820, grad_fn=<NegBackward0>) tensor(11064.5361, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11064.4873046875
tensor(11064.5361, grad_fn=<NegBackward0>) tensor(11064.4873, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11064.4580078125
tensor(11064.4873, grad_fn=<NegBackward0>) tensor(11064.4580, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11064.4384765625
tensor(11064.4580, grad_fn=<NegBackward0>) tensor(11064.4385, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11064.4267578125
tensor(11064.4385, grad_fn=<NegBackward0>) tensor(11064.4268, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11064.4169921875
tensor(11064.4268, grad_fn=<NegBackward0>) tensor(11064.4170, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11064.41015625
tensor(11064.4170, grad_fn=<NegBackward0>) tensor(11064.4102, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11064.4033203125
tensor(11064.4102, grad_fn=<NegBackward0>) tensor(11064.4033, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11064.3994140625
tensor(11064.4033, grad_fn=<NegBackward0>) tensor(11064.3994, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11064.39453125
tensor(11064.3994, grad_fn=<NegBackward0>) tensor(11064.3945, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11064.3896484375
tensor(11064.3945, grad_fn=<NegBackward0>) tensor(11064.3896, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11064.38671875
tensor(11064.3896, grad_fn=<NegBackward0>) tensor(11064.3867, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11064.380859375
tensor(11064.3867, grad_fn=<NegBackward0>) tensor(11064.3809, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11064.3759765625
tensor(11064.3809, grad_fn=<NegBackward0>) tensor(11064.3760, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11064.3671875
tensor(11064.3760, grad_fn=<NegBackward0>) tensor(11064.3672, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11064.3564453125
tensor(11064.3672, grad_fn=<NegBackward0>) tensor(11064.3564, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11064.328125
tensor(11064.3564, grad_fn=<NegBackward0>) tensor(11064.3281, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11064.2216796875
tensor(11064.3281, grad_fn=<NegBackward0>) tensor(11064.2217, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11063.3037109375
tensor(11064.2217, grad_fn=<NegBackward0>) tensor(11063.3037, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11063.1005859375
tensor(11063.3037, grad_fn=<NegBackward0>) tensor(11063.1006, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11063.068359375
tensor(11063.1006, grad_fn=<NegBackward0>) tensor(11063.0684, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11063.052734375
tensor(11063.0684, grad_fn=<NegBackward0>) tensor(11063.0527, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11063.0400390625
tensor(11063.0527, grad_fn=<NegBackward0>) tensor(11063.0400, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11063.029296875
tensor(11063.0400, grad_fn=<NegBackward0>) tensor(11063.0293, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11063.0224609375
tensor(11063.0293, grad_fn=<NegBackward0>) tensor(11063.0225, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11063.0185546875
tensor(11063.0225, grad_fn=<NegBackward0>) tensor(11063.0186, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11063.0146484375
tensor(11063.0186, grad_fn=<NegBackward0>) tensor(11063.0146, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11063.0078125
tensor(11063.0146, grad_fn=<NegBackward0>) tensor(11063.0078, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11062.9970703125
tensor(11063.0078, grad_fn=<NegBackward0>) tensor(11062.9971, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11062.9462890625
tensor(11062.9971, grad_fn=<NegBackward0>) tensor(11062.9463, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11062.87109375
tensor(11062.9463, grad_fn=<NegBackward0>) tensor(11062.8711, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11062.884765625
tensor(11062.8711, grad_fn=<NegBackward0>) tensor(11062.8848, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11062.8603515625
tensor(11062.8711, grad_fn=<NegBackward0>) tensor(11062.8604, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11062.869140625
tensor(11062.8604, grad_fn=<NegBackward0>) tensor(11062.8691, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11062.8583984375
tensor(11062.8604, grad_fn=<NegBackward0>) tensor(11062.8584, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11062.8642578125
tensor(11062.8584, grad_fn=<NegBackward0>) tensor(11062.8643, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11062.857421875
tensor(11062.8584, grad_fn=<NegBackward0>) tensor(11062.8574, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11062.9326171875
tensor(11062.8574, grad_fn=<NegBackward0>) tensor(11062.9326, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11062.85546875
tensor(11062.8574, grad_fn=<NegBackward0>) tensor(11062.8555, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11062.8564453125
tensor(11062.8555, grad_fn=<NegBackward0>) tensor(11062.8564, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11062.8564453125
tensor(11062.8555, grad_fn=<NegBackward0>) tensor(11062.8564, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -11062.8564453125
tensor(11062.8555, grad_fn=<NegBackward0>) tensor(11062.8564, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -11062.8564453125
tensor(11062.8555, grad_fn=<NegBackward0>) tensor(11062.8564, grad_fn=<NegBackward0>)
4
Iteration 5100: Loss = -11062.85546875
tensor(11062.8555, grad_fn=<NegBackward0>) tensor(11062.8555, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11062.85546875
tensor(11062.8555, grad_fn=<NegBackward0>) tensor(11062.8555, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11062.8544921875
tensor(11062.8555, grad_fn=<NegBackward0>) tensor(11062.8545, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11062.85546875
tensor(11062.8545, grad_fn=<NegBackward0>) tensor(11062.8555, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11062.8544921875
tensor(11062.8545, grad_fn=<NegBackward0>) tensor(11062.8545, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11062.8544921875
tensor(11062.8545, grad_fn=<NegBackward0>) tensor(11062.8545, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11062.85546875
tensor(11062.8545, grad_fn=<NegBackward0>) tensor(11062.8555, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11062.8544921875
tensor(11062.8545, grad_fn=<NegBackward0>) tensor(11062.8545, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11062.8623046875
tensor(11062.8545, grad_fn=<NegBackward0>) tensor(11062.8623, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11062.85546875
tensor(11062.8545, grad_fn=<NegBackward0>) tensor(11062.8555, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11062.8544921875
tensor(11062.8545, grad_fn=<NegBackward0>) tensor(11062.8545, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11062.8544921875
tensor(11062.8545, grad_fn=<NegBackward0>) tensor(11062.8545, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11062.8544921875
tensor(11062.8545, grad_fn=<NegBackward0>) tensor(11062.8545, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11062.8544921875
tensor(11062.8545, grad_fn=<NegBackward0>) tensor(11062.8545, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11062.8759765625
tensor(11062.8545, grad_fn=<NegBackward0>) tensor(11062.8760, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11062.853515625
tensor(11062.8545, grad_fn=<NegBackward0>) tensor(11062.8535, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11062.853515625
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8535, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11062.8544921875
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8545, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11062.8544921875
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8545, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11062.8544921875
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8545, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11062.8544921875
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8545, grad_fn=<NegBackward0>)
4
Iteration 7200: Loss = -11062.853515625
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8535, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11062.857421875
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8574, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11062.853515625
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8535, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11062.853515625
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8535, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11062.8544921875
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8545, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11062.8544921875
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8545, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11062.853515625
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8535, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11062.8564453125
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8564, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11062.8544921875
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8545, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11062.857421875
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8574, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -11062.9169921875
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.9170, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -11062.853515625
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8535, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11062.8544921875
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8545, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11062.853515625
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8535, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11062.8720703125
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8721, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11062.8779296875
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8779, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11062.935546875
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.9355, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -11062.8544921875
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8545, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -11062.8544921875
tensor(11062.8535, grad_fn=<NegBackward0>) tensor(11062.8545, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[2.0135e-05, 9.9998e-01],
        [3.3322e-01, 6.6678e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9714, 0.0286], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1742, 0.1366],
         [0.6982, 0.1596]],

        [[0.6946, 0.0823],
         [0.6854, 0.5322]],

        [[0.5564, 0.1699],
         [0.5120, 0.6891]],

        [[0.6493, 0.1770],
         [0.5716, 0.6256]],

        [[0.5050, 0.1583],
         [0.6216, 0.5192]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.000258897182759451
Average Adjusted Rand Index: 0.0
[-0.0004163923331970167, -0.000258897182759451] [-0.003627816869061387, 0.0] [11060.228515625, 11062.8544921875]
-------------------------------------
This iteration is 39
True Objective function: Loss = -10930.881506308191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22951.32421875
inf tensor(22951.3242, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11039.2255859375
tensor(22951.3242, grad_fn=<NegBackward0>) tensor(11039.2256, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11038.3369140625
tensor(11039.2256, grad_fn=<NegBackward0>) tensor(11038.3369, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11038.0361328125
tensor(11038.3369, grad_fn=<NegBackward0>) tensor(11038.0361, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11037.8232421875
tensor(11038.0361, grad_fn=<NegBackward0>) tensor(11037.8232, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11037.666015625
tensor(11037.8232, grad_fn=<NegBackward0>) tensor(11037.6660, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11037.5693359375
tensor(11037.6660, grad_fn=<NegBackward0>) tensor(11037.5693, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11037.50390625
tensor(11037.5693, grad_fn=<NegBackward0>) tensor(11037.5039, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11037.447265625
tensor(11037.5039, grad_fn=<NegBackward0>) tensor(11037.4473, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11037.388671875
tensor(11037.4473, grad_fn=<NegBackward0>) tensor(11037.3887, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11037.330078125
tensor(11037.3887, grad_fn=<NegBackward0>) tensor(11037.3301, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11037.259765625
tensor(11037.3301, grad_fn=<NegBackward0>) tensor(11037.2598, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11037.185546875
tensor(11037.2598, grad_fn=<NegBackward0>) tensor(11037.1855, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11037.1103515625
tensor(11037.1855, grad_fn=<NegBackward0>) tensor(11037.1104, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11037.0400390625
tensor(11037.1104, grad_fn=<NegBackward0>) tensor(11037.0400, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11036.935546875
tensor(11037.0400, grad_fn=<NegBackward0>) tensor(11036.9355, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11036.7978515625
tensor(11036.9355, grad_fn=<NegBackward0>) tensor(11036.7979, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11036.5576171875
tensor(11036.7979, grad_fn=<NegBackward0>) tensor(11036.5576, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11036.1962890625
tensor(11036.5576, grad_fn=<NegBackward0>) tensor(11036.1963, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11036.0263671875
tensor(11036.1963, grad_fn=<NegBackward0>) tensor(11036.0264, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11035.9580078125
tensor(11036.0264, grad_fn=<NegBackward0>) tensor(11035.9580, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11035.9228515625
tensor(11035.9580, grad_fn=<NegBackward0>) tensor(11035.9229, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11035.9033203125
tensor(11035.9229, grad_fn=<NegBackward0>) tensor(11035.9033, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11035.890625
tensor(11035.9033, grad_fn=<NegBackward0>) tensor(11035.8906, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11035.8818359375
tensor(11035.8906, grad_fn=<NegBackward0>) tensor(11035.8818, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11035.8740234375
tensor(11035.8818, grad_fn=<NegBackward0>) tensor(11035.8740, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11035.8701171875
tensor(11035.8740, grad_fn=<NegBackward0>) tensor(11035.8701, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11035.8662109375
tensor(11035.8701, grad_fn=<NegBackward0>) tensor(11035.8662, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11035.86328125
tensor(11035.8662, grad_fn=<NegBackward0>) tensor(11035.8633, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11035.8623046875
tensor(11035.8633, grad_fn=<NegBackward0>) tensor(11035.8623, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11035.859375
tensor(11035.8623, grad_fn=<NegBackward0>) tensor(11035.8594, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11035.859375
tensor(11035.8594, grad_fn=<NegBackward0>) tensor(11035.8594, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11035.8583984375
tensor(11035.8594, grad_fn=<NegBackward0>) tensor(11035.8584, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11035.857421875
tensor(11035.8584, grad_fn=<NegBackward0>) tensor(11035.8574, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11035.857421875
tensor(11035.8574, grad_fn=<NegBackward0>) tensor(11035.8574, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11035.85546875
tensor(11035.8574, grad_fn=<NegBackward0>) tensor(11035.8555, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11035.85546875
tensor(11035.8555, grad_fn=<NegBackward0>) tensor(11035.8555, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11035.85546875
tensor(11035.8555, grad_fn=<NegBackward0>) tensor(11035.8555, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11035.8544921875
tensor(11035.8555, grad_fn=<NegBackward0>) tensor(11035.8545, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11035.8544921875
tensor(11035.8545, grad_fn=<NegBackward0>) tensor(11035.8545, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11035.8544921875
tensor(11035.8545, grad_fn=<NegBackward0>) tensor(11035.8545, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11035.8525390625
tensor(11035.8545, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11035.85546875
tensor(11035.8525, grad_fn=<NegBackward0>) tensor(11035.8555, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11035.8525390625
tensor(11035.8525, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11035.85546875
tensor(11035.8525, grad_fn=<NegBackward0>) tensor(11035.8555, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11035.8544921875
tensor(11035.8525, grad_fn=<NegBackward0>) tensor(11035.8545, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -11035.8544921875
tensor(11035.8525, grad_fn=<NegBackward0>) tensor(11035.8545, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -11035.8525390625
tensor(11035.8525, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11035.8525390625
tensor(11035.8525, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11035.853515625
tensor(11035.8525, grad_fn=<NegBackward0>) tensor(11035.8535, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11035.8525390625
tensor(11035.8525, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11035.8544921875
tensor(11035.8525, grad_fn=<NegBackward0>) tensor(11035.8545, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11035.8525390625
tensor(11035.8525, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11035.8515625
tensor(11035.8525, grad_fn=<NegBackward0>) tensor(11035.8516, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11035.8525390625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11035.8515625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8516, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11035.8525390625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11035.8525390625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11035.8515625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8516, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11035.8515625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8516, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11035.8525390625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11035.853515625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8535, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11035.8525390625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -11035.8515625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8516, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11035.8525390625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11035.8525390625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11035.8515625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8516, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11035.8515625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8516, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11035.853515625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8535, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11035.8515625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8516, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11035.8525390625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11035.8515625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8516, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11035.8525390625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11035.8515625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8516, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11035.8525390625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11035.8515625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8516, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11035.8515625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8516, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11035.8515625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8516, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11035.8525390625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11035.8515625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8516, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11035.8515625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8516, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11035.8525390625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11035.8544921875
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8545, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11035.8515625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8516, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11035.8515625
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8516, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11035.8505859375
tensor(11035.8516, grad_fn=<NegBackward0>) tensor(11035.8506, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11035.8515625
tensor(11035.8506, grad_fn=<NegBackward0>) tensor(11035.8516, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11035.861328125
tensor(11035.8506, grad_fn=<NegBackward0>) tensor(11035.8613, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11035.8525390625
tensor(11035.8506, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -11035.8515625
tensor(11035.8506, grad_fn=<NegBackward0>) tensor(11035.8516, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -11035.8505859375
tensor(11035.8506, grad_fn=<NegBackward0>) tensor(11035.8506, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11035.8759765625
tensor(11035.8506, grad_fn=<NegBackward0>) tensor(11035.8760, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11035.8515625
tensor(11035.8506, grad_fn=<NegBackward0>) tensor(11035.8516, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11035.8505859375
tensor(11035.8506, grad_fn=<NegBackward0>) tensor(11035.8506, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11035.8515625
tensor(11035.8506, grad_fn=<NegBackward0>) tensor(11035.8516, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11035.8525390625
tensor(11035.8506, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11035.85546875
tensor(11035.8506, grad_fn=<NegBackward0>) tensor(11035.8555, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -11035.8505859375
tensor(11035.8506, grad_fn=<NegBackward0>) tensor(11035.8506, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11035.8525390625
tensor(11035.8506, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11035.8525390625
tensor(11035.8506, grad_fn=<NegBackward0>) tensor(11035.8525, grad_fn=<NegBackward0>)
2
pi: tensor([[0.0779, 0.9221],
        [0.0301, 0.9699]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9948, 0.0052], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1709, 0.1723],
         [0.7295, 0.1631]],

        [[0.6324, 0.1434],
         [0.6290, 0.6640]],

        [[0.5920, 0.2099],
         [0.5924, 0.6878]],

        [[0.6744, 0.1308],
         [0.5412, 0.5358]],

        [[0.6347, 0.1238],
         [0.6738, 0.5748]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0030649934363088487
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24338.580078125
inf tensor(24338.5801, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11039.3095703125
tensor(24338.5801, grad_fn=<NegBackward0>) tensor(11039.3096, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11038.205078125
tensor(11039.3096, grad_fn=<NegBackward0>) tensor(11038.2051, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11037.9169921875
tensor(11038.2051, grad_fn=<NegBackward0>) tensor(11037.9170, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11037.7314453125
tensor(11037.9170, grad_fn=<NegBackward0>) tensor(11037.7314, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11037.5322265625
tensor(11037.7314, grad_fn=<NegBackward0>) tensor(11037.5322, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11037.24609375
tensor(11037.5322, grad_fn=<NegBackward0>) tensor(11037.2461, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11036.8447265625
tensor(11037.2461, grad_fn=<NegBackward0>) tensor(11036.8447, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11035.8359375
tensor(11036.8447, grad_fn=<NegBackward0>) tensor(11035.8359, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11034.744140625
tensor(11035.8359, grad_fn=<NegBackward0>) tensor(11034.7441, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11034.1474609375
tensor(11034.7441, grad_fn=<NegBackward0>) tensor(11034.1475, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11033.701171875
tensor(11034.1475, grad_fn=<NegBackward0>) tensor(11033.7012, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11033.09375
tensor(11033.7012, grad_fn=<NegBackward0>) tensor(11033.0938, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11032.017578125
tensor(11033.0938, grad_fn=<NegBackward0>) tensor(11032.0176, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11023.2255859375
tensor(11032.0176, grad_fn=<NegBackward0>) tensor(11023.2256, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11008.8466796875
tensor(11023.2256, grad_fn=<NegBackward0>) tensor(11008.8467, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10954.2421875
tensor(11008.8467, grad_fn=<NegBackward0>) tensor(10954.2422, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10907.8427734375
tensor(10954.2422, grad_fn=<NegBackward0>) tensor(10907.8428, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10901.9501953125
tensor(10907.8428, grad_fn=<NegBackward0>) tensor(10901.9502, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10895.2294921875
tensor(10901.9502, grad_fn=<NegBackward0>) tensor(10895.2295, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10895.060546875
tensor(10895.2295, grad_fn=<NegBackward0>) tensor(10895.0605, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10895.0244140625
tensor(10895.0605, grad_fn=<NegBackward0>) tensor(10895.0244, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10894.9921875
tensor(10895.0244, grad_fn=<NegBackward0>) tensor(10894.9922, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10894.9345703125
tensor(10894.9922, grad_fn=<NegBackward0>) tensor(10894.9346, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10893.458984375
tensor(10894.9346, grad_fn=<NegBackward0>) tensor(10893.4590, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10893.447265625
tensor(10893.4590, grad_fn=<NegBackward0>) tensor(10893.4473, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10893.4296875
tensor(10893.4473, grad_fn=<NegBackward0>) tensor(10893.4297, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10893.240234375
tensor(10893.4297, grad_fn=<NegBackward0>) tensor(10893.2402, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10893.232421875
tensor(10893.2402, grad_fn=<NegBackward0>) tensor(10893.2324, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10893.2177734375
tensor(10893.2324, grad_fn=<NegBackward0>) tensor(10893.2178, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10893.19921875
tensor(10893.2178, grad_fn=<NegBackward0>) tensor(10893.1992, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10893.12109375
tensor(10893.1992, grad_fn=<NegBackward0>) tensor(10893.1211, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10893.09765625
tensor(10893.1211, grad_fn=<NegBackward0>) tensor(10893.0977, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10893.0908203125
tensor(10893.0977, grad_fn=<NegBackward0>) tensor(10893.0908, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10892.96484375
tensor(10893.0908, grad_fn=<NegBackward0>) tensor(10892.9648, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10892.68359375
tensor(10892.9648, grad_fn=<NegBackward0>) tensor(10892.6836, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10892.68359375
tensor(10892.6836, grad_fn=<NegBackward0>) tensor(10892.6836, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10892.6826171875
tensor(10892.6836, grad_fn=<NegBackward0>) tensor(10892.6826, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10892.6796875
tensor(10892.6826, grad_fn=<NegBackward0>) tensor(10892.6797, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10892.671875
tensor(10892.6797, grad_fn=<NegBackward0>) tensor(10892.6719, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10892.595703125
tensor(10892.6719, grad_fn=<NegBackward0>) tensor(10892.5957, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10892.595703125
tensor(10892.5957, grad_fn=<NegBackward0>) tensor(10892.5957, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10892.59375
tensor(10892.5957, grad_fn=<NegBackward0>) tensor(10892.5938, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10892.591796875
tensor(10892.5938, grad_fn=<NegBackward0>) tensor(10892.5918, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10892.5830078125
tensor(10892.5918, grad_fn=<NegBackward0>) tensor(10892.5830, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10892.55078125
tensor(10892.5830, grad_fn=<NegBackward0>) tensor(10892.5508, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10892.560546875
tensor(10892.5508, grad_fn=<NegBackward0>) tensor(10892.5605, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -10892.5302734375
tensor(10892.5508, grad_fn=<NegBackward0>) tensor(10892.5303, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10892.541015625
tensor(10892.5303, grad_fn=<NegBackward0>) tensor(10892.5410, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10892.5224609375
tensor(10892.5303, grad_fn=<NegBackward0>) tensor(10892.5225, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10892.517578125
tensor(10892.5225, grad_fn=<NegBackward0>) tensor(10892.5176, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10892.5126953125
tensor(10892.5176, grad_fn=<NegBackward0>) tensor(10892.5127, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10892.5126953125
tensor(10892.5127, grad_fn=<NegBackward0>) tensor(10892.5127, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10892.5185546875
tensor(10892.5127, grad_fn=<NegBackward0>) tensor(10892.5186, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10892.517578125
tensor(10892.5127, grad_fn=<NegBackward0>) tensor(10892.5176, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -10892.5205078125
tensor(10892.5127, grad_fn=<NegBackward0>) tensor(10892.5205, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -10892.509765625
tensor(10892.5127, grad_fn=<NegBackward0>) tensor(10892.5098, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10892.5078125
tensor(10892.5098, grad_fn=<NegBackward0>) tensor(10892.5078, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10892.509765625
tensor(10892.5078, grad_fn=<NegBackward0>) tensor(10892.5098, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10892.5087890625
tensor(10892.5078, grad_fn=<NegBackward0>) tensor(10892.5088, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -10892.5166015625
tensor(10892.5078, grad_fn=<NegBackward0>) tensor(10892.5166, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -10892.509765625
tensor(10892.5078, grad_fn=<NegBackward0>) tensor(10892.5098, grad_fn=<NegBackward0>)
4
Iteration 6200: Loss = -10892.5107421875
tensor(10892.5078, grad_fn=<NegBackward0>) tensor(10892.5107, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6200 due to no improvement.
pi: tensor([[0.7783, 0.2217],
        [0.2671, 0.7329]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4805, 0.5195], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2487, 0.1054],
         [0.7086, 0.2032]],

        [[0.6965, 0.0941],
         [0.7187, 0.5257]],

        [[0.5618, 0.1042],
         [0.6985, 0.6189]],

        [[0.5351, 0.0952],
         [0.5346, 0.5946]],

        [[0.5685, 0.1001],
         [0.5112, 0.5649]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 1
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 2
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 3
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.8985009524368007
Average Adjusted Rand Index: 0.8980968723467312
[0.0030649934363088487, 0.8985009524368007] [0.0, 0.8980968723467312] [11035.8525390625, 10892.5107421875]
-------------------------------------
This iteration is 40
True Objective function: Loss = -10974.043697778518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21961.841796875
inf tensor(21961.8418, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11075.1181640625
tensor(21961.8418, grad_fn=<NegBackward0>) tensor(11075.1182, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11074.2138671875
tensor(11075.1182, grad_fn=<NegBackward0>) tensor(11074.2139, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11073.1865234375
tensor(11074.2139, grad_fn=<NegBackward0>) tensor(11073.1865, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11070.7998046875
tensor(11073.1865, grad_fn=<NegBackward0>) tensor(11070.7998, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11063.2119140625
tensor(11070.7998, grad_fn=<NegBackward0>) tensor(11063.2119, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11035.3583984375
tensor(11063.2119, grad_fn=<NegBackward0>) tensor(11035.3584, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11030.1689453125
tensor(11035.3584, grad_fn=<NegBackward0>) tensor(11030.1689, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11025.7294921875
tensor(11030.1689, grad_fn=<NegBackward0>) tensor(11025.7295, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11016.4443359375
tensor(11025.7295, grad_fn=<NegBackward0>) tensor(11016.4443, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10967.1416015625
tensor(11016.4443, grad_fn=<NegBackward0>) tensor(10967.1416, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10945.4560546875
tensor(10967.1416, grad_fn=<NegBackward0>) tensor(10945.4561, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10940.9853515625
tensor(10945.4561, grad_fn=<NegBackward0>) tensor(10940.9854, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10939.6552734375
tensor(10940.9854, grad_fn=<NegBackward0>) tensor(10939.6553, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10937.451171875
tensor(10939.6553, grad_fn=<NegBackward0>) tensor(10937.4512, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10937.162109375
tensor(10937.4512, grad_fn=<NegBackward0>) tensor(10937.1621, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10937.11328125
tensor(10937.1621, grad_fn=<NegBackward0>) tensor(10937.1133, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10937.0849609375
tensor(10937.1133, grad_fn=<NegBackward0>) tensor(10937.0850, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10937.0595703125
tensor(10937.0850, grad_fn=<NegBackward0>) tensor(10937.0596, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10937.04296875
tensor(10937.0596, grad_fn=<NegBackward0>) tensor(10937.0430, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10937.00390625
tensor(10937.0430, grad_fn=<NegBackward0>) tensor(10937.0039, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10936.9794921875
tensor(10937.0039, grad_fn=<NegBackward0>) tensor(10936.9795, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10936.9521484375
tensor(10936.9795, grad_fn=<NegBackward0>) tensor(10936.9521, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10936.9140625
tensor(10936.9521, grad_fn=<NegBackward0>) tensor(10936.9141, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10936.87109375
tensor(10936.9141, grad_fn=<NegBackward0>) tensor(10936.8711, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10936.857421875
tensor(10936.8711, grad_fn=<NegBackward0>) tensor(10936.8574, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10936.51953125
tensor(10936.8574, grad_fn=<NegBackward0>) tensor(10936.5195, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10936.4677734375
tensor(10936.5195, grad_fn=<NegBackward0>) tensor(10936.4678, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10936.4609375
tensor(10936.4678, grad_fn=<NegBackward0>) tensor(10936.4609, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10936.45703125
tensor(10936.4609, grad_fn=<NegBackward0>) tensor(10936.4570, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10936.4453125
tensor(10936.4570, grad_fn=<NegBackward0>) tensor(10936.4453, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10936.4423828125
tensor(10936.4453, grad_fn=<NegBackward0>) tensor(10936.4424, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10936.4404296875
tensor(10936.4424, grad_fn=<NegBackward0>) tensor(10936.4404, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10936.4375
tensor(10936.4404, grad_fn=<NegBackward0>) tensor(10936.4375, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10936.4326171875
tensor(10936.4375, grad_fn=<NegBackward0>) tensor(10936.4326, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10936.19140625
tensor(10936.4326, grad_fn=<NegBackward0>) tensor(10936.1914, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10936.185546875
tensor(10936.1914, grad_fn=<NegBackward0>) tensor(10936.1855, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10936.1845703125
tensor(10936.1855, grad_fn=<NegBackward0>) tensor(10936.1846, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10936.1435546875
tensor(10936.1846, grad_fn=<NegBackward0>) tensor(10936.1436, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10936.140625
tensor(10936.1436, grad_fn=<NegBackward0>) tensor(10936.1406, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10936.13671875
tensor(10936.1406, grad_fn=<NegBackward0>) tensor(10936.1367, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10936.1318359375
tensor(10936.1367, grad_fn=<NegBackward0>) tensor(10936.1318, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10936.1123046875
tensor(10936.1318, grad_fn=<NegBackward0>) tensor(10936.1123, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10936.107421875
tensor(10936.1123, grad_fn=<NegBackward0>) tensor(10936.1074, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10936.1044921875
tensor(10936.1074, grad_fn=<NegBackward0>) tensor(10936.1045, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10936.1025390625
tensor(10936.1045, grad_fn=<NegBackward0>) tensor(10936.1025, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10936.0927734375
tensor(10936.1025, grad_fn=<NegBackward0>) tensor(10936.0928, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10936.0888671875
tensor(10936.0928, grad_fn=<NegBackward0>) tensor(10936.0889, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10936.0830078125
tensor(10936.0889, grad_fn=<NegBackward0>) tensor(10936.0830, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10936.08203125
tensor(10936.0830, grad_fn=<NegBackward0>) tensor(10936.0820, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10936.0771484375
tensor(10936.0820, grad_fn=<NegBackward0>) tensor(10936.0771, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10936.07421875
tensor(10936.0771, grad_fn=<NegBackward0>) tensor(10936.0742, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10936.076171875
tensor(10936.0742, grad_fn=<NegBackward0>) tensor(10936.0762, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10936.072265625
tensor(10936.0742, grad_fn=<NegBackward0>) tensor(10936.0723, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10936.072265625
tensor(10936.0723, grad_fn=<NegBackward0>) tensor(10936.0723, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10936.072265625
tensor(10936.0723, grad_fn=<NegBackward0>) tensor(10936.0723, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10936.0703125
tensor(10936.0723, grad_fn=<NegBackward0>) tensor(10936.0703, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10936.0703125
tensor(10936.0703, grad_fn=<NegBackward0>) tensor(10936.0703, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10936.0712890625
tensor(10936.0703, grad_fn=<NegBackward0>) tensor(10936.0713, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10936.0703125
tensor(10936.0703, grad_fn=<NegBackward0>) tensor(10936.0703, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10936.068359375
tensor(10936.0703, grad_fn=<NegBackward0>) tensor(10936.0684, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10936.0703125
tensor(10936.0684, grad_fn=<NegBackward0>) tensor(10936.0703, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10936.0673828125
tensor(10936.0684, grad_fn=<NegBackward0>) tensor(10936.0674, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10936.06640625
tensor(10936.0674, grad_fn=<NegBackward0>) tensor(10936.0664, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10936.0673828125
tensor(10936.0664, grad_fn=<NegBackward0>) tensor(10936.0674, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10936.06640625
tensor(10936.0664, grad_fn=<NegBackward0>) tensor(10936.0664, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10936.064453125
tensor(10936.0664, grad_fn=<NegBackward0>) tensor(10936.0645, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10936.0615234375
tensor(10936.0645, grad_fn=<NegBackward0>) tensor(10936.0615, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10936.0615234375
tensor(10936.0615, grad_fn=<NegBackward0>) tensor(10936.0615, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10936.0576171875
tensor(10936.0615, grad_fn=<NegBackward0>) tensor(10936.0576, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10936.0546875
tensor(10936.0576, grad_fn=<NegBackward0>) tensor(10936.0547, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10936.0537109375
tensor(10936.0547, grad_fn=<NegBackward0>) tensor(10936.0537, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10936.0546875
tensor(10936.0537, grad_fn=<NegBackward0>) tensor(10936.0547, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10936.052734375
tensor(10936.0537, grad_fn=<NegBackward0>) tensor(10936.0527, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10936.0517578125
tensor(10936.0527, grad_fn=<NegBackward0>) tensor(10936.0518, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10936.044921875
tensor(10936.0518, grad_fn=<NegBackward0>) tensor(10936.0449, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10936.04296875
tensor(10936.0449, grad_fn=<NegBackward0>) tensor(10936.0430, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10936.0439453125
tensor(10936.0430, grad_fn=<NegBackward0>) tensor(10936.0439, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10936.05078125
tensor(10936.0430, grad_fn=<NegBackward0>) tensor(10936.0508, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -10936.0537109375
tensor(10936.0430, grad_fn=<NegBackward0>) tensor(10936.0537, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -10936.044921875
tensor(10936.0430, grad_fn=<NegBackward0>) tensor(10936.0449, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -10936.0439453125
tensor(10936.0430, grad_fn=<NegBackward0>) tensor(10936.0439, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8100 due to no improvement.
pi: tensor([[0.7386, 0.2615],
        [0.2608, 0.7392]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5483, 0.4517], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2503, 0.1029],
         [0.6411, 0.2003]],

        [[0.6612, 0.1003],
         [0.5255, 0.5684]],

        [[0.5368, 0.1018],
         [0.6501, 0.7072]],

        [[0.6736, 0.1057],
         [0.5397, 0.6595]],

        [[0.6127, 0.0926],
         [0.5595, 0.7225]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 1
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9207702484198148
time is 2
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.844845420066659
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080682750429576
time is 4
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.8833667357452548
Average Adjusted Rand Index: 0.8832212923947301
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23656.400390625
inf tensor(23656.4004, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11074.0087890625
tensor(23656.4004, grad_fn=<NegBackward0>) tensor(11074.0088, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11071.978515625
tensor(11074.0088, grad_fn=<NegBackward0>) tensor(11071.9785, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11070.1875
tensor(11071.9785, grad_fn=<NegBackward0>) tensor(11070.1875, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11061.931640625
tensor(11070.1875, grad_fn=<NegBackward0>) tensor(11061.9316, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11033.03125
tensor(11061.9316, grad_fn=<NegBackward0>) tensor(11033.0312, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11023.533203125
tensor(11033.0312, grad_fn=<NegBackward0>) tensor(11023.5332, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10959.52734375
tensor(11023.5332, grad_fn=<NegBackward0>) tensor(10959.5273, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10939.865234375
tensor(10959.5273, grad_fn=<NegBackward0>) tensor(10939.8652, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10937.169921875
tensor(10939.8652, grad_fn=<NegBackward0>) tensor(10937.1699, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10936.9814453125
tensor(10937.1699, grad_fn=<NegBackward0>) tensor(10936.9814, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10936.7314453125
tensor(10936.9814, grad_fn=<NegBackward0>) tensor(10936.7314, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10936.6015625
tensor(10936.7314, grad_fn=<NegBackward0>) tensor(10936.6016, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10936.5703125
tensor(10936.6016, grad_fn=<NegBackward0>) tensor(10936.5703, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10936.4873046875
tensor(10936.5703, grad_fn=<NegBackward0>) tensor(10936.4873, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10936.2578125
tensor(10936.4873, grad_fn=<NegBackward0>) tensor(10936.2578, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10936.1708984375
tensor(10936.2578, grad_fn=<NegBackward0>) tensor(10936.1709, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10936.1572265625
tensor(10936.1709, grad_fn=<NegBackward0>) tensor(10936.1572, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10936.1416015625
tensor(10936.1572, grad_fn=<NegBackward0>) tensor(10936.1416, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10936.1328125
tensor(10936.1416, grad_fn=<NegBackward0>) tensor(10936.1328, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10936.1240234375
tensor(10936.1328, grad_fn=<NegBackward0>) tensor(10936.1240, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10936.111328125
tensor(10936.1240, grad_fn=<NegBackward0>) tensor(10936.1113, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10936.09765625
tensor(10936.1113, grad_fn=<NegBackward0>) tensor(10936.0977, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10936.095703125
tensor(10936.0977, grad_fn=<NegBackward0>) tensor(10936.0957, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10936.0927734375
tensor(10936.0957, grad_fn=<NegBackward0>) tensor(10936.0928, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10936.08984375
tensor(10936.0928, grad_fn=<NegBackward0>) tensor(10936.0898, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10936.0810546875
tensor(10936.0898, grad_fn=<NegBackward0>) tensor(10936.0811, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10936.0751953125
tensor(10936.0811, grad_fn=<NegBackward0>) tensor(10936.0752, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10936.076171875
tensor(10936.0752, grad_fn=<NegBackward0>) tensor(10936.0762, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -10936.0751953125
tensor(10936.0752, grad_fn=<NegBackward0>) tensor(10936.0752, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10936.0732421875
tensor(10936.0752, grad_fn=<NegBackward0>) tensor(10936.0732, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10936.072265625
tensor(10936.0732, grad_fn=<NegBackward0>) tensor(10936.0723, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10936.0703125
tensor(10936.0723, grad_fn=<NegBackward0>) tensor(10936.0703, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10936.0810546875
tensor(10936.0703, grad_fn=<NegBackward0>) tensor(10936.0811, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -10936.068359375
tensor(10936.0703, grad_fn=<NegBackward0>) tensor(10936.0684, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10936.0634765625
tensor(10936.0684, grad_fn=<NegBackward0>) tensor(10936.0635, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10936.0634765625
tensor(10936.0635, grad_fn=<NegBackward0>) tensor(10936.0635, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10936.0615234375
tensor(10936.0635, grad_fn=<NegBackward0>) tensor(10936.0615, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10936.0712890625
tensor(10936.0615, grad_fn=<NegBackward0>) tensor(10936.0713, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -10936.0576171875
tensor(10936.0615, grad_fn=<NegBackward0>) tensor(10936.0576, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10936.05859375
tensor(10936.0576, grad_fn=<NegBackward0>) tensor(10936.0586, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -10936.0576171875
tensor(10936.0576, grad_fn=<NegBackward0>) tensor(10936.0576, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10936.0556640625
tensor(10936.0576, grad_fn=<NegBackward0>) tensor(10936.0557, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10936.06640625
tensor(10936.0557, grad_fn=<NegBackward0>) tensor(10936.0664, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -10936.0546875
tensor(10936.0557, grad_fn=<NegBackward0>) tensor(10936.0547, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10936.0537109375
tensor(10936.0547, grad_fn=<NegBackward0>) tensor(10936.0537, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10936.0517578125
tensor(10936.0537, grad_fn=<NegBackward0>) tensor(10936.0518, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10936.05078125
tensor(10936.0518, grad_fn=<NegBackward0>) tensor(10936.0508, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10936.0537109375
tensor(10936.0508, grad_fn=<NegBackward0>) tensor(10936.0537, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10936.0478515625
tensor(10936.0508, grad_fn=<NegBackward0>) tensor(10936.0479, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10936.046875
tensor(10936.0479, grad_fn=<NegBackward0>) tensor(10936.0469, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10936.0458984375
tensor(10936.0469, grad_fn=<NegBackward0>) tensor(10936.0459, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10936.0439453125
tensor(10936.0459, grad_fn=<NegBackward0>) tensor(10936.0439, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10936.048828125
tensor(10936.0439, grad_fn=<NegBackward0>) tensor(10936.0488, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10936.04296875
tensor(10936.0439, grad_fn=<NegBackward0>) tensor(10936.0430, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10936.04296875
tensor(10936.0430, grad_fn=<NegBackward0>) tensor(10936.0430, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10936.0419921875
tensor(10936.0430, grad_fn=<NegBackward0>) tensor(10936.0420, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10936.03515625
tensor(10936.0420, grad_fn=<NegBackward0>) tensor(10936.0352, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10936.0361328125
tensor(10936.0352, grad_fn=<NegBackward0>) tensor(10936.0361, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10936.048828125
tensor(10936.0352, grad_fn=<NegBackward0>) tensor(10936.0488, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -10936.0361328125
tensor(10936.0352, grad_fn=<NegBackward0>) tensor(10936.0361, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -10936.0419921875
tensor(10936.0352, grad_fn=<NegBackward0>) tensor(10936.0420, grad_fn=<NegBackward0>)
4
Iteration 6200: Loss = -10936.03515625
tensor(10936.0352, grad_fn=<NegBackward0>) tensor(10936.0352, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10936.03515625
tensor(10936.0352, grad_fn=<NegBackward0>) tensor(10936.0352, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10936.03515625
tensor(10936.0352, grad_fn=<NegBackward0>) tensor(10936.0352, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10936.03515625
tensor(10936.0352, grad_fn=<NegBackward0>) tensor(10936.0352, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10936.0341796875
tensor(10936.0352, grad_fn=<NegBackward0>) tensor(10936.0342, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10936.03515625
tensor(10936.0342, grad_fn=<NegBackward0>) tensor(10936.0352, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10936.0341796875
tensor(10936.0342, grad_fn=<NegBackward0>) tensor(10936.0342, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10936.033203125
tensor(10936.0342, grad_fn=<NegBackward0>) tensor(10936.0332, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10936.033203125
tensor(10936.0332, grad_fn=<NegBackward0>) tensor(10936.0332, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10936.033203125
tensor(10936.0332, grad_fn=<NegBackward0>) tensor(10936.0332, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10936.0322265625
tensor(10936.0332, grad_fn=<NegBackward0>) tensor(10936.0322, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10936.037109375
tensor(10936.0322, grad_fn=<NegBackward0>) tensor(10936.0371, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -10936.033203125
tensor(10936.0322, grad_fn=<NegBackward0>) tensor(10936.0332, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -10936.033203125
tensor(10936.0322, grad_fn=<NegBackward0>) tensor(10936.0332, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -10936.033203125
tensor(10936.0322, grad_fn=<NegBackward0>) tensor(10936.0332, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -10936.05859375
tensor(10936.0322, grad_fn=<NegBackward0>) tensor(10936.0586, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.7372, 0.2628],
        [0.2603, 0.7397]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5458, 0.4542], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2508, 0.1029],
         [0.6054, 0.2003]],

        [[0.7013, 0.1003],
         [0.7281, 0.7237]],

        [[0.5065, 0.1018],
         [0.6640, 0.5635]],

        [[0.7296, 0.1057],
         [0.6801, 0.6534]],

        [[0.6666, 0.0926],
         [0.5993, 0.5465]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 1
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9207702484198148
time is 2
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824191749678101
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080682750429576
time is 4
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.8909178406058668
Average Adjusted Rand Index: 0.8907360433749604
[0.8833667357452548, 0.8909178406058668] [0.8832212923947301, 0.8907360433749604] [10936.0439453125, 10936.05859375]
-------------------------------------
This iteration is 41
True Objective function: Loss = -11101.501561924195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22624.60546875
inf tensor(22624.6055, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11190.76171875
tensor(22624.6055, grad_fn=<NegBackward0>) tensor(11190.7617, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11190.1875
tensor(11190.7617, grad_fn=<NegBackward0>) tensor(11190.1875, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11190.0380859375
tensor(11190.1875, grad_fn=<NegBackward0>) tensor(11190.0381, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11189.986328125
tensor(11190.0381, grad_fn=<NegBackward0>) tensor(11189.9863, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11189.9619140625
tensor(11189.9863, grad_fn=<NegBackward0>) tensor(11189.9619, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11189.939453125
tensor(11189.9619, grad_fn=<NegBackward0>) tensor(11189.9395, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11189.9228515625
tensor(11189.9395, grad_fn=<NegBackward0>) tensor(11189.9229, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11189.9091796875
tensor(11189.9229, grad_fn=<NegBackward0>) tensor(11189.9092, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11189.89453125
tensor(11189.9092, grad_fn=<NegBackward0>) tensor(11189.8945, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11189.8818359375
tensor(11189.8945, grad_fn=<NegBackward0>) tensor(11189.8818, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11189.8671875
tensor(11189.8818, grad_fn=<NegBackward0>) tensor(11189.8672, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11189.84765625
tensor(11189.8672, grad_fn=<NegBackward0>) tensor(11189.8477, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11189.828125
tensor(11189.8477, grad_fn=<NegBackward0>) tensor(11189.8281, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11189.7978515625
tensor(11189.8281, grad_fn=<NegBackward0>) tensor(11189.7979, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11189.7548828125
tensor(11189.7979, grad_fn=<NegBackward0>) tensor(11189.7549, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11189.6953125
tensor(11189.7549, grad_fn=<NegBackward0>) tensor(11189.6953, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11189.640625
tensor(11189.6953, grad_fn=<NegBackward0>) tensor(11189.6406, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11189.59765625
tensor(11189.6406, grad_fn=<NegBackward0>) tensor(11189.5977, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11189.560546875
tensor(11189.5977, grad_fn=<NegBackward0>) tensor(11189.5605, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11189.53125
tensor(11189.5605, grad_fn=<NegBackward0>) tensor(11189.5312, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11189.5048828125
tensor(11189.5312, grad_fn=<NegBackward0>) tensor(11189.5049, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11189.482421875
tensor(11189.5049, grad_fn=<NegBackward0>) tensor(11189.4824, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11189.4619140625
tensor(11189.4824, grad_fn=<NegBackward0>) tensor(11189.4619, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11189.4404296875
tensor(11189.4619, grad_fn=<NegBackward0>) tensor(11189.4404, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11189.4130859375
tensor(11189.4404, grad_fn=<NegBackward0>) tensor(11189.4131, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11189.3505859375
tensor(11189.4131, grad_fn=<NegBackward0>) tensor(11189.3506, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11188.6845703125
tensor(11189.3506, grad_fn=<NegBackward0>) tensor(11188.6846, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11187.9453125
tensor(11188.6846, grad_fn=<NegBackward0>) tensor(11187.9453, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11187.4970703125
tensor(11187.9453, grad_fn=<NegBackward0>) tensor(11187.4971, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11187.265625
tensor(11187.4971, grad_fn=<NegBackward0>) tensor(11187.2656, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11187.171875
tensor(11187.2656, grad_fn=<NegBackward0>) tensor(11187.1719, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11187.1220703125
tensor(11187.1719, grad_fn=<NegBackward0>) tensor(11187.1221, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11187.08984375
tensor(11187.1221, grad_fn=<NegBackward0>) tensor(11187.0898, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11187.060546875
tensor(11187.0898, grad_fn=<NegBackward0>) tensor(11187.0605, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11187.03125
tensor(11187.0605, grad_fn=<NegBackward0>) tensor(11187.0312, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11187.0
tensor(11187.0312, grad_fn=<NegBackward0>) tensor(11187., grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11186.9658203125
tensor(11187., grad_fn=<NegBackward0>) tensor(11186.9658, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11186.931640625
tensor(11186.9658, grad_fn=<NegBackward0>) tensor(11186.9316, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11186.89453125
tensor(11186.9316, grad_fn=<NegBackward0>) tensor(11186.8945, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11186.8525390625
tensor(11186.8945, grad_fn=<NegBackward0>) tensor(11186.8525, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11186.744140625
tensor(11186.8525, grad_fn=<NegBackward0>) tensor(11186.7441, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11186.5673828125
tensor(11186.7441, grad_fn=<NegBackward0>) tensor(11186.5674, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11186.5048828125
tensor(11186.5674, grad_fn=<NegBackward0>) tensor(11186.5049, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11186.48046875
tensor(11186.5049, grad_fn=<NegBackward0>) tensor(11186.4805, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11186.4677734375
tensor(11186.4805, grad_fn=<NegBackward0>) tensor(11186.4678, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11186.4609375
tensor(11186.4678, grad_fn=<NegBackward0>) tensor(11186.4609, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11186.455078125
tensor(11186.4609, grad_fn=<NegBackward0>) tensor(11186.4551, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11186.451171875
tensor(11186.4551, grad_fn=<NegBackward0>) tensor(11186.4512, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11186.4462890625
tensor(11186.4512, grad_fn=<NegBackward0>) tensor(11186.4463, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11186.4443359375
tensor(11186.4463, grad_fn=<NegBackward0>) tensor(11186.4443, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11186.4423828125
tensor(11186.4443, grad_fn=<NegBackward0>) tensor(11186.4424, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11186.4423828125
tensor(11186.4424, grad_fn=<NegBackward0>) tensor(11186.4424, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11186.44140625
tensor(11186.4424, grad_fn=<NegBackward0>) tensor(11186.4414, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11186.439453125
tensor(11186.4414, grad_fn=<NegBackward0>) tensor(11186.4395, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11186.4404296875
tensor(11186.4395, grad_fn=<NegBackward0>) tensor(11186.4404, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11186.4375
tensor(11186.4395, grad_fn=<NegBackward0>) tensor(11186.4375, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11186.4375
tensor(11186.4375, grad_fn=<NegBackward0>) tensor(11186.4375, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11186.4365234375
tensor(11186.4375, grad_fn=<NegBackward0>) tensor(11186.4365, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11186.4365234375
tensor(11186.4365, grad_fn=<NegBackward0>) tensor(11186.4365, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11186.4345703125
tensor(11186.4365, grad_fn=<NegBackward0>) tensor(11186.4346, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11186.435546875
tensor(11186.4346, grad_fn=<NegBackward0>) tensor(11186.4355, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11186.435546875
tensor(11186.4346, grad_fn=<NegBackward0>) tensor(11186.4355, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11186.4345703125
tensor(11186.4346, grad_fn=<NegBackward0>) tensor(11186.4346, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11186.4345703125
tensor(11186.4346, grad_fn=<NegBackward0>) tensor(11186.4346, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11186.4365234375
tensor(11186.4346, grad_fn=<NegBackward0>) tensor(11186.4365, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11186.43359375
tensor(11186.4346, grad_fn=<NegBackward0>) tensor(11186.4336, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11186.43359375
tensor(11186.4336, grad_fn=<NegBackward0>) tensor(11186.4336, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11186.4326171875
tensor(11186.4336, grad_fn=<NegBackward0>) tensor(11186.4326, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11186.43359375
tensor(11186.4326, grad_fn=<NegBackward0>) tensor(11186.4336, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11186.4306640625
tensor(11186.4326, grad_fn=<NegBackward0>) tensor(11186.4307, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11186.431640625
tensor(11186.4307, grad_fn=<NegBackward0>) tensor(11186.4316, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11186.4775390625
tensor(11186.4307, grad_fn=<NegBackward0>) tensor(11186.4775, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11186.431640625
tensor(11186.4307, grad_fn=<NegBackward0>) tensor(11186.4316, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11186.431640625
tensor(11186.4307, grad_fn=<NegBackward0>) tensor(11186.4316, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -11186.4306640625
tensor(11186.4307, grad_fn=<NegBackward0>) tensor(11186.4307, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11186.4296875
tensor(11186.4307, grad_fn=<NegBackward0>) tensor(11186.4297, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11186.6806640625
tensor(11186.4297, grad_fn=<NegBackward0>) tensor(11186.6807, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11186.4306640625
tensor(11186.4297, grad_fn=<NegBackward0>) tensor(11186.4307, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11186.4296875
tensor(11186.4297, grad_fn=<NegBackward0>) tensor(11186.4297, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11186.431640625
tensor(11186.4297, grad_fn=<NegBackward0>) tensor(11186.4316, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11186.4296875
tensor(11186.4297, grad_fn=<NegBackward0>) tensor(11186.4297, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11186.5439453125
tensor(11186.4297, grad_fn=<NegBackward0>) tensor(11186.5439, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11186.4296875
tensor(11186.4297, grad_fn=<NegBackward0>) tensor(11186.4297, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11186.4326171875
tensor(11186.4297, grad_fn=<NegBackward0>) tensor(11186.4326, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11186.5078125
tensor(11186.4297, grad_fn=<NegBackward0>) tensor(11186.5078, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11186.4296875
tensor(11186.4297, grad_fn=<NegBackward0>) tensor(11186.4297, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11186.4296875
tensor(11186.4297, grad_fn=<NegBackward0>) tensor(11186.4297, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11186.4296875
tensor(11186.4297, grad_fn=<NegBackward0>) tensor(11186.4297, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11186.4296875
tensor(11186.4297, grad_fn=<NegBackward0>) tensor(11186.4297, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11186.4296875
tensor(11186.4297, grad_fn=<NegBackward0>) tensor(11186.4297, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11186.4296875
tensor(11186.4297, grad_fn=<NegBackward0>) tensor(11186.4297, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11186.4296875
tensor(11186.4297, grad_fn=<NegBackward0>) tensor(11186.4297, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11186.44921875
tensor(11186.4297, grad_fn=<NegBackward0>) tensor(11186.4492, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11186.4306640625
tensor(11186.4297, grad_fn=<NegBackward0>) tensor(11186.4307, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -11186.4287109375
tensor(11186.4297, grad_fn=<NegBackward0>) tensor(11186.4287, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11186.4306640625
tensor(11186.4287, grad_fn=<NegBackward0>) tensor(11186.4307, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11186.4287109375
tensor(11186.4287, grad_fn=<NegBackward0>) tensor(11186.4287, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11186.4296875
tensor(11186.4287, grad_fn=<NegBackward0>) tensor(11186.4297, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11186.4306640625
tensor(11186.4287, grad_fn=<NegBackward0>) tensor(11186.4307, grad_fn=<NegBackward0>)
2
pi: tensor([[9.8953e-01, 1.0473e-02],
        [9.9997e-01, 2.5180e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9179, 0.0821], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1699, 0.1284],
         [0.6322, 0.0968]],

        [[0.5561, 0.0412],
         [0.5410, 0.7147]],

        [[0.5744, 0.1527],
         [0.6592, 0.6996]],

        [[0.6084, 0.0779],
         [0.5375, 0.5042]],

        [[0.6195, 0.1926],
         [0.6845, 0.6890]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0006480087953594284
Average Adjusted Rand Index: -0.0009865056647124326
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25679.19921875
inf tensor(25679.1992, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11166.6982421875
tensor(25679.1992, grad_fn=<NegBackward0>) tensor(11166.6982, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11094.5634765625
tensor(11166.6982, grad_fn=<NegBackward0>) tensor(11094.5635, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11093.529296875
tensor(11094.5635, grad_fn=<NegBackward0>) tensor(11093.5293, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11093.490234375
tensor(11093.5293, grad_fn=<NegBackward0>) tensor(11093.4902, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11093.46875
tensor(11093.4902, grad_fn=<NegBackward0>) tensor(11093.4688, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11093.4580078125
tensor(11093.4688, grad_fn=<NegBackward0>) tensor(11093.4580, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11093.453125
tensor(11093.4580, grad_fn=<NegBackward0>) tensor(11093.4531, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11093.4501953125
tensor(11093.4531, grad_fn=<NegBackward0>) tensor(11093.4502, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11093.4482421875
tensor(11093.4502, grad_fn=<NegBackward0>) tensor(11093.4482, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11093.4580078125
tensor(11093.4482, grad_fn=<NegBackward0>) tensor(11093.4580, grad_fn=<NegBackward0>)
1
Iteration 1100: Loss = -11093.4453125
tensor(11093.4482, grad_fn=<NegBackward0>) tensor(11093.4453, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11093.4443359375
tensor(11093.4453, grad_fn=<NegBackward0>) tensor(11093.4443, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11093.4443359375
tensor(11093.4443, grad_fn=<NegBackward0>) tensor(11093.4443, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11093.4443359375
tensor(11093.4443, grad_fn=<NegBackward0>) tensor(11093.4443, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11093.4443359375
tensor(11093.4443, grad_fn=<NegBackward0>) tensor(11093.4443, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11093.443359375
tensor(11093.4443, grad_fn=<NegBackward0>) tensor(11093.4434, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11093.447265625
tensor(11093.4434, grad_fn=<NegBackward0>) tensor(11093.4473, grad_fn=<NegBackward0>)
1
Iteration 1800: Loss = -11093.4423828125
tensor(11093.4434, grad_fn=<NegBackward0>) tensor(11093.4424, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11093.4423828125
tensor(11093.4424, grad_fn=<NegBackward0>) tensor(11093.4424, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11093.44140625
tensor(11093.4424, grad_fn=<NegBackward0>) tensor(11093.4414, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11093.4423828125
tensor(11093.4414, grad_fn=<NegBackward0>) tensor(11093.4424, grad_fn=<NegBackward0>)
1
Iteration 2200: Loss = -11093.4423828125
tensor(11093.4414, grad_fn=<NegBackward0>) tensor(11093.4424, grad_fn=<NegBackward0>)
2
Iteration 2300: Loss = -11093.4423828125
tensor(11093.4414, grad_fn=<NegBackward0>) tensor(11093.4424, grad_fn=<NegBackward0>)
3
Iteration 2400: Loss = -11093.4423828125
tensor(11093.4414, grad_fn=<NegBackward0>) tensor(11093.4424, grad_fn=<NegBackward0>)
4
Iteration 2500: Loss = -11093.44140625
tensor(11093.4414, grad_fn=<NegBackward0>) tensor(11093.4414, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11093.443359375
tensor(11093.4414, grad_fn=<NegBackward0>) tensor(11093.4434, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -11093.4482421875
tensor(11093.4414, grad_fn=<NegBackward0>) tensor(11093.4482, grad_fn=<NegBackward0>)
2
Iteration 2800: Loss = -11093.4462890625
tensor(11093.4414, grad_fn=<NegBackward0>) tensor(11093.4463, grad_fn=<NegBackward0>)
3
Iteration 2900: Loss = -11093.44140625
tensor(11093.4414, grad_fn=<NegBackward0>) tensor(11093.4414, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11093.4404296875
tensor(11093.4414, grad_fn=<NegBackward0>) tensor(11093.4404, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11093.44140625
tensor(11093.4404, grad_fn=<NegBackward0>) tensor(11093.4414, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11093.443359375
tensor(11093.4404, grad_fn=<NegBackward0>) tensor(11093.4434, grad_fn=<NegBackward0>)
2
Iteration 3300: Loss = -11093.44140625
tensor(11093.4404, grad_fn=<NegBackward0>) tensor(11093.4414, grad_fn=<NegBackward0>)
3
Iteration 3400: Loss = -11093.44140625
tensor(11093.4404, grad_fn=<NegBackward0>) tensor(11093.4414, grad_fn=<NegBackward0>)
4
Iteration 3500: Loss = -11093.44140625
tensor(11093.4404, grad_fn=<NegBackward0>) tensor(11093.4414, grad_fn=<NegBackward0>)
5
Stopping early at iteration 3500 due to no improvement.
pi: tensor([[0.3642, 0.6358],
        [0.6154, 0.3846]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4908, 0.5092], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2366, 0.1036],
         [0.5243, 0.2291]],

        [[0.6606, 0.0960],
         [0.6694, 0.6551]],

        [[0.7248, 0.0975],
         [0.7061, 0.5028]],

        [[0.5866, 0.1057],
         [0.5758, 0.6529]],

        [[0.5046, 0.1136],
         [0.6421, 0.6506]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 9
Adjusted Rand Index: 0.6690909090909091
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7721069260785004
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 94
Adjusted Rand Index: 0.7721128416103438
time is 4
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448373142749671
Global Adjusted Rand Index: 0.043028559718032114
Average Adjusted Rand Index: 0.7957907256800745
[-0.0006480087953594284, 0.043028559718032114] [-0.0009865056647124326, 0.7957907256800745] [11186.4287109375, 11093.44140625]
-------------------------------------
This iteration is 42
True Objective function: Loss = -10920.70885998401
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20142.833984375
inf tensor(20142.8340, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11026.37109375
tensor(20142.8340, grad_fn=<NegBackward0>) tensor(11026.3711, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11025.625
tensor(11026.3711, grad_fn=<NegBackward0>) tensor(11025.6250, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11024.6875
tensor(11025.6250, grad_fn=<NegBackward0>) tensor(11024.6875, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11023.9189453125
tensor(11024.6875, grad_fn=<NegBackward0>) tensor(11023.9189, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11023.6884765625
tensor(11023.9189, grad_fn=<NegBackward0>) tensor(11023.6885, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11023.537109375
tensor(11023.6885, grad_fn=<NegBackward0>) tensor(11023.5371, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11023.412109375
tensor(11023.5371, grad_fn=<NegBackward0>) tensor(11023.4121, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11023.201171875
tensor(11023.4121, grad_fn=<NegBackward0>) tensor(11023.2012, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11022.0888671875
tensor(11023.2012, grad_fn=<NegBackward0>) tensor(11022.0889, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11021.6748046875
tensor(11022.0889, grad_fn=<NegBackward0>) tensor(11021.6748, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11021.609375
tensor(11021.6748, grad_fn=<NegBackward0>) tensor(11021.6094, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11021.53125
tensor(11021.6094, grad_fn=<NegBackward0>) tensor(11021.5312, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11021.361328125
tensor(11021.5312, grad_fn=<NegBackward0>) tensor(11021.3613, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11020.865234375
tensor(11021.3613, grad_fn=<NegBackward0>) tensor(11020.8652, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11018.4892578125
tensor(11020.8652, grad_fn=<NegBackward0>) tensor(11018.4893, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11016.853515625
tensor(11018.4893, grad_fn=<NegBackward0>) tensor(11016.8535, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11016.7998046875
tensor(11016.8535, grad_fn=<NegBackward0>) tensor(11016.7998, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11016.7939453125
tensor(11016.7998, grad_fn=<NegBackward0>) tensor(11016.7939, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11016.7666015625
tensor(11016.7939, grad_fn=<NegBackward0>) tensor(11016.7666, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11016.7607421875
tensor(11016.7666, grad_fn=<NegBackward0>) tensor(11016.7607, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11016.7587890625
tensor(11016.7607, grad_fn=<NegBackward0>) tensor(11016.7588, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11016.755859375
tensor(11016.7588, grad_fn=<NegBackward0>) tensor(11016.7559, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11016.755859375
tensor(11016.7559, grad_fn=<NegBackward0>) tensor(11016.7559, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11016.7529296875
tensor(11016.7559, grad_fn=<NegBackward0>) tensor(11016.7529, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11016.7490234375
tensor(11016.7529, grad_fn=<NegBackward0>) tensor(11016.7490, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11016.74609375
tensor(11016.7490, grad_fn=<NegBackward0>) tensor(11016.7461, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11016.7412109375
tensor(11016.7461, grad_fn=<NegBackward0>) tensor(11016.7412, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11016.705078125
tensor(11016.7412, grad_fn=<NegBackward0>) tensor(11016.7051, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11016.6064453125
tensor(11016.7051, grad_fn=<NegBackward0>) tensor(11016.6064, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11016.576171875
tensor(11016.6064, grad_fn=<NegBackward0>) tensor(11016.5762, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11016.572265625
tensor(11016.5762, grad_fn=<NegBackward0>) tensor(11016.5723, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11016.5712890625
tensor(11016.5723, grad_fn=<NegBackward0>) tensor(11016.5713, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11016.5732421875
tensor(11016.5713, grad_fn=<NegBackward0>) tensor(11016.5732, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11016.5693359375
tensor(11016.5713, grad_fn=<NegBackward0>) tensor(11016.5693, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11016.5703125
tensor(11016.5693, grad_fn=<NegBackward0>) tensor(11016.5703, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11016.5693359375
tensor(11016.5693, grad_fn=<NegBackward0>) tensor(11016.5693, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11016.5703125
tensor(11016.5693, grad_fn=<NegBackward0>) tensor(11016.5703, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11016.5703125
tensor(11016.5693, grad_fn=<NegBackward0>) tensor(11016.5703, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -11016.5703125
tensor(11016.5693, grad_fn=<NegBackward0>) tensor(11016.5703, grad_fn=<NegBackward0>)
3
Iteration 4000: Loss = -11016.5693359375
tensor(11016.5693, grad_fn=<NegBackward0>) tensor(11016.5693, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11016.5693359375
tensor(11016.5693, grad_fn=<NegBackward0>) tensor(11016.5693, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11016.5703125
tensor(11016.5693, grad_fn=<NegBackward0>) tensor(11016.5703, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11016.5703125
tensor(11016.5693, grad_fn=<NegBackward0>) tensor(11016.5703, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -11016.5693359375
tensor(11016.5693, grad_fn=<NegBackward0>) tensor(11016.5693, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11016.5830078125
tensor(11016.5693, grad_fn=<NegBackward0>) tensor(11016.5830, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11016.5703125
tensor(11016.5693, grad_fn=<NegBackward0>) tensor(11016.5703, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11016.5703125
tensor(11016.5693, grad_fn=<NegBackward0>) tensor(11016.5703, grad_fn=<NegBackward0>)
3
Iteration 4800: Loss = -11016.5703125
tensor(11016.5693, grad_fn=<NegBackward0>) tensor(11016.5703, grad_fn=<NegBackward0>)
4
Iteration 4900: Loss = -11016.568359375
tensor(11016.5693, grad_fn=<NegBackward0>) tensor(11016.5684, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11016.5693359375
tensor(11016.5684, grad_fn=<NegBackward0>) tensor(11016.5693, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11016.5703125
tensor(11016.5684, grad_fn=<NegBackward0>) tensor(11016.5703, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11016.5703125
tensor(11016.5684, grad_fn=<NegBackward0>) tensor(11016.5703, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -11016.5693359375
tensor(11016.5684, grad_fn=<NegBackward0>) tensor(11016.5693, grad_fn=<NegBackward0>)
4
Iteration 5400: Loss = -11016.5703125
tensor(11016.5684, grad_fn=<NegBackward0>) tensor(11016.5703, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5400 due to no improvement.
pi: tensor([[0.0443, 0.9557],
        [0.0943, 0.9057]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9879, 0.0121], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1712, 0.0940],
         [0.6329, 0.1709]],

        [[0.6931, 0.0751],
         [0.6990, 0.6417]],

        [[0.5316, 0.0930],
         [0.7200, 0.5151]],

        [[0.5513, 0.0899],
         [0.6946, 0.6196]],

        [[0.6786, 0.1678],
         [0.5085, 0.6133]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.00038912871648324923
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.02704564545834377
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.03301803860895249
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.002646996685389661
Average Adjusted Rand Index: 0.012424008107341835
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22818.328125
inf tensor(22818.3281, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11027.1435546875
tensor(22818.3281, grad_fn=<NegBackward0>) tensor(11027.1436, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11026.63671875
tensor(11027.1436, grad_fn=<NegBackward0>) tensor(11026.6367, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11026.3896484375
tensor(11026.6367, grad_fn=<NegBackward0>) tensor(11026.3896, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11026.17578125
tensor(11026.3896, grad_fn=<NegBackward0>) tensor(11026.1758, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11026.103515625
tensor(11026.1758, grad_fn=<NegBackward0>) tensor(11026.1035, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11026.0634765625
tensor(11026.1035, grad_fn=<NegBackward0>) tensor(11026.0635, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11026.017578125
tensor(11026.0635, grad_fn=<NegBackward0>) tensor(11026.0176, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11025.9541015625
tensor(11026.0176, grad_fn=<NegBackward0>) tensor(11025.9541, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11025.833984375
tensor(11025.9541, grad_fn=<NegBackward0>) tensor(11025.8340, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11025.705078125
tensor(11025.8340, grad_fn=<NegBackward0>) tensor(11025.7051, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11025.630859375
tensor(11025.7051, grad_fn=<NegBackward0>) tensor(11025.6309, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11025.5869140625
tensor(11025.6309, grad_fn=<NegBackward0>) tensor(11025.5869, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11025.5595703125
tensor(11025.5869, grad_fn=<NegBackward0>) tensor(11025.5596, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11025.5400390625
tensor(11025.5596, grad_fn=<NegBackward0>) tensor(11025.5400, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11025.5283203125
tensor(11025.5400, grad_fn=<NegBackward0>) tensor(11025.5283, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11025.5146484375
tensor(11025.5283, grad_fn=<NegBackward0>) tensor(11025.5146, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11025.5009765625
tensor(11025.5146, grad_fn=<NegBackward0>) tensor(11025.5010, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11025.4853515625
tensor(11025.5010, grad_fn=<NegBackward0>) tensor(11025.4854, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11025.4609375
tensor(11025.4854, grad_fn=<NegBackward0>) tensor(11025.4609, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11025.4140625
tensor(11025.4609, grad_fn=<NegBackward0>) tensor(11025.4141, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11025.271484375
tensor(11025.4141, grad_fn=<NegBackward0>) tensor(11025.2715, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11024.375
tensor(11025.2715, grad_fn=<NegBackward0>) tensor(11024.3750, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11023.3193359375
tensor(11024.3750, grad_fn=<NegBackward0>) tensor(11023.3193, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11022.201171875
tensor(11023.3193, grad_fn=<NegBackward0>) tensor(11022.2012, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11021.3671875
tensor(11022.2012, grad_fn=<NegBackward0>) tensor(11021.3672, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11021.2470703125
tensor(11021.3672, grad_fn=<NegBackward0>) tensor(11021.2471, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11021.1884765625
tensor(11021.2471, grad_fn=<NegBackward0>) tensor(11021.1885, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11021.146484375
tensor(11021.1885, grad_fn=<NegBackward0>) tensor(11021.1465, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11021.1103515625
tensor(11021.1465, grad_fn=<NegBackward0>) tensor(11021.1104, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11021.0712890625
tensor(11021.1104, grad_fn=<NegBackward0>) tensor(11021.0713, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11021.0234375
tensor(11021.0713, grad_fn=<NegBackward0>) tensor(11021.0234, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11020.94140625
tensor(11021.0234, grad_fn=<NegBackward0>) tensor(11020.9414, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11020.5947265625
tensor(11020.9414, grad_fn=<NegBackward0>) tensor(11020.5947, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11016.9931640625
tensor(11020.5947, grad_fn=<NegBackward0>) tensor(11016.9932, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11016.8681640625
tensor(11016.9932, grad_fn=<NegBackward0>) tensor(11016.8682, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11016.69140625
tensor(11016.8682, grad_fn=<NegBackward0>) tensor(11016.6914, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11016.3134765625
tensor(11016.6914, grad_fn=<NegBackward0>) tensor(11016.3135, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11016.0947265625
tensor(11016.3135, grad_fn=<NegBackward0>) tensor(11016.0947, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11015.9638671875
tensor(11016.0947, grad_fn=<NegBackward0>) tensor(11015.9639, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11015.896484375
tensor(11015.9639, grad_fn=<NegBackward0>) tensor(11015.8965, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11015.857421875
tensor(11015.8965, grad_fn=<NegBackward0>) tensor(11015.8574, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11015.8203125
tensor(11015.8574, grad_fn=<NegBackward0>) tensor(11015.8203, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11015.783203125
tensor(11015.8203, grad_fn=<NegBackward0>) tensor(11015.7832, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11015.755859375
tensor(11015.7832, grad_fn=<NegBackward0>) tensor(11015.7559, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11015.744140625
tensor(11015.7559, grad_fn=<NegBackward0>) tensor(11015.7441, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11015.7265625
tensor(11015.7441, grad_fn=<NegBackward0>) tensor(11015.7266, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11015.720703125
tensor(11015.7266, grad_fn=<NegBackward0>) tensor(11015.7207, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11014.861328125
tensor(11015.7207, grad_fn=<NegBackward0>) tensor(11014.8613, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11014.599609375
tensor(11014.8613, grad_fn=<NegBackward0>) tensor(11014.5996, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11014.5771484375
tensor(11014.5996, grad_fn=<NegBackward0>) tensor(11014.5771, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11014.5703125
tensor(11014.5771, grad_fn=<NegBackward0>) tensor(11014.5703, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11014.5615234375
tensor(11014.5703, grad_fn=<NegBackward0>) tensor(11014.5615, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11014.4833984375
tensor(11014.5615, grad_fn=<NegBackward0>) tensor(11014.4834, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11014.47265625
tensor(11014.4834, grad_fn=<NegBackward0>) tensor(11014.4727, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11014.455078125
tensor(11014.4727, grad_fn=<NegBackward0>) tensor(11014.4551, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11014.4228515625
tensor(11014.4551, grad_fn=<NegBackward0>) tensor(11014.4229, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11014.421875
tensor(11014.4229, grad_fn=<NegBackward0>) tensor(11014.4219, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11014.419921875
tensor(11014.4219, grad_fn=<NegBackward0>) tensor(11014.4199, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11014.419921875
tensor(11014.4199, grad_fn=<NegBackward0>) tensor(11014.4199, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11014.4169921875
tensor(11014.4199, grad_fn=<NegBackward0>) tensor(11014.4170, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11014.416015625
tensor(11014.4170, grad_fn=<NegBackward0>) tensor(11014.4160, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11014.4150390625
tensor(11014.4160, grad_fn=<NegBackward0>) tensor(11014.4150, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11014.4111328125
tensor(11014.4150, grad_fn=<NegBackward0>) tensor(11014.4111, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11014.3681640625
tensor(11014.4111, grad_fn=<NegBackward0>) tensor(11014.3682, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11014.3671875
tensor(11014.3682, grad_fn=<NegBackward0>) tensor(11014.3672, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11014.3662109375
tensor(11014.3672, grad_fn=<NegBackward0>) tensor(11014.3662, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11014.365234375
tensor(11014.3662, grad_fn=<NegBackward0>) tensor(11014.3652, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11014.3681640625
tensor(11014.3652, grad_fn=<NegBackward0>) tensor(11014.3682, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11014.3642578125
tensor(11014.3652, grad_fn=<NegBackward0>) tensor(11014.3643, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11014.36328125
tensor(11014.3643, grad_fn=<NegBackward0>) tensor(11014.3633, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11014.361328125
tensor(11014.3633, grad_fn=<NegBackward0>) tensor(11014.3613, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11014.359375
tensor(11014.3613, grad_fn=<NegBackward0>) tensor(11014.3594, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11014.3603515625
tensor(11014.3594, grad_fn=<NegBackward0>) tensor(11014.3604, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11014.359375
tensor(11014.3594, grad_fn=<NegBackward0>) tensor(11014.3594, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11014.3623046875
tensor(11014.3594, grad_fn=<NegBackward0>) tensor(11014.3623, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11014.359375
tensor(11014.3594, grad_fn=<NegBackward0>) tensor(11014.3594, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11014.359375
tensor(11014.3594, grad_fn=<NegBackward0>) tensor(11014.3594, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11014.3603515625
tensor(11014.3594, grad_fn=<NegBackward0>) tensor(11014.3604, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11014.3583984375
tensor(11014.3594, grad_fn=<NegBackward0>) tensor(11014.3584, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11014.3701171875
tensor(11014.3584, grad_fn=<NegBackward0>) tensor(11014.3701, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11014.357421875
tensor(11014.3584, grad_fn=<NegBackward0>) tensor(11014.3574, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11014.4140625
tensor(11014.3574, grad_fn=<NegBackward0>) tensor(11014.4141, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11014.3583984375
tensor(11014.3574, grad_fn=<NegBackward0>) tensor(11014.3584, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11014.357421875
tensor(11014.3574, grad_fn=<NegBackward0>) tensor(11014.3574, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11014.357421875
tensor(11014.3574, grad_fn=<NegBackward0>) tensor(11014.3574, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11014.3583984375
tensor(11014.3574, grad_fn=<NegBackward0>) tensor(11014.3584, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11014.37890625
tensor(11014.3574, grad_fn=<NegBackward0>) tensor(11014.3789, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11014.3583984375
tensor(11014.3574, grad_fn=<NegBackward0>) tensor(11014.3584, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -11014.359375
tensor(11014.3574, grad_fn=<NegBackward0>) tensor(11014.3594, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -11014.3583984375
tensor(11014.3574, grad_fn=<NegBackward0>) tensor(11014.3584, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[0.5537, 0.4463],
        [0.1076, 0.8924]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.5509e-05, 9.9998e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1501, 0.1764],
         [0.6009, 0.1770]],

        [[0.7172, 0.1076],
         [0.7080, 0.6495]],

        [[0.6660, 0.1001],
         [0.5688, 0.5892]],

        [[0.7216, 0.0988],
         [0.5349, 0.6963]],

        [[0.6514, 0.1528],
         [0.6082, 0.5813]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0017536168347216134
time is 2
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 67
Adjusted Rand Index: 0.10962256350146055
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 70
Adjusted Rand Index: 0.15292274655448576
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.025238515991671172
Global Adjusted Rand Index: 0.04138718404394468
Average Adjusted Rand Index: 0.05790748857646781
[0.002646996685389661, 0.04138718404394468] [0.012424008107341835, 0.05790748857646781] [11016.5703125, 11014.3583984375]
-------------------------------------
This iteration is 43
True Objective function: Loss = -10890.659785573027
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24218.083984375
inf tensor(24218.0840, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11022.626953125
tensor(24218.0840, grad_fn=<NegBackward0>) tensor(11022.6270, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11021.8896484375
tensor(11022.6270, grad_fn=<NegBackward0>) tensor(11021.8896, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11021.69140625
tensor(11021.8896, grad_fn=<NegBackward0>) tensor(11021.6914, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11021.5556640625
tensor(11021.6914, grad_fn=<NegBackward0>) tensor(11021.5557, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11021.076171875
tensor(11021.5557, grad_fn=<NegBackward0>) tensor(11021.0762, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11020.1416015625
tensor(11021.0762, grad_fn=<NegBackward0>) tensor(11020.1416, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11019.6220703125
tensor(11020.1416, grad_fn=<NegBackward0>) tensor(11019.6221, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11019.279296875
tensor(11019.6221, grad_fn=<NegBackward0>) tensor(11019.2793, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11019.0302734375
tensor(11019.2793, grad_fn=<NegBackward0>) tensor(11019.0303, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11018.763671875
tensor(11019.0303, grad_fn=<NegBackward0>) tensor(11018.7637, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11018.3818359375
tensor(11018.7637, grad_fn=<NegBackward0>) tensor(11018.3818, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11017.576171875
tensor(11018.3818, grad_fn=<NegBackward0>) tensor(11017.5762, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10927.169921875
tensor(11017.5762, grad_fn=<NegBackward0>) tensor(10927.1699, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10900.65625
tensor(10927.1699, grad_fn=<NegBackward0>) tensor(10900.6562, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10892.431640625
tensor(10900.6562, grad_fn=<NegBackward0>) tensor(10892.4316, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10887.99609375
tensor(10892.4316, grad_fn=<NegBackward0>) tensor(10887.9961, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10887.7431640625
tensor(10887.9961, grad_fn=<NegBackward0>) tensor(10887.7432, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10887.0087890625
tensor(10887.7432, grad_fn=<NegBackward0>) tensor(10887.0088, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10880.33984375
tensor(10887.0088, grad_fn=<NegBackward0>) tensor(10880.3398, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10880.111328125
tensor(10880.3398, grad_fn=<NegBackward0>) tensor(10880.1113, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10880.07421875
tensor(10880.1113, grad_fn=<NegBackward0>) tensor(10880.0742, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10880.0517578125
tensor(10880.0742, grad_fn=<NegBackward0>) tensor(10880.0518, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10880.033203125
tensor(10880.0518, grad_fn=<NegBackward0>) tensor(10880.0332, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10880.0244140625
tensor(10880.0332, grad_fn=<NegBackward0>) tensor(10880.0244, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10880.0166015625
tensor(10880.0244, grad_fn=<NegBackward0>) tensor(10880.0166, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10880.0107421875
tensor(10880.0166, grad_fn=<NegBackward0>) tensor(10880.0107, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10880.00390625
tensor(10880.0107, grad_fn=<NegBackward0>) tensor(10880.0039, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10880.0009765625
tensor(10880.0039, grad_fn=<NegBackward0>) tensor(10880.0010, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10879.994140625
tensor(10880.0010, grad_fn=<NegBackward0>) tensor(10879.9941, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10879.9912109375
tensor(10879.9941, grad_fn=<NegBackward0>) tensor(10879.9912, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10879.9873046875
tensor(10879.9912, grad_fn=<NegBackward0>) tensor(10879.9873, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10879.96484375
tensor(10879.9873, grad_fn=<NegBackward0>) tensor(10879.9648, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10879.9443359375
tensor(10879.9648, grad_fn=<NegBackward0>) tensor(10879.9443, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10879.935546875
tensor(10879.9443, grad_fn=<NegBackward0>) tensor(10879.9355, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10879.9296875
tensor(10879.9355, grad_fn=<NegBackward0>) tensor(10879.9297, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10879.921875
tensor(10879.9297, grad_fn=<NegBackward0>) tensor(10879.9219, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10879.90234375
tensor(10879.9219, grad_fn=<NegBackward0>) tensor(10879.9023, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10879.853515625
tensor(10879.9023, grad_fn=<NegBackward0>) tensor(10879.8535, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10879.490234375
tensor(10879.8535, grad_fn=<NegBackward0>) tensor(10879.4902, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10879.2158203125
tensor(10879.4902, grad_fn=<NegBackward0>) tensor(10879.2158, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10879.1337890625
tensor(10879.2158, grad_fn=<NegBackward0>) tensor(10879.1338, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10878.9912109375
tensor(10879.1338, grad_fn=<NegBackward0>) tensor(10878.9912, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10878.9697265625
tensor(10878.9912, grad_fn=<NegBackward0>) tensor(10878.9697, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10878.783203125
tensor(10878.9697, grad_fn=<NegBackward0>) tensor(10878.7832, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10878.744140625
tensor(10878.7832, grad_fn=<NegBackward0>) tensor(10878.7441, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10878.50390625
tensor(10878.7441, grad_fn=<NegBackward0>) tensor(10878.5039, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10856.7822265625
tensor(10878.5039, grad_fn=<NegBackward0>) tensor(10856.7822, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10856.572265625
tensor(10856.7822, grad_fn=<NegBackward0>) tensor(10856.5723, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10845.615234375
tensor(10856.5723, grad_fn=<NegBackward0>) tensor(10845.6152, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10838.7373046875
tensor(10845.6152, grad_fn=<NegBackward0>) tensor(10838.7373, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10838.7177734375
tensor(10838.7373, grad_fn=<NegBackward0>) tensor(10838.7178, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10838.71875
tensor(10838.7178, grad_fn=<NegBackward0>) tensor(10838.7188, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10838.7158203125
tensor(10838.7178, grad_fn=<NegBackward0>) tensor(10838.7158, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10838.7158203125
tensor(10838.7158, grad_fn=<NegBackward0>) tensor(10838.7158, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10838.716796875
tensor(10838.7158, grad_fn=<NegBackward0>) tensor(10838.7168, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10838.71484375
tensor(10838.7158, grad_fn=<NegBackward0>) tensor(10838.7148, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10838.71484375
tensor(10838.7148, grad_fn=<NegBackward0>) tensor(10838.7148, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10838.7138671875
tensor(10838.7148, grad_fn=<NegBackward0>) tensor(10838.7139, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10838.71484375
tensor(10838.7139, grad_fn=<NegBackward0>) tensor(10838.7148, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10838.712890625
tensor(10838.7139, grad_fn=<NegBackward0>) tensor(10838.7129, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10838.7177734375
tensor(10838.7129, grad_fn=<NegBackward0>) tensor(10838.7178, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10838.71484375
tensor(10838.7129, grad_fn=<NegBackward0>) tensor(10838.7148, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -10838.7138671875
tensor(10838.7129, grad_fn=<NegBackward0>) tensor(10838.7139, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -10838.7138671875
tensor(10838.7129, grad_fn=<NegBackward0>) tensor(10838.7139, grad_fn=<NegBackward0>)
4
Iteration 6500: Loss = -10838.712890625
tensor(10838.7129, grad_fn=<NegBackward0>) tensor(10838.7129, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10838.7138671875
tensor(10838.7129, grad_fn=<NegBackward0>) tensor(10838.7139, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10838.71484375
tensor(10838.7129, grad_fn=<NegBackward0>) tensor(10838.7148, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -10838.7138671875
tensor(10838.7129, grad_fn=<NegBackward0>) tensor(10838.7139, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -10838.720703125
tensor(10838.7129, grad_fn=<NegBackward0>) tensor(10838.7207, grad_fn=<NegBackward0>)
4
Iteration 7000: Loss = -10838.71875
tensor(10838.7129, grad_fn=<NegBackward0>) tensor(10838.7188, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7000 due to no improvement.
pi: tensor([[0.7891, 0.2109],
        [0.2643, 0.7357]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4239, 0.5761], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2565, 0.1021],
         [0.5810, 0.2025]],

        [[0.6196, 0.0974],
         [0.5263, 0.6664]],

        [[0.6397, 0.1044],
         [0.5275, 0.5488]],

        [[0.5987, 0.0824],
         [0.5220, 0.6944]],

        [[0.6244, 0.0996],
         [0.5025, 0.5793]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 8
Adjusted Rand Index: 0.7025982975809663
time is 1
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 7
Adjusted Rand Index: 0.7369696969696969
time is 2
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 3
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824283882000855
Global Adjusted Rand Index: 0.8168490101771756
Average Adjusted Rand Index: 0.8180152500994478
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23408.015625
inf tensor(23408.0156, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11022.736328125
tensor(23408.0156, grad_fn=<NegBackward0>) tensor(11022.7363, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11021.4873046875
tensor(11022.7363, grad_fn=<NegBackward0>) tensor(11021.4873, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11020.814453125
tensor(11021.4873, grad_fn=<NegBackward0>) tensor(11020.8145, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11020.4345703125
tensor(11020.8145, grad_fn=<NegBackward0>) tensor(11020.4346, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11020.1708984375
tensor(11020.4346, grad_fn=<NegBackward0>) tensor(11020.1709, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11019.912109375
tensor(11020.1709, grad_fn=<NegBackward0>) tensor(11019.9121, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11019.6484375
tensor(11019.9121, grad_fn=<NegBackward0>) tensor(11019.6484, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11019.3056640625
tensor(11019.6484, grad_fn=<NegBackward0>) tensor(11019.3057, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11018.9365234375
tensor(11019.3057, grad_fn=<NegBackward0>) tensor(11018.9365, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11018.7197265625
tensor(11018.9365, grad_fn=<NegBackward0>) tensor(11018.7197, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11018.4501953125
tensor(11018.7197, grad_fn=<NegBackward0>) tensor(11018.4502, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11018.0107421875
tensor(11018.4502, grad_fn=<NegBackward0>) tensor(11018.0107, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11016.9033203125
tensor(11018.0107, grad_fn=<NegBackward0>) tensor(11016.9033, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10914.734375
tensor(11016.9033, grad_fn=<NegBackward0>) tensor(10914.7344, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10887.9423828125
tensor(10914.7344, grad_fn=<NegBackward0>) tensor(10887.9424, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10887.626953125
tensor(10887.9424, grad_fn=<NegBackward0>) tensor(10887.6270, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10883.8681640625
tensor(10887.6270, grad_fn=<NegBackward0>) tensor(10883.8682, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10883.5859375
tensor(10883.8682, grad_fn=<NegBackward0>) tensor(10883.5859, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10883.5302734375
tensor(10883.5859, grad_fn=<NegBackward0>) tensor(10883.5303, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10883.5
tensor(10883.5303, grad_fn=<NegBackward0>) tensor(10883.5000, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10882.93359375
tensor(10883.5000, grad_fn=<NegBackward0>) tensor(10882.9336, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10882.8984375
tensor(10882.9336, grad_fn=<NegBackward0>) tensor(10882.8984, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10882.873046875
tensor(10882.8984, grad_fn=<NegBackward0>) tensor(10882.8730, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10882.8193359375
tensor(10882.8730, grad_fn=<NegBackward0>) tensor(10882.8193, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10882.6142578125
tensor(10882.8193, grad_fn=<NegBackward0>) tensor(10882.6143, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10882.099609375
tensor(10882.6143, grad_fn=<NegBackward0>) tensor(10882.0996, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10881.9248046875
tensor(10882.0996, grad_fn=<NegBackward0>) tensor(10881.9248, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10881.8740234375
tensor(10881.9248, grad_fn=<NegBackward0>) tensor(10881.8740, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10881.80078125
tensor(10881.8740, grad_fn=<NegBackward0>) tensor(10881.8008, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10881.6064453125
tensor(10881.8008, grad_fn=<NegBackward0>) tensor(10881.6064, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10881.5771484375
tensor(10881.6064, grad_fn=<NegBackward0>) tensor(10881.5771, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10849.36328125
tensor(10881.5771, grad_fn=<NegBackward0>) tensor(10849.3633, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10846.814453125
tensor(10849.3633, grad_fn=<NegBackward0>) tensor(10846.8145, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10846.80078125
tensor(10846.8145, grad_fn=<NegBackward0>) tensor(10846.8008, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10846.796875
tensor(10846.8008, grad_fn=<NegBackward0>) tensor(10846.7969, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10846.7919921875
tensor(10846.7969, grad_fn=<NegBackward0>) tensor(10846.7920, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10841.857421875
tensor(10846.7920, grad_fn=<NegBackward0>) tensor(10841.8574, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10839.7255859375
tensor(10841.8574, grad_fn=<NegBackward0>) tensor(10839.7256, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10839.6982421875
tensor(10839.7256, grad_fn=<NegBackward0>) tensor(10839.6982, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10838.75
tensor(10839.6982, grad_fn=<NegBackward0>) tensor(10838.7500, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10838.7490234375
tensor(10838.7500, grad_fn=<NegBackward0>) tensor(10838.7490, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10838.748046875
tensor(10838.7490, grad_fn=<NegBackward0>) tensor(10838.7480, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10838.7470703125
tensor(10838.7480, grad_fn=<NegBackward0>) tensor(10838.7471, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10838.7470703125
tensor(10838.7471, grad_fn=<NegBackward0>) tensor(10838.7471, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10838.74609375
tensor(10838.7471, grad_fn=<NegBackward0>) tensor(10838.7461, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10838.7470703125
tensor(10838.7461, grad_fn=<NegBackward0>) tensor(10838.7471, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -10838.7392578125
tensor(10838.7461, grad_fn=<NegBackward0>) tensor(10838.7393, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10838.744140625
tensor(10838.7393, grad_fn=<NegBackward0>) tensor(10838.7441, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10838.7392578125
tensor(10838.7393, grad_fn=<NegBackward0>) tensor(10838.7393, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10838.73828125
tensor(10838.7393, grad_fn=<NegBackward0>) tensor(10838.7383, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10838.7373046875
tensor(10838.7383, grad_fn=<NegBackward0>) tensor(10838.7373, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10838.736328125
tensor(10838.7373, grad_fn=<NegBackward0>) tensor(10838.7363, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10838.7373046875
tensor(10838.7363, grad_fn=<NegBackward0>) tensor(10838.7373, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10838.7353515625
tensor(10838.7363, grad_fn=<NegBackward0>) tensor(10838.7354, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10838.7353515625
tensor(10838.7354, grad_fn=<NegBackward0>) tensor(10838.7354, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10838.748046875
tensor(10838.7354, grad_fn=<NegBackward0>) tensor(10838.7480, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10838.7353515625
tensor(10838.7354, grad_fn=<NegBackward0>) tensor(10838.7354, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10838.7353515625
tensor(10838.7354, grad_fn=<NegBackward0>) tensor(10838.7354, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10838.734375
tensor(10838.7354, grad_fn=<NegBackward0>) tensor(10838.7344, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10838.7333984375
tensor(10838.7344, grad_fn=<NegBackward0>) tensor(10838.7334, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10838.7353515625
tensor(10838.7334, grad_fn=<NegBackward0>) tensor(10838.7354, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10838.73828125
tensor(10838.7334, grad_fn=<NegBackward0>) tensor(10838.7383, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -10838.734375
tensor(10838.7334, grad_fn=<NegBackward0>) tensor(10838.7344, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -10838.7333984375
tensor(10838.7334, grad_fn=<NegBackward0>) tensor(10838.7334, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10838.73828125
tensor(10838.7334, grad_fn=<NegBackward0>) tensor(10838.7383, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10838.7333984375
tensor(10838.7334, grad_fn=<NegBackward0>) tensor(10838.7334, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10838.7333984375
tensor(10838.7334, grad_fn=<NegBackward0>) tensor(10838.7334, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10838.732421875
tensor(10838.7334, grad_fn=<NegBackward0>) tensor(10838.7324, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10838.7314453125
tensor(10838.7324, grad_fn=<NegBackward0>) tensor(10838.7314, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10838.7314453125
tensor(10838.7314, grad_fn=<NegBackward0>) tensor(10838.7314, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10838.7177734375
tensor(10838.7314, grad_fn=<NegBackward0>) tensor(10838.7178, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10838.7216796875
tensor(10838.7178, grad_fn=<NegBackward0>) tensor(10838.7217, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10838.712890625
tensor(10838.7178, grad_fn=<NegBackward0>) tensor(10838.7129, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10838.7138671875
tensor(10838.7129, grad_fn=<NegBackward0>) tensor(10838.7139, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10838.712890625
tensor(10838.7129, grad_fn=<NegBackward0>) tensor(10838.7129, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10838.712890625
tensor(10838.7129, grad_fn=<NegBackward0>) tensor(10838.7129, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10838.716796875
tensor(10838.7129, grad_fn=<NegBackward0>) tensor(10838.7168, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10838.7255859375
tensor(10838.7129, grad_fn=<NegBackward0>) tensor(10838.7256, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -10838.7177734375
tensor(10838.7129, grad_fn=<NegBackward0>) tensor(10838.7178, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -10838.7119140625
tensor(10838.7129, grad_fn=<NegBackward0>) tensor(10838.7119, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10838.712890625
tensor(10838.7119, grad_fn=<NegBackward0>) tensor(10838.7129, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10838.712890625
tensor(10838.7119, grad_fn=<NegBackward0>) tensor(10838.7129, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -10838.7158203125
tensor(10838.7119, grad_fn=<NegBackward0>) tensor(10838.7158, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -10838.712890625
tensor(10838.7119, grad_fn=<NegBackward0>) tensor(10838.7129, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -10838.7109375
tensor(10838.7119, grad_fn=<NegBackward0>) tensor(10838.7109, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10838.712890625
tensor(10838.7109, grad_fn=<NegBackward0>) tensor(10838.7129, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10838.7119140625
tensor(10838.7109, grad_fn=<NegBackward0>) tensor(10838.7119, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -10838.71484375
tensor(10838.7109, grad_fn=<NegBackward0>) tensor(10838.7148, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -10838.7119140625
tensor(10838.7109, grad_fn=<NegBackward0>) tensor(10838.7119, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -10838.7353515625
tensor(10838.7109, grad_fn=<NegBackward0>) tensor(10838.7354, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[0.7904, 0.2096],
        [0.2675, 0.7325]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4246, 0.5754], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2554, 0.1021],
         [0.6224, 0.2026]],

        [[0.5556, 0.0974],
         [0.6492, 0.7260]],

        [[0.5323, 0.1043],
         [0.5614, 0.5036]],

        [[0.5363, 0.0824],
         [0.5184, 0.5584]],

        [[0.6681, 0.0996],
         [0.6402, 0.5302]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 8
Adjusted Rand Index: 0.7025982975809663
time is 1
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 7
Adjusted Rand Index: 0.7369696969696969
time is 2
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 3
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824283882000855
Global Adjusted Rand Index: 0.8168490101771756
Average Adjusted Rand Index: 0.8180152500994478
[0.8168490101771756, 0.8168490101771756] [0.8180152500994478, 0.8180152500994478] [10838.71875, 10838.7353515625]
-------------------------------------
This iteration is 44
True Objective function: Loss = -10724.883295242535
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23762.470703125
inf tensor(23762.4707, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10812.5078125
tensor(23762.4707, grad_fn=<NegBackward0>) tensor(10812.5078, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10811.64453125
tensor(10812.5078, grad_fn=<NegBackward0>) tensor(10811.6445, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10810.99609375
tensor(10811.6445, grad_fn=<NegBackward0>) tensor(10810.9961, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10810.89453125
tensor(10810.9961, grad_fn=<NegBackward0>) tensor(10810.8945, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10810.8486328125
tensor(10810.8945, grad_fn=<NegBackward0>) tensor(10810.8486, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10810.8193359375
tensor(10810.8486, grad_fn=<NegBackward0>) tensor(10810.8193, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10810.798828125
tensor(10810.8193, grad_fn=<NegBackward0>) tensor(10810.7988, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10810.779296875
tensor(10810.7988, grad_fn=<NegBackward0>) tensor(10810.7793, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10810.7607421875
tensor(10810.7793, grad_fn=<NegBackward0>) tensor(10810.7607, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10810.7392578125
tensor(10810.7607, grad_fn=<NegBackward0>) tensor(10810.7393, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10810.712890625
tensor(10810.7393, grad_fn=<NegBackward0>) tensor(10810.7129, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10810.6748046875
tensor(10810.7129, grad_fn=<NegBackward0>) tensor(10810.6748, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10810.595703125
tensor(10810.6748, grad_fn=<NegBackward0>) tensor(10810.5957, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10809.91796875
tensor(10810.5957, grad_fn=<NegBackward0>) tensor(10809.9180, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10808.5478515625
tensor(10809.9180, grad_fn=<NegBackward0>) tensor(10808.5479, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10808.0087890625
tensor(10808.5479, grad_fn=<NegBackward0>) tensor(10808.0088, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10807.734375
tensor(10808.0088, grad_fn=<NegBackward0>) tensor(10807.7344, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10807.5966796875
tensor(10807.7344, grad_fn=<NegBackward0>) tensor(10807.5967, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10807.51953125
tensor(10807.5967, grad_fn=<NegBackward0>) tensor(10807.5195, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10807.46875
tensor(10807.5195, grad_fn=<NegBackward0>) tensor(10807.4688, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10807.4365234375
tensor(10807.4688, grad_fn=<NegBackward0>) tensor(10807.4365, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10807.4111328125
tensor(10807.4365, grad_fn=<NegBackward0>) tensor(10807.4111, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10807.3916015625
tensor(10807.4111, grad_fn=<NegBackward0>) tensor(10807.3916, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10807.3798828125
tensor(10807.3916, grad_fn=<NegBackward0>) tensor(10807.3799, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10807.3681640625
tensor(10807.3799, grad_fn=<NegBackward0>) tensor(10807.3682, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10807.357421875
tensor(10807.3682, grad_fn=<NegBackward0>) tensor(10807.3574, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10807.3505859375
tensor(10807.3574, grad_fn=<NegBackward0>) tensor(10807.3506, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10807.3447265625
tensor(10807.3506, grad_fn=<NegBackward0>) tensor(10807.3447, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10807.33984375
tensor(10807.3447, grad_fn=<NegBackward0>) tensor(10807.3398, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10807.3359375
tensor(10807.3398, grad_fn=<NegBackward0>) tensor(10807.3359, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10807.3310546875
tensor(10807.3359, grad_fn=<NegBackward0>) tensor(10807.3311, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10807.3291015625
tensor(10807.3311, grad_fn=<NegBackward0>) tensor(10807.3291, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10807.3251953125
tensor(10807.3291, grad_fn=<NegBackward0>) tensor(10807.3252, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10807.322265625
tensor(10807.3252, grad_fn=<NegBackward0>) tensor(10807.3223, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10807.3203125
tensor(10807.3223, grad_fn=<NegBackward0>) tensor(10807.3203, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10807.3193359375
tensor(10807.3203, grad_fn=<NegBackward0>) tensor(10807.3193, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10807.3193359375
tensor(10807.3193, grad_fn=<NegBackward0>) tensor(10807.3193, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10807.31640625
tensor(10807.3193, grad_fn=<NegBackward0>) tensor(10807.3164, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10807.314453125
tensor(10807.3164, grad_fn=<NegBackward0>) tensor(10807.3145, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10807.314453125
tensor(10807.3145, grad_fn=<NegBackward0>) tensor(10807.3145, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10807.3125
tensor(10807.3145, grad_fn=<NegBackward0>) tensor(10807.3125, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10807.3115234375
tensor(10807.3125, grad_fn=<NegBackward0>) tensor(10807.3115, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10807.3095703125
tensor(10807.3115, grad_fn=<NegBackward0>) tensor(10807.3096, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10807.30859375
tensor(10807.3096, grad_fn=<NegBackward0>) tensor(10807.3086, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10807.30859375
tensor(10807.3086, grad_fn=<NegBackward0>) tensor(10807.3086, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10807.30859375
tensor(10807.3086, grad_fn=<NegBackward0>) tensor(10807.3086, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10807.30859375
tensor(10807.3086, grad_fn=<NegBackward0>) tensor(10807.3086, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10807.3056640625
tensor(10807.3086, grad_fn=<NegBackward0>) tensor(10807.3057, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10807.3046875
tensor(10807.3057, grad_fn=<NegBackward0>) tensor(10807.3047, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10807.3046875
tensor(10807.3047, grad_fn=<NegBackward0>) tensor(10807.3047, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10807.3046875
tensor(10807.3047, grad_fn=<NegBackward0>) tensor(10807.3047, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10807.3037109375
tensor(10807.3047, grad_fn=<NegBackward0>) tensor(10807.3037, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10807.3037109375
tensor(10807.3037, grad_fn=<NegBackward0>) tensor(10807.3037, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10807.302734375
tensor(10807.3037, grad_fn=<NegBackward0>) tensor(10807.3027, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10807.3017578125
tensor(10807.3027, grad_fn=<NegBackward0>) tensor(10807.3018, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10807.3037109375
tensor(10807.3018, grad_fn=<NegBackward0>) tensor(10807.3037, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10807.302734375
tensor(10807.3018, grad_fn=<NegBackward0>) tensor(10807.3027, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -10807.30078125
tensor(10807.3018, grad_fn=<NegBackward0>) tensor(10807.3008, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10807.3017578125
tensor(10807.3008, grad_fn=<NegBackward0>) tensor(10807.3018, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10807.30078125
tensor(10807.3008, grad_fn=<NegBackward0>) tensor(10807.3008, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10807.30078125
tensor(10807.3008, grad_fn=<NegBackward0>) tensor(10807.3008, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10807.3017578125
tensor(10807.3008, grad_fn=<NegBackward0>) tensor(10807.3018, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10807.30078125
tensor(10807.3008, grad_fn=<NegBackward0>) tensor(10807.3008, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10807.3017578125
tensor(10807.3008, grad_fn=<NegBackward0>) tensor(10807.3018, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10807.2998046875
tensor(10807.3008, grad_fn=<NegBackward0>) tensor(10807.2998, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10807.30078125
tensor(10807.2998, grad_fn=<NegBackward0>) tensor(10807.3008, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10807.30078125
tensor(10807.2998, grad_fn=<NegBackward0>) tensor(10807.3008, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -10807.30078125
tensor(10807.2998, grad_fn=<NegBackward0>) tensor(10807.3008, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -10807.2998046875
tensor(10807.2998, grad_fn=<NegBackward0>) tensor(10807.2998, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10807.298828125
tensor(10807.2998, grad_fn=<NegBackward0>) tensor(10807.2988, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10807.2998046875
tensor(10807.2988, grad_fn=<NegBackward0>) tensor(10807.2998, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10807.2998046875
tensor(10807.2988, grad_fn=<NegBackward0>) tensor(10807.2998, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -10807.2978515625
tensor(10807.2988, grad_fn=<NegBackward0>) tensor(10807.2979, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10807.2998046875
tensor(10807.2979, grad_fn=<NegBackward0>) tensor(10807.2998, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10807.2978515625
tensor(10807.2979, grad_fn=<NegBackward0>) tensor(10807.2979, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10807.2998046875
tensor(10807.2979, grad_fn=<NegBackward0>) tensor(10807.2998, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10807.296875
tensor(10807.2979, grad_fn=<NegBackward0>) tensor(10807.2969, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10807.298828125
tensor(10807.2969, grad_fn=<NegBackward0>) tensor(10807.2988, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10807.298828125
tensor(10807.2969, grad_fn=<NegBackward0>) tensor(10807.2988, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -10807.2958984375
tensor(10807.2969, grad_fn=<NegBackward0>) tensor(10807.2959, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10807.298828125
tensor(10807.2959, grad_fn=<NegBackward0>) tensor(10807.2988, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10807.3037109375
tensor(10807.2959, grad_fn=<NegBackward0>) tensor(10807.3037, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -10807.388671875
tensor(10807.2959, grad_fn=<NegBackward0>) tensor(10807.3887, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -10807.298828125
tensor(10807.2959, grad_fn=<NegBackward0>) tensor(10807.2988, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -10807.296875
tensor(10807.2959, grad_fn=<NegBackward0>) tensor(10807.2969, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[9.9999e-01, 5.1495e-06],
        [3.8517e-01, 6.1483e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9579, 0.0421], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1566, 0.2237],
         [0.6784, 0.3703]],

        [[0.7263, 0.2124],
         [0.5193, 0.6106]],

        [[0.5815, 0.0835],
         [0.6924, 0.6555]],

        [[0.6734, 0.2512],
         [0.5904, 0.5628]],

        [[0.5345, 0.1618],
         [0.5946, 0.6200]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
Global Adjusted Rand Index: 2.805622592152369e-06
Average Adjusted Rand Index: -0.00022592098723064015
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20817.771484375
inf tensor(20817.7715, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10812.1513671875
tensor(20817.7715, grad_fn=<NegBackward0>) tensor(10812.1514, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10811.0693359375
tensor(10812.1514, grad_fn=<NegBackward0>) tensor(10811.0693, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10810.591796875
tensor(10811.0693, grad_fn=<NegBackward0>) tensor(10810.5918, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10810.5146484375
tensor(10810.5918, grad_fn=<NegBackward0>) tensor(10810.5146, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10810.45703125
tensor(10810.5146, grad_fn=<NegBackward0>) tensor(10810.4570, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10810.4189453125
tensor(10810.4570, grad_fn=<NegBackward0>) tensor(10810.4189, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10810.3984375
tensor(10810.4189, grad_fn=<NegBackward0>) tensor(10810.3984, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10810.3857421875
tensor(10810.3984, grad_fn=<NegBackward0>) tensor(10810.3857, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10810.3779296875
tensor(10810.3857, grad_fn=<NegBackward0>) tensor(10810.3779, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10810.3701171875
tensor(10810.3779, grad_fn=<NegBackward0>) tensor(10810.3701, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10810.3642578125
tensor(10810.3701, grad_fn=<NegBackward0>) tensor(10810.3643, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10810.36328125
tensor(10810.3643, grad_fn=<NegBackward0>) tensor(10810.3633, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10810.3515625
tensor(10810.3633, grad_fn=<NegBackward0>) tensor(10810.3516, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10810.3408203125
tensor(10810.3516, grad_fn=<NegBackward0>) tensor(10810.3408, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10810.3056640625
tensor(10810.3408, grad_fn=<NegBackward0>) tensor(10810.3057, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10809.7626953125
tensor(10810.3057, grad_fn=<NegBackward0>) tensor(10809.7627, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10808.64453125
tensor(10809.7627, grad_fn=<NegBackward0>) tensor(10808.6445, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10808.0048828125
tensor(10808.6445, grad_fn=<NegBackward0>) tensor(10808.0049, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10807.6669921875
tensor(10808.0049, grad_fn=<NegBackward0>) tensor(10807.6670, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10807.533203125
tensor(10807.6670, grad_fn=<NegBackward0>) tensor(10807.5332, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10807.466796875
tensor(10807.5332, grad_fn=<NegBackward0>) tensor(10807.4668, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10807.4267578125
tensor(10807.4668, grad_fn=<NegBackward0>) tensor(10807.4268, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10807.4013671875
tensor(10807.4268, grad_fn=<NegBackward0>) tensor(10807.4014, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10807.3837890625
tensor(10807.4014, grad_fn=<NegBackward0>) tensor(10807.3838, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10807.369140625
tensor(10807.3838, grad_fn=<NegBackward0>) tensor(10807.3691, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10807.3583984375
tensor(10807.3691, grad_fn=<NegBackward0>) tensor(10807.3584, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10807.3515625
tensor(10807.3584, grad_fn=<NegBackward0>) tensor(10807.3516, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10807.345703125
tensor(10807.3516, grad_fn=<NegBackward0>) tensor(10807.3457, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10807.3388671875
tensor(10807.3457, grad_fn=<NegBackward0>) tensor(10807.3389, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10807.3349609375
tensor(10807.3389, grad_fn=<NegBackward0>) tensor(10807.3350, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10807.3310546875
tensor(10807.3350, grad_fn=<NegBackward0>) tensor(10807.3311, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10807.328125
tensor(10807.3311, grad_fn=<NegBackward0>) tensor(10807.3281, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10807.326171875
tensor(10807.3281, grad_fn=<NegBackward0>) tensor(10807.3262, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10807.326171875
tensor(10807.3262, grad_fn=<NegBackward0>) tensor(10807.3262, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10807.3212890625
tensor(10807.3262, grad_fn=<NegBackward0>) tensor(10807.3213, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10807.3203125
tensor(10807.3213, grad_fn=<NegBackward0>) tensor(10807.3203, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10807.318359375
tensor(10807.3203, grad_fn=<NegBackward0>) tensor(10807.3184, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10807.31640625
tensor(10807.3184, grad_fn=<NegBackward0>) tensor(10807.3164, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10807.3154296875
tensor(10807.3164, grad_fn=<NegBackward0>) tensor(10807.3154, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10807.3134765625
tensor(10807.3154, grad_fn=<NegBackward0>) tensor(10807.3135, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10807.3125
tensor(10807.3135, grad_fn=<NegBackward0>) tensor(10807.3125, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10807.3125
tensor(10807.3125, grad_fn=<NegBackward0>) tensor(10807.3125, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10807.3115234375
tensor(10807.3125, grad_fn=<NegBackward0>) tensor(10807.3115, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10807.310546875
tensor(10807.3115, grad_fn=<NegBackward0>) tensor(10807.3105, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10807.310546875
tensor(10807.3105, grad_fn=<NegBackward0>) tensor(10807.3105, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10807.30859375
tensor(10807.3105, grad_fn=<NegBackward0>) tensor(10807.3086, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10807.3076171875
tensor(10807.3086, grad_fn=<NegBackward0>) tensor(10807.3076, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10807.306640625
tensor(10807.3076, grad_fn=<NegBackward0>) tensor(10807.3066, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10807.3056640625
tensor(10807.3066, grad_fn=<NegBackward0>) tensor(10807.3057, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10807.3056640625
tensor(10807.3057, grad_fn=<NegBackward0>) tensor(10807.3057, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10807.3046875
tensor(10807.3057, grad_fn=<NegBackward0>) tensor(10807.3047, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10807.3037109375
tensor(10807.3047, grad_fn=<NegBackward0>) tensor(10807.3037, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10807.3046875
tensor(10807.3037, grad_fn=<NegBackward0>) tensor(10807.3047, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10807.302734375
tensor(10807.3037, grad_fn=<NegBackward0>) tensor(10807.3027, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10807.3037109375
tensor(10807.3027, grad_fn=<NegBackward0>) tensor(10807.3037, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10807.3037109375
tensor(10807.3027, grad_fn=<NegBackward0>) tensor(10807.3037, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -10807.302734375
tensor(10807.3027, grad_fn=<NegBackward0>) tensor(10807.3027, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10807.30078125
tensor(10807.3027, grad_fn=<NegBackward0>) tensor(10807.3008, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10807.3017578125
tensor(10807.3008, grad_fn=<NegBackward0>) tensor(10807.3018, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10807.3017578125
tensor(10807.3008, grad_fn=<NegBackward0>) tensor(10807.3018, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -10807.30078125
tensor(10807.3008, grad_fn=<NegBackward0>) tensor(10807.3008, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10807.3017578125
tensor(10807.3008, grad_fn=<NegBackward0>) tensor(10807.3018, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10807.30078125
tensor(10807.3008, grad_fn=<NegBackward0>) tensor(10807.3008, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10807.2998046875
tensor(10807.3008, grad_fn=<NegBackward0>) tensor(10807.2998, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10807.298828125
tensor(10807.2998, grad_fn=<NegBackward0>) tensor(10807.2988, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10807.2978515625
tensor(10807.2988, grad_fn=<NegBackward0>) tensor(10807.2979, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10807.298828125
tensor(10807.2979, grad_fn=<NegBackward0>) tensor(10807.2988, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10807.298828125
tensor(10807.2979, grad_fn=<NegBackward0>) tensor(10807.2988, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -10807.2998046875
tensor(10807.2979, grad_fn=<NegBackward0>) tensor(10807.2998, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -10807.30078125
tensor(10807.2979, grad_fn=<NegBackward0>) tensor(10807.3008, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -10807.298828125
tensor(10807.2979, grad_fn=<NegBackward0>) tensor(10807.2988, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[6.1482e-01, 3.8518e-01],
        [8.5728e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0422, 0.9578], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3703, 0.2237],
         [0.6990, 0.1567]],

        [[0.6984, 0.2124],
         [0.5109, 0.6568]],

        [[0.5983, 0.0835],
         [0.5971, 0.5663]],

        [[0.6146, 0.2512],
         [0.7122, 0.5971]],

        [[0.6471, 0.1618],
         [0.6100, 0.5415]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
Global Adjusted Rand Index: 2.805622592152369e-06
Average Adjusted Rand Index: -0.00022592098723064015
[2.805622592152369e-06, 2.805622592152369e-06] [-0.00022592098723064015, -0.00022592098723064015] [10807.296875, 10807.298828125]
-------------------------------------
This iteration is 45
True Objective function: Loss = -10883.838414415199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23470.0625
inf tensor(23470.0625, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10934.0986328125
tensor(23470.0625, grad_fn=<NegBackward0>) tensor(10934.0986, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10933.3359375
tensor(10934.0986, grad_fn=<NegBackward0>) tensor(10933.3359, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10933.1689453125
tensor(10933.3359, grad_fn=<NegBackward0>) tensor(10933.1689, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10933.0869140625
tensor(10933.1689, grad_fn=<NegBackward0>) tensor(10933.0869, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10933.0361328125
tensor(10933.0869, grad_fn=<NegBackward0>) tensor(10933.0361, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10933.0009765625
tensor(10933.0361, grad_fn=<NegBackward0>) tensor(10933.0010, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10932.9833984375
tensor(10933.0010, grad_fn=<NegBackward0>) tensor(10932.9834, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10932.96875
tensor(10932.9834, grad_fn=<NegBackward0>) tensor(10932.9688, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10932.9599609375
tensor(10932.9688, grad_fn=<NegBackward0>) tensor(10932.9600, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10932.9521484375
tensor(10932.9600, grad_fn=<NegBackward0>) tensor(10932.9521, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10932.9453125
tensor(10932.9521, grad_fn=<NegBackward0>) tensor(10932.9453, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10932.9404296875
tensor(10932.9453, grad_fn=<NegBackward0>) tensor(10932.9404, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10932.9365234375
tensor(10932.9404, grad_fn=<NegBackward0>) tensor(10932.9365, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10932.9326171875
tensor(10932.9365, grad_fn=<NegBackward0>) tensor(10932.9326, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10932.927734375
tensor(10932.9326, grad_fn=<NegBackward0>) tensor(10932.9277, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10932.9228515625
tensor(10932.9277, grad_fn=<NegBackward0>) tensor(10932.9229, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10932.91796875
tensor(10932.9229, grad_fn=<NegBackward0>) tensor(10932.9180, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10932.9150390625
tensor(10932.9180, grad_fn=<NegBackward0>) tensor(10932.9150, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10932.9111328125
tensor(10932.9150, grad_fn=<NegBackward0>) tensor(10932.9111, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10932.908203125
tensor(10932.9111, grad_fn=<NegBackward0>) tensor(10932.9082, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10932.9033203125
tensor(10932.9082, grad_fn=<NegBackward0>) tensor(10932.9033, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10932.8994140625
tensor(10932.9033, grad_fn=<NegBackward0>) tensor(10932.8994, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10932.8955078125
tensor(10932.8994, grad_fn=<NegBackward0>) tensor(10932.8955, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10932.8916015625
tensor(10932.8955, grad_fn=<NegBackward0>) tensor(10932.8916, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10932.884765625
tensor(10932.8916, grad_fn=<NegBackward0>) tensor(10932.8848, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10932.8798828125
tensor(10932.8848, grad_fn=<NegBackward0>) tensor(10932.8799, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10932.8720703125
tensor(10932.8799, grad_fn=<NegBackward0>) tensor(10932.8721, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10932.8642578125
tensor(10932.8721, grad_fn=<NegBackward0>) tensor(10932.8643, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10932.853515625
tensor(10932.8643, grad_fn=<NegBackward0>) tensor(10932.8535, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10932.837890625
tensor(10932.8535, grad_fn=<NegBackward0>) tensor(10932.8379, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10932.8154296875
tensor(10932.8379, grad_fn=<NegBackward0>) tensor(10932.8154, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10932.78125
tensor(10932.8154, grad_fn=<NegBackward0>) tensor(10932.7812, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10932.728515625
tensor(10932.7812, grad_fn=<NegBackward0>) tensor(10932.7285, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10932.6748046875
tensor(10932.7285, grad_fn=<NegBackward0>) tensor(10932.6748, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10932.625
tensor(10932.6748, grad_fn=<NegBackward0>) tensor(10932.6250, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10932.5859375
tensor(10932.6250, grad_fn=<NegBackward0>) tensor(10932.5859, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10932.5576171875
tensor(10932.5859, grad_fn=<NegBackward0>) tensor(10932.5576, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10932.5380859375
tensor(10932.5576, grad_fn=<NegBackward0>) tensor(10932.5381, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10932.5234375
tensor(10932.5381, grad_fn=<NegBackward0>) tensor(10932.5234, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10932.51171875
tensor(10932.5234, grad_fn=<NegBackward0>) tensor(10932.5117, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10932.5048828125
tensor(10932.5117, grad_fn=<NegBackward0>) tensor(10932.5049, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10932.4990234375
tensor(10932.5049, grad_fn=<NegBackward0>) tensor(10932.4990, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10932.4951171875
tensor(10932.4990, grad_fn=<NegBackward0>) tensor(10932.4951, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10932.4921875
tensor(10932.4951, grad_fn=<NegBackward0>) tensor(10932.4922, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10932.4931640625
tensor(10932.4922, grad_fn=<NegBackward0>) tensor(10932.4932, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10932.4873046875
tensor(10932.4922, grad_fn=<NegBackward0>) tensor(10932.4873, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10932.4853515625
tensor(10932.4873, grad_fn=<NegBackward0>) tensor(10932.4854, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10932.48828125
tensor(10932.4854, grad_fn=<NegBackward0>) tensor(10932.4883, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10932.482421875
tensor(10932.4854, grad_fn=<NegBackward0>) tensor(10932.4824, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10932.482421875
tensor(10932.4824, grad_fn=<NegBackward0>) tensor(10932.4824, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10932.484375
tensor(10932.4824, grad_fn=<NegBackward0>) tensor(10932.4844, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10932.48046875
tensor(10932.4824, grad_fn=<NegBackward0>) tensor(10932.4805, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10932.4814453125
tensor(10932.4805, grad_fn=<NegBackward0>) tensor(10932.4814, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10932.48046875
tensor(10932.4805, grad_fn=<NegBackward0>) tensor(10932.4805, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10932.478515625
tensor(10932.4805, grad_fn=<NegBackward0>) tensor(10932.4785, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10932.4775390625
tensor(10932.4785, grad_fn=<NegBackward0>) tensor(10932.4775, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10932.4775390625
tensor(10932.4775, grad_fn=<NegBackward0>) tensor(10932.4775, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10932.4765625
tensor(10932.4775, grad_fn=<NegBackward0>) tensor(10932.4766, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10932.4765625
tensor(10932.4766, grad_fn=<NegBackward0>) tensor(10932.4766, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10932.4775390625
tensor(10932.4766, grad_fn=<NegBackward0>) tensor(10932.4775, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10932.4755859375
tensor(10932.4766, grad_fn=<NegBackward0>) tensor(10932.4756, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10932.4765625
tensor(10932.4756, grad_fn=<NegBackward0>) tensor(10932.4766, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10932.4755859375
tensor(10932.4756, grad_fn=<NegBackward0>) tensor(10932.4756, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10932.474609375
tensor(10932.4756, grad_fn=<NegBackward0>) tensor(10932.4746, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10932.4755859375
tensor(10932.4746, grad_fn=<NegBackward0>) tensor(10932.4756, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10932.4736328125
tensor(10932.4746, grad_fn=<NegBackward0>) tensor(10932.4736, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10932.474609375
tensor(10932.4736, grad_fn=<NegBackward0>) tensor(10932.4746, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10932.474609375
tensor(10932.4736, grad_fn=<NegBackward0>) tensor(10932.4746, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -10932.4736328125
tensor(10932.4736, grad_fn=<NegBackward0>) tensor(10932.4736, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10932.474609375
tensor(10932.4736, grad_fn=<NegBackward0>) tensor(10932.4746, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10932.474609375
tensor(10932.4736, grad_fn=<NegBackward0>) tensor(10932.4746, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -10932.4755859375
tensor(10932.4736, grad_fn=<NegBackward0>) tensor(10932.4756, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -10932.47265625
tensor(10932.4736, grad_fn=<NegBackward0>) tensor(10932.4727, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10932.4755859375
tensor(10932.4727, grad_fn=<NegBackward0>) tensor(10932.4756, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10932.47265625
tensor(10932.4727, grad_fn=<NegBackward0>) tensor(10932.4727, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10932.4736328125
tensor(10932.4727, grad_fn=<NegBackward0>) tensor(10932.4736, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10932.47265625
tensor(10932.4727, grad_fn=<NegBackward0>) tensor(10932.4727, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10932.4716796875
tensor(10932.4727, grad_fn=<NegBackward0>) tensor(10932.4717, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10932.4736328125
tensor(10932.4717, grad_fn=<NegBackward0>) tensor(10932.4736, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10932.4736328125
tensor(10932.4717, grad_fn=<NegBackward0>) tensor(10932.4736, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -10932.47265625
tensor(10932.4717, grad_fn=<NegBackward0>) tensor(10932.4727, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -10932.4736328125
tensor(10932.4717, grad_fn=<NegBackward0>) tensor(10932.4736, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -10932.47265625
tensor(10932.4717, grad_fn=<NegBackward0>) tensor(10932.4727, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8300 due to no improvement.
pi: tensor([[0.9753, 0.0247],
        [0.9965, 0.0035]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1023, 0.8977], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1611, 0.1636],
         [0.6134, 0.1654]],

        [[0.7257, 0.1715],
         [0.6918, 0.6395]],

        [[0.6756, 0.1668],
         [0.6512, 0.5400]],

        [[0.6270, 0.0972],
         [0.6694, 0.5561]],

        [[0.7023, 0.1512],
         [0.6377, 0.6689]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.002124832869639863
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21617.154296875
inf tensor(21617.1543, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10933.9892578125
tensor(21617.1543, grad_fn=<NegBackward0>) tensor(10933.9893, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10933.4130859375
tensor(10933.9893, grad_fn=<NegBackward0>) tensor(10933.4131, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10933.2138671875
tensor(10933.4131, grad_fn=<NegBackward0>) tensor(10933.2139, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10933.1083984375
tensor(10933.2139, grad_fn=<NegBackward0>) tensor(10933.1084, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10933.01171875
tensor(10933.1084, grad_fn=<NegBackward0>) tensor(10933.0117, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10932.9365234375
tensor(10933.0117, grad_fn=<NegBackward0>) tensor(10932.9365, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10932.9169921875
tensor(10932.9365, grad_fn=<NegBackward0>) tensor(10932.9170, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10932.904296875
tensor(10932.9170, grad_fn=<NegBackward0>) tensor(10932.9043, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10932.892578125
tensor(10932.9043, grad_fn=<NegBackward0>) tensor(10932.8926, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10932.8818359375
tensor(10932.8926, grad_fn=<NegBackward0>) tensor(10932.8818, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10932.8671875
tensor(10932.8818, grad_fn=<NegBackward0>) tensor(10932.8672, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10932.8486328125
tensor(10932.8672, grad_fn=<NegBackward0>) tensor(10932.8486, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10932.8251953125
tensor(10932.8486, grad_fn=<NegBackward0>) tensor(10932.8252, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10932.7919921875
tensor(10932.8252, grad_fn=<NegBackward0>) tensor(10932.7920, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10932.7529296875
tensor(10932.7920, grad_fn=<NegBackward0>) tensor(10932.7529, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10932.7177734375
tensor(10932.7529, grad_fn=<NegBackward0>) tensor(10932.7178, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10932.685546875
tensor(10932.7178, grad_fn=<NegBackward0>) tensor(10932.6855, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10932.6611328125
tensor(10932.6855, grad_fn=<NegBackward0>) tensor(10932.6611, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10932.63671875
tensor(10932.6611, grad_fn=<NegBackward0>) tensor(10932.6367, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10932.615234375
tensor(10932.6367, grad_fn=<NegBackward0>) tensor(10932.6152, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10932.5947265625
tensor(10932.6152, grad_fn=<NegBackward0>) tensor(10932.5947, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10932.576171875
tensor(10932.5947, grad_fn=<NegBackward0>) tensor(10932.5762, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10932.5625
tensor(10932.5762, grad_fn=<NegBackward0>) tensor(10932.5625, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10932.5478515625
tensor(10932.5625, grad_fn=<NegBackward0>) tensor(10932.5479, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10932.5361328125
tensor(10932.5479, grad_fn=<NegBackward0>) tensor(10932.5361, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10932.529296875
tensor(10932.5361, grad_fn=<NegBackward0>) tensor(10932.5293, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10932.521484375
tensor(10932.5293, grad_fn=<NegBackward0>) tensor(10932.5215, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10932.515625
tensor(10932.5215, grad_fn=<NegBackward0>) tensor(10932.5156, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10932.5078125
tensor(10932.5156, grad_fn=<NegBackward0>) tensor(10932.5078, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10932.5029296875
tensor(10932.5078, grad_fn=<NegBackward0>) tensor(10932.5029, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10932.4970703125
tensor(10932.5029, grad_fn=<NegBackward0>) tensor(10932.4971, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10932.4912109375
tensor(10932.4971, grad_fn=<NegBackward0>) tensor(10932.4912, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10932.4794921875
tensor(10932.4912, grad_fn=<NegBackward0>) tensor(10932.4795, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10932.4638671875
tensor(10932.4795, grad_fn=<NegBackward0>) tensor(10932.4639, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10932.4130859375
tensor(10932.4639, grad_fn=<NegBackward0>) tensor(10932.4131, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10932.259765625
tensor(10932.4131, grad_fn=<NegBackward0>) tensor(10932.2598, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10932.1328125
tensor(10932.2598, grad_fn=<NegBackward0>) tensor(10932.1328, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10931.98828125
tensor(10932.1328, grad_fn=<NegBackward0>) tensor(10931.9883, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10931.85546875
tensor(10931.9883, grad_fn=<NegBackward0>) tensor(10931.8555, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10931.77734375
tensor(10931.8555, grad_fn=<NegBackward0>) tensor(10931.7773, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10931.73046875
tensor(10931.7773, grad_fn=<NegBackward0>) tensor(10931.7305, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10931.6982421875
tensor(10931.7305, grad_fn=<NegBackward0>) tensor(10931.6982, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10931.6748046875
tensor(10931.6982, grad_fn=<NegBackward0>) tensor(10931.6748, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10931.65625
tensor(10931.6748, grad_fn=<NegBackward0>) tensor(10931.6562, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10931.6416015625
tensor(10931.6562, grad_fn=<NegBackward0>) tensor(10931.6416, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10931.6298828125
tensor(10931.6416, grad_fn=<NegBackward0>) tensor(10931.6299, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10931.6220703125
tensor(10931.6299, grad_fn=<NegBackward0>) tensor(10931.6221, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10931.615234375
tensor(10931.6221, grad_fn=<NegBackward0>) tensor(10931.6152, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10931.609375
tensor(10931.6152, grad_fn=<NegBackward0>) tensor(10931.6094, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10931.6064453125
tensor(10931.6094, grad_fn=<NegBackward0>) tensor(10931.6064, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10931.6025390625
tensor(10931.6064, grad_fn=<NegBackward0>) tensor(10931.6025, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10931.5986328125
tensor(10931.6025, grad_fn=<NegBackward0>) tensor(10931.5986, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10931.595703125
tensor(10931.5986, grad_fn=<NegBackward0>) tensor(10931.5957, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10931.591796875
tensor(10931.5957, grad_fn=<NegBackward0>) tensor(10931.5918, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10931.58984375
tensor(10931.5918, grad_fn=<NegBackward0>) tensor(10931.5898, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10931.5888671875
tensor(10931.5898, grad_fn=<NegBackward0>) tensor(10931.5889, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10931.5859375
tensor(10931.5889, grad_fn=<NegBackward0>) tensor(10931.5859, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10931.5859375
tensor(10931.5859, grad_fn=<NegBackward0>) tensor(10931.5859, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10931.583984375
tensor(10931.5859, grad_fn=<NegBackward0>) tensor(10931.5840, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10931.58203125
tensor(10931.5840, grad_fn=<NegBackward0>) tensor(10931.5820, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10931.5830078125
tensor(10931.5820, grad_fn=<NegBackward0>) tensor(10931.5830, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10931.5810546875
tensor(10931.5820, grad_fn=<NegBackward0>) tensor(10931.5811, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10931.580078125
tensor(10931.5811, grad_fn=<NegBackward0>) tensor(10931.5801, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10931.578125
tensor(10931.5801, grad_fn=<NegBackward0>) tensor(10931.5781, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10931.578125
tensor(10931.5781, grad_fn=<NegBackward0>) tensor(10931.5781, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10931.578125
tensor(10931.5781, grad_fn=<NegBackward0>) tensor(10931.5781, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10931.5771484375
tensor(10931.5781, grad_fn=<NegBackward0>) tensor(10931.5771, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10931.5751953125
tensor(10931.5771, grad_fn=<NegBackward0>) tensor(10931.5752, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10931.5771484375
tensor(10931.5752, grad_fn=<NegBackward0>) tensor(10931.5771, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10931.576171875
tensor(10931.5752, grad_fn=<NegBackward0>) tensor(10931.5762, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -10931.576171875
tensor(10931.5752, grad_fn=<NegBackward0>) tensor(10931.5762, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -10931.5751953125
tensor(10931.5752, grad_fn=<NegBackward0>) tensor(10931.5752, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10931.5751953125
tensor(10931.5752, grad_fn=<NegBackward0>) tensor(10931.5752, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10931.57421875
tensor(10931.5752, grad_fn=<NegBackward0>) tensor(10931.5742, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10931.5791015625
tensor(10931.5742, grad_fn=<NegBackward0>) tensor(10931.5791, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10931.572265625
tensor(10931.5742, grad_fn=<NegBackward0>) tensor(10931.5723, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10931.5732421875
tensor(10931.5723, grad_fn=<NegBackward0>) tensor(10931.5732, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10931.5712890625
tensor(10931.5723, grad_fn=<NegBackward0>) tensor(10931.5713, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10931.572265625
tensor(10931.5713, grad_fn=<NegBackward0>) tensor(10931.5723, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10931.5732421875
tensor(10931.5713, grad_fn=<NegBackward0>) tensor(10931.5732, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -10931.5712890625
tensor(10931.5713, grad_fn=<NegBackward0>) tensor(10931.5713, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10931.8779296875
tensor(10931.5713, grad_fn=<NegBackward0>) tensor(10931.8779, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10931.5712890625
tensor(10931.5713, grad_fn=<NegBackward0>) tensor(10931.5713, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10931.572265625
tensor(10931.5713, grad_fn=<NegBackward0>) tensor(10931.5723, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -10931.57421875
tensor(10931.5713, grad_fn=<NegBackward0>) tensor(10931.5742, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -10931.5703125
tensor(10931.5713, grad_fn=<NegBackward0>) tensor(10931.5703, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10931.5693359375
tensor(10931.5703, grad_fn=<NegBackward0>) tensor(10931.5693, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10931.576171875
tensor(10931.5693, grad_fn=<NegBackward0>) tensor(10931.5762, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10931.5703125
tensor(10931.5693, grad_fn=<NegBackward0>) tensor(10931.5703, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -10931.5703125
tensor(10931.5693, grad_fn=<NegBackward0>) tensor(10931.5703, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -10931.5703125
tensor(10931.5693, grad_fn=<NegBackward0>) tensor(10931.5703, grad_fn=<NegBackward0>)
4
Iteration 9200: Loss = -10931.5693359375
tensor(10931.5693, grad_fn=<NegBackward0>) tensor(10931.5693, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10931.5693359375
tensor(10931.5693, grad_fn=<NegBackward0>) tensor(10931.5693, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10931.5703125
tensor(10931.5693, grad_fn=<NegBackward0>) tensor(10931.5703, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -10931.568359375
tensor(10931.5693, grad_fn=<NegBackward0>) tensor(10931.5684, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10931.6142578125
tensor(10931.5684, grad_fn=<NegBackward0>) tensor(10931.6143, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -10931.5693359375
tensor(10931.5684, grad_fn=<NegBackward0>) tensor(10931.5693, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -10931.5673828125
tensor(10931.5684, grad_fn=<NegBackward0>) tensor(10931.5674, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10931.58203125
tensor(10931.5674, grad_fn=<NegBackward0>) tensor(10931.5820, grad_fn=<NegBackward0>)
1
pi: tensor([[6.7243e-01, 3.2757e-01],
        [1.3894e-05, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0770, 0.9230], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2528, 0.1842],
         [0.6061, 0.1591]],

        [[0.5432, 0.1757],
         [0.5271, 0.6928]],

        [[0.5247, 0.1817],
         [0.6684, 0.5891]],

        [[0.5982, 0.2522],
         [0.5634, 0.7147]],

        [[0.5561, 0.1293],
         [0.7088, 0.5437]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: 0.005066695522858484
Average Adjusted Rand Index: 0.0044688685293292904
[-0.002124832869639863, 0.005066695522858484] [0.0, 0.0044688685293292904] [10932.47265625, 10931.5693359375]
-------------------------------------
This iteration is 46
True Objective function: Loss = -10945.562478228168
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23192.236328125
inf tensor(23192.2363, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11040.0263671875
tensor(23192.2363, grad_fn=<NegBackward0>) tensor(11040.0264, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11039.2646484375
tensor(11040.0264, grad_fn=<NegBackward0>) tensor(11039.2646, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11038.67578125
tensor(11039.2646, grad_fn=<NegBackward0>) tensor(11038.6758, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11036.232421875
tensor(11038.6758, grad_fn=<NegBackward0>) tensor(11036.2324, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11035.4921875
tensor(11036.2324, grad_fn=<NegBackward0>) tensor(11035.4922, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11035.0576171875
tensor(11035.4922, grad_fn=<NegBackward0>) tensor(11035.0576, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11034.666015625
tensor(11035.0576, grad_fn=<NegBackward0>) tensor(11034.6660, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11034.1689453125
tensor(11034.6660, grad_fn=<NegBackward0>) tensor(11034.1689, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11033.2724609375
tensor(11034.1689, grad_fn=<NegBackward0>) tensor(11033.2725, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10935.6962890625
tensor(11033.2725, grad_fn=<NegBackward0>) tensor(10935.6963, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10930.8779296875
tensor(10935.6963, grad_fn=<NegBackward0>) tensor(10930.8779, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10930.8115234375
tensor(10930.8779, grad_fn=<NegBackward0>) tensor(10930.8115, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10930.7724609375
tensor(10930.8115, grad_fn=<NegBackward0>) tensor(10930.7725, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10930.7529296875
tensor(10930.7725, grad_fn=<NegBackward0>) tensor(10930.7529, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10930.7431640625
tensor(10930.7529, grad_fn=<NegBackward0>) tensor(10930.7432, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10930.736328125
tensor(10930.7432, grad_fn=<NegBackward0>) tensor(10930.7363, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10930.724609375
tensor(10930.7363, grad_fn=<NegBackward0>) tensor(10930.7246, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10930.703125
tensor(10930.7246, grad_fn=<NegBackward0>) tensor(10930.7031, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10930.6845703125
tensor(10930.7031, grad_fn=<NegBackward0>) tensor(10930.6846, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10930.576171875
tensor(10930.6846, grad_fn=<NegBackward0>) tensor(10930.5762, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10924.8662109375
tensor(10930.5762, grad_fn=<NegBackward0>) tensor(10924.8662, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10924.6982421875
tensor(10924.8662, grad_fn=<NegBackward0>) tensor(10924.6982, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10924.6953125
tensor(10924.6982, grad_fn=<NegBackward0>) tensor(10924.6953, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10924.693359375
tensor(10924.6953, grad_fn=<NegBackward0>) tensor(10924.6934, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10918.6171875
tensor(10924.6934, grad_fn=<NegBackward0>) tensor(10918.6172, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10907.0341796875
tensor(10918.6172, grad_fn=<NegBackward0>) tensor(10907.0342, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10906.9794921875
tensor(10907.0342, grad_fn=<NegBackward0>) tensor(10906.9795, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10906.978515625
tensor(10906.9795, grad_fn=<NegBackward0>) tensor(10906.9785, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10906.9765625
tensor(10906.9785, grad_fn=<NegBackward0>) tensor(10906.9766, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10906.9775390625
tensor(10906.9766, grad_fn=<NegBackward0>) tensor(10906.9775, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -10906.9755859375
tensor(10906.9766, grad_fn=<NegBackward0>) tensor(10906.9756, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10906.9775390625
tensor(10906.9756, grad_fn=<NegBackward0>) tensor(10906.9775, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -10906.9765625
tensor(10906.9756, grad_fn=<NegBackward0>) tensor(10906.9766, grad_fn=<NegBackward0>)
2
Iteration 3400: Loss = -10906.9765625
tensor(10906.9756, grad_fn=<NegBackward0>) tensor(10906.9766, grad_fn=<NegBackward0>)
3
Iteration 3500: Loss = -10906.9755859375
tensor(10906.9756, grad_fn=<NegBackward0>) tensor(10906.9756, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10906.9765625
tensor(10906.9756, grad_fn=<NegBackward0>) tensor(10906.9766, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -10906.9765625
tensor(10906.9756, grad_fn=<NegBackward0>) tensor(10906.9766, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -10906.9755859375
tensor(10906.9756, grad_fn=<NegBackward0>) tensor(10906.9756, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10906.9755859375
tensor(10906.9756, grad_fn=<NegBackward0>) tensor(10906.9756, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10906.9755859375
tensor(10906.9756, grad_fn=<NegBackward0>) tensor(10906.9756, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10906.9755859375
tensor(10906.9756, grad_fn=<NegBackward0>) tensor(10906.9756, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10906.974609375
tensor(10906.9756, grad_fn=<NegBackward0>) tensor(10906.9746, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10906.9736328125
tensor(10906.9746, grad_fn=<NegBackward0>) tensor(10906.9736, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10906.96875
tensor(10906.9736, grad_fn=<NegBackward0>) tensor(10906.9688, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10906.9697265625
tensor(10906.9688, grad_fn=<NegBackward0>) tensor(10906.9697, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10906.9697265625
tensor(10906.9688, grad_fn=<NegBackward0>) tensor(10906.9697, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -10906.9697265625
tensor(10906.9688, grad_fn=<NegBackward0>) tensor(10906.9697, grad_fn=<NegBackward0>)
3
Iteration 4800: Loss = -10906.96875
tensor(10906.9688, grad_fn=<NegBackward0>) tensor(10906.9688, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10906.9677734375
tensor(10906.9688, grad_fn=<NegBackward0>) tensor(10906.9678, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10906.9697265625
tensor(10906.9678, grad_fn=<NegBackward0>) tensor(10906.9697, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10906.96875
tensor(10906.9678, grad_fn=<NegBackward0>) tensor(10906.9688, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -10906.9677734375
tensor(10906.9678, grad_fn=<NegBackward0>) tensor(10906.9678, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10906.9697265625
tensor(10906.9678, grad_fn=<NegBackward0>) tensor(10906.9697, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10906.96875
tensor(10906.9678, grad_fn=<NegBackward0>) tensor(10906.9688, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -10906.96875
tensor(10906.9678, grad_fn=<NegBackward0>) tensor(10906.9688, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -10906.9736328125
tensor(10906.9678, grad_fn=<NegBackward0>) tensor(10906.9736, grad_fn=<NegBackward0>)
4
Iteration 5700: Loss = -10906.9677734375
tensor(10906.9678, grad_fn=<NegBackward0>) tensor(10906.9678, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10906.9677734375
tensor(10906.9678, grad_fn=<NegBackward0>) tensor(10906.9678, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10906.96875
tensor(10906.9678, grad_fn=<NegBackward0>) tensor(10906.9688, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10906.96875
tensor(10906.9678, grad_fn=<NegBackward0>) tensor(10906.9688, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -10906.9677734375
tensor(10906.9678, grad_fn=<NegBackward0>) tensor(10906.9678, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10906.9677734375
tensor(10906.9678, grad_fn=<NegBackward0>) tensor(10906.9678, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10906.9677734375
tensor(10906.9678, grad_fn=<NegBackward0>) tensor(10906.9678, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10906.96875
tensor(10906.9678, grad_fn=<NegBackward0>) tensor(10906.9688, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10906.96875
tensor(10906.9678, grad_fn=<NegBackward0>) tensor(10906.9688, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -10906.96875
tensor(10906.9678, grad_fn=<NegBackward0>) tensor(10906.9688, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -10906.96875
tensor(10906.9678, grad_fn=<NegBackward0>) tensor(10906.9688, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -10906.96875
tensor(10906.9678, grad_fn=<NegBackward0>) tensor(10906.9688, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6800 due to no improvement.
pi: tensor([[0.8338, 0.1662],
        [0.3022, 0.6978]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3975, 0.6025], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2427, 0.1064],
         [0.5616, 0.1978]],

        [[0.7244, 0.0926],
         [0.5166, 0.6458]],

        [[0.7239, 0.1022],
         [0.5326, 0.5214]],

        [[0.5867, 0.1066],
         [0.6915, 0.6185]],

        [[0.6509, 0.0938],
         [0.5849, 0.6428]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9206925302859384
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 3
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8443214938060299
time is 4
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8073834974290766
Global Adjusted Rand Index: 0.8758472445256282
Average Adjusted Rand Index: 0.8751252067121303
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20035.294921875
inf tensor(20035.2949, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11039.9482421875
tensor(20035.2949, grad_fn=<NegBackward0>) tensor(11039.9482, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11039.0244140625
tensor(11039.9482, grad_fn=<NegBackward0>) tensor(11039.0244, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11037.8681640625
tensor(11039.0244, grad_fn=<NegBackward0>) tensor(11037.8682, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11037.0537109375
tensor(11037.8682, grad_fn=<NegBackward0>) tensor(11037.0537, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11036.515625
tensor(11037.0537, grad_fn=<NegBackward0>) tensor(11036.5156, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11035.9248046875
tensor(11036.5156, grad_fn=<NegBackward0>) tensor(11035.9248, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11035.3076171875
tensor(11035.9248, grad_fn=<NegBackward0>) tensor(11035.3076, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11034.8017578125
tensor(11035.3076, grad_fn=<NegBackward0>) tensor(11034.8018, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11034.3720703125
tensor(11034.8018, grad_fn=<NegBackward0>) tensor(11034.3721, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11033.9111328125
tensor(11034.3721, grad_fn=<NegBackward0>) tensor(11033.9111, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11033.4931640625
tensor(11033.9111, grad_fn=<NegBackward0>) tensor(11033.4932, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11032.953125
tensor(11033.4932, grad_fn=<NegBackward0>) tensor(11032.9531, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10935.9775390625
tensor(11032.9531, grad_fn=<NegBackward0>) tensor(10935.9775, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10932.162109375
tensor(10935.9775, grad_fn=<NegBackward0>) tensor(10932.1621, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10932.0146484375
tensor(10932.1621, grad_fn=<NegBackward0>) tensor(10932.0146, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10931.7490234375
tensor(10932.0146, grad_fn=<NegBackward0>) tensor(10931.7490, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10930.861328125
tensor(10931.7490, grad_fn=<NegBackward0>) tensor(10930.8613, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10930.8388671875
tensor(10930.8613, grad_fn=<NegBackward0>) tensor(10930.8389, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10930.8251953125
tensor(10930.8389, grad_fn=<NegBackward0>) tensor(10930.8252, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10930.814453125
tensor(10930.8252, grad_fn=<NegBackward0>) tensor(10930.8145, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10930.8037109375
tensor(10930.8145, grad_fn=<NegBackward0>) tensor(10930.8037, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10930.7958984375
tensor(10930.8037, grad_fn=<NegBackward0>) tensor(10930.7959, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10930.791015625
tensor(10930.7959, grad_fn=<NegBackward0>) tensor(10930.7910, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10930.787109375
tensor(10930.7910, grad_fn=<NegBackward0>) tensor(10930.7871, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10930.7841796875
tensor(10930.7871, grad_fn=<NegBackward0>) tensor(10930.7842, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10930.7822265625
tensor(10930.7842, grad_fn=<NegBackward0>) tensor(10930.7822, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10930.7783203125
tensor(10930.7822, grad_fn=<NegBackward0>) tensor(10930.7783, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10930.77734375
tensor(10930.7783, grad_fn=<NegBackward0>) tensor(10930.7773, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10930.775390625
tensor(10930.7773, grad_fn=<NegBackward0>) tensor(10930.7754, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10930.771484375
tensor(10930.7754, grad_fn=<NegBackward0>) tensor(10930.7715, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10930.7666015625
tensor(10930.7715, grad_fn=<NegBackward0>) tensor(10930.7666, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10930.7646484375
tensor(10930.7666, grad_fn=<NegBackward0>) tensor(10930.7646, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10930.76171875
tensor(10930.7646, grad_fn=<NegBackward0>) tensor(10930.7617, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10930.7578125
tensor(10930.7617, grad_fn=<NegBackward0>) tensor(10930.7578, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10930.755859375
tensor(10930.7578, grad_fn=<NegBackward0>) tensor(10930.7559, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10930.75390625
tensor(10930.7559, grad_fn=<NegBackward0>) tensor(10930.7539, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10930.75390625
tensor(10930.7539, grad_fn=<NegBackward0>) tensor(10930.7539, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10930.7529296875
tensor(10930.7539, grad_fn=<NegBackward0>) tensor(10930.7529, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10930.751953125
tensor(10930.7529, grad_fn=<NegBackward0>) tensor(10930.7520, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10930.75
tensor(10930.7520, grad_fn=<NegBackward0>) tensor(10930.7500, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10930.7490234375
tensor(10930.7500, grad_fn=<NegBackward0>) tensor(10930.7490, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10930.7470703125
tensor(10930.7490, grad_fn=<NegBackward0>) tensor(10930.7471, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10930.74609375
tensor(10930.7471, grad_fn=<NegBackward0>) tensor(10930.7461, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10930.7470703125
tensor(10930.7461, grad_fn=<NegBackward0>) tensor(10930.7471, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10930.74609375
tensor(10930.7461, grad_fn=<NegBackward0>) tensor(10930.7461, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10930.74609375
tensor(10930.7461, grad_fn=<NegBackward0>) tensor(10930.7461, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10930.7470703125
tensor(10930.7461, grad_fn=<NegBackward0>) tensor(10930.7471, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10930.7099609375
tensor(10930.7461, grad_fn=<NegBackward0>) tensor(10930.7100, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10930.6982421875
tensor(10930.7100, grad_fn=<NegBackward0>) tensor(10930.6982, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10930.6953125
tensor(10930.6982, grad_fn=<NegBackward0>) tensor(10930.6953, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10930.6953125
tensor(10930.6953, grad_fn=<NegBackward0>) tensor(10930.6953, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10930.6943359375
tensor(10930.6953, grad_fn=<NegBackward0>) tensor(10930.6943, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10930.6943359375
tensor(10930.6943, grad_fn=<NegBackward0>) tensor(10930.6943, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10930.6943359375
tensor(10930.6943, grad_fn=<NegBackward0>) tensor(10930.6943, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10930.6943359375
tensor(10930.6943, grad_fn=<NegBackward0>) tensor(10930.6943, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10930.693359375
tensor(10930.6943, grad_fn=<NegBackward0>) tensor(10930.6934, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10930.693359375
tensor(10930.6934, grad_fn=<NegBackward0>) tensor(10930.6934, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10930.693359375
tensor(10930.6934, grad_fn=<NegBackward0>) tensor(10930.6934, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10930.693359375
tensor(10930.6934, grad_fn=<NegBackward0>) tensor(10930.6934, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10930.693359375
tensor(10930.6934, grad_fn=<NegBackward0>) tensor(10930.6934, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10930.6923828125
tensor(10930.6934, grad_fn=<NegBackward0>) tensor(10930.6924, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10930.6923828125
tensor(10930.6924, grad_fn=<NegBackward0>) tensor(10930.6924, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10930.6923828125
tensor(10930.6924, grad_fn=<NegBackward0>) tensor(10930.6924, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10930.6923828125
tensor(10930.6924, grad_fn=<NegBackward0>) tensor(10930.6924, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10930.693359375
tensor(10930.6924, grad_fn=<NegBackward0>) tensor(10930.6934, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10930.6923828125
tensor(10930.6924, grad_fn=<NegBackward0>) tensor(10930.6924, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10930.6923828125
tensor(10930.6924, grad_fn=<NegBackward0>) tensor(10930.6924, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10930.6923828125
tensor(10930.6924, grad_fn=<NegBackward0>) tensor(10930.6924, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10930.693359375
tensor(10930.6924, grad_fn=<NegBackward0>) tensor(10930.6934, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10930.69140625
tensor(10930.6924, grad_fn=<NegBackward0>) tensor(10930.6914, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10930.69140625
tensor(10930.6914, grad_fn=<NegBackward0>) tensor(10930.6914, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10930.6923828125
tensor(10930.6914, grad_fn=<NegBackward0>) tensor(10930.6924, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10930.6923828125
tensor(10930.6914, grad_fn=<NegBackward0>) tensor(10930.6924, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -10930.7265625
tensor(10930.6914, grad_fn=<NegBackward0>) tensor(10930.7266, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -10930.6923828125
tensor(10930.6914, grad_fn=<NegBackward0>) tensor(10930.6924, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -10930.6982421875
tensor(10930.6914, grad_fn=<NegBackward0>) tensor(10930.6982, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7600 due to no improvement.
pi: tensor([[0.8244, 0.1756],
        [0.3754, 0.6246]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([4.3446e-06, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2493, 0.1543],
         [0.5878, 0.1744]],

        [[0.5930, 0.0942],
         [0.5195, 0.6515]],

        [[0.5517, 0.1032],
         [0.5242, 0.5947]],

        [[0.6001, 0.1075],
         [0.5497, 0.5667]],

        [[0.7000, 0.0963],
         [0.6705, 0.6162]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 40
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448509923071951
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 3
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 7
Adjusted Rand Index: 0.7365051330168892
time is 4
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8821083056742698
Global Adjusted Rand Index: 0.6013739636223492
Average Adjusted Rand Index: 0.6768542929253428
[0.8758472445256282, 0.6013739636223492] [0.8751252067121303, 0.6768542929253428] [10906.96875, 10930.6982421875]
-------------------------------------
This iteration is 47
True Objective function: Loss = -11032.375729028518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22820.275390625
inf tensor(22820.2754, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11183.9443359375
tensor(22820.2754, grad_fn=<NegBackward0>) tensor(11183.9443, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11183.220703125
tensor(11183.9443, grad_fn=<NegBackward0>) tensor(11183.2207, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11182.7138671875
tensor(11183.2207, grad_fn=<NegBackward0>) tensor(11182.7139, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11178.73828125
tensor(11182.7139, grad_fn=<NegBackward0>) tensor(11178.7383, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11178.2978515625
tensor(11178.7383, grad_fn=<NegBackward0>) tensor(11178.2979, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11178.01171875
tensor(11178.2979, grad_fn=<NegBackward0>) tensor(11178.0117, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11177.263671875
tensor(11178.0117, grad_fn=<NegBackward0>) tensor(11177.2637, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11176.00390625
tensor(11177.2637, grad_fn=<NegBackward0>) tensor(11176.0039, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11164.923828125
tensor(11176.0039, grad_fn=<NegBackward0>) tensor(11164.9238, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11088.513671875
tensor(11164.9238, grad_fn=<NegBackward0>) tensor(11088.5137, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11057.818359375
tensor(11088.5137, grad_fn=<NegBackward0>) tensor(11057.8184, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11047.7509765625
tensor(11057.8184, grad_fn=<NegBackward0>) tensor(11047.7510, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11042.560546875
tensor(11047.7510, grad_fn=<NegBackward0>) tensor(11042.5605, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11042.2529296875
tensor(11042.5605, grad_fn=<NegBackward0>) tensor(11042.2529, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11042.1064453125
tensor(11042.2529, grad_fn=<NegBackward0>) tensor(11042.1064, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11041.775390625
tensor(11042.1064, grad_fn=<NegBackward0>) tensor(11041.7754, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11040.32421875
tensor(11041.7754, grad_fn=<NegBackward0>) tensor(11040.3242, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11040.2890625
tensor(11040.3242, grad_fn=<NegBackward0>) tensor(11040.2891, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11039.5146484375
tensor(11040.2891, grad_fn=<NegBackward0>) tensor(11039.5146, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11039.49609375
tensor(11039.5146, grad_fn=<NegBackward0>) tensor(11039.4961, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11039.478515625
tensor(11039.4961, grad_fn=<NegBackward0>) tensor(11039.4785, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11039.4638671875
tensor(11039.4785, grad_fn=<NegBackward0>) tensor(11039.4639, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11039.45703125
tensor(11039.4639, grad_fn=<NegBackward0>) tensor(11039.4570, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11039.447265625
tensor(11039.4570, grad_fn=<NegBackward0>) tensor(11039.4473, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11039.435546875
tensor(11039.4473, grad_fn=<NegBackward0>) tensor(11039.4355, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11039.4208984375
tensor(11039.4355, grad_fn=<NegBackward0>) tensor(11039.4209, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11039.404296875
tensor(11039.4209, grad_fn=<NegBackward0>) tensor(11039.4043, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11039.400390625
tensor(11039.4043, grad_fn=<NegBackward0>) tensor(11039.4004, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11039.3974609375
tensor(11039.4004, grad_fn=<NegBackward0>) tensor(11039.3975, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11039.39453125
tensor(11039.3975, grad_fn=<NegBackward0>) tensor(11039.3945, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11039.3916015625
tensor(11039.3945, grad_fn=<NegBackward0>) tensor(11039.3916, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11039.388671875
tensor(11039.3916, grad_fn=<NegBackward0>) tensor(11039.3887, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11039.3876953125
tensor(11039.3887, grad_fn=<NegBackward0>) tensor(11039.3877, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11039.3828125
tensor(11039.3877, grad_fn=<NegBackward0>) tensor(11039.3828, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11039.3515625
tensor(11039.3828, grad_fn=<NegBackward0>) tensor(11039.3516, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11039.3408203125
tensor(11039.3516, grad_fn=<NegBackward0>) tensor(11039.3408, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11039.330078125
tensor(11039.3408, grad_fn=<NegBackward0>) tensor(11039.3301, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11039.3203125
tensor(11039.3301, grad_fn=<NegBackward0>) tensor(11039.3203, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11039.3173828125
tensor(11039.3203, grad_fn=<NegBackward0>) tensor(11039.3174, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11039.3125
tensor(11039.3174, grad_fn=<NegBackward0>) tensor(11039.3125, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11039.310546875
tensor(11039.3125, grad_fn=<NegBackward0>) tensor(11039.3105, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11039.3095703125
tensor(11039.3105, grad_fn=<NegBackward0>) tensor(11039.3096, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11039.30859375
tensor(11039.3096, grad_fn=<NegBackward0>) tensor(11039.3086, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11039.30859375
tensor(11039.3086, grad_fn=<NegBackward0>) tensor(11039.3086, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11039.306640625
tensor(11039.3086, grad_fn=<NegBackward0>) tensor(11039.3066, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11039.3076171875
tensor(11039.3066, grad_fn=<NegBackward0>) tensor(11039.3076, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11039.306640625
tensor(11039.3066, grad_fn=<NegBackward0>) tensor(11039.3066, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11039.3076171875
tensor(11039.3066, grad_fn=<NegBackward0>) tensor(11039.3076, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11039.3056640625
tensor(11039.3066, grad_fn=<NegBackward0>) tensor(11039.3057, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11039.3046875
tensor(11039.3057, grad_fn=<NegBackward0>) tensor(11039.3047, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11039.3046875
tensor(11039.3047, grad_fn=<NegBackward0>) tensor(11039.3047, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11039.3046875
tensor(11039.3047, grad_fn=<NegBackward0>) tensor(11039.3047, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11039.3037109375
tensor(11039.3047, grad_fn=<NegBackward0>) tensor(11039.3037, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11039.3037109375
tensor(11039.3037, grad_fn=<NegBackward0>) tensor(11039.3037, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11039.302734375
tensor(11039.3037, grad_fn=<NegBackward0>) tensor(11039.3027, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11039.2939453125
tensor(11039.3027, grad_fn=<NegBackward0>) tensor(11039.2939, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11039.294921875
tensor(11039.2939, grad_fn=<NegBackward0>) tensor(11039.2949, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11039.29296875
tensor(11039.2939, grad_fn=<NegBackward0>) tensor(11039.2930, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11039.291015625
tensor(11039.2930, grad_fn=<NegBackward0>) tensor(11039.2910, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11039.28515625
tensor(11039.2910, grad_fn=<NegBackward0>) tensor(11039.2852, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11039.287109375
tensor(11039.2852, grad_fn=<NegBackward0>) tensor(11039.2871, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11039.28515625
tensor(11039.2852, grad_fn=<NegBackward0>) tensor(11039.2852, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11039.28515625
tensor(11039.2852, grad_fn=<NegBackward0>) tensor(11039.2852, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11039.28515625
tensor(11039.2852, grad_fn=<NegBackward0>) tensor(11039.2852, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11039.2841796875
tensor(11039.2852, grad_fn=<NegBackward0>) tensor(11039.2842, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11039.2841796875
tensor(11039.2842, grad_fn=<NegBackward0>) tensor(11039.2842, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11039.2841796875
tensor(11039.2842, grad_fn=<NegBackward0>) tensor(11039.2842, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11039.2841796875
tensor(11039.2842, grad_fn=<NegBackward0>) tensor(11039.2842, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11039.2841796875
tensor(11039.2842, grad_fn=<NegBackward0>) tensor(11039.2842, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11039.2841796875
tensor(11039.2842, grad_fn=<NegBackward0>) tensor(11039.2842, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11039.283203125
tensor(11039.2842, grad_fn=<NegBackward0>) tensor(11039.2832, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11039.2890625
tensor(11039.2832, grad_fn=<NegBackward0>) tensor(11039.2891, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11039.283203125
tensor(11039.2832, grad_fn=<NegBackward0>) tensor(11039.2832, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11039.283203125
tensor(11039.2832, grad_fn=<NegBackward0>) tensor(11039.2832, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11039.283203125
tensor(11039.2832, grad_fn=<NegBackward0>) tensor(11039.2832, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11039.283203125
tensor(11039.2832, grad_fn=<NegBackward0>) tensor(11039.2832, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11039.283203125
tensor(11039.2832, grad_fn=<NegBackward0>) tensor(11039.2832, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11039.283203125
tensor(11039.2832, grad_fn=<NegBackward0>) tensor(11039.2832, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11039.3173828125
tensor(11039.2832, grad_fn=<NegBackward0>) tensor(11039.3174, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11039.2939453125
tensor(11039.2832, grad_fn=<NegBackward0>) tensor(11039.2939, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11039.283203125
tensor(11039.2832, grad_fn=<NegBackward0>) tensor(11039.2832, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11039.2822265625
tensor(11039.2832, grad_fn=<NegBackward0>) tensor(11039.2822, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11039.2822265625
tensor(11039.2822, grad_fn=<NegBackward0>) tensor(11039.2822, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11039.28515625
tensor(11039.2822, grad_fn=<NegBackward0>) tensor(11039.2852, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11039.28125
tensor(11039.2822, grad_fn=<NegBackward0>) tensor(11039.2812, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11039.2822265625
tensor(11039.2812, grad_fn=<NegBackward0>) tensor(11039.2822, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11039.2822265625
tensor(11039.2812, grad_fn=<NegBackward0>) tensor(11039.2822, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11039.28125
tensor(11039.2812, grad_fn=<NegBackward0>) tensor(11039.2812, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11039.28125
tensor(11039.2812, grad_fn=<NegBackward0>) tensor(11039.2812, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11039.2822265625
tensor(11039.2812, grad_fn=<NegBackward0>) tensor(11039.2822, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11039.2802734375
tensor(11039.2812, grad_fn=<NegBackward0>) tensor(11039.2803, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11039.40234375
tensor(11039.2803, grad_fn=<NegBackward0>) tensor(11039.4023, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11039.28125
tensor(11039.2803, grad_fn=<NegBackward0>) tensor(11039.2812, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11039.2822265625
tensor(11039.2803, grad_fn=<NegBackward0>) tensor(11039.2822, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -11039.2802734375
tensor(11039.2803, grad_fn=<NegBackward0>) tensor(11039.2803, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11039.2802734375
tensor(11039.2803, grad_fn=<NegBackward0>) tensor(11039.2803, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11039.2802734375
tensor(11039.2803, grad_fn=<NegBackward0>) tensor(11039.2803, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11039.2802734375
tensor(11039.2803, grad_fn=<NegBackward0>) tensor(11039.2803, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11039.2822265625
tensor(11039.2803, grad_fn=<NegBackward0>) tensor(11039.2822, grad_fn=<NegBackward0>)
1
pi: tensor([[0.6483, 0.3517],
        [0.2486, 0.7514]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9994e-01, 5.7093e-05], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1775, 0.1568],
         [0.5514, 0.2764]],

        [[0.6021, 0.1166],
         [0.6812, 0.6502]],

        [[0.5772, 0.1021],
         [0.6713, 0.6585]],

        [[0.6207, 0.0918],
         [0.7169, 0.5126]],

        [[0.5298, 0.1019],
         [0.5641, 0.6782]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 91
Adjusted Rand Index: 0.6690636805379029
time is 2
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080650161849864
Global Adjusted Rand Index: 0.5059270088681651
Average Adjusted Rand Index: 0.6639101489929893
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26162.5
inf tensor(26162.5000, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11181.9765625
tensor(26162.5000, grad_fn=<NegBackward0>) tensor(11181.9766, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11180.7958984375
tensor(11181.9766, grad_fn=<NegBackward0>) tensor(11180.7959, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11180.2080078125
tensor(11180.7959, grad_fn=<NegBackward0>) tensor(11180.2080, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11179.275390625
tensor(11180.2080, grad_fn=<NegBackward0>) tensor(11179.2754, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11177.90625
tensor(11179.2754, grad_fn=<NegBackward0>) tensor(11177.9062, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11177.0849609375
tensor(11177.9062, grad_fn=<NegBackward0>) tensor(11177.0850, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11176.216796875
tensor(11177.0850, grad_fn=<NegBackward0>) tensor(11176.2168, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11168.490234375
tensor(11176.2168, grad_fn=<NegBackward0>) tensor(11168.4902, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11161.23046875
tensor(11168.4902, grad_fn=<NegBackward0>) tensor(11161.2305, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11087.1142578125
tensor(11161.2305, grad_fn=<NegBackward0>) tensor(11087.1143, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11073.1171875
tensor(11087.1143, grad_fn=<NegBackward0>) tensor(11073.1172, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11072.615234375
tensor(11073.1172, grad_fn=<NegBackward0>) tensor(11072.6152, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11066.6162109375
tensor(11072.6152, grad_fn=<NegBackward0>) tensor(11066.6162, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11063.2490234375
tensor(11066.6162, grad_fn=<NegBackward0>) tensor(11063.2490, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11056.125
tensor(11063.2490, grad_fn=<NegBackward0>) tensor(11056.1250, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11055.1142578125
tensor(11056.1250, grad_fn=<NegBackward0>) tensor(11055.1143, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11055.0810546875
tensor(11055.1143, grad_fn=<NegBackward0>) tensor(11055.0811, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11055.03125
tensor(11055.0811, grad_fn=<NegBackward0>) tensor(11055.0312, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11055.0244140625
tensor(11055.0312, grad_fn=<NegBackward0>) tensor(11055.0244, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11055.015625
tensor(11055.0244, grad_fn=<NegBackward0>) tensor(11055.0156, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11054.169921875
tensor(11055.0156, grad_fn=<NegBackward0>) tensor(11054.1699, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11052.892578125
tensor(11054.1699, grad_fn=<NegBackward0>) tensor(11052.8926, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11052.890625
tensor(11052.8926, grad_fn=<NegBackward0>) tensor(11052.8906, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11052.888671875
tensor(11052.8906, grad_fn=<NegBackward0>) tensor(11052.8887, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11052.8828125
tensor(11052.8887, grad_fn=<NegBackward0>) tensor(11052.8828, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11052.84375
tensor(11052.8828, grad_fn=<NegBackward0>) tensor(11052.8438, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11052.822265625
tensor(11052.8438, grad_fn=<NegBackward0>) tensor(11052.8223, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11052.439453125
tensor(11052.8223, grad_fn=<NegBackward0>) tensor(11052.4395, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11052.4375
tensor(11052.4395, grad_fn=<NegBackward0>) tensor(11052.4375, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11052.4326171875
tensor(11052.4375, grad_fn=<NegBackward0>) tensor(11052.4326, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11051.029296875
tensor(11052.4326, grad_fn=<NegBackward0>) tensor(11051.0293, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11050.61328125
tensor(11051.0293, grad_fn=<NegBackward0>) tensor(11050.6133, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11048.8232421875
tensor(11050.6133, grad_fn=<NegBackward0>) tensor(11048.8232, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11048.7900390625
tensor(11048.8232, grad_fn=<NegBackward0>) tensor(11048.7900, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11048.779296875
tensor(11048.7900, grad_fn=<NegBackward0>) tensor(11048.7793, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11048.50390625
tensor(11048.7793, grad_fn=<NegBackward0>) tensor(11048.5039, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11048.4697265625
tensor(11048.5039, grad_fn=<NegBackward0>) tensor(11048.4697, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11048.4375
tensor(11048.4697, grad_fn=<NegBackward0>) tensor(11048.4375, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11048.4345703125
tensor(11048.4375, grad_fn=<NegBackward0>) tensor(11048.4346, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11048.4326171875
tensor(11048.4346, grad_fn=<NegBackward0>) tensor(11048.4326, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11048.4287109375
tensor(11048.4326, grad_fn=<NegBackward0>) tensor(11048.4287, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11048.34765625
tensor(11048.4287, grad_fn=<NegBackward0>) tensor(11048.3477, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11048.3447265625
tensor(11048.3477, grad_fn=<NegBackward0>) tensor(11048.3447, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11048.33984375
tensor(11048.3447, grad_fn=<NegBackward0>) tensor(11048.3398, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11048.33984375
tensor(11048.3398, grad_fn=<NegBackward0>) tensor(11048.3398, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11048.3408203125
tensor(11048.3398, grad_fn=<NegBackward0>) tensor(11048.3408, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11048.33984375
tensor(11048.3398, grad_fn=<NegBackward0>) tensor(11048.3398, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11048.3408203125
tensor(11048.3398, grad_fn=<NegBackward0>) tensor(11048.3408, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11048.3408203125
tensor(11048.3398, grad_fn=<NegBackward0>) tensor(11048.3408, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11048.3427734375
tensor(11048.3398, grad_fn=<NegBackward0>) tensor(11048.3428, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -11048.287109375
tensor(11048.3398, grad_fn=<NegBackward0>) tensor(11048.2871, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11048.287109375
tensor(11048.2871, grad_fn=<NegBackward0>) tensor(11048.2871, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11048.27734375
tensor(11048.2871, grad_fn=<NegBackward0>) tensor(11048.2773, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11048.2646484375
tensor(11048.2773, grad_fn=<NegBackward0>) tensor(11048.2646, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11048.255859375
tensor(11048.2646, grad_fn=<NegBackward0>) tensor(11048.2559, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11048.255859375
tensor(11048.2559, grad_fn=<NegBackward0>) tensor(11048.2559, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11048.2548828125
tensor(11048.2559, grad_fn=<NegBackward0>) tensor(11048.2549, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11044.205078125
tensor(11048.2549, grad_fn=<NegBackward0>) tensor(11044.2051, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11044.203125
tensor(11044.2051, grad_fn=<NegBackward0>) tensor(11044.2031, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11044.201171875
tensor(11044.2031, grad_fn=<NegBackward0>) tensor(11044.2012, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11044.201171875
tensor(11044.2012, grad_fn=<NegBackward0>) tensor(11044.2012, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11044.19921875
tensor(11044.2012, grad_fn=<NegBackward0>) tensor(11044.1992, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11044.0859375
tensor(11044.1992, grad_fn=<NegBackward0>) tensor(11044.0859, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11044.0859375
tensor(11044.0859, grad_fn=<NegBackward0>) tensor(11044.0859, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11044.0849609375
tensor(11044.0859, grad_fn=<NegBackward0>) tensor(11044.0850, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11044.078125
tensor(11044.0850, grad_fn=<NegBackward0>) tensor(11044.0781, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11043.0048828125
tensor(11044.0781, grad_fn=<NegBackward0>) tensor(11043.0049, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11043.0048828125
tensor(11043.0049, grad_fn=<NegBackward0>) tensor(11043.0049, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11043.0029296875
tensor(11043.0049, grad_fn=<NegBackward0>) tensor(11043.0029, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11043.01171875
tensor(11043.0029, grad_fn=<NegBackward0>) tensor(11043.0117, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11042.919921875
tensor(11043.0029, grad_fn=<NegBackward0>) tensor(11042.9199, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11042.9140625
tensor(11042.9199, grad_fn=<NegBackward0>) tensor(11042.9141, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11042.9072265625
tensor(11042.9141, grad_fn=<NegBackward0>) tensor(11042.9072, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11042.9052734375
tensor(11042.9072, grad_fn=<NegBackward0>) tensor(11042.9053, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11042.904296875
tensor(11042.9053, grad_fn=<NegBackward0>) tensor(11042.9043, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11042.904296875
tensor(11042.9043, grad_fn=<NegBackward0>) tensor(11042.9043, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11042.90234375
tensor(11042.9043, grad_fn=<NegBackward0>) tensor(11042.9023, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11042.90234375
tensor(11042.9023, grad_fn=<NegBackward0>) tensor(11042.9023, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11042.90234375
tensor(11042.9023, grad_fn=<NegBackward0>) tensor(11042.9023, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11042.90234375
tensor(11042.9023, grad_fn=<NegBackward0>) tensor(11042.9023, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11042.90234375
tensor(11042.9023, grad_fn=<NegBackward0>) tensor(11042.9023, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11042.90234375
tensor(11042.9023, grad_fn=<NegBackward0>) tensor(11042.9023, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11042.9013671875
tensor(11042.9023, grad_fn=<NegBackward0>) tensor(11042.9014, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11042.9013671875
tensor(11042.9014, grad_fn=<NegBackward0>) tensor(11042.9014, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11042.8935546875
tensor(11042.9014, grad_fn=<NegBackward0>) tensor(11042.8936, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11042.91015625
tensor(11042.8936, grad_fn=<NegBackward0>) tensor(11042.9102, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11042.89453125
tensor(11042.8936, grad_fn=<NegBackward0>) tensor(11042.8945, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11042.8935546875
tensor(11042.8936, grad_fn=<NegBackward0>) tensor(11042.8936, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11042.89453125
tensor(11042.8936, grad_fn=<NegBackward0>) tensor(11042.8945, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11042.89453125
tensor(11042.8936, grad_fn=<NegBackward0>) tensor(11042.8945, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11042.8935546875
tensor(11042.8936, grad_fn=<NegBackward0>) tensor(11042.8936, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11042.8935546875
tensor(11042.8936, grad_fn=<NegBackward0>) tensor(11042.8936, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11042.6904296875
tensor(11042.8936, grad_fn=<NegBackward0>) tensor(11042.6904, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11042.67578125
tensor(11042.6904, grad_fn=<NegBackward0>) tensor(11042.6758, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11042.66796875
tensor(11042.6758, grad_fn=<NegBackward0>) tensor(11042.6680, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11042.6640625
tensor(11042.6680, grad_fn=<NegBackward0>) tensor(11042.6641, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11042.7060546875
tensor(11042.6641, grad_fn=<NegBackward0>) tensor(11042.7061, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11042.666015625
tensor(11042.6641, grad_fn=<NegBackward0>) tensor(11042.6660, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -11042.6650390625
tensor(11042.6641, grad_fn=<NegBackward0>) tensor(11042.6650, grad_fn=<NegBackward0>)
3
pi: tensor([[0.6471, 0.3529],
        [0.2546, 0.7454]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 7.2825e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1772, 0.1834],
         [0.5126, 0.2770]],

        [[0.6205, 0.1184],
         [0.6025, 0.5916]],

        [[0.5509, 0.1021],
         [0.5694, 0.6589]],

        [[0.5762, 0.0918],
         [0.6861, 0.6198]],

        [[0.5599, 0.1020],
         [0.6635, 0.5557]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 90
Adjusted Rand Index: 0.6363294399767251
time is 2
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080650161849864
Global Adjusted Rand Index: 0.5002365392053894
Average Adjusted Rand Index: 0.6573633008807538
[0.5059270088681651, 0.5002365392053894] [0.6639101489929893, 0.6573633008807538] [11039.2802734375, 11042.666015625]
-------------------------------------
This iteration is 48
True Objective function: Loss = -10793.313023790199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24110.177734375
inf tensor(24110.1777, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10874.2822265625
tensor(24110.1777, grad_fn=<NegBackward0>) tensor(10874.2822, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10873.7412109375
tensor(10874.2822, grad_fn=<NegBackward0>) tensor(10873.7412, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10873.5654296875
tensor(10873.7412, grad_fn=<NegBackward0>) tensor(10873.5654, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10873.337890625
tensor(10873.5654, grad_fn=<NegBackward0>) tensor(10873.3379, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10872.82421875
tensor(10873.3379, grad_fn=<NegBackward0>) tensor(10872.8242, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10872.4248046875
tensor(10872.8242, grad_fn=<NegBackward0>) tensor(10872.4248, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10872.1220703125
tensor(10872.4248, grad_fn=<NegBackward0>) tensor(10872.1221, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10871.7578125
tensor(10872.1221, grad_fn=<NegBackward0>) tensor(10871.7578, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10871.5146484375
tensor(10871.7578, grad_fn=<NegBackward0>) tensor(10871.5146, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10871.326171875
tensor(10871.5146, grad_fn=<NegBackward0>) tensor(10871.3262, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10871.1279296875
tensor(10871.3262, grad_fn=<NegBackward0>) tensor(10871.1279, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10870.8603515625
tensor(10871.1279, grad_fn=<NegBackward0>) tensor(10870.8604, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10870.423828125
tensor(10870.8604, grad_fn=<NegBackward0>) tensor(10870.4238, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10869.837890625
tensor(10870.4238, grad_fn=<NegBackward0>) tensor(10869.8379, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10869.451171875
tensor(10869.8379, grad_fn=<NegBackward0>) tensor(10869.4512, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10869.2783203125
tensor(10869.4512, grad_fn=<NegBackward0>) tensor(10869.2783, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10868.982421875
tensor(10869.2783, grad_fn=<NegBackward0>) tensor(10868.9824, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10868.7890625
tensor(10868.9824, grad_fn=<NegBackward0>) tensor(10868.7891, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10860.912109375
tensor(10868.7891, grad_fn=<NegBackward0>) tensor(10860.9121, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10754.505859375
tensor(10860.9121, grad_fn=<NegBackward0>) tensor(10754.5059, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10750.7314453125
tensor(10754.5059, grad_fn=<NegBackward0>) tensor(10750.7314, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10742.7333984375
tensor(10750.7314, grad_fn=<NegBackward0>) tensor(10742.7334, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10742.6904296875
tensor(10742.7334, grad_fn=<NegBackward0>) tensor(10742.6904, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10741.904296875
tensor(10742.6904, grad_fn=<NegBackward0>) tensor(10741.9043, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10741.880859375
tensor(10741.9043, grad_fn=<NegBackward0>) tensor(10741.8809, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10741.8671875
tensor(10741.8809, grad_fn=<NegBackward0>) tensor(10741.8672, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10741.8583984375
tensor(10741.8672, grad_fn=<NegBackward0>) tensor(10741.8584, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10741.85546875
tensor(10741.8584, grad_fn=<NegBackward0>) tensor(10741.8555, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10741.85546875
tensor(10741.8555, grad_fn=<NegBackward0>) tensor(10741.8555, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10741.853515625
tensor(10741.8555, grad_fn=<NegBackward0>) tensor(10741.8535, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10741.8515625
tensor(10741.8535, grad_fn=<NegBackward0>) tensor(10741.8516, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10741.8505859375
tensor(10741.8516, grad_fn=<NegBackward0>) tensor(10741.8506, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10741.8271484375
tensor(10741.8506, grad_fn=<NegBackward0>) tensor(10741.8271, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10741.8251953125
tensor(10741.8271, grad_fn=<NegBackward0>) tensor(10741.8252, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10741.822265625
tensor(10741.8252, grad_fn=<NegBackward0>) tensor(10741.8223, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10741.8212890625
tensor(10741.8223, grad_fn=<NegBackward0>) tensor(10741.8213, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10741.8203125
tensor(10741.8213, grad_fn=<NegBackward0>) tensor(10741.8203, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10741.8125
tensor(10741.8203, grad_fn=<NegBackward0>) tensor(10741.8125, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10741.806640625
tensor(10741.8125, grad_fn=<NegBackward0>) tensor(10741.8066, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10741.798828125
tensor(10741.8066, grad_fn=<NegBackward0>) tensor(10741.7988, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10741.798828125
tensor(10741.7988, grad_fn=<NegBackward0>) tensor(10741.7988, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10741.7978515625
tensor(10741.7988, grad_fn=<NegBackward0>) tensor(10741.7979, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10741.798828125
tensor(10741.7979, grad_fn=<NegBackward0>) tensor(10741.7988, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -10741.7978515625
tensor(10741.7979, grad_fn=<NegBackward0>) tensor(10741.7979, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10741.796875
tensor(10741.7979, grad_fn=<NegBackward0>) tensor(10741.7969, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10741.796875
tensor(10741.7969, grad_fn=<NegBackward0>) tensor(10741.7969, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10741.7958984375
tensor(10741.7969, grad_fn=<NegBackward0>) tensor(10741.7959, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10741.7939453125
tensor(10741.7959, grad_fn=<NegBackward0>) tensor(10741.7939, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10741.79296875
tensor(10741.7939, grad_fn=<NegBackward0>) tensor(10741.7930, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10741.7939453125
tensor(10741.7930, grad_fn=<NegBackward0>) tensor(10741.7939, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10741.8056640625
tensor(10741.7930, grad_fn=<NegBackward0>) tensor(10741.8057, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -10741.7919921875
tensor(10741.7930, grad_fn=<NegBackward0>) tensor(10741.7920, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10741.7919921875
tensor(10741.7920, grad_fn=<NegBackward0>) tensor(10741.7920, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10741.7919921875
tensor(10741.7920, grad_fn=<NegBackward0>) tensor(10741.7920, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10741.791015625
tensor(10741.7920, grad_fn=<NegBackward0>) tensor(10741.7910, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10741.80859375
tensor(10741.7910, grad_fn=<NegBackward0>) tensor(10741.8086, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10741.7861328125
tensor(10741.7910, grad_fn=<NegBackward0>) tensor(10741.7861, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10741.7578125
tensor(10741.7861, grad_fn=<NegBackward0>) tensor(10741.7578, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10741.7685546875
tensor(10741.7578, grad_fn=<NegBackward0>) tensor(10741.7686, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10741.7587890625
tensor(10741.7578, grad_fn=<NegBackward0>) tensor(10741.7588, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -10741.7998046875
tensor(10741.7578, grad_fn=<NegBackward0>) tensor(10741.7998, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -10741.755859375
tensor(10741.7578, grad_fn=<NegBackward0>) tensor(10741.7559, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10741.7548828125
tensor(10741.7559, grad_fn=<NegBackward0>) tensor(10741.7549, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10741.7548828125
tensor(10741.7549, grad_fn=<NegBackward0>) tensor(10741.7549, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10741.755859375
tensor(10741.7549, grad_fn=<NegBackward0>) tensor(10741.7559, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10741.7548828125
tensor(10741.7549, grad_fn=<NegBackward0>) tensor(10741.7549, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10741.75390625
tensor(10741.7549, grad_fn=<NegBackward0>) tensor(10741.7539, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10741.7548828125
tensor(10741.7539, grad_fn=<NegBackward0>) tensor(10741.7549, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10741.7587890625
tensor(10741.7539, grad_fn=<NegBackward0>) tensor(10741.7588, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -10741.7548828125
tensor(10741.7539, grad_fn=<NegBackward0>) tensor(10741.7549, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -10741.7548828125
tensor(10741.7539, grad_fn=<NegBackward0>) tensor(10741.7549, grad_fn=<NegBackward0>)
4
Iteration 7200: Loss = -10741.7548828125
tensor(10741.7539, grad_fn=<NegBackward0>) tensor(10741.7549, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7200 due to no improvement.
pi: tensor([[0.7162, 0.2838],
        [0.2403, 0.7597]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5582, 0.4418], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2471, 0.0937],
         [0.6943, 0.1968]],

        [[0.6078, 0.0976],
         [0.6883, 0.6611]],

        [[0.5148, 0.1089],
         [0.7288, 0.7256]],

        [[0.5128, 0.1025],
         [0.5848, 0.6225]],

        [[0.5968, 0.0844],
         [0.5425, 0.5353]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
time is 2
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 8
Adjusted Rand Index: 0.7026048523603536
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448338943900694
time is 4
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.8460917583623252
Average Adjusted Rand Index: 0.8467772438669083
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21204.060546875
inf tensor(21204.0605, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10872.8662109375
tensor(21204.0605, grad_fn=<NegBackward0>) tensor(10872.8662, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10871.6611328125
tensor(10872.8662, grad_fn=<NegBackward0>) tensor(10871.6611, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10871.1826171875
tensor(10871.6611, grad_fn=<NegBackward0>) tensor(10871.1826, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10870.974609375
tensor(10871.1826, grad_fn=<NegBackward0>) tensor(10870.9746, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10870.80859375
tensor(10870.9746, grad_fn=<NegBackward0>) tensor(10870.8086, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10870.6240234375
tensor(10870.8086, grad_fn=<NegBackward0>) tensor(10870.6240, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10870.416015625
tensor(10870.6240, grad_fn=<NegBackward0>) tensor(10870.4160, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10870.2607421875
tensor(10870.4160, grad_fn=<NegBackward0>) tensor(10870.2607, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10870.1455078125
tensor(10870.2607, grad_fn=<NegBackward0>) tensor(10870.1455, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10870.01953125
tensor(10870.1455, grad_fn=<NegBackward0>) tensor(10870.0195, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10869.8623046875
tensor(10870.0195, grad_fn=<NegBackward0>) tensor(10869.8623, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10869.640625
tensor(10869.8623, grad_fn=<NegBackward0>) tensor(10869.6406, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10869.29296875
tensor(10869.6406, grad_fn=<NegBackward0>) tensor(10869.2930, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10868.8173828125
tensor(10869.2930, grad_fn=<NegBackward0>) tensor(10868.8174, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10868.4140625
tensor(10868.8174, grad_fn=<NegBackward0>) tensor(10868.4141, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10868.171875
tensor(10868.4141, grad_fn=<NegBackward0>) tensor(10868.1719, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10867.9697265625
tensor(10868.1719, grad_fn=<NegBackward0>) tensor(10867.9697, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10867.7783203125
tensor(10867.9697, grad_fn=<NegBackward0>) tensor(10867.7783, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10867.6142578125
tensor(10867.7783, grad_fn=<NegBackward0>) tensor(10867.6143, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10867.2861328125
tensor(10867.6143, grad_fn=<NegBackward0>) tensor(10867.2861, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10822.8408203125
tensor(10867.2861, grad_fn=<NegBackward0>) tensor(10822.8408, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10817.3828125
tensor(10822.8408, grad_fn=<NegBackward0>) tensor(10817.3828, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10743.6220703125
tensor(10817.3828, grad_fn=<NegBackward0>) tensor(10743.6221, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10742.9384765625
tensor(10743.6221, grad_fn=<NegBackward0>) tensor(10742.9385, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10742.0888671875
tensor(10742.9385, grad_fn=<NegBackward0>) tensor(10742.0889, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10742.03125
tensor(10742.0889, grad_fn=<NegBackward0>) tensor(10742.0312, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10742.005859375
tensor(10742.0312, grad_fn=<NegBackward0>) tensor(10742.0059, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10741.986328125
tensor(10742.0059, grad_fn=<NegBackward0>) tensor(10741.9863, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10741.9638671875
tensor(10741.9863, grad_fn=<NegBackward0>) tensor(10741.9639, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10741.9521484375
tensor(10741.9639, grad_fn=<NegBackward0>) tensor(10741.9521, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10741.9423828125
tensor(10741.9521, grad_fn=<NegBackward0>) tensor(10741.9424, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10741.8720703125
tensor(10741.9424, grad_fn=<NegBackward0>) tensor(10741.8721, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10741.865234375
tensor(10741.8721, grad_fn=<NegBackward0>) tensor(10741.8652, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10741.8603515625
tensor(10741.8652, grad_fn=<NegBackward0>) tensor(10741.8604, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10741.853515625
tensor(10741.8604, grad_fn=<NegBackward0>) tensor(10741.8535, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10741.8447265625
tensor(10741.8535, grad_fn=<NegBackward0>) tensor(10741.8447, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10741.8232421875
tensor(10741.8447, grad_fn=<NegBackward0>) tensor(10741.8232, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10741.81640625
tensor(10741.8232, grad_fn=<NegBackward0>) tensor(10741.8164, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10741.8134765625
tensor(10741.8164, grad_fn=<NegBackward0>) tensor(10741.8135, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10741.80859375
tensor(10741.8135, grad_fn=<NegBackward0>) tensor(10741.8086, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10741.8046875
tensor(10741.8086, grad_fn=<NegBackward0>) tensor(10741.8047, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10741.8046875
tensor(10741.8047, grad_fn=<NegBackward0>) tensor(10741.8047, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10741.802734375
tensor(10741.8047, grad_fn=<NegBackward0>) tensor(10741.8027, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10741.8017578125
tensor(10741.8027, grad_fn=<NegBackward0>) tensor(10741.8018, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10741.7998046875
tensor(10741.8018, grad_fn=<NegBackward0>) tensor(10741.7998, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10741.796875
tensor(10741.7998, grad_fn=<NegBackward0>) tensor(10741.7969, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10741.796875
tensor(10741.7969, grad_fn=<NegBackward0>) tensor(10741.7969, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10741.7998046875
tensor(10741.7969, grad_fn=<NegBackward0>) tensor(10741.7998, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10741.794921875
tensor(10741.7969, grad_fn=<NegBackward0>) tensor(10741.7949, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10741.796875
tensor(10741.7949, grad_fn=<NegBackward0>) tensor(10741.7969, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10741.7822265625
tensor(10741.7949, grad_fn=<NegBackward0>) tensor(10741.7822, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10741.78125
tensor(10741.7822, grad_fn=<NegBackward0>) tensor(10741.7812, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10741.78125
tensor(10741.7812, grad_fn=<NegBackward0>) tensor(10741.7812, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10741.7802734375
tensor(10741.7812, grad_fn=<NegBackward0>) tensor(10741.7803, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10741.7841796875
tensor(10741.7803, grad_fn=<NegBackward0>) tensor(10741.7842, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10741.779296875
tensor(10741.7803, grad_fn=<NegBackward0>) tensor(10741.7793, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10741.7783203125
tensor(10741.7793, grad_fn=<NegBackward0>) tensor(10741.7783, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10741.7783203125
tensor(10741.7783, grad_fn=<NegBackward0>) tensor(10741.7783, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10741.77734375
tensor(10741.7783, grad_fn=<NegBackward0>) tensor(10741.7773, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10741.78125
tensor(10741.7773, grad_fn=<NegBackward0>) tensor(10741.7812, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10741.77734375
tensor(10741.7773, grad_fn=<NegBackward0>) tensor(10741.7773, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10741.775390625
tensor(10741.7773, grad_fn=<NegBackward0>) tensor(10741.7754, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10741.7734375
tensor(10741.7754, grad_fn=<NegBackward0>) tensor(10741.7734, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10741.7724609375
tensor(10741.7734, grad_fn=<NegBackward0>) tensor(10741.7725, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10741.7724609375
tensor(10741.7725, grad_fn=<NegBackward0>) tensor(10741.7725, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10741.7734375
tensor(10741.7725, grad_fn=<NegBackward0>) tensor(10741.7734, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10741.7734375
tensor(10741.7725, grad_fn=<NegBackward0>) tensor(10741.7734, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -10741.771484375
tensor(10741.7725, grad_fn=<NegBackward0>) tensor(10741.7715, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10741.7724609375
tensor(10741.7715, grad_fn=<NegBackward0>) tensor(10741.7725, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10741.7724609375
tensor(10741.7715, grad_fn=<NegBackward0>) tensor(10741.7725, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -10741.7734375
tensor(10741.7715, grad_fn=<NegBackward0>) tensor(10741.7734, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -10741.771484375
tensor(10741.7715, grad_fn=<NegBackward0>) tensor(10741.7715, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10741.771484375
tensor(10741.7715, grad_fn=<NegBackward0>) tensor(10741.7715, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10741.7705078125
tensor(10741.7715, grad_fn=<NegBackward0>) tensor(10741.7705, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10741.775390625
tensor(10741.7705, grad_fn=<NegBackward0>) tensor(10741.7754, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10741.771484375
tensor(10741.7705, grad_fn=<NegBackward0>) tensor(10741.7715, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -10741.7734375
tensor(10741.7705, grad_fn=<NegBackward0>) tensor(10741.7734, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -10741.771484375
tensor(10741.7705, grad_fn=<NegBackward0>) tensor(10741.7715, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -10741.7724609375
tensor(10741.7705, grad_fn=<NegBackward0>) tensor(10741.7725, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7900 due to no improvement.
pi: tensor([[0.7167, 0.2833],
        [0.2393, 0.7607]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5592, 0.4408], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2469, 0.0937],
         [0.5781, 0.1968]],

        [[0.6639, 0.0976],
         [0.5620, 0.5081]],

        [[0.5821, 0.1089],
         [0.6227, 0.5928]],

        [[0.7153, 0.1025],
         [0.5419, 0.6652]],

        [[0.5486, 0.0844],
         [0.6356, 0.5877]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
time is 2
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 8
Adjusted Rand Index: 0.7026048523603536
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448338943900694
time is 4
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.8460917583623252
Average Adjusted Rand Index: 0.8467772438669083
[0.8460917583623252, 0.8460917583623252] [0.8467772438669083, 0.8467772438669083] [10741.7548828125, 10741.7724609375]
-------------------------------------
This iteration is 49
True Objective function: Loss = -10994.447994653518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19917.103515625
inf tensor(19917.1035, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11119.4990234375
tensor(19917.1035, grad_fn=<NegBackward0>) tensor(11119.4990, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11119.208984375
tensor(11119.4990, grad_fn=<NegBackward0>) tensor(11119.2090, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11119.09765625
tensor(11119.2090, grad_fn=<NegBackward0>) tensor(11119.0977, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11118.966796875
tensor(11119.0977, grad_fn=<NegBackward0>) tensor(11118.9668, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11118.7265625
tensor(11118.9668, grad_fn=<NegBackward0>) tensor(11118.7266, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11118.3505859375
tensor(11118.7266, grad_fn=<NegBackward0>) tensor(11118.3506, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11118.1162109375
tensor(11118.3506, grad_fn=<NegBackward0>) tensor(11118.1162, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11118.0224609375
tensor(11118.1162, grad_fn=<NegBackward0>) tensor(11118.0225, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11117.9677734375
tensor(11118.0225, grad_fn=<NegBackward0>) tensor(11117.9678, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11117.9296875
tensor(11117.9678, grad_fn=<NegBackward0>) tensor(11117.9297, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11117.9033203125
tensor(11117.9297, grad_fn=<NegBackward0>) tensor(11117.9033, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11117.8837890625
tensor(11117.9033, grad_fn=<NegBackward0>) tensor(11117.8838, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11117.87109375
tensor(11117.8838, grad_fn=<NegBackward0>) tensor(11117.8711, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11117.861328125
tensor(11117.8711, grad_fn=<NegBackward0>) tensor(11117.8613, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11117.8544921875
tensor(11117.8613, grad_fn=<NegBackward0>) tensor(11117.8545, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11117.849609375
tensor(11117.8545, grad_fn=<NegBackward0>) tensor(11117.8496, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11117.845703125
tensor(11117.8496, grad_fn=<NegBackward0>) tensor(11117.8457, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11117.84375
tensor(11117.8457, grad_fn=<NegBackward0>) tensor(11117.8438, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11117.8427734375
tensor(11117.8438, grad_fn=<NegBackward0>) tensor(11117.8428, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11117.83984375
tensor(11117.8428, grad_fn=<NegBackward0>) tensor(11117.8398, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11117.83984375
tensor(11117.8398, grad_fn=<NegBackward0>) tensor(11117.8398, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11117.8369140625
tensor(11117.8398, grad_fn=<NegBackward0>) tensor(11117.8369, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11117.8369140625
tensor(11117.8369, grad_fn=<NegBackward0>) tensor(11117.8369, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11117.8359375
tensor(11117.8369, grad_fn=<NegBackward0>) tensor(11117.8359, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11117.8359375
tensor(11117.8359, grad_fn=<NegBackward0>) tensor(11117.8359, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11117.8359375
tensor(11117.8359, grad_fn=<NegBackward0>) tensor(11117.8359, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11117.833984375
tensor(11117.8359, grad_fn=<NegBackward0>) tensor(11117.8340, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11117.833984375
tensor(11117.8340, grad_fn=<NegBackward0>) tensor(11117.8340, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11117.830078125
tensor(11117.8340, grad_fn=<NegBackward0>) tensor(11117.8301, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11117.8291015625
tensor(11117.8301, grad_fn=<NegBackward0>) tensor(11117.8291, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11117.8251953125
tensor(11117.8291, grad_fn=<NegBackward0>) tensor(11117.8252, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11117.822265625
tensor(11117.8252, grad_fn=<NegBackward0>) tensor(11117.8223, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11117.818359375
tensor(11117.8223, grad_fn=<NegBackward0>) tensor(11117.8184, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11117.8095703125
tensor(11117.8184, grad_fn=<NegBackward0>) tensor(11117.8096, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11117.794921875
tensor(11117.8096, grad_fn=<NegBackward0>) tensor(11117.7949, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11117.76953125
tensor(11117.7949, grad_fn=<NegBackward0>) tensor(11117.7695, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11117.7236328125
tensor(11117.7695, grad_fn=<NegBackward0>) tensor(11117.7236, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11117.6630859375
tensor(11117.7236, grad_fn=<NegBackward0>) tensor(11117.6631, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11117.615234375
tensor(11117.6631, grad_fn=<NegBackward0>) tensor(11117.6152, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11117.572265625
tensor(11117.6152, grad_fn=<NegBackward0>) tensor(11117.5723, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11117.5
tensor(11117.5723, grad_fn=<NegBackward0>) tensor(11117.5000, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11117.4423828125
tensor(11117.5000, grad_fn=<NegBackward0>) tensor(11117.4424, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11117.4072265625
tensor(11117.4424, grad_fn=<NegBackward0>) tensor(11117.4072, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11019.0625
tensor(11117.4072, grad_fn=<NegBackward0>) tensor(11019.0625, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11008.2529296875
tensor(11019.0625, grad_fn=<NegBackward0>) tensor(11008.2529, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11003.1728515625
tensor(11008.2529, grad_fn=<NegBackward0>) tensor(11003.1729, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10999.4091796875
tensor(11003.1729, grad_fn=<NegBackward0>) tensor(10999.4092, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10998.197265625
tensor(10999.4092, grad_fn=<NegBackward0>) tensor(10998.1973, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10998.173828125
tensor(10998.1973, grad_fn=<NegBackward0>) tensor(10998.1738, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10998.150390625
tensor(10998.1738, grad_fn=<NegBackward0>) tensor(10998.1504, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10998.1396484375
tensor(10998.1504, grad_fn=<NegBackward0>) tensor(10998.1396, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10998.0009765625
tensor(10998.1396, grad_fn=<NegBackward0>) tensor(10998.0010, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10996.263671875
tensor(10998.0010, grad_fn=<NegBackward0>) tensor(10996.2637, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10996.1689453125
tensor(10996.2637, grad_fn=<NegBackward0>) tensor(10996.1689, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10996.1162109375
tensor(10996.1689, grad_fn=<NegBackward0>) tensor(10996.1162, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10995.7109375
tensor(10996.1162, grad_fn=<NegBackward0>) tensor(10995.7109, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10995.669921875
tensor(10995.7109, grad_fn=<NegBackward0>) tensor(10995.6699, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10995.591796875
tensor(10995.6699, grad_fn=<NegBackward0>) tensor(10995.5918, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10995.58984375
tensor(10995.5918, grad_fn=<NegBackward0>) tensor(10995.5898, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10995.5888671875
tensor(10995.5898, grad_fn=<NegBackward0>) tensor(10995.5889, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10995.5859375
tensor(10995.5889, grad_fn=<NegBackward0>) tensor(10995.5859, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10995.587890625
tensor(10995.5859, grad_fn=<NegBackward0>) tensor(10995.5879, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10995.5849609375
tensor(10995.5859, grad_fn=<NegBackward0>) tensor(10995.5850, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10995.583984375
tensor(10995.5850, grad_fn=<NegBackward0>) tensor(10995.5840, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10995.6298828125
tensor(10995.5840, grad_fn=<NegBackward0>) tensor(10995.6299, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10995.4970703125
tensor(10995.5840, grad_fn=<NegBackward0>) tensor(10995.4971, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10995.482421875
tensor(10995.4971, grad_fn=<NegBackward0>) tensor(10995.4824, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10992.259765625
tensor(10995.4824, grad_fn=<NegBackward0>) tensor(10992.2598, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10992.2529296875
tensor(10992.2598, grad_fn=<NegBackward0>) tensor(10992.2529, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10992.251953125
tensor(10992.2529, grad_fn=<NegBackward0>) tensor(10992.2520, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10992.2529296875
tensor(10992.2520, grad_fn=<NegBackward0>) tensor(10992.2529, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10992.25
tensor(10992.2520, grad_fn=<NegBackward0>) tensor(10992.2500, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10992.248046875
tensor(10992.2500, grad_fn=<NegBackward0>) tensor(10992.2480, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10992.2490234375
tensor(10992.2480, grad_fn=<NegBackward0>) tensor(10992.2490, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10992.248046875
tensor(10992.2480, grad_fn=<NegBackward0>) tensor(10992.2480, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10992.2470703125
tensor(10992.2480, grad_fn=<NegBackward0>) tensor(10992.2471, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10992.2587890625
tensor(10992.2471, grad_fn=<NegBackward0>) tensor(10992.2588, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10992.240234375
tensor(10992.2471, grad_fn=<NegBackward0>) tensor(10992.2402, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10992.2412109375
tensor(10992.2402, grad_fn=<NegBackward0>) tensor(10992.2412, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10992.2392578125
tensor(10992.2402, grad_fn=<NegBackward0>) tensor(10992.2393, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10992.22265625
tensor(10992.2393, grad_fn=<NegBackward0>) tensor(10992.2227, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10992.2275390625
tensor(10992.2227, grad_fn=<NegBackward0>) tensor(10992.2275, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10992.2236328125
tensor(10992.2227, grad_fn=<NegBackward0>) tensor(10992.2236, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -10992.283203125
tensor(10992.2227, grad_fn=<NegBackward0>) tensor(10992.2832, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -10992.22265625
tensor(10992.2227, grad_fn=<NegBackward0>) tensor(10992.2227, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10992.2216796875
tensor(10992.2227, grad_fn=<NegBackward0>) tensor(10992.2217, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10992.22265625
tensor(10992.2217, grad_fn=<NegBackward0>) tensor(10992.2227, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -10992.2236328125
tensor(10992.2217, grad_fn=<NegBackward0>) tensor(10992.2236, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -10992.2353515625
tensor(10992.2217, grad_fn=<NegBackward0>) tensor(10992.2354, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -10992.2197265625
tensor(10992.2217, grad_fn=<NegBackward0>) tensor(10992.2197, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10992.21875
tensor(10992.2197, grad_fn=<NegBackward0>) tensor(10992.2188, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10992.2197265625
tensor(10992.2188, grad_fn=<NegBackward0>) tensor(10992.2197, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -10992.21875
tensor(10992.2188, grad_fn=<NegBackward0>) tensor(10992.2188, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10992.2216796875
tensor(10992.2188, grad_fn=<NegBackward0>) tensor(10992.2217, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -10992.2177734375
tensor(10992.2188, grad_fn=<NegBackward0>) tensor(10992.2178, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10992.35546875
tensor(10992.2178, grad_fn=<NegBackward0>) tensor(10992.3555, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -10992.2177734375
tensor(10992.2178, grad_fn=<NegBackward0>) tensor(10992.2178, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10992.216796875
tensor(10992.2178, grad_fn=<NegBackward0>) tensor(10992.2168, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10992.216796875
tensor(10992.2168, grad_fn=<NegBackward0>) tensor(10992.2168, grad_fn=<NegBackward0>)
pi: tensor([[0.6268, 0.3732],
        [0.3048, 0.6952]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7391, 0.2609], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2032, 0.0969],
         [0.5115, 0.2579]],

        [[0.7210, 0.1012],
         [0.6340, 0.5487]],

        [[0.5843, 0.1104],
         [0.5951, 0.5452]],

        [[0.7073, 0.0956],
         [0.6009, 0.5906]],

        [[0.6234, 0.0948],
         [0.6503, 0.7035]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 25
Adjusted Rand Index: 0.24160711878117838
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721069260785004
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448387581811067
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208065164923572
Global Adjusted Rand Index: 0.4084128697738826
Average Adjusted Rand Index: 0.7323544088758327
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22260.544921875
inf tensor(22260.5449, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11119.5703125
tensor(22260.5449, grad_fn=<NegBackward0>) tensor(11119.5703, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11119.1787109375
tensor(11119.5703, grad_fn=<NegBackward0>) tensor(11119.1787, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11119.10546875
tensor(11119.1787, grad_fn=<NegBackward0>) tensor(11119.1055, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11119.0400390625
tensor(11119.1055, grad_fn=<NegBackward0>) tensor(11119.0400, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11118.9541015625
tensor(11119.0400, grad_fn=<NegBackward0>) tensor(11118.9541, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11118.9169921875
tensor(11118.9541, grad_fn=<NegBackward0>) tensor(11118.9170, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11118.892578125
tensor(11118.9170, grad_fn=<NegBackward0>) tensor(11118.8926, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11118.8603515625
tensor(11118.8926, grad_fn=<NegBackward0>) tensor(11118.8604, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11118.7958984375
tensor(11118.8604, grad_fn=<NegBackward0>) tensor(11118.7959, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11118.54296875
tensor(11118.7959, grad_fn=<NegBackward0>) tensor(11118.5430, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11117.853515625
tensor(11118.5430, grad_fn=<NegBackward0>) tensor(11117.8535, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11117.720703125
tensor(11117.8535, grad_fn=<NegBackward0>) tensor(11117.7207, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11117.630859375
tensor(11117.7207, grad_fn=<NegBackward0>) tensor(11117.6309, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11117.5546875
tensor(11117.6309, grad_fn=<NegBackward0>) tensor(11117.5547, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11117.4677734375
tensor(11117.5547, grad_fn=<NegBackward0>) tensor(11117.4678, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11117.353515625
tensor(11117.4678, grad_fn=<NegBackward0>) tensor(11117.3535, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11117.126953125
tensor(11117.3535, grad_fn=<NegBackward0>) tensor(11117.1270, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11116.392578125
tensor(11117.1270, grad_fn=<NegBackward0>) tensor(11116.3926, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11023.7763671875
tensor(11116.3926, grad_fn=<NegBackward0>) tensor(11023.7764, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11007.5947265625
tensor(11023.7764, grad_fn=<NegBackward0>) tensor(11007.5947, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11007.2890625
tensor(11007.5947, grad_fn=<NegBackward0>) tensor(11007.2891, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11007.0390625
tensor(11007.2891, grad_fn=<NegBackward0>) tensor(11007.0391, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10954.296875
tensor(11007.0391, grad_fn=<NegBackward0>) tensor(10954.2969, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10952.564453125
tensor(10954.2969, grad_fn=<NegBackward0>) tensor(10952.5645, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10952.4833984375
tensor(10952.5645, grad_fn=<NegBackward0>) tensor(10952.4834, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10952.4716796875
tensor(10952.4834, grad_fn=<NegBackward0>) tensor(10952.4717, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10952.46484375
tensor(10952.4717, grad_fn=<NegBackward0>) tensor(10952.4648, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10952.4580078125
tensor(10952.4648, grad_fn=<NegBackward0>) tensor(10952.4580, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10952.451171875
tensor(10952.4580, grad_fn=<NegBackward0>) tensor(10952.4512, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10952.4453125
tensor(10952.4512, grad_fn=<NegBackward0>) tensor(10952.4453, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10952.3837890625
tensor(10952.4453, grad_fn=<NegBackward0>) tensor(10952.3838, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10952.291015625
tensor(10952.3838, grad_fn=<NegBackward0>) tensor(10952.2910, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10952.2880859375
tensor(10952.2910, grad_fn=<NegBackward0>) tensor(10952.2881, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10952.279296875
tensor(10952.2881, grad_fn=<NegBackward0>) tensor(10952.2793, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10952.2734375
tensor(10952.2793, grad_fn=<NegBackward0>) tensor(10952.2734, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10952.271484375
tensor(10952.2734, grad_fn=<NegBackward0>) tensor(10952.2715, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10952.271484375
tensor(10952.2715, grad_fn=<NegBackward0>) tensor(10952.2715, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10952.271484375
tensor(10952.2715, grad_fn=<NegBackward0>) tensor(10952.2715, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10952.26953125
tensor(10952.2715, grad_fn=<NegBackward0>) tensor(10952.2695, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10952.2666015625
tensor(10952.2695, grad_fn=<NegBackward0>) tensor(10952.2666, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10952.2841796875
tensor(10952.2666, grad_fn=<NegBackward0>) tensor(10952.2842, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -10952.26171875
tensor(10952.2666, grad_fn=<NegBackward0>) tensor(10952.2617, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10952.2646484375
tensor(10952.2617, grad_fn=<NegBackward0>) tensor(10952.2646, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -10952.2607421875
tensor(10952.2617, grad_fn=<NegBackward0>) tensor(10952.2607, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10952.2685546875
tensor(10952.2607, grad_fn=<NegBackward0>) tensor(10952.2686, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10952.2587890625
tensor(10952.2607, grad_fn=<NegBackward0>) tensor(10952.2588, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10952.2587890625
tensor(10952.2588, grad_fn=<NegBackward0>) tensor(10952.2588, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10952.2763671875
tensor(10952.2588, grad_fn=<NegBackward0>) tensor(10952.2764, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10952.248046875
tensor(10952.2588, grad_fn=<NegBackward0>) tensor(10952.2480, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10952.2470703125
tensor(10952.2480, grad_fn=<NegBackward0>) tensor(10952.2471, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10952.2470703125
tensor(10952.2471, grad_fn=<NegBackward0>) tensor(10952.2471, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10952.2470703125
tensor(10952.2471, grad_fn=<NegBackward0>) tensor(10952.2471, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10952.2470703125
tensor(10952.2471, grad_fn=<NegBackward0>) tensor(10952.2471, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10952.24609375
tensor(10952.2471, grad_fn=<NegBackward0>) tensor(10952.2461, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10952.2470703125
tensor(10952.2461, grad_fn=<NegBackward0>) tensor(10952.2471, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10952.248046875
tensor(10952.2461, grad_fn=<NegBackward0>) tensor(10952.2480, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -10952.2451171875
tensor(10952.2461, grad_fn=<NegBackward0>) tensor(10952.2451, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10952.2451171875
tensor(10952.2451, grad_fn=<NegBackward0>) tensor(10952.2451, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10952.24609375
tensor(10952.2451, grad_fn=<NegBackward0>) tensor(10952.2461, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10952.24609375
tensor(10952.2451, grad_fn=<NegBackward0>) tensor(10952.2461, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -10952.24609375
tensor(10952.2451, grad_fn=<NegBackward0>) tensor(10952.2461, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -10952.25390625
tensor(10952.2451, grad_fn=<NegBackward0>) tensor(10952.2539, grad_fn=<NegBackward0>)
4
Iteration 6300: Loss = -10952.2451171875
tensor(10952.2451, grad_fn=<NegBackward0>) tensor(10952.2451, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10952.24609375
tensor(10952.2451, grad_fn=<NegBackward0>) tensor(10952.2461, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10952.244140625
tensor(10952.2451, grad_fn=<NegBackward0>) tensor(10952.2441, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10952.24609375
tensor(10952.2441, grad_fn=<NegBackward0>) tensor(10952.2461, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10952.2451171875
tensor(10952.2441, grad_fn=<NegBackward0>) tensor(10952.2451, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -10952.24609375
tensor(10952.2441, grad_fn=<NegBackward0>) tensor(10952.2461, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -10952.2548828125
tensor(10952.2441, grad_fn=<NegBackward0>) tensor(10952.2549, grad_fn=<NegBackward0>)
4
Iteration 7000: Loss = -10952.2451171875
tensor(10952.2441, grad_fn=<NegBackward0>) tensor(10952.2451, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7000 due to no improvement.
pi: tensor([[0.7697, 0.2303],
        [0.2636, 0.7364]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5110, 0.4890], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2573, 0.1009],
         [0.5638, 0.2021]],

        [[0.5645, 0.0999],
         [0.5768, 0.6164]],

        [[0.7174, 0.1110],
         [0.7110, 0.6856]],

        [[0.6365, 0.0955],
         [0.6433, 0.5226]],

        [[0.5755, 0.0948],
         [0.7055, 0.5546]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 12
Adjusted Rand Index: 0.5733524869815048
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
Global Adjusted Rand Index: 0.8314069555427573
Average Adjusted Rand Index: 0.8361206163455241
[0.4084128697738826, 0.8314069555427573] [0.7323544088758327, 0.8361206163455241] [10992.2236328125, 10952.2451171875]
-------------------------------------
This iteration is 50
True Objective function: Loss = -11025.363969230853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22568.8515625
inf tensor(22568.8516, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11103.6513671875
tensor(22568.8516, grad_fn=<NegBackward0>) tensor(11103.6514, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11103.291015625
tensor(11103.6514, grad_fn=<NegBackward0>) tensor(11103.2910, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11103.1845703125
tensor(11103.2910, grad_fn=<NegBackward0>) tensor(11103.1846, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11103.1162109375
tensor(11103.1846, grad_fn=<NegBackward0>) tensor(11103.1162, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11103.0654296875
tensor(11103.1162, grad_fn=<NegBackward0>) tensor(11103.0654, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11103.0244140625
tensor(11103.0654, grad_fn=<NegBackward0>) tensor(11103.0244, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11102.986328125
tensor(11103.0244, grad_fn=<NegBackward0>) tensor(11102.9863, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11102.9541015625
tensor(11102.9863, grad_fn=<NegBackward0>) tensor(11102.9541, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11102.9267578125
tensor(11102.9541, grad_fn=<NegBackward0>) tensor(11102.9268, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11102.896484375
tensor(11102.9268, grad_fn=<NegBackward0>) tensor(11102.8965, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11102.8681640625
tensor(11102.8965, grad_fn=<NegBackward0>) tensor(11102.8682, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11102.8349609375
tensor(11102.8682, grad_fn=<NegBackward0>) tensor(11102.8350, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11102.7958984375
tensor(11102.8350, grad_fn=<NegBackward0>) tensor(11102.7959, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11102.7509765625
tensor(11102.7959, grad_fn=<NegBackward0>) tensor(11102.7510, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11102.693359375
tensor(11102.7510, grad_fn=<NegBackward0>) tensor(11102.6934, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11102.611328125
tensor(11102.6934, grad_fn=<NegBackward0>) tensor(11102.6113, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11102.46875
tensor(11102.6113, grad_fn=<NegBackward0>) tensor(11102.4688, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11102.0703125
tensor(11102.4688, grad_fn=<NegBackward0>) tensor(11102.0703, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11101.654296875
tensor(11102.0703, grad_fn=<NegBackward0>) tensor(11101.6543, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11101.38671875
tensor(11101.6543, grad_fn=<NegBackward0>) tensor(11101.3867, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11101.1396484375
tensor(11101.3867, grad_fn=<NegBackward0>) tensor(11101.1396, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11100.91015625
tensor(11101.1396, grad_fn=<NegBackward0>) tensor(11100.9102, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11100.69921875
tensor(11100.9102, grad_fn=<NegBackward0>) tensor(11100.6992, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11100.5078125
tensor(11100.6992, grad_fn=<NegBackward0>) tensor(11100.5078, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11100.2919921875
tensor(11100.5078, grad_fn=<NegBackward0>) tensor(11100.2920, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11099.9775390625
tensor(11100.2920, grad_fn=<NegBackward0>) tensor(11099.9775, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11099.7333984375
tensor(11099.9775, grad_fn=<NegBackward0>) tensor(11099.7334, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11099.59375
tensor(11099.7334, grad_fn=<NegBackward0>) tensor(11099.5938, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11099.5
tensor(11099.5938, grad_fn=<NegBackward0>) tensor(11099.5000, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11099.4150390625
tensor(11099.5000, grad_fn=<NegBackward0>) tensor(11099.4150, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11099.318359375
tensor(11099.4150, grad_fn=<NegBackward0>) tensor(11099.3184, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11099.0458984375
tensor(11099.3184, grad_fn=<NegBackward0>) tensor(11099.0459, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11031.466796875
tensor(11099.0459, grad_fn=<NegBackward0>) tensor(11031.4668, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10972.6123046875
tensor(11031.4668, grad_fn=<NegBackward0>) tensor(10972.6123, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10972.333984375
tensor(10972.6123, grad_fn=<NegBackward0>) tensor(10972.3340, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10972.23046875
tensor(10972.3340, grad_fn=<NegBackward0>) tensor(10972.2305, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10972.1689453125
tensor(10972.2305, grad_fn=<NegBackward0>) tensor(10972.1689, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10972.134765625
tensor(10972.1689, grad_fn=<NegBackward0>) tensor(10972.1348, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10972.1181640625
tensor(10972.1348, grad_fn=<NegBackward0>) tensor(10972.1182, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10972.0849609375
tensor(10972.1182, grad_fn=<NegBackward0>) tensor(10972.0850, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10972.0751953125
tensor(10972.0850, grad_fn=<NegBackward0>) tensor(10972.0752, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10972.0703125
tensor(10972.0752, grad_fn=<NegBackward0>) tensor(10972.0703, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10972.0654296875
tensor(10972.0703, grad_fn=<NegBackward0>) tensor(10972.0654, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10972.0634765625
tensor(10972.0654, grad_fn=<NegBackward0>) tensor(10972.0635, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10972.0576171875
tensor(10972.0635, grad_fn=<NegBackward0>) tensor(10972.0576, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10972.0341796875
tensor(10972.0576, grad_fn=<NegBackward0>) tensor(10972.0342, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10972.033203125
tensor(10972.0342, grad_fn=<NegBackward0>) tensor(10972.0332, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10972.029296875
tensor(10972.0332, grad_fn=<NegBackward0>) tensor(10972.0293, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10972.0322265625
tensor(10972.0293, grad_fn=<NegBackward0>) tensor(10972.0322, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10972.0263671875
tensor(10972.0293, grad_fn=<NegBackward0>) tensor(10972.0264, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10972.025390625
tensor(10972.0264, grad_fn=<NegBackward0>) tensor(10972.0254, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10972.0244140625
tensor(10972.0254, grad_fn=<NegBackward0>) tensor(10972.0244, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10972.0234375
tensor(10972.0244, grad_fn=<NegBackward0>) tensor(10972.0234, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10972.0244140625
tensor(10972.0234, grad_fn=<NegBackward0>) tensor(10972.0244, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10972.0263671875
tensor(10972.0234, grad_fn=<NegBackward0>) tensor(10972.0264, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -10972.021484375
tensor(10972.0234, grad_fn=<NegBackward0>) tensor(10972.0215, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10972.021484375
tensor(10972.0215, grad_fn=<NegBackward0>) tensor(10972.0215, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10972.0283203125
tensor(10972.0215, grad_fn=<NegBackward0>) tensor(10972.0283, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10972.01953125
tensor(10972.0215, grad_fn=<NegBackward0>) tensor(10972.0195, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10972.021484375
tensor(10972.0195, grad_fn=<NegBackward0>) tensor(10972.0215, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10972.025390625
tensor(10972.0195, grad_fn=<NegBackward0>) tensor(10972.0254, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -10972.01953125
tensor(10972.0195, grad_fn=<NegBackward0>) tensor(10972.0195, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10972.021484375
tensor(10972.0195, grad_fn=<NegBackward0>) tensor(10972.0215, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10972.0185546875
tensor(10972.0195, grad_fn=<NegBackward0>) tensor(10972.0186, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10972.017578125
tensor(10972.0186, grad_fn=<NegBackward0>) tensor(10972.0176, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10972.0166015625
tensor(10972.0176, grad_fn=<NegBackward0>) tensor(10972.0166, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10972.017578125
tensor(10972.0166, grad_fn=<NegBackward0>) tensor(10972.0176, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10972.017578125
tensor(10972.0166, grad_fn=<NegBackward0>) tensor(10972.0176, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -10972.017578125
tensor(10972.0166, grad_fn=<NegBackward0>) tensor(10972.0176, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -10972.0166015625
tensor(10972.0166, grad_fn=<NegBackward0>) tensor(10972.0166, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10972.0166015625
tensor(10972.0166, grad_fn=<NegBackward0>) tensor(10972.0166, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10972.0263671875
tensor(10972.0166, grad_fn=<NegBackward0>) tensor(10972.0264, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10972.0224609375
tensor(10972.0166, grad_fn=<NegBackward0>) tensor(10972.0225, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -10972.0146484375
tensor(10972.0166, grad_fn=<NegBackward0>) tensor(10972.0146, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10972.015625
tensor(10972.0146, grad_fn=<NegBackward0>) tensor(10972.0156, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10972.015625
tensor(10972.0146, grad_fn=<NegBackward0>) tensor(10972.0156, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -10972.0205078125
tensor(10972.0146, grad_fn=<NegBackward0>) tensor(10972.0205, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -10972.0146484375
tensor(10972.0146, grad_fn=<NegBackward0>) tensor(10972.0146, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10972.0146484375
tensor(10972.0146, grad_fn=<NegBackward0>) tensor(10972.0146, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10972.013671875
tensor(10972.0146, grad_fn=<NegBackward0>) tensor(10972.0137, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10972.013671875
tensor(10972.0137, grad_fn=<NegBackward0>) tensor(10972.0137, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10972.013671875
tensor(10972.0137, grad_fn=<NegBackward0>) tensor(10972.0137, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10972.015625
tensor(10972.0137, grad_fn=<NegBackward0>) tensor(10972.0156, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10972.013671875
tensor(10972.0137, grad_fn=<NegBackward0>) tensor(10972.0137, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10972.0126953125
tensor(10972.0137, grad_fn=<NegBackward0>) tensor(10972.0127, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10972.01171875
tensor(10972.0127, grad_fn=<NegBackward0>) tensor(10972.0117, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10972.0107421875
tensor(10972.0117, grad_fn=<NegBackward0>) tensor(10972.0107, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10972.01171875
tensor(10972.0107, grad_fn=<NegBackward0>) tensor(10972.0117, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10972.01171875
tensor(10972.0107, grad_fn=<NegBackward0>) tensor(10972.0117, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -10972.0146484375
tensor(10972.0107, grad_fn=<NegBackward0>) tensor(10972.0146, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -10972.0107421875
tensor(10972.0107, grad_fn=<NegBackward0>) tensor(10972.0107, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10972.01171875
tensor(10972.0107, grad_fn=<NegBackward0>) tensor(10972.0117, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -10972.01171875
tensor(10972.0107, grad_fn=<NegBackward0>) tensor(10972.0117, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -10972.009765625
tensor(10972.0107, grad_fn=<NegBackward0>) tensor(10972.0098, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10972.10546875
tensor(10972.0098, grad_fn=<NegBackward0>) tensor(10972.1055, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -10972.0107421875
tensor(10972.0098, grad_fn=<NegBackward0>) tensor(10972.0107, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -10972.0107421875
tensor(10972.0098, grad_fn=<NegBackward0>) tensor(10972.0107, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -10972.09375
tensor(10972.0098, grad_fn=<NegBackward0>) tensor(10972.0938, grad_fn=<NegBackward0>)
4
Iteration 9900: Loss = -10972.009765625
tensor(10972.0098, grad_fn=<NegBackward0>) tensor(10972.0098, grad_fn=<NegBackward0>)
pi: tensor([[0.7610, 0.2390],
        [0.2817, 0.7183]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5574, 0.4426], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2480, 0.0957],
         [0.5513, 0.2002]],

        [[0.5036, 0.1136],
         [0.5404, 0.5133]],

        [[0.6577, 0.0935],
         [0.5754, 0.6410]],

        [[0.5342, 0.0987],
         [0.5947, 0.6488]],

        [[0.5212, 0.1109],
         [0.5396, 0.6233]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 1
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080725364594837
time is 2
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824200963560147
time is 3
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721167107229054
time is 4
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 7
Adjusted Rand Index: 0.7369789715403975
Global Adjusted Rand Index: 0.8024055039681797
Average Adjusted Rand Index: 0.8015338246319219
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20504.46484375
inf tensor(20504.4648, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11103.6064453125
tensor(20504.4648, grad_fn=<NegBackward0>) tensor(11103.6064, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11103.228515625
tensor(11103.6064, grad_fn=<NegBackward0>) tensor(11103.2285, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11103.0966796875
tensor(11103.2285, grad_fn=<NegBackward0>) tensor(11103.0967, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11103.0087890625
tensor(11103.0967, grad_fn=<NegBackward0>) tensor(11103.0088, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11102.951171875
tensor(11103.0088, grad_fn=<NegBackward0>) tensor(11102.9512, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11102.91015625
tensor(11102.9512, grad_fn=<NegBackward0>) tensor(11102.9102, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11102.8798828125
tensor(11102.9102, grad_fn=<NegBackward0>) tensor(11102.8799, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11102.853515625
tensor(11102.8799, grad_fn=<NegBackward0>) tensor(11102.8535, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11102.830078125
tensor(11102.8535, grad_fn=<NegBackward0>) tensor(11102.8301, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11102.802734375
tensor(11102.8301, grad_fn=<NegBackward0>) tensor(11102.8027, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11102.775390625
tensor(11102.8027, grad_fn=<NegBackward0>) tensor(11102.7754, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11102.7421875
tensor(11102.7754, grad_fn=<NegBackward0>) tensor(11102.7422, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11102.697265625
tensor(11102.7422, grad_fn=<NegBackward0>) tensor(11102.6973, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11102.6328125
tensor(11102.6973, grad_fn=<NegBackward0>) tensor(11102.6328, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11102.5458984375
tensor(11102.6328, grad_fn=<NegBackward0>) tensor(11102.5459, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11102.439453125
tensor(11102.5459, grad_fn=<NegBackward0>) tensor(11102.4395, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11102.3369140625
tensor(11102.4395, grad_fn=<NegBackward0>) tensor(11102.3369, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11102.2275390625
tensor(11102.3369, grad_fn=<NegBackward0>) tensor(11102.2275, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11101.966796875
tensor(11102.2275, grad_fn=<NegBackward0>) tensor(11101.9668, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11100.9482421875
tensor(11101.9668, grad_fn=<NegBackward0>) tensor(11100.9482, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11100.44921875
tensor(11100.9482, grad_fn=<NegBackward0>) tensor(11100.4492, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11099.7841796875
tensor(11100.4492, grad_fn=<NegBackward0>) tensor(11099.7842, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11099.4638671875
tensor(11099.7842, grad_fn=<NegBackward0>) tensor(11099.4639, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11099.369140625
tensor(11099.4639, grad_fn=<NegBackward0>) tensor(11099.3691, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11099.302734375
tensor(11099.3691, grad_fn=<NegBackward0>) tensor(11099.3027, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11099.216796875
tensor(11099.3027, grad_fn=<NegBackward0>) tensor(11099.2168, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11098.5791015625
tensor(11099.2168, grad_fn=<NegBackward0>) tensor(11098.5791, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10975.400390625
tensor(11098.5791, grad_fn=<NegBackward0>) tensor(10975.4004, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10972.4306640625
tensor(10975.4004, grad_fn=<NegBackward0>) tensor(10972.4307, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10972.21875
tensor(10972.4307, grad_fn=<NegBackward0>) tensor(10972.2188, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10972.1767578125
tensor(10972.2188, grad_fn=<NegBackward0>) tensor(10972.1768, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10972.15234375
tensor(10972.1768, grad_fn=<NegBackward0>) tensor(10972.1523, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10972.140625
tensor(10972.1523, grad_fn=<NegBackward0>) tensor(10972.1406, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10972.130859375
tensor(10972.1406, grad_fn=<NegBackward0>) tensor(10972.1309, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10972.1142578125
tensor(10972.1309, grad_fn=<NegBackward0>) tensor(10972.1143, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10972.0859375
tensor(10972.1143, grad_fn=<NegBackward0>) tensor(10972.0859, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10972.080078125
tensor(10972.0859, grad_fn=<NegBackward0>) tensor(10972.0801, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10972.076171875
tensor(10972.0801, grad_fn=<NegBackward0>) tensor(10972.0762, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10972.068359375
tensor(10972.0762, grad_fn=<NegBackward0>) tensor(10972.0684, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10972.03515625
tensor(10972.0684, grad_fn=<NegBackward0>) tensor(10972.0352, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10972.0244140625
tensor(10972.0352, grad_fn=<NegBackward0>) tensor(10972.0244, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10972.0234375
tensor(10972.0244, grad_fn=<NegBackward0>) tensor(10972.0234, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10972.021484375
tensor(10972.0234, grad_fn=<NegBackward0>) tensor(10972.0215, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10972.0205078125
tensor(10972.0215, grad_fn=<NegBackward0>) tensor(10972.0205, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10972.01953125
tensor(10972.0205, grad_fn=<NegBackward0>) tensor(10972.0195, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10972.0244140625
tensor(10972.0195, grad_fn=<NegBackward0>) tensor(10972.0244, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -10972.0185546875
tensor(10972.0195, grad_fn=<NegBackward0>) tensor(10972.0186, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10972.01953125
tensor(10972.0186, grad_fn=<NegBackward0>) tensor(10972.0195, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10972.017578125
tensor(10972.0186, grad_fn=<NegBackward0>) tensor(10972.0176, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10972.0166015625
tensor(10972.0176, grad_fn=<NegBackward0>) tensor(10972.0166, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10972.017578125
tensor(10972.0166, grad_fn=<NegBackward0>) tensor(10972.0176, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10972.0166015625
tensor(10972.0166, grad_fn=<NegBackward0>) tensor(10972.0166, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10972.0146484375
tensor(10972.0166, grad_fn=<NegBackward0>) tensor(10972.0146, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10972.0185546875
tensor(10972.0146, grad_fn=<NegBackward0>) tensor(10972.0186, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10972.015625
tensor(10972.0146, grad_fn=<NegBackward0>) tensor(10972.0156, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -10972.015625
tensor(10972.0146, grad_fn=<NegBackward0>) tensor(10972.0156, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -10972.0146484375
tensor(10972.0146, grad_fn=<NegBackward0>) tensor(10972.0146, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10972.0146484375
tensor(10972.0146, grad_fn=<NegBackward0>) tensor(10972.0146, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10972.015625
tensor(10972.0146, grad_fn=<NegBackward0>) tensor(10972.0156, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10972.0146484375
tensor(10972.0146, grad_fn=<NegBackward0>) tensor(10972.0146, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10972.015625
tensor(10972.0146, grad_fn=<NegBackward0>) tensor(10972.0156, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10972.013671875
tensor(10972.0146, grad_fn=<NegBackward0>) tensor(10972.0137, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10972.0205078125
tensor(10972.0137, grad_fn=<NegBackward0>) tensor(10972.0205, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10972.0146484375
tensor(10972.0137, grad_fn=<NegBackward0>) tensor(10972.0146, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -10972.0146484375
tensor(10972.0137, grad_fn=<NegBackward0>) tensor(10972.0146, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -10972.013671875
tensor(10972.0137, grad_fn=<NegBackward0>) tensor(10972.0137, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10972.0146484375
tensor(10972.0137, grad_fn=<NegBackward0>) tensor(10972.0146, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10972.013671875
tensor(10972.0137, grad_fn=<NegBackward0>) tensor(10972.0137, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10972.0126953125
tensor(10972.0137, grad_fn=<NegBackward0>) tensor(10972.0127, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10972.0126953125
tensor(10972.0127, grad_fn=<NegBackward0>) tensor(10972.0127, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10972.01171875
tensor(10972.0127, grad_fn=<NegBackward0>) tensor(10972.0117, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10972.0126953125
tensor(10972.0117, grad_fn=<NegBackward0>) tensor(10972.0127, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10972.0126953125
tensor(10972.0117, grad_fn=<NegBackward0>) tensor(10972.0127, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -10972.03125
tensor(10972.0117, grad_fn=<NegBackward0>) tensor(10972.0312, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -10972.0205078125
tensor(10972.0117, grad_fn=<NegBackward0>) tensor(10972.0205, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -10972.0107421875
tensor(10972.0117, grad_fn=<NegBackward0>) tensor(10972.0107, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10972.013671875
tensor(10972.0107, grad_fn=<NegBackward0>) tensor(10972.0137, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10972.0107421875
tensor(10972.0107, grad_fn=<NegBackward0>) tensor(10972.0107, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10972.013671875
tensor(10972.0107, grad_fn=<NegBackward0>) tensor(10972.0137, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10972.0146484375
tensor(10972.0107, grad_fn=<NegBackward0>) tensor(10972.0146, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -10972.009765625
tensor(10972.0107, grad_fn=<NegBackward0>) tensor(10972.0098, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10972.009765625
tensor(10972.0098, grad_fn=<NegBackward0>) tensor(10972.0098, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10972.009765625
tensor(10972.0098, grad_fn=<NegBackward0>) tensor(10972.0098, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10972.025390625
tensor(10972.0098, grad_fn=<NegBackward0>) tensor(10972.0254, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -10972.0087890625
tensor(10972.0098, grad_fn=<NegBackward0>) tensor(10972.0088, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10972.009765625
tensor(10972.0088, grad_fn=<NegBackward0>) tensor(10972.0098, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10972.0087890625
tensor(10972.0088, grad_fn=<NegBackward0>) tensor(10972.0088, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10972.009765625
tensor(10972.0088, grad_fn=<NegBackward0>) tensor(10972.0098, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10972.0087890625
tensor(10972.0088, grad_fn=<NegBackward0>) tensor(10972.0088, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10972.0078125
tensor(10972.0088, grad_fn=<NegBackward0>) tensor(10972.0078, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10972.0087890625
tensor(10972.0078, grad_fn=<NegBackward0>) tensor(10972.0088, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10972.0107421875
tensor(10972.0078, grad_fn=<NegBackward0>) tensor(10972.0107, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -10972.0078125
tensor(10972.0078, grad_fn=<NegBackward0>) tensor(10972.0078, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10972.0087890625
tensor(10972.0078, grad_fn=<NegBackward0>) tensor(10972.0088, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -10972.0087890625
tensor(10972.0078, grad_fn=<NegBackward0>) tensor(10972.0088, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -10972.009765625
tensor(10972.0078, grad_fn=<NegBackward0>) tensor(10972.0098, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -10972.2197265625
tensor(10972.0078, grad_fn=<NegBackward0>) tensor(10972.2197, grad_fn=<NegBackward0>)
4
Iteration 9800: Loss = -10972.0087890625
tensor(10972.0078, grad_fn=<NegBackward0>) tensor(10972.0088, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9800 due to no improvement.
pi: tensor([[0.7181, 0.2819],
        [0.2389, 0.7611]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4424, 0.5576], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2003, 0.0956],
         [0.6423, 0.2479]],

        [[0.5101, 0.1135],
         [0.6345, 0.5095]],

        [[0.6096, 0.0935],
         [0.5215, 0.6638]],

        [[0.5469, 0.0987],
         [0.6685, 0.6767]],

        [[0.5255, 0.1109],
         [0.5348, 0.6186]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080808080808081
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080725364594837
time is 2
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824200963560147
time is 3
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721167107229054
time is 4
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 93
Adjusted Rand Index: 0.7369789715403975
Global Adjusted Rand Index: 0.8024055039681797
Average Adjusted Rand Index: 0.8015338246319219
[0.8024055039681797, 0.8024055039681797] [0.8015338246319219, 0.8015338246319219] [10972.009765625, 10972.0087890625]
-------------------------------------
This iteration is 51
True Objective function: Loss = -10931.184281730853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22054.30859375
inf tensor(22054.3086, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11004.6162109375
tensor(22054.3086, grad_fn=<NegBackward0>) tensor(11004.6162, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11003.927734375
tensor(11004.6162, grad_fn=<NegBackward0>) tensor(11003.9277, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11003.7236328125
tensor(11003.9277, grad_fn=<NegBackward0>) tensor(11003.7236, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11003.603515625
tensor(11003.7236, grad_fn=<NegBackward0>) tensor(11003.6035, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11003.4814453125
tensor(11003.6035, grad_fn=<NegBackward0>) tensor(11003.4814, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11003.296875
tensor(11003.4814, grad_fn=<NegBackward0>) tensor(11003.2969, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11003.0224609375
tensor(11003.2969, grad_fn=<NegBackward0>) tensor(11003.0225, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11002.732421875
tensor(11003.0225, grad_fn=<NegBackward0>) tensor(11002.7324, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11002.4462890625
tensor(11002.7324, grad_fn=<NegBackward0>) tensor(11002.4463, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11002.208984375
tensor(11002.4463, grad_fn=<NegBackward0>) tensor(11002.2090, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11002.025390625
tensor(11002.2090, grad_fn=<NegBackward0>) tensor(11002.0254, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11001.8896484375
tensor(11002.0254, grad_fn=<NegBackward0>) tensor(11001.8896, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11001.8017578125
tensor(11001.8896, grad_fn=<NegBackward0>) tensor(11001.8018, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11001.7578125
tensor(11001.8018, grad_fn=<NegBackward0>) tensor(11001.7578, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11001.7353515625
tensor(11001.7578, grad_fn=<NegBackward0>) tensor(11001.7354, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11001.7216796875
tensor(11001.7354, grad_fn=<NegBackward0>) tensor(11001.7217, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11001.7109375
tensor(11001.7217, grad_fn=<NegBackward0>) tensor(11001.7109, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11001.7060546875
tensor(11001.7109, grad_fn=<NegBackward0>) tensor(11001.7061, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11001.7021484375
tensor(11001.7061, grad_fn=<NegBackward0>) tensor(11001.7021, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11001.701171875
tensor(11001.7021, grad_fn=<NegBackward0>) tensor(11001.7012, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11001.6982421875
tensor(11001.7012, grad_fn=<NegBackward0>) tensor(11001.6982, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11001.6962890625
tensor(11001.6982, grad_fn=<NegBackward0>) tensor(11001.6963, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11001.6943359375
tensor(11001.6963, grad_fn=<NegBackward0>) tensor(11001.6943, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11001.693359375
tensor(11001.6943, grad_fn=<NegBackward0>) tensor(11001.6934, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11001.69140625
tensor(11001.6934, grad_fn=<NegBackward0>) tensor(11001.6914, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11001.693359375
tensor(11001.6914, grad_fn=<NegBackward0>) tensor(11001.6934, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -11001.689453125
tensor(11001.6914, grad_fn=<NegBackward0>) tensor(11001.6895, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11001.69140625
tensor(11001.6895, grad_fn=<NegBackward0>) tensor(11001.6914, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11001.689453125
tensor(11001.6895, grad_fn=<NegBackward0>) tensor(11001.6895, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11001.6884765625
tensor(11001.6895, grad_fn=<NegBackward0>) tensor(11001.6885, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11001.6875
tensor(11001.6885, grad_fn=<NegBackward0>) tensor(11001.6875, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11001.689453125
tensor(11001.6875, grad_fn=<NegBackward0>) tensor(11001.6895, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11001.6865234375
tensor(11001.6875, grad_fn=<NegBackward0>) tensor(11001.6865, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11001.685546875
tensor(11001.6865, grad_fn=<NegBackward0>) tensor(11001.6855, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11001.685546875
tensor(11001.6855, grad_fn=<NegBackward0>) tensor(11001.6855, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11001.68359375
tensor(11001.6855, grad_fn=<NegBackward0>) tensor(11001.6836, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11001.68359375
tensor(11001.6836, grad_fn=<NegBackward0>) tensor(11001.6836, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11001.6826171875
tensor(11001.6836, grad_fn=<NegBackward0>) tensor(11001.6826, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11001.6826171875
tensor(11001.6826, grad_fn=<NegBackward0>) tensor(11001.6826, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11001.6826171875
tensor(11001.6826, grad_fn=<NegBackward0>) tensor(11001.6826, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11001.6826171875
tensor(11001.6826, grad_fn=<NegBackward0>) tensor(11001.6826, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11001.6826171875
tensor(11001.6826, grad_fn=<NegBackward0>) tensor(11001.6826, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11001.681640625
tensor(11001.6826, grad_fn=<NegBackward0>) tensor(11001.6816, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11001.6806640625
tensor(11001.6816, grad_fn=<NegBackward0>) tensor(11001.6807, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11001.681640625
tensor(11001.6807, grad_fn=<NegBackward0>) tensor(11001.6816, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11001.6806640625
tensor(11001.6807, grad_fn=<NegBackward0>) tensor(11001.6807, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11001.6806640625
tensor(11001.6807, grad_fn=<NegBackward0>) tensor(11001.6807, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11001.6787109375
tensor(11001.6807, grad_fn=<NegBackward0>) tensor(11001.6787, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11001.6787109375
tensor(11001.6787, grad_fn=<NegBackward0>) tensor(11001.6787, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11001.6796875
tensor(11001.6787, grad_fn=<NegBackward0>) tensor(11001.6797, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11001.677734375
tensor(11001.6787, grad_fn=<NegBackward0>) tensor(11001.6777, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11001.677734375
tensor(11001.6777, grad_fn=<NegBackward0>) tensor(11001.6777, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11001.677734375
tensor(11001.6777, grad_fn=<NegBackward0>) tensor(11001.6777, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11001.677734375
tensor(11001.6777, grad_fn=<NegBackward0>) tensor(11001.6777, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11001.677734375
tensor(11001.6777, grad_fn=<NegBackward0>) tensor(11001.6777, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11001.677734375
tensor(11001.6777, grad_fn=<NegBackward0>) tensor(11001.6777, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11001.677734375
tensor(11001.6777, grad_fn=<NegBackward0>) tensor(11001.6777, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11001.6767578125
tensor(11001.6777, grad_fn=<NegBackward0>) tensor(11001.6768, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11001.677734375
tensor(11001.6768, grad_fn=<NegBackward0>) tensor(11001.6777, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11001.67578125
tensor(11001.6768, grad_fn=<NegBackward0>) tensor(11001.6758, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11001.677734375
tensor(11001.6758, grad_fn=<NegBackward0>) tensor(11001.6777, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11001.6767578125
tensor(11001.6758, grad_fn=<NegBackward0>) tensor(11001.6768, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11001.67578125
tensor(11001.6758, grad_fn=<NegBackward0>) tensor(11001.6758, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11001.6767578125
tensor(11001.6758, grad_fn=<NegBackward0>) tensor(11001.6768, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11001.6748046875
tensor(11001.6758, grad_fn=<NegBackward0>) tensor(11001.6748, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11001.6767578125
tensor(11001.6748, grad_fn=<NegBackward0>) tensor(11001.6768, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11001.673828125
tensor(11001.6748, grad_fn=<NegBackward0>) tensor(11001.6738, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11001.67578125
tensor(11001.6738, grad_fn=<NegBackward0>) tensor(11001.6758, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11001.6748046875
tensor(11001.6738, grad_fn=<NegBackward0>) tensor(11001.6748, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11001.67578125
tensor(11001.6738, grad_fn=<NegBackward0>) tensor(11001.6758, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11001.6748046875
tensor(11001.6738, grad_fn=<NegBackward0>) tensor(11001.6748, grad_fn=<NegBackward0>)
4
Iteration 7200: Loss = -11001.6748046875
tensor(11001.6738, grad_fn=<NegBackward0>) tensor(11001.6748, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7200 due to no improvement.
pi: tensor([[0.9468, 0.0532],
        [0.9984, 0.0016]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9978, 0.0022], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1614, 0.1688],
         [0.6050, 0.2231]],

        [[0.5513, 0.2043],
         [0.7237, 0.7274]],

        [[0.5673, 0.1063],
         [0.5546, 0.6867]],

        [[0.6040, 0.1982],
         [0.6519, 0.5960]],

        [[0.5496, 0.1966],
         [0.7047, 0.6569]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -3.1873015856761384e-05
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21878.7578125
inf tensor(21878.7578, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11004.322265625
tensor(21878.7578, grad_fn=<NegBackward0>) tensor(11004.3223, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11003.8984375
tensor(11004.3223, grad_fn=<NegBackward0>) tensor(11003.8984, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11003.75390625
tensor(11003.8984, grad_fn=<NegBackward0>) tensor(11003.7539, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11003.6328125
tensor(11003.7539, grad_fn=<NegBackward0>) tensor(11003.6328, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11003.4873046875
tensor(11003.6328, grad_fn=<NegBackward0>) tensor(11003.4873, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11003.22265625
tensor(11003.4873, grad_fn=<NegBackward0>) tensor(11003.2227, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11002.50390625
tensor(11003.2227, grad_fn=<NegBackward0>) tensor(11002.5039, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11002.0390625
tensor(11002.5039, grad_fn=<NegBackward0>) tensor(11002.0391, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11001.8779296875
tensor(11002.0391, grad_fn=<NegBackward0>) tensor(11001.8779, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11001.7861328125
tensor(11001.8779, grad_fn=<NegBackward0>) tensor(11001.7861, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11001.7451171875
tensor(11001.7861, grad_fn=<NegBackward0>) tensor(11001.7451, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11001.724609375
tensor(11001.7451, grad_fn=<NegBackward0>) tensor(11001.7246, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11001.7158203125
tensor(11001.7246, grad_fn=<NegBackward0>) tensor(11001.7158, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11001.708984375
tensor(11001.7158, grad_fn=<NegBackward0>) tensor(11001.7090, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11001.70703125
tensor(11001.7090, grad_fn=<NegBackward0>) tensor(11001.7070, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11001.703125
tensor(11001.7070, grad_fn=<NegBackward0>) tensor(11001.7031, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11001.69921875
tensor(11001.7031, grad_fn=<NegBackward0>) tensor(11001.6992, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11001.69921875
tensor(11001.6992, grad_fn=<NegBackward0>) tensor(11001.6992, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11001.6962890625
tensor(11001.6992, grad_fn=<NegBackward0>) tensor(11001.6963, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11001.6953125
tensor(11001.6963, grad_fn=<NegBackward0>) tensor(11001.6953, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11001.6943359375
tensor(11001.6953, grad_fn=<NegBackward0>) tensor(11001.6943, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11001.6923828125
tensor(11001.6943, grad_fn=<NegBackward0>) tensor(11001.6924, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11001.6904296875
tensor(11001.6924, grad_fn=<NegBackward0>) tensor(11001.6904, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11001.689453125
tensor(11001.6904, grad_fn=<NegBackward0>) tensor(11001.6895, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11001.689453125
tensor(11001.6895, grad_fn=<NegBackward0>) tensor(11001.6895, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11001.6875
tensor(11001.6895, grad_fn=<NegBackward0>) tensor(11001.6875, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11001.6865234375
tensor(11001.6875, grad_fn=<NegBackward0>) tensor(11001.6865, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11001.6865234375
tensor(11001.6865, grad_fn=<NegBackward0>) tensor(11001.6865, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11001.6845703125
tensor(11001.6865, grad_fn=<NegBackward0>) tensor(11001.6846, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11001.6845703125
tensor(11001.6846, grad_fn=<NegBackward0>) tensor(11001.6846, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11001.68359375
tensor(11001.6846, grad_fn=<NegBackward0>) tensor(11001.6836, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11001.6826171875
tensor(11001.6836, grad_fn=<NegBackward0>) tensor(11001.6826, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11001.6826171875
tensor(11001.6826, grad_fn=<NegBackward0>) tensor(11001.6826, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11001.68359375
tensor(11001.6826, grad_fn=<NegBackward0>) tensor(11001.6836, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11001.681640625
tensor(11001.6826, grad_fn=<NegBackward0>) tensor(11001.6816, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11001.6796875
tensor(11001.6816, grad_fn=<NegBackward0>) tensor(11001.6797, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11001.6806640625
tensor(11001.6797, grad_fn=<NegBackward0>) tensor(11001.6807, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11001.6796875
tensor(11001.6797, grad_fn=<NegBackward0>) tensor(11001.6797, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11001.6787109375
tensor(11001.6797, grad_fn=<NegBackward0>) tensor(11001.6787, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11001.6787109375
tensor(11001.6787, grad_fn=<NegBackward0>) tensor(11001.6787, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11001.6796875
tensor(11001.6787, grad_fn=<NegBackward0>) tensor(11001.6797, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11001.6767578125
tensor(11001.6787, grad_fn=<NegBackward0>) tensor(11001.6768, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11001.6796875
tensor(11001.6768, grad_fn=<NegBackward0>) tensor(11001.6797, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11001.677734375
tensor(11001.6768, grad_fn=<NegBackward0>) tensor(11001.6777, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -11001.677734375
tensor(11001.6768, grad_fn=<NegBackward0>) tensor(11001.6777, grad_fn=<NegBackward0>)
3
Iteration 4600: Loss = -11001.6787109375
tensor(11001.6768, grad_fn=<NegBackward0>) tensor(11001.6787, grad_fn=<NegBackward0>)
4
Iteration 4700: Loss = -11001.677734375
tensor(11001.6768, grad_fn=<NegBackward0>) tensor(11001.6777, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4700 due to no improvement.
pi: tensor([[0.9468, 0.0532],
        [0.9965, 0.0035]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9972, 0.0028], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1615, 0.1693],
         [0.7301, 0.2228]],

        [[0.6673, 0.2042],
         [0.6483, 0.5054]],

        [[0.6532, 0.1065],
         [0.5652, 0.5365]],

        [[0.7280, 0.1981],
         [0.6344, 0.5602]],

        [[0.7073, 0.1965],
         [0.6909, 0.6198]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -3.1873015856761384e-05
Average Adjusted Rand Index: 0.0
[-3.1873015856761384e-05, -3.1873015856761384e-05] [0.0, 0.0] [11001.6748046875, 11001.677734375]
-------------------------------------
This iteration is 52
True Objective function: Loss = -10935.573323034829
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24020.810546875
inf tensor(24020.8105, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11013.4013671875
tensor(24020.8105, grad_fn=<NegBackward0>) tensor(11013.4014, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11011.5869140625
tensor(11013.4014, grad_fn=<NegBackward0>) tensor(11011.5869, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11010.515625
tensor(11011.5869, grad_fn=<NegBackward0>) tensor(11010.5156, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11009.7705078125
tensor(11010.5156, grad_fn=<NegBackward0>) tensor(11009.7705, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11009.365234375
tensor(11009.7705, grad_fn=<NegBackward0>) tensor(11009.3652, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11009.02734375
tensor(11009.3652, grad_fn=<NegBackward0>) tensor(11009.0273, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11008.72265625
tensor(11009.0273, grad_fn=<NegBackward0>) tensor(11008.7227, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11008.43359375
tensor(11008.7227, grad_fn=<NegBackward0>) tensor(11008.4336, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11008.1494140625
tensor(11008.4336, grad_fn=<NegBackward0>) tensor(11008.1494, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11007.8740234375
tensor(11008.1494, grad_fn=<NegBackward0>) tensor(11007.8740, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11007.6123046875
tensor(11007.8740, grad_fn=<NegBackward0>) tensor(11007.6123, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11007.4013671875
tensor(11007.6123, grad_fn=<NegBackward0>) tensor(11007.4014, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11007.26171875
tensor(11007.4014, grad_fn=<NegBackward0>) tensor(11007.2617, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11007.1708984375
tensor(11007.2617, grad_fn=<NegBackward0>) tensor(11007.1709, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11007.103515625
tensor(11007.1709, grad_fn=<NegBackward0>) tensor(11007.1035, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11007.046875
tensor(11007.1035, grad_fn=<NegBackward0>) tensor(11007.0469, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11007.001953125
tensor(11007.0469, grad_fn=<NegBackward0>) tensor(11007.0020, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11006.9736328125
tensor(11007.0020, grad_fn=<NegBackward0>) tensor(11006.9736, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11006.9560546875
tensor(11006.9736, grad_fn=<NegBackward0>) tensor(11006.9561, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11006.9443359375
tensor(11006.9561, grad_fn=<NegBackward0>) tensor(11006.9443, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11006.9384765625
tensor(11006.9443, grad_fn=<NegBackward0>) tensor(11006.9385, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11006.9326171875
tensor(11006.9385, grad_fn=<NegBackward0>) tensor(11006.9326, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11006.9296875
tensor(11006.9326, grad_fn=<NegBackward0>) tensor(11006.9297, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11006.92578125
tensor(11006.9297, grad_fn=<NegBackward0>) tensor(11006.9258, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11006.921875
tensor(11006.9258, grad_fn=<NegBackward0>) tensor(11006.9219, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11006.919921875
tensor(11006.9219, grad_fn=<NegBackward0>) tensor(11006.9199, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11006.9189453125
tensor(11006.9199, grad_fn=<NegBackward0>) tensor(11006.9189, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11006.91796875
tensor(11006.9189, grad_fn=<NegBackward0>) tensor(11006.9180, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11006.9150390625
tensor(11006.9180, grad_fn=<NegBackward0>) tensor(11006.9150, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11006.9150390625
tensor(11006.9150, grad_fn=<NegBackward0>) tensor(11006.9150, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11006.9140625
tensor(11006.9150, grad_fn=<NegBackward0>) tensor(11006.9141, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11006.912109375
tensor(11006.9141, grad_fn=<NegBackward0>) tensor(11006.9121, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11006.912109375
tensor(11006.9121, grad_fn=<NegBackward0>) tensor(11006.9121, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11006.91015625
tensor(11006.9121, grad_fn=<NegBackward0>) tensor(11006.9102, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11006.9091796875
tensor(11006.9102, grad_fn=<NegBackward0>) tensor(11006.9092, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11006.908203125
tensor(11006.9092, grad_fn=<NegBackward0>) tensor(11006.9082, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11006.908203125
tensor(11006.9082, grad_fn=<NegBackward0>) tensor(11006.9082, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11006.9072265625
tensor(11006.9082, grad_fn=<NegBackward0>) tensor(11006.9072, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11006.9072265625
tensor(11006.9072, grad_fn=<NegBackward0>) tensor(11006.9072, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11006.9052734375
tensor(11006.9072, grad_fn=<NegBackward0>) tensor(11006.9053, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11006.90625
tensor(11006.9053, grad_fn=<NegBackward0>) tensor(11006.9062, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11006.9052734375
tensor(11006.9053, grad_fn=<NegBackward0>) tensor(11006.9053, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11006.904296875
tensor(11006.9053, grad_fn=<NegBackward0>) tensor(11006.9043, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11006.9052734375
tensor(11006.9043, grad_fn=<NegBackward0>) tensor(11006.9053, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11006.904296875
tensor(11006.9043, grad_fn=<NegBackward0>) tensor(11006.9043, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11006.904296875
tensor(11006.9043, grad_fn=<NegBackward0>) tensor(11006.9043, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11006.904296875
tensor(11006.9043, grad_fn=<NegBackward0>) tensor(11006.9043, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11006.9033203125
tensor(11006.9043, grad_fn=<NegBackward0>) tensor(11006.9033, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11006.904296875
tensor(11006.9033, grad_fn=<NegBackward0>) tensor(11006.9043, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11006.9033203125
tensor(11006.9033, grad_fn=<NegBackward0>) tensor(11006.9033, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11006.90234375
tensor(11006.9033, grad_fn=<NegBackward0>) tensor(11006.9023, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11006.904296875
tensor(11006.9023, grad_fn=<NegBackward0>) tensor(11006.9043, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11006.9033203125
tensor(11006.9023, grad_fn=<NegBackward0>) tensor(11006.9033, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -11006.90234375
tensor(11006.9023, grad_fn=<NegBackward0>) tensor(11006.9023, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11006.90234375
tensor(11006.9023, grad_fn=<NegBackward0>) tensor(11006.9023, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11006.9013671875
tensor(11006.9023, grad_fn=<NegBackward0>) tensor(11006.9014, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11006.9013671875
tensor(11006.9014, grad_fn=<NegBackward0>) tensor(11006.9014, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11006.9033203125
tensor(11006.9014, grad_fn=<NegBackward0>) tensor(11006.9033, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11006.900390625
tensor(11006.9014, grad_fn=<NegBackward0>) tensor(11006.9004, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11006.9013671875
tensor(11006.9004, grad_fn=<NegBackward0>) tensor(11006.9014, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11006.8994140625
tensor(11006.9004, grad_fn=<NegBackward0>) tensor(11006.8994, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11006.900390625
tensor(11006.8994, grad_fn=<NegBackward0>) tensor(11006.9004, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11006.90234375
tensor(11006.8994, grad_fn=<NegBackward0>) tensor(11006.9023, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11006.900390625
tensor(11006.8994, grad_fn=<NegBackward0>) tensor(11006.9004, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -11006.9013671875
tensor(11006.8994, grad_fn=<NegBackward0>) tensor(11006.9014, grad_fn=<NegBackward0>)
4
Iteration 6600: Loss = -11006.9013671875
tensor(11006.8994, grad_fn=<NegBackward0>) tensor(11006.9014, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6600 due to no improvement.
pi: tensor([[0.9634, 0.0366],
        [0.9370, 0.0630]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0227, 0.9773], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1574, 0.1679],
         [0.6346, 0.1707]],

        [[0.7018, 0.2276],
         [0.5242, 0.6137]],

        [[0.6153, 0.1390],
         [0.6834, 0.6348]],

        [[0.5010, 0.1648],
         [0.5120, 0.6923]],

        [[0.6013, 0.2722],
         [0.5005, 0.6977]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0032454170932134756
Global Adjusted Rand Index: -0.0012751047570591862
Average Adjusted Rand Index: -0.0004873391686507823
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22157.0546875
inf tensor(22157.0547, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11013.244140625
tensor(22157.0547, grad_fn=<NegBackward0>) tensor(11013.2441, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11010.9970703125
tensor(11013.2441, grad_fn=<NegBackward0>) tensor(11010.9971, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11009.2802734375
tensor(11010.9971, grad_fn=<NegBackward0>) tensor(11009.2803, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11008.6337890625
tensor(11009.2803, grad_fn=<NegBackward0>) tensor(11008.6338, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11008.158203125
tensor(11008.6338, grad_fn=<NegBackward0>) tensor(11008.1582, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11007.73828125
tensor(11008.1582, grad_fn=<NegBackward0>) tensor(11007.7383, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11007.4013671875
tensor(11007.7383, grad_fn=<NegBackward0>) tensor(11007.4014, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11007.185546875
tensor(11007.4014, grad_fn=<NegBackward0>) tensor(11007.1855, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11007.0654296875
tensor(11007.1855, grad_fn=<NegBackward0>) tensor(11007.0654, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11006.9970703125
tensor(11007.0654, grad_fn=<NegBackward0>) tensor(11006.9971, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11006.962890625
tensor(11006.9971, grad_fn=<NegBackward0>) tensor(11006.9629, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11006.9453125
tensor(11006.9629, grad_fn=<NegBackward0>) tensor(11006.9453, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11006.9345703125
tensor(11006.9453, grad_fn=<NegBackward0>) tensor(11006.9346, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11006.9267578125
tensor(11006.9346, grad_fn=<NegBackward0>) tensor(11006.9268, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11006.9208984375
tensor(11006.9268, grad_fn=<NegBackward0>) tensor(11006.9209, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11006.91796875
tensor(11006.9209, grad_fn=<NegBackward0>) tensor(11006.9180, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11006.9150390625
tensor(11006.9180, grad_fn=<NegBackward0>) tensor(11006.9150, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11006.912109375
tensor(11006.9150, grad_fn=<NegBackward0>) tensor(11006.9121, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11006.91015625
tensor(11006.9121, grad_fn=<NegBackward0>) tensor(11006.9102, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11006.9091796875
tensor(11006.9102, grad_fn=<NegBackward0>) tensor(11006.9092, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11006.908203125
tensor(11006.9092, grad_fn=<NegBackward0>) tensor(11006.9082, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11006.9052734375
tensor(11006.9082, grad_fn=<NegBackward0>) tensor(11006.9053, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11006.9072265625
tensor(11006.9053, grad_fn=<NegBackward0>) tensor(11006.9072, grad_fn=<NegBackward0>)
1
Iteration 2400: Loss = -11006.904296875
tensor(11006.9053, grad_fn=<NegBackward0>) tensor(11006.9043, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11006.9033203125
tensor(11006.9043, grad_fn=<NegBackward0>) tensor(11006.9033, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11006.9052734375
tensor(11006.9033, grad_fn=<NegBackward0>) tensor(11006.9053, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -11006.90234375
tensor(11006.9033, grad_fn=<NegBackward0>) tensor(11006.9023, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11006.9033203125
tensor(11006.9023, grad_fn=<NegBackward0>) tensor(11006.9033, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11006.9033203125
tensor(11006.9023, grad_fn=<NegBackward0>) tensor(11006.9033, grad_fn=<NegBackward0>)
2
Iteration 3000: Loss = -11006.90234375
tensor(11006.9023, grad_fn=<NegBackward0>) tensor(11006.9023, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11006.9033203125
tensor(11006.9023, grad_fn=<NegBackward0>) tensor(11006.9033, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11006.90234375
tensor(11006.9023, grad_fn=<NegBackward0>) tensor(11006.9023, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11006.9013671875
tensor(11006.9023, grad_fn=<NegBackward0>) tensor(11006.9014, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11006.9013671875
tensor(11006.9014, grad_fn=<NegBackward0>) tensor(11006.9014, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11006.9013671875
tensor(11006.9014, grad_fn=<NegBackward0>) tensor(11006.9014, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11006.9013671875
tensor(11006.9014, grad_fn=<NegBackward0>) tensor(11006.9014, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11006.900390625
tensor(11006.9014, grad_fn=<NegBackward0>) tensor(11006.9004, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11006.9013671875
tensor(11006.9004, grad_fn=<NegBackward0>) tensor(11006.9014, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11006.9013671875
tensor(11006.9004, grad_fn=<NegBackward0>) tensor(11006.9014, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -11006.9013671875
tensor(11006.9004, grad_fn=<NegBackward0>) tensor(11006.9014, grad_fn=<NegBackward0>)
3
Iteration 4100: Loss = -11006.8994140625
tensor(11006.9004, grad_fn=<NegBackward0>) tensor(11006.8994, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11006.9013671875
tensor(11006.8994, grad_fn=<NegBackward0>) tensor(11006.9014, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11006.900390625
tensor(11006.8994, grad_fn=<NegBackward0>) tensor(11006.9004, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -11006.9013671875
tensor(11006.8994, grad_fn=<NegBackward0>) tensor(11006.9014, grad_fn=<NegBackward0>)
3
Iteration 4500: Loss = -11006.900390625
tensor(11006.8994, grad_fn=<NegBackward0>) tensor(11006.9004, grad_fn=<NegBackward0>)
4
Iteration 4600: Loss = -11006.900390625
tensor(11006.8994, grad_fn=<NegBackward0>) tensor(11006.9004, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4600 due to no improvement.
pi: tensor([[0.0628, 0.9372],
        [0.0366, 0.9634]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9846, 0.0154], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1707, 0.1689],
         [0.7137, 0.1574]],

        [[0.6187, 0.2276],
         [0.5586, 0.6872]],

        [[0.5922, 0.1389],
         [0.7163, 0.5887]],

        [[0.6531, 0.1647],
         [0.5714, 0.5301]],

        [[0.6755, 0.2722],
         [0.6390, 0.5468]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0032454170932134756
Global Adjusted Rand Index: -0.0012751047570591862
Average Adjusted Rand Index: -0.0004873391686507823
[-0.0012751047570591862, -0.0012751047570591862] [-0.0004873391686507823, -0.0004873391686507823] [11006.9013671875, 11006.900390625]
-------------------------------------
This iteration is 53
True Objective function: Loss = -10787.70420293366
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23077.189453125
inf tensor(23077.1895, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10957.6494140625
tensor(23077.1895, grad_fn=<NegBackward0>) tensor(10957.6494, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10956.603515625
tensor(10957.6494, grad_fn=<NegBackward0>) tensor(10956.6035, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10956.267578125
tensor(10956.6035, grad_fn=<NegBackward0>) tensor(10956.2676, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10956.1494140625
tensor(10956.2676, grad_fn=<NegBackward0>) tensor(10956.1494, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10956.0947265625
tensor(10956.1494, grad_fn=<NegBackward0>) tensor(10956.0947, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10956.0625
tensor(10956.0947, grad_fn=<NegBackward0>) tensor(10956.0625, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10956.0380859375
tensor(10956.0625, grad_fn=<NegBackward0>) tensor(10956.0381, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10956.013671875
tensor(10956.0381, grad_fn=<NegBackward0>) tensor(10956.0137, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10955.986328125
tensor(10956.0137, grad_fn=<NegBackward0>) tensor(10955.9863, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10955.9521484375
tensor(10955.9863, grad_fn=<NegBackward0>) tensor(10955.9521, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10955.912109375
tensor(10955.9521, grad_fn=<NegBackward0>) tensor(10955.9121, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10955.86328125
tensor(10955.9121, grad_fn=<NegBackward0>) tensor(10955.8633, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10955.80859375
tensor(10955.8633, grad_fn=<NegBackward0>) tensor(10955.8086, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10955.755859375
tensor(10955.8086, grad_fn=<NegBackward0>) tensor(10955.7559, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10955.70703125
tensor(10955.7559, grad_fn=<NegBackward0>) tensor(10955.7070, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10955.6630859375
tensor(10955.7070, grad_fn=<NegBackward0>) tensor(10955.6631, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10955.62109375
tensor(10955.6631, grad_fn=<NegBackward0>) tensor(10955.6211, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10955.5751953125
tensor(10955.6211, grad_fn=<NegBackward0>) tensor(10955.5752, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10955.521484375
tensor(10955.5752, grad_fn=<NegBackward0>) tensor(10955.5215, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10955.4453125
tensor(10955.5215, grad_fn=<NegBackward0>) tensor(10955.4453, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10955.3583984375
tensor(10955.4453, grad_fn=<NegBackward0>) tensor(10955.3584, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10955.2705078125
tensor(10955.3584, grad_fn=<NegBackward0>) tensor(10955.2705, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10955.1904296875
tensor(10955.2705, grad_fn=<NegBackward0>) tensor(10955.1904, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10955.1005859375
tensor(10955.1904, grad_fn=<NegBackward0>) tensor(10955.1006, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10954.9912109375
tensor(10955.1006, grad_fn=<NegBackward0>) tensor(10954.9912, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10954.8037109375
tensor(10954.9912, grad_fn=<NegBackward0>) tensor(10954.8037, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10954.49609375
tensor(10954.8037, grad_fn=<NegBackward0>) tensor(10954.4961, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10954.205078125
tensor(10954.4961, grad_fn=<NegBackward0>) tensor(10954.2051, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10954.0068359375
tensor(10954.2051, grad_fn=<NegBackward0>) tensor(10954.0068, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10953.8173828125
tensor(10954.0068, grad_fn=<NegBackward0>) tensor(10953.8174, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10953.720703125
tensor(10953.8174, grad_fn=<NegBackward0>) tensor(10953.7207, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10953.662109375
tensor(10953.7207, grad_fn=<NegBackward0>) tensor(10953.6621, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10953.6181640625
tensor(10953.6621, grad_fn=<NegBackward0>) tensor(10953.6182, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10953.576171875
tensor(10953.6182, grad_fn=<NegBackward0>) tensor(10953.5762, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10952.5859375
tensor(10953.5762, grad_fn=<NegBackward0>) tensor(10952.5859, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10949.279296875
tensor(10952.5859, grad_fn=<NegBackward0>) tensor(10949.2793, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10947.79296875
tensor(10949.2793, grad_fn=<NegBackward0>) tensor(10947.7930, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10947.0654296875
tensor(10947.7930, grad_fn=<NegBackward0>) tensor(10947.0654, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10946.6689453125
tensor(10947.0654, grad_fn=<NegBackward0>) tensor(10946.6689, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10946.4287109375
tensor(10946.6689, grad_fn=<NegBackward0>) tensor(10946.4287, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10946.271484375
tensor(10946.4287, grad_fn=<NegBackward0>) tensor(10946.2715, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10946.1611328125
tensor(10946.2715, grad_fn=<NegBackward0>) tensor(10946.1611, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10946.0791015625
tensor(10946.1611, grad_fn=<NegBackward0>) tensor(10946.0791, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10946.0185546875
tensor(10946.0791, grad_fn=<NegBackward0>) tensor(10946.0186, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10945.970703125
tensor(10946.0186, grad_fn=<NegBackward0>) tensor(10945.9707, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10945.931640625
tensor(10945.9707, grad_fn=<NegBackward0>) tensor(10945.9316, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10945.8994140625
tensor(10945.9316, grad_fn=<NegBackward0>) tensor(10945.8994, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10945.873046875
tensor(10945.8994, grad_fn=<NegBackward0>) tensor(10945.8730, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10945.8505859375
tensor(10945.8730, grad_fn=<NegBackward0>) tensor(10945.8506, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10945.8330078125
tensor(10945.8506, grad_fn=<NegBackward0>) tensor(10945.8330, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10945.8154296875
tensor(10945.8330, grad_fn=<NegBackward0>) tensor(10945.8154, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10945.80078125
tensor(10945.8154, grad_fn=<NegBackward0>) tensor(10945.8008, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10945.7900390625
tensor(10945.8008, grad_fn=<NegBackward0>) tensor(10945.7900, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10945.7783203125
tensor(10945.7900, grad_fn=<NegBackward0>) tensor(10945.7783, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10945.76953125
tensor(10945.7783, grad_fn=<NegBackward0>) tensor(10945.7695, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10945.759765625
tensor(10945.7695, grad_fn=<NegBackward0>) tensor(10945.7598, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10945.7529296875
tensor(10945.7598, grad_fn=<NegBackward0>) tensor(10945.7529, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10945.74609375
tensor(10945.7529, grad_fn=<NegBackward0>) tensor(10945.7461, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10945.7392578125
tensor(10945.7461, grad_fn=<NegBackward0>) tensor(10945.7393, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10945.7333984375
tensor(10945.7393, grad_fn=<NegBackward0>) tensor(10945.7334, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10945.728515625
tensor(10945.7334, grad_fn=<NegBackward0>) tensor(10945.7285, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10945.72265625
tensor(10945.7285, grad_fn=<NegBackward0>) tensor(10945.7227, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10945.720703125
tensor(10945.7227, grad_fn=<NegBackward0>) tensor(10945.7207, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10945.7158203125
tensor(10945.7207, grad_fn=<NegBackward0>) tensor(10945.7158, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10945.712890625
tensor(10945.7158, grad_fn=<NegBackward0>) tensor(10945.7129, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10945.7099609375
tensor(10945.7129, grad_fn=<NegBackward0>) tensor(10945.7100, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10945.7060546875
tensor(10945.7100, grad_fn=<NegBackward0>) tensor(10945.7061, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10945.703125
tensor(10945.7061, grad_fn=<NegBackward0>) tensor(10945.7031, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10945.7001953125
tensor(10945.7031, grad_fn=<NegBackward0>) tensor(10945.7002, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10945.697265625
tensor(10945.7002, grad_fn=<NegBackward0>) tensor(10945.6973, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10945.6962890625
tensor(10945.6973, grad_fn=<NegBackward0>) tensor(10945.6963, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10945.6943359375
tensor(10945.6963, grad_fn=<NegBackward0>) tensor(10945.6943, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10945.6923828125
tensor(10945.6943, grad_fn=<NegBackward0>) tensor(10945.6924, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10945.6904296875
tensor(10945.6924, grad_fn=<NegBackward0>) tensor(10945.6904, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10945.7041015625
tensor(10945.6904, grad_fn=<NegBackward0>) tensor(10945.7041, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10945.6865234375
tensor(10945.6904, grad_fn=<NegBackward0>) tensor(10945.6865, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10945.6845703125
tensor(10945.6865, grad_fn=<NegBackward0>) tensor(10945.6846, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10945.68359375
tensor(10945.6846, grad_fn=<NegBackward0>) tensor(10945.6836, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10945.6826171875
tensor(10945.6836, grad_fn=<NegBackward0>) tensor(10945.6826, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10945.681640625
tensor(10945.6826, grad_fn=<NegBackward0>) tensor(10945.6816, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10945.6806640625
tensor(10945.6816, grad_fn=<NegBackward0>) tensor(10945.6807, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10945.6796875
tensor(10945.6807, grad_fn=<NegBackward0>) tensor(10945.6797, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10945.6767578125
tensor(10945.6797, grad_fn=<NegBackward0>) tensor(10945.6768, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10945.6767578125
tensor(10945.6768, grad_fn=<NegBackward0>) tensor(10945.6768, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10945.68359375
tensor(10945.6768, grad_fn=<NegBackward0>) tensor(10945.6836, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -10945.67578125
tensor(10945.6768, grad_fn=<NegBackward0>) tensor(10945.6758, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10945.673828125
tensor(10945.6758, grad_fn=<NegBackward0>) tensor(10945.6738, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10945.7626953125
tensor(10945.6738, grad_fn=<NegBackward0>) tensor(10945.7627, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10945.6728515625
tensor(10945.6738, grad_fn=<NegBackward0>) tensor(10945.6729, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10945.671875
tensor(10945.6729, grad_fn=<NegBackward0>) tensor(10945.6719, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10945.6708984375
tensor(10945.6719, grad_fn=<NegBackward0>) tensor(10945.6709, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10945.671875
tensor(10945.6709, grad_fn=<NegBackward0>) tensor(10945.6719, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -10945.669921875
tensor(10945.6709, grad_fn=<NegBackward0>) tensor(10945.6699, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10945.669921875
tensor(10945.6699, grad_fn=<NegBackward0>) tensor(10945.6699, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10945.697265625
tensor(10945.6699, grad_fn=<NegBackward0>) tensor(10945.6973, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -10945.6689453125
tensor(10945.6699, grad_fn=<NegBackward0>) tensor(10945.6689, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10945.666015625
tensor(10945.6689, grad_fn=<NegBackward0>) tensor(10945.6660, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10945.6728515625
tensor(10945.6660, grad_fn=<NegBackward0>) tensor(10945.6729, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -10945.66796875
tensor(10945.6660, grad_fn=<NegBackward0>) tensor(10945.6680, grad_fn=<NegBackward0>)
2
pi: tensor([[9.9998e-01, 2.1883e-05],
        [3.9514e-03, 9.9605e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0195, 0.9805], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.9997, 0.1379],
         [0.5899, 0.1606]],

        [[0.6304, 0.1752],
         [0.5881, 0.6884]],

        [[0.5587, 0.1890],
         [0.7218, 0.7191]],

        [[0.7177, 0.1993],
         [0.6098, 0.5376]],

        [[0.6394, 0.1600],
         [0.7096, 0.5701]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.00035341041046094813
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.004371511787304062
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.002427096100870114
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.006467401572035518
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
Global Adjusted Rand Index: 0.0013251815974001855
Average Adjusted Rand Index: -0.0015974049994707834
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23121.1640625
inf tensor(23121.1641, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10956.8291015625
tensor(23121.1641, grad_fn=<NegBackward0>) tensor(10956.8291, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10956.3974609375
tensor(10956.8291, grad_fn=<NegBackward0>) tensor(10956.3975, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10956.255859375
tensor(10956.3975, grad_fn=<NegBackward0>) tensor(10956.2559, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10956.15625
tensor(10956.2559, grad_fn=<NegBackward0>) tensor(10956.1562, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10956.0625
tensor(10956.1562, grad_fn=<NegBackward0>) tensor(10956.0625, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10955.943359375
tensor(10956.0625, grad_fn=<NegBackward0>) tensor(10955.9434, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10955.744140625
tensor(10955.9434, grad_fn=<NegBackward0>) tensor(10955.7441, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10955.1298828125
tensor(10955.7441, grad_fn=<NegBackward0>) tensor(10955.1299, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10953.0810546875
tensor(10955.1299, grad_fn=<NegBackward0>) tensor(10953.0811, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10952.3427734375
tensor(10953.0811, grad_fn=<NegBackward0>) tensor(10952.3428, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10952.0419921875
tensor(10952.3428, grad_fn=<NegBackward0>) tensor(10952.0420, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10951.8525390625
tensor(10952.0420, grad_fn=<NegBackward0>) tensor(10951.8525, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10951.7158203125
tensor(10951.8525, grad_fn=<NegBackward0>) tensor(10951.7158, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10951.6123046875
tensor(10951.7158, grad_fn=<NegBackward0>) tensor(10951.6123, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10951.53515625
tensor(10951.6123, grad_fn=<NegBackward0>) tensor(10951.5352, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10951.4765625
tensor(10951.5352, grad_fn=<NegBackward0>) tensor(10951.4766, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10951.4306640625
tensor(10951.4766, grad_fn=<NegBackward0>) tensor(10951.4307, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10951.3955078125
tensor(10951.4307, grad_fn=<NegBackward0>) tensor(10951.3955, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10951.37109375
tensor(10951.3955, grad_fn=<NegBackward0>) tensor(10951.3711, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10951.349609375
tensor(10951.3711, grad_fn=<NegBackward0>) tensor(10951.3496, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10951.3330078125
tensor(10951.3496, grad_fn=<NegBackward0>) tensor(10951.3330, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10951.3203125
tensor(10951.3330, grad_fn=<NegBackward0>) tensor(10951.3203, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10951.3134765625
tensor(10951.3203, grad_fn=<NegBackward0>) tensor(10951.3135, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10951.3095703125
tensor(10951.3135, grad_fn=<NegBackward0>) tensor(10951.3096, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10951.3046875
tensor(10951.3096, grad_fn=<NegBackward0>) tensor(10951.3047, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10951.30078125
tensor(10951.3047, grad_fn=<NegBackward0>) tensor(10951.3008, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10951.2978515625
tensor(10951.3008, grad_fn=<NegBackward0>) tensor(10951.2979, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10951.296875
tensor(10951.2979, grad_fn=<NegBackward0>) tensor(10951.2969, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10951.294921875
tensor(10951.2969, grad_fn=<NegBackward0>) tensor(10951.2949, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10951.29296875
tensor(10951.2949, grad_fn=<NegBackward0>) tensor(10951.2930, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10951.2919921875
tensor(10951.2930, grad_fn=<NegBackward0>) tensor(10951.2920, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10951.2919921875
tensor(10951.2920, grad_fn=<NegBackward0>) tensor(10951.2920, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10951.2900390625
tensor(10951.2920, grad_fn=<NegBackward0>) tensor(10951.2900, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10951.2890625
tensor(10951.2900, grad_fn=<NegBackward0>) tensor(10951.2891, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10951.2890625
tensor(10951.2891, grad_fn=<NegBackward0>) tensor(10951.2891, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10951.2861328125
tensor(10951.2891, grad_fn=<NegBackward0>) tensor(10951.2861, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10951.287109375
tensor(10951.2861, grad_fn=<NegBackward0>) tensor(10951.2871, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -10951.28515625
tensor(10951.2861, grad_fn=<NegBackward0>) tensor(10951.2852, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10951.283203125
tensor(10951.2852, grad_fn=<NegBackward0>) tensor(10951.2832, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10951.28515625
tensor(10951.2832, grad_fn=<NegBackward0>) tensor(10951.2852, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -10951.283203125
tensor(10951.2832, grad_fn=<NegBackward0>) tensor(10951.2832, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10951.2822265625
tensor(10951.2832, grad_fn=<NegBackward0>) tensor(10951.2822, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10951.2822265625
tensor(10951.2822, grad_fn=<NegBackward0>) tensor(10951.2822, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10951.283203125
tensor(10951.2822, grad_fn=<NegBackward0>) tensor(10951.2832, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10951.28125
tensor(10951.2822, grad_fn=<NegBackward0>) tensor(10951.2812, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10951.28125
tensor(10951.2812, grad_fn=<NegBackward0>) tensor(10951.2812, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10951.2802734375
tensor(10951.2812, grad_fn=<NegBackward0>) tensor(10951.2803, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10951.279296875
tensor(10951.2803, grad_fn=<NegBackward0>) tensor(10951.2793, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10951.279296875
tensor(10951.2793, grad_fn=<NegBackward0>) tensor(10951.2793, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10951.2783203125
tensor(10951.2793, grad_fn=<NegBackward0>) tensor(10951.2783, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10951.279296875
tensor(10951.2783, grad_fn=<NegBackward0>) tensor(10951.2793, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10951.27734375
tensor(10951.2783, grad_fn=<NegBackward0>) tensor(10951.2773, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10951.279296875
tensor(10951.2773, grad_fn=<NegBackward0>) tensor(10951.2793, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10951.2763671875
tensor(10951.2773, grad_fn=<NegBackward0>) tensor(10951.2764, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10951.2763671875
tensor(10951.2764, grad_fn=<NegBackward0>) tensor(10951.2764, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10951.2744140625
tensor(10951.2764, grad_fn=<NegBackward0>) tensor(10951.2744, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10951.27734375
tensor(10951.2744, grad_fn=<NegBackward0>) tensor(10951.2773, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10951.2763671875
tensor(10951.2744, grad_fn=<NegBackward0>) tensor(10951.2764, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -10951.2763671875
tensor(10951.2744, grad_fn=<NegBackward0>) tensor(10951.2764, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -10951.275390625
tensor(10951.2744, grad_fn=<NegBackward0>) tensor(10951.2754, grad_fn=<NegBackward0>)
4
Iteration 6100: Loss = -10951.2763671875
tensor(10951.2744, grad_fn=<NegBackward0>) tensor(10951.2764, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6100 due to no improvement.
pi: tensor([[0.0028, 0.9972],
        [0.0678, 0.9322]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0089, 0.9911], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1995, 0.1683],
         [0.6238, 0.1628]],

        [[0.6865, 0.1794],
         [0.6195, 0.5773]],

        [[0.7041, 0.1904],
         [0.5679, 0.5101]],

        [[0.5513, 0.0763],
         [0.5492, 0.5688]],

        [[0.6488, 0.1589],
         [0.5649, 0.5554]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.02548983702618568
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0006432713465041949
Average Adjusted Rand Index: 0.005097967405237136
[0.0013251815974001855, 0.0006432713465041949] [-0.0015974049994707834, 0.005097967405237136] [10945.6650390625, 10951.2763671875]
-------------------------------------
This iteration is 54
True Objective function: Loss = -10966.054624680504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20116.373046875
inf tensor(20116.3730, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11052.109375
tensor(20116.3730, grad_fn=<NegBackward0>) tensor(11052.1094, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11051.0146484375
tensor(11052.1094, grad_fn=<NegBackward0>) tensor(11051.0146, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11049.9443359375
tensor(11051.0146, grad_fn=<NegBackward0>) tensor(11049.9443, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11049.583984375
tensor(11049.9443, grad_fn=<NegBackward0>) tensor(11049.5840, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11049.4296875
tensor(11049.5840, grad_fn=<NegBackward0>) tensor(11049.4297, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11049.3056640625
tensor(11049.4297, grad_fn=<NegBackward0>) tensor(11049.3057, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11049.1796875
tensor(11049.3057, grad_fn=<NegBackward0>) tensor(11049.1797, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11049.0625
tensor(11049.1797, grad_fn=<NegBackward0>) tensor(11049.0625, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11048.9345703125
tensor(11049.0625, grad_fn=<NegBackward0>) tensor(11048.9346, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11048.796875
tensor(11048.9346, grad_fn=<NegBackward0>) tensor(11048.7969, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11048.666015625
tensor(11048.7969, grad_fn=<NegBackward0>) tensor(11048.6660, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11048.51171875
tensor(11048.6660, grad_fn=<NegBackward0>) tensor(11048.5117, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11048.2607421875
tensor(11048.5117, grad_fn=<NegBackward0>) tensor(11048.2607, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11047.7470703125
tensor(11048.2607, grad_fn=<NegBackward0>) tensor(11047.7471, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11046.4462890625
tensor(11047.7471, grad_fn=<NegBackward0>) tensor(11046.4463, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10936.3994140625
tensor(11046.4463, grad_fn=<NegBackward0>) tensor(10936.3994, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10928.7236328125
tensor(10936.3994, grad_fn=<NegBackward0>) tensor(10928.7236, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10928.5732421875
tensor(10928.7236, grad_fn=<NegBackward0>) tensor(10928.5732, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10928.51171875
tensor(10928.5732, grad_fn=<NegBackward0>) tensor(10928.5117, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10928.455078125
tensor(10928.5117, grad_fn=<NegBackward0>) tensor(10928.4551, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10928.2685546875
tensor(10928.4551, grad_fn=<NegBackward0>) tensor(10928.2686, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10928.2421875
tensor(10928.2686, grad_fn=<NegBackward0>) tensor(10928.2422, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10928.2158203125
tensor(10928.2422, grad_fn=<NegBackward0>) tensor(10928.2158, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10927.9228515625
tensor(10928.2158, grad_fn=<NegBackward0>) tensor(10927.9229, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10927.8671875
tensor(10927.9229, grad_fn=<NegBackward0>) tensor(10927.8672, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10927.8642578125
tensor(10927.8672, grad_fn=<NegBackward0>) tensor(10927.8643, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10927.8603515625
tensor(10927.8643, grad_fn=<NegBackward0>) tensor(10927.8604, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10927.8583984375
tensor(10927.8604, grad_fn=<NegBackward0>) tensor(10927.8584, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10927.85546875
tensor(10927.8584, grad_fn=<NegBackward0>) tensor(10927.8555, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10927.853515625
tensor(10927.8555, grad_fn=<NegBackward0>) tensor(10927.8535, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10927.8505859375
tensor(10927.8535, grad_fn=<NegBackward0>) tensor(10927.8506, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10927.84765625
tensor(10927.8506, grad_fn=<NegBackward0>) tensor(10927.8477, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10927.8466796875
tensor(10927.8477, grad_fn=<NegBackward0>) tensor(10927.8467, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10927.845703125
tensor(10927.8467, grad_fn=<NegBackward0>) tensor(10927.8457, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10927.84375
tensor(10927.8457, grad_fn=<NegBackward0>) tensor(10927.8438, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10927.8427734375
tensor(10927.8438, grad_fn=<NegBackward0>) tensor(10927.8428, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10927.841796875
tensor(10927.8428, grad_fn=<NegBackward0>) tensor(10927.8418, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10927.841796875
tensor(10927.8418, grad_fn=<NegBackward0>) tensor(10927.8418, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10927.8408203125
tensor(10927.8418, grad_fn=<NegBackward0>) tensor(10927.8408, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10927.83984375
tensor(10927.8408, grad_fn=<NegBackward0>) tensor(10927.8398, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10927.83984375
tensor(10927.8398, grad_fn=<NegBackward0>) tensor(10927.8398, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10927.8388671875
tensor(10927.8398, grad_fn=<NegBackward0>) tensor(10927.8389, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10927.8388671875
tensor(10927.8389, grad_fn=<NegBackward0>) tensor(10927.8389, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10927.83984375
tensor(10927.8389, grad_fn=<NegBackward0>) tensor(10927.8398, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10927.8369140625
tensor(10927.8389, grad_fn=<NegBackward0>) tensor(10927.8369, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10927.8349609375
tensor(10927.8369, grad_fn=<NegBackward0>) tensor(10927.8350, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10927.8359375
tensor(10927.8350, grad_fn=<NegBackward0>) tensor(10927.8359, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10927.8232421875
tensor(10927.8350, grad_fn=<NegBackward0>) tensor(10927.8232, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10927.8232421875
tensor(10927.8232, grad_fn=<NegBackward0>) tensor(10927.8232, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10927.8251953125
tensor(10927.8232, grad_fn=<NegBackward0>) tensor(10927.8252, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10927.822265625
tensor(10927.8232, grad_fn=<NegBackward0>) tensor(10927.8223, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10927.8251953125
tensor(10927.8223, grad_fn=<NegBackward0>) tensor(10927.8252, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10927.82421875
tensor(10927.8223, grad_fn=<NegBackward0>) tensor(10927.8242, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -10927.822265625
tensor(10927.8223, grad_fn=<NegBackward0>) tensor(10927.8223, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10927.822265625
tensor(10927.8223, grad_fn=<NegBackward0>) tensor(10927.8223, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10927.8212890625
tensor(10927.8223, grad_fn=<NegBackward0>) tensor(10927.8213, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10927.822265625
tensor(10927.8213, grad_fn=<NegBackward0>) tensor(10927.8223, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10927.82421875
tensor(10927.8213, grad_fn=<NegBackward0>) tensor(10927.8242, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -10927.822265625
tensor(10927.8213, grad_fn=<NegBackward0>) tensor(10927.8223, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -10927.8232421875
tensor(10927.8213, grad_fn=<NegBackward0>) tensor(10927.8232, grad_fn=<NegBackward0>)
4
Iteration 6100: Loss = -10927.8212890625
tensor(10927.8213, grad_fn=<NegBackward0>) tensor(10927.8213, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10927.8212890625
tensor(10927.8213, grad_fn=<NegBackward0>) tensor(10927.8213, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10927.8212890625
tensor(10927.8213, grad_fn=<NegBackward0>) tensor(10927.8213, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10927.8203125
tensor(10927.8213, grad_fn=<NegBackward0>) tensor(10927.8203, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10927.8212890625
tensor(10927.8203, grad_fn=<NegBackward0>) tensor(10927.8213, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10927.8212890625
tensor(10927.8203, grad_fn=<NegBackward0>) tensor(10927.8213, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -10927.8212890625
tensor(10927.8203, grad_fn=<NegBackward0>) tensor(10927.8213, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -10927.8291015625
tensor(10927.8203, grad_fn=<NegBackward0>) tensor(10927.8291, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -10927.8212890625
tensor(10927.8203, grad_fn=<NegBackward0>) tensor(10927.8213, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6900 due to no improvement.
pi: tensor([[0.7592, 0.2408],
        [0.2187, 0.7813]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4194, 0.5806], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2049, 0.1086],
         [0.6803, 0.2394]],

        [[0.5023, 0.0928],
         [0.6404, 0.6019]],

        [[0.5373, 0.0982],
         [0.6440, 0.5951]],

        [[0.5583, 0.1013],
         [0.5487, 0.5001]],

        [[0.6247, 0.1076],
         [0.5289, 0.6952]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9207907017983009
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7717887095983128
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080965973782139
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7720997823854144
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.8460850150498936
Average Adjusted Rand Index: 0.8465544999593384
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23899.037109375
inf tensor(23899.0371, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11052.1142578125
tensor(23899.0371, grad_fn=<NegBackward0>) tensor(11052.1143, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11051.5732421875
tensor(11052.1143, grad_fn=<NegBackward0>) tensor(11051.5732, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11051.4462890625
tensor(11051.5732, grad_fn=<NegBackward0>) tensor(11051.4463, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11051.2841796875
tensor(11051.4463, grad_fn=<NegBackward0>) tensor(11051.2842, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11050.6494140625
tensor(11051.2842, grad_fn=<NegBackward0>) tensor(11050.6494, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11049.3388671875
tensor(11050.6494, grad_fn=<NegBackward0>) tensor(11049.3389, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11049.1103515625
tensor(11049.3389, grad_fn=<NegBackward0>) tensor(11049.1104, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11048.9423828125
tensor(11049.1104, grad_fn=<NegBackward0>) tensor(11048.9424, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11048.806640625
tensor(11048.9424, grad_fn=<NegBackward0>) tensor(11048.8066, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11048.6435546875
tensor(11048.8066, grad_fn=<NegBackward0>) tensor(11048.6436, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11048.4169921875
tensor(11048.6436, grad_fn=<NegBackward0>) tensor(11048.4170, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11047.8544921875
tensor(11048.4170, grad_fn=<NegBackward0>) tensor(11047.8545, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11026.8525390625
tensor(11047.8545, grad_fn=<NegBackward0>) tensor(11026.8525, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10928.96875
tensor(11026.8525, grad_fn=<NegBackward0>) tensor(10928.9688, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10928.701171875
tensor(10928.9688, grad_fn=<NegBackward0>) tensor(10928.7012, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10928.26953125
tensor(10928.7012, grad_fn=<NegBackward0>) tensor(10928.2695, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10928.25
tensor(10928.2695, grad_fn=<NegBackward0>) tensor(10928.2500, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10928.2080078125
tensor(10928.2500, grad_fn=<NegBackward0>) tensor(10928.2080, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10928.1787109375
tensor(10928.2080, grad_fn=<NegBackward0>) tensor(10928.1787, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10928.1474609375
tensor(10928.1787, grad_fn=<NegBackward0>) tensor(10928.1475, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10928.01953125
tensor(10928.1475, grad_fn=<NegBackward0>) tensor(10928.0195, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10928.013671875
tensor(10928.0195, grad_fn=<NegBackward0>) tensor(10928.0137, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10928.0107421875
tensor(10928.0137, grad_fn=<NegBackward0>) tensor(10928.0107, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10928.0068359375
tensor(10928.0107, grad_fn=<NegBackward0>) tensor(10928.0068, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10928.005859375
tensor(10928.0068, grad_fn=<NegBackward0>) tensor(10928.0059, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10928.0029296875
tensor(10928.0059, grad_fn=<NegBackward0>) tensor(10928.0029, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10927.890625
tensor(10928.0029, grad_fn=<NegBackward0>) tensor(10927.8906, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10927.83984375
tensor(10927.8906, grad_fn=<NegBackward0>) tensor(10927.8398, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10927.83984375
tensor(10927.8398, grad_fn=<NegBackward0>) tensor(10927.8398, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10927.8388671875
tensor(10927.8398, grad_fn=<NegBackward0>) tensor(10927.8389, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10927.837890625
tensor(10927.8389, grad_fn=<NegBackward0>) tensor(10927.8379, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10927.837890625
tensor(10927.8379, grad_fn=<NegBackward0>) tensor(10927.8379, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10927.8359375
tensor(10927.8379, grad_fn=<NegBackward0>) tensor(10927.8359, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10927.833984375
tensor(10927.8359, grad_fn=<NegBackward0>) tensor(10927.8340, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10927.833984375
tensor(10927.8340, grad_fn=<NegBackward0>) tensor(10927.8340, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10927.8330078125
tensor(10927.8340, grad_fn=<NegBackward0>) tensor(10927.8330, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10927.83203125
tensor(10927.8330, grad_fn=<NegBackward0>) tensor(10927.8320, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10927.8310546875
tensor(10927.8320, grad_fn=<NegBackward0>) tensor(10927.8311, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10927.828125
tensor(10927.8311, grad_fn=<NegBackward0>) tensor(10927.8281, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10927.826171875
tensor(10927.8281, grad_fn=<NegBackward0>) tensor(10927.8262, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10927.8251953125
tensor(10927.8262, grad_fn=<NegBackward0>) tensor(10927.8252, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10927.8251953125
tensor(10927.8252, grad_fn=<NegBackward0>) tensor(10927.8252, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10927.82421875
tensor(10927.8252, grad_fn=<NegBackward0>) tensor(10927.8242, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10927.8271484375
tensor(10927.8242, grad_fn=<NegBackward0>) tensor(10927.8271, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10927.82421875
tensor(10927.8242, grad_fn=<NegBackward0>) tensor(10927.8242, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10927.8232421875
tensor(10927.8242, grad_fn=<NegBackward0>) tensor(10927.8232, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10927.8203125
tensor(10927.8232, grad_fn=<NegBackward0>) tensor(10927.8203, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10927.8212890625
tensor(10927.8203, grad_fn=<NegBackward0>) tensor(10927.8213, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10927.8212890625
tensor(10927.8203, grad_fn=<NegBackward0>) tensor(10927.8213, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -10927.822265625
tensor(10927.8203, grad_fn=<NegBackward0>) tensor(10927.8223, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -10927.8193359375
tensor(10927.8203, grad_fn=<NegBackward0>) tensor(10927.8193, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10927.8173828125
tensor(10927.8193, grad_fn=<NegBackward0>) tensor(10927.8174, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10927.81640625
tensor(10927.8174, grad_fn=<NegBackward0>) tensor(10927.8164, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10927.8173828125
tensor(10927.8164, grad_fn=<NegBackward0>) tensor(10927.8174, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10927.81640625
tensor(10927.8164, grad_fn=<NegBackward0>) tensor(10927.8164, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10927.81640625
tensor(10927.8164, grad_fn=<NegBackward0>) tensor(10927.8164, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10927.8154296875
tensor(10927.8164, grad_fn=<NegBackward0>) tensor(10927.8154, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10927.81640625
tensor(10927.8154, grad_fn=<NegBackward0>) tensor(10927.8164, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10927.81640625
tensor(10927.8154, grad_fn=<NegBackward0>) tensor(10927.8164, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -10927.81640625
tensor(10927.8154, grad_fn=<NegBackward0>) tensor(10927.8164, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -10927.8173828125
tensor(10927.8154, grad_fn=<NegBackward0>) tensor(10927.8174, grad_fn=<NegBackward0>)
4
Iteration 6200: Loss = -10927.81640625
tensor(10927.8154, grad_fn=<NegBackward0>) tensor(10927.8164, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6200 due to no improvement.
pi: tensor([[0.7822, 0.2178],
        [0.2392, 0.7608]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5800, 0.4200], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2394, 0.1086],
         [0.5661, 0.2049]],

        [[0.6739, 0.0928],
         [0.5062, 0.6152]],

        [[0.6301, 0.0982],
         [0.6742, 0.5745]],

        [[0.6333, 0.1013],
         [0.6441, 0.6751]],

        [[0.5840, 0.1076],
         [0.6947, 0.7157]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9207907017983009
time is 1
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7717887095983128
time is 2
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080965973782139
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7720997823854144
time is 4
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.8460850150498936
Average Adjusted Rand Index: 0.8465544999593384
[0.8460850150498936, 0.8460850150498936] [0.8465544999593384, 0.8465544999593384] [10927.8212890625, 10927.81640625]
-------------------------------------
This iteration is 55
True Objective function: Loss = -10870.243872567185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22566.79296875
inf tensor(22566.7930, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10967.7119140625
tensor(22566.7930, grad_fn=<NegBackward0>) tensor(10967.7119, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10967.14453125
tensor(10967.7119, grad_fn=<NegBackward0>) tensor(10967.1445, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10966.849609375
tensor(10967.1445, grad_fn=<NegBackward0>) tensor(10966.8496, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10966.12109375
tensor(10966.8496, grad_fn=<NegBackward0>) tensor(10966.1211, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10964.7509765625
tensor(10966.1211, grad_fn=<NegBackward0>) tensor(10964.7510, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10964.029296875
tensor(10964.7510, grad_fn=<NegBackward0>) tensor(10964.0293, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10963.2177734375
tensor(10964.0293, grad_fn=<NegBackward0>) tensor(10963.2178, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10961.8876953125
tensor(10963.2178, grad_fn=<NegBackward0>) tensor(10961.8877, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10956.8388671875
tensor(10961.8877, grad_fn=<NegBackward0>) tensor(10956.8389, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10836.2900390625
tensor(10956.8389, grad_fn=<NegBackward0>) tensor(10836.2900, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10827.98046875
tensor(10836.2900, grad_fn=<NegBackward0>) tensor(10827.9805, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10826.1455078125
tensor(10827.9805, grad_fn=<NegBackward0>) tensor(10826.1455, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10826.1123046875
tensor(10826.1455, grad_fn=<NegBackward0>) tensor(10826.1123, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10826.080078125
tensor(10826.1123, grad_fn=<NegBackward0>) tensor(10826.0801, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10826.0634765625
tensor(10826.0801, grad_fn=<NegBackward0>) tensor(10826.0635, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10826.0498046875
tensor(10826.0635, grad_fn=<NegBackward0>) tensor(10826.0498, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10826.0458984375
tensor(10826.0498, grad_fn=<NegBackward0>) tensor(10826.0459, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10826.041015625
tensor(10826.0459, grad_fn=<NegBackward0>) tensor(10826.0410, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10826.0400390625
tensor(10826.0410, grad_fn=<NegBackward0>) tensor(10826.0400, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10826.037109375
tensor(10826.0400, grad_fn=<NegBackward0>) tensor(10826.0371, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10826.033203125
tensor(10826.0371, grad_fn=<NegBackward0>) tensor(10826.0332, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10825.99609375
tensor(10826.0332, grad_fn=<NegBackward0>) tensor(10825.9961, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10825.9921875
tensor(10825.9961, grad_fn=<NegBackward0>) tensor(10825.9922, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10825.9912109375
tensor(10825.9922, grad_fn=<NegBackward0>) tensor(10825.9912, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10825.9892578125
tensor(10825.9912, grad_fn=<NegBackward0>) tensor(10825.9893, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10825.9892578125
tensor(10825.9893, grad_fn=<NegBackward0>) tensor(10825.9893, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10825.9892578125
tensor(10825.9893, grad_fn=<NegBackward0>) tensor(10825.9893, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10825.9873046875
tensor(10825.9893, grad_fn=<NegBackward0>) tensor(10825.9873, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10825.986328125
tensor(10825.9873, grad_fn=<NegBackward0>) tensor(10825.9863, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10825.984375
tensor(10825.9863, grad_fn=<NegBackward0>) tensor(10825.9844, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10825.984375
tensor(10825.9844, grad_fn=<NegBackward0>) tensor(10825.9844, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10825.982421875
tensor(10825.9844, grad_fn=<NegBackward0>) tensor(10825.9824, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10825.9814453125
tensor(10825.9824, grad_fn=<NegBackward0>) tensor(10825.9814, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10825.984375
tensor(10825.9814, grad_fn=<NegBackward0>) tensor(10825.9844, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -10825.98046875
tensor(10825.9814, grad_fn=<NegBackward0>) tensor(10825.9805, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10825.8857421875
tensor(10825.9805, grad_fn=<NegBackward0>) tensor(10825.8857, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10825.88671875
tensor(10825.8857, grad_fn=<NegBackward0>) tensor(10825.8867, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -10825.884765625
tensor(10825.8857, grad_fn=<NegBackward0>) tensor(10825.8848, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10825.8857421875
tensor(10825.8848, grad_fn=<NegBackward0>) tensor(10825.8857, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -10825.8857421875
tensor(10825.8848, grad_fn=<NegBackward0>) tensor(10825.8857, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -10825.8857421875
tensor(10825.8848, grad_fn=<NegBackward0>) tensor(10825.8857, grad_fn=<NegBackward0>)
3
Iteration 4200: Loss = -10825.8857421875
tensor(10825.8848, grad_fn=<NegBackward0>) tensor(10825.8857, grad_fn=<NegBackward0>)
4
Iteration 4300: Loss = -10825.8837890625
tensor(10825.8848, grad_fn=<NegBackward0>) tensor(10825.8838, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10825.8857421875
tensor(10825.8838, grad_fn=<NegBackward0>) tensor(10825.8857, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10825.884765625
tensor(10825.8838, grad_fn=<NegBackward0>) tensor(10825.8848, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -10825.8837890625
tensor(10825.8838, grad_fn=<NegBackward0>) tensor(10825.8838, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10825.8837890625
tensor(10825.8838, grad_fn=<NegBackward0>) tensor(10825.8838, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10825.8837890625
tensor(10825.8838, grad_fn=<NegBackward0>) tensor(10825.8838, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10825.888671875
tensor(10825.8838, grad_fn=<NegBackward0>) tensor(10825.8887, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10825.8828125
tensor(10825.8838, grad_fn=<NegBackward0>) tensor(10825.8828, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10825.8828125
tensor(10825.8828, grad_fn=<NegBackward0>) tensor(10825.8828, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10825.8896484375
tensor(10825.8828, grad_fn=<NegBackward0>) tensor(10825.8896, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10825.8837890625
tensor(10825.8828, grad_fn=<NegBackward0>) tensor(10825.8838, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -10825.8857421875
tensor(10825.8828, grad_fn=<NegBackward0>) tensor(10825.8857, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -10825.8818359375
tensor(10825.8828, grad_fn=<NegBackward0>) tensor(10825.8818, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10825.8876953125
tensor(10825.8818, grad_fn=<NegBackward0>) tensor(10825.8877, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10825.8828125
tensor(10825.8818, grad_fn=<NegBackward0>) tensor(10825.8828, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -10825.88671875
tensor(10825.8818, grad_fn=<NegBackward0>) tensor(10825.8867, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -10825.8857421875
tensor(10825.8818, grad_fn=<NegBackward0>) tensor(10825.8857, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -10825.8828125
tensor(10825.8818, grad_fn=<NegBackward0>) tensor(10825.8828, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6000 due to no improvement.
pi: tensor([[0.7500, 0.2500],
        [0.2430, 0.7570]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5763, 0.4237], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2466, 0.1008],
         [0.6324, 0.2005]],

        [[0.6251, 0.0937],
         [0.6217, 0.6604]],

        [[0.5766, 0.1022],
         [0.6190, 0.6653]],

        [[0.5988, 0.0990],
         [0.6665, 0.6779]],

        [[0.6578, 0.0973],
         [0.6139, 0.7215]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 8
Adjusted Rand Index: 0.7026262626262626
time is 1
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448326530612245
time is 3
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448351863643042
Global Adjusted Rand Index: 0.846092191331029
Average Adjusted Rand Index: 0.8469430106224966
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22675.90234375
inf tensor(22675.9023, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10968.5556640625
tensor(22675.9023, grad_fn=<NegBackward0>) tensor(10968.5557, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10967.2109375
tensor(10968.5557, grad_fn=<NegBackward0>) tensor(10967.2109, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10966.3408203125
tensor(10967.2109, grad_fn=<NegBackward0>) tensor(10966.3408, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10965.439453125
tensor(10966.3408, grad_fn=<NegBackward0>) tensor(10965.4395, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10965.0390625
tensor(10965.4395, grad_fn=<NegBackward0>) tensor(10965.0391, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10964.70703125
tensor(10965.0391, grad_fn=<NegBackward0>) tensor(10964.7070, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10963.8203125
tensor(10964.7070, grad_fn=<NegBackward0>) tensor(10963.8203, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10963.3203125
tensor(10963.8203, grad_fn=<NegBackward0>) tensor(10963.3203, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10962.8505859375
tensor(10963.3203, grad_fn=<NegBackward0>) tensor(10962.8506, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10961.966796875
tensor(10962.8506, grad_fn=<NegBackward0>) tensor(10961.9668, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10959.126953125
tensor(10961.9668, grad_fn=<NegBackward0>) tensor(10959.1270, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10940.734375
tensor(10959.1270, grad_fn=<NegBackward0>) tensor(10940.7344, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10929.6875
tensor(10940.7344, grad_fn=<NegBackward0>) tensor(10929.6875, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10839.7431640625
tensor(10929.6875, grad_fn=<NegBackward0>) tensor(10839.7432, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10831.5029296875
tensor(10839.7432, grad_fn=<NegBackward0>) tensor(10831.5029, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10829.49609375
tensor(10831.5029, grad_fn=<NegBackward0>) tensor(10829.4961, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10829.3251953125
tensor(10829.4961, grad_fn=<NegBackward0>) tensor(10829.3252, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10829.0498046875
tensor(10829.3252, grad_fn=<NegBackward0>) tensor(10829.0498, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10828.6650390625
tensor(10829.0498, grad_fn=<NegBackward0>) tensor(10828.6650, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10827.345703125
tensor(10828.6650, grad_fn=<NegBackward0>) tensor(10827.3457, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10827.056640625
tensor(10827.3457, grad_fn=<NegBackward0>) tensor(10827.0566, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10827.0390625
tensor(10827.0566, grad_fn=<NegBackward0>) tensor(10827.0391, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10827.0263671875
tensor(10827.0391, grad_fn=<NegBackward0>) tensor(10827.0264, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10827.01953125
tensor(10827.0264, grad_fn=<NegBackward0>) tensor(10827.0195, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10827.0126953125
tensor(10827.0195, grad_fn=<NegBackward0>) tensor(10827.0127, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10827.0078125
tensor(10827.0127, grad_fn=<NegBackward0>) tensor(10827.0078, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10827.0048828125
tensor(10827.0078, grad_fn=<NegBackward0>) tensor(10827.0049, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10827.0
tensor(10827.0049, grad_fn=<NegBackward0>) tensor(10827., grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10826.9970703125
tensor(10827., grad_fn=<NegBackward0>) tensor(10826.9971, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10826.9931640625
tensor(10826.9971, grad_fn=<NegBackward0>) tensor(10826.9932, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10826.990234375
tensor(10826.9932, grad_fn=<NegBackward0>) tensor(10826.9902, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10826.9833984375
tensor(10826.9902, grad_fn=<NegBackward0>) tensor(10826.9834, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10826.8359375
tensor(10826.9834, grad_fn=<NegBackward0>) tensor(10826.8359, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10826.833984375
tensor(10826.8359, grad_fn=<NegBackward0>) tensor(10826.8340, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10826.8310546875
tensor(10826.8340, grad_fn=<NegBackward0>) tensor(10826.8311, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10826.830078125
tensor(10826.8311, grad_fn=<NegBackward0>) tensor(10826.8301, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10826.826171875
tensor(10826.8301, grad_fn=<NegBackward0>) tensor(10826.8262, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10826.8291015625
tensor(10826.8262, grad_fn=<NegBackward0>) tensor(10826.8291, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -10826.552734375
tensor(10826.8262, grad_fn=<NegBackward0>) tensor(10826.5527, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10826.4921875
tensor(10826.5527, grad_fn=<NegBackward0>) tensor(10826.4922, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10826.490234375
tensor(10826.4922, grad_fn=<NegBackward0>) tensor(10826.4902, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10826.48828125
tensor(10826.4902, grad_fn=<NegBackward0>) tensor(10826.4883, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10826.4873046875
tensor(10826.4883, grad_fn=<NegBackward0>) tensor(10826.4873, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10826.482421875
tensor(10826.4873, grad_fn=<NegBackward0>) tensor(10826.4824, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10826.4853515625
tensor(10826.4824, grad_fn=<NegBackward0>) tensor(10826.4854, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10826.478515625
tensor(10826.4824, grad_fn=<NegBackward0>) tensor(10826.4785, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10826.48046875
tensor(10826.4785, grad_fn=<NegBackward0>) tensor(10826.4805, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10826.4716796875
tensor(10826.4785, grad_fn=<NegBackward0>) tensor(10826.4717, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10826.46875
tensor(10826.4717, grad_fn=<NegBackward0>) tensor(10826.4688, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10826.46875
tensor(10826.4688, grad_fn=<NegBackward0>) tensor(10826.4688, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10826.466796875
tensor(10826.4688, grad_fn=<NegBackward0>) tensor(10826.4668, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10826.46484375
tensor(10826.4668, grad_fn=<NegBackward0>) tensor(10826.4648, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10826.4638671875
tensor(10826.4648, grad_fn=<NegBackward0>) tensor(10826.4639, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10825.984375
tensor(10826.4639, grad_fn=<NegBackward0>) tensor(10825.9844, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10825.9296875
tensor(10825.9844, grad_fn=<NegBackward0>) tensor(10825.9297, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10825.9326171875
tensor(10825.9297, grad_fn=<NegBackward0>) tensor(10825.9326, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10825.927734375
tensor(10825.9297, grad_fn=<NegBackward0>) tensor(10825.9277, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10825.92578125
tensor(10825.9277, grad_fn=<NegBackward0>) tensor(10825.9258, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10825.931640625
tensor(10825.9258, grad_fn=<NegBackward0>) tensor(10825.9316, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10825.9248046875
tensor(10825.9258, grad_fn=<NegBackward0>) tensor(10825.9248, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10825.9248046875
tensor(10825.9248, grad_fn=<NegBackward0>) tensor(10825.9248, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10825.923828125
tensor(10825.9248, grad_fn=<NegBackward0>) tensor(10825.9238, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10825.923828125
tensor(10825.9238, grad_fn=<NegBackward0>) tensor(10825.9238, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10825.9228515625
tensor(10825.9238, grad_fn=<NegBackward0>) tensor(10825.9229, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10825.923828125
tensor(10825.9229, grad_fn=<NegBackward0>) tensor(10825.9238, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10825.923828125
tensor(10825.9229, grad_fn=<NegBackward0>) tensor(10825.9238, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -10825.923828125
tensor(10825.9229, grad_fn=<NegBackward0>) tensor(10825.9238, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -10825.923828125
tensor(10825.9229, grad_fn=<NegBackward0>) tensor(10825.9238, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -10825.9228515625
tensor(10825.9229, grad_fn=<NegBackward0>) tensor(10825.9229, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10825.923828125
tensor(10825.9229, grad_fn=<NegBackward0>) tensor(10825.9238, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10825.9208984375
tensor(10825.9229, grad_fn=<NegBackward0>) tensor(10825.9209, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10825.9208984375
tensor(10825.9209, grad_fn=<NegBackward0>) tensor(10825.9209, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10825.9208984375
tensor(10825.9209, grad_fn=<NegBackward0>) tensor(10825.9209, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10825.921875
tensor(10825.9209, grad_fn=<NegBackward0>) tensor(10825.9219, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10825.9189453125
tensor(10825.9209, grad_fn=<NegBackward0>) tensor(10825.9189, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10825.9208984375
tensor(10825.9189, grad_fn=<NegBackward0>) tensor(10825.9209, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10825.9189453125
tensor(10825.9189, grad_fn=<NegBackward0>) tensor(10825.9189, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10825.9189453125
tensor(10825.9189, grad_fn=<NegBackward0>) tensor(10825.9189, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10826.0390625
tensor(10825.9189, grad_fn=<NegBackward0>) tensor(10826.0391, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10825.919921875
tensor(10825.9189, grad_fn=<NegBackward0>) tensor(10825.9199, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -10826.0625
tensor(10825.9189, grad_fn=<NegBackward0>) tensor(10826.0625, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -10825.91796875
tensor(10825.9189, grad_fn=<NegBackward0>) tensor(10825.9180, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10825.91796875
tensor(10825.9180, grad_fn=<NegBackward0>) tensor(10825.9180, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10825.9189453125
tensor(10825.9180, grad_fn=<NegBackward0>) tensor(10825.9189, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -10825.9189453125
tensor(10825.9180, grad_fn=<NegBackward0>) tensor(10825.9189, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -10825.9248046875
tensor(10825.9180, grad_fn=<NegBackward0>) tensor(10825.9248, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -10825.912109375
tensor(10825.9180, grad_fn=<NegBackward0>) tensor(10825.9121, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10825.9619140625
tensor(10825.9121, grad_fn=<NegBackward0>) tensor(10825.9619, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10825.91015625
tensor(10825.9121, grad_fn=<NegBackward0>) tensor(10825.9102, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10825.91015625
tensor(10825.9102, grad_fn=<NegBackward0>) tensor(10825.9102, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10825.90625
tensor(10825.9102, grad_fn=<NegBackward0>) tensor(10825.9062, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10825.9326171875
tensor(10825.9062, grad_fn=<NegBackward0>) tensor(10825.9326, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -10825.9013671875
tensor(10825.9062, grad_fn=<NegBackward0>) tensor(10825.9014, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10825.912109375
tensor(10825.9014, grad_fn=<NegBackward0>) tensor(10825.9121, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -10825.8974609375
tensor(10825.9014, grad_fn=<NegBackward0>) tensor(10825.8975, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10825.8916015625
tensor(10825.8975, grad_fn=<NegBackward0>) tensor(10825.8916, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10825.890625
tensor(10825.8916, grad_fn=<NegBackward0>) tensor(10825.8906, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10825.890625
tensor(10825.8906, grad_fn=<NegBackward0>) tensor(10825.8906, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10825.890625
tensor(10825.8906, grad_fn=<NegBackward0>) tensor(10825.8906, grad_fn=<NegBackward0>)
pi: tensor([[0.7493, 0.2507],
        [0.2432, 0.7568]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5735, 0.4265], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2475, 0.1010],
         [0.6183, 0.1999]],

        [[0.6170, 0.0937],
         [0.7253, 0.6550]],

        [[0.5319, 0.1025],
         [0.6915, 0.6185]],

        [[0.7092, 0.0990],
         [0.5915, 0.6620]],

        [[0.5014, 0.0977],
         [0.5289, 0.5007]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 8
Adjusted Rand Index: 0.7026262626262626
time is 1
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448326530612245
time is 3
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448351863643042
Global Adjusted Rand Index: 0.846092191331029
Average Adjusted Rand Index: 0.8469430106224966
[0.846092191331029, 0.846092191331029] [0.8469430106224966, 0.8469430106224966] [10825.8828125, 10825.919921875]
-------------------------------------
This iteration is 56
True Objective function: Loss = -10961.783932153518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21980.978515625
inf tensor(21980.9785, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11024.8876953125
tensor(21980.9785, grad_fn=<NegBackward0>) tensor(11024.8877, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11023.873046875
tensor(11024.8877, grad_fn=<NegBackward0>) tensor(11023.8730, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11023.369140625
tensor(11023.8730, grad_fn=<NegBackward0>) tensor(11023.3691, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11020.966796875
tensor(11023.3691, grad_fn=<NegBackward0>) tensor(11020.9668, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11020.576171875
tensor(11020.9668, grad_fn=<NegBackward0>) tensor(11020.5762, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11020.41796875
tensor(11020.5762, grad_fn=<NegBackward0>) tensor(11020.4180, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11020.302734375
tensor(11020.4180, grad_fn=<NegBackward0>) tensor(11020.3027, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11020.1943359375
tensor(11020.3027, grad_fn=<NegBackward0>) tensor(11020.1943, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11020.0947265625
tensor(11020.1943, grad_fn=<NegBackward0>) tensor(11020.0947, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11020.02734375
tensor(11020.0947, grad_fn=<NegBackward0>) tensor(11020.0273, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11019.9892578125
tensor(11020.0273, grad_fn=<NegBackward0>) tensor(11019.9893, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11019.966796875
tensor(11019.9893, grad_fn=<NegBackward0>) tensor(11019.9668, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11019.9521484375
tensor(11019.9668, grad_fn=<NegBackward0>) tensor(11019.9521, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11019.939453125
tensor(11019.9521, grad_fn=<NegBackward0>) tensor(11019.9395, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11019.927734375
tensor(11019.9395, grad_fn=<NegBackward0>) tensor(11019.9277, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11019.91796875
tensor(11019.9277, grad_fn=<NegBackward0>) tensor(11019.9180, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11019.904296875
tensor(11019.9180, grad_fn=<NegBackward0>) tensor(11019.9043, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11019.890625
tensor(11019.9043, grad_fn=<NegBackward0>) tensor(11019.8906, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11019.873046875
tensor(11019.8906, grad_fn=<NegBackward0>) tensor(11019.8730, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11019.853515625
tensor(11019.8730, grad_fn=<NegBackward0>) tensor(11019.8535, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11019.826171875
tensor(11019.8535, grad_fn=<NegBackward0>) tensor(11019.8262, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11019.794921875
tensor(11019.8262, grad_fn=<NegBackward0>) tensor(11019.7949, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11019.755859375
tensor(11019.7949, grad_fn=<NegBackward0>) tensor(11019.7559, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11019.7138671875
tensor(11019.7559, grad_fn=<NegBackward0>) tensor(11019.7139, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11019.666015625
tensor(11019.7139, grad_fn=<NegBackward0>) tensor(11019.6660, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11019.6171875
tensor(11019.6660, grad_fn=<NegBackward0>) tensor(11019.6172, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11019.56640625
tensor(11019.6172, grad_fn=<NegBackward0>) tensor(11019.5664, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11019.51953125
tensor(11019.5664, grad_fn=<NegBackward0>) tensor(11019.5195, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11019.4814453125
tensor(11019.5195, grad_fn=<NegBackward0>) tensor(11019.4814, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11019.45703125
tensor(11019.4814, grad_fn=<NegBackward0>) tensor(11019.4570, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11019.4404296875
tensor(11019.4570, grad_fn=<NegBackward0>) tensor(11019.4404, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11019.4208984375
tensor(11019.4404, grad_fn=<NegBackward0>) tensor(11019.4209, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11019.40234375
tensor(11019.4209, grad_fn=<NegBackward0>) tensor(11019.4023, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11019.384765625
tensor(11019.4023, grad_fn=<NegBackward0>) tensor(11019.3848, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11019.3740234375
tensor(11019.3848, grad_fn=<NegBackward0>) tensor(11019.3740, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11019.3662109375
tensor(11019.3740, grad_fn=<NegBackward0>) tensor(11019.3662, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11019.3603515625
tensor(11019.3662, grad_fn=<NegBackward0>) tensor(11019.3604, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11019.357421875
tensor(11019.3604, grad_fn=<NegBackward0>) tensor(11019.3574, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11019.3564453125
tensor(11019.3574, grad_fn=<NegBackward0>) tensor(11019.3564, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11019.3544921875
tensor(11019.3564, grad_fn=<NegBackward0>) tensor(11019.3545, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11019.353515625
tensor(11019.3545, grad_fn=<NegBackward0>) tensor(11019.3535, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11019.3525390625
tensor(11019.3535, grad_fn=<NegBackward0>) tensor(11019.3525, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11019.3505859375
tensor(11019.3525, grad_fn=<NegBackward0>) tensor(11019.3506, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11019.3515625
tensor(11019.3506, grad_fn=<NegBackward0>) tensor(11019.3516, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11019.349609375
tensor(11019.3506, grad_fn=<NegBackward0>) tensor(11019.3496, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11019.34765625
tensor(11019.3496, grad_fn=<NegBackward0>) tensor(11019.3477, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11019.349609375
tensor(11019.3477, grad_fn=<NegBackward0>) tensor(11019.3496, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11019.34765625
tensor(11019.3477, grad_fn=<NegBackward0>) tensor(11019.3477, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11019.3486328125
tensor(11019.3477, grad_fn=<NegBackward0>) tensor(11019.3486, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11019.345703125
tensor(11019.3477, grad_fn=<NegBackward0>) tensor(11019.3457, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11019.34765625
tensor(11019.3457, grad_fn=<NegBackward0>) tensor(11019.3477, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11019.345703125
tensor(11019.3457, grad_fn=<NegBackward0>) tensor(11019.3457, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11019.345703125
tensor(11019.3457, grad_fn=<NegBackward0>) tensor(11019.3457, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11019.34765625
tensor(11019.3457, grad_fn=<NegBackward0>) tensor(11019.3477, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11019.34765625
tensor(11019.3457, grad_fn=<NegBackward0>) tensor(11019.3477, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11019.345703125
tensor(11019.3457, grad_fn=<NegBackward0>) tensor(11019.3457, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11019.345703125
tensor(11019.3457, grad_fn=<NegBackward0>) tensor(11019.3457, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11019.345703125
tensor(11019.3457, grad_fn=<NegBackward0>) tensor(11019.3457, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11019.3447265625
tensor(11019.3457, grad_fn=<NegBackward0>) tensor(11019.3447, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11019.3447265625
tensor(11019.3447, grad_fn=<NegBackward0>) tensor(11019.3447, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11019.3505859375
tensor(11019.3447, grad_fn=<NegBackward0>) tensor(11019.3506, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11019.3447265625
tensor(11019.3447, grad_fn=<NegBackward0>) tensor(11019.3447, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11019.34375
tensor(11019.3447, grad_fn=<NegBackward0>) tensor(11019.3438, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11019.3447265625
tensor(11019.3438, grad_fn=<NegBackward0>) tensor(11019.3447, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11019.3447265625
tensor(11019.3438, grad_fn=<NegBackward0>) tensor(11019.3447, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11019.3447265625
tensor(11019.3438, grad_fn=<NegBackward0>) tensor(11019.3447, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11019.3427734375
tensor(11019.3438, grad_fn=<NegBackward0>) tensor(11019.3428, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11019.34375
tensor(11019.3428, grad_fn=<NegBackward0>) tensor(11019.3438, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11019.34375
tensor(11019.3428, grad_fn=<NegBackward0>) tensor(11019.3438, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11019.3427734375
tensor(11019.3428, grad_fn=<NegBackward0>) tensor(11019.3428, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11019.3447265625
tensor(11019.3428, grad_fn=<NegBackward0>) tensor(11019.3447, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11019.34375
tensor(11019.3428, grad_fn=<NegBackward0>) tensor(11019.3438, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11019.341796875
tensor(11019.3428, grad_fn=<NegBackward0>) tensor(11019.3418, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11019.3427734375
tensor(11019.3418, grad_fn=<NegBackward0>) tensor(11019.3428, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11019.3427734375
tensor(11019.3418, grad_fn=<NegBackward0>) tensor(11019.3428, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11019.34375
tensor(11019.3418, grad_fn=<NegBackward0>) tensor(11019.3438, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11019.34375
tensor(11019.3418, grad_fn=<NegBackward0>) tensor(11019.3438, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -11019.3466796875
tensor(11019.3418, grad_fn=<NegBackward0>) tensor(11019.3467, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7800 due to no improvement.
pi: tensor([[0.8635, 0.1365],
        [0.9919, 0.0081]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9752, 0.0248], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1608, 0.2779],
         [0.5398, 0.1805]],

        [[0.5319, 0.1873],
         [0.7080, 0.6739]],

        [[0.5411, 0.1715],
         [0.5656, 0.6844]],

        [[0.6291, 0.1590],
         [0.5180, 0.5414]],

        [[0.6094, 0.1559],
         [0.5117, 0.7286]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.005119313988496187
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0002264028228132921
Average Adjusted Rand Index: -0.0010238627976992373
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23673.62109375
inf tensor(23673.6211, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11024.1328125
tensor(23673.6211, grad_fn=<NegBackward0>) tensor(11024.1328, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11023.3427734375
tensor(11024.1328, grad_fn=<NegBackward0>) tensor(11023.3428, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11020.732421875
tensor(11023.3428, grad_fn=<NegBackward0>) tensor(11020.7324, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11020.2900390625
tensor(11020.7324, grad_fn=<NegBackward0>) tensor(11020.2900, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11020.0693359375
tensor(11020.2900, grad_fn=<NegBackward0>) tensor(11020.0693, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11019.9931640625
tensor(11020.0693, grad_fn=<NegBackward0>) tensor(11019.9932, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11019.9638671875
tensor(11019.9932, grad_fn=<NegBackward0>) tensor(11019.9639, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11019.9462890625
tensor(11019.9639, grad_fn=<NegBackward0>) tensor(11019.9463, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11019.93359375
tensor(11019.9463, grad_fn=<NegBackward0>) tensor(11019.9336, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11019.9228515625
tensor(11019.9336, grad_fn=<NegBackward0>) tensor(11019.9229, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11019.912109375
tensor(11019.9229, grad_fn=<NegBackward0>) tensor(11019.9121, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11019.8994140625
tensor(11019.9121, grad_fn=<NegBackward0>) tensor(11019.8994, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11019.8818359375
tensor(11019.8994, grad_fn=<NegBackward0>) tensor(11019.8818, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11019.845703125
tensor(11019.8818, grad_fn=<NegBackward0>) tensor(11019.8457, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11019.70703125
tensor(11019.8457, grad_fn=<NegBackward0>) tensor(11019.7070, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11016.75
tensor(11019.7070, grad_fn=<NegBackward0>) tensor(11016.7500, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10948.94140625
tensor(11016.7500, grad_fn=<NegBackward0>) tensor(10948.9414, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10916.1279296875
tensor(10948.9414, grad_fn=<NegBackward0>) tensor(10916.1279, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10915.1640625
tensor(10916.1279, grad_fn=<NegBackward0>) tensor(10915.1641, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10915.0625
tensor(10915.1641, grad_fn=<NegBackward0>) tensor(10915.0625, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10914.95703125
tensor(10915.0625, grad_fn=<NegBackward0>) tensor(10914.9570, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10914.9326171875
tensor(10914.9570, grad_fn=<NegBackward0>) tensor(10914.9326, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10914.8212890625
tensor(10914.9326, grad_fn=<NegBackward0>) tensor(10914.8213, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10914.7880859375
tensor(10914.8213, grad_fn=<NegBackward0>) tensor(10914.7881, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10914.7783203125
tensor(10914.7881, grad_fn=<NegBackward0>) tensor(10914.7783, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10914.7685546875
tensor(10914.7783, grad_fn=<NegBackward0>) tensor(10914.7686, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10914.759765625
tensor(10914.7686, grad_fn=<NegBackward0>) tensor(10914.7598, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10914.748046875
tensor(10914.7598, grad_fn=<NegBackward0>) tensor(10914.7480, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10914.7294921875
tensor(10914.7480, grad_fn=<NegBackward0>) tensor(10914.7295, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10914.724609375
tensor(10914.7295, grad_fn=<NegBackward0>) tensor(10914.7246, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10914.720703125
tensor(10914.7246, grad_fn=<NegBackward0>) tensor(10914.7207, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10914.7138671875
tensor(10914.7207, grad_fn=<NegBackward0>) tensor(10914.7139, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10914.7109375
tensor(10914.7139, grad_fn=<NegBackward0>) tensor(10914.7109, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10914.70703125
tensor(10914.7109, grad_fn=<NegBackward0>) tensor(10914.7070, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10914.7041015625
tensor(10914.7070, grad_fn=<NegBackward0>) tensor(10914.7041, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10914.7021484375
tensor(10914.7041, grad_fn=<NegBackward0>) tensor(10914.7021, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10914.7001953125
tensor(10914.7021, grad_fn=<NegBackward0>) tensor(10914.7002, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10914.6962890625
tensor(10914.7002, grad_fn=<NegBackward0>) tensor(10914.6963, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10914.6943359375
tensor(10914.6963, grad_fn=<NegBackward0>) tensor(10914.6943, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10914.69921875
tensor(10914.6943, grad_fn=<NegBackward0>) tensor(10914.6992, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -10914.689453125
tensor(10914.6943, grad_fn=<NegBackward0>) tensor(10914.6895, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10914.6865234375
tensor(10914.6895, grad_fn=<NegBackward0>) tensor(10914.6865, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10914.6865234375
tensor(10914.6865, grad_fn=<NegBackward0>) tensor(10914.6865, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10914.685546875
tensor(10914.6865, grad_fn=<NegBackward0>) tensor(10914.6855, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10914.6845703125
tensor(10914.6855, grad_fn=<NegBackward0>) tensor(10914.6846, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10914.68359375
tensor(10914.6846, grad_fn=<NegBackward0>) tensor(10914.6836, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10914.68359375
tensor(10914.6836, grad_fn=<NegBackward0>) tensor(10914.6836, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10914.6865234375
tensor(10914.6836, grad_fn=<NegBackward0>) tensor(10914.6865, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10914.6845703125
tensor(10914.6836, grad_fn=<NegBackward0>) tensor(10914.6846, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -10914.6806640625
tensor(10914.6836, grad_fn=<NegBackward0>) tensor(10914.6807, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10914.67578125
tensor(10914.6807, grad_fn=<NegBackward0>) tensor(10914.6758, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10914.673828125
tensor(10914.6758, grad_fn=<NegBackward0>) tensor(10914.6738, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10914.6728515625
tensor(10914.6738, grad_fn=<NegBackward0>) tensor(10914.6729, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10914.6728515625
tensor(10914.6729, grad_fn=<NegBackward0>) tensor(10914.6729, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10914.6708984375
tensor(10914.6729, grad_fn=<NegBackward0>) tensor(10914.6709, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10914.671875
tensor(10914.6709, grad_fn=<NegBackward0>) tensor(10914.6719, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10914.6708984375
tensor(10914.6709, grad_fn=<NegBackward0>) tensor(10914.6709, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10914.669921875
tensor(10914.6709, grad_fn=<NegBackward0>) tensor(10914.6699, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10914.66796875
tensor(10914.6699, grad_fn=<NegBackward0>) tensor(10914.6680, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10914.666015625
tensor(10914.6680, grad_fn=<NegBackward0>) tensor(10914.6660, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10914.6669921875
tensor(10914.6660, grad_fn=<NegBackward0>) tensor(10914.6670, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10914.666015625
tensor(10914.6660, grad_fn=<NegBackward0>) tensor(10914.6660, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10914.6630859375
tensor(10914.6660, grad_fn=<NegBackward0>) tensor(10914.6631, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10914.6650390625
tensor(10914.6631, grad_fn=<NegBackward0>) tensor(10914.6650, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10914.662109375
tensor(10914.6631, grad_fn=<NegBackward0>) tensor(10914.6621, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10914.6591796875
tensor(10914.6621, grad_fn=<NegBackward0>) tensor(10914.6592, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10914.658203125
tensor(10914.6592, grad_fn=<NegBackward0>) tensor(10914.6582, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10914.6552734375
tensor(10914.6582, grad_fn=<NegBackward0>) tensor(10914.6553, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10914.6552734375
tensor(10914.6553, grad_fn=<NegBackward0>) tensor(10914.6553, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10914.6552734375
tensor(10914.6553, grad_fn=<NegBackward0>) tensor(10914.6553, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10914.654296875
tensor(10914.6553, grad_fn=<NegBackward0>) tensor(10914.6543, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10914.6533203125
tensor(10914.6543, grad_fn=<NegBackward0>) tensor(10914.6533, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10914.654296875
tensor(10914.6533, grad_fn=<NegBackward0>) tensor(10914.6543, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -10914.65234375
tensor(10914.6533, grad_fn=<NegBackward0>) tensor(10914.6523, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10914.6513671875
tensor(10914.6523, grad_fn=<NegBackward0>) tensor(10914.6514, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10914.65234375
tensor(10914.6514, grad_fn=<NegBackward0>) tensor(10914.6523, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10914.650390625
tensor(10914.6514, grad_fn=<NegBackward0>) tensor(10914.6504, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10914.666015625
tensor(10914.6504, grad_fn=<NegBackward0>) tensor(10914.6660, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10914.650390625
tensor(10914.6504, grad_fn=<NegBackward0>) tensor(10914.6504, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10914.6533203125
tensor(10914.6504, grad_fn=<NegBackward0>) tensor(10914.6533, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10914.650390625
tensor(10914.6504, grad_fn=<NegBackward0>) tensor(10914.6504, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10914.6513671875
tensor(10914.6504, grad_fn=<NegBackward0>) tensor(10914.6514, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10914.6513671875
tensor(10914.6504, grad_fn=<NegBackward0>) tensor(10914.6514, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -10914.650390625
tensor(10914.6504, grad_fn=<NegBackward0>) tensor(10914.6504, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10914.650390625
tensor(10914.6504, grad_fn=<NegBackward0>) tensor(10914.6504, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10914.650390625
tensor(10914.6504, grad_fn=<NegBackward0>) tensor(10914.6504, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10914.6240234375
tensor(10914.6504, grad_fn=<NegBackward0>) tensor(10914.6240, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10914.623046875
tensor(10914.6240, grad_fn=<NegBackward0>) tensor(10914.6230, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10914.6337890625
tensor(10914.6230, grad_fn=<NegBackward0>) tensor(10914.6338, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -10914.6259765625
tensor(10914.6230, grad_fn=<NegBackward0>) tensor(10914.6260, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -10914.623046875
tensor(10914.6230, grad_fn=<NegBackward0>) tensor(10914.6230, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10914.619140625
tensor(10914.6230, grad_fn=<NegBackward0>) tensor(10914.6191, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10914.6259765625
tensor(10914.6191, grad_fn=<NegBackward0>) tensor(10914.6260, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -10914.619140625
tensor(10914.6191, grad_fn=<NegBackward0>) tensor(10914.6191, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10914.6201171875
tensor(10914.6191, grad_fn=<NegBackward0>) tensor(10914.6201, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -10914.6396484375
tensor(10914.6191, grad_fn=<NegBackward0>) tensor(10914.6396, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -10914.619140625
tensor(10914.6191, grad_fn=<NegBackward0>) tensor(10914.6191, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10914.615234375
tensor(10914.6191, grad_fn=<NegBackward0>) tensor(10914.6152, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10914.615234375
tensor(10914.6152, grad_fn=<NegBackward0>) tensor(10914.6152, grad_fn=<NegBackward0>)
pi: tensor([[0.7897, 0.2103],
        [0.2574, 0.7426]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4498, 0.5502], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1921, 0.1032],
         [0.6259, 0.2544]],

        [[0.7154, 0.1102],
         [0.7175, 0.6221]],

        [[0.5978, 0.1070],
         [0.6546, 0.5158]],

        [[0.5637, 0.1029],
         [0.5052, 0.5806]],

        [[0.6433, 0.1010],
         [0.7050, 0.6396]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7721141809334062
time is 3
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080863220989386
time is 4
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.8758473252278274
Average Adjusted Rand Index: 0.876363123493757
[0.0002264028228132921, 0.8758473252278274] [-0.0010238627976992373, 0.876363123493757] [11019.3466796875, 10914.615234375]
-------------------------------------
This iteration is 57
True Objective function: Loss = -10985.539955719172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21397.56640625
inf tensor(21397.5664, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11151.9072265625
tensor(21397.5664, grad_fn=<NegBackward0>) tensor(11151.9072, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11150.95703125
tensor(11151.9072, grad_fn=<NegBackward0>) tensor(11150.9570, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11150.2412109375
tensor(11150.9570, grad_fn=<NegBackward0>) tensor(11150.2412, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11149.53515625
tensor(11150.2412, grad_fn=<NegBackward0>) tensor(11149.5352, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11148.2509765625
tensor(11149.5352, grad_fn=<NegBackward0>) tensor(11148.2510, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11146.8388671875
tensor(11148.2510, grad_fn=<NegBackward0>) tensor(11146.8389, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11142.685546875
tensor(11146.8389, grad_fn=<NegBackward0>) tensor(11142.6855, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11129.052734375
tensor(11142.6855, grad_fn=<NegBackward0>) tensor(11129.0527, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11121.287109375
tensor(11129.0527, grad_fn=<NegBackward0>) tensor(11121.2871, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11117.2197265625
tensor(11121.2871, grad_fn=<NegBackward0>) tensor(11117.2197, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11077.076171875
tensor(11117.2197, grad_fn=<NegBackward0>) tensor(11077.0762, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10964.591796875
tensor(11077.0762, grad_fn=<NegBackward0>) tensor(10964.5918, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10960.380859375
tensor(10964.5918, grad_fn=<NegBackward0>) tensor(10960.3809, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10956.7734375
tensor(10960.3809, grad_fn=<NegBackward0>) tensor(10956.7734, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10956.4794921875
tensor(10956.7734, grad_fn=<NegBackward0>) tensor(10956.4795, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10955.1611328125
tensor(10956.4795, grad_fn=<NegBackward0>) tensor(10955.1611, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10955.1025390625
tensor(10955.1611, grad_fn=<NegBackward0>) tensor(10955.1025, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10955.0322265625
tensor(10955.1025, grad_fn=<NegBackward0>) tensor(10955.0322, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10954.9921875
tensor(10955.0322, grad_fn=<NegBackward0>) tensor(10954.9922, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10954.9599609375
tensor(10954.9922, grad_fn=<NegBackward0>) tensor(10954.9600, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10954.9169921875
tensor(10954.9600, grad_fn=<NegBackward0>) tensor(10954.9170, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10953.9521484375
tensor(10954.9170, grad_fn=<NegBackward0>) tensor(10953.9521, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10953.876953125
tensor(10953.9521, grad_fn=<NegBackward0>) tensor(10953.8770, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10953.8642578125
tensor(10953.8770, grad_fn=<NegBackward0>) tensor(10953.8643, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10953.8125
tensor(10953.8643, grad_fn=<NegBackward0>) tensor(10953.8125, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10953.7666015625
tensor(10953.8125, grad_fn=<NegBackward0>) tensor(10953.7666, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10953.7578125
tensor(10953.7666, grad_fn=<NegBackward0>) tensor(10953.7578, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10953.75390625
tensor(10953.7578, grad_fn=<NegBackward0>) tensor(10953.7539, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10953.751953125
tensor(10953.7539, grad_fn=<NegBackward0>) tensor(10953.7520, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10953.7490234375
tensor(10953.7520, grad_fn=<NegBackward0>) tensor(10953.7490, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10953.748046875
tensor(10953.7490, grad_fn=<NegBackward0>) tensor(10953.7480, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10953.74609375
tensor(10953.7480, grad_fn=<NegBackward0>) tensor(10953.7461, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10953.744140625
tensor(10953.7461, grad_fn=<NegBackward0>) tensor(10953.7441, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10953.7421875
tensor(10953.7441, grad_fn=<NegBackward0>) tensor(10953.7422, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10953.73828125
tensor(10953.7422, grad_fn=<NegBackward0>) tensor(10953.7383, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10953.736328125
tensor(10953.7383, grad_fn=<NegBackward0>) tensor(10953.7363, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10953.7236328125
tensor(10953.7363, grad_fn=<NegBackward0>) tensor(10953.7236, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10953.7138671875
tensor(10953.7236, grad_fn=<NegBackward0>) tensor(10953.7139, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10953.7060546875
tensor(10953.7139, grad_fn=<NegBackward0>) tensor(10953.7061, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10953.6884765625
tensor(10953.7061, grad_fn=<NegBackward0>) tensor(10953.6885, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10953.685546875
tensor(10953.6885, grad_fn=<NegBackward0>) tensor(10953.6855, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10953.685546875
tensor(10953.6855, grad_fn=<NegBackward0>) tensor(10953.6855, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10953.6845703125
tensor(10953.6855, grad_fn=<NegBackward0>) tensor(10953.6846, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10953.681640625
tensor(10953.6846, grad_fn=<NegBackward0>) tensor(10953.6816, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10953.6796875
tensor(10953.6816, grad_fn=<NegBackward0>) tensor(10953.6797, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10953.6796875
tensor(10953.6797, grad_fn=<NegBackward0>) tensor(10953.6797, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10953.681640625
tensor(10953.6797, grad_fn=<NegBackward0>) tensor(10953.6816, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10953.6767578125
tensor(10953.6797, grad_fn=<NegBackward0>) tensor(10953.6768, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10953.5615234375
tensor(10953.6768, grad_fn=<NegBackward0>) tensor(10953.5615, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10953.5556640625
tensor(10953.5615, grad_fn=<NegBackward0>) tensor(10953.5557, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10953.5546875
tensor(10953.5557, grad_fn=<NegBackward0>) tensor(10953.5547, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10953.548828125
tensor(10953.5547, grad_fn=<NegBackward0>) tensor(10953.5488, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10953.55078125
tensor(10953.5488, grad_fn=<NegBackward0>) tensor(10953.5508, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10953.5400390625
tensor(10953.5488, grad_fn=<NegBackward0>) tensor(10953.5400, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10953.525390625
tensor(10953.5400, grad_fn=<NegBackward0>) tensor(10953.5254, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10953.525390625
tensor(10953.5254, grad_fn=<NegBackward0>) tensor(10953.5254, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10953.5234375
tensor(10953.5254, grad_fn=<NegBackward0>) tensor(10953.5234, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10953.5234375
tensor(10953.5234, grad_fn=<NegBackward0>) tensor(10953.5234, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10953.5263671875
tensor(10953.5234, grad_fn=<NegBackward0>) tensor(10953.5264, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10953.515625
tensor(10953.5234, grad_fn=<NegBackward0>) tensor(10953.5156, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10953.515625
tensor(10953.5156, grad_fn=<NegBackward0>) tensor(10953.5156, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10953.515625
tensor(10953.5156, grad_fn=<NegBackward0>) tensor(10953.5156, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10953.515625
tensor(10953.5156, grad_fn=<NegBackward0>) tensor(10953.5156, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10953.517578125
tensor(10953.5156, grad_fn=<NegBackward0>) tensor(10953.5176, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10953.515625
tensor(10953.5156, grad_fn=<NegBackward0>) tensor(10953.5156, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10953.517578125
tensor(10953.5156, grad_fn=<NegBackward0>) tensor(10953.5176, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10953.513671875
tensor(10953.5156, grad_fn=<NegBackward0>) tensor(10953.5137, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10953.515625
tensor(10953.5137, grad_fn=<NegBackward0>) tensor(10953.5156, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10953.513671875
tensor(10953.5137, grad_fn=<NegBackward0>) tensor(10953.5137, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10953.513671875
tensor(10953.5137, grad_fn=<NegBackward0>) tensor(10953.5137, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10953.5419921875
tensor(10953.5137, grad_fn=<NegBackward0>) tensor(10953.5420, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10953.5126953125
tensor(10953.5137, grad_fn=<NegBackward0>) tensor(10953.5127, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10953.51171875
tensor(10953.5127, grad_fn=<NegBackward0>) tensor(10953.5117, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10953.51171875
tensor(10953.5117, grad_fn=<NegBackward0>) tensor(10953.5117, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10953.5087890625
tensor(10953.5117, grad_fn=<NegBackward0>) tensor(10953.5088, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10953.5126953125
tensor(10953.5088, grad_fn=<NegBackward0>) tensor(10953.5127, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10953.513671875
tensor(10953.5088, grad_fn=<NegBackward0>) tensor(10953.5137, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -10953.5087890625
tensor(10953.5088, grad_fn=<NegBackward0>) tensor(10953.5088, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10953.5078125
tensor(10953.5088, grad_fn=<NegBackward0>) tensor(10953.5078, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10953.529296875
tensor(10953.5078, grad_fn=<NegBackward0>) tensor(10953.5293, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10953.5068359375
tensor(10953.5078, grad_fn=<NegBackward0>) tensor(10953.5068, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10953.5078125
tensor(10953.5068, grad_fn=<NegBackward0>) tensor(10953.5078, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10953.5126953125
tensor(10953.5068, grad_fn=<NegBackward0>) tensor(10953.5127, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -10953.5537109375
tensor(10953.5068, grad_fn=<NegBackward0>) tensor(10953.5537, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -10953.51953125
tensor(10953.5068, grad_fn=<NegBackward0>) tensor(10953.5195, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -10953.5654296875
tensor(10953.5068, grad_fn=<NegBackward0>) tensor(10953.5654, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[0.7396, 0.2604],
        [0.2311, 0.7689]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4437, 0.5563], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2037, 0.1023],
         [0.6632, 0.2608]],

        [[0.6811, 0.0966],
         [0.5103, 0.5808]],

        [[0.5724, 0.0992],
         [0.6778, 0.5891]],

        [[0.5317, 0.0931],
         [0.5846, 0.5189]],

        [[0.7092, 0.0965],
         [0.6911, 0.7026]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.844845420066659
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208
time is 3
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.9214413799558656
Average Adjusted Rand Index: 0.9212896971062332
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21670.740234375
inf tensor(21670.7402, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11151.7958984375
tensor(21670.7402, grad_fn=<NegBackward0>) tensor(11151.7959, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11150.9150390625
tensor(11151.7959, grad_fn=<NegBackward0>) tensor(11150.9150, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11149.8896484375
tensor(11150.9150, grad_fn=<NegBackward0>) tensor(11149.8896, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11148.552734375
tensor(11149.8896, grad_fn=<NegBackward0>) tensor(11148.5527, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11147.4345703125
tensor(11148.5527, grad_fn=<NegBackward0>) tensor(11147.4346, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11145.40625
tensor(11147.4346, grad_fn=<NegBackward0>) tensor(11145.4062, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11138.568359375
tensor(11145.4062, grad_fn=<NegBackward0>) tensor(11138.5684, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11123.2919921875
tensor(11138.5684, grad_fn=<NegBackward0>) tensor(11123.2920, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11120.9423828125
tensor(11123.2920, grad_fn=<NegBackward0>) tensor(11120.9424, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11117.634765625
tensor(11120.9424, grad_fn=<NegBackward0>) tensor(11117.6348, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11103.3212890625
tensor(11117.6348, grad_fn=<NegBackward0>) tensor(11103.3213, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10966.8369140625
tensor(11103.3213, grad_fn=<NegBackward0>) tensor(10966.8369, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10958.396484375
tensor(10966.8369, grad_fn=<NegBackward0>) tensor(10958.3965, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10957.7998046875
tensor(10958.3965, grad_fn=<NegBackward0>) tensor(10957.7998, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10956.310546875
tensor(10957.7998, grad_fn=<NegBackward0>) tensor(10956.3105, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10955.056640625
tensor(10956.3105, grad_fn=<NegBackward0>) tensor(10955.0566, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10954.9892578125
tensor(10955.0566, grad_fn=<NegBackward0>) tensor(10954.9893, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10954.94140625
tensor(10954.9893, grad_fn=<NegBackward0>) tensor(10954.9414, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10954.9072265625
tensor(10954.9414, grad_fn=<NegBackward0>) tensor(10954.9072, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10954.8564453125
tensor(10954.9072, grad_fn=<NegBackward0>) tensor(10954.8564, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10954.806640625
tensor(10954.8564, grad_fn=<NegBackward0>) tensor(10954.8066, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10954.7880859375
tensor(10954.8066, grad_fn=<NegBackward0>) tensor(10954.7881, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10954.763671875
tensor(10954.7881, grad_fn=<NegBackward0>) tensor(10954.7637, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10954.7314453125
tensor(10954.7637, grad_fn=<NegBackward0>) tensor(10954.7314, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10954.70703125
tensor(10954.7314, grad_fn=<NegBackward0>) tensor(10954.7070, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10954.6904296875
tensor(10954.7070, grad_fn=<NegBackward0>) tensor(10954.6904, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10954.662109375
tensor(10954.6904, grad_fn=<NegBackward0>) tensor(10954.6621, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10954.63671875
tensor(10954.6621, grad_fn=<NegBackward0>) tensor(10954.6367, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10954.6328125
tensor(10954.6367, grad_fn=<NegBackward0>) tensor(10954.6328, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10954.62890625
tensor(10954.6328, grad_fn=<NegBackward0>) tensor(10954.6289, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10954.6220703125
tensor(10954.6289, grad_fn=<NegBackward0>) tensor(10954.6221, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10954.607421875
tensor(10954.6221, grad_fn=<NegBackward0>) tensor(10954.6074, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10954.49609375
tensor(10954.6074, grad_fn=<NegBackward0>) tensor(10954.4961, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10954.4912109375
tensor(10954.4961, grad_fn=<NegBackward0>) tensor(10954.4912, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10954.4873046875
tensor(10954.4912, grad_fn=<NegBackward0>) tensor(10954.4873, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10954.484375
tensor(10954.4873, grad_fn=<NegBackward0>) tensor(10954.4844, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10954.474609375
tensor(10954.4844, grad_fn=<NegBackward0>) tensor(10954.4746, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10954.46484375
tensor(10954.4746, grad_fn=<NegBackward0>) tensor(10954.4648, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10954.4619140625
tensor(10954.4648, grad_fn=<NegBackward0>) tensor(10954.4619, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10954.458984375
tensor(10954.4619, grad_fn=<NegBackward0>) tensor(10954.4590, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10954.45703125
tensor(10954.4590, grad_fn=<NegBackward0>) tensor(10954.4570, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10954.4560546875
tensor(10954.4570, grad_fn=<NegBackward0>) tensor(10954.4561, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10954.4599609375
tensor(10954.4561, grad_fn=<NegBackward0>) tensor(10954.4600, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -10954.4541015625
tensor(10954.4561, grad_fn=<NegBackward0>) tensor(10954.4541, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10954.4560546875
tensor(10954.4541, grad_fn=<NegBackward0>) tensor(10954.4561, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10954.4521484375
tensor(10954.4541, grad_fn=<NegBackward0>) tensor(10954.4521, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10954.451171875
tensor(10954.4521, grad_fn=<NegBackward0>) tensor(10954.4512, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10954.4501953125
tensor(10954.4512, grad_fn=<NegBackward0>) tensor(10954.4502, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10954.4453125
tensor(10954.4502, grad_fn=<NegBackward0>) tensor(10954.4453, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10954.4462890625
tensor(10954.4453, grad_fn=<NegBackward0>) tensor(10954.4463, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10954.4462890625
tensor(10954.4453, grad_fn=<NegBackward0>) tensor(10954.4463, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -10954.447265625
tensor(10954.4453, grad_fn=<NegBackward0>) tensor(10954.4473, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -10954.4443359375
tensor(10954.4453, grad_fn=<NegBackward0>) tensor(10954.4443, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10954.4423828125
tensor(10954.4443, grad_fn=<NegBackward0>) tensor(10954.4424, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10954.443359375
tensor(10954.4424, grad_fn=<NegBackward0>) tensor(10954.4434, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10954.4423828125
tensor(10954.4424, grad_fn=<NegBackward0>) tensor(10954.4424, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10954.447265625
tensor(10954.4424, grad_fn=<NegBackward0>) tensor(10954.4473, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10954.4404296875
tensor(10954.4424, grad_fn=<NegBackward0>) tensor(10954.4404, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10954.3125
tensor(10954.4404, grad_fn=<NegBackward0>) tensor(10954.3125, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10954.3037109375
tensor(10954.3125, grad_fn=<NegBackward0>) tensor(10954.3037, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10954.302734375
tensor(10954.3037, grad_fn=<NegBackward0>) tensor(10954.3027, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10954.306640625
tensor(10954.3027, grad_fn=<NegBackward0>) tensor(10954.3066, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10954.302734375
tensor(10954.3027, grad_fn=<NegBackward0>) tensor(10954.3027, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10954.3037109375
tensor(10954.3027, grad_fn=<NegBackward0>) tensor(10954.3037, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10954.3037109375
tensor(10954.3027, grad_fn=<NegBackward0>) tensor(10954.3037, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -10954.302734375
tensor(10954.3027, grad_fn=<NegBackward0>) tensor(10954.3027, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10954.3017578125
tensor(10954.3027, grad_fn=<NegBackward0>) tensor(10954.3018, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10953.3974609375
tensor(10954.3018, grad_fn=<NegBackward0>) tensor(10953.3975, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10953.3876953125
tensor(10953.3975, grad_fn=<NegBackward0>) tensor(10953.3877, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10953.38671875
tensor(10953.3877, grad_fn=<NegBackward0>) tensor(10953.3867, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10953.3857421875
tensor(10953.3867, grad_fn=<NegBackward0>) tensor(10953.3857, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10953.375
tensor(10953.3857, grad_fn=<NegBackward0>) tensor(10953.3750, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10953.375
tensor(10953.3750, grad_fn=<NegBackward0>) tensor(10953.3750, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10953.490234375
tensor(10953.3750, grad_fn=<NegBackward0>) tensor(10953.4902, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10953.373046875
tensor(10953.3750, grad_fn=<NegBackward0>) tensor(10953.3730, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10953.3779296875
tensor(10953.3730, grad_fn=<NegBackward0>) tensor(10953.3779, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10953.373046875
tensor(10953.3730, grad_fn=<NegBackward0>) tensor(10953.3730, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10953.375
tensor(10953.3730, grad_fn=<NegBackward0>) tensor(10953.3750, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10953.3720703125
tensor(10953.3730, grad_fn=<NegBackward0>) tensor(10953.3721, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10953.3662109375
tensor(10953.3721, grad_fn=<NegBackward0>) tensor(10953.3662, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10953.365234375
tensor(10953.3662, grad_fn=<NegBackward0>) tensor(10953.3652, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10953.3544921875
tensor(10953.3652, grad_fn=<NegBackward0>) tensor(10953.3545, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10953.359375
tensor(10953.3545, grad_fn=<NegBackward0>) tensor(10953.3594, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10953.353515625
tensor(10953.3545, grad_fn=<NegBackward0>) tensor(10953.3535, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10953.353515625
tensor(10953.3535, grad_fn=<NegBackward0>) tensor(10953.3535, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10953.3544921875
tensor(10953.3535, grad_fn=<NegBackward0>) tensor(10953.3545, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10953.37109375
tensor(10953.3535, grad_fn=<NegBackward0>) tensor(10953.3711, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -10953.353515625
tensor(10953.3535, grad_fn=<NegBackward0>) tensor(10953.3535, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10953.353515625
tensor(10953.3535, grad_fn=<NegBackward0>) tensor(10953.3535, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10953.3505859375
tensor(10953.3535, grad_fn=<NegBackward0>) tensor(10953.3506, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10953.3525390625
tensor(10953.3506, grad_fn=<NegBackward0>) tensor(10953.3525, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10953.3681640625
tensor(10953.3506, grad_fn=<NegBackward0>) tensor(10953.3682, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -10953.35546875
tensor(10953.3506, grad_fn=<NegBackward0>) tensor(10953.3555, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -10953.3544921875
tensor(10953.3506, grad_fn=<NegBackward0>) tensor(10953.3545, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -10953.3515625
tensor(10953.3506, grad_fn=<NegBackward0>) tensor(10953.3516, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9500 due to no improvement.
pi: tensor([[0.7363, 0.2637],
        [0.2301, 0.7699]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4398, 0.5602], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2038, 0.1023],
         [0.7179, 0.2591]],

        [[0.6457, 0.0966],
         [0.5380, 0.6592]],

        [[0.6656, 0.0992],
         [0.7238, 0.6366]],

        [[0.7066, 0.0931],
         [0.7095, 0.6067]],

        [[0.7243, 0.0965],
         [0.5485, 0.5551]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.844845420066659
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208
time is 3
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.9291531268164688
Average Adjusted Rand Index: 0.9291280809446171
[0.9214413799558656, 0.9291531268164688] [0.9212896971062332, 0.9291280809446171] [10953.5654296875, 10953.3515625]
-------------------------------------
This iteration is 58
True Objective function: Loss = -11060.06011462653
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21060.892578125
inf tensor(21060.8926, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11193.3515625
tensor(21060.8926, grad_fn=<NegBackward0>) tensor(11193.3516, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11193.00390625
tensor(11193.3516, grad_fn=<NegBackward0>) tensor(11193.0039, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11192.4658203125
tensor(11193.0039, grad_fn=<NegBackward0>) tensor(11192.4658, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11191.353515625
tensor(11192.4658, grad_fn=<NegBackward0>) tensor(11191.3535, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11190.2900390625
tensor(11191.3535, grad_fn=<NegBackward0>) tensor(11190.2900, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11189.548828125
tensor(11190.2900, grad_fn=<NegBackward0>) tensor(11189.5488, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11188.9541015625
tensor(11189.5488, grad_fn=<NegBackward0>) tensor(11188.9541, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11187.6923828125
tensor(11188.9541, grad_fn=<NegBackward0>) tensor(11187.6924, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11150.912109375
tensor(11187.6924, grad_fn=<NegBackward0>) tensor(11150.9121, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11054.376953125
tensor(11150.9121, grad_fn=<NegBackward0>) tensor(11054.3770, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11029.1103515625
tensor(11054.3770, grad_fn=<NegBackward0>) tensor(11029.1104, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11027.4833984375
tensor(11029.1104, grad_fn=<NegBackward0>) tensor(11027.4834, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11027.3408203125
tensor(11027.4834, grad_fn=<NegBackward0>) tensor(11027.3408, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11027.27734375
tensor(11027.3408, grad_fn=<NegBackward0>) tensor(11027.2773, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11027.236328125
tensor(11027.2773, grad_fn=<NegBackward0>) tensor(11027.2363, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11027.1796875
tensor(11027.2363, grad_fn=<NegBackward0>) tensor(11027.1797, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11026.4658203125
tensor(11027.1797, grad_fn=<NegBackward0>) tensor(11026.4658, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11026.4453125
tensor(11026.4658, grad_fn=<NegBackward0>) tensor(11026.4453, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11026.423828125
tensor(11026.4453, grad_fn=<NegBackward0>) tensor(11026.4238, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11026.4072265625
tensor(11026.4238, grad_fn=<NegBackward0>) tensor(11026.4072, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11026.37109375
tensor(11026.4072, grad_fn=<NegBackward0>) tensor(11026.3711, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11026.3701171875
tensor(11026.3711, grad_fn=<NegBackward0>) tensor(11026.3701, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11026.345703125
tensor(11026.3701, grad_fn=<NegBackward0>) tensor(11026.3457, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11026.33984375
tensor(11026.3457, grad_fn=<NegBackward0>) tensor(11026.3398, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11026.3349609375
tensor(11026.3398, grad_fn=<NegBackward0>) tensor(11026.3350, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11026.33203125
tensor(11026.3350, grad_fn=<NegBackward0>) tensor(11026.3320, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11026.326171875
tensor(11026.3320, grad_fn=<NegBackward0>) tensor(11026.3262, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11026.3212890625
tensor(11026.3262, grad_fn=<NegBackward0>) tensor(11026.3213, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11026.310546875
tensor(11026.3213, grad_fn=<NegBackward0>) tensor(11026.3105, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11026.2998046875
tensor(11026.3105, grad_fn=<NegBackward0>) tensor(11026.2998, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11026.29296875
tensor(11026.2998, grad_fn=<NegBackward0>) tensor(11026.2930, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11026.2890625
tensor(11026.2930, grad_fn=<NegBackward0>) tensor(11026.2891, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11026.2763671875
tensor(11026.2891, grad_fn=<NegBackward0>) tensor(11026.2764, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11026.26953125
tensor(11026.2764, grad_fn=<NegBackward0>) tensor(11026.2695, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11026.265625
tensor(11026.2695, grad_fn=<NegBackward0>) tensor(11026.2656, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11026.265625
tensor(11026.2656, grad_fn=<NegBackward0>) tensor(11026.2656, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11026.2568359375
tensor(11026.2656, grad_fn=<NegBackward0>) tensor(11026.2568, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11026.25390625
tensor(11026.2568, grad_fn=<NegBackward0>) tensor(11026.2539, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11026.248046875
tensor(11026.2539, grad_fn=<NegBackward0>) tensor(11026.2480, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11026.248046875
tensor(11026.2480, grad_fn=<NegBackward0>) tensor(11026.2480, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11026.2470703125
tensor(11026.2480, grad_fn=<NegBackward0>) tensor(11026.2471, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11026.24609375
tensor(11026.2471, grad_fn=<NegBackward0>) tensor(11026.2461, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11026.25
tensor(11026.2461, grad_fn=<NegBackward0>) tensor(11026.2500, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11026.244140625
tensor(11026.2461, grad_fn=<NegBackward0>) tensor(11026.2441, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11026.2421875
tensor(11026.2441, grad_fn=<NegBackward0>) tensor(11026.2422, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11026.2412109375
tensor(11026.2422, grad_fn=<NegBackward0>) tensor(11026.2412, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11026.240234375
tensor(11026.2412, grad_fn=<NegBackward0>) tensor(11026.2402, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11026.2392578125
tensor(11026.2402, grad_fn=<NegBackward0>) tensor(11026.2393, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11026.236328125
tensor(11026.2393, grad_fn=<NegBackward0>) tensor(11026.2363, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11026.2333984375
tensor(11026.2363, grad_fn=<NegBackward0>) tensor(11026.2334, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11026.2353515625
tensor(11026.2334, grad_fn=<NegBackward0>) tensor(11026.2354, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11026.234375
tensor(11026.2334, grad_fn=<NegBackward0>) tensor(11026.2344, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11026.234375
tensor(11026.2334, grad_fn=<NegBackward0>) tensor(11026.2344, grad_fn=<NegBackward0>)
3
Iteration 5400: Loss = -11026.234375
tensor(11026.2334, grad_fn=<NegBackward0>) tensor(11026.2344, grad_fn=<NegBackward0>)
4
Iteration 5500: Loss = -11026.234375
tensor(11026.2334, grad_fn=<NegBackward0>) tensor(11026.2344, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5500 due to no improvement.
pi: tensor([[0.7220, 0.2780],
        [0.2976, 0.7024]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5225, 0.4775], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2621, 0.1026],
         [0.5811, 0.2044]],

        [[0.7294, 0.1064],
         [0.5013, 0.5911]],

        [[0.6066, 0.1011],
         [0.6642, 0.5006]],

        [[0.5348, 0.0935],
         [0.6896, 0.5411]],

        [[0.6789, 0.0987],
         [0.6797, 0.5680]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448338943900694
time is 2
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 6
Adjusted Rand Index: 0.772104805341358
time is 3
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208025343189018
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.8909178489889494
Average Adjusted Rand Index: 0.8915474004704921
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24723.48828125
inf tensor(24723.4883, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11193.4912109375
tensor(24723.4883, grad_fn=<NegBackward0>) tensor(11193.4912, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11193.041015625
tensor(11193.4912, grad_fn=<NegBackward0>) tensor(11193.0410, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11192.9130859375
tensor(11193.0410, grad_fn=<NegBackward0>) tensor(11192.9131, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11192.8408203125
tensor(11192.9131, grad_fn=<NegBackward0>) tensor(11192.8408, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11192.78515625
tensor(11192.8408, grad_fn=<NegBackward0>) tensor(11192.7852, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11192.7333984375
tensor(11192.7852, grad_fn=<NegBackward0>) tensor(11192.7334, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11192.6787109375
tensor(11192.7334, grad_fn=<NegBackward0>) tensor(11192.6787, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11192.6123046875
tensor(11192.6787, grad_fn=<NegBackward0>) tensor(11192.6123, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11192.53125
tensor(11192.6123, grad_fn=<NegBackward0>) tensor(11192.5312, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11192.4296875
tensor(11192.5312, grad_fn=<NegBackward0>) tensor(11192.4297, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11192.302734375
tensor(11192.4297, grad_fn=<NegBackward0>) tensor(11192.3027, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11192.1240234375
tensor(11192.3027, grad_fn=<NegBackward0>) tensor(11192.1240, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11191.71875
tensor(11192.1240, grad_fn=<NegBackward0>) tensor(11191.7188, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11188.1171875
tensor(11191.7188, grad_fn=<NegBackward0>) tensor(11188.1172, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11074.052734375
tensor(11188.1172, grad_fn=<NegBackward0>) tensor(11074.0527, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11028.8349609375
tensor(11074.0527, grad_fn=<NegBackward0>) tensor(11028.8350, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11028.63671875
tensor(11028.8350, grad_fn=<NegBackward0>) tensor(11028.6367, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11028.556640625
tensor(11028.6367, grad_fn=<NegBackward0>) tensor(11028.5566, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11028.5205078125
tensor(11028.5566, grad_fn=<NegBackward0>) tensor(11028.5205, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11028.5107421875
tensor(11028.5205, grad_fn=<NegBackward0>) tensor(11028.5107, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11028.50390625
tensor(11028.5107, grad_fn=<NegBackward0>) tensor(11028.5039, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11028.5009765625
tensor(11028.5039, grad_fn=<NegBackward0>) tensor(11028.5010, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11028.486328125
tensor(11028.5010, grad_fn=<NegBackward0>) tensor(11028.4863, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11028.4814453125
tensor(11028.4863, grad_fn=<NegBackward0>) tensor(11028.4814, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11028.478515625
tensor(11028.4814, grad_fn=<NegBackward0>) tensor(11028.4785, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11028.478515625
tensor(11028.4785, grad_fn=<NegBackward0>) tensor(11028.4785, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11028.4736328125
tensor(11028.4785, grad_fn=<NegBackward0>) tensor(11028.4736, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11028.4697265625
tensor(11028.4736, grad_fn=<NegBackward0>) tensor(11028.4697, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11028.4599609375
tensor(11028.4697, grad_fn=<NegBackward0>) tensor(11028.4600, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11028.439453125
tensor(11028.4600, grad_fn=<NegBackward0>) tensor(11028.4395, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11028.4375
tensor(11028.4395, grad_fn=<NegBackward0>) tensor(11028.4375, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11028.435546875
tensor(11028.4375, grad_fn=<NegBackward0>) tensor(11028.4355, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11027.21484375
tensor(11028.4355, grad_fn=<NegBackward0>) tensor(11027.2148, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11027.1953125
tensor(11027.2148, grad_fn=<NegBackward0>) tensor(11027.1953, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11027.193359375
tensor(11027.1953, grad_fn=<NegBackward0>) tensor(11027.1934, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11027.19140625
tensor(11027.1934, grad_fn=<NegBackward0>) tensor(11027.1914, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11027.0498046875
tensor(11027.1914, grad_fn=<NegBackward0>) tensor(11027.0498, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11027.033203125
tensor(11027.0498, grad_fn=<NegBackward0>) tensor(11027.0332, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11027.0322265625
tensor(11027.0332, grad_fn=<NegBackward0>) tensor(11027.0322, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11026.98046875
tensor(11027.0322, grad_fn=<NegBackward0>) tensor(11026.9805, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11026.974609375
tensor(11026.9805, grad_fn=<NegBackward0>) tensor(11026.9746, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11026.9736328125
tensor(11026.9746, grad_fn=<NegBackward0>) tensor(11026.9736, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11026.3388671875
tensor(11026.9736, grad_fn=<NegBackward0>) tensor(11026.3389, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11026.24609375
tensor(11026.3389, grad_fn=<NegBackward0>) tensor(11026.2461, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11026.2451171875
tensor(11026.2461, grad_fn=<NegBackward0>) tensor(11026.2451, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11026.2451171875
tensor(11026.2451, grad_fn=<NegBackward0>) tensor(11026.2451, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11026.244140625
tensor(11026.2451, grad_fn=<NegBackward0>) tensor(11026.2441, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11026.2431640625
tensor(11026.2441, grad_fn=<NegBackward0>) tensor(11026.2432, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11026.2099609375
tensor(11026.2432, grad_fn=<NegBackward0>) tensor(11026.2100, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11026.205078125
tensor(11026.2100, grad_fn=<NegBackward0>) tensor(11026.2051, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11026.203125
tensor(11026.2051, grad_fn=<NegBackward0>) tensor(11026.2031, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11026.2041015625
tensor(11026.2031, grad_fn=<NegBackward0>) tensor(11026.2041, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11026.2001953125
tensor(11026.2031, grad_fn=<NegBackward0>) tensor(11026.2002, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11026.2001953125
tensor(11026.2002, grad_fn=<NegBackward0>) tensor(11026.2002, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11026.2001953125
tensor(11026.2002, grad_fn=<NegBackward0>) tensor(11026.2002, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11026.2060546875
tensor(11026.2002, grad_fn=<NegBackward0>) tensor(11026.2061, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11026.19921875
tensor(11026.2002, grad_fn=<NegBackward0>) tensor(11026.1992, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11026.19921875
tensor(11026.1992, grad_fn=<NegBackward0>) tensor(11026.1992, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11026.203125
tensor(11026.1992, grad_fn=<NegBackward0>) tensor(11026.2031, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11026.2099609375
tensor(11026.1992, grad_fn=<NegBackward0>) tensor(11026.2100, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11026.1962890625
tensor(11026.1992, grad_fn=<NegBackward0>) tensor(11026.1963, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11026.1962890625
tensor(11026.1963, grad_fn=<NegBackward0>) tensor(11026.1963, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11026.1953125
tensor(11026.1963, grad_fn=<NegBackward0>) tensor(11026.1953, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11026.1962890625
tensor(11026.1953, grad_fn=<NegBackward0>) tensor(11026.1963, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11026.1962890625
tensor(11026.1953, grad_fn=<NegBackward0>) tensor(11026.1963, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11026.13671875
tensor(11026.1953, grad_fn=<NegBackward0>) tensor(11026.1367, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11026.1376953125
tensor(11026.1367, grad_fn=<NegBackward0>) tensor(11026.1377, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11026.13671875
tensor(11026.1367, grad_fn=<NegBackward0>) tensor(11026.1367, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11026.13671875
tensor(11026.1367, grad_fn=<NegBackward0>) tensor(11026.1367, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11026.134765625
tensor(11026.1367, grad_fn=<NegBackward0>) tensor(11026.1348, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11026.134765625
tensor(11026.1348, grad_fn=<NegBackward0>) tensor(11026.1348, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11026.1708984375
tensor(11026.1348, grad_fn=<NegBackward0>) tensor(11026.1709, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11026.1357421875
tensor(11026.1348, grad_fn=<NegBackward0>) tensor(11026.1357, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11026.1416015625
tensor(11026.1348, grad_fn=<NegBackward0>) tensor(11026.1416, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11026.1337890625
tensor(11026.1348, grad_fn=<NegBackward0>) tensor(11026.1338, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11026.134765625
tensor(11026.1338, grad_fn=<NegBackward0>) tensor(11026.1348, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11026.134765625
tensor(11026.1338, grad_fn=<NegBackward0>) tensor(11026.1348, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11026.1337890625
tensor(11026.1338, grad_fn=<NegBackward0>) tensor(11026.1338, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11026.1337890625
tensor(11026.1338, grad_fn=<NegBackward0>) tensor(11026.1338, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11026.1337890625
tensor(11026.1338, grad_fn=<NegBackward0>) tensor(11026.1338, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11026.1357421875
tensor(11026.1338, grad_fn=<NegBackward0>) tensor(11026.1357, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11026.1337890625
tensor(11026.1338, grad_fn=<NegBackward0>) tensor(11026.1338, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11026.1376953125
tensor(11026.1338, grad_fn=<NegBackward0>) tensor(11026.1377, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11026.1328125
tensor(11026.1338, grad_fn=<NegBackward0>) tensor(11026.1328, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11026.1337890625
tensor(11026.1328, grad_fn=<NegBackward0>) tensor(11026.1338, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11026.1328125
tensor(11026.1328, grad_fn=<NegBackward0>) tensor(11026.1328, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11026.1337890625
tensor(11026.1328, grad_fn=<NegBackward0>) tensor(11026.1338, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11026.1328125
tensor(11026.1328, grad_fn=<NegBackward0>) tensor(11026.1328, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11026.1328125
tensor(11026.1328, grad_fn=<NegBackward0>) tensor(11026.1328, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11026.1318359375
tensor(11026.1328, grad_fn=<NegBackward0>) tensor(11026.1318, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11026.13671875
tensor(11026.1318, grad_fn=<NegBackward0>) tensor(11026.1367, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11026.1337890625
tensor(11026.1318, grad_fn=<NegBackward0>) tensor(11026.1338, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11026.1328125
tensor(11026.1318, grad_fn=<NegBackward0>) tensor(11026.1328, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -11026.1298828125
tensor(11026.1318, grad_fn=<NegBackward0>) tensor(11026.1299, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11026.1337890625
tensor(11026.1299, grad_fn=<NegBackward0>) tensor(11026.1338, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11026.1298828125
tensor(11026.1299, grad_fn=<NegBackward0>) tensor(11026.1299, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11026.1298828125
tensor(11026.1299, grad_fn=<NegBackward0>) tensor(11026.1299, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11026.1337890625
tensor(11026.1299, grad_fn=<NegBackward0>) tensor(11026.1338, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11026.130859375
tensor(11026.1299, grad_fn=<NegBackward0>) tensor(11026.1309, grad_fn=<NegBackward0>)
2
pi: tensor([[0.7018, 0.2982],
        [0.2778, 0.7222]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4771, 0.5229], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2044, 0.1026],
         [0.7306, 0.2618]],

        [[0.6314, 0.1063],
         [0.7086, 0.6589]],

        [[0.5508, 0.1011],
         [0.7182, 0.6755]],

        [[0.5430, 0.0934],
         [0.5302, 0.6802]],

        [[0.5647, 0.0987],
         [0.5325, 0.5045]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
time is 2
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.772104805341358
time is 3
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208025343189018
time is 4
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.8909178489889494
Average Adjusted Rand Index: 0.8915474004704921
[0.8909178489889494, 0.8909178489889494] [0.8915474004704921, 0.8915474004704921] [11026.234375, 11026.1318359375]
-------------------------------------
This iteration is 59
True Objective function: Loss = -10858.272172355853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23808.28515625
inf tensor(23808.2852, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10899.1826171875
tensor(23808.2852, grad_fn=<NegBackward0>) tensor(10899.1826, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10898.658203125
tensor(10899.1826, grad_fn=<NegBackward0>) tensor(10898.6582, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10898.494140625
tensor(10898.6582, grad_fn=<NegBackward0>) tensor(10898.4941, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10898.4189453125
tensor(10898.4941, grad_fn=<NegBackward0>) tensor(10898.4189, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10898.3759765625
tensor(10898.4189, grad_fn=<NegBackward0>) tensor(10898.3760, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10898.349609375
tensor(10898.3760, grad_fn=<NegBackward0>) tensor(10898.3496, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10898.3310546875
tensor(10898.3496, grad_fn=<NegBackward0>) tensor(10898.3311, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10898.3095703125
tensor(10898.3311, grad_fn=<NegBackward0>) tensor(10898.3096, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10898.2900390625
tensor(10898.3096, grad_fn=<NegBackward0>) tensor(10898.2900, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10898.2685546875
tensor(10898.2900, grad_fn=<NegBackward0>) tensor(10898.2686, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10898.2431640625
tensor(10898.2686, grad_fn=<NegBackward0>) tensor(10898.2432, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10898.212890625
tensor(10898.2432, grad_fn=<NegBackward0>) tensor(10898.2129, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10898.16796875
tensor(10898.2129, grad_fn=<NegBackward0>) tensor(10898.1680, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10898.103515625
tensor(10898.1680, grad_fn=<NegBackward0>) tensor(10898.1035, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10897.9990234375
tensor(10898.1035, grad_fn=<NegBackward0>) tensor(10897.9990, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10897.85546875
tensor(10897.9990, grad_fn=<NegBackward0>) tensor(10897.8555, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10897.75390625
tensor(10897.8555, grad_fn=<NegBackward0>) tensor(10897.7539, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10897.701171875
tensor(10897.7539, grad_fn=<NegBackward0>) tensor(10897.7012, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10897.6689453125
tensor(10897.7012, grad_fn=<NegBackward0>) tensor(10897.6689, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10897.642578125
tensor(10897.6689, grad_fn=<NegBackward0>) tensor(10897.6426, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10897.619140625
tensor(10897.6426, grad_fn=<NegBackward0>) tensor(10897.6191, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10897.59765625
tensor(10897.6191, grad_fn=<NegBackward0>) tensor(10897.5977, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10897.5771484375
tensor(10897.5977, grad_fn=<NegBackward0>) tensor(10897.5771, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10897.5595703125
tensor(10897.5771, grad_fn=<NegBackward0>) tensor(10897.5596, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10897.54296875
tensor(10897.5596, grad_fn=<NegBackward0>) tensor(10897.5430, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10897.5302734375
tensor(10897.5430, grad_fn=<NegBackward0>) tensor(10897.5303, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10897.5283203125
tensor(10897.5303, grad_fn=<NegBackward0>) tensor(10897.5283, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10897.5107421875
tensor(10897.5283, grad_fn=<NegBackward0>) tensor(10897.5107, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10897.505859375
tensor(10897.5107, grad_fn=<NegBackward0>) tensor(10897.5059, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10897.501953125
tensor(10897.5059, grad_fn=<NegBackward0>) tensor(10897.5020, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10897.5
tensor(10897.5020, grad_fn=<NegBackward0>) tensor(10897.5000, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10897.498046875
tensor(10897.5000, grad_fn=<NegBackward0>) tensor(10897.4980, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10897.4970703125
tensor(10897.4980, grad_fn=<NegBackward0>) tensor(10897.4971, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10897.49609375
tensor(10897.4971, grad_fn=<NegBackward0>) tensor(10897.4961, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10897.494140625
tensor(10897.4961, grad_fn=<NegBackward0>) tensor(10897.4941, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10897.4921875
tensor(10897.4941, grad_fn=<NegBackward0>) tensor(10897.4922, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10897.4912109375
tensor(10897.4922, grad_fn=<NegBackward0>) tensor(10897.4912, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10897.4892578125
tensor(10897.4912, grad_fn=<NegBackward0>) tensor(10897.4893, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10897.4873046875
tensor(10897.4893, grad_fn=<NegBackward0>) tensor(10897.4873, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10897.486328125
tensor(10897.4873, grad_fn=<NegBackward0>) tensor(10897.4863, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10897.486328125
tensor(10897.4863, grad_fn=<NegBackward0>) tensor(10897.4863, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10897.484375
tensor(10897.4863, grad_fn=<NegBackward0>) tensor(10897.4844, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10897.4833984375
tensor(10897.4844, grad_fn=<NegBackward0>) tensor(10897.4834, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10897.4833984375
tensor(10897.4834, grad_fn=<NegBackward0>) tensor(10897.4834, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10897.482421875
tensor(10897.4834, grad_fn=<NegBackward0>) tensor(10897.4824, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10897.482421875
tensor(10897.4824, grad_fn=<NegBackward0>) tensor(10897.4824, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10897.482421875
tensor(10897.4824, grad_fn=<NegBackward0>) tensor(10897.4824, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10897.4833984375
tensor(10897.4824, grad_fn=<NegBackward0>) tensor(10897.4834, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10897.4814453125
tensor(10897.4824, grad_fn=<NegBackward0>) tensor(10897.4814, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10897.48046875
tensor(10897.4814, grad_fn=<NegBackward0>) tensor(10897.4805, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10897.4814453125
tensor(10897.4805, grad_fn=<NegBackward0>) tensor(10897.4814, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10897.48046875
tensor(10897.4805, grad_fn=<NegBackward0>) tensor(10897.4805, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10897.4814453125
tensor(10897.4805, grad_fn=<NegBackward0>) tensor(10897.4814, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10897.4794921875
tensor(10897.4805, grad_fn=<NegBackward0>) tensor(10897.4795, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10897.4814453125
tensor(10897.4795, grad_fn=<NegBackward0>) tensor(10897.4814, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10897.482421875
tensor(10897.4795, grad_fn=<NegBackward0>) tensor(10897.4824, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -10897.48046875
tensor(10897.4795, grad_fn=<NegBackward0>) tensor(10897.4805, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -10897.48046875
tensor(10897.4795, grad_fn=<NegBackward0>) tensor(10897.4805, grad_fn=<NegBackward0>)
4
Iteration 5900: Loss = -10897.4814453125
tensor(10897.4795, grad_fn=<NegBackward0>) tensor(10897.4814, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5900 due to no improvement.
pi: tensor([[0.9749, 0.0251],
        [0.9695, 0.0305]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0927, 0.9073], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1596, 0.1620],
         [0.6755, 0.1631]],

        [[0.5244, 0.2126],
         [0.5657, 0.5486]],

        [[0.6834, 0.1698],
         [0.5240, 0.5489]],

        [[0.6954, 0.0935],
         [0.7111, 0.6888]],

        [[0.7139, 0.1676],
         [0.5962, 0.6921]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001776001976594112
Average Adjusted Rand Index: 0.0005107695272972583
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21247.267578125
inf tensor(21247.2676, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10899.015625
tensor(21247.2676, grad_fn=<NegBackward0>) tensor(10899.0156, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10898.560546875
tensor(10899.0156, grad_fn=<NegBackward0>) tensor(10898.5605, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10898.4208984375
tensor(10898.5605, grad_fn=<NegBackward0>) tensor(10898.4209, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10898.3603515625
tensor(10898.4209, grad_fn=<NegBackward0>) tensor(10898.3604, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10898.3203125
tensor(10898.3604, grad_fn=<NegBackward0>) tensor(10898.3203, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10898.287109375
tensor(10898.3203, grad_fn=<NegBackward0>) tensor(10898.2871, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10898.2548828125
tensor(10898.2871, grad_fn=<NegBackward0>) tensor(10898.2549, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10898.224609375
tensor(10898.2549, grad_fn=<NegBackward0>) tensor(10898.2246, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10898.1904296875
tensor(10898.2246, grad_fn=<NegBackward0>) tensor(10898.1904, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10898.1533203125
tensor(10898.1904, grad_fn=<NegBackward0>) tensor(10898.1533, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10898.109375
tensor(10898.1533, grad_fn=<NegBackward0>) tensor(10898.1094, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10898.0595703125
tensor(10898.1094, grad_fn=<NegBackward0>) tensor(10898.0596, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10897.99609375
tensor(10898.0596, grad_fn=<NegBackward0>) tensor(10897.9961, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10897.912109375
tensor(10897.9961, grad_fn=<NegBackward0>) tensor(10897.9121, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10897.8046875
tensor(10897.9121, grad_fn=<NegBackward0>) tensor(10897.8047, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10897.701171875
tensor(10897.8047, grad_fn=<NegBackward0>) tensor(10897.7012, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10897.630859375
tensor(10897.7012, grad_fn=<NegBackward0>) tensor(10897.6309, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10897.59375
tensor(10897.6309, grad_fn=<NegBackward0>) tensor(10897.5938, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10897.5751953125
tensor(10897.5938, grad_fn=<NegBackward0>) tensor(10897.5752, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10897.5517578125
tensor(10897.5752, grad_fn=<NegBackward0>) tensor(10897.5518, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10897.5390625
tensor(10897.5518, grad_fn=<NegBackward0>) tensor(10897.5391, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10897.5302734375
tensor(10897.5391, grad_fn=<NegBackward0>) tensor(10897.5303, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10897.521484375
tensor(10897.5303, grad_fn=<NegBackward0>) tensor(10897.5215, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10897.5146484375
tensor(10897.5215, grad_fn=<NegBackward0>) tensor(10897.5146, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10897.5087890625
tensor(10897.5146, grad_fn=<NegBackward0>) tensor(10897.5088, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10897.5087890625
tensor(10897.5088, grad_fn=<NegBackward0>) tensor(10897.5088, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10897.501953125
tensor(10897.5088, grad_fn=<NegBackward0>) tensor(10897.5020, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10897.4990234375
tensor(10897.5020, grad_fn=<NegBackward0>) tensor(10897.4990, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10897.4990234375
tensor(10897.4990, grad_fn=<NegBackward0>) tensor(10897.4990, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10897.498046875
tensor(10897.4990, grad_fn=<NegBackward0>) tensor(10897.4980, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10897.498046875
tensor(10897.4980, grad_fn=<NegBackward0>) tensor(10897.4980, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10897.494140625
tensor(10897.4980, grad_fn=<NegBackward0>) tensor(10897.4941, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10897.4921875
tensor(10897.4941, grad_fn=<NegBackward0>) tensor(10897.4922, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10897.4912109375
tensor(10897.4922, grad_fn=<NegBackward0>) tensor(10897.4912, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10897.490234375
tensor(10897.4912, grad_fn=<NegBackward0>) tensor(10897.4902, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10897.4892578125
tensor(10897.4902, grad_fn=<NegBackward0>) tensor(10897.4893, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10897.48828125
tensor(10897.4893, grad_fn=<NegBackward0>) tensor(10897.4883, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10897.486328125
tensor(10897.4883, grad_fn=<NegBackward0>) tensor(10897.4863, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10897.486328125
tensor(10897.4863, grad_fn=<NegBackward0>) tensor(10897.4863, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10897.486328125
tensor(10897.4863, grad_fn=<NegBackward0>) tensor(10897.4863, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10897.484375
tensor(10897.4863, grad_fn=<NegBackward0>) tensor(10897.4844, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10897.484375
tensor(10897.4844, grad_fn=<NegBackward0>) tensor(10897.4844, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10897.4833984375
tensor(10897.4844, grad_fn=<NegBackward0>) tensor(10897.4834, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10897.482421875
tensor(10897.4834, grad_fn=<NegBackward0>) tensor(10897.4824, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10897.484375
tensor(10897.4824, grad_fn=<NegBackward0>) tensor(10897.4844, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10897.482421875
tensor(10897.4824, grad_fn=<NegBackward0>) tensor(10897.4824, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10897.482421875
tensor(10897.4824, grad_fn=<NegBackward0>) tensor(10897.4824, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10897.4814453125
tensor(10897.4824, grad_fn=<NegBackward0>) tensor(10897.4814, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10897.482421875
tensor(10897.4814, grad_fn=<NegBackward0>) tensor(10897.4824, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10897.4814453125
tensor(10897.4814, grad_fn=<NegBackward0>) tensor(10897.4814, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10897.4814453125
tensor(10897.4814, grad_fn=<NegBackward0>) tensor(10897.4814, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10897.4814453125
tensor(10897.4814, grad_fn=<NegBackward0>) tensor(10897.4814, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10897.4814453125
tensor(10897.4814, grad_fn=<NegBackward0>) tensor(10897.4814, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10897.4794921875
tensor(10897.4814, grad_fn=<NegBackward0>) tensor(10897.4795, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10897.482421875
tensor(10897.4795, grad_fn=<NegBackward0>) tensor(10897.4824, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10897.4814453125
tensor(10897.4795, grad_fn=<NegBackward0>) tensor(10897.4814, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -10897.48046875
tensor(10897.4795, grad_fn=<NegBackward0>) tensor(10897.4805, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -10897.48046875
tensor(10897.4795, grad_fn=<NegBackward0>) tensor(10897.4805, grad_fn=<NegBackward0>)
4
Iteration 5900: Loss = -10897.48046875
tensor(10897.4795, grad_fn=<NegBackward0>) tensor(10897.4805, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5900 due to no improvement.
pi: tensor([[0.9749, 0.0251],
        [0.9696, 0.0304]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1053, 0.8947], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1596, 0.1620],
         [0.6906, 0.1631]],

        [[0.5196, 0.2126],
         [0.7167, 0.6496]],

        [[0.5533, 0.1698],
         [0.5912, 0.6461]],

        [[0.6881, 0.0935],
         [0.5677, 0.5562]],

        [[0.6905, 0.1676],
         [0.6805, 0.6596]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001776001976594112
Average Adjusted Rand Index: 0.0005107695272972583
[-0.001776001976594112, -0.001776001976594112] [0.0005107695272972583, 0.0005107695272972583] [10897.4814453125, 10897.48046875]
-------------------------------------
This iteration is 60
True Objective function: Loss = -11020.968420370691
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25373.52734375
inf tensor(25373.5273, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11090.40234375
tensor(25373.5273, grad_fn=<NegBackward0>) tensor(11090.4023, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11089.888671875
tensor(11090.4023, grad_fn=<NegBackward0>) tensor(11089.8887, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11089.83203125
tensor(11089.8887, grad_fn=<NegBackward0>) tensor(11089.8320, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11089.80078125
tensor(11089.8320, grad_fn=<NegBackward0>) tensor(11089.8008, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11089.783203125
tensor(11089.8008, grad_fn=<NegBackward0>) tensor(11089.7832, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11089.771484375
tensor(11089.7832, grad_fn=<NegBackward0>) tensor(11089.7715, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11089.763671875
tensor(11089.7715, grad_fn=<NegBackward0>) tensor(11089.7637, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11089.759765625
tensor(11089.7637, grad_fn=<NegBackward0>) tensor(11089.7598, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11089.755859375
tensor(11089.7598, grad_fn=<NegBackward0>) tensor(11089.7559, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11089.75
tensor(11089.7559, grad_fn=<NegBackward0>) tensor(11089.7500, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11089.7490234375
tensor(11089.7500, grad_fn=<NegBackward0>) tensor(11089.7490, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11089.74609375
tensor(11089.7490, grad_fn=<NegBackward0>) tensor(11089.7461, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11089.7431640625
tensor(11089.7461, grad_fn=<NegBackward0>) tensor(11089.7432, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11089.7412109375
tensor(11089.7432, grad_fn=<NegBackward0>) tensor(11089.7412, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11089.73828125
tensor(11089.7412, grad_fn=<NegBackward0>) tensor(11089.7383, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11089.734375
tensor(11089.7383, grad_fn=<NegBackward0>) tensor(11089.7344, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11089.732421875
tensor(11089.7344, grad_fn=<NegBackward0>) tensor(11089.7324, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11089.7314453125
tensor(11089.7324, grad_fn=<NegBackward0>) tensor(11089.7314, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11089.73046875
tensor(11089.7314, grad_fn=<NegBackward0>) tensor(11089.7305, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11089.7275390625
tensor(11089.7305, grad_fn=<NegBackward0>) tensor(11089.7275, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11089.7275390625
tensor(11089.7275, grad_fn=<NegBackward0>) tensor(11089.7275, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11089.7255859375
tensor(11089.7275, grad_fn=<NegBackward0>) tensor(11089.7256, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11089.7255859375
tensor(11089.7256, grad_fn=<NegBackward0>) tensor(11089.7256, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11089.724609375
tensor(11089.7256, grad_fn=<NegBackward0>) tensor(11089.7246, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11089.7236328125
tensor(11089.7246, grad_fn=<NegBackward0>) tensor(11089.7236, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11089.72265625
tensor(11089.7236, grad_fn=<NegBackward0>) tensor(11089.7227, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11089.7236328125
tensor(11089.7227, grad_fn=<NegBackward0>) tensor(11089.7236, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11089.7216796875
tensor(11089.7227, grad_fn=<NegBackward0>) tensor(11089.7217, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11089.72265625
tensor(11089.7217, grad_fn=<NegBackward0>) tensor(11089.7227, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11089.7216796875
tensor(11089.7217, grad_fn=<NegBackward0>) tensor(11089.7217, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11089.720703125
tensor(11089.7217, grad_fn=<NegBackward0>) tensor(11089.7207, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11089.720703125
tensor(11089.7207, grad_fn=<NegBackward0>) tensor(11089.7207, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11089.7197265625
tensor(11089.7207, grad_fn=<NegBackward0>) tensor(11089.7197, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11089.7197265625
tensor(11089.7197, grad_fn=<NegBackward0>) tensor(11089.7197, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11089.7177734375
tensor(11089.7197, grad_fn=<NegBackward0>) tensor(11089.7178, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11089.720703125
tensor(11089.7178, grad_fn=<NegBackward0>) tensor(11089.7207, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11089.7197265625
tensor(11089.7178, grad_fn=<NegBackward0>) tensor(11089.7197, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -11089.71875
tensor(11089.7178, grad_fn=<NegBackward0>) tensor(11089.7188, grad_fn=<NegBackward0>)
3
Iteration 3900: Loss = -11089.7197265625
tensor(11089.7178, grad_fn=<NegBackward0>) tensor(11089.7197, grad_fn=<NegBackward0>)
4
Iteration 4000: Loss = -11089.71875
tensor(11089.7178, grad_fn=<NegBackward0>) tensor(11089.7188, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4000 due to no improvement.
pi: tensor([[0.9831, 0.0169],
        [0.9985, 0.0015]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9984, 0.0016], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1644, 0.1977],
         [0.6713, 0.2500]],

        [[0.7192, 0.2364],
         [0.7202, 0.6436]],

        [[0.6679, 0.1155],
         [0.5386, 0.7166]],

        [[0.6314, 0.1974],
         [0.5060, 0.6155]],

        [[0.6340, 0.1967],
         [0.6146, 0.7112]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24907.201171875
inf tensor(24907.2012, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11090.5205078125
tensor(24907.2012, grad_fn=<NegBackward0>) tensor(11090.5205, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11090.0908203125
tensor(11090.5205, grad_fn=<NegBackward0>) tensor(11090.0908, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11090.048828125
tensor(11090.0908, grad_fn=<NegBackward0>) tensor(11090.0488, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11090.0234375
tensor(11090.0488, grad_fn=<NegBackward0>) tensor(11090.0234, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11090.0068359375
tensor(11090.0234, grad_fn=<NegBackward0>) tensor(11090.0068, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11089.990234375
tensor(11090.0068, grad_fn=<NegBackward0>) tensor(11089.9902, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11089.9658203125
tensor(11089.9902, grad_fn=<NegBackward0>) tensor(11089.9658, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11089.92578125
tensor(11089.9658, grad_fn=<NegBackward0>) tensor(11089.9258, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11089.806640625
tensor(11089.9258, grad_fn=<NegBackward0>) tensor(11089.8066, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11089.35546875
tensor(11089.8066, grad_fn=<NegBackward0>) tensor(11089.3555, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11089.1572265625
tensor(11089.3555, grad_fn=<NegBackward0>) tensor(11089.1572, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11089.1025390625
tensor(11089.1572, grad_fn=<NegBackward0>) tensor(11089.1025, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11089.080078125
tensor(11089.1025, grad_fn=<NegBackward0>) tensor(11089.0801, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11089.064453125
tensor(11089.0801, grad_fn=<NegBackward0>) tensor(11089.0645, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11089.0546875
tensor(11089.0645, grad_fn=<NegBackward0>) tensor(11089.0547, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11089.0458984375
tensor(11089.0547, grad_fn=<NegBackward0>) tensor(11089.0459, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11089.037109375
tensor(11089.0459, grad_fn=<NegBackward0>) tensor(11089.0371, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11089.0283203125
tensor(11089.0371, grad_fn=<NegBackward0>) tensor(11089.0283, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11089.01953125
tensor(11089.0283, grad_fn=<NegBackward0>) tensor(11089.0195, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11089.0126953125
tensor(11089.0195, grad_fn=<NegBackward0>) tensor(11089.0127, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11089.0068359375
tensor(11089.0127, grad_fn=<NegBackward0>) tensor(11089.0068, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11089.0
tensor(11089.0068, grad_fn=<NegBackward0>) tensor(11089., grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11088.9990234375
tensor(11089., grad_fn=<NegBackward0>) tensor(11088.9990, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11088.9970703125
tensor(11088.9990, grad_fn=<NegBackward0>) tensor(11088.9971, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11088.99609375
tensor(11088.9971, grad_fn=<NegBackward0>) tensor(11088.9961, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11088.9921875
tensor(11088.9961, grad_fn=<NegBackward0>) tensor(11088.9922, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11088.9931640625
tensor(11088.9922, grad_fn=<NegBackward0>) tensor(11088.9932, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11088.9912109375
tensor(11088.9922, grad_fn=<NegBackward0>) tensor(11088.9912, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11088.9912109375
tensor(11088.9912, grad_fn=<NegBackward0>) tensor(11088.9912, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11088.990234375
tensor(11088.9912, grad_fn=<NegBackward0>) tensor(11088.9902, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11088.9892578125
tensor(11088.9902, grad_fn=<NegBackward0>) tensor(11088.9893, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11088.9892578125
tensor(11088.9893, grad_fn=<NegBackward0>) tensor(11088.9893, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11088.9892578125
tensor(11088.9893, grad_fn=<NegBackward0>) tensor(11088.9893, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11088.98828125
tensor(11088.9893, grad_fn=<NegBackward0>) tensor(11088.9883, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11088.98828125
tensor(11088.9883, grad_fn=<NegBackward0>) tensor(11088.9883, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11088.9873046875
tensor(11088.9883, grad_fn=<NegBackward0>) tensor(11088.9873, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11088.98828125
tensor(11088.9873, grad_fn=<NegBackward0>) tensor(11088.9883, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11088.9873046875
tensor(11088.9873, grad_fn=<NegBackward0>) tensor(11088.9873, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11088.9873046875
tensor(11088.9873, grad_fn=<NegBackward0>) tensor(11088.9873, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11088.9873046875
tensor(11088.9873, grad_fn=<NegBackward0>) tensor(11088.9873, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11088.9873046875
tensor(11088.9873, grad_fn=<NegBackward0>) tensor(11088.9873, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11088.9873046875
tensor(11088.9873, grad_fn=<NegBackward0>) tensor(11088.9873, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11088.986328125
tensor(11088.9873, grad_fn=<NegBackward0>) tensor(11088.9863, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11088.9853515625
tensor(11088.9863, grad_fn=<NegBackward0>) tensor(11088.9854, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11088.9853515625
tensor(11088.9854, grad_fn=<NegBackward0>) tensor(11088.9854, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11088.9873046875
tensor(11088.9854, grad_fn=<NegBackward0>) tensor(11088.9873, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11088.9892578125
tensor(11088.9854, grad_fn=<NegBackward0>) tensor(11088.9893, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -11088.986328125
tensor(11088.9854, grad_fn=<NegBackward0>) tensor(11088.9863, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -11088.9853515625
tensor(11088.9854, grad_fn=<NegBackward0>) tensor(11088.9854, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11088.9853515625
tensor(11088.9854, grad_fn=<NegBackward0>) tensor(11088.9854, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11088.9853515625
tensor(11088.9854, grad_fn=<NegBackward0>) tensor(11088.9854, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11088.986328125
tensor(11088.9854, grad_fn=<NegBackward0>) tensor(11088.9863, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11088.9853515625
tensor(11088.9854, grad_fn=<NegBackward0>) tensor(11088.9854, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11088.9853515625
tensor(11088.9854, grad_fn=<NegBackward0>) tensor(11088.9854, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11088.9853515625
tensor(11088.9854, grad_fn=<NegBackward0>) tensor(11088.9854, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11088.984375
tensor(11088.9854, grad_fn=<NegBackward0>) tensor(11088.9844, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11088.9853515625
tensor(11088.9844, grad_fn=<NegBackward0>) tensor(11088.9854, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11088.9853515625
tensor(11088.9844, grad_fn=<NegBackward0>) tensor(11088.9854, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11088.984375
tensor(11088.9844, grad_fn=<NegBackward0>) tensor(11088.9844, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11088.9853515625
tensor(11088.9844, grad_fn=<NegBackward0>) tensor(11088.9854, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11088.9853515625
tensor(11088.9844, grad_fn=<NegBackward0>) tensor(11088.9854, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11088.9853515625
tensor(11088.9844, grad_fn=<NegBackward0>) tensor(11088.9854, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -11088.984375
tensor(11088.9844, grad_fn=<NegBackward0>) tensor(11088.9844, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11088.9853515625
tensor(11088.9844, grad_fn=<NegBackward0>) tensor(11088.9854, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11088.984375
tensor(11088.9844, grad_fn=<NegBackward0>) tensor(11088.9844, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11088.984375
tensor(11088.9844, grad_fn=<NegBackward0>) tensor(11088.9844, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11088.984375
tensor(11088.9844, grad_fn=<NegBackward0>) tensor(11088.9844, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11088.984375
tensor(11088.9844, grad_fn=<NegBackward0>) tensor(11088.9844, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11088.984375
tensor(11088.9844, grad_fn=<NegBackward0>) tensor(11088.9844, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11088.984375
tensor(11088.9844, grad_fn=<NegBackward0>) tensor(11088.9844, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11088.984375
tensor(11088.9844, grad_fn=<NegBackward0>) tensor(11088.9844, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11088.9833984375
tensor(11088.9844, grad_fn=<NegBackward0>) tensor(11088.9834, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11088.984375
tensor(11088.9834, grad_fn=<NegBackward0>) tensor(11088.9844, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11088.9853515625
tensor(11088.9834, grad_fn=<NegBackward0>) tensor(11088.9854, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11088.9853515625
tensor(11088.9834, grad_fn=<NegBackward0>) tensor(11088.9854, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11088.9833984375
tensor(11088.9834, grad_fn=<NegBackward0>) tensor(11088.9834, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11088.9853515625
tensor(11088.9834, grad_fn=<NegBackward0>) tensor(11088.9854, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11088.984375
tensor(11088.9834, grad_fn=<NegBackward0>) tensor(11088.9844, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11088.984375
tensor(11088.9834, grad_fn=<NegBackward0>) tensor(11088.9844, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11088.984375
tensor(11088.9834, grad_fn=<NegBackward0>) tensor(11088.9844, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -11088.9853515625
tensor(11088.9834, grad_fn=<NegBackward0>) tensor(11088.9854, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8100 due to no improvement.
pi: tensor([[9.7959e-01, 2.0406e-02],
        [9.9981e-01, 1.8717e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9978, 0.0022], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1641, 0.2006],
         [0.7307, 0.1988]],

        [[0.5832, 0.1354],
         [0.6842, 0.6969]],

        [[0.7120, 0.2618],
         [0.5070, 0.6835]],

        [[0.7141, 0.1901],
         [0.6391, 0.5076]],

        [[0.6831, 0.1934],
         [0.7256, 0.6275]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0003371649239211524
Average Adjusted Rand Index: -0.0004529465619428516
[0.0, -0.0003371649239211524] [0.0, -0.0004529465619428516] [11089.71875, 11088.9853515625]
-------------------------------------
This iteration is 61
True Objective function: Loss = -11106.916703605853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23005.169921875
inf tensor(23005.1699, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11255.4638671875
tensor(23005.1699, grad_fn=<NegBackward0>) tensor(11255.4639, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11254.90625
tensor(11255.4639, grad_fn=<NegBackward0>) tensor(11254.9062, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11254.6767578125
tensor(11254.9062, grad_fn=<NegBackward0>) tensor(11254.6768, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11254.498046875
tensor(11254.6768, grad_fn=<NegBackward0>) tensor(11254.4980, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11253.9931640625
tensor(11254.4980, grad_fn=<NegBackward0>) tensor(11253.9932, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11251.568359375
tensor(11253.9932, grad_fn=<NegBackward0>) tensor(11251.5684, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11251.0771484375
tensor(11251.5684, grad_fn=<NegBackward0>) tensor(11251.0771, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11250.7734375
tensor(11251.0771, grad_fn=<NegBackward0>) tensor(11250.7734, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11250.73046875
tensor(11250.7734, grad_fn=<NegBackward0>) tensor(11250.7305, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11250.6923828125
tensor(11250.7305, grad_fn=<NegBackward0>) tensor(11250.6924, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11250.65625
tensor(11250.6924, grad_fn=<NegBackward0>) tensor(11250.6562, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11250.607421875
tensor(11250.6562, grad_fn=<NegBackward0>) tensor(11250.6074, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11250.5439453125
tensor(11250.6074, grad_fn=<NegBackward0>) tensor(11250.5439, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11250.474609375
tensor(11250.5439, grad_fn=<NegBackward0>) tensor(11250.4746, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11250.39453125
tensor(11250.4746, grad_fn=<NegBackward0>) tensor(11250.3945, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11250.3046875
tensor(11250.3945, grad_fn=<NegBackward0>) tensor(11250.3047, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11250.2578125
tensor(11250.3047, grad_fn=<NegBackward0>) tensor(11250.2578, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11250.2392578125
tensor(11250.2578, grad_fn=<NegBackward0>) tensor(11250.2393, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11250.2333984375
tensor(11250.2393, grad_fn=<NegBackward0>) tensor(11250.2334, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11250.2294921875
tensor(11250.2334, grad_fn=<NegBackward0>) tensor(11250.2295, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11250.23046875
tensor(11250.2295, grad_fn=<NegBackward0>) tensor(11250.2305, grad_fn=<NegBackward0>)
1
Iteration 2200: Loss = -11250.2294921875
tensor(11250.2295, grad_fn=<NegBackward0>) tensor(11250.2295, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11250.228515625
tensor(11250.2295, grad_fn=<NegBackward0>) tensor(11250.2285, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11250.2275390625
tensor(11250.2285, grad_fn=<NegBackward0>) tensor(11250.2275, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11250.2265625
tensor(11250.2275, grad_fn=<NegBackward0>) tensor(11250.2266, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11250.2265625
tensor(11250.2266, grad_fn=<NegBackward0>) tensor(11250.2266, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11250.2275390625
tensor(11250.2266, grad_fn=<NegBackward0>) tensor(11250.2275, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11250.2255859375
tensor(11250.2266, grad_fn=<NegBackward0>) tensor(11250.2256, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11250.2265625
tensor(11250.2256, grad_fn=<NegBackward0>) tensor(11250.2266, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11250.2275390625
tensor(11250.2256, grad_fn=<NegBackward0>) tensor(11250.2275, grad_fn=<NegBackward0>)
2
Iteration 3100: Loss = -11250.2275390625
tensor(11250.2256, grad_fn=<NegBackward0>) tensor(11250.2275, grad_fn=<NegBackward0>)
3
Iteration 3200: Loss = -11250.2265625
tensor(11250.2256, grad_fn=<NegBackward0>) tensor(11250.2266, grad_fn=<NegBackward0>)
4
Iteration 3300: Loss = -11250.2265625
tensor(11250.2256, grad_fn=<NegBackward0>) tensor(11250.2266, grad_fn=<NegBackward0>)
5
Stopping early at iteration 3300 due to no improvement.
pi: tensor([[0.9668, 0.0332],
        [0.6737, 0.3263]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8581, 0.1419], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1648, 0.1942],
         [0.5115, 0.2887]],

        [[0.6945, 0.2129],
         [0.6357, 0.7280]],

        [[0.5323, 0.2430],
         [0.6331, 0.5924]],

        [[0.5462, 0.0973],
         [0.5503, 0.5472]],

        [[0.5230, 0.1607],
         [0.5285, 0.5413]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: -0.02385462411194539
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: -0.011374456256342739
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: -0.020678759622205528
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.018778022358394132
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.004754597455639465
Average Adjusted Rand Index: -0.007425963526419904
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20544.708984375
inf tensor(20544.7090, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11255.6669921875
tensor(20544.7090, grad_fn=<NegBackward0>) tensor(11255.6670, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11255.248046875
tensor(11255.6670, grad_fn=<NegBackward0>) tensor(11255.2480, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11254.9541015625
tensor(11255.2480, grad_fn=<NegBackward0>) tensor(11254.9541, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11253.2216796875
tensor(11254.9541, grad_fn=<NegBackward0>) tensor(11253.2217, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11251.6640625
tensor(11253.2217, grad_fn=<NegBackward0>) tensor(11251.6641, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11251.2763671875
tensor(11251.6641, grad_fn=<NegBackward0>) tensor(11251.2764, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11251.048828125
tensor(11251.2764, grad_fn=<NegBackward0>) tensor(11251.0488, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11250.9111328125
tensor(11251.0488, grad_fn=<NegBackward0>) tensor(11250.9111, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11250.8359375
tensor(11250.9111, grad_fn=<NegBackward0>) tensor(11250.8359, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11250.7919921875
tensor(11250.8359, grad_fn=<NegBackward0>) tensor(11250.7920, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11250.755859375
tensor(11250.7920, grad_fn=<NegBackward0>) tensor(11250.7559, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11250.712890625
tensor(11250.7559, grad_fn=<NegBackward0>) tensor(11250.7129, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11250.6435546875
tensor(11250.7129, grad_fn=<NegBackward0>) tensor(11250.6436, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11250.4453125
tensor(11250.6436, grad_fn=<NegBackward0>) tensor(11250.4453, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11248.9912109375
tensor(11250.4453, grad_fn=<NegBackward0>) tensor(11248.9912, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11073.138671875
tensor(11248.9912, grad_fn=<NegBackward0>) tensor(11073.1387, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11057.3857421875
tensor(11073.1387, grad_fn=<NegBackward0>) tensor(11057.3857, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11055.9892578125
tensor(11057.3857, grad_fn=<NegBackward0>) tensor(11055.9893, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11055.8212890625
tensor(11055.9893, grad_fn=<NegBackward0>) tensor(11055.8213, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11055.7890625
tensor(11055.8213, grad_fn=<NegBackward0>) tensor(11055.7891, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11055.7685546875
tensor(11055.7891, grad_fn=<NegBackward0>) tensor(11055.7686, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11055.7587890625
tensor(11055.7686, grad_fn=<NegBackward0>) tensor(11055.7588, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11055.7490234375
tensor(11055.7588, grad_fn=<NegBackward0>) tensor(11055.7490, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11055.740234375
tensor(11055.7490, grad_fn=<NegBackward0>) tensor(11055.7402, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11055.7255859375
tensor(11055.7402, grad_fn=<NegBackward0>) tensor(11055.7256, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11055.7197265625
tensor(11055.7256, grad_fn=<NegBackward0>) tensor(11055.7197, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11055.71484375
tensor(11055.7197, grad_fn=<NegBackward0>) tensor(11055.7148, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11055.708984375
tensor(11055.7148, grad_fn=<NegBackward0>) tensor(11055.7090, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11055.7080078125
tensor(11055.7090, grad_fn=<NegBackward0>) tensor(11055.7080, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11055.7041015625
tensor(11055.7080, grad_fn=<NegBackward0>) tensor(11055.7041, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11055.7001953125
tensor(11055.7041, grad_fn=<NegBackward0>) tensor(11055.7002, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11055.689453125
tensor(11055.7002, grad_fn=<NegBackward0>) tensor(11055.6895, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11055.69140625
tensor(11055.6895, grad_fn=<NegBackward0>) tensor(11055.6914, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11055.6826171875
tensor(11055.6895, grad_fn=<NegBackward0>) tensor(11055.6826, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11055.6796875
tensor(11055.6826, grad_fn=<NegBackward0>) tensor(11055.6797, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11055.6796875
tensor(11055.6797, grad_fn=<NegBackward0>) tensor(11055.6797, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11055.6787109375
tensor(11055.6797, grad_fn=<NegBackward0>) tensor(11055.6787, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11055.6796875
tensor(11055.6787, grad_fn=<NegBackward0>) tensor(11055.6797, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11055.677734375
tensor(11055.6787, grad_fn=<NegBackward0>) tensor(11055.6777, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11055.67578125
tensor(11055.6777, grad_fn=<NegBackward0>) tensor(11055.6758, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11055.673828125
tensor(11055.6758, grad_fn=<NegBackward0>) tensor(11055.6738, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11055.6689453125
tensor(11055.6738, grad_fn=<NegBackward0>) tensor(11055.6689, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11055.62890625
tensor(11055.6689, grad_fn=<NegBackward0>) tensor(11055.6289, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11055.630859375
tensor(11055.6289, grad_fn=<NegBackward0>) tensor(11055.6309, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11055.623046875
tensor(11055.6289, grad_fn=<NegBackward0>) tensor(11055.6230, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11055.611328125
tensor(11055.6230, grad_fn=<NegBackward0>) tensor(11055.6113, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11055.6103515625
tensor(11055.6113, grad_fn=<NegBackward0>) tensor(11055.6104, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11055.599609375
tensor(11055.6104, grad_fn=<NegBackward0>) tensor(11055.5996, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11055.5986328125
tensor(11055.5996, grad_fn=<NegBackward0>) tensor(11055.5986, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11055.5986328125
tensor(11055.5986, grad_fn=<NegBackward0>) tensor(11055.5986, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11055.59765625
tensor(11055.5986, grad_fn=<NegBackward0>) tensor(11055.5977, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11055.59765625
tensor(11055.5977, grad_fn=<NegBackward0>) tensor(11055.5977, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11055.318359375
tensor(11055.5977, grad_fn=<NegBackward0>) tensor(11055.3184, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11055.298828125
tensor(11055.3184, grad_fn=<NegBackward0>) tensor(11055.2988, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11055.2978515625
tensor(11055.2988, grad_fn=<NegBackward0>) tensor(11055.2979, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11055.296875
tensor(11055.2979, grad_fn=<NegBackward0>) tensor(11055.2969, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11055.296875
tensor(11055.2969, grad_fn=<NegBackward0>) tensor(11055.2969, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11055.298828125
tensor(11055.2969, grad_fn=<NegBackward0>) tensor(11055.2988, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11055.296875
tensor(11055.2969, grad_fn=<NegBackward0>) tensor(11055.2969, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11055.2998046875
tensor(11055.2969, grad_fn=<NegBackward0>) tensor(11055.2998, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11055.2958984375
tensor(11055.2969, grad_fn=<NegBackward0>) tensor(11055.2959, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11055.2958984375
tensor(11055.2959, grad_fn=<NegBackward0>) tensor(11055.2959, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11055.294921875
tensor(11055.2959, grad_fn=<NegBackward0>) tensor(11055.2949, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11055.2958984375
tensor(11055.2949, grad_fn=<NegBackward0>) tensor(11055.2959, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11055.298828125
tensor(11055.2949, grad_fn=<NegBackward0>) tensor(11055.2988, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11055.29296875
tensor(11055.2949, grad_fn=<NegBackward0>) tensor(11055.2930, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11055.294921875
tensor(11055.2930, grad_fn=<NegBackward0>) tensor(11055.2949, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11055.2939453125
tensor(11055.2930, grad_fn=<NegBackward0>) tensor(11055.2939, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11055.29296875
tensor(11055.2930, grad_fn=<NegBackward0>) tensor(11055.2930, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11055.298828125
tensor(11055.2930, grad_fn=<NegBackward0>) tensor(11055.2988, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11055.2900390625
tensor(11055.2930, grad_fn=<NegBackward0>) tensor(11055.2900, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11055.2373046875
tensor(11055.2900, grad_fn=<NegBackward0>) tensor(11055.2373, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11055.212890625
tensor(11055.2373, grad_fn=<NegBackward0>) tensor(11055.2129, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11055.2109375
tensor(11055.2129, grad_fn=<NegBackward0>) tensor(11055.2109, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11055.2099609375
tensor(11055.2109, grad_fn=<NegBackward0>) tensor(11055.2100, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11055.212890625
tensor(11055.2100, grad_fn=<NegBackward0>) tensor(11055.2129, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11055.2216796875
tensor(11055.2100, grad_fn=<NegBackward0>) tensor(11055.2217, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11055.193359375
tensor(11055.2100, grad_fn=<NegBackward0>) tensor(11055.1934, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11055.1884765625
tensor(11055.1934, grad_fn=<NegBackward0>) tensor(11055.1885, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11055.1943359375
tensor(11055.1885, grad_fn=<NegBackward0>) tensor(11055.1943, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11055.2001953125
tensor(11055.1885, grad_fn=<NegBackward0>) tensor(11055.2002, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11055.0302734375
tensor(11055.1885, grad_fn=<NegBackward0>) tensor(11055.0303, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11055.0087890625
tensor(11055.0303, grad_fn=<NegBackward0>) tensor(11055.0088, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11055.0078125
tensor(11055.0088, grad_fn=<NegBackward0>) tensor(11055.0078, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11055.0068359375
tensor(11055.0078, grad_fn=<NegBackward0>) tensor(11055.0068, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11055.0068359375
tensor(11055.0068, grad_fn=<NegBackward0>) tensor(11055.0068, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11055.005859375
tensor(11055.0068, grad_fn=<NegBackward0>) tensor(11055.0059, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11055.01171875
tensor(11055.0059, grad_fn=<NegBackward0>) tensor(11055.0117, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11055.0068359375
tensor(11055.0059, grad_fn=<NegBackward0>) tensor(11055.0068, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -11055.0068359375
tensor(11055.0059, grad_fn=<NegBackward0>) tensor(11055.0068, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -11055.009765625
tensor(11055.0059, grad_fn=<NegBackward0>) tensor(11055.0098, grad_fn=<NegBackward0>)
4
Iteration 9200: Loss = -11055.0078125
tensor(11055.0059, grad_fn=<NegBackward0>) tensor(11055.0078, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9200 due to no improvement.
pi: tensor([[0.7635, 0.2365],
        [0.2744, 0.7256]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6240, 0.3760], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2512, 0.1034],
         [0.7246, 0.2149]],

        [[0.5708, 0.0928],
         [0.7304, 0.7246]],

        [[0.5639, 0.0993],
         [0.5794, 0.5547]],

        [[0.6163, 0.0980],
         [0.5854, 0.7193]],

        [[0.6706, 0.0918],
         [0.6530, 0.6803]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 6
Adjusted Rand Index: 0.7716057836307171
time is 1
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8078717939205692
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.882296193749233
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080863220989386
time is 4
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448338943900694
Global Adjusted Rand Index: 0.8240380143666807
Average Adjusted Rand Index: 0.8229387975579053
[-0.004754597455639465, 0.8240380143666807] [-0.007425963526419904, 0.8229387975579053] [11250.2265625, 11055.0078125]
-------------------------------------
This iteration is 62
True Objective function: Loss = -10889.10796618903
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23954.509765625
inf tensor(23954.5098, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11003.4453125
tensor(23954.5098, grad_fn=<NegBackward0>) tensor(11003.4453, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11002.3759765625
tensor(11003.4453, grad_fn=<NegBackward0>) tensor(11002.3760, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11002.1337890625
tensor(11002.3760, grad_fn=<NegBackward0>) tensor(11002.1338, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11002.0263671875
tensor(11002.1338, grad_fn=<NegBackward0>) tensor(11002.0264, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11001.94140625
tensor(11002.0264, grad_fn=<NegBackward0>) tensor(11001.9414, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11001.85546875
tensor(11001.9414, grad_fn=<NegBackward0>) tensor(11001.8555, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11001.7626953125
tensor(11001.8555, grad_fn=<NegBackward0>) tensor(11001.7627, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11001.6650390625
tensor(11001.7627, grad_fn=<NegBackward0>) tensor(11001.6650, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11001.564453125
tensor(11001.6650, grad_fn=<NegBackward0>) tensor(11001.5645, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11001.470703125
tensor(11001.5645, grad_fn=<NegBackward0>) tensor(11001.4707, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11001.3818359375
tensor(11001.4707, grad_fn=<NegBackward0>) tensor(11001.3818, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11001.29296875
tensor(11001.3818, grad_fn=<NegBackward0>) tensor(11001.2930, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11001.201171875
tensor(11001.2930, grad_fn=<NegBackward0>) tensor(11001.2012, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11001.1015625
tensor(11001.2012, grad_fn=<NegBackward0>) tensor(11001.1016, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11000.982421875
tensor(11001.1016, grad_fn=<NegBackward0>) tensor(11000.9824, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11000.8291015625
tensor(11000.9824, grad_fn=<NegBackward0>) tensor(11000.8291, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11000.603515625
tensor(11000.8291, grad_fn=<NegBackward0>) tensor(11000.6035, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11000.2646484375
tensor(11000.6035, grad_fn=<NegBackward0>) tensor(11000.2646, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10999.8681640625
tensor(11000.2646, grad_fn=<NegBackward0>) tensor(10999.8682, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10999.529296875
tensor(10999.8682, grad_fn=<NegBackward0>) tensor(10999.5293, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10999.25
tensor(10999.5293, grad_fn=<NegBackward0>) tensor(10999.2500, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10999.0322265625
tensor(10999.2500, grad_fn=<NegBackward0>) tensor(10999.0322, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10998.84765625
tensor(10999.0322, grad_fn=<NegBackward0>) tensor(10998.8477, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10998.703125
tensor(10998.8477, grad_fn=<NegBackward0>) tensor(10998.7031, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10998.59375
tensor(10998.7031, grad_fn=<NegBackward0>) tensor(10998.5938, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10998.505859375
tensor(10998.5938, grad_fn=<NegBackward0>) tensor(10998.5059, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10998.4228515625
tensor(10998.5059, grad_fn=<NegBackward0>) tensor(10998.4229, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10998.353515625
tensor(10998.4229, grad_fn=<NegBackward0>) tensor(10998.3535, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10998.298828125
tensor(10998.3535, grad_fn=<NegBackward0>) tensor(10998.2988, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10998.2587890625
tensor(10998.2988, grad_fn=<NegBackward0>) tensor(10998.2588, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10998.224609375
tensor(10998.2588, grad_fn=<NegBackward0>) tensor(10998.2246, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10998.203125
tensor(10998.2246, grad_fn=<NegBackward0>) tensor(10998.2031, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10998.1708984375
tensor(10998.2031, grad_fn=<NegBackward0>) tensor(10998.1709, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10998.1435546875
tensor(10998.1709, grad_fn=<NegBackward0>) tensor(10998.1436, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10998.1171875
tensor(10998.1436, grad_fn=<NegBackward0>) tensor(10998.1172, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10998.0830078125
tensor(10998.1172, grad_fn=<NegBackward0>) tensor(10998.0830, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10998.0546875
tensor(10998.0830, grad_fn=<NegBackward0>) tensor(10998.0547, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10998.0390625
tensor(10998.0547, grad_fn=<NegBackward0>) tensor(10998.0391, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10998.0302734375
tensor(10998.0391, grad_fn=<NegBackward0>) tensor(10998.0303, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10998.0224609375
tensor(10998.0303, grad_fn=<NegBackward0>) tensor(10998.0225, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10998.01171875
tensor(10998.0225, grad_fn=<NegBackward0>) tensor(10998.0117, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10998.0048828125
tensor(10998.0117, grad_fn=<NegBackward0>) tensor(10998.0049, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10997.9970703125
tensor(10998.0049, grad_fn=<NegBackward0>) tensor(10997.9971, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10997.990234375
tensor(10997.9971, grad_fn=<NegBackward0>) tensor(10997.9902, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10997.986328125
tensor(10997.9902, grad_fn=<NegBackward0>) tensor(10997.9863, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10997.9833984375
tensor(10997.9863, grad_fn=<NegBackward0>) tensor(10997.9834, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10997.978515625
tensor(10997.9834, grad_fn=<NegBackward0>) tensor(10997.9785, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10997.9765625
tensor(10997.9785, grad_fn=<NegBackward0>) tensor(10997.9766, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10997.974609375
tensor(10997.9766, grad_fn=<NegBackward0>) tensor(10997.9746, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10997.9736328125
tensor(10997.9746, grad_fn=<NegBackward0>) tensor(10997.9736, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10997.974609375
tensor(10997.9736, grad_fn=<NegBackward0>) tensor(10997.9746, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10997.9677734375
tensor(10997.9736, grad_fn=<NegBackward0>) tensor(10997.9678, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10997.9638671875
tensor(10997.9678, grad_fn=<NegBackward0>) tensor(10997.9639, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10997.9638671875
tensor(10997.9639, grad_fn=<NegBackward0>) tensor(10997.9639, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10997.9619140625
tensor(10997.9639, grad_fn=<NegBackward0>) tensor(10997.9619, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10997.9609375
tensor(10997.9619, grad_fn=<NegBackward0>) tensor(10997.9609, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10997.95703125
tensor(10997.9609, grad_fn=<NegBackward0>) tensor(10997.9570, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10997.95703125
tensor(10997.9570, grad_fn=<NegBackward0>) tensor(10997.9570, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10997.955078125
tensor(10997.9570, grad_fn=<NegBackward0>) tensor(10997.9551, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10997.955078125
tensor(10997.9551, grad_fn=<NegBackward0>) tensor(10997.9551, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10997.953125
tensor(10997.9551, grad_fn=<NegBackward0>) tensor(10997.9531, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10997.951171875
tensor(10997.9531, grad_fn=<NegBackward0>) tensor(10997.9512, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10997.953125
tensor(10997.9512, grad_fn=<NegBackward0>) tensor(10997.9531, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10997.951171875
tensor(10997.9512, grad_fn=<NegBackward0>) tensor(10997.9512, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10997.9560546875
tensor(10997.9512, grad_fn=<NegBackward0>) tensor(10997.9561, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10997.9462890625
tensor(10997.9512, grad_fn=<NegBackward0>) tensor(10997.9463, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10997.943359375
tensor(10997.9463, grad_fn=<NegBackward0>) tensor(10997.9434, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10997.9384765625
tensor(10997.9434, grad_fn=<NegBackward0>) tensor(10997.9385, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10997.9306640625
tensor(10997.9385, grad_fn=<NegBackward0>) tensor(10997.9307, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10997.9140625
tensor(10997.9307, grad_fn=<NegBackward0>) tensor(10997.9141, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10997.880859375
tensor(10997.9141, grad_fn=<NegBackward0>) tensor(10997.8809, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10997.845703125
tensor(10997.8809, grad_fn=<NegBackward0>) tensor(10997.8457, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10997.7744140625
tensor(10997.8457, grad_fn=<NegBackward0>) tensor(10997.7744, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10997.4755859375
tensor(10997.7744, grad_fn=<NegBackward0>) tensor(10997.4756, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10996.01953125
tensor(10997.4756, grad_fn=<NegBackward0>) tensor(10996.0195, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10995.958984375
tensor(10996.0195, grad_fn=<NegBackward0>) tensor(10995.9590, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10995.9345703125
tensor(10995.9590, grad_fn=<NegBackward0>) tensor(10995.9346, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10995.9130859375
tensor(10995.9346, grad_fn=<NegBackward0>) tensor(10995.9131, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10995.724609375
tensor(10995.9131, grad_fn=<NegBackward0>) tensor(10995.7246, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10995.71484375
tensor(10995.7246, grad_fn=<NegBackward0>) tensor(10995.7148, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10995.716796875
tensor(10995.7148, grad_fn=<NegBackward0>) tensor(10995.7168, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10995.7080078125
tensor(10995.7148, grad_fn=<NegBackward0>) tensor(10995.7080, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10995.7021484375
tensor(10995.7080, grad_fn=<NegBackward0>) tensor(10995.7021, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10995.6962890625
tensor(10995.7021, grad_fn=<NegBackward0>) tensor(10995.6963, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10995.8427734375
tensor(10995.6963, grad_fn=<NegBackward0>) tensor(10995.8428, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -10995.693359375
tensor(10995.6963, grad_fn=<NegBackward0>) tensor(10995.6934, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10995.6923828125
tensor(10995.6934, grad_fn=<NegBackward0>) tensor(10995.6924, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10995.693359375
tensor(10995.6924, grad_fn=<NegBackward0>) tensor(10995.6934, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10995.671875
tensor(10995.6924, grad_fn=<NegBackward0>) tensor(10995.6719, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10995.66796875
tensor(10995.6719, grad_fn=<NegBackward0>) tensor(10995.6680, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10995.658203125
tensor(10995.6680, grad_fn=<NegBackward0>) tensor(10995.6582, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10995.6591796875
tensor(10995.6582, grad_fn=<NegBackward0>) tensor(10995.6592, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -10995.6591796875
tensor(10995.6582, grad_fn=<NegBackward0>) tensor(10995.6592, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -10995.6494140625
tensor(10995.6582, grad_fn=<NegBackward0>) tensor(10995.6494, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10995.6474609375
tensor(10995.6494, grad_fn=<NegBackward0>) tensor(10995.6475, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10995.646484375
tensor(10995.6475, grad_fn=<NegBackward0>) tensor(10995.6465, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10995.6474609375
tensor(10995.6465, grad_fn=<NegBackward0>) tensor(10995.6475, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -10995.6474609375
tensor(10995.6465, grad_fn=<NegBackward0>) tensor(10995.6475, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -10995.6474609375
tensor(10995.6465, grad_fn=<NegBackward0>) tensor(10995.6475, grad_fn=<NegBackward0>)
3
pi: tensor([[9.9998e-01, 2.1542e-05],
        [3.5772e-05, 9.9996e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8146, 0.1854], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1564, 0.1346],
         [0.6894, 0.2656]],

        [[0.7110, 0.1829],
         [0.6026, 0.6449]],

        [[0.6118, 0.1455],
         [0.6487, 0.7272]],

        [[0.5998, 0.1887],
         [0.6861, 0.5036]],

        [[0.6789, 0.1748],
         [0.7007, 0.6065]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.005082450433636901
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.03266797500765804
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.007637526033689536
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.008082023536093899
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.01935381872318374
Global Adjusted Rand Index: 0.009567767447860356
Average Adjusted Rand Index: 0.00947676815992185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23836.126953125
inf tensor(23836.1270, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11003.6083984375
tensor(23836.1270, grad_fn=<NegBackward0>) tensor(11003.6084, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11002.4990234375
tensor(11003.6084, grad_fn=<NegBackward0>) tensor(11002.4990, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11002.26171875
tensor(11002.4990, grad_fn=<NegBackward0>) tensor(11002.2617, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11002.1572265625
tensor(11002.2617, grad_fn=<NegBackward0>) tensor(11002.1572, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11002.0751953125
tensor(11002.1572, grad_fn=<NegBackward0>) tensor(11002.0752, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11001.9853515625
tensor(11002.0752, grad_fn=<NegBackward0>) tensor(11001.9854, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11001.8798828125
tensor(11001.9854, grad_fn=<NegBackward0>) tensor(11001.8799, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11001.7646484375
tensor(11001.8799, grad_fn=<NegBackward0>) tensor(11001.7646, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11001.64453125
tensor(11001.7646, grad_fn=<NegBackward0>) tensor(11001.6445, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11001.517578125
tensor(11001.6445, grad_fn=<NegBackward0>) tensor(11001.5176, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11001.3779296875
tensor(11001.5176, grad_fn=<NegBackward0>) tensor(11001.3779, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11001.2294921875
tensor(11001.3779, grad_fn=<NegBackward0>) tensor(11001.2295, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11001.0791015625
tensor(11001.2295, grad_fn=<NegBackward0>) tensor(11001.0791, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11000.9072265625
tensor(11001.0791, grad_fn=<NegBackward0>) tensor(11000.9072, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11000.671875
tensor(11000.9072, grad_fn=<NegBackward0>) tensor(11000.6719, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11000.345703125
tensor(11000.6719, grad_fn=<NegBackward0>) tensor(11000.3457, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10999.97265625
tensor(11000.3457, grad_fn=<NegBackward0>) tensor(10999.9727, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10999.6171875
tensor(10999.9727, grad_fn=<NegBackward0>) tensor(10999.6172, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10999.349609375
tensor(10999.6172, grad_fn=<NegBackward0>) tensor(10999.3496, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10999.1357421875
tensor(10999.3496, grad_fn=<NegBackward0>) tensor(10999.1357, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10998.9765625
tensor(10999.1357, grad_fn=<NegBackward0>) tensor(10998.9766, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10998.841796875
tensor(10998.9766, grad_fn=<NegBackward0>) tensor(10998.8418, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10998.7119140625
tensor(10998.8418, grad_fn=<NegBackward0>) tensor(10998.7119, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10998.5966796875
tensor(10998.7119, grad_fn=<NegBackward0>) tensor(10998.5967, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10998.501953125
tensor(10998.5967, grad_fn=<NegBackward0>) tensor(10998.5020, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10998.4345703125
tensor(10998.5020, grad_fn=<NegBackward0>) tensor(10998.4346, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10998.376953125
tensor(10998.4346, grad_fn=<NegBackward0>) tensor(10998.3770, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10998.3134765625
tensor(10998.3770, grad_fn=<NegBackward0>) tensor(10998.3135, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10998.259765625
tensor(10998.3135, grad_fn=<NegBackward0>) tensor(10998.2598, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10998.20703125
tensor(10998.2598, grad_fn=<NegBackward0>) tensor(10998.2070, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10998.1513671875
tensor(10998.2070, grad_fn=<NegBackward0>) tensor(10998.1514, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10998.109375
tensor(10998.1514, grad_fn=<NegBackward0>) tensor(10998.1094, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10998.0791015625
tensor(10998.1094, grad_fn=<NegBackward0>) tensor(10998.0791, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10998.0517578125
tensor(10998.0791, grad_fn=<NegBackward0>) tensor(10998.0518, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10998.0263671875
tensor(10998.0518, grad_fn=<NegBackward0>) tensor(10998.0264, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10998.0068359375
tensor(10998.0264, grad_fn=<NegBackward0>) tensor(10998.0068, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10997.9892578125
tensor(10998.0068, grad_fn=<NegBackward0>) tensor(10997.9893, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10997.97265625
tensor(10997.9893, grad_fn=<NegBackward0>) tensor(10997.9727, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10997.953125
tensor(10997.9727, grad_fn=<NegBackward0>) tensor(10997.9531, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10997.9267578125
tensor(10997.9531, grad_fn=<NegBackward0>) tensor(10997.9268, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10997.892578125
tensor(10997.9268, grad_fn=<NegBackward0>) tensor(10997.8926, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10997.83984375
tensor(10997.8926, grad_fn=<NegBackward0>) tensor(10997.8398, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10997.7734375
tensor(10997.8398, grad_fn=<NegBackward0>) tensor(10997.7734, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10997.705078125
tensor(10997.7734, grad_fn=<NegBackward0>) tensor(10997.7051, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10997.6181640625
tensor(10997.7051, grad_fn=<NegBackward0>) tensor(10997.6182, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10997.4921875
tensor(10997.6182, grad_fn=<NegBackward0>) tensor(10997.4922, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10997.1953125
tensor(10997.4922, grad_fn=<NegBackward0>) tensor(10997.1953, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10996.65625
tensor(10997.1953, grad_fn=<NegBackward0>) tensor(10996.6562, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10995.8779296875
tensor(10996.6562, grad_fn=<NegBackward0>) tensor(10995.8779, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10995.7919921875
tensor(10995.8779, grad_fn=<NegBackward0>) tensor(10995.7920, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10995.7490234375
tensor(10995.7920, grad_fn=<NegBackward0>) tensor(10995.7490, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10995.7265625
tensor(10995.7490, grad_fn=<NegBackward0>) tensor(10995.7266, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10995.7109375
tensor(10995.7266, grad_fn=<NegBackward0>) tensor(10995.7109, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10995.697265625
tensor(10995.7109, grad_fn=<NegBackward0>) tensor(10995.6973, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10995.6865234375
tensor(10995.6973, grad_fn=<NegBackward0>) tensor(10995.6865, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10995.6787109375
tensor(10995.6865, grad_fn=<NegBackward0>) tensor(10995.6787, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10995.673828125
tensor(10995.6787, grad_fn=<NegBackward0>) tensor(10995.6738, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10995.669921875
tensor(10995.6738, grad_fn=<NegBackward0>) tensor(10995.6699, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10995.6640625
tensor(10995.6699, grad_fn=<NegBackward0>) tensor(10995.6641, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10995.662109375
tensor(10995.6641, grad_fn=<NegBackward0>) tensor(10995.6621, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10995.6591796875
tensor(10995.6621, grad_fn=<NegBackward0>) tensor(10995.6592, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10995.65625
tensor(10995.6592, grad_fn=<NegBackward0>) tensor(10995.6562, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10995.654296875
tensor(10995.6562, grad_fn=<NegBackward0>) tensor(10995.6543, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10995.6572265625
tensor(10995.6543, grad_fn=<NegBackward0>) tensor(10995.6572, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10995.65234375
tensor(10995.6543, grad_fn=<NegBackward0>) tensor(10995.6523, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10995.650390625
tensor(10995.6523, grad_fn=<NegBackward0>) tensor(10995.6504, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10995.6494140625
tensor(10995.6504, grad_fn=<NegBackward0>) tensor(10995.6494, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10995.6484375
tensor(10995.6494, grad_fn=<NegBackward0>) tensor(10995.6484, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10995.646484375
tensor(10995.6484, grad_fn=<NegBackward0>) tensor(10995.6465, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10995.646484375
tensor(10995.6465, grad_fn=<NegBackward0>) tensor(10995.6465, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10995.6455078125
tensor(10995.6465, grad_fn=<NegBackward0>) tensor(10995.6455, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10995.64453125
tensor(10995.6455, grad_fn=<NegBackward0>) tensor(10995.6445, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10995.6435546875
tensor(10995.6445, grad_fn=<NegBackward0>) tensor(10995.6436, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10995.646484375
tensor(10995.6436, grad_fn=<NegBackward0>) tensor(10995.6465, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10995.6416015625
tensor(10995.6436, grad_fn=<NegBackward0>) tensor(10995.6416, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10995.6416015625
tensor(10995.6416, grad_fn=<NegBackward0>) tensor(10995.6416, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10995.640625
tensor(10995.6416, grad_fn=<NegBackward0>) tensor(10995.6406, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10995.6328125
tensor(10995.6406, grad_fn=<NegBackward0>) tensor(10995.6328, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10995.6318359375
tensor(10995.6328, grad_fn=<NegBackward0>) tensor(10995.6318, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10995.62890625
tensor(10995.6318, grad_fn=<NegBackward0>) tensor(10995.6289, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10995.630859375
tensor(10995.6289, grad_fn=<NegBackward0>) tensor(10995.6309, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10995.6298828125
tensor(10995.6289, grad_fn=<NegBackward0>) tensor(10995.6299, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -10995.62890625
tensor(10995.6289, grad_fn=<NegBackward0>) tensor(10995.6289, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10995.62890625
tensor(10995.6289, grad_fn=<NegBackward0>) tensor(10995.6289, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10995.62890625
tensor(10995.6289, grad_fn=<NegBackward0>) tensor(10995.6289, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10995.6416015625
tensor(10995.6289, grad_fn=<NegBackward0>) tensor(10995.6416, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10995.6279296875
tensor(10995.6289, grad_fn=<NegBackward0>) tensor(10995.6279, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10995.630859375
tensor(10995.6279, grad_fn=<NegBackward0>) tensor(10995.6309, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10995.626953125
tensor(10995.6279, grad_fn=<NegBackward0>) tensor(10995.6270, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10995.6318359375
tensor(10995.6270, grad_fn=<NegBackward0>) tensor(10995.6318, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10995.6259765625
tensor(10995.6270, grad_fn=<NegBackward0>) tensor(10995.6260, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10995.6259765625
tensor(10995.6260, grad_fn=<NegBackward0>) tensor(10995.6260, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10995.626953125
tensor(10995.6260, grad_fn=<NegBackward0>) tensor(10995.6270, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -10995.6298828125
tensor(10995.6260, grad_fn=<NegBackward0>) tensor(10995.6299, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -10995.71484375
tensor(10995.6260, grad_fn=<NegBackward0>) tensor(10995.7148, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -10995.6259765625
tensor(10995.6260, grad_fn=<NegBackward0>) tensor(10995.6260, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10995.6513671875
tensor(10995.6260, grad_fn=<NegBackward0>) tensor(10995.6514, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -10995.6259765625
tensor(10995.6260, grad_fn=<NegBackward0>) tensor(10995.6260, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10995.6259765625
tensor(10995.6260, grad_fn=<NegBackward0>) tensor(10995.6260, grad_fn=<NegBackward0>)
pi: tensor([[9.9997e-01, 3.0733e-05],
        [1.5584e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1859, 0.8141], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2652, 0.1345],
         [0.7078, 0.1568]],

        [[0.5209, 0.1827],
         [0.6984, 0.6795]],

        [[0.7075, 0.1454],
         [0.6379, 0.5375]],

        [[0.5808, 0.1888],
         [0.5539, 0.5755]],

        [[0.6058, 0.1746],
         [0.6239, 0.5920]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.005082450433636901
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.03266797500765804
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.007637526033689536
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.008082023536093899
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.01935381872318374
Global Adjusted Rand Index: 0.009567767447860356
Average Adjusted Rand Index: 0.00947676815992185
[0.009567767447860356, 0.009567767447860356] [0.00947676815992185, 0.00947676815992185] [10995.6611328125, 10995.6259765625]
-------------------------------------
This iteration is 63
True Objective function: Loss = -11123.01344486485
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21321.927734375
inf tensor(21321.9277, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11190.169921875
tensor(21321.9277, grad_fn=<NegBackward0>) tensor(11190.1699, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11189.6591796875
tensor(11190.1699, grad_fn=<NegBackward0>) tensor(11189.6592, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11189.3408203125
tensor(11189.6592, grad_fn=<NegBackward0>) tensor(11189.3408, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11189.099609375
tensor(11189.3408, grad_fn=<NegBackward0>) tensor(11189.0996, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11188.9111328125
tensor(11189.0996, grad_fn=<NegBackward0>) tensor(11188.9111, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11188.7529296875
tensor(11188.9111, grad_fn=<NegBackward0>) tensor(11188.7529, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11188.6123046875
tensor(11188.7529, grad_fn=<NegBackward0>) tensor(11188.6123, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11188.4794921875
tensor(11188.6123, grad_fn=<NegBackward0>) tensor(11188.4795, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11188.3642578125
tensor(11188.4795, grad_fn=<NegBackward0>) tensor(11188.3643, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11188.2724609375
tensor(11188.3643, grad_fn=<NegBackward0>) tensor(11188.2725, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11188.2060546875
tensor(11188.2725, grad_fn=<NegBackward0>) tensor(11188.2061, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11188.1572265625
tensor(11188.2061, grad_fn=<NegBackward0>) tensor(11188.1572, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11188.12109375
tensor(11188.1572, grad_fn=<NegBackward0>) tensor(11188.1211, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11188.087890625
tensor(11188.1211, grad_fn=<NegBackward0>) tensor(11188.0879, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11188.05078125
tensor(11188.0879, grad_fn=<NegBackward0>) tensor(11188.0508, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11187.9990234375
tensor(11188.0508, grad_fn=<NegBackward0>) tensor(11187.9990, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11187.8818359375
tensor(11187.9990, grad_fn=<NegBackward0>) tensor(11187.8818, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11187.337890625
tensor(11187.8818, grad_fn=<NegBackward0>) tensor(11187.3379, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11183.62890625
tensor(11187.3379, grad_fn=<NegBackward0>) tensor(11183.6289, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11104.4296875
tensor(11183.6289, grad_fn=<NegBackward0>) tensor(11104.4297, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11103.4375
tensor(11104.4297, grad_fn=<NegBackward0>) tensor(11103.4375, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11103.375
tensor(11103.4375, grad_fn=<NegBackward0>) tensor(11103.3750, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11103.349609375
tensor(11103.3750, grad_fn=<NegBackward0>) tensor(11103.3496, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11103.333984375
tensor(11103.3496, grad_fn=<NegBackward0>) tensor(11103.3340, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11103.3193359375
tensor(11103.3340, grad_fn=<NegBackward0>) tensor(11103.3193, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11103.30859375
tensor(11103.3193, grad_fn=<NegBackward0>) tensor(11103.3086, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11103.2978515625
tensor(11103.3086, grad_fn=<NegBackward0>) tensor(11103.2979, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11103.28515625
tensor(11103.2979, grad_fn=<NegBackward0>) tensor(11103.2852, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11103.263671875
tensor(11103.2852, grad_fn=<NegBackward0>) tensor(11103.2637, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11103.1767578125
tensor(11103.2637, grad_fn=<NegBackward0>) tensor(11103.1768, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11102.802734375
tensor(11103.1768, grad_fn=<NegBackward0>) tensor(11102.8027, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11102.6298828125
tensor(11102.8027, grad_fn=<NegBackward0>) tensor(11102.6299, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11102.548828125
tensor(11102.6299, grad_fn=<NegBackward0>) tensor(11102.5488, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11072.1171875
tensor(11102.5488, grad_fn=<NegBackward0>) tensor(11072.1172, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11071.8671875
tensor(11072.1172, grad_fn=<NegBackward0>) tensor(11071.8672, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11071.8232421875
tensor(11071.8672, grad_fn=<NegBackward0>) tensor(11071.8232, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11070.109375
tensor(11071.8232, grad_fn=<NegBackward0>) tensor(11070.1094, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11070.11328125
tensor(11070.1094, grad_fn=<NegBackward0>) tensor(11070.1133, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11070.0517578125
tensor(11070.1094, grad_fn=<NegBackward0>) tensor(11070.0518, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11070.046875
tensor(11070.0518, grad_fn=<NegBackward0>) tensor(11070.0469, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11070.04296875
tensor(11070.0469, grad_fn=<NegBackward0>) tensor(11070.0430, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11070.0400390625
tensor(11070.0430, grad_fn=<NegBackward0>) tensor(11070.0400, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11070.0400390625
tensor(11070.0400, grad_fn=<NegBackward0>) tensor(11070.0400, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11070.03125
tensor(11070.0400, grad_fn=<NegBackward0>) tensor(11070.0312, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11070.0302734375
tensor(11070.0312, grad_fn=<NegBackward0>) tensor(11070.0303, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11070.0302734375
tensor(11070.0303, grad_fn=<NegBackward0>) tensor(11070.0303, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11070.0302734375
tensor(11070.0303, grad_fn=<NegBackward0>) tensor(11070.0303, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11070.0302734375
tensor(11070.0303, grad_fn=<NegBackward0>) tensor(11070.0303, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11070.0302734375
tensor(11070.0303, grad_fn=<NegBackward0>) tensor(11070.0303, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11070.033203125
tensor(11070.0303, grad_fn=<NegBackward0>) tensor(11070.0332, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11070.029296875
tensor(11070.0303, grad_fn=<NegBackward0>) tensor(11070.0293, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11070.02734375
tensor(11070.0293, grad_fn=<NegBackward0>) tensor(11070.0273, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11070.029296875
tensor(11070.0273, grad_fn=<NegBackward0>) tensor(11070.0293, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11070.0283203125
tensor(11070.0273, grad_fn=<NegBackward0>) tensor(11070.0283, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11070.0302734375
tensor(11070.0273, grad_fn=<NegBackward0>) tensor(11070.0303, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -11070.0283203125
tensor(11070.0273, grad_fn=<NegBackward0>) tensor(11070.0283, grad_fn=<NegBackward0>)
4
Iteration 5700: Loss = -11070.0283203125
tensor(11070.0273, grad_fn=<NegBackward0>) tensor(11070.0283, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5700 due to no improvement.
pi: tensor([[0.7906, 0.2094],
        [0.2750, 0.7250]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4482, 0.5518], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2534, 0.1096],
         [0.5386, 0.1950]],

        [[0.5217, 0.1025],
         [0.6146, 0.5074]],

        [[0.6611, 0.1076],
         [0.6731, 0.6282]],

        [[0.7074, 0.0985],
         [0.5067, 0.6477]],

        [[0.6994, 0.1199],
         [0.6523, 0.5351]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 10
Adjusted Rand Index: 0.6363636363636364
time is 1
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7721141809334062
Global Adjusted Rand Index: 0.8241114669417046
Average Adjusted Rand Index: 0.8273084329414531
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21579.708984375
inf tensor(21579.7090, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11190.8603515625
tensor(21579.7090, grad_fn=<NegBackward0>) tensor(11190.8604, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11190.26171875
tensor(11190.8604, grad_fn=<NegBackward0>) tensor(11190.2617, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11190.080078125
tensor(11190.2617, grad_fn=<NegBackward0>) tensor(11190.0801, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11189.98828125
tensor(11190.0801, grad_fn=<NegBackward0>) tensor(11189.9883, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11189.9072265625
tensor(11189.9883, grad_fn=<NegBackward0>) tensor(11189.9072, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11189.7041015625
tensor(11189.9072, grad_fn=<NegBackward0>) tensor(11189.7041, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11189.0546875
tensor(11189.7041, grad_fn=<NegBackward0>) tensor(11189.0547, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11188.8232421875
tensor(11189.0547, grad_fn=<NegBackward0>) tensor(11188.8232, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11188.7607421875
tensor(11188.8232, grad_fn=<NegBackward0>) tensor(11188.7607, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11188.7197265625
tensor(11188.7607, grad_fn=<NegBackward0>) tensor(11188.7197, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11188.685546875
tensor(11188.7197, grad_fn=<NegBackward0>) tensor(11188.6855, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11188.6533203125
tensor(11188.6855, grad_fn=<NegBackward0>) tensor(11188.6533, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11188.62109375
tensor(11188.6533, grad_fn=<NegBackward0>) tensor(11188.6211, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11188.587890625
tensor(11188.6211, grad_fn=<NegBackward0>) tensor(11188.5879, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11188.548828125
tensor(11188.5879, grad_fn=<NegBackward0>) tensor(11188.5488, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11188.4921875
tensor(11188.5488, grad_fn=<NegBackward0>) tensor(11188.4922, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11188.3701171875
tensor(11188.4922, grad_fn=<NegBackward0>) tensor(11188.3701, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11188.1767578125
tensor(11188.3701, grad_fn=<NegBackward0>) tensor(11188.1768, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11188.1103515625
tensor(11188.1768, grad_fn=<NegBackward0>) tensor(11188.1104, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11188.0810546875
tensor(11188.1104, grad_fn=<NegBackward0>) tensor(11188.0811, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11188.056640625
tensor(11188.0811, grad_fn=<NegBackward0>) tensor(11188.0566, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11188.0029296875
tensor(11188.0566, grad_fn=<NegBackward0>) tensor(11188.0029, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11187.5224609375
tensor(11188.0029, grad_fn=<NegBackward0>) tensor(11187.5225, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11112.171875
tensor(11187.5225, grad_fn=<NegBackward0>) tensor(11112.1719, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11103.6376953125
tensor(11112.1719, grad_fn=<NegBackward0>) tensor(11103.6377, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11103.4931640625
tensor(11103.6377, grad_fn=<NegBackward0>) tensor(11103.4932, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11103.4609375
tensor(11103.4932, grad_fn=<NegBackward0>) tensor(11103.4609, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11103.443359375
tensor(11103.4609, grad_fn=<NegBackward0>) tensor(11103.4434, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11103.4306640625
tensor(11103.4434, grad_fn=<NegBackward0>) tensor(11103.4307, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11103.4189453125
tensor(11103.4307, grad_fn=<NegBackward0>) tensor(11103.4189, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11103.408203125
tensor(11103.4189, grad_fn=<NegBackward0>) tensor(11103.4082, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11103.388671875
tensor(11103.4082, grad_fn=<NegBackward0>) tensor(11103.3887, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11103.306640625
tensor(11103.3887, grad_fn=<NegBackward0>) tensor(11103.3066, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11103.302734375
tensor(11103.3066, grad_fn=<NegBackward0>) tensor(11103.3027, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11103.30078125
tensor(11103.3027, grad_fn=<NegBackward0>) tensor(11103.3008, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11103.2998046875
tensor(11103.3008, grad_fn=<NegBackward0>) tensor(11103.2998, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11103.296875
tensor(11103.2998, grad_fn=<NegBackward0>) tensor(11103.2969, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11103.294921875
tensor(11103.2969, grad_fn=<NegBackward0>) tensor(11103.2949, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11103.2919921875
tensor(11103.2949, grad_fn=<NegBackward0>) tensor(11103.2920, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11103.2890625
tensor(11103.2920, grad_fn=<NegBackward0>) tensor(11103.2891, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11103.25390625
tensor(11103.2891, grad_fn=<NegBackward0>) tensor(11103.2539, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11103.25390625
tensor(11103.2539, grad_fn=<NegBackward0>) tensor(11103.2539, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11103.2529296875
tensor(11103.2539, grad_fn=<NegBackward0>) tensor(11103.2529, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11103.2509765625
tensor(11103.2529, grad_fn=<NegBackward0>) tensor(11103.2510, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11103.25
tensor(11103.2510, grad_fn=<NegBackward0>) tensor(11103.2500, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11103.248046875
tensor(11103.2500, grad_fn=<NegBackward0>) tensor(11103.2480, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11103.244140625
tensor(11103.2480, grad_fn=<NegBackward0>) tensor(11103.2441, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11103.2412109375
tensor(11103.2441, grad_fn=<NegBackward0>) tensor(11103.2412, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11103.2421875
tensor(11103.2412, grad_fn=<NegBackward0>) tensor(11103.2422, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11103.240234375
tensor(11103.2412, grad_fn=<NegBackward0>) tensor(11103.2402, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11103.2392578125
tensor(11103.2402, grad_fn=<NegBackward0>) tensor(11103.2393, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11103.244140625
tensor(11103.2393, grad_fn=<NegBackward0>) tensor(11103.2441, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11103.2392578125
tensor(11103.2393, grad_fn=<NegBackward0>) tensor(11103.2393, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11103.2392578125
tensor(11103.2393, grad_fn=<NegBackward0>) tensor(11103.2393, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11103.23828125
tensor(11103.2393, grad_fn=<NegBackward0>) tensor(11103.2383, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11103.23828125
tensor(11103.2383, grad_fn=<NegBackward0>) tensor(11103.2383, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11103.244140625
tensor(11103.2383, grad_fn=<NegBackward0>) tensor(11103.2441, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11103.236328125
tensor(11103.2383, grad_fn=<NegBackward0>) tensor(11103.2363, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11103.2353515625
tensor(11103.2363, grad_fn=<NegBackward0>) tensor(11103.2354, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11103.232421875
tensor(11103.2354, grad_fn=<NegBackward0>) tensor(11103.2324, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11103.23046875
tensor(11103.2324, grad_fn=<NegBackward0>) tensor(11103.2305, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11103.2216796875
tensor(11103.2305, grad_fn=<NegBackward0>) tensor(11103.2217, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11103.1708984375
tensor(11103.2217, grad_fn=<NegBackward0>) tensor(11103.1709, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11103.03125
tensor(11103.1709, grad_fn=<NegBackward0>) tensor(11103.0312, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11102.9384765625
tensor(11103.0312, grad_fn=<NegBackward0>) tensor(11102.9385, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11102.814453125
tensor(11102.9385, grad_fn=<NegBackward0>) tensor(11102.8145, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11102.7529296875
tensor(11102.8145, grad_fn=<NegBackward0>) tensor(11102.7529, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11102.6484375
tensor(11102.7529, grad_fn=<NegBackward0>) tensor(11102.6484, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11102.5068359375
tensor(11102.6484, grad_fn=<NegBackward0>) tensor(11102.5068, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11102.51171875
tensor(11102.5068, grad_fn=<NegBackward0>) tensor(11102.5117, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11102.4697265625
tensor(11102.5068, grad_fn=<NegBackward0>) tensor(11102.4697, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11102.4697265625
tensor(11102.4697, grad_fn=<NegBackward0>) tensor(11102.4697, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11102.4697265625
tensor(11102.4697, grad_fn=<NegBackward0>) tensor(11102.4697, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11102.46875
tensor(11102.4697, grad_fn=<NegBackward0>) tensor(11102.4688, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11102.419921875
tensor(11102.4688, grad_fn=<NegBackward0>) tensor(11102.4199, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11073.759765625
tensor(11102.4199, grad_fn=<NegBackward0>) tensor(11073.7598, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11073.4794921875
tensor(11073.7598, grad_fn=<NegBackward0>) tensor(11073.4795, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11073.4765625
tensor(11073.4795, grad_fn=<NegBackward0>) tensor(11073.4766, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11073.4208984375
tensor(11073.4766, grad_fn=<NegBackward0>) tensor(11073.4209, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11072.7333984375
tensor(11073.4209, grad_fn=<NegBackward0>) tensor(11072.7334, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11070.2509765625
tensor(11072.7334, grad_fn=<NegBackward0>) tensor(11070.2510, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11070.1201171875
tensor(11070.2510, grad_fn=<NegBackward0>) tensor(11070.1201, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11070.1123046875
tensor(11070.1201, grad_fn=<NegBackward0>) tensor(11070.1123, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11070.103515625
tensor(11070.1123, grad_fn=<NegBackward0>) tensor(11070.1035, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11070.1025390625
tensor(11070.1035, grad_fn=<NegBackward0>) tensor(11070.1025, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11070.1025390625
tensor(11070.1025, grad_fn=<NegBackward0>) tensor(11070.1025, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11070.08984375
tensor(11070.1025, grad_fn=<NegBackward0>) tensor(11070.0898, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11070.1689453125
tensor(11070.0898, grad_fn=<NegBackward0>) tensor(11070.1689, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11070.087890625
tensor(11070.0898, grad_fn=<NegBackward0>) tensor(11070.0879, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11070.0869140625
tensor(11070.0879, grad_fn=<NegBackward0>) tensor(11070.0869, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11070.0859375
tensor(11070.0869, grad_fn=<NegBackward0>) tensor(11070.0859, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11070.0869140625
tensor(11070.0859, grad_fn=<NegBackward0>) tensor(11070.0869, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11070.2958984375
tensor(11070.0859, grad_fn=<NegBackward0>) tensor(11070.2959, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11070.041015625
tensor(11070.0859, grad_fn=<NegBackward0>) tensor(11070.0410, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11070.0400390625
tensor(11070.0410, grad_fn=<NegBackward0>) tensor(11070.0400, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11070.0400390625
tensor(11070.0400, grad_fn=<NegBackward0>) tensor(11070.0400, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11070.041015625
tensor(11070.0400, grad_fn=<NegBackward0>) tensor(11070.0410, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11070.044921875
tensor(11070.0400, grad_fn=<NegBackward0>) tensor(11070.0449, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -11070.0400390625
tensor(11070.0400, grad_fn=<NegBackward0>) tensor(11070.0400, grad_fn=<NegBackward0>)
pi: tensor([[0.7286, 0.2714],
        [0.2129, 0.7871]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5557, 0.4443], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1933, 0.1096],
         [0.6085, 0.2555]],

        [[0.7301, 0.1028],
         [0.7191, 0.7271]],

        [[0.6716, 0.1088],
         [0.7265, 0.7087]],

        [[0.6300, 0.0997],
         [0.5365, 0.6565]],

        [[0.6161, 0.1212],
         [0.6677, 0.6568]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 90
Adjusted Rand Index: 0.6363636363636364
time is 1
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080808080808081
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 94
Adjusted Rand Index: 0.7721141809334062
Global Adjusted Rand Index: 0.8241114669417046
Average Adjusted Rand Index: 0.8273084329414531
[0.8241114669417046, 0.8241114669417046] [0.8273084329414531, 0.8273084329414531] [11070.0283203125, 11070.3720703125]
-------------------------------------
This iteration is 64
True Objective function: Loss = -10939.030190094172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22211.400390625
inf tensor(22211.4004, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11015.9091796875
tensor(22211.4004, grad_fn=<NegBackward0>) tensor(11015.9092, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11015.328125
tensor(11015.9092, grad_fn=<NegBackward0>) tensor(11015.3281, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11015.1220703125
tensor(11015.3281, grad_fn=<NegBackward0>) tensor(11015.1221, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11014.8896484375
tensor(11015.1221, grad_fn=<NegBackward0>) tensor(11014.8896, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11014.7431640625
tensor(11014.8896, grad_fn=<NegBackward0>) tensor(11014.7432, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11014.5927734375
tensor(11014.7432, grad_fn=<NegBackward0>) tensor(11014.5928, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11014.2880859375
tensor(11014.5928, grad_fn=<NegBackward0>) tensor(11014.2881, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11013.6865234375
tensor(11014.2881, grad_fn=<NegBackward0>) tensor(11013.6865, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11013.0302734375
tensor(11013.6865, grad_fn=<NegBackward0>) tensor(11013.0303, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11012.7490234375
tensor(11013.0303, grad_fn=<NegBackward0>) tensor(11012.7490, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11012.6220703125
tensor(11012.7490, grad_fn=<NegBackward0>) tensor(11012.6221, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11012.5654296875
tensor(11012.6221, grad_fn=<NegBackward0>) tensor(11012.5654, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11012.5400390625
tensor(11012.5654, grad_fn=<NegBackward0>) tensor(11012.5400, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11012.5244140625
tensor(11012.5400, grad_fn=<NegBackward0>) tensor(11012.5244, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11012.517578125
tensor(11012.5244, grad_fn=<NegBackward0>) tensor(11012.5176, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11012.5107421875
tensor(11012.5176, grad_fn=<NegBackward0>) tensor(11012.5107, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11012.5048828125
tensor(11012.5107, grad_fn=<NegBackward0>) tensor(11012.5049, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11012.49609375
tensor(11012.5049, grad_fn=<NegBackward0>) tensor(11012.4961, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11012.4921875
tensor(11012.4961, grad_fn=<NegBackward0>) tensor(11012.4922, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11012.4892578125
tensor(11012.4922, grad_fn=<NegBackward0>) tensor(11012.4893, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11012.48828125
tensor(11012.4893, grad_fn=<NegBackward0>) tensor(11012.4883, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11012.4873046875
tensor(11012.4883, grad_fn=<NegBackward0>) tensor(11012.4873, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11012.4873046875
tensor(11012.4873, grad_fn=<NegBackward0>) tensor(11012.4873, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11012.4853515625
tensor(11012.4873, grad_fn=<NegBackward0>) tensor(11012.4854, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11012.486328125
tensor(11012.4854, grad_fn=<NegBackward0>) tensor(11012.4863, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -11012.4853515625
tensor(11012.4854, grad_fn=<NegBackward0>) tensor(11012.4854, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11012.484375
tensor(11012.4854, grad_fn=<NegBackward0>) tensor(11012.4844, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11012.4853515625
tensor(11012.4844, grad_fn=<NegBackward0>) tensor(11012.4854, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11012.4853515625
tensor(11012.4844, grad_fn=<NegBackward0>) tensor(11012.4854, grad_fn=<NegBackward0>)
2
Iteration 3000: Loss = -11012.486328125
tensor(11012.4844, grad_fn=<NegBackward0>) tensor(11012.4863, grad_fn=<NegBackward0>)
3
Iteration 3100: Loss = -11012.486328125
tensor(11012.4844, grad_fn=<NegBackward0>) tensor(11012.4863, grad_fn=<NegBackward0>)
4
Iteration 3200: Loss = -11012.4833984375
tensor(11012.4844, grad_fn=<NegBackward0>) tensor(11012.4834, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11012.4853515625
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4854, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11012.4833984375
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4834, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11012.4833984375
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4834, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11012.4833984375
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4834, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11012.4853515625
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4854, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11012.484375
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4844, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -11012.4833984375
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4834, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11012.484375
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4844, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11012.484375
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4844, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -11012.484375
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4844, grad_fn=<NegBackward0>)
3
Iteration 4300: Loss = -11012.4833984375
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4834, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11012.482421875
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4824, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11012.484375
tensor(11012.4824, grad_fn=<NegBackward0>) tensor(11012.4844, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11012.484375
tensor(11012.4824, grad_fn=<NegBackward0>) tensor(11012.4844, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11012.484375
tensor(11012.4824, grad_fn=<NegBackward0>) tensor(11012.4844, grad_fn=<NegBackward0>)
3
Iteration 4800: Loss = -11012.484375
tensor(11012.4824, grad_fn=<NegBackward0>) tensor(11012.4844, grad_fn=<NegBackward0>)
4
Iteration 4900: Loss = -11012.484375
tensor(11012.4824, grad_fn=<NegBackward0>) tensor(11012.4844, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4900 due to no improvement.
pi: tensor([[0.9896, 0.0104],
        [0.5854, 0.4146]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8841, 0.1159], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1603, 0.1921],
         [0.5176, 0.2457]],

        [[0.7090, 0.1912],
         [0.5331, 0.5211]],

        [[0.7168, 0.2192],
         [0.5287, 0.7289]],

        [[0.5485, 0.0871],
         [0.6994, 0.5353]],

        [[0.6635, 0.2283],
         [0.5500, 0.6908]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00015367833708429535
Average Adjusted Rand Index: -0.000610994019009615
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19553.966796875
inf tensor(19553.9668, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11015.6328125
tensor(19553.9668, grad_fn=<NegBackward0>) tensor(11015.6328, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11015.30078125
tensor(11015.6328, grad_fn=<NegBackward0>) tensor(11015.3008, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11015.1435546875
tensor(11015.3008, grad_fn=<NegBackward0>) tensor(11015.1436, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11014.9345703125
tensor(11015.1436, grad_fn=<NegBackward0>) tensor(11014.9346, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11014.6220703125
tensor(11014.9346, grad_fn=<NegBackward0>) tensor(11014.6221, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11014.240234375
tensor(11014.6221, grad_fn=<NegBackward0>) tensor(11014.2402, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11013.8505859375
tensor(11014.2402, grad_fn=<NegBackward0>) tensor(11013.8506, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11013.6181640625
tensor(11013.8506, grad_fn=<NegBackward0>) tensor(11013.6182, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11013.4853515625
tensor(11013.6182, grad_fn=<NegBackward0>) tensor(11013.4854, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11013.3427734375
tensor(11013.4854, grad_fn=<NegBackward0>) tensor(11013.3428, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11013.0849609375
tensor(11013.3428, grad_fn=<NegBackward0>) tensor(11013.0850, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11012.79296875
tensor(11013.0850, grad_fn=<NegBackward0>) tensor(11012.7930, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11012.6474609375
tensor(11012.7930, grad_fn=<NegBackward0>) tensor(11012.6475, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11012.578125
tensor(11012.6475, grad_fn=<NegBackward0>) tensor(11012.5781, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11012.5419921875
tensor(11012.5781, grad_fn=<NegBackward0>) tensor(11012.5420, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11012.521484375
tensor(11012.5420, grad_fn=<NegBackward0>) tensor(11012.5215, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11012.5078125
tensor(11012.5215, grad_fn=<NegBackward0>) tensor(11012.5078, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11012.501953125
tensor(11012.5078, grad_fn=<NegBackward0>) tensor(11012.5020, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11012.49609375
tensor(11012.5020, grad_fn=<NegBackward0>) tensor(11012.4961, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11012.494140625
tensor(11012.4961, grad_fn=<NegBackward0>) tensor(11012.4941, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11012.490234375
tensor(11012.4941, grad_fn=<NegBackward0>) tensor(11012.4902, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11012.4892578125
tensor(11012.4902, grad_fn=<NegBackward0>) tensor(11012.4893, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11012.48828125
tensor(11012.4893, grad_fn=<NegBackward0>) tensor(11012.4883, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11012.486328125
tensor(11012.4883, grad_fn=<NegBackward0>) tensor(11012.4863, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11012.4873046875
tensor(11012.4863, grad_fn=<NegBackward0>) tensor(11012.4873, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -11012.4873046875
tensor(11012.4863, grad_fn=<NegBackward0>) tensor(11012.4873, grad_fn=<NegBackward0>)
2
Iteration 2700: Loss = -11012.486328125
tensor(11012.4863, grad_fn=<NegBackward0>) tensor(11012.4863, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11012.4873046875
tensor(11012.4863, grad_fn=<NegBackward0>) tensor(11012.4873, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11012.486328125
tensor(11012.4863, grad_fn=<NegBackward0>) tensor(11012.4863, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11012.4853515625
tensor(11012.4863, grad_fn=<NegBackward0>) tensor(11012.4854, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11012.4853515625
tensor(11012.4854, grad_fn=<NegBackward0>) tensor(11012.4854, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11012.4853515625
tensor(11012.4854, grad_fn=<NegBackward0>) tensor(11012.4854, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11012.484375
tensor(11012.4854, grad_fn=<NegBackward0>) tensor(11012.4844, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11012.4853515625
tensor(11012.4844, grad_fn=<NegBackward0>) tensor(11012.4854, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11012.484375
tensor(11012.4844, grad_fn=<NegBackward0>) tensor(11012.4844, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11012.484375
tensor(11012.4844, grad_fn=<NegBackward0>) tensor(11012.4844, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11012.4853515625
tensor(11012.4844, grad_fn=<NegBackward0>) tensor(11012.4854, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11012.484375
tensor(11012.4844, grad_fn=<NegBackward0>) tensor(11012.4844, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11012.484375
tensor(11012.4844, grad_fn=<NegBackward0>) tensor(11012.4844, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11012.4833984375
tensor(11012.4844, grad_fn=<NegBackward0>) tensor(11012.4834, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11012.4853515625
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4854, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11012.484375
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4844, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -11012.4833984375
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4834, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11012.484375
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4844, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11012.4833984375
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4834, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11012.4853515625
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4854, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11012.4833984375
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4834, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11012.4853515625
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4854, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11012.4833984375
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4834, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11012.4853515625
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4854, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11012.484375
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4844, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11012.484375
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4844, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -11012.484375
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4844, grad_fn=<NegBackward0>)
4
Iteration 5400: Loss = -11012.484375
tensor(11012.4834, grad_fn=<NegBackward0>) tensor(11012.4844, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5400 due to no improvement.
pi: tensor([[0.4146, 0.5854],
        [0.0104, 0.9896]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1166, 0.8834], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2457, 0.1921],
         [0.7102, 0.1603]],

        [[0.5299, 0.1912],
         [0.6863, 0.5990]],

        [[0.5669, 0.2192],
         [0.5892, 0.5157]],

        [[0.6263, 0.0871],
         [0.6309, 0.6078]],

        [[0.5184, 0.2283],
         [0.5816, 0.6331]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00015367833708429535
Average Adjusted Rand Index: -0.000610994019009615
[-0.00015367833708429535, -0.00015367833708429535] [-0.000610994019009615, -0.000610994019009615] [11012.484375, 11012.484375]
-------------------------------------
This iteration is 65
True Objective function: Loss = -10905.59807742104
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21662.130859375
inf tensor(21662.1309, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10994.66796875
tensor(21662.1309, grad_fn=<NegBackward0>) tensor(10994.6680, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10993.953125
tensor(10994.6680, grad_fn=<NegBackward0>) tensor(10993.9531, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10993.7900390625
tensor(10993.9531, grad_fn=<NegBackward0>) tensor(10993.7900, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10993.3828125
tensor(10993.7900, grad_fn=<NegBackward0>) tensor(10993.3828, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10991.3203125
tensor(10993.3828, grad_fn=<NegBackward0>) tensor(10991.3203, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10989.6865234375
tensor(10991.3203, grad_fn=<NegBackward0>) tensor(10989.6865, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10988.9013671875
tensor(10989.6865, grad_fn=<NegBackward0>) tensor(10988.9014, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10988.498046875
tensor(10988.9014, grad_fn=<NegBackward0>) tensor(10988.4980, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10988.267578125
tensor(10988.4980, grad_fn=<NegBackward0>) tensor(10988.2676, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10988.123046875
tensor(10988.2676, grad_fn=<NegBackward0>) tensor(10988.1230, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10988.0146484375
tensor(10988.1230, grad_fn=<NegBackward0>) tensor(10988.0146, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10987.8955078125
tensor(10988.0146, grad_fn=<NegBackward0>) tensor(10987.8955, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10987.490234375
tensor(10987.8955, grad_fn=<NegBackward0>) tensor(10987.4902, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10985.7646484375
tensor(10987.4902, grad_fn=<NegBackward0>) tensor(10985.7646, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10985.5732421875
tensor(10985.7646, grad_fn=<NegBackward0>) tensor(10985.5732, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10985.5048828125
tensor(10985.5732, grad_fn=<NegBackward0>) tensor(10985.5049, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10985.46484375
tensor(10985.5049, grad_fn=<NegBackward0>) tensor(10985.4648, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10985.431640625
tensor(10985.4648, grad_fn=<NegBackward0>) tensor(10985.4316, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10985.4033203125
tensor(10985.4316, grad_fn=<NegBackward0>) tensor(10985.4033, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10985.3779296875
tensor(10985.4033, grad_fn=<NegBackward0>) tensor(10985.3779, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10985.357421875
tensor(10985.3779, grad_fn=<NegBackward0>) tensor(10985.3574, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10985.34765625
tensor(10985.3574, grad_fn=<NegBackward0>) tensor(10985.3477, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10985.3388671875
tensor(10985.3477, grad_fn=<NegBackward0>) tensor(10985.3389, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10985.337890625
tensor(10985.3389, grad_fn=<NegBackward0>) tensor(10985.3379, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10985.3369140625
tensor(10985.3379, grad_fn=<NegBackward0>) tensor(10985.3369, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10985.3349609375
tensor(10985.3369, grad_fn=<NegBackward0>) tensor(10985.3350, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10985.333984375
tensor(10985.3350, grad_fn=<NegBackward0>) tensor(10985.3340, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10985.3330078125
tensor(10985.3340, grad_fn=<NegBackward0>) tensor(10985.3330, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10985.333984375
tensor(10985.3330, grad_fn=<NegBackward0>) tensor(10985.3340, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -10985.333984375
tensor(10985.3330, grad_fn=<NegBackward0>) tensor(10985.3340, grad_fn=<NegBackward0>)
2
Iteration 3100: Loss = -10985.3330078125
tensor(10985.3330, grad_fn=<NegBackward0>) tensor(10985.3330, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10985.33203125
tensor(10985.3330, grad_fn=<NegBackward0>) tensor(10985.3320, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10985.3330078125
tensor(10985.3320, grad_fn=<NegBackward0>) tensor(10985.3330, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -10985.33203125
tensor(10985.3320, grad_fn=<NegBackward0>) tensor(10985.3320, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10985.33203125
tensor(10985.3320, grad_fn=<NegBackward0>) tensor(10985.3320, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10985.33203125
tensor(10985.3320, grad_fn=<NegBackward0>) tensor(10985.3320, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10985.3310546875
tensor(10985.3320, grad_fn=<NegBackward0>) tensor(10985.3311, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10985.33203125
tensor(10985.3311, grad_fn=<NegBackward0>) tensor(10985.3320, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -10985.330078125
tensor(10985.3311, grad_fn=<NegBackward0>) tensor(10985.3301, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10985.3310546875
tensor(10985.3301, grad_fn=<NegBackward0>) tensor(10985.3311, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -10985.333984375
tensor(10985.3301, grad_fn=<NegBackward0>) tensor(10985.3340, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -10985.3310546875
tensor(10985.3301, grad_fn=<NegBackward0>) tensor(10985.3311, grad_fn=<NegBackward0>)
3
Iteration 4300: Loss = -10985.3291015625
tensor(10985.3301, grad_fn=<NegBackward0>) tensor(10985.3291, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10985.330078125
tensor(10985.3291, grad_fn=<NegBackward0>) tensor(10985.3301, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10985.3310546875
tensor(10985.3291, grad_fn=<NegBackward0>) tensor(10985.3311, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -10985.333984375
tensor(10985.3291, grad_fn=<NegBackward0>) tensor(10985.3340, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -10985.330078125
tensor(10985.3291, grad_fn=<NegBackward0>) tensor(10985.3301, grad_fn=<NegBackward0>)
4
Iteration 4800: Loss = -10985.330078125
tensor(10985.3291, grad_fn=<NegBackward0>) tensor(10985.3301, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4800 due to no improvement.
pi: tensor([[0.5188, 0.4812],
        [0.0642, 0.9358]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0101, 0.9899], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1175, 0.0209],
         [0.6546, 0.1700]],

        [[0.6405, 0.1484],
         [0.5240, 0.7037]],

        [[0.5751, 0.1329],
         [0.6024, 0.7012]],

        [[0.5809, 0.1130],
         [0.6224, 0.5078]],

        [[0.5739, 0.1171],
         [0.6872, 0.6947]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.02548983702618568
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.01701445012943135
Global Adjusted Rand Index: -0.002564614317456072
Average Adjusted Rand Index: 0.006495790046094921
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23500.75390625
inf tensor(23500.7539, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10994.84765625
tensor(23500.7539, grad_fn=<NegBackward0>) tensor(10994.8477, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10994.2607421875
tensor(10994.8477, grad_fn=<NegBackward0>) tensor(10994.2607, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10994.1005859375
tensor(10994.2607, grad_fn=<NegBackward0>) tensor(10994.1006, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10994.025390625
tensor(10994.1006, grad_fn=<NegBackward0>) tensor(10994.0254, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10993.9814453125
tensor(10994.0254, grad_fn=<NegBackward0>) tensor(10993.9814, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10993.955078125
tensor(10993.9814, grad_fn=<NegBackward0>) tensor(10993.9551, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10993.93359375
tensor(10993.9551, grad_fn=<NegBackward0>) tensor(10993.9336, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10993.9169921875
tensor(10993.9336, grad_fn=<NegBackward0>) tensor(10993.9170, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10993.90234375
tensor(10993.9170, grad_fn=<NegBackward0>) tensor(10993.9023, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10993.8896484375
tensor(10993.9023, grad_fn=<NegBackward0>) tensor(10993.8896, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10993.8759765625
tensor(10993.8896, grad_fn=<NegBackward0>) tensor(10993.8760, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10993.865234375
tensor(10993.8760, grad_fn=<NegBackward0>) tensor(10993.8652, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10993.853515625
tensor(10993.8652, grad_fn=<NegBackward0>) tensor(10993.8535, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10993.841796875
tensor(10993.8535, grad_fn=<NegBackward0>) tensor(10993.8418, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10993.833984375
tensor(10993.8418, grad_fn=<NegBackward0>) tensor(10993.8340, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10993.8251953125
tensor(10993.8340, grad_fn=<NegBackward0>) tensor(10993.8252, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10993.81640625
tensor(10993.8252, grad_fn=<NegBackward0>) tensor(10993.8164, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10993.80859375
tensor(10993.8164, grad_fn=<NegBackward0>) tensor(10993.8086, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10993.798828125
tensor(10993.8086, grad_fn=<NegBackward0>) tensor(10993.7988, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10993.7900390625
tensor(10993.7988, grad_fn=<NegBackward0>) tensor(10993.7900, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10993.7783203125
tensor(10993.7900, grad_fn=<NegBackward0>) tensor(10993.7783, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10993.7666015625
tensor(10993.7783, grad_fn=<NegBackward0>) tensor(10993.7666, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10993.7490234375
tensor(10993.7666, grad_fn=<NegBackward0>) tensor(10993.7490, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10993.7333984375
tensor(10993.7490, grad_fn=<NegBackward0>) tensor(10993.7334, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10993.7109375
tensor(10993.7334, grad_fn=<NegBackward0>) tensor(10993.7109, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10993.6875
tensor(10993.7109, grad_fn=<NegBackward0>) tensor(10993.6875, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10993.66015625
tensor(10993.6875, grad_fn=<NegBackward0>) tensor(10993.6602, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10993.6298828125
tensor(10993.6602, grad_fn=<NegBackward0>) tensor(10993.6299, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10993.5927734375
tensor(10993.6299, grad_fn=<NegBackward0>) tensor(10993.5928, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10993.546875
tensor(10993.5928, grad_fn=<NegBackward0>) tensor(10993.5469, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10993.4775390625
tensor(10993.5469, grad_fn=<NegBackward0>) tensor(10993.4775, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10993.328125
tensor(10993.4775, grad_fn=<NegBackward0>) tensor(10993.3281, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10992.0908203125
tensor(10993.3281, grad_fn=<NegBackward0>) tensor(10992.0908, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10991.173828125
tensor(10992.0908, grad_fn=<NegBackward0>) tensor(10991.1738, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10991.0546875
tensor(10991.1738, grad_fn=<NegBackward0>) tensor(10991.0547, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10991.0185546875
tensor(10991.0547, grad_fn=<NegBackward0>) tensor(10991.0186, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10991.0
tensor(10991.0186, grad_fn=<NegBackward0>) tensor(10991., grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10990.9873046875
tensor(10991., grad_fn=<NegBackward0>) tensor(10990.9873, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10990.9775390625
tensor(10990.9873, grad_fn=<NegBackward0>) tensor(10990.9775, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10990.970703125
tensor(10990.9775, grad_fn=<NegBackward0>) tensor(10990.9707, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10990.962890625
tensor(10990.9707, grad_fn=<NegBackward0>) tensor(10990.9629, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10986.2529296875
tensor(10990.9629, grad_fn=<NegBackward0>) tensor(10986.2529, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10985.3994140625
tensor(10986.2529, grad_fn=<NegBackward0>) tensor(10985.3994, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10985.365234375
tensor(10985.3994, grad_fn=<NegBackward0>) tensor(10985.3652, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10985.353515625
tensor(10985.3652, grad_fn=<NegBackward0>) tensor(10985.3535, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10985.341796875
tensor(10985.3535, grad_fn=<NegBackward0>) tensor(10985.3418, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10985.3408203125
tensor(10985.3418, grad_fn=<NegBackward0>) tensor(10985.3408, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10985.3369140625
tensor(10985.3408, grad_fn=<NegBackward0>) tensor(10985.3369, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10985.3359375
tensor(10985.3369, grad_fn=<NegBackward0>) tensor(10985.3359, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10985.333984375
tensor(10985.3359, grad_fn=<NegBackward0>) tensor(10985.3340, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10985.333984375
tensor(10985.3340, grad_fn=<NegBackward0>) tensor(10985.3340, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10985.3330078125
tensor(10985.3340, grad_fn=<NegBackward0>) tensor(10985.3330, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10985.333984375
tensor(10985.3330, grad_fn=<NegBackward0>) tensor(10985.3340, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10985.3310546875
tensor(10985.3330, grad_fn=<NegBackward0>) tensor(10985.3311, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10985.33203125
tensor(10985.3311, grad_fn=<NegBackward0>) tensor(10985.3320, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10985.33203125
tensor(10985.3311, grad_fn=<NegBackward0>) tensor(10985.3320, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -10985.33203125
tensor(10985.3311, grad_fn=<NegBackward0>) tensor(10985.3320, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -10985.33203125
tensor(10985.3311, grad_fn=<NegBackward0>) tensor(10985.3320, grad_fn=<NegBackward0>)
4
Iteration 5900: Loss = -10985.33203125
tensor(10985.3311, grad_fn=<NegBackward0>) tensor(10985.3320, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5900 due to no improvement.
pi: tensor([[0.5188, 0.4812],
        [0.0642, 0.9358]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0101, 0.9899], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1175, 0.0209],
         [0.6203, 0.1700]],

        [[0.6220, 0.1484],
         [0.6687, 0.7216]],

        [[0.6649, 0.1329],
         [0.6555, 0.5344]],

        [[0.6293, 0.1130],
         [0.6211, 0.6420]],

        [[0.5285, 0.1170],
         [0.6081, 0.7248]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.02548983702618568
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.01701445012943135
Global Adjusted Rand Index: -0.002564614317456072
Average Adjusted Rand Index: 0.006495790046094921
[-0.002564614317456072, -0.002564614317456072] [0.006495790046094921, 0.006495790046094921] [10985.330078125, 10985.33203125]
-------------------------------------
This iteration is 66
True Objective function: Loss = -11015.71656986485
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22695.44140625
inf tensor(22695.4414, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11159.013671875
tensor(22695.4414, grad_fn=<NegBackward0>) tensor(11159.0137, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11157.5048828125
tensor(11159.0137, grad_fn=<NegBackward0>) tensor(11157.5049, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11156.8603515625
tensor(11157.5049, grad_fn=<NegBackward0>) tensor(11156.8604, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11156.4404296875
tensor(11156.8604, grad_fn=<NegBackward0>) tensor(11156.4404, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11156.10546875
tensor(11156.4404, grad_fn=<NegBackward0>) tensor(11156.1055, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11155.814453125
tensor(11156.1055, grad_fn=<NegBackward0>) tensor(11155.8145, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11155.5166015625
tensor(11155.8145, grad_fn=<NegBackward0>) tensor(11155.5166, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11155.2158203125
tensor(11155.5166, grad_fn=<NegBackward0>) tensor(11155.2158, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11154.947265625
tensor(11155.2158, grad_fn=<NegBackward0>) tensor(11154.9473, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11154.7353515625
tensor(11154.9473, grad_fn=<NegBackward0>) tensor(11154.7354, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11154.5400390625
tensor(11154.7354, grad_fn=<NegBackward0>) tensor(11154.5400, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11154.380859375
tensor(11154.5400, grad_fn=<NegBackward0>) tensor(11154.3809, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11154.2353515625
tensor(11154.3809, grad_fn=<NegBackward0>) tensor(11154.2354, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11154.0771484375
tensor(11154.2354, grad_fn=<NegBackward0>) tensor(11154.0771, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11153.912109375
tensor(11154.0771, grad_fn=<NegBackward0>) tensor(11153.9121, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11153.7548828125
tensor(11153.9121, grad_fn=<NegBackward0>) tensor(11153.7549, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11153.5859375
tensor(11153.7549, grad_fn=<NegBackward0>) tensor(11153.5859, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11153.4775390625
tensor(11153.5859, grad_fn=<NegBackward0>) tensor(11153.4775, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11153.384765625
tensor(11153.4775, grad_fn=<NegBackward0>) tensor(11153.3848, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11153.2841796875
tensor(11153.3848, grad_fn=<NegBackward0>) tensor(11153.2842, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11153.197265625
tensor(11153.2842, grad_fn=<NegBackward0>) tensor(11153.1973, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11153.0810546875
tensor(11153.1973, grad_fn=<NegBackward0>) tensor(11153.0811, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11152.9111328125
tensor(11153.0811, grad_fn=<NegBackward0>) tensor(11152.9111, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11152.7734375
tensor(11152.9111, grad_fn=<NegBackward0>) tensor(11152.7734, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11152.66015625
tensor(11152.7734, grad_fn=<NegBackward0>) tensor(11152.6602, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11152.5634765625
tensor(11152.6602, grad_fn=<NegBackward0>) tensor(11152.5635, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11152.484375
tensor(11152.5635, grad_fn=<NegBackward0>) tensor(11152.4844, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11152.419921875
tensor(11152.4844, grad_fn=<NegBackward0>) tensor(11152.4199, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11152.361328125
tensor(11152.4199, grad_fn=<NegBackward0>) tensor(11152.3613, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11152.2900390625
tensor(11152.3613, grad_fn=<NegBackward0>) tensor(11152.2900, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11152.189453125
tensor(11152.2900, grad_fn=<NegBackward0>) tensor(11152.1895, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11151.7841796875
tensor(11152.1895, grad_fn=<NegBackward0>) tensor(11151.7842, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11052.1474609375
tensor(11151.7842, grad_fn=<NegBackward0>) tensor(11052.1475, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11050.939453125
tensor(11052.1475, grad_fn=<NegBackward0>) tensor(11050.9395, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11045.6943359375
tensor(11050.9395, grad_fn=<NegBackward0>) tensor(11045.6943, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11044.068359375
tensor(11045.6943, grad_fn=<NegBackward0>) tensor(11044.0684, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11041.3740234375
tensor(11044.0684, grad_fn=<NegBackward0>) tensor(11041.3740, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11039.1396484375
tensor(11041.3740, grad_fn=<NegBackward0>) tensor(11039.1396, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11038.76171875
tensor(11039.1396, grad_fn=<NegBackward0>) tensor(11038.7617, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11038.3896484375
tensor(11038.7617, grad_fn=<NegBackward0>) tensor(11038.3896, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11035.509765625
tensor(11038.3896, grad_fn=<NegBackward0>) tensor(11035.5098, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11035.4619140625
tensor(11035.5098, grad_fn=<NegBackward0>) tensor(11035.4619, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11035.3271484375
tensor(11035.4619, grad_fn=<NegBackward0>) tensor(11035.3271, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11035.3095703125
tensor(11035.3271, grad_fn=<NegBackward0>) tensor(11035.3096, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11035.2958984375
tensor(11035.3096, grad_fn=<NegBackward0>) tensor(11035.2959, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11035.015625
tensor(11035.2959, grad_fn=<NegBackward0>) tensor(11035.0156, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11035.005859375
tensor(11035.0156, grad_fn=<NegBackward0>) tensor(11035.0059, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11034.9970703125
tensor(11035.0059, grad_fn=<NegBackward0>) tensor(11034.9971, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11031.23046875
tensor(11034.9971, grad_fn=<NegBackward0>) tensor(11031.2305, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11031.1328125
tensor(11031.2305, grad_fn=<NegBackward0>) tensor(11031.1328, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11031.1181640625
tensor(11031.1328, grad_fn=<NegBackward0>) tensor(11031.1182, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11031.1171875
tensor(11031.1182, grad_fn=<NegBackward0>) tensor(11031.1172, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11031.11328125
tensor(11031.1172, grad_fn=<NegBackward0>) tensor(11031.1133, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11031.1044921875
tensor(11031.1133, grad_fn=<NegBackward0>) tensor(11031.1045, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11031.1005859375
tensor(11031.1045, grad_fn=<NegBackward0>) tensor(11031.1006, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11031.099609375
tensor(11031.1006, grad_fn=<NegBackward0>) tensor(11031.0996, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11031.10546875
tensor(11031.0996, grad_fn=<NegBackward0>) tensor(11031.1055, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11031.0986328125
tensor(11031.0996, grad_fn=<NegBackward0>) tensor(11031.0986, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11031.0888671875
tensor(11031.0986, grad_fn=<NegBackward0>) tensor(11031.0889, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11030.7294921875
tensor(11031.0889, grad_fn=<NegBackward0>) tensor(11030.7295, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11030.7265625
tensor(11030.7295, grad_fn=<NegBackward0>) tensor(11030.7266, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11030.6982421875
tensor(11030.7266, grad_fn=<NegBackward0>) tensor(11030.6982, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11030.6953125
tensor(11030.6982, grad_fn=<NegBackward0>) tensor(11030.6953, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11030.693359375
tensor(11030.6953, grad_fn=<NegBackward0>) tensor(11030.6934, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11030.6904296875
tensor(11030.6934, grad_fn=<NegBackward0>) tensor(11030.6904, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11030.6884765625
tensor(11030.6904, grad_fn=<NegBackward0>) tensor(11030.6885, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11030.685546875
tensor(11030.6885, grad_fn=<NegBackward0>) tensor(11030.6855, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11021.5146484375
tensor(11030.6855, grad_fn=<NegBackward0>) tensor(11021.5146, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11019.3173828125
tensor(11021.5146, grad_fn=<NegBackward0>) tensor(11019.3174, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11019.2978515625
tensor(11019.3174, grad_fn=<NegBackward0>) tensor(11019.2979, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11019.2822265625
tensor(11019.2979, grad_fn=<NegBackward0>) tensor(11019.2822, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11019.27734375
tensor(11019.2822, grad_fn=<NegBackward0>) tensor(11019.2773, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11019.2744140625
tensor(11019.2773, grad_fn=<NegBackward0>) tensor(11019.2744, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11019.2734375
tensor(11019.2744, grad_fn=<NegBackward0>) tensor(11019.2734, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11019.2490234375
tensor(11019.2734, grad_fn=<NegBackward0>) tensor(11019.2490, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11019.166015625
tensor(11019.2490, grad_fn=<NegBackward0>) tensor(11019.1660, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11019.1572265625
tensor(11019.1660, grad_fn=<NegBackward0>) tensor(11019.1572, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11019.1611328125
tensor(11019.1572, grad_fn=<NegBackward0>) tensor(11019.1611, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11019.220703125
tensor(11019.1572, grad_fn=<NegBackward0>) tensor(11019.2207, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11019.1533203125
tensor(11019.1572, grad_fn=<NegBackward0>) tensor(11019.1533, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11019.1220703125
tensor(11019.1533, grad_fn=<NegBackward0>) tensor(11019.1221, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11019.123046875
tensor(11019.1221, grad_fn=<NegBackward0>) tensor(11019.1230, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11019.1181640625
tensor(11019.1221, grad_fn=<NegBackward0>) tensor(11019.1182, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11019.1083984375
tensor(11019.1182, grad_fn=<NegBackward0>) tensor(11019.1084, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11019.12109375
tensor(11019.1084, grad_fn=<NegBackward0>) tensor(11019.1211, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11019.068359375
tensor(11019.1084, grad_fn=<NegBackward0>) tensor(11019.0684, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11019.0673828125
tensor(11019.0684, grad_fn=<NegBackward0>) tensor(11019.0674, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11019.0673828125
tensor(11019.0674, grad_fn=<NegBackward0>) tensor(11019.0674, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11019.0673828125
tensor(11019.0674, grad_fn=<NegBackward0>) tensor(11019.0674, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11019.0693359375
tensor(11019.0674, grad_fn=<NegBackward0>) tensor(11019.0693, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11019.06640625
tensor(11019.0674, grad_fn=<NegBackward0>) tensor(11019.0664, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11019.0693359375
tensor(11019.0664, grad_fn=<NegBackward0>) tensor(11019.0693, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11019.0615234375
tensor(11019.0664, grad_fn=<NegBackward0>) tensor(11019.0615, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11019.060546875
tensor(11019.0615, grad_fn=<NegBackward0>) tensor(11019.0605, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11019.056640625
tensor(11019.0605, grad_fn=<NegBackward0>) tensor(11019.0566, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11019.05859375
tensor(11019.0566, grad_fn=<NegBackward0>) tensor(11019.0586, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11019.0546875
tensor(11019.0566, grad_fn=<NegBackward0>) tensor(11019.0547, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11019.2275390625
tensor(11019.0547, grad_fn=<NegBackward0>) tensor(11019.2275, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11019.046875
tensor(11019.0547, grad_fn=<NegBackward0>) tensor(11019.0469, grad_fn=<NegBackward0>)
pi: tensor([[0.6834, 0.3166],
        [0.3613, 0.6387]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3113, 0.6887], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2556, 0.0877],
         [0.6788, 0.2143]],

        [[0.7137, 0.0953],
         [0.7115, 0.6637]],

        [[0.6043, 0.0982],
         [0.6034, 0.5976]],

        [[0.5211, 0.1120],
         [0.5583, 0.5587]],

        [[0.7051, 0.0983],
         [0.6851, 0.6875]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 85
Adjusted Rand Index: 0.48510267107807936
time is 1
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 2
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.39822161639915415
Average Adjusted Rand Index: 0.8418234645118291
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24368.658203125
inf tensor(24368.6582, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11159.841796875
tensor(24368.6582, grad_fn=<NegBackward0>) tensor(11159.8418, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11158.4404296875
tensor(11159.8418, grad_fn=<NegBackward0>) tensor(11158.4404, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11155.8916015625
tensor(11158.4404, grad_fn=<NegBackward0>) tensor(11155.8916, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11155.3134765625
tensor(11155.8916, grad_fn=<NegBackward0>) tensor(11155.3135, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11154.61328125
tensor(11155.3135, grad_fn=<NegBackward0>) tensor(11154.6133, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11152.130859375
tensor(11154.6133, grad_fn=<NegBackward0>) tensor(11152.1309, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11150.1318359375
tensor(11152.1309, grad_fn=<NegBackward0>) tensor(11150.1318, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11148.0361328125
tensor(11150.1318, grad_fn=<NegBackward0>) tensor(11148.0361, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11146.2734375
tensor(11148.0361, grad_fn=<NegBackward0>) tensor(11146.2734, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11146.1005859375
tensor(11146.2734, grad_fn=<NegBackward0>) tensor(11146.1006, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11146.0205078125
tensor(11146.1006, grad_fn=<NegBackward0>) tensor(11146.0205, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11145.9677734375
tensor(11146.0205, grad_fn=<NegBackward0>) tensor(11145.9678, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11145.9306640625
tensor(11145.9678, grad_fn=<NegBackward0>) tensor(11145.9307, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11145.9013671875
tensor(11145.9307, grad_fn=<NegBackward0>) tensor(11145.9014, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11145.876953125
tensor(11145.9014, grad_fn=<NegBackward0>) tensor(11145.8770, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11145.8564453125
tensor(11145.8770, grad_fn=<NegBackward0>) tensor(11145.8564, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11145.841796875
tensor(11145.8564, grad_fn=<NegBackward0>) tensor(11145.8418, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11145.8271484375
tensor(11145.8418, grad_fn=<NegBackward0>) tensor(11145.8271, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11145.8193359375
tensor(11145.8271, grad_fn=<NegBackward0>) tensor(11145.8193, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11145.814453125
tensor(11145.8193, grad_fn=<NegBackward0>) tensor(11145.8145, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11145.806640625
tensor(11145.8145, grad_fn=<NegBackward0>) tensor(11145.8066, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11145.8046875
tensor(11145.8066, grad_fn=<NegBackward0>) tensor(11145.8047, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11145.8037109375
tensor(11145.8047, grad_fn=<NegBackward0>) tensor(11145.8037, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11145.80078125
tensor(11145.8037, grad_fn=<NegBackward0>) tensor(11145.8008, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11145.798828125
tensor(11145.8008, grad_fn=<NegBackward0>) tensor(11145.7988, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11145.7978515625
tensor(11145.7988, grad_fn=<NegBackward0>) tensor(11145.7979, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11145.794921875
tensor(11145.7979, grad_fn=<NegBackward0>) tensor(11145.7949, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11145.7939453125
tensor(11145.7949, grad_fn=<NegBackward0>) tensor(11145.7939, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11145.7939453125
tensor(11145.7939, grad_fn=<NegBackward0>) tensor(11145.7939, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11145.7919921875
tensor(11145.7939, grad_fn=<NegBackward0>) tensor(11145.7920, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11145.791015625
tensor(11145.7920, grad_fn=<NegBackward0>) tensor(11145.7910, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11145.791015625
tensor(11145.7910, grad_fn=<NegBackward0>) tensor(11145.7910, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11145.7890625
tensor(11145.7910, grad_fn=<NegBackward0>) tensor(11145.7891, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11145.7880859375
tensor(11145.7891, grad_fn=<NegBackward0>) tensor(11145.7881, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11145.7880859375
tensor(11145.7881, grad_fn=<NegBackward0>) tensor(11145.7881, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11145.787109375
tensor(11145.7881, grad_fn=<NegBackward0>) tensor(11145.7871, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11145.787109375
tensor(11145.7871, grad_fn=<NegBackward0>) tensor(11145.7871, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11145.7890625
tensor(11145.7871, grad_fn=<NegBackward0>) tensor(11145.7891, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11145.7861328125
tensor(11145.7871, grad_fn=<NegBackward0>) tensor(11145.7861, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11145.78515625
tensor(11145.7861, grad_fn=<NegBackward0>) tensor(11145.7852, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11145.7861328125
tensor(11145.7852, grad_fn=<NegBackward0>) tensor(11145.7861, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11145.7841796875
tensor(11145.7852, grad_fn=<NegBackward0>) tensor(11145.7842, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11145.7861328125
tensor(11145.7842, grad_fn=<NegBackward0>) tensor(11145.7861, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11145.7841796875
tensor(11145.7842, grad_fn=<NegBackward0>) tensor(11145.7842, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11145.783203125
tensor(11145.7842, grad_fn=<NegBackward0>) tensor(11145.7832, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11145.7861328125
tensor(11145.7832, grad_fn=<NegBackward0>) tensor(11145.7861, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11145.783203125
tensor(11145.7832, grad_fn=<NegBackward0>) tensor(11145.7832, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11145.7822265625
tensor(11145.7832, grad_fn=<NegBackward0>) tensor(11145.7822, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11145.783203125
tensor(11145.7822, grad_fn=<NegBackward0>) tensor(11145.7832, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11145.783203125
tensor(11145.7822, grad_fn=<NegBackward0>) tensor(11145.7832, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11145.79296875
tensor(11145.7822, grad_fn=<NegBackward0>) tensor(11145.7930, grad_fn=<NegBackward0>)
3
Iteration 5200: Loss = -11145.78125
tensor(11145.7822, grad_fn=<NegBackward0>) tensor(11145.7812, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11145.779296875
tensor(11145.7812, grad_fn=<NegBackward0>) tensor(11145.7793, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11145.78125
tensor(11145.7793, grad_fn=<NegBackward0>) tensor(11145.7812, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11145.78125
tensor(11145.7793, grad_fn=<NegBackward0>) tensor(11145.7812, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11145.78125
tensor(11145.7793, grad_fn=<NegBackward0>) tensor(11145.7812, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -11145.7802734375
tensor(11145.7793, grad_fn=<NegBackward0>) tensor(11145.7803, grad_fn=<NegBackward0>)
4
Iteration 5800: Loss = -11145.7802734375
tensor(11145.7793, grad_fn=<NegBackward0>) tensor(11145.7803, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5800 due to no improvement.
pi: tensor([[4.7567e-04, 9.9952e-01],
        [3.6729e-02, 9.6327e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2949, 0.7051], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2160, 0.0898],
         [0.6252, 0.1732]],

        [[0.7107, 0.1206],
         [0.6724, 0.7125]],

        [[0.6275, 0.1317],
         [0.6289, 0.6531]],

        [[0.7069, 0.2664],
         [0.5626, 0.5173]],

        [[0.6177, 0.1513],
         [0.5886, 0.7305]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 82
Adjusted Rand Index: 0.4039647577092511
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.01636565176714659
Average Adjusted Rand Index: 0.08095469579184214
[0.39822161639915415, 0.01636565176714659] [0.8418234645118291, 0.08095469579184214] [11019.052734375, 11145.7802734375]
-------------------------------------
This iteration is 67
True Objective function: Loss = -10865.799434010527
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22561.87890625
inf tensor(22561.8789, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11020.33984375
tensor(22561.8789, grad_fn=<NegBackward0>) tensor(11020.3398, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11019.2041015625
tensor(11020.3398, grad_fn=<NegBackward0>) tensor(11019.2041, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11018.3525390625
tensor(11019.2041, grad_fn=<NegBackward0>) tensor(11018.3525, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11017.671875
tensor(11018.3525, grad_fn=<NegBackward0>) tensor(11017.6719, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11017.1767578125
tensor(11017.6719, grad_fn=<NegBackward0>) tensor(11017.1768, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11016.74609375
tensor(11017.1768, grad_fn=<NegBackward0>) tensor(11016.7461, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11016.302734375
tensor(11016.7461, grad_fn=<NegBackward0>) tensor(11016.3027, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11015.8173828125
tensor(11016.3027, grad_fn=<NegBackward0>) tensor(11015.8174, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11015.5234375
tensor(11015.8174, grad_fn=<NegBackward0>) tensor(11015.5234, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11015.2998046875
tensor(11015.5234, grad_fn=<NegBackward0>) tensor(11015.2998, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11015.095703125
tensor(11015.2998, grad_fn=<NegBackward0>) tensor(11015.0957, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11014.7783203125
tensor(11015.0957, grad_fn=<NegBackward0>) tensor(11014.7783, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10987.41796875
tensor(11014.7783, grad_fn=<NegBackward0>) tensor(10987.4180, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10882.6865234375
tensor(10987.4180, grad_fn=<NegBackward0>) tensor(10882.6865, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10870.783203125
tensor(10882.6865, grad_fn=<NegBackward0>) tensor(10870.7832, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10870.5947265625
tensor(10870.7832, grad_fn=<NegBackward0>) tensor(10870.5947, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10870.498046875
tensor(10870.5947, grad_fn=<NegBackward0>) tensor(10870.4980, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10870.44140625
tensor(10870.4980, grad_fn=<NegBackward0>) tensor(10870.4414, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10870.4052734375
tensor(10870.4414, grad_fn=<NegBackward0>) tensor(10870.4053, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10870.369140625
tensor(10870.4053, grad_fn=<NegBackward0>) tensor(10870.3691, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10870.3154296875
tensor(10870.3691, grad_fn=<NegBackward0>) tensor(10870.3154, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10870.28125
tensor(10870.3154, grad_fn=<NegBackward0>) tensor(10870.2812, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10870.26953125
tensor(10870.2812, grad_fn=<NegBackward0>) tensor(10870.2695, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10870.2587890625
tensor(10870.2695, grad_fn=<NegBackward0>) tensor(10870.2588, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10870.2470703125
tensor(10870.2588, grad_fn=<NegBackward0>) tensor(10870.2471, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10870.2314453125
tensor(10870.2471, grad_fn=<NegBackward0>) tensor(10870.2314, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10870.1064453125
tensor(10870.2314, grad_fn=<NegBackward0>) tensor(10870.1064, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10870.0849609375
tensor(10870.1064, grad_fn=<NegBackward0>) tensor(10870.0850, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10870.0146484375
tensor(10870.0850, grad_fn=<NegBackward0>) tensor(10870.0146, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10868.7314453125
tensor(10870.0146, grad_fn=<NegBackward0>) tensor(10868.7314, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10866.591796875
tensor(10868.7314, grad_fn=<NegBackward0>) tensor(10866.5918, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10866.3056640625
tensor(10866.5918, grad_fn=<NegBackward0>) tensor(10866.3057, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10860.33984375
tensor(10866.3057, grad_fn=<NegBackward0>) tensor(10860.3398, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10859.173828125
tensor(10860.3398, grad_fn=<NegBackward0>) tensor(10859.1738, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10856.73828125
tensor(10859.1738, grad_fn=<NegBackward0>) tensor(10856.7383, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10856.7158203125
tensor(10856.7383, grad_fn=<NegBackward0>) tensor(10856.7158, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10856.7060546875
tensor(10856.7158, grad_fn=<NegBackward0>) tensor(10856.7061, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10856.69140625
tensor(10856.7061, grad_fn=<NegBackward0>) tensor(10856.6914, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10849.263671875
tensor(10856.6914, grad_fn=<NegBackward0>) tensor(10849.2637, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10847.875
tensor(10849.2637, grad_fn=<NegBackward0>) tensor(10847.8750, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10847.6005859375
tensor(10847.8750, grad_fn=<NegBackward0>) tensor(10847.6006, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10847.611328125
tensor(10847.6006, grad_fn=<NegBackward0>) tensor(10847.6113, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10847.5869140625
tensor(10847.6006, grad_fn=<NegBackward0>) tensor(10847.5869, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10847.5859375
tensor(10847.5869, grad_fn=<NegBackward0>) tensor(10847.5859, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10847.5869140625
tensor(10847.5859, grad_fn=<NegBackward0>) tensor(10847.5869, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10847.5859375
tensor(10847.5859, grad_fn=<NegBackward0>) tensor(10847.5859, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10847.58984375
tensor(10847.5859, grad_fn=<NegBackward0>) tensor(10847.5898, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10847.58984375
tensor(10847.5859, grad_fn=<NegBackward0>) tensor(10847.5898, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -10847.5859375
tensor(10847.5859, grad_fn=<NegBackward0>) tensor(10847.5859, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10847.58203125
tensor(10847.5859, grad_fn=<NegBackward0>) tensor(10847.5820, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10847.580078125
tensor(10847.5820, grad_fn=<NegBackward0>) tensor(10847.5801, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10847.578125
tensor(10847.5801, grad_fn=<NegBackward0>) tensor(10847.5781, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10847.5830078125
tensor(10847.5781, grad_fn=<NegBackward0>) tensor(10847.5830, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10847.578125
tensor(10847.5781, grad_fn=<NegBackward0>) tensor(10847.5781, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10847.5751953125
tensor(10847.5781, grad_fn=<NegBackward0>) tensor(10847.5752, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10847.5751953125
tensor(10847.5752, grad_fn=<NegBackward0>) tensor(10847.5752, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10847.57421875
tensor(10847.5752, grad_fn=<NegBackward0>) tensor(10847.5742, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10847.57421875
tensor(10847.5742, grad_fn=<NegBackward0>) tensor(10847.5742, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10847.5751953125
tensor(10847.5742, grad_fn=<NegBackward0>) tensor(10847.5752, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10847.5732421875
tensor(10847.5742, grad_fn=<NegBackward0>) tensor(10847.5732, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10847.5732421875
tensor(10847.5732, grad_fn=<NegBackward0>) tensor(10847.5732, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10847.5830078125
tensor(10847.5732, grad_fn=<NegBackward0>) tensor(10847.5830, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10847.572265625
tensor(10847.5732, grad_fn=<NegBackward0>) tensor(10847.5723, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10847.5732421875
tensor(10847.5723, grad_fn=<NegBackward0>) tensor(10847.5732, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10847.572265625
tensor(10847.5723, grad_fn=<NegBackward0>) tensor(10847.5723, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10847.572265625
tensor(10847.5723, grad_fn=<NegBackward0>) tensor(10847.5723, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10847.572265625
tensor(10847.5723, grad_fn=<NegBackward0>) tensor(10847.5723, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10847.552734375
tensor(10847.5723, grad_fn=<NegBackward0>) tensor(10847.5527, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10847.5400390625
tensor(10847.5527, grad_fn=<NegBackward0>) tensor(10847.5400, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10847.53515625
tensor(10847.5400, grad_fn=<NegBackward0>) tensor(10847.5352, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10847.533203125
tensor(10847.5352, grad_fn=<NegBackward0>) tensor(10847.5332, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10847.537109375
tensor(10847.5332, grad_fn=<NegBackward0>) tensor(10847.5371, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10847.5322265625
tensor(10847.5332, grad_fn=<NegBackward0>) tensor(10847.5322, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10847.5322265625
tensor(10847.5322, grad_fn=<NegBackward0>) tensor(10847.5322, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10847.53515625
tensor(10847.5322, grad_fn=<NegBackward0>) tensor(10847.5352, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10847.5322265625
tensor(10847.5322, grad_fn=<NegBackward0>) tensor(10847.5322, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10847.53515625
tensor(10847.5322, grad_fn=<NegBackward0>) tensor(10847.5352, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10847.5322265625
tensor(10847.5322, grad_fn=<NegBackward0>) tensor(10847.5322, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10847.5458984375
tensor(10847.5322, grad_fn=<NegBackward0>) tensor(10847.5459, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10847.53125
tensor(10847.5322, grad_fn=<NegBackward0>) tensor(10847.5312, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10847.591796875
tensor(10847.5312, grad_fn=<NegBackward0>) tensor(10847.5918, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10847.53125
tensor(10847.5312, grad_fn=<NegBackward0>) tensor(10847.5312, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10847.53125
tensor(10847.5312, grad_fn=<NegBackward0>) tensor(10847.5312, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10847.5341796875
tensor(10847.5312, grad_fn=<NegBackward0>) tensor(10847.5342, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -10847.53125
tensor(10847.5312, grad_fn=<NegBackward0>) tensor(10847.5312, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10847.5322265625
tensor(10847.5312, grad_fn=<NegBackward0>) tensor(10847.5322, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10847.5322265625
tensor(10847.5312, grad_fn=<NegBackward0>) tensor(10847.5322, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -10847.5302734375
tensor(10847.5312, grad_fn=<NegBackward0>) tensor(10847.5303, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10847.5634765625
tensor(10847.5303, grad_fn=<NegBackward0>) tensor(10847.5635, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -10847.53125
tensor(10847.5303, grad_fn=<NegBackward0>) tensor(10847.5312, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -10847.533203125
tensor(10847.5303, grad_fn=<NegBackward0>) tensor(10847.5332, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -10847.529296875
tensor(10847.5303, grad_fn=<NegBackward0>) tensor(10847.5293, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10847.529296875
tensor(10847.5293, grad_fn=<NegBackward0>) tensor(10847.5293, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10847.529296875
tensor(10847.5293, grad_fn=<NegBackward0>) tensor(10847.5293, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10847.5380859375
tensor(10847.5293, grad_fn=<NegBackward0>) tensor(10847.5381, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -10847.5283203125
tensor(10847.5293, grad_fn=<NegBackward0>) tensor(10847.5283, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10847.529296875
tensor(10847.5283, grad_fn=<NegBackward0>) tensor(10847.5293, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -10847.5322265625
tensor(10847.5283, grad_fn=<NegBackward0>) tensor(10847.5322, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -10847.529296875
tensor(10847.5283, grad_fn=<NegBackward0>) tensor(10847.5293, grad_fn=<NegBackward0>)
3
pi: tensor([[0.6390, 0.3610],
        [0.2663, 0.7337]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6899, 0.3101], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2026, 0.0930],
         [0.7142, 0.2565]],

        [[0.5934, 0.0941],
         [0.6507, 0.6258]],

        [[0.6909, 0.0931],
         [0.6674, 0.6856]],

        [[0.5019, 0.0914],
         [0.5229, 0.7083]],

        [[0.7122, 0.1000],
         [0.5364, 0.7147]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 20
Adjusted Rand Index: 0.3545232273838631
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.7369696969696969
time is 2
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.844846433231073
time is 3
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 4
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.3931727108173828
Average Adjusted Rand Index: 0.7714292782425987
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23196.578125
inf tensor(23196.5781, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11021.9384765625
tensor(23196.5781, grad_fn=<NegBackward0>) tensor(11021.9385, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11019.42578125
tensor(11021.9385, grad_fn=<NegBackward0>) tensor(11019.4258, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11019.169921875
tensor(11019.4258, grad_fn=<NegBackward0>) tensor(11019.1699, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11019.021484375
tensor(11019.1699, grad_fn=<NegBackward0>) tensor(11019.0215, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11018.8876953125
tensor(11019.0215, grad_fn=<NegBackward0>) tensor(11018.8877, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11018.7470703125
tensor(11018.8877, grad_fn=<NegBackward0>) tensor(11018.7471, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11018.6005859375
tensor(11018.7471, grad_fn=<NegBackward0>) tensor(11018.6006, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11018.4501953125
tensor(11018.6006, grad_fn=<NegBackward0>) tensor(11018.4502, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11018.3203125
tensor(11018.4502, grad_fn=<NegBackward0>) tensor(11018.3203, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11018.2470703125
tensor(11018.3203, grad_fn=<NegBackward0>) tensor(11018.2471, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11018.203125
tensor(11018.2471, grad_fn=<NegBackward0>) tensor(11018.2031, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11018.1171875
tensor(11018.2031, grad_fn=<NegBackward0>) tensor(11018.1172, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11017.3662109375
tensor(11018.1172, grad_fn=<NegBackward0>) tensor(11017.3662, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11016.0498046875
tensor(11017.3662, grad_fn=<NegBackward0>) tensor(11016.0498, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11015.8837890625
tensor(11016.0498, grad_fn=<NegBackward0>) tensor(11015.8838, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11015.8603515625
tensor(11015.8838, grad_fn=<NegBackward0>) tensor(11015.8604, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11015.8515625
tensor(11015.8604, grad_fn=<NegBackward0>) tensor(11015.8516, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11015.845703125
tensor(11015.8516, grad_fn=<NegBackward0>) tensor(11015.8457, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11015.8447265625
tensor(11015.8457, grad_fn=<NegBackward0>) tensor(11015.8447, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11015.8427734375
tensor(11015.8447, grad_fn=<NegBackward0>) tensor(11015.8428, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11015.8408203125
tensor(11015.8428, grad_fn=<NegBackward0>) tensor(11015.8408, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11015.8388671875
tensor(11015.8408, grad_fn=<NegBackward0>) tensor(11015.8389, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11015.8330078125
tensor(11015.8389, grad_fn=<NegBackward0>) tensor(11015.8330, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11015.828125
tensor(11015.8330, grad_fn=<NegBackward0>) tensor(11015.8281, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11015.8212890625
tensor(11015.8281, grad_fn=<NegBackward0>) tensor(11015.8213, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11015.80859375
tensor(11015.8213, grad_fn=<NegBackward0>) tensor(11015.8086, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11015.7841796875
tensor(11015.8086, grad_fn=<NegBackward0>) tensor(11015.7842, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11015.736328125
tensor(11015.7842, grad_fn=<NegBackward0>) tensor(11015.7363, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11015.6552734375
tensor(11015.7363, grad_fn=<NegBackward0>) tensor(11015.6553, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11013.697265625
tensor(11015.6553, grad_fn=<NegBackward0>) tensor(11013.6973, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10881.74609375
tensor(11013.6973, grad_fn=<NegBackward0>) tensor(10881.7461, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10868.34765625
tensor(10881.7461, grad_fn=<NegBackward0>) tensor(10868.3477, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10866.296875
tensor(10868.3477, grad_fn=<NegBackward0>) tensor(10866.2969, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10866.240234375
tensor(10866.2969, grad_fn=<NegBackward0>) tensor(10866.2402, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10864.4296875
tensor(10866.2402, grad_fn=<NegBackward0>) tensor(10864.4297, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10862.708984375
tensor(10864.4297, grad_fn=<NegBackward0>) tensor(10862.7090, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10862.6533203125
tensor(10862.7090, grad_fn=<NegBackward0>) tensor(10862.6533, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10859.771484375
tensor(10862.6533, grad_fn=<NegBackward0>) tensor(10859.7715, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10859.693359375
tensor(10859.7715, grad_fn=<NegBackward0>) tensor(10859.6934, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10859.6611328125
tensor(10859.6934, grad_fn=<NegBackward0>) tensor(10859.6611, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10859.568359375
tensor(10859.6611, grad_fn=<NegBackward0>) tensor(10859.5684, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10859.5654296875
tensor(10859.5684, grad_fn=<NegBackward0>) tensor(10859.5654, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10850.9755859375
tensor(10859.5654, grad_fn=<NegBackward0>) tensor(10850.9756, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10850.9697265625
tensor(10850.9756, grad_fn=<NegBackward0>) tensor(10850.9697, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10850.9638671875
tensor(10850.9697, grad_fn=<NegBackward0>) tensor(10850.9639, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10850.9521484375
tensor(10850.9639, grad_fn=<NegBackward0>) tensor(10850.9521, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10850.947265625
tensor(10850.9521, grad_fn=<NegBackward0>) tensor(10850.9473, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10850.9560546875
tensor(10850.9473, grad_fn=<NegBackward0>) tensor(10850.9561, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10850.9384765625
tensor(10850.9473, grad_fn=<NegBackward0>) tensor(10850.9385, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10850.919921875
tensor(10850.9385, grad_fn=<NegBackward0>) tensor(10850.9199, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10846.048828125
tensor(10850.9199, grad_fn=<NegBackward0>) tensor(10846.0488, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10846.017578125
tensor(10846.0488, grad_fn=<NegBackward0>) tensor(10846.0176, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10846.01953125
tensor(10846.0176, grad_fn=<NegBackward0>) tensor(10846.0195, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10846.0185546875
tensor(10846.0176, grad_fn=<NegBackward0>) tensor(10846.0186, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -10846.015625
tensor(10846.0176, grad_fn=<NegBackward0>) tensor(10846.0156, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10846.0166015625
tensor(10846.0156, grad_fn=<NegBackward0>) tensor(10846.0166, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10846.01953125
tensor(10846.0156, grad_fn=<NegBackward0>) tensor(10846.0195, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -10846.0078125
tensor(10846.0156, grad_fn=<NegBackward0>) tensor(10846.0078, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10846.0087890625
tensor(10846.0078, grad_fn=<NegBackward0>) tensor(10846.0088, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10846.0078125
tensor(10846.0078, grad_fn=<NegBackward0>) tensor(10846.0078, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10846.0556640625
tensor(10846.0078, grad_fn=<NegBackward0>) tensor(10846.0557, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10846.001953125
tensor(10846.0078, grad_fn=<NegBackward0>) tensor(10846.0020, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10846.0009765625
tensor(10846.0020, grad_fn=<NegBackward0>) tensor(10846.0010, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10846.0009765625
tensor(10846.0010, grad_fn=<NegBackward0>) tensor(10846.0010, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10846.0
tensor(10846.0010, grad_fn=<NegBackward0>) tensor(10846., grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10846.0068359375
tensor(10846., grad_fn=<NegBackward0>) tensor(10846.0068, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10845.923828125
tensor(10846., grad_fn=<NegBackward0>) tensor(10845.9238, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10845.9228515625
tensor(10845.9238, grad_fn=<NegBackward0>) tensor(10845.9229, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10845.919921875
tensor(10845.9229, grad_fn=<NegBackward0>) tensor(10845.9199, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10845.888671875
tensor(10845.9199, grad_fn=<NegBackward0>) tensor(10845.8887, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10845.890625
tensor(10845.8887, grad_fn=<NegBackward0>) tensor(10845.8906, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10845.888671875
tensor(10845.8887, grad_fn=<NegBackward0>) tensor(10845.8887, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10839.958984375
tensor(10845.8887, grad_fn=<NegBackward0>) tensor(10839.9590, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10839.861328125
tensor(10839.9590, grad_fn=<NegBackward0>) tensor(10839.8613, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10839.8447265625
tensor(10839.8613, grad_fn=<NegBackward0>) tensor(10839.8447, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10839.8310546875
tensor(10839.8447, grad_fn=<NegBackward0>) tensor(10839.8311, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10839.8232421875
tensor(10839.8311, grad_fn=<NegBackward0>) tensor(10839.8232, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10839.8212890625
tensor(10839.8232, grad_fn=<NegBackward0>) tensor(10839.8213, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10839.8203125
tensor(10839.8213, grad_fn=<NegBackward0>) tensor(10839.8203, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10839.814453125
tensor(10839.8203, grad_fn=<NegBackward0>) tensor(10839.8145, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10839.80859375
tensor(10839.8145, grad_fn=<NegBackward0>) tensor(10839.8086, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10839.806640625
tensor(10839.8086, grad_fn=<NegBackward0>) tensor(10839.8066, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10839.8076171875
tensor(10839.8066, grad_fn=<NegBackward0>) tensor(10839.8076, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10839.806640625
tensor(10839.8066, grad_fn=<NegBackward0>) tensor(10839.8066, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10839.80859375
tensor(10839.8066, grad_fn=<NegBackward0>) tensor(10839.8086, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -10839.8076171875
tensor(10839.8066, grad_fn=<NegBackward0>) tensor(10839.8076, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -10839.8076171875
tensor(10839.8066, grad_fn=<NegBackward0>) tensor(10839.8076, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -10839.8056640625
tensor(10839.8066, grad_fn=<NegBackward0>) tensor(10839.8057, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10839.798828125
tensor(10839.8057, grad_fn=<NegBackward0>) tensor(10839.7988, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10839.8203125
tensor(10839.7988, grad_fn=<NegBackward0>) tensor(10839.8203, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10839.798828125
tensor(10839.7988, grad_fn=<NegBackward0>) tensor(10839.7988, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10839.798828125
tensor(10839.7988, grad_fn=<NegBackward0>) tensor(10839.7988, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10839.8017578125
tensor(10839.7988, grad_fn=<NegBackward0>) tensor(10839.8018, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -10839.78515625
tensor(10839.7988, grad_fn=<NegBackward0>) tensor(10839.7852, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10839.7822265625
tensor(10839.7852, grad_fn=<NegBackward0>) tensor(10839.7822, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10839.826171875
tensor(10839.7822, grad_fn=<NegBackward0>) tensor(10839.8262, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -10839.75
tensor(10839.7822, grad_fn=<NegBackward0>) tensor(10839.7500, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10839.748046875
tensor(10839.7500, grad_fn=<NegBackward0>) tensor(10839.7480, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10839.74609375
tensor(10839.7480, grad_fn=<NegBackward0>) tensor(10839.7461, grad_fn=<NegBackward0>)
pi: tensor([[0.7132, 0.2868],
        [0.2335, 0.7665]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5178, 0.4822], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1957, 0.1048],
         [0.5683, 0.2582]],

        [[0.7219, 0.0949],
         [0.7206, 0.7309]],

        [[0.6316, 0.0935],
         [0.6346, 0.5884]],

        [[0.6706, 0.0917],
         [0.5444, 0.6992]],

        [[0.6311, 0.1006],
         [0.5821, 0.5039]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7721016799725718
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.7369696969696969
time is 2
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080808080808081
time is 3
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
time is 4
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.8387329239387932
Average Adjusted Rand Index: 0.8399147326868647
[0.3931727108173828, 0.8387329239387932] [0.7714292782425987, 0.8399147326868647] [10847.5302734375, 10839.751953125]
-------------------------------------
This iteration is 68
True Objective function: Loss = -10604.278052254685
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20886.984375
inf tensor(20886.9844, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10693.1123046875
tensor(20886.9844, grad_fn=<NegBackward0>) tensor(10693.1123, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10692.392578125
tensor(10693.1123, grad_fn=<NegBackward0>) tensor(10692.3926, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10692.005859375
tensor(10692.3926, grad_fn=<NegBackward0>) tensor(10692.0059, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10691.7646484375
tensor(10692.0059, grad_fn=<NegBackward0>) tensor(10691.7646, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10691.64453125
tensor(10691.7646, grad_fn=<NegBackward0>) tensor(10691.6445, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10691.546875
tensor(10691.6445, grad_fn=<NegBackward0>) tensor(10691.5469, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10691.4580078125
tensor(10691.5469, grad_fn=<NegBackward0>) tensor(10691.4580, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10691.373046875
tensor(10691.4580, grad_fn=<NegBackward0>) tensor(10691.3730, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10691.2919921875
tensor(10691.3730, grad_fn=<NegBackward0>) tensor(10691.2920, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10691.2255859375
tensor(10691.2920, grad_fn=<NegBackward0>) tensor(10691.2256, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10691.1767578125
tensor(10691.2256, grad_fn=<NegBackward0>) tensor(10691.1768, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10691.146484375
tensor(10691.1768, grad_fn=<NegBackward0>) tensor(10691.1465, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10691.125
tensor(10691.1465, grad_fn=<NegBackward0>) tensor(10691.1250, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10691.1103515625
tensor(10691.1250, grad_fn=<NegBackward0>) tensor(10691.1104, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10691.095703125
tensor(10691.1104, grad_fn=<NegBackward0>) tensor(10691.0957, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10691.08203125
tensor(10691.0957, grad_fn=<NegBackward0>) tensor(10691.0820, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10691.0703125
tensor(10691.0820, grad_fn=<NegBackward0>) tensor(10691.0703, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10691.0615234375
tensor(10691.0703, grad_fn=<NegBackward0>) tensor(10691.0615, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10691.048828125
tensor(10691.0615, grad_fn=<NegBackward0>) tensor(10691.0488, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10691.0400390625
tensor(10691.0488, grad_fn=<NegBackward0>) tensor(10691.0400, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10691.0302734375
tensor(10691.0400, grad_fn=<NegBackward0>) tensor(10691.0303, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10691.017578125
tensor(10691.0303, grad_fn=<NegBackward0>) tensor(10691.0176, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10691.0068359375
tensor(10691.0176, grad_fn=<NegBackward0>) tensor(10691.0068, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10690.9951171875
tensor(10691.0068, grad_fn=<NegBackward0>) tensor(10690.9951, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10690.982421875
tensor(10690.9951, grad_fn=<NegBackward0>) tensor(10690.9824, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10690.9677734375
tensor(10690.9824, grad_fn=<NegBackward0>) tensor(10690.9678, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10690.953125
tensor(10690.9678, grad_fn=<NegBackward0>) tensor(10690.9531, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10690.9375
tensor(10690.9531, grad_fn=<NegBackward0>) tensor(10690.9375, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10690.9169921875
tensor(10690.9375, grad_fn=<NegBackward0>) tensor(10690.9170, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10690.8974609375
tensor(10690.9170, grad_fn=<NegBackward0>) tensor(10690.8975, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10690.873046875
tensor(10690.8975, grad_fn=<NegBackward0>) tensor(10690.8730, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10690.84375
tensor(10690.8730, grad_fn=<NegBackward0>) tensor(10690.8438, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10690.8115234375
tensor(10690.8438, grad_fn=<NegBackward0>) tensor(10690.8115, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10690.7734375
tensor(10690.8115, grad_fn=<NegBackward0>) tensor(10690.7734, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10690.732421875
tensor(10690.7734, grad_fn=<NegBackward0>) tensor(10690.7324, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10690.6875
tensor(10690.7324, grad_fn=<NegBackward0>) tensor(10690.6875, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10690.64453125
tensor(10690.6875, grad_fn=<NegBackward0>) tensor(10690.6445, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10690.60546875
tensor(10690.6445, grad_fn=<NegBackward0>) tensor(10690.6055, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10690.572265625
tensor(10690.6055, grad_fn=<NegBackward0>) tensor(10690.5723, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10690.54296875
tensor(10690.5723, grad_fn=<NegBackward0>) tensor(10690.5430, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10690.5205078125
tensor(10690.5430, grad_fn=<NegBackward0>) tensor(10690.5205, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10690.4970703125
tensor(10690.5205, grad_fn=<NegBackward0>) tensor(10690.4971, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10690.4755859375
tensor(10690.4971, grad_fn=<NegBackward0>) tensor(10690.4756, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10690.4501953125
tensor(10690.4756, grad_fn=<NegBackward0>) tensor(10690.4502, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10690.40625
tensor(10690.4502, grad_fn=<NegBackward0>) tensor(10690.4062, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10690.2958984375
tensor(10690.4062, grad_fn=<NegBackward0>) tensor(10690.2959, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10690.1201171875
tensor(10690.2959, grad_fn=<NegBackward0>) tensor(10690.1201, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10690.0390625
tensor(10690.1201, grad_fn=<NegBackward0>) tensor(10690.0391, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10690.0087890625
tensor(10690.0391, grad_fn=<NegBackward0>) tensor(10690.0088, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10689.990234375
tensor(10690.0088, grad_fn=<NegBackward0>) tensor(10689.9902, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10689.9765625
tensor(10689.9902, grad_fn=<NegBackward0>) tensor(10689.9766, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10689.96875
tensor(10689.9766, grad_fn=<NegBackward0>) tensor(10689.9688, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10689.9609375
tensor(10689.9688, grad_fn=<NegBackward0>) tensor(10689.9609, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10689.9658203125
tensor(10689.9609, grad_fn=<NegBackward0>) tensor(10689.9658, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10689.9541015625
tensor(10689.9609, grad_fn=<NegBackward0>) tensor(10689.9541, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10689.94921875
tensor(10689.9541, grad_fn=<NegBackward0>) tensor(10689.9492, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10689.947265625
tensor(10689.9492, grad_fn=<NegBackward0>) tensor(10689.9473, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10689.9462890625
tensor(10689.9473, grad_fn=<NegBackward0>) tensor(10689.9463, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10689.9453125
tensor(10689.9463, grad_fn=<NegBackward0>) tensor(10689.9453, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10689.943359375
tensor(10689.9453, grad_fn=<NegBackward0>) tensor(10689.9434, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10689.94921875
tensor(10689.9434, grad_fn=<NegBackward0>) tensor(10689.9492, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10689.94140625
tensor(10689.9434, grad_fn=<NegBackward0>) tensor(10689.9414, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10689.9384765625
tensor(10689.9414, grad_fn=<NegBackward0>) tensor(10689.9385, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10689.9384765625
tensor(10689.9385, grad_fn=<NegBackward0>) tensor(10689.9385, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10689.9384765625
tensor(10689.9385, grad_fn=<NegBackward0>) tensor(10689.9385, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10689.9365234375
tensor(10689.9385, grad_fn=<NegBackward0>) tensor(10689.9365, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10689.9384765625
tensor(10689.9365, grad_fn=<NegBackward0>) tensor(10689.9385, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10689.935546875
tensor(10689.9365, grad_fn=<NegBackward0>) tensor(10689.9355, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10689.9345703125
tensor(10689.9355, grad_fn=<NegBackward0>) tensor(10689.9346, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10689.935546875
tensor(10689.9346, grad_fn=<NegBackward0>) tensor(10689.9355, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10689.935546875
tensor(10689.9346, grad_fn=<NegBackward0>) tensor(10689.9355, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -10689.9345703125
tensor(10689.9346, grad_fn=<NegBackward0>) tensor(10689.9346, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10689.93359375
tensor(10689.9346, grad_fn=<NegBackward0>) tensor(10689.9336, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10689.9326171875
tensor(10689.9336, grad_fn=<NegBackward0>) tensor(10689.9326, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10689.93359375
tensor(10689.9326, grad_fn=<NegBackward0>) tensor(10689.9336, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10689.93359375
tensor(10689.9326, grad_fn=<NegBackward0>) tensor(10689.9336, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -10689.9326171875
tensor(10689.9326, grad_fn=<NegBackward0>) tensor(10689.9326, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10689.931640625
tensor(10689.9326, grad_fn=<NegBackward0>) tensor(10689.9316, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10689.962890625
tensor(10689.9316, grad_fn=<NegBackward0>) tensor(10689.9629, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10689.931640625
tensor(10689.9316, grad_fn=<NegBackward0>) tensor(10689.9316, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10689.9326171875
tensor(10689.9316, grad_fn=<NegBackward0>) tensor(10689.9326, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10689.9326171875
tensor(10689.9316, grad_fn=<NegBackward0>) tensor(10689.9326, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -10689.931640625
tensor(10689.9316, grad_fn=<NegBackward0>) tensor(10689.9316, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10689.931640625
tensor(10689.9316, grad_fn=<NegBackward0>) tensor(10689.9316, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10689.9326171875
tensor(10689.9316, grad_fn=<NegBackward0>) tensor(10689.9326, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -10689.931640625
tensor(10689.9316, grad_fn=<NegBackward0>) tensor(10689.9316, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10689.9716796875
tensor(10689.9316, grad_fn=<NegBackward0>) tensor(10689.9717, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -10689.9306640625
tensor(10689.9316, grad_fn=<NegBackward0>) tensor(10689.9307, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10689.9306640625
tensor(10689.9307, grad_fn=<NegBackward0>) tensor(10689.9307, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10689.9296875
tensor(10689.9307, grad_fn=<NegBackward0>) tensor(10689.9297, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10689.9306640625
tensor(10689.9297, grad_fn=<NegBackward0>) tensor(10689.9307, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10689.9345703125
tensor(10689.9297, grad_fn=<NegBackward0>) tensor(10689.9346, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -10689.9306640625
tensor(10689.9297, grad_fn=<NegBackward0>) tensor(10689.9307, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -10689.9677734375
tensor(10689.9297, grad_fn=<NegBackward0>) tensor(10689.9678, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -10689.9423828125
tensor(10689.9297, grad_fn=<NegBackward0>) tensor(10689.9424, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9500 due to no improvement.
pi: tensor([[1.7226e-03, 9.9828e-01],
        [9.9926e-01, 7.3772e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2267, 0.7733], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1467, 0.1587],
         [0.6093, 0.1627]],

        [[0.6245, 0.1582],
         [0.5426, 0.7129]],

        [[0.7247, 0.1526],
         [0.5607, 0.5366]],

        [[0.5011, 0.1477],
         [0.5739, 0.6412]],

        [[0.5136, 0.1546],
         [0.6767, 0.5104]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 61
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.002114612825173073
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21018.28125
inf tensor(21018.2812, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10693.669921875
tensor(21018.2812, grad_fn=<NegBackward0>) tensor(10693.6699, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10692.52734375
tensor(10693.6699, grad_fn=<NegBackward0>) tensor(10692.5273, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10691.5009765625
tensor(10692.5273, grad_fn=<NegBackward0>) tensor(10691.5010, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10691.3544921875
tensor(10691.5010, grad_fn=<NegBackward0>) tensor(10691.3545, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10691.2861328125
tensor(10691.3545, grad_fn=<NegBackward0>) tensor(10691.2861, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10691.2451171875
tensor(10691.2861, grad_fn=<NegBackward0>) tensor(10691.2451, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10691.2177734375
tensor(10691.2451, grad_fn=<NegBackward0>) tensor(10691.2178, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10691.197265625
tensor(10691.2178, grad_fn=<NegBackward0>) tensor(10691.1973, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10691.181640625
tensor(10691.1973, grad_fn=<NegBackward0>) tensor(10691.1816, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10691.166015625
tensor(10691.1816, grad_fn=<NegBackward0>) tensor(10691.1660, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10691.15234375
tensor(10691.1660, grad_fn=<NegBackward0>) tensor(10691.1523, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10691.1435546875
tensor(10691.1523, grad_fn=<NegBackward0>) tensor(10691.1436, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10691.130859375
tensor(10691.1436, grad_fn=<NegBackward0>) tensor(10691.1309, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10691.119140625
tensor(10691.1309, grad_fn=<NegBackward0>) tensor(10691.1191, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10691.1083984375
tensor(10691.1191, grad_fn=<NegBackward0>) tensor(10691.1084, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10691.0986328125
tensor(10691.1084, grad_fn=<NegBackward0>) tensor(10691.0986, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10691.0869140625
tensor(10691.0986, grad_fn=<NegBackward0>) tensor(10691.0869, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10691.0751953125
tensor(10691.0869, grad_fn=<NegBackward0>) tensor(10691.0752, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10691.0615234375
tensor(10691.0752, grad_fn=<NegBackward0>) tensor(10691.0615, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10691.05078125
tensor(10691.0615, grad_fn=<NegBackward0>) tensor(10691.0508, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10691.037109375
tensor(10691.0508, grad_fn=<NegBackward0>) tensor(10691.0371, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10691.0234375
tensor(10691.0371, grad_fn=<NegBackward0>) tensor(10691.0234, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10691.0087890625
tensor(10691.0234, grad_fn=<NegBackward0>) tensor(10691.0088, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10690.9931640625
tensor(10691.0088, grad_fn=<NegBackward0>) tensor(10690.9932, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10690.9765625
tensor(10690.9932, grad_fn=<NegBackward0>) tensor(10690.9766, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10690.955078125
tensor(10690.9766, grad_fn=<NegBackward0>) tensor(10690.9551, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10690.9345703125
tensor(10690.9551, grad_fn=<NegBackward0>) tensor(10690.9346, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10690.9072265625
tensor(10690.9346, grad_fn=<NegBackward0>) tensor(10690.9072, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10690.87890625
tensor(10690.9072, grad_fn=<NegBackward0>) tensor(10690.8789, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10690.841796875
tensor(10690.8789, grad_fn=<NegBackward0>) tensor(10690.8418, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10690.796875
tensor(10690.8418, grad_fn=<NegBackward0>) tensor(10690.7969, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10690.8154296875
tensor(10690.7969, grad_fn=<NegBackward0>) tensor(10690.8154, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -10690.6845703125
tensor(10690.7969, grad_fn=<NegBackward0>) tensor(10690.6846, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10690.638671875
tensor(10690.6846, grad_fn=<NegBackward0>) tensor(10690.6387, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10690.5810546875
tensor(10690.6387, grad_fn=<NegBackward0>) tensor(10690.5811, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10690.548828125
tensor(10690.5811, grad_fn=<NegBackward0>) tensor(10690.5488, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10690.51171875
tensor(10690.5488, grad_fn=<NegBackward0>) tensor(10690.5117, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10690.486328125
tensor(10690.5117, grad_fn=<NegBackward0>) tensor(10690.4863, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10690.458984375
tensor(10690.4863, grad_fn=<NegBackward0>) tensor(10690.4590, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10690.42578125
tensor(10690.4590, grad_fn=<NegBackward0>) tensor(10690.4258, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10690.35546875
tensor(10690.4258, grad_fn=<NegBackward0>) tensor(10690.3555, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10690.1962890625
tensor(10690.3555, grad_fn=<NegBackward0>) tensor(10690.1963, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10690.07421875
tensor(10690.1963, grad_fn=<NegBackward0>) tensor(10690.0742, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10690.02734375
tensor(10690.0742, grad_fn=<NegBackward0>) tensor(10690.0273, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10690.0
tensor(10690.0273, grad_fn=<NegBackward0>) tensor(10690., grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10689.9833984375
tensor(10690., grad_fn=<NegBackward0>) tensor(10689.9834, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10689.97265625
tensor(10689.9834, grad_fn=<NegBackward0>) tensor(10689.9727, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10689.9658203125
tensor(10689.9727, grad_fn=<NegBackward0>) tensor(10689.9658, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10689.9599609375
tensor(10689.9658, grad_fn=<NegBackward0>) tensor(10689.9600, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10689.9560546875
tensor(10689.9600, grad_fn=<NegBackward0>) tensor(10689.9561, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10689.953125
tensor(10689.9561, grad_fn=<NegBackward0>) tensor(10689.9531, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10689.951171875
tensor(10689.9531, grad_fn=<NegBackward0>) tensor(10689.9512, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10689.9482421875
tensor(10689.9512, grad_fn=<NegBackward0>) tensor(10689.9482, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10689.9462890625
tensor(10689.9482, grad_fn=<NegBackward0>) tensor(10689.9463, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10689.9443359375
tensor(10689.9463, grad_fn=<NegBackward0>) tensor(10689.9443, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10689.9423828125
tensor(10689.9443, grad_fn=<NegBackward0>) tensor(10689.9424, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10689.94140625
tensor(10689.9424, grad_fn=<NegBackward0>) tensor(10689.9414, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10689.9404296875
tensor(10689.9414, grad_fn=<NegBackward0>) tensor(10689.9404, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10689.9404296875
tensor(10689.9404, grad_fn=<NegBackward0>) tensor(10689.9404, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10689.9384765625
tensor(10689.9404, grad_fn=<NegBackward0>) tensor(10689.9385, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10689.9384765625
tensor(10689.9385, grad_fn=<NegBackward0>) tensor(10689.9385, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10689.9375
tensor(10689.9385, grad_fn=<NegBackward0>) tensor(10689.9375, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10689.9365234375
tensor(10689.9375, grad_fn=<NegBackward0>) tensor(10689.9365, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10689.935546875
tensor(10689.9365, grad_fn=<NegBackward0>) tensor(10689.9355, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10689.9404296875
tensor(10689.9355, grad_fn=<NegBackward0>) tensor(10689.9404, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10689.9345703125
tensor(10689.9355, grad_fn=<NegBackward0>) tensor(10689.9346, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10689.9345703125
tensor(10689.9346, grad_fn=<NegBackward0>) tensor(10689.9346, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10689.9345703125
tensor(10689.9346, grad_fn=<NegBackward0>) tensor(10689.9346, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10689.93359375
tensor(10689.9346, grad_fn=<NegBackward0>) tensor(10689.9336, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10689.9345703125
tensor(10689.9336, grad_fn=<NegBackward0>) tensor(10689.9346, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10689.9375
tensor(10689.9336, grad_fn=<NegBackward0>) tensor(10689.9375, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -10689.9326171875
tensor(10689.9336, grad_fn=<NegBackward0>) tensor(10689.9326, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10689.931640625
tensor(10689.9326, grad_fn=<NegBackward0>) tensor(10689.9316, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10689.96875
tensor(10689.9316, grad_fn=<NegBackward0>) tensor(10689.9688, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10689.9326171875
tensor(10689.9316, grad_fn=<NegBackward0>) tensor(10689.9326, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -10689.9326171875
tensor(10689.9316, grad_fn=<NegBackward0>) tensor(10689.9326, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -10689.9326171875
tensor(10689.9316, grad_fn=<NegBackward0>) tensor(10689.9326, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -10689.9326171875
tensor(10689.9316, grad_fn=<NegBackward0>) tensor(10689.9326, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7800 due to no improvement.
pi: tensor([[0.0017, 0.9983],
        [0.9965, 0.0035]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7806, 0.2194], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1626, 0.1586],
         [0.7212, 0.1468]],

        [[0.6574, 0.1583],
         [0.7296, 0.6977]],

        [[0.7177, 0.1524],
         [0.5156, 0.6704]],

        [[0.6568, 0.1477],
         [0.5647, 0.5943]],

        [[0.6447, 0.1545],
         [0.7257, 0.7002]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 39
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.002114612825173073
Average Adjusted Rand Index: 0.0
[-0.002114612825173073, -0.002114612825173073] [0.0, 0.0] [10689.9423828125, 10689.9326171875]
-------------------------------------
This iteration is 69
True Objective function: Loss = -10942.307595415668
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21731.08984375
inf tensor(21731.0898, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11020.5224609375
tensor(21731.0898, grad_fn=<NegBackward0>) tensor(11020.5225, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11019.923828125
tensor(11020.5225, grad_fn=<NegBackward0>) tensor(11019.9238, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11018.6064453125
tensor(11019.9238, grad_fn=<NegBackward0>) tensor(11018.6064, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11017.32421875
tensor(11018.6064, grad_fn=<NegBackward0>) tensor(11017.3242, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11015.4208984375
tensor(11017.3242, grad_fn=<NegBackward0>) tensor(11015.4209, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11014.8291015625
tensor(11015.4209, grad_fn=<NegBackward0>) tensor(11014.8291, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11014.052734375
tensor(11014.8291, grad_fn=<NegBackward0>) tensor(11014.0527, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11012.1962890625
tensor(11014.0527, grad_fn=<NegBackward0>) tensor(11012.1963, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11011.6630859375
tensor(11012.1963, grad_fn=<NegBackward0>) tensor(11011.6631, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11011.5029296875
tensor(11011.6631, grad_fn=<NegBackward0>) tensor(11011.5029, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11011.400390625
tensor(11011.5029, grad_fn=<NegBackward0>) tensor(11011.4004, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11011.322265625
tensor(11011.4004, grad_fn=<NegBackward0>) tensor(11011.3223, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11011.2607421875
tensor(11011.3223, grad_fn=<NegBackward0>) tensor(11011.2607, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11011.21484375
tensor(11011.2607, grad_fn=<NegBackward0>) tensor(11011.2148, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11011.1748046875
tensor(11011.2148, grad_fn=<NegBackward0>) tensor(11011.1748, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11011.1513671875
tensor(11011.1748, grad_fn=<NegBackward0>) tensor(11011.1514, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11011.1103515625
tensor(11011.1514, grad_fn=<NegBackward0>) tensor(11011.1104, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11011.08203125
tensor(11011.1104, grad_fn=<NegBackward0>) tensor(11011.0820, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11011.0537109375
tensor(11011.0820, grad_fn=<NegBackward0>) tensor(11011.0537, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11011.017578125
tensor(11011.0537, grad_fn=<NegBackward0>) tensor(11011.0176, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11010.9765625
tensor(11011.0176, grad_fn=<NegBackward0>) tensor(11010.9766, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11010.927734375
tensor(11010.9766, grad_fn=<NegBackward0>) tensor(11010.9277, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11010.865234375
tensor(11010.9277, grad_fn=<NegBackward0>) tensor(11010.8652, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11010.7919921875
tensor(11010.8652, grad_fn=<NegBackward0>) tensor(11010.7920, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11010.73046875
tensor(11010.7920, grad_fn=<NegBackward0>) tensor(11010.7305, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11010.6220703125
tensor(11010.7305, grad_fn=<NegBackward0>) tensor(11010.6221, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11010.5234375
tensor(11010.6221, grad_fn=<NegBackward0>) tensor(11010.5234, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11010.2802734375
tensor(11010.5234, grad_fn=<NegBackward0>) tensor(11010.2803, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10962.8251953125
tensor(11010.2803, grad_fn=<NegBackward0>) tensor(10962.8252, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10936.6767578125
tensor(10962.8252, grad_fn=<NegBackward0>) tensor(10936.6768, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10908.79296875
tensor(10936.6768, grad_fn=<NegBackward0>) tensor(10908.7930, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10907.7275390625
tensor(10908.7930, grad_fn=<NegBackward0>) tensor(10907.7275, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10907.66796875
tensor(10907.7275, grad_fn=<NegBackward0>) tensor(10907.6680, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10907.6513671875
tensor(10907.6680, grad_fn=<NegBackward0>) tensor(10907.6514, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10907.642578125
tensor(10907.6514, grad_fn=<NegBackward0>) tensor(10907.6426, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10907.6376953125
tensor(10907.6426, grad_fn=<NegBackward0>) tensor(10907.6377, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10907.6328125
tensor(10907.6377, grad_fn=<NegBackward0>) tensor(10907.6328, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10907.62890625
tensor(10907.6328, grad_fn=<NegBackward0>) tensor(10907.6289, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10907.6240234375
tensor(10907.6289, grad_fn=<NegBackward0>) tensor(10907.6240, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10907.103515625
tensor(10907.6240, grad_fn=<NegBackward0>) tensor(10907.1035, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10907.078125
tensor(10907.1035, grad_fn=<NegBackward0>) tensor(10907.0781, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10907.078125
tensor(10907.0781, grad_fn=<NegBackward0>) tensor(10907.0781, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10907.0771484375
tensor(10907.0781, grad_fn=<NegBackward0>) tensor(10907.0771, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10907.076171875
tensor(10907.0771, grad_fn=<NegBackward0>) tensor(10907.0762, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10907.0751953125
tensor(10907.0762, grad_fn=<NegBackward0>) tensor(10907.0752, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10907.07421875
tensor(10907.0752, grad_fn=<NegBackward0>) tensor(10907.0742, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10907.0771484375
tensor(10907.0742, grad_fn=<NegBackward0>) tensor(10907.0771, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10907.0732421875
tensor(10907.0742, grad_fn=<NegBackward0>) tensor(10907.0732, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10907.07421875
tensor(10907.0732, grad_fn=<NegBackward0>) tensor(10907.0742, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10907.072265625
tensor(10907.0732, grad_fn=<NegBackward0>) tensor(10907.0723, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10907.07421875
tensor(10907.0723, grad_fn=<NegBackward0>) tensor(10907.0742, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10907.07421875
tensor(10907.0723, grad_fn=<NegBackward0>) tensor(10907.0742, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -10907.0732421875
tensor(10907.0723, grad_fn=<NegBackward0>) tensor(10907.0732, grad_fn=<NegBackward0>)
3
Iteration 5400: Loss = -10907.0810546875
tensor(10907.0723, grad_fn=<NegBackward0>) tensor(10907.0811, grad_fn=<NegBackward0>)
4
Iteration 5500: Loss = -10907.0712890625
tensor(10907.0723, grad_fn=<NegBackward0>) tensor(10907.0713, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10907.0712890625
tensor(10907.0713, grad_fn=<NegBackward0>) tensor(10907.0713, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10907.072265625
tensor(10907.0713, grad_fn=<NegBackward0>) tensor(10907.0723, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10907.072265625
tensor(10907.0713, grad_fn=<NegBackward0>) tensor(10907.0723, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -10907.0712890625
tensor(10907.0713, grad_fn=<NegBackward0>) tensor(10907.0713, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10907.0712890625
tensor(10907.0713, grad_fn=<NegBackward0>) tensor(10907.0713, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10907.0703125
tensor(10907.0713, grad_fn=<NegBackward0>) tensor(10907.0703, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10907.0712890625
tensor(10907.0703, grad_fn=<NegBackward0>) tensor(10907.0713, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10907.072265625
tensor(10907.0703, grad_fn=<NegBackward0>) tensor(10907.0723, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -10907.0703125
tensor(10907.0703, grad_fn=<NegBackward0>) tensor(10907.0703, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10907.0703125
tensor(10907.0703, grad_fn=<NegBackward0>) tensor(10907.0703, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10907.0703125
tensor(10907.0703, grad_fn=<NegBackward0>) tensor(10907.0703, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10907.0693359375
tensor(10907.0703, grad_fn=<NegBackward0>) tensor(10907.0693, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10907.068359375
tensor(10907.0693, grad_fn=<NegBackward0>) tensor(10907.0684, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10907.068359375
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.0684, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10907.0732421875
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.0732, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10907.0673828125
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.0674, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10907.0849609375
tensor(10907.0674, grad_fn=<NegBackward0>) tensor(10907.0850, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10907.068359375
tensor(10907.0674, grad_fn=<NegBackward0>) tensor(10907.0684, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -10907.0673828125
tensor(10907.0674, grad_fn=<NegBackward0>) tensor(10907.0674, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10907.068359375
tensor(10907.0674, grad_fn=<NegBackward0>) tensor(10907.0684, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10907.068359375
tensor(10907.0674, grad_fn=<NegBackward0>) tensor(10907.0684, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -10907.068359375
tensor(10907.0674, grad_fn=<NegBackward0>) tensor(10907.0684, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -10907.068359375
tensor(10907.0674, grad_fn=<NegBackward0>) tensor(10907.0684, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -10907.068359375
tensor(10907.0674, grad_fn=<NegBackward0>) tensor(10907.0684, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7900 due to no improvement.
pi: tensor([[0.7840, 0.2160],
        [0.2261, 0.7739]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4796, 0.5204], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2488, 0.1083],
         [0.5508, 0.1998]],

        [[0.6295, 0.1070],
         [0.5348, 0.6352]],

        [[0.7252, 0.0895],
         [0.6877, 0.5555]],

        [[0.6478, 0.1159],
         [0.5184, 0.5501]],

        [[0.5749, 0.0991],
         [0.5362, 0.5426]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 4
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208025343189018
Global Adjusted Rand Index: 0.9368977708572352
Average Adjusted Rand Index: 0.936643281803024
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22899.7734375
inf tensor(22899.7734, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11020.7001953125
tensor(22899.7734, grad_fn=<NegBackward0>) tensor(11020.7002, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11019.9423828125
tensor(11020.7002, grad_fn=<NegBackward0>) tensor(11019.9424, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11019.5615234375
tensor(11019.9424, grad_fn=<NegBackward0>) tensor(11019.5615, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11018.9921875
tensor(11019.5615, grad_fn=<NegBackward0>) tensor(11018.9922, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11014.744140625
tensor(11018.9922, grad_fn=<NegBackward0>) tensor(11014.7441, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11013.0244140625
tensor(11014.7441, grad_fn=<NegBackward0>) tensor(11013.0244, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11012.2421875
tensor(11013.0244, grad_fn=<NegBackward0>) tensor(11012.2422, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11011.9892578125
tensor(11012.2422, grad_fn=<NegBackward0>) tensor(11011.9893, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11011.814453125
tensor(11011.9893, grad_fn=<NegBackward0>) tensor(11011.8145, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11011.671875
tensor(11011.8145, grad_fn=<NegBackward0>) tensor(11011.6719, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11011.5478515625
tensor(11011.6719, grad_fn=<NegBackward0>) tensor(11011.5479, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11011.44921875
tensor(11011.5479, grad_fn=<NegBackward0>) tensor(11011.4492, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11011.37109375
tensor(11011.4492, grad_fn=<NegBackward0>) tensor(11011.3711, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11011.3125
tensor(11011.3711, grad_fn=<NegBackward0>) tensor(11011.3125, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11011.2724609375
tensor(11011.3125, grad_fn=<NegBackward0>) tensor(11011.2725, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11011.248046875
tensor(11011.2725, grad_fn=<NegBackward0>) tensor(11011.2480, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11011.2314453125
tensor(11011.2480, grad_fn=<NegBackward0>) tensor(11011.2314, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11011.2177734375
tensor(11011.2314, grad_fn=<NegBackward0>) tensor(11011.2178, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11011.20703125
tensor(11011.2178, grad_fn=<NegBackward0>) tensor(11011.2070, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11011.197265625
tensor(11011.2070, grad_fn=<NegBackward0>) tensor(11011.1973, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11011.1845703125
tensor(11011.1973, grad_fn=<NegBackward0>) tensor(11011.1846, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11011.162109375
tensor(11011.1846, grad_fn=<NegBackward0>) tensor(11011.1621, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11011.126953125
tensor(11011.1621, grad_fn=<NegBackward0>) tensor(11011.1270, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11011.0625
tensor(11011.1270, grad_fn=<NegBackward0>) tensor(11011.0625, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11010.9404296875
tensor(11011.0625, grad_fn=<NegBackward0>) tensor(11010.9404, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11010.7587890625
tensor(11010.9404, grad_fn=<NegBackward0>) tensor(11010.7588, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11010.5693359375
tensor(11010.7588, grad_fn=<NegBackward0>) tensor(11010.5693, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10995.9677734375
tensor(11010.5693, grad_fn=<NegBackward0>) tensor(10995.9678, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10929.9267578125
tensor(10995.9678, grad_fn=<NegBackward0>) tensor(10929.9268, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10907.7490234375
tensor(10929.9268, grad_fn=<NegBackward0>) tensor(10907.7490, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10907.697265625
tensor(10907.7490, grad_fn=<NegBackward0>) tensor(10907.6973, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10907.666015625
tensor(10907.6973, grad_fn=<NegBackward0>) tensor(10907.6660, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10907.65234375
tensor(10907.6660, grad_fn=<NegBackward0>) tensor(10907.6523, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10907.1064453125
tensor(10907.6523, grad_fn=<NegBackward0>) tensor(10907.1064, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10907.095703125
tensor(10907.1064, grad_fn=<NegBackward0>) tensor(10907.0957, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10907.09375
tensor(10907.0957, grad_fn=<NegBackward0>) tensor(10907.0938, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10907.091796875
tensor(10907.0938, grad_fn=<NegBackward0>) tensor(10907.0918, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10907.0908203125
tensor(10907.0918, grad_fn=<NegBackward0>) tensor(10907.0908, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10907.0908203125
tensor(10907.0908, grad_fn=<NegBackward0>) tensor(10907.0908, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10907.0888671875
tensor(10907.0908, grad_fn=<NegBackward0>) tensor(10907.0889, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10907.08203125
tensor(10907.0889, grad_fn=<NegBackward0>) tensor(10907.0820, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10907.07421875
tensor(10907.0820, grad_fn=<NegBackward0>) tensor(10907.0742, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10907.0732421875
tensor(10907.0742, grad_fn=<NegBackward0>) tensor(10907.0732, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10907.072265625
tensor(10907.0732, grad_fn=<NegBackward0>) tensor(10907.0723, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10907.0712890625
tensor(10907.0723, grad_fn=<NegBackward0>) tensor(10907.0713, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10907.0712890625
tensor(10907.0713, grad_fn=<NegBackward0>) tensor(10907.0713, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10907.0732421875
tensor(10907.0713, grad_fn=<NegBackward0>) tensor(10907.0732, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10907.072265625
tensor(10907.0713, grad_fn=<NegBackward0>) tensor(10907.0723, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -10907.0712890625
tensor(10907.0713, grad_fn=<NegBackward0>) tensor(10907.0713, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10907.0712890625
tensor(10907.0713, grad_fn=<NegBackward0>) tensor(10907.0713, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10907.0712890625
tensor(10907.0713, grad_fn=<NegBackward0>) tensor(10907.0713, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10907.0791015625
tensor(10907.0713, grad_fn=<NegBackward0>) tensor(10907.0791, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10907.0693359375
tensor(10907.0713, grad_fn=<NegBackward0>) tensor(10907.0693, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10907.0751953125
tensor(10907.0693, grad_fn=<NegBackward0>) tensor(10907.0752, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10907.0693359375
tensor(10907.0693, grad_fn=<NegBackward0>) tensor(10907.0693, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10907.0693359375
tensor(10907.0693, grad_fn=<NegBackward0>) tensor(10907.0693, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10907.0693359375
tensor(10907.0693, grad_fn=<NegBackward0>) tensor(10907.0693, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10907.068359375
tensor(10907.0693, grad_fn=<NegBackward0>) tensor(10907.0684, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10907.068359375
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.0684, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10907.068359375
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.0684, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10907.0703125
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.0703, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10907.0712890625
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.0713, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -10907.0703125
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.0703, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -10907.068359375
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.0684, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10907.0703125
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.0703, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10907.068359375
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.0684, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10907.0693359375
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.0693, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10907.0712890625
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.0713, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -10907.068359375
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.0684, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10907.068359375
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.0684, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10907.07421875
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.0742, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10907.0703125
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.0703, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -10907.0693359375
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.0693, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -10907.068359375
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.0684, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10907.068359375
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.0684, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10907.1015625
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.1016, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10907.10546875
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.1055, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -10907.0673828125
tensor(10907.0684, grad_fn=<NegBackward0>) tensor(10907.0674, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10907.0673828125
tensor(10907.0674, grad_fn=<NegBackward0>) tensor(10907.0674, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10907.076171875
tensor(10907.0674, grad_fn=<NegBackward0>) tensor(10907.0762, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10907.068359375
tensor(10907.0674, grad_fn=<NegBackward0>) tensor(10907.0684, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -10907.0693359375
tensor(10907.0674, grad_fn=<NegBackward0>) tensor(10907.0693, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -10907.068359375
tensor(10907.0674, grad_fn=<NegBackward0>) tensor(10907.0684, grad_fn=<NegBackward0>)
4
Iteration 8400: Loss = -10907.068359375
tensor(10907.0674, grad_fn=<NegBackward0>) tensor(10907.0684, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8400 due to no improvement.
pi: tensor([[0.7842, 0.2158],
        [0.2259, 0.7741]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4798, 0.5202], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2488, 0.1083],
         [0.5563, 0.1998]],

        [[0.7180, 0.1070],
         [0.5238, 0.5164]],

        [[0.6918, 0.0895],
         [0.6721, 0.7046]],

        [[0.6028, 0.1159],
         [0.6673, 0.6026]],

        [[0.7006, 0.0991],
         [0.5650, 0.5816]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 4
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208025343189018
Global Adjusted Rand Index: 0.9368977708572352
Average Adjusted Rand Index: 0.936643281803024
[0.9368977708572352, 0.9368977708572352] [0.936643281803024, 0.936643281803024] [10907.068359375, 10907.068359375]
-------------------------------------
This iteration is 70
True Objective function: Loss = -10919.18316150153
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18829.62109375
inf tensor(18829.6211, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10979.525390625
tensor(18829.6211, grad_fn=<NegBackward0>) tensor(10979.5254, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10979.2314453125
tensor(10979.5254, grad_fn=<NegBackward0>) tensor(10979.2314, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10979.1181640625
tensor(10979.2314, grad_fn=<NegBackward0>) tensor(10979.1182, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10979.009765625
tensor(10979.1182, grad_fn=<NegBackward0>) tensor(10979.0098, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10978.8857421875
tensor(10979.0098, grad_fn=<NegBackward0>) tensor(10978.8857, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10978.7744140625
tensor(10978.8857, grad_fn=<NegBackward0>) tensor(10978.7744, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10978.70703125
tensor(10978.7744, grad_fn=<NegBackward0>) tensor(10978.7070, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10978.630859375
tensor(10978.7070, grad_fn=<NegBackward0>) tensor(10978.6309, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10977.779296875
tensor(10978.6309, grad_fn=<NegBackward0>) tensor(10977.7793, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10976.5537109375
tensor(10977.7793, grad_fn=<NegBackward0>) tensor(10976.5537, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10976.5048828125
tensor(10976.5537, grad_fn=<NegBackward0>) tensor(10976.5049, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10976.4833984375
tensor(10976.5049, grad_fn=<NegBackward0>) tensor(10976.4834, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10976.46875
tensor(10976.4834, grad_fn=<NegBackward0>) tensor(10976.4688, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10976.4580078125
tensor(10976.4688, grad_fn=<NegBackward0>) tensor(10976.4580, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10976.451171875
tensor(10976.4580, grad_fn=<NegBackward0>) tensor(10976.4512, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10976.4462890625
tensor(10976.4512, grad_fn=<NegBackward0>) tensor(10976.4463, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10976.44140625
tensor(10976.4463, grad_fn=<NegBackward0>) tensor(10976.4414, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10976.4375
tensor(10976.4414, grad_fn=<NegBackward0>) tensor(10976.4375, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10976.4345703125
tensor(10976.4375, grad_fn=<NegBackward0>) tensor(10976.4346, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10976.431640625
tensor(10976.4346, grad_fn=<NegBackward0>) tensor(10976.4316, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10976.4306640625
tensor(10976.4316, grad_fn=<NegBackward0>) tensor(10976.4307, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10976.4267578125
tensor(10976.4307, grad_fn=<NegBackward0>) tensor(10976.4268, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10976.427734375
tensor(10976.4268, grad_fn=<NegBackward0>) tensor(10976.4277, grad_fn=<NegBackward0>)
1
Iteration 2400: Loss = -10976.4248046875
tensor(10976.4268, grad_fn=<NegBackward0>) tensor(10976.4248, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10976.4248046875
tensor(10976.4248, grad_fn=<NegBackward0>) tensor(10976.4248, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10976.4228515625
tensor(10976.4248, grad_fn=<NegBackward0>) tensor(10976.4229, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10976.423828125
tensor(10976.4229, grad_fn=<NegBackward0>) tensor(10976.4238, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -10976.4228515625
tensor(10976.4229, grad_fn=<NegBackward0>) tensor(10976.4229, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10976.4228515625
tensor(10976.4229, grad_fn=<NegBackward0>) tensor(10976.4229, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10976.419921875
tensor(10976.4229, grad_fn=<NegBackward0>) tensor(10976.4199, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10976.4208984375
tensor(10976.4199, grad_fn=<NegBackward0>) tensor(10976.4209, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -10976.4189453125
tensor(10976.4199, grad_fn=<NegBackward0>) tensor(10976.4189, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10976.4189453125
tensor(10976.4189, grad_fn=<NegBackward0>) tensor(10976.4189, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10976.4189453125
tensor(10976.4189, grad_fn=<NegBackward0>) tensor(10976.4189, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10976.41796875
tensor(10976.4189, grad_fn=<NegBackward0>) tensor(10976.4180, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10976.416015625
tensor(10976.4180, grad_fn=<NegBackward0>) tensor(10976.4160, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10976.41796875
tensor(10976.4160, grad_fn=<NegBackward0>) tensor(10976.4180, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -10976.416015625
tensor(10976.4160, grad_fn=<NegBackward0>) tensor(10976.4160, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10976.419921875
tensor(10976.4160, grad_fn=<NegBackward0>) tensor(10976.4199, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -10976.416015625
tensor(10976.4160, grad_fn=<NegBackward0>) tensor(10976.4160, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10976.416015625
tensor(10976.4160, grad_fn=<NegBackward0>) tensor(10976.4160, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10976.416015625
tensor(10976.4160, grad_fn=<NegBackward0>) tensor(10976.4160, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10976.4150390625
tensor(10976.4160, grad_fn=<NegBackward0>) tensor(10976.4150, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10976.4140625
tensor(10976.4150, grad_fn=<NegBackward0>) tensor(10976.4141, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10976.4130859375
tensor(10976.4141, grad_fn=<NegBackward0>) tensor(10976.4131, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10976.4150390625
tensor(10976.4131, grad_fn=<NegBackward0>) tensor(10976.4150, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -10976.416015625
tensor(10976.4131, grad_fn=<NegBackward0>) tensor(10976.4160, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -10976.4140625
tensor(10976.4131, grad_fn=<NegBackward0>) tensor(10976.4141, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -10976.4130859375
tensor(10976.4131, grad_fn=<NegBackward0>) tensor(10976.4131, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10976.4130859375
tensor(10976.4131, grad_fn=<NegBackward0>) tensor(10976.4131, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10976.4140625
tensor(10976.4131, grad_fn=<NegBackward0>) tensor(10976.4141, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10976.4130859375
tensor(10976.4131, grad_fn=<NegBackward0>) tensor(10976.4131, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10976.412109375
tensor(10976.4131, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10976.4140625
tensor(10976.4121, grad_fn=<NegBackward0>) tensor(10976.4141, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10976.412109375
tensor(10976.4121, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10976.4140625
tensor(10976.4121, grad_fn=<NegBackward0>) tensor(10976.4141, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10976.412109375
tensor(10976.4121, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10976.4140625
tensor(10976.4121, grad_fn=<NegBackward0>) tensor(10976.4141, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10976.4111328125
tensor(10976.4121, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10976.412109375
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10976.4130859375
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4131, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -10976.4111328125
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10976.4130859375
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4131, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10976.4111328125
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10976.412109375
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10976.412109375
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -10976.4140625
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4141, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -10976.412109375
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -10976.4111328125
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10976.412109375
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10976.4111328125
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10976.4111328125
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10976.4111328125
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10976.412109375
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10976.4111328125
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10976.4111328125
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10976.4130859375
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4131, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10976.4111328125
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10976.4130859375
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4131, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10976.4111328125
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10976.41015625
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4102, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10976.4111328125
tensor(10976.4102, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10976.4111328125
tensor(10976.4102, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -10976.44921875
tensor(10976.4102, grad_fn=<NegBackward0>) tensor(10976.4492, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -10976.4111328125
tensor(10976.4102, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -10976.4248046875
tensor(10976.4102, grad_fn=<NegBackward0>) tensor(10976.4248, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[4.2623e-04, 9.9957e-01],
        [3.1629e-02, 9.6837e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9319, 0.0681], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1662, 0.1633],
         [0.6666, 0.1585]],

        [[0.5060, 0.1609],
         [0.6034, 0.5411]],

        [[0.6929, 0.2149],
         [0.6702, 0.6590]],

        [[0.5287, 0.2684],
         [0.6912, 0.5431]],

        [[0.6933, 0.1680],
         [0.5910, 0.6507]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 71%|███████   | 71/100 [17:38:59<7:17:28, 905.13s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 72%|███████▏  | 72/100 [17:53:52<7:00:41, 901.49s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 73%|███████▎  | 73/100 [18:03:31<6:02:06, 804.67s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 74%|███████▍  | 74/100 [18:14:20<5:28:33, 758.22s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 75%|███████▌  | 75/100 [18:32:25<5:56:45, 856.22s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 76%|███████▌  | 76/100 [18:49:01<5:59:15, 898.15s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 77%|███████▋  | 77/100 [19:05:29<5:54:34, 925.00s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 78%|███████▊  | 78/100 [19:20:21<5:35:31, 915.05s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 79%|███████▉  | 79/100 [19:37:36<5:32:51, 951.04s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 80%|████████  | 80/100 [19:54:51<5:25:27, 976.35s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 81%|████████  | 81/100 [20:12:52<5:19:04, 1007.60s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 82%|████████▏ | 82/100 [20:31:01<5:09:36, 1032.05s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 83%|████████▎ | 83/100 [20:49:33<4:59:13, 1056.07s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 84%|████████▍ | 84/100 [21:07:52<4:45:04, 1069.04s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 85%|████████▌ | 85/100 [21:25:31<4:26:27, 1065.84s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 86%|████████▌ | 86/100 [21:43:44<4:10:39, 1074.23s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 87%|████████▋ | 87/100 [21:57:07<3:35:05, 992.76s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 88%|████████▊ | 88/100 [22:13:47<3:19:00, 995.01s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 89%|████████▉ | 89/100 [22:25:41<2:46:58, 910.73s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 90%|█████████ | 90/100 [22:42:44<2:37:22, 944.21s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 91%|█████████ | 91/100 [22:57:05<2:17:53, 919.32s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 92%|█████████▏| 92/100 [23:14:16<2:07:01, 952.73s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 93%|█████████▎| 93/100 [23:28:25<1:47:33, 921.87s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 94%|█████████▍| 94/100 [23:37:02<1:20:01, 800.33s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 95%|█████████▌| 95/100 [23:48:23<1:03:42, 764.42s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 96%|█████████▌| 96/100 [24:06:52<57:50, 867.71s/it]  /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 97%|█████████▋| 97/100 [24:25:13<46:53, 937.76s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 98%|█████████▊| 98/100 [24:34:08<27:13, 816.85s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 99%|█████████▉| 99/100 [24:43:49<12:26, 746.27s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
100%|██████████| 100/100 [24:58:40<00:00, 789.59s/it]100%|██████████| 100/100 [24:58:40<00:00, 899.20s/it]
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.007431593898976406
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0001651235040896123
Average Adjusted Rand Index: 0.0014863187797952812
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21144.923828125
inf tensor(21144.9238, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10979.48828125
tensor(21144.9238, grad_fn=<NegBackward0>) tensor(10979.4883, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10979.1416015625
tensor(10979.4883, grad_fn=<NegBackward0>) tensor(10979.1416, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10979.07421875
tensor(10979.1416, grad_fn=<NegBackward0>) tensor(10979.0742, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10978.9931640625
tensor(10979.0742, grad_fn=<NegBackward0>) tensor(10978.9932, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10978.869140625
tensor(10978.9932, grad_fn=<NegBackward0>) tensor(10978.8691, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10978.22265625
tensor(10978.8691, grad_fn=<NegBackward0>) tensor(10978.2227, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10977.396484375
tensor(10978.2227, grad_fn=<NegBackward0>) tensor(10977.3965, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10977.12890625
tensor(10977.3965, grad_fn=<NegBackward0>) tensor(10977.1289, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10976.4892578125
tensor(10977.1289, grad_fn=<NegBackward0>) tensor(10976.4893, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10976.427734375
tensor(10976.4893, grad_fn=<NegBackward0>) tensor(10976.4277, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10976.423828125
tensor(10976.4277, grad_fn=<NegBackward0>) tensor(10976.4238, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10976.4228515625
tensor(10976.4238, grad_fn=<NegBackward0>) tensor(10976.4229, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10976.423828125
tensor(10976.4229, grad_fn=<NegBackward0>) tensor(10976.4238, grad_fn=<NegBackward0>)
1
Iteration 1400: Loss = -10976.421875
tensor(10976.4229, grad_fn=<NegBackward0>) tensor(10976.4219, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10976.419921875
tensor(10976.4219, grad_fn=<NegBackward0>) tensor(10976.4199, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10976.4189453125
tensor(10976.4199, grad_fn=<NegBackward0>) tensor(10976.4189, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10976.4189453125
tensor(10976.4189, grad_fn=<NegBackward0>) tensor(10976.4189, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10976.4189453125
tensor(10976.4189, grad_fn=<NegBackward0>) tensor(10976.4189, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10976.4189453125
tensor(10976.4189, grad_fn=<NegBackward0>) tensor(10976.4189, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10976.4169921875
tensor(10976.4189, grad_fn=<NegBackward0>) tensor(10976.4170, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10976.41796875
tensor(10976.4170, grad_fn=<NegBackward0>) tensor(10976.4180, grad_fn=<NegBackward0>)
1
Iteration 2200: Loss = -10976.41796875
tensor(10976.4170, grad_fn=<NegBackward0>) tensor(10976.4180, grad_fn=<NegBackward0>)
2
Iteration 2300: Loss = -10976.416015625
tensor(10976.4170, grad_fn=<NegBackward0>) tensor(10976.4160, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10976.416015625
tensor(10976.4160, grad_fn=<NegBackward0>) tensor(10976.4160, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10976.4150390625
tensor(10976.4160, grad_fn=<NegBackward0>) tensor(10976.4150, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10976.4150390625
tensor(10976.4150, grad_fn=<NegBackward0>) tensor(10976.4150, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10976.4150390625
tensor(10976.4150, grad_fn=<NegBackward0>) tensor(10976.4150, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10976.4150390625
tensor(10976.4150, grad_fn=<NegBackward0>) tensor(10976.4150, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10976.4140625
tensor(10976.4150, grad_fn=<NegBackward0>) tensor(10976.4141, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10976.4140625
tensor(10976.4141, grad_fn=<NegBackward0>) tensor(10976.4141, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10976.4130859375
tensor(10976.4141, grad_fn=<NegBackward0>) tensor(10976.4131, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10976.4140625
tensor(10976.4131, grad_fn=<NegBackward0>) tensor(10976.4141, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -10976.4130859375
tensor(10976.4131, grad_fn=<NegBackward0>) tensor(10976.4131, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10976.412109375
tensor(10976.4131, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10976.412109375
tensor(10976.4121, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10976.4140625
tensor(10976.4121, grad_fn=<NegBackward0>) tensor(10976.4141, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -10976.4130859375
tensor(10976.4121, grad_fn=<NegBackward0>) tensor(10976.4131, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -10976.412109375
tensor(10976.4121, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10976.4130859375
tensor(10976.4121, grad_fn=<NegBackward0>) tensor(10976.4131, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -10976.412109375
tensor(10976.4121, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10976.412109375
tensor(10976.4121, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10976.4130859375
tensor(10976.4121, grad_fn=<NegBackward0>) tensor(10976.4131, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10976.4130859375
tensor(10976.4121, grad_fn=<NegBackward0>) tensor(10976.4131, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -10976.412109375
tensor(10976.4121, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10976.4130859375
tensor(10976.4121, grad_fn=<NegBackward0>) tensor(10976.4131, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10976.412109375
tensor(10976.4121, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10976.4111328125
tensor(10976.4121, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10976.412109375
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10976.412109375
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -10976.412109375
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -10976.412109375
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
4
Iteration 5200: Loss = -10976.4111328125
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10976.412109375
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10976.4111328125
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10976.412109375
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10976.4111328125
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10976.4111328125
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10976.4111328125
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10976.41015625
tensor(10976.4111, grad_fn=<NegBackward0>) tensor(10976.4102, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10976.4111328125
tensor(10976.4102, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10976.4111328125
tensor(10976.4102, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -10976.4111328125
tensor(10976.4102, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -10976.41015625
tensor(10976.4102, grad_fn=<NegBackward0>) tensor(10976.4102, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10976.4150390625
tensor(10976.4102, grad_fn=<NegBackward0>) tensor(10976.4150, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10976.4111328125
tensor(10976.4102, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -10976.4111328125
tensor(10976.4102, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -10976.4140625
tensor(10976.4102, grad_fn=<NegBackward0>) tensor(10976.4141, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -10976.41015625
tensor(10976.4102, grad_fn=<NegBackward0>) tensor(10976.4102, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10976.412109375
tensor(10976.4102, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10976.4111328125
tensor(10976.4102, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -10976.4111328125
tensor(10976.4102, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -10976.4140625
tensor(10976.4102, grad_fn=<NegBackward0>) tensor(10976.4141, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -10976.4091796875
tensor(10976.4102, grad_fn=<NegBackward0>) tensor(10976.4092, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10976.4111328125
tensor(10976.4092, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10976.412109375
tensor(10976.4092, grad_fn=<NegBackward0>) tensor(10976.4121, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -10976.4111328125
tensor(10976.4092, grad_fn=<NegBackward0>) tensor(10976.4111, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -10976.4150390625
tensor(10976.4092, grad_fn=<NegBackward0>) tensor(10976.4150, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -10976.41015625
tensor(10976.4092, grad_fn=<NegBackward0>) tensor(10976.4102, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7800 due to no improvement.
pi: tensor([[2.5180e-04, 9.9975e-01],
        [3.1441e-02, 9.6856e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9446, 0.0554], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1662, 0.1634],
         [0.5515, 0.1590]],

        [[0.6984, 0.1609],
         [0.7136, 0.6101]],

        [[0.5265, 0.2149],
         [0.6447, 0.5700]],

        [[0.6880, 0.2684],
         [0.5196, 0.6453]],

        [[0.7143, 0.1679],
         [0.5715, 0.6270]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.007431593898976406
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0001651235040896123
Average Adjusted Rand Index: 0.0014863187797952812
[-0.0001651235040896123, -0.0001651235040896123] [0.0014863187797952812, 0.0014863187797952812] [10976.4248046875, 10976.41015625]
-------------------------------------
This iteration is 71
True Objective function: Loss = -10844.187437180504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21235.03515625
inf tensor(21235.0352, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10994.7353515625
tensor(21235.0352, grad_fn=<NegBackward0>) tensor(10994.7354, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10993.96484375
tensor(10994.7354, grad_fn=<NegBackward0>) tensor(10993.9648, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10993.1708984375
tensor(10993.9648, grad_fn=<NegBackward0>) tensor(10993.1709, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10991.962890625
tensor(10993.1709, grad_fn=<NegBackward0>) tensor(10991.9629, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10991.2802734375
tensor(10991.9629, grad_fn=<NegBackward0>) tensor(10991.2803, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10990.49609375
tensor(10991.2803, grad_fn=<NegBackward0>) tensor(10990.4961, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10989.548828125
tensor(10990.4961, grad_fn=<NegBackward0>) tensor(10989.5488, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10988.939453125
tensor(10989.5488, grad_fn=<NegBackward0>) tensor(10988.9395, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10988.693359375
tensor(10988.9395, grad_fn=<NegBackward0>) tensor(10988.6934, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10988.537109375
tensor(10988.6934, grad_fn=<NegBackward0>) tensor(10988.5371, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10988.4375
tensor(10988.5371, grad_fn=<NegBackward0>) tensor(10988.4375, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10988.373046875
tensor(10988.4375, grad_fn=<NegBackward0>) tensor(10988.3730, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10988.33203125
tensor(10988.3730, grad_fn=<NegBackward0>) tensor(10988.3320, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10988.3046875
tensor(10988.3320, grad_fn=<NegBackward0>) tensor(10988.3047, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10988.283203125
tensor(10988.3047, grad_fn=<NegBackward0>) tensor(10988.2832, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10988.26171875
tensor(10988.2832, grad_fn=<NegBackward0>) tensor(10988.2617, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10988.23828125
tensor(10988.2617, grad_fn=<NegBackward0>) tensor(10988.2383, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10988.2021484375
tensor(10988.2383, grad_fn=<NegBackward0>) tensor(10988.2021, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10988.1435546875
tensor(10988.2021, grad_fn=<NegBackward0>) tensor(10988.1436, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10988.017578125
tensor(10988.1436, grad_fn=<NegBackward0>) tensor(10988.0176, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10975.908203125
tensor(10988.0176, grad_fn=<NegBackward0>) tensor(10975.9082, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10856.7958984375
tensor(10975.9082, grad_fn=<NegBackward0>) tensor(10856.7959, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10856.31640625
tensor(10856.7959, grad_fn=<NegBackward0>) tensor(10856.3164, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10856.228515625
tensor(10856.3164, grad_fn=<NegBackward0>) tensor(10856.2285, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10856.1943359375
tensor(10856.2285, grad_fn=<NegBackward0>) tensor(10856.1943, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10856.171875
tensor(10856.1943, grad_fn=<NegBackward0>) tensor(10856.1719, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10856.1552734375
tensor(10856.1719, grad_fn=<NegBackward0>) tensor(10856.1553, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10856.1171875
tensor(10856.1553, grad_fn=<NegBackward0>) tensor(10856.1172, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10856.10546875
tensor(10856.1172, grad_fn=<NegBackward0>) tensor(10856.1055, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10856.1015625
tensor(10856.1055, grad_fn=<NegBackward0>) tensor(10856.1016, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10856.095703125
tensor(10856.1016, grad_fn=<NegBackward0>) tensor(10856.0957, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10856.08984375
tensor(10856.0957, grad_fn=<NegBackward0>) tensor(10856.0898, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10856.0869140625
tensor(10856.0898, grad_fn=<NegBackward0>) tensor(10856.0869, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10856.08203125
tensor(10856.0869, grad_fn=<NegBackward0>) tensor(10856.0820, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10856.0791015625
tensor(10856.0820, grad_fn=<NegBackward0>) tensor(10856.0791, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10856.0771484375
tensor(10856.0791, grad_fn=<NegBackward0>) tensor(10856.0771, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10856.076171875
tensor(10856.0771, grad_fn=<NegBackward0>) tensor(10856.0762, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10856.07421875
tensor(10856.0762, grad_fn=<NegBackward0>) tensor(10856.0742, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10856.0712890625
tensor(10856.0742, grad_fn=<NegBackward0>) tensor(10856.0713, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10856.0673828125
tensor(10856.0713, grad_fn=<NegBackward0>) tensor(10856.0674, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10856.0625
tensor(10856.0674, grad_fn=<NegBackward0>) tensor(10856.0625, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10856.060546875
tensor(10856.0625, grad_fn=<NegBackward0>) tensor(10856.0605, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10856.060546875
tensor(10856.0605, grad_fn=<NegBackward0>) tensor(10856.0605, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10856.05859375
tensor(10856.0605, grad_fn=<NegBackward0>) tensor(10856.0586, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10856.0595703125
tensor(10856.0586, grad_fn=<NegBackward0>) tensor(10856.0596, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10856.0576171875
tensor(10856.0586, grad_fn=<NegBackward0>) tensor(10856.0576, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10856.05859375
tensor(10856.0576, grad_fn=<NegBackward0>) tensor(10856.0586, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10856.0537109375
tensor(10856.0576, grad_fn=<NegBackward0>) tensor(10856.0537, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10856.05078125
tensor(10856.0537, grad_fn=<NegBackward0>) tensor(10856.0508, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10856.0498046875
tensor(10856.0508, grad_fn=<NegBackward0>) tensor(10856.0498, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10856.0498046875
tensor(10856.0498, grad_fn=<NegBackward0>) tensor(10856.0498, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10856.0478515625
tensor(10856.0498, grad_fn=<NegBackward0>) tensor(10856.0479, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10856.046875
tensor(10856.0479, grad_fn=<NegBackward0>) tensor(10856.0469, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10856.046875
tensor(10856.0469, grad_fn=<NegBackward0>) tensor(10856.0469, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10856.060546875
tensor(10856.0469, grad_fn=<NegBackward0>) tensor(10856.0605, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10856.0458984375
tensor(10856.0469, grad_fn=<NegBackward0>) tensor(10856.0459, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10856.04296875
tensor(10856.0459, grad_fn=<NegBackward0>) tensor(10856.0430, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10856.0419921875
tensor(10856.0430, grad_fn=<NegBackward0>) tensor(10856.0420, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10856.0810546875
tensor(10856.0420, grad_fn=<NegBackward0>) tensor(10856.0811, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10856.0380859375
tensor(10856.0420, grad_fn=<NegBackward0>) tensor(10856.0381, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10856.033203125
tensor(10856.0381, grad_fn=<NegBackward0>) tensor(10856.0332, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10856.0341796875
tensor(10856.0332, grad_fn=<NegBackward0>) tensor(10856.0342, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10856.0185546875
tensor(10856.0332, grad_fn=<NegBackward0>) tensor(10856.0186, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10856.0
tensor(10856.0186, grad_fn=<NegBackward0>) tensor(10856., grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10855.9736328125
tensor(10856., grad_fn=<NegBackward0>) tensor(10855.9736, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10824.880859375
tensor(10855.9736, grad_fn=<NegBackward0>) tensor(10824.8809, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10824.392578125
tensor(10824.8809, grad_fn=<NegBackward0>) tensor(10824.3926, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10821.7158203125
tensor(10824.3926, grad_fn=<NegBackward0>) tensor(10821.7158, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10821.607421875
tensor(10821.7158, grad_fn=<NegBackward0>) tensor(10821.6074, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10821.6064453125
tensor(10821.6074, grad_fn=<NegBackward0>) tensor(10821.6064, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10821.60546875
tensor(10821.6064, grad_fn=<NegBackward0>) tensor(10821.6055, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10821.607421875
tensor(10821.6055, grad_fn=<NegBackward0>) tensor(10821.6074, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10821.6025390625
tensor(10821.6055, grad_fn=<NegBackward0>) tensor(10821.6025, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10821.55078125
tensor(10821.6025, grad_fn=<NegBackward0>) tensor(10821.5508, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10814.9306640625
tensor(10821.5508, grad_fn=<NegBackward0>) tensor(10814.9307, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10814.7978515625
tensor(10814.9307, grad_fn=<NegBackward0>) tensor(10814.7979, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10814.7880859375
tensor(10814.7979, grad_fn=<NegBackward0>) tensor(10814.7881, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10814.7880859375
tensor(10814.7881, grad_fn=<NegBackward0>) tensor(10814.7881, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10814.787109375
tensor(10814.7881, grad_fn=<NegBackward0>) tensor(10814.7871, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10814.7939453125
tensor(10814.7871, grad_fn=<NegBackward0>) tensor(10814.7939, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10814.7880859375
tensor(10814.7871, grad_fn=<NegBackward0>) tensor(10814.7881, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -10814.7421875
tensor(10814.7871, grad_fn=<NegBackward0>) tensor(10814.7422, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10814.7919921875
tensor(10814.7422, grad_fn=<NegBackward0>) tensor(10814.7920, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10814.740234375
tensor(10814.7422, grad_fn=<NegBackward0>) tensor(10814.7402, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10814.7373046875
tensor(10814.7402, grad_fn=<NegBackward0>) tensor(10814.7373, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10814.73828125
tensor(10814.7373, grad_fn=<NegBackward0>) tensor(10814.7383, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10814.7373046875
tensor(10814.7373, grad_fn=<NegBackward0>) tensor(10814.7373, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10814.662109375
tensor(10814.7373, grad_fn=<NegBackward0>) tensor(10814.6621, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10814.6787109375
tensor(10814.6621, grad_fn=<NegBackward0>) tensor(10814.6787, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -10814.65234375
tensor(10814.6621, grad_fn=<NegBackward0>) tensor(10814.6523, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10814.65234375
tensor(10814.6523, grad_fn=<NegBackward0>) tensor(10814.6523, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10814.7099609375
tensor(10814.6523, grad_fn=<NegBackward0>) tensor(10814.7100, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -10814.65234375
tensor(10814.6523, grad_fn=<NegBackward0>) tensor(10814.6523, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10814.6591796875
tensor(10814.6523, grad_fn=<NegBackward0>) tensor(10814.6592, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -10814.65234375
tensor(10814.6523, grad_fn=<NegBackward0>) tensor(10814.6523, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10814.65234375
tensor(10814.6523, grad_fn=<NegBackward0>) tensor(10814.6523, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10814.6513671875
tensor(10814.6523, grad_fn=<NegBackward0>) tensor(10814.6514, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10814.6533203125
tensor(10814.6514, grad_fn=<NegBackward0>) tensor(10814.6533, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -10814.650390625
tensor(10814.6514, grad_fn=<NegBackward0>) tensor(10814.6504, grad_fn=<NegBackward0>)
pi: tensor([[0.8136, 0.1864],
        [0.2550, 0.7450]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4188, 0.5812], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2595, 0.1017],
         [0.6338, 0.1932]],

        [[0.5528, 0.0946],
         [0.6948, 0.5528]],

        [[0.7087, 0.0932],
         [0.5789, 0.6520]],

        [[0.5102, 0.1005],
         [0.5931, 0.7133]],

        [[0.5349, 0.0981],
         [0.5510, 0.5733]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 3
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 7
Adjusted Rand Index: 0.7369552685595733
time is 4
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9204282407407407
Global Adjusted Rand Index: 0.8909177419152994
Average Adjusted Rand Index: 0.8917980999446605
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23401.201171875
inf tensor(23401.2012, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10992.7900390625
tensor(23401.2012, grad_fn=<NegBackward0>) tensor(10992.7900, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10991.4794921875
tensor(10992.7900, grad_fn=<NegBackward0>) tensor(10991.4795, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10989.1787109375
tensor(10991.4795, grad_fn=<NegBackward0>) tensor(10989.1787, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10988.4326171875
tensor(10989.1787, grad_fn=<NegBackward0>) tensor(10988.4326, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10988.169921875
tensor(10988.4326, grad_fn=<NegBackward0>) tensor(10988.1699, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10987.9833984375
tensor(10988.1699, grad_fn=<NegBackward0>) tensor(10987.9834, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10987.7744140625
tensor(10987.9834, grad_fn=<NegBackward0>) tensor(10987.7744, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10987.572265625
tensor(10987.7744, grad_fn=<NegBackward0>) tensor(10987.5723, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10987.396484375
tensor(10987.5723, grad_fn=<NegBackward0>) tensor(10987.3965, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10987.2724609375
tensor(10987.3965, grad_fn=<NegBackward0>) tensor(10987.2725, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10987.1875
tensor(10987.2725, grad_fn=<NegBackward0>) tensor(10987.1875, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10987.1328125
tensor(10987.1875, grad_fn=<NegBackward0>) tensor(10987.1328, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10987.09765625
tensor(10987.1328, grad_fn=<NegBackward0>) tensor(10987.0977, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10987.0693359375
tensor(10987.0977, grad_fn=<NegBackward0>) tensor(10987.0693, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10987.0478515625
tensor(10987.0693, grad_fn=<NegBackward0>) tensor(10987.0479, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10987.03125
tensor(10987.0479, grad_fn=<NegBackward0>) tensor(10987.0312, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10987.0205078125
tensor(10987.0312, grad_fn=<NegBackward0>) tensor(10987.0205, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10987.0107421875
tensor(10987.0205, grad_fn=<NegBackward0>) tensor(10987.0107, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10987.0029296875
tensor(10987.0107, grad_fn=<NegBackward0>) tensor(10987.0029, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10986.998046875
tensor(10987.0029, grad_fn=<NegBackward0>) tensor(10986.9980, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10986.994140625
tensor(10986.9980, grad_fn=<NegBackward0>) tensor(10986.9941, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10986.9912109375
tensor(10986.9941, grad_fn=<NegBackward0>) tensor(10986.9912, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10986.990234375
tensor(10986.9912, grad_fn=<NegBackward0>) tensor(10986.9902, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10986.9873046875
tensor(10986.9902, grad_fn=<NegBackward0>) tensor(10986.9873, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10986.986328125
tensor(10986.9873, grad_fn=<NegBackward0>) tensor(10986.9863, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10986.986328125
tensor(10986.9863, grad_fn=<NegBackward0>) tensor(10986.9863, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10986.984375
tensor(10986.9863, grad_fn=<NegBackward0>) tensor(10986.9844, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10986.9814453125
tensor(10986.9844, grad_fn=<NegBackward0>) tensor(10986.9814, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10986.9833984375
tensor(10986.9814, grad_fn=<NegBackward0>) tensor(10986.9834, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -10986.982421875
tensor(10986.9814, grad_fn=<NegBackward0>) tensor(10986.9824, grad_fn=<NegBackward0>)
2
Iteration 3100: Loss = -10986.9814453125
tensor(10986.9814, grad_fn=<NegBackward0>) tensor(10986.9814, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10986.9814453125
tensor(10986.9814, grad_fn=<NegBackward0>) tensor(10986.9814, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10986.982421875
tensor(10986.9814, grad_fn=<NegBackward0>) tensor(10986.9824, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -10986.9814453125
tensor(10986.9814, grad_fn=<NegBackward0>) tensor(10986.9814, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10986.9814453125
tensor(10986.9814, grad_fn=<NegBackward0>) tensor(10986.9814, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10986.9814453125
tensor(10986.9814, grad_fn=<NegBackward0>) tensor(10986.9814, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10986.982421875
tensor(10986.9814, grad_fn=<NegBackward0>) tensor(10986.9824, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -10986.98046875
tensor(10986.9814, grad_fn=<NegBackward0>) tensor(10986.9805, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10986.98046875
tensor(10986.9805, grad_fn=<NegBackward0>) tensor(10986.9805, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10986.98046875
tensor(10986.9805, grad_fn=<NegBackward0>) tensor(10986.9805, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10986.98046875
tensor(10986.9805, grad_fn=<NegBackward0>) tensor(10986.9805, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10986.9794921875
tensor(10986.9805, grad_fn=<NegBackward0>) tensor(10986.9795, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10986.9794921875
tensor(10986.9795, grad_fn=<NegBackward0>) tensor(10986.9795, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10986.9794921875
tensor(10986.9795, grad_fn=<NegBackward0>) tensor(10986.9795, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10986.978515625
tensor(10986.9795, grad_fn=<NegBackward0>) tensor(10986.9785, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10986.978515625
tensor(10986.9785, grad_fn=<NegBackward0>) tensor(10986.9785, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10986.978515625
tensor(10986.9785, grad_fn=<NegBackward0>) tensor(10986.9785, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10986.978515625
tensor(10986.9785, grad_fn=<NegBackward0>) tensor(10986.9785, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10986.978515625
tensor(10986.9785, grad_fn=<NegBackward0>) tensor(10986.9785, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10986.978515625
tensor(10986.9785, grad_fn=<NegBackward0>) tensor(10986.9785, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10986.978515625
tensor(10986.9785, grad_fn=<NegBackward0>) tensor(10986.9785, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10986.978515625
tensor(10986.9785, grad_fn=<NegBackward0>) tensor(10986.9785, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10986.978515625
tensor(10986.9785, grad_fn=<NegBackward0>) tensor(10986.9785, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10986.9794921875
tensor(10986.9785, grad_fn=<NegBackward0>) tensor(10986.9795, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10986.978515625
tensor(10986.9785, grad_fn=<NegBackward0>) tensor(10986.9785, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10986.9775390625
tensor(10986.9785, grad_fn=<NegBackward0>) tensor(10986.9775, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10986.9775390625
tensor(10986.9775, grad_fn=<NegBackward0>) tensor(10986.9775, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10986.978515625
tensor(10986.9775, grad_fn=<NegBackward0>) tensor(10986.9785, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10986.978515625
tensor(10986.9775, grad_fn=<NegBackward0>) tensor(10986.9785, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -10986.978515625
tensor(10986.9775, grad_fn=<NegBackward0>) tensor(10986.9785, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -10986.978515625
tensor(10986.9775, grad_fn=<NegBackward0>) tensor(10986.9785, grad_fn=<NegBackward0>)
4
Iteration 6200: Loss = -10986.978515625
tensor(10986.9775, grad_fn=<NegBackward0>) tensor(10986.9785, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6200 due to no improvement.
pi: tensor([[0.1507, 0.8493],
        [0.0312, 0.9688]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0095, 0.9905], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3752, 0.2891],
         [0.7290, 0.1627]],

        [[0.5331, 0.1749],
         [0.6908, 0.5960]],

        [[0.5200, 0.0640],
         [0.6731, 0.6206]],

        [[0.7135, 0.0968],
         [0.5987, 0.6337]],

        [[0.6683, 0.2359],
         [0.5794, 0.6410]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.022626262626262626
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 62
Adjusted Rand Index: -0.016594385273424505
Global Adjusted Rand Index: 0.0005963754440256917
Average Adjusted Rand Index: -0.000228642577043835
[0.8909177419152994, 0.0005963754440256917] [0.8917980999446605, -0.000228642577043835] [10814.7705078125, 10986.978515625]
-------------------------------------
This iteration is 72
True Objective function: Loss = -10822.548436924195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22541.29296875
inf tensor(22541.2930, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10978.0439453125
tensor(22541.2930, grad_fn=<NegBackward0>) tensor(10978.0439, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10977.7265625
tensor(10978.0439, grad_fn=<NegBackward0>) tensor(10977.7266, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10977.62109375
tensor(10977.7266, grad_fn=<NegBackward0>) tensor(10977.6211, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10977.5400390625
tensor(10977.6211, grad_fn=<NegBackward0>) tensor(10977.5400, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10977.46484375
tensor(10977.5400, grad_fn=<NegBackward0>) tensor(10977.4648, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10977.3828125
tensor(10977.4648, grad_fn=<NegBackward0>) tensor(10977.3828, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10977.2802734375
tensor(10977.3828, grad_fn=<NegBackward0>) tensor(10977.2803, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10977.1298828125
tensor(10977.2803, grad_fn=<NegBackward0>) tensor(10977.1299, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10976.9501953125
tensor(10977.1299, grad_fn=<NegBackward0>) tensor(10976.9502, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10976.7646484375
tensor(10976.9502, grad_fn=<NegBackward0>) tensor(10976.7646, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10976.517578125
tensor(10976.7646, grad_fn=<NegBackward0>) tensor(10976.5176, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10976.158203125
tensor(10976.5176, grad_fn=<NegBackward0>) tensor(10976.1582, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10975.6923828125
tensor(10976.1582, grad_fn=<NegBackward0>) tensor(10975.6924, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10975.2587890625
tensor(10975.6924, grad_fn=<NegBackward0>) tensor(10975.2588, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10975.099609375
tensor(10975.2588, grad_fn=<NegBackward0>) tensor(10975.0996, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10975.0517578125
tensor(10975.0996, grad_fn=<NegBackward0>) tensor(10975.0518, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10975.044921875
tensor(10975.0518, grad_fn=<NegBackward0>) tensor(10975.0449, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10975.041015625
tensor(10975.0449, grad_fn=<NegBackward0>) tensor(10975.0410, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10975.0380859375
tensor(10975.0410, grad_fn=<NegBackward0>) tensor(10975.0381, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10975.0380859375
tensor(10975.0381, grad_fn=<NegBackward0>) tensor(10975.0381, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10975.0380859375
tensor(10975.0381, grad_fn=<NegBackward0>) tensor(10975.0381, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10975.0380859375
tensor(10975.0381, grad_fn=<NegBackward0>) tensor(10975.0381, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10975.037109375
tensor(10975.0381, grad_fn=<NegBackward0>) tensor(10975.0371, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10975.0361328125
tensor(10975.0371, grad_fn=<NegBackward0>) tensor(10975.0361, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10975.0341796875
tensor(10975.0361, grad_fn=<NegBackward0>) tensor(10975.0342, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10975.033203125
tensor(10975.0342, grad_fn=<NegBackward0>) tensor(10975.0332, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10975.0302734375
tensor(10975.0332, grad_fn=<NegBackward0>) tensor(10975.0303, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10975.0244140625
tensor(10975.0303, grad_fn=<NegBackward0>) tensor(10975.0244, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10975.015625
tensor(10975.0244, grad_fn=<NegBackward0>) tensor(10975.0156, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10974.974609375
tensor(10975.0156, grad_fn=<NegBackward0>) tensor(10974.9746, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10974.53515625
tensor(10974.9746, grad_fn=<NegBackward0>) tensor(10974.5352, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10974.0166015625
tensor(10974.5352, grad_fn=<NegBackward0>) tensor(10974.0166, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10973.4345703125
tensor(10974.0166, grad_fn=<NegBackward0>) tensor(10973.4346, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10973.38671875
tensor(10973.4346, grad_fn=<NegBackward0>) tensor(10973.3867, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10973.384765625
tensor(10973.3867, grad_fn=<NegBackward0>) tensor(10973.3848, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10973.384765625
tensor(10973.3848, grad_fn=<NegBackward0>) tensor(10973.3848, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10973.384765625
tensor(10973.3848, grad_fn=<NegBackward0>) tensor(10973.3848, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10973.384765625
tensor(10973.3848, grad_fn=<NegBackward0>) tensor(10973.3848, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10973.388671875
tensor(10973.3848, grad_fn=<NegBackward0>) tensor(10973.3887, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -10973.384765625
tensor(10973.3848, grad_fn=<NegBackward0>) tensor(10973.3848, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10973.38671875
tensor(10973.3848, grad_fn=<NegBackward0>) tensor(10973.3867, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -10973.3857421875
tensor(10973.3848, grad_fn=<NegBackward0>) tensor(10973.3857, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -10973.384765625
tensor(10973.3848, grad_fn=<NegBackward0>) tensor(10973.3848, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10973.384765625
tensor(10973.3848, grad_fn=<NegBackward0>) tensor(10973.3848, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10973.384765625
tensor(10973.3848, grad_fn=<NegBackward0>) tensor(10973.3848, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10973.3837890625
tensor(10973.3848, grad_fn=<NegBackward0>) tensor(10973.3838, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10973.384765625
tensor(10973.3838, grad_fn=<NegBackward0>) tensor(10973.3848, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10973.3857421875
tensor(10973.3838, grad_fn=<NegBackward0>) tensor(10973.3857, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -10973.4208984375
tensor(10973.3838, grad_fn=<NegBackward0>) tensor(10973.4209, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -10973.3857421875
tensor(10973.3838, grad_fn=<NegBackward0>) tensor(10973.3857, grad_fn=<NegBackward0>)
4
Iteration 5100: Loss = -10973.38671875
tensor(10973.3838, grad_fn=<NegBackward0>) tensor(10973.3867, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5100 due to no improvement.
pi: tensor([[0.4471, 0.5529],
        [0.0657, 0.9343]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2674, 0.7326], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1310, 0.1448],
         [0.6701, 0.1720]],

        [[0.5513, 0.1420],
         [0.5330, 0.7297]],

        [[0.6642, 0.1676],
         [0.6031, 0.6580]],

        [[0.7125, 0.1185],
         [0.6230, 0.5012]],

        [[0.6588, 0.1140],
         [0.5569, 0.5116]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.01150857271232653
time is 1
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.004130624939255516
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.0006803382252891437
Global Adjusted Rand Index: 0.0031967744216733008
Average Adjusted Rand Index: 0.0030060482553895007
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23110.109375
inf tensor(23110.1094, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10979.025390625
tensor(23110.1094, grad_fn=<NegBackward0>) tensor(10979.0254, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10977.771484375
tensor(10979.0254, grad_fn=<NegBackward0>) tensor(10977.7715, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10977.482421875
tensor(10977.7715, grad_fn=<NegBackward0>) tensor(10977.4824, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10977.326171875
tensor(10977.4824, grad_fn=<NegBackward0>) tensor(10977.3262, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10977.2041015625
tensor(10977.3262, grad_fn=<NegBackward0>) tensor(10977.2041, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10977.0927734375
tensor(10977.2041, grad_fn=<NegBackward0>) tensor(10977.0928, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10976.982421875
tensor(10977.0928, grad_fn=<NegBackward0>) tensor(10976.9824, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10976.869140625
tensor(10976.9824, grad_fn=<NegBackward0>) tensor(10976.8691, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10976.759765625
tensor(10976.8691, grad_fn=<NegBackward0>) tensor(10976.7598, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10976.6572265625
tensor(10976.7598, grad_fn=<NegBackward0>) tensor(10976.6572, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10976.5595703125
tensor(10976.6572, grad_fn=<NegBackward0>) tensor(10976.5596, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10976.47265625
tensor(10976.5596, grad_fn=<NegBackward0>) tensor(10976.4727, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10976.392578125
tensor(10976.4727, grad_fn=<NegBackward0>) tensor(10976.3926, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10976.32421875
tensor(10976.3926, grad_fn=<NegBackward0>) tensor(10976.3242, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10976.263671875
tensor(10976.3242, grad_fn=<NegBackward0>) tensor(10976.2637, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10976.205078125
tensor(10976.2637, grad_fn=<NegBackward0>) tensor(10976.2051, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10976.1533203125
tensor(10976.2051, grad_fn=<NegBackward0>) tensor(10976.1533, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10976.1044921875
tensor(10976.1533, grad_fn=<NegBackward0>) tensor(10976.1045, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10976.060546875
tensor(10976.1045, grad_fn=<NegBackward0>) tensor(10976.0605, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10976.0244140625
tensor(10976.0605, grad_fn=<NegBackward0>) tensor(10976.0244, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10975.990234375
tensor(10976.0244, grad_fn=<NegBackward0>) tensor(10975.9902, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10975.9619140625
tensor(10975.9902, grad_fn=<NegBackward0>) tensor(10975.9619, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10975.939453125
tensor(10975.9619, grad_fn=<NegBackward0>) tensor(10975.9395, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10975.9208984375
tensor(10975.9395, grad_fn=<NegBackward0>) tensor(10975.9209, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10975.9052734375
tensor(10975.9209, grad_fn=<NegBackward0>) tensor(10975.9053, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10975.8935546875
tensor(10975.9053, grad_fn=<NegBackward0>) tensor(10975.8936, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10975.8818359375
tensor(10975.8936, grad_fn=<NegBackward0>) tensor(10975.8818, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10975.87109375
tensor(10975.8818, grad_fn=<NegBackward0>) tensor(10975.8711, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10975.8603515625
tensor(10975.8711, grad_fn=<NegBackward0>) tensor(10975.8604, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10975.849609375
tensor(10975.8604, grad_fn=<NegBackward0>) tensor(10975.8496, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10975.8388671875
tensor(10975.8496, grad_fn=<NegBackward0>) tensor(10975.8389, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10975.82421875
tensor(10975.8389, grad_fn=<NegBackward0>) tensor(10975.8242, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10975.80859375
tensor(10975.8242, grad_fn=<NegBackward0>) tensor(10975.8086, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10975.7880859375
tensor(10975.8086, grad_fn=<NegBackward0>) tensor(10975.7881, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10975.7578125
tensor(10975.7881, grad_fn=<NegBackward0>) tensor(10975.7578, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10975.6923828125
tensor(10975.7578, grad_fn=<NegBackward0>) tensor(10975.6924, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10975.2998046875
tensor(10975.6924, grad_fn=<NegBackward0>) tensor(10975.2998, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10973.5615234375
tensor(10975.2998, grad_fn=<NegBackward0>) tensor(10973.5615, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10973.44921875
tensor(10973.5615, grad_fn=<NegBackward0>) tensor(10973.4492, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10973.41015625
tensor(10973.4492, grad_fn=<NegBackward0>) tensor(10973.4102, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10973.392578125
tensor(10973.4102, grad_fn=<NegBackward0>) tensor(10973.3926, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10973.3896484375
tensor(10973.3926, grad_fn=<NegBackward0>) tensor(10973.3896, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10973.3876953125
tensor(10973.3896, grad_fn=<NegBackward0>) tensor(10973.3877, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10973.384765625
tensor(10973.3877, grad_fn=<NegBackward0>) tensor(10973.3848, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10973.3857421875
tensor(10973.3848, grad_fn=<NegBackward0>) tensor(10973.3857, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10973.384765625
tensor(10973.3848, grad_fn=<NegBackward0>) tensor(10973.3848, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10973.384765625
tensor(10973.3848, grad_fn=<NegBackward0>) tensor(10973.3848, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10973.3837890625
tensor(10973.3848, grad_fn=<NegBackward0>) tensor(10973.3838, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10973.384765625
tensor(10973.3838, grad_fn=<NegBackward0>) tensor(10973.3848, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10973.3896484375
tensor(10973.3838, grad_fn=<NegBackward0>) tensor(10973.3896, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -10973.3857421875
tensor(10973.3838, grad_fn=<NegBackward0>) tensor(10973.3857, grad_fn=<NegBackward0>)
3
Iteration 5200: Loss = -10973.384765625
tensor(10973.3838, grad_fn=<NegBackward0>) tensor(10973.3848, grad_fn=<NegBackward0>)
4
Iteration 5300: Loss = -10973.384765625
tensor(10973.3838, grad_fn=<NegBackward0>) tensor(10973.3848, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5300 due to no improvement.
pi: tensor([[0.9344, 0.0656],
        [0.5533, 0.4467]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7330, 0.2670], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1720, 0.1448],
         [0.6138, 0.1310]],

        [[0.6886, 0.1421],
         [0.7079, 0.5509]],

        [[0.7001, 0.1676],
         [0.5381, 0.5776]],

        [[0.7101, 0.1186],
         [0.5117, 0.5115]],

        [[0.5527, 0.1140],
         [0.5944, 0.5034]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.01150857271232653
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.004130624939255516
time is 4
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.0006803382252891437
Global Adjusted Rand Index: 0.0031967744216733008
Average Adjusted Rand Index: 0.0030060482553895007
[0.0031967744216733008, 0.0031967744216733008] [0.0030060482553895007, 0.0030060482553895007] [10973.38671875, 10973.384765625]
-------------------------------------
This iteration is 73
True Objective function: Loss = -10928.172829790668
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21573.201171875
inf tensor(21573.2012, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11025.056640625
tensor(21573.2012, grad_fn=<NegBackward0>) tensor(11025.0566, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11024.3251953125
tensor(11025.0566, grad_fn=<NegBackward0>) tensor(11024.3252, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11023.64453125
tensor(11024.3252, grad_fn=<NegBackward0>) tensor(11023.6445, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11022.8701171875
tensor(11023.6445, grad_fn=<NegBackward0>) tensor(11022.8701, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11022.2177734375
tensor(11022.8701, grad_fn=<NegBackward0>) tensor(11022.2178, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11021.46875
tensor(11022.2178, grad_fn=<NegBackward0>) tensor(11021.4688, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11021.27734375
tensor(11021.4688, grad_fn=<NegBackward0>) tensor(11021.2773, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11021.1728515625
tensor(11021.2773, grad_fn=<NegBackward0>) tensor(11021.1729, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11021.103515625
tensor(11021.1729, grad_fn=<NegBackward0>) tensor(11021.1035, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11021.0439453125
tensor(11021.1035, grad_fn=<NegBackward0>) tensor(11021.0439, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11020.9677734375
tensor(11021.0439, grad_fn=<NegBackward0>) tensor(11020.9678, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11020.8291015625
tensor(11020.9678, grad_fn=<NegBackward0>) tensor(11020.8291, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11020.7099609375
tensor(11020.8291, grad_fn=<NegBackward0>) tensor(11020.7100, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11020.6318359375
tensor(11020.7100, grad_fn=<NegBackward0>) tensor(11020.6318, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11020.556640625
tensor(11020.6318, grad_fn=<NegBackward0>) tensor(11020.5566, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11020.486328125
tensor(11020.5566, grad_fn=<NegBackward0>) tensor(11020.4863, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11020.4189453125
tensor(11020.4863, grad_fn=<NegBackward0>) tensor(11020.4189, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11020.3583984375
tensor(11020.4189, grad_fn=<NegBackward0>) tensor(11020.3584, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11020.3056640625
tensor(11020.3584, grad_fn=<NegBackward0>) tensor(11020.3057, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11020.25390625
tensor(11020.3057, grad_fn=<NegBackward0>) tensor(11020.2539, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11020.189453125
tensor(11020.2539, grad_fn=<NegBackward0>) tensor(11020.1895, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11020.0576171875
tensor(11020.1895, grad_fn=<NegBackward0>) tensor(11020.0576, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11019.265625
tensor(11020.0576, grad_fn=<NegBackward0>) tensor(11019.2656, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10894.73828125
tensor(11019.2656, grad_fn=<NegBackward0>) tensor(10894.7383, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10881.9951171875
tensor(10894.7383, grad_fn=<NegBackward0>) tensor(10881.9951, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10881.90234375
tensor(10881.9951, grad_fn=<NegBackward0>) tensor(10881.9023, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10881.8095703125
tensor(10881.9023, grad_fn=<NegBackward0>) tensor(10881.8096, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10881.7978515625
tensor(10881.8096, grad_fn=<NegBackward0>) tensor(10881.7979, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10881.7890625
tensor(10881.7979, grad_fn=<NegBackward0>) tensor(10881.7891, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10881.783203125
tensor(10881.7891, grad_fn=<NegBackward0>) tensor(10881.7832, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10881.7763671875
tensor(10881.7832, grad_fn=<NegBackward0>) tensor(10881.7764, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10881.751953125
tensor(10881.7764, grad_fn=<NegBackward0>) tensor(10881.7520, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10881.7470703125
tensor(10881.7520, grad_fn=<NegBackward0>) tensor(10881.7471, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10881.7451171875
tensor(10881.7471, grad_fn=<NegBackward0>) tensor(10881.7451, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10881.7431640625
tensor(10881.7451, grad_fn=<NegBackward0>) tensor(10881.7432, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10881.7412109375
tensor(10881.7432, grad_fn=<NegBackward0>) tensor(10881.7412, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10881.7412109375
tensor(10881.7412, grad_fn=<NegBackward0>) tensor(10881.7412, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10881.744140625
tensor(10881.7412, grad_fn=<NegBackward0>) tensor(10881.7441, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -10881.7392578125
tensor(10881.7412, grad_fn=<NegBackward0>) tensor(10881.7393, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10881.73828125
tensor(10881.7393, grad_fn=<NegBackward0>) tensor(10881.7383, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10881.736328125
tensor(10881.7383, grad_fn=<NegBackward0>) tensor(10881.7363, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10881.732421875
tensor(10881.7363, grad_fn=<NegBackward0>) tensor(10881.7324, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10881.732421875
tensor(10881.7324, grad_fn=<NegBackward0>) tensor(10881.7324, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10881.7421875
tensor(10881.7324, grad_fn=<NegBackward0>) tensor(10881.7422, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10881.732421875
tensor(10881.7324, grad_fn=<NegBackward0>) tensor(10881.7324, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10881.7314453125
tensor(10881.7324, grad_fn=<NegBackward0>) tensor(10881.7314, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10881.732421875
tensor(10881.7314, grad_fn=<NegBackward0>) tensor(10881.7324, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10881.732421875
tensor(10881.7314, grad_fn=<NegBackward0>) tensor(10881.7324, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -10881.734375
tensor(10881.7314, grad_fn=<NegBackward0>) tensor(10881.7344, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -10881.7333984375
tensor(10881.7314, grad_fn=<NegBackward0>) tensor(10881.7334, grad_fn=<NegBackward0>)
4
Iteration 5100: Loss = -10881.732421875
tensor(10881.7314, grad_fn=<NegBackward0>) tensor(10881.7324, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5100 due to no improvement.
pi: tensor([[0.8125, 0.1875],
        [0.2227, 0.7773]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5113, 0.4887], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2507, 0.0957],
         [0.6715, 0.1935]],

        [[0.7143, 0.1112],
         [0.5918, 0.6711]],

        [[0.5068, 0.1074],
         [0.6261, 0.6867]],

        [[0.6840, 0.1054],
         [0.7160, 0.5178]],

        [[0.5465, 0.0919],
         [0.5807, 0.6943]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448338943900694
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 11
Adjusted Rand Index: 0.6044118965160035
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
Global Adjusted Rand Index: 0.8387334700840575
Average Adjusted Rand Index: 0.8428175652377485
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19309.15625
inf tensor(19309.1562, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11023.12890625
tensor(19309.1562, grad_fn=<NegBackward0>) tensor(11023.1289, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11022.16796875
tensor(11023.1289, grad_fn=<NegBackward0>) tensor(11022.1680, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11020.685546875
tensor(11022.1680, grad_fn=<NegBackward0>) tensor(11020.6855, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10977.052734375
tensor(11020.6855, grad_fn=<NegBackward0>) tensor(10977.0527, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10881.9853515625
tensor(10977.0527, grad_fn=<NegBackward0>) tensor(10881.9854, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10881.8291015625
tensor(10881.9854, grad_fn=<NegBackward0>) tensor(10881.8291, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10881.7841796875
tensor(10881.8291, grad_fn=<NegBackward0>) tensor(10881.7842, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10881.7744140625
tensor(10881.7842, grad_fn=<NegBackward0>) tensor(10881.7744, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10881.7548828125
tensor(10881.7744, grad_fn=<NegBackward0>) tensor(10881.7549, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10881.7373046875
tensor(10881.7549, grad_fn=<NegBackward0>) tensor(10881.7373, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10881.736328125
tensor(10881.7373, grad_fn=<NegBackward0>) tensor(10881.7363, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10881.7353515625
tensor(10881.7363, grad_fn=<NegBackward0>) tensor(10881.7354, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10881.7333984375
tensor(10881.7354, grad_fn=<NegBackward0>) tensor(10881.7334, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10881.7333984375
tensor(10881.7334, grad_fn=<NegBackward0>) tensor(10881.7334, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10881.732421875
tensor(10881.7334, grad_fn=<NegBackward0>) tensor(10881.7324, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10881.7314453125
tensor(10881.7324, grad_fn=<NegBackward0>) tensor(10881.7314, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10881.7314453125
tensor(10881.7314, grad_fn=<NegBackward0>) tensor(10881.7314, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10881.73046875
tensor(10881.7314, grad_fn=<NegBackward0>) tensor(10881.7305, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10881.73046875
tensor(10881.7305, grad_fn=<NegBackward0>) tensor(10881.7305, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10881.73046875
tensor(10881.7305, grad_fn=<NegBackward0>) tensor(10881.7305, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10881.73046875
tensor(10881.7305, grad_fn=<NegBackward0>) tensor(10881.7305, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10881.7314453125
tensor(10881.7305, grad_fn=<NegBackward0>) tensor(10881.7314, grad_fn=<NegBackward0>)
1
Iteration 2300: Loss = -10881.73046875
tensor(10881.7305, grad_fn=<NegBackward0>) tensor(10881.7305, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10881.7294921875
tensor(10881.7305, grad_fn=<NegBackward0>) tensor(10881.7295, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10881.7294921875
tensor(10881.7295, grad_fn=<NegBackward0>) tensor(10881.7295, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10881.7294921875
tensor(10881.7295, grad_fn=<NegBackward0>) tensor(10881.7295, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10881.73046875
tensor(10881.7295, grad_fn=<NegBackward0>) tensor(10881.7305, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -10881.728515625
tensor(10881.7295, grad_fn=<NegBackward0>) tensor(10881.7285, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10881.7294921875
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7295, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -10881.73046875
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7305, grad_fn=<NegBackward0>)
2
Iteration 3100: Loss = -10881.7294921875
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7295, grad_fn=<NegBackward0>)
3
Iteration 3200: Loss = -10881.73046875
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7305, grad_fn=<NegBackward0>)
4
Iteration 3300: Loss = -10881.728515625
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7285, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10881.728515625
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7285, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10881.728515625
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7285, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10881.728515625
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7285, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10881.7412109375
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7412, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -10881.7314453125
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7314, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -10881.728515625
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7285, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10881.728515625
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7285, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10881.73046875
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7305, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -10881.728515625
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7285, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10881.728515625
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7285, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10881.728515625
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7285, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10881.728515625
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7285, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10881.732421875
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7324, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -10881.728515625
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7285, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10881.7294921875
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7295, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10881.728515625
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7285, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10881.7294921875
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7295, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10881.73828125
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7383, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -10881.728515625
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7285, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10881.73046875
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7305, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10881.728515625
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7285, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10881.7275390625
tensor(10881.7285, grad_fn=<NegBackward0>) tensor(10881.7275, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10881.7275390625
tensor(10881.7275, grad_fn=<NegBackward0>) tensor(10881.7275, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10881.7275390625
tensor(10881.7275, grad_fn=<NegBackward0>) tensor(10881.7275, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10881.7294921875
tensor(10881.7275, grad_fn=<NegBackward0>) tensor(10881.7295, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10881.728515625
tensor(10881.7275, grad_fn=<NegBackward0>) tensor(10881.7285, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -10881.7353515625
tensor(10881.7275, grad_fn=<NegBackward0>) tensor(10881.7354, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -10881.7294921875
tensor(10881.7275, grad_fn=<NegBackward0>) tensor(10881.7295, grad_fn=<NegBackward0>)
4
Iteration 6200: Loss = -10881.751953125
tensor(10881.7275, grad_fn=<NegBackward0>) tensor(10881.7520, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6200 due to no improvement.
pi: tensor([[0.8087, 0.1913],
        [0.2254, 0.7746]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5100, 0.4900], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2507, 0.0957],
         [0.5598, 0.1934]],

        [[0.5705, 0.1112],
         [0.5675, 0.5042]],

        [[0.7109, 0.1074],
         [0.6706, 0.6137]],

        [[0.6793, 0.1053],
         [0.5228, 0.5334]],

        [[0.6986, 0.0919],
         [0.5764, 0.5190]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448338943900694
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 11
Adjusted Rand Index: 0.6044118965160035
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
Global Adjusted Rand Index: 0.8387334700840575
Average Adjusted Rand Index: 0.8428175652377485
[0.8387334700840575, 0.8387334700840575] [0.8428175652377485, 0.8428175652377485] [10881.732421875, 10881.751953125]
-------------------------------------
This iteration is 74
True Objective function: Loss = -10995.971432153518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21116.310546875
inf tensor(21116.3105, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11067.3388671875
tensor(21116.3105, grad_fn=<NegBackward0>) tensor(11067.3389, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11067.0400390625
tensor(11067.3389, grad_fn=<NegBackward0>) tensor(11067.0400, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11066.8955078125
tensor(11067.0400, grad_fn=<NegBackward0>) tensor(11066.8955, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11066.740234375
tensor(11066.8955, grad_fn=<NegBackward0>) tensor(11066.7402, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11066.5517578125
tensor(11066.7402, grad_fn=<NegBackward0>) tensor(11066.5518, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11066.3681640625
tensor(11066.5518, grad_fn=<NegBackward0>) tensor(11066.3682, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11066.2353515625
tensor(11066.3682, grad_fn=<NegBackward0>) tensor(11066.2354, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11066.146484375
tensor(11066.2354, grad_fn=<NegBackward0>) tensor(11066.1465, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11066.07421875
tensor(11066.1465, grad_fn=<NegBackward0>) tensor(11066.0742, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11066.0126953125
tensor(11066.0742, grad_fn=<NegBackward0>) tensor(11066.0127, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11065.958984375
tensor(11066.0127, grad_fn=<NegBackward0>) tensor(11065.9590, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11065.9052734375
tensor(11065.9590, grad_fn=<NegBackward0>) tensor(11065.9053, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11065.853515625
tensor(11065.9053, grad_fn=<NegBackward0>) tensor(11065.8535, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11065.8154296875
tensor(11065.8535, grad_fn=<NegBackward0>) tensor(11065.8154, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11065.783203125
tensor(11065.8154, grad_fn=<NegBackward0>) tensor(11065.7832, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11065.7568359375
tensor(11065.7832, grad_fn=<NegBackward0>) tensor(11065.7568, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11065.7353515625
tensor(11065.7568, grad_fn=<NegBackward0>) tensor(11065.7354, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11065.71484375
tensor(11065.7354, grad_fn=<NegBackward0>) tensor(11065.7148, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11065.6982421875
tensor(11065.7148, grad_fn=<NegBackward0>) tensor(11065.6982, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11065.6875
tensor(11065.6982, grad_fn=<NegBackward0>) tensor(11065.6875, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11065.677734375
tensor(11065.6875, grad_fn=<NegBackward0>) tensor(11065.6777, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11065.6728515625
tensor(11065.6777, grad_fn=<NegBackward0>) tensor(11065.6729, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11065.66796875
tensor(11065.6729, grad_fn=<NegBackward0>) tensor(11065.6680, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11065.6640625
tensor(11065.6680, grad_fn=<NegBackward0>) tensor(11065.6641, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11065.6650390625
tensor(11065.6641, grad_fn=<NegBackward0>) tensor(11065.6650, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -11065.662109375
tensor(11065.6641, grad_fn=<NegBackward0>) tensor(11065.6621, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11065.66015625
tensor(11065.6621, grad_fn=<NegBackward0>) tensor(11065.6602, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11065.6611328125
tensor(11065.6602, grad_fn=<NegBackward0>) tensor(11065.6611, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11065.66015625
tensor(11065.6602, grad_fn=<NegBackward0>) tensor(11065.6602, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11065.658203125
tensor(11065.6602, grad_fn=<NegBackward0>) tensor(11065.6582, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11065.65625
tensor(11065.6582, grad_fn=<NegBackward0>) tensor(11065.6562, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11065.6572265625
tensor(11065.6562, grad_fn=<NegBackward0>) tensor(11065.6572, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11065.654296875
tensor(11065.6562, grad_fn=<NegBackward0>) tensor(11065.6543, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11065.650390625
tensor(11065.6543, grad_fn=<NegBackward0>) tensor(11065.6504, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11065.6484375
tensor(11065.6504, grad_fn=<NegBackward0>) tensor(11065.6484, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11065.64453125
tensor(11065.6484, grad_fn=<NegBackward0>) tensor(11065.6445, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11065.640625
tensor(11065.6445, grad_fn=<NegBackward0>) tensor(11065.6406, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11065.634765625
tensor(11065.6406, grad_fn=<NegBackward0>) tensor(11065.6348, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11065.626953125
tensor(11065.6348, grad_fn=<NegBackward0>) tensor(11065.6270, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11065.6181640625
tensor(11065.6270, grad_fn=<NegBackward0>) tensor(11065.6182, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11065.6044921875
tensor(11065.6182, grad_fn=<NegBackward0>) tensor(11065.6045, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11065.5869140625
tensor(11065.6045, grad_fn=<NegBackward0>) tensor(11065.5869, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11065.564453125
tensor(11065.5869, grad_fn=<NegBackward0>) tensor(11065.5645, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11065.529296875
tensor(11065.5645, grad_fn=<NegBackward0>) tensor(11065.5293, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11065.478515625
tensor(11065.5293, grad_fn=<NegBackward0>) tensor(11065.4785, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11065.4345703125
tensor(11065.4785, grad_fn=<NegBackward0>) tensor(11065.4346, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11065.4111328125
tensor(11065.4346, grad_fn=<NegBackward0>) tensor(11065.4111, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11065.3935546875
tensor(11065.4111, grad_fn=<NegBackward0>) tensor(11065.3936, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11065.3828125
tensor(11065.3936, grad_fn=<NegBackward0>) tensor(11065.3828, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11065.3720703125
tensor(11065.3828, grad_fn=<NegBackward0>) tensor(11065.3721, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11065.3662109375
tensor(11065.3721, grad_fn=<NegBackward0>) tensor(11065.3662, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11065.3603515625
tensor(11065.3662, grad_fn=<NegBackward0>) tensor(11065.3604, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11065.3564453125
tensor(11065.3604, grad_fn=<NegBackward0>) tensor(11065.3564, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11065.3525390625
tensor(11065.3564, grad_fn=<NegBackward0>) tensor(11065.3525, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11065.34765625
tensor(11065.3525, grad_fn=<NegBackward0>) tensor(11065.3477, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11065.34375
tensor(11065.3477, grad_fn=<NegBackward0>) tensor(11065.3438, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11065.3291015625
tensor(11065.3438, grad_fn=<NegBackward0>) tensor(11065.3291, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11064.9521484375
tensor(11065.3291, grad_fn=<NegBackward0>) tensor(11064.9521, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10943.9853515625
tensor(11064.9521, grad_fn=<NegBackward0>) tensor(10943.9854, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10943.21875
tensor(10943.9854, grad_fn=<NegBackward0>) tensor(10943.2188, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10943.1162109375
tensor(10943.2188, grad_fn=<NegBackward0>) tensor(10943.1162, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10942.7744140625
tensor(10943.1162, grad_fn=<NegBackward0>) tensor(10942.7744, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10941.087890625
tensor(10942.7744, grad_fn=<NegBackward0>) tensor(10941.0879, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10940.8974609375
tensor(10941.0879, grad_fn=<NegBackward0>) tensor(10940.8975, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10940.748046875
tensor(10940.8975, grad_fn=<NegBackward0>) tensor(10940.7480, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10940.7265625
tensor(10940.7480, grad_fn=<NegBackward0>) tensor(10940.7266, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10940.62890625
tensor(10940.7266, grad_fn=<NegBackward0>) tensor(10940.6289, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10940.552734375
tensor(10940.6289, grad_fn=<NegBackward0>) tensor(10940.5527, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10940.376953125
tensor(10940.5527, grad_fn=<NegBackward0>) tensor(10940.3770, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10940.3427734375
tensor(10940.3770, grad_fn=<NegBackward0>) tensor(10940.3428, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10940.21875
tensor(10940.3428, grad_fn=<NegBackward0>) tensor(10940.2188, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10940.1875
tensor(10940.2188, grad_fn=<NegBackward0>) tensor(10940.1875, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10939.951171875
tensor(10940.1875, grad_fn=<NegBackward0>) tensor(10939.9512, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10939.94921875
tensor(10939.9512, grad_fn=<NegBackward0>) tensor(10939.9492, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10939.943359375
tensor(10939.9492, grad_fn=<NegBackward0>) tensor(10939.9434, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10939.92578125
tensor(10939.9434, grad_fn=<NegBackward0>) tensor(10939.9258, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10939.921875
tensor(10939.9258, grad_fn=<NegBackward0>) tensor(10939.9219, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10939.91796875
tensor(10939.9219, grad_fn=<NegBackward0>) tensor(10939.9180, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10939.865234375
tensor(10939.9180, grad_fn=<NegBackward0>) tensor(10939.8652, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10937.3125
tensor(10939.8652, grad_fn=<NegBackward0>) tensor(10937.3125, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10937.1357421875
tensor(10937.3125, grad_fn=<NegBackward0>) tensor(10937.1357, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10937.16015625
tensor(10937.1357, grad_fn=<NegBackward0>) tensor(10937.1602, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10937.1318359375
tensor(10937.1357, grad_fn=<NegBackward0>) tensor(10937.1318, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10937.12890625
tensor(10937.1318, grad_fn=<NegBackward0>) tensor(10937.1289, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10937.072265625
tensor(10937.1289, grad_fn=<NegBackward0>) tensor(10937.0723, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10937.0712890625
tensor(10937.0723, grad_fn=<NegBackward0>) tensor(10937.0713, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10937.0712890625
tensor(10937.0713, grad_fn=<NegBackward0>) tensor(10937.0713, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10937.0771484375
tensor(10937.0713, grad_fn=<NegBackward0>) tensor(10937.0771, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10937.0703125
tensor(10937.0713, grad_fn=<NegBackward0>) tensor(10937.0703, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10937.072265625
tensor(10937.0703, grad_fn=<NegBackward0>) tensor(10937.0723, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10937.0751953125
tensor(10937.0703, grad_fn=<NegBackward0>) tensor(10937.0752, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -10937.083984375
tensor(10937.0703, grad_fn=<NegBackward0>) tensor(10937.0840, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -10937.1123046875
tensor(10937.0703, grad_fn=<NegBackward0>) tensor(10937.1123, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -10937.076171875
tensor(10937.0703, grad_fn=<NegBackward0>) tensor(10937.0762, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9400 due to no improvement.
pi: tensor([[0.7382, 0.2618],
        [0.2133, 0.7867]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5508, 0.4492], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2019, 0.1048],
         [0.5544, 0.2508]],

        [[0.6819, 0.0995],
         [0.5538, 0.6608]],

        [[0.6473, 0.1080],
         [0.6976, 0.6855]],

        [[0.5380, 0.1102],
         [0.7253, 0.5686]],

        [[0.5826, 0.0912],
         [0.5747, 0.6961]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369696969696969
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.7369480537608971
time is 3
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080650161849864
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824062740165256
Global Adjusted Rand Index: 0.8168494591297903
Average Adjusted Rand Index: 0.8170379349054075
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21178.275390625
inf tensor(21178.2754, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11067.6953125
tensor(21178.2754, grad_fn=<NegBackward0>) tensor(11067.6953, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11067.24609375
tensor(11067.6953, grad_fn=<NegBackward0>) tensor(11067.2461, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11067.0517578125
tensor(11067.2461, grad_fn=<NegBackward0>) tensor(11067.0518, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11066.853515625
tensor(11067.0518, grad_fn=<NegBackward0>) tensor(11066.8535, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11066.64453125
tensor(11066.8535, grad_fn=<NegBackward0>) tensor(11066.6445, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11066.458984375
tensor(11066.6445, grad_fn=<NegBackward0>) tensor(11066.4590, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11066.3173828125
tensor(11066.4590, grad_fn=<NegBackward0>) tensor(11066.3174, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11066.2119140625
tensor(11066.3174, grad_fn=<NegBackward0>) tensor(11066.2119, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11066.12109375
tensor(11066.2119, grad_fn=<NegBackward0>) tensor(11066.1211, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11066.0439453125
tensor(11066.1211, grad_fn=<NegBackward0>) tensor(11066.0439, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11065.9755859375
tensor(11066.0439, grad_fn=<NegBackward0>) tensor(11065.9756, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11065.916015625
tensor(11065.9756, grad_fn=<NegBackward0>) tensor(11065.9160, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11065.861328125
tensor(11065.9160, grad_fn=<NegBackward0>) tensor(11065.8613, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11065.8173828125
tensor(11065.8613, grad_fn=<NegBackward0>) tensor(11065.8174, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11065.7841796875
tensor(11065.8174, grad_fn=<NegBackward0>) tensor(11065.7842, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11065.755859375
tensor(11065.7842, grad_fn=<NegBackward0>) tensor(11065.7559, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11065.7314453125
tensor(11065.7559, grad_fn=<NegBackward0>) tensor(11065.7314, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11065.7119140625
tensor(11065.7314, grad_fn=<NegBackward0>) tensor(11065.7119, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11065.6953125
tensor(11065.7119, grad_fn=<NegBackward0>) tensor(11065.6953, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11065.681640625
tensor(11065.6953, grad_fn=<NegBackward0>) tensor(11065.6816, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11065.6728515625
tensor(11065.6816, grad_fn=<NegBackward0>) tensor(11065.6729, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11065.6650390625
tensor(11065.6729, grad_fn=<NegBackward0>) tensor(11065.6650, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11065.6611328125
tensor(11065.6650, grad_fn=<NegBackward0>) tensor(11065.6611, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11065.6572265625
tensor(11065.6611, grad_fn=<NegBackward0>) tensor(11065.6572, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11065.654296875
tensor(11065.6572, grad_fn=<NegBackward0>) tensor(11065.6543, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11065.65234375
tensor(11065.6543, grad_fn=<NegBackward0>) tensor(11065.6523, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11065.650390625
tensor(11065.6523, grad_fn=<NegBackward0>) tensor(11065.6504, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11065.6474609375
tensor(11065.6504, grad_fn=<NegBackward0>) tensor(11065.6475, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11065.6455078125
tensor(11065.6475, grad_fn=<NegBackward0>) tensor(11065.6455, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11065.642578125
tensor(11065.6455, grad_fn=<NegBackward0>) tensor(11065.6426, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11065.6416015625
tensor(11065.6426, grad_fn=<NegBackward0>) tensor(11065.6416, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11065.638671875
tensor(11065.6416, grad_fn=<NegBackward0>) tensor(11065.6387, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11065.6337890625
tensor(11065.6387, grad_fn=<NegBackward0>) tensor(11065.6338, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11065.630859375
tensor(11065.6338, grad_fn=<NegBackward0>) tensor(11065.6309, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11065.6259765625
tensor(11065.6309, grad_fn=<NegBackward0>) tensor(11065.6260, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11065.62109375
tensor(11065.6260, grad_fn=<NegBackward0>) tensor(11065.6211, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11065.615234375
tensor(11065.6211, grad_fn=<NegBackward0>) tensor(11065.6152, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11065.6083984375
tensor(11065.6152, grad_fn=<NegBackward0>) tensor(11065.6084, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11065.6005859375
tensor(11065.6084, grad_fn=<NegBackward0>) tensor(11065.6006, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11065.5908203125
tensor(11065.6006, grad_fn=<NegBackward0>) tensor(11065.5908, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11065.5791015625
tensor(11065.5908, grad_fn=<NegBackward0>) tensor(11065.5791, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11065.5625
tensor(11065.5791, grad_fn=<NegBackward0>) tensor(11065.5625, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11065.544921875
tensor(11065.5625, grad_fn=<NegBackward0>) tensor(11065.5449, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11065.5166015625
tensor(11065.5449, grad_fn=<NegBackward0>) tensor(11065.5166, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11065.4853515625
tensor(11065.5166, grad_fn=<NegBackward0>) tensor(11065.4854, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11065.4541015625
tensor(11065.4854, grad_fn=<NegBackward0>) tensor(11065.4541, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11065.4326171875
tensor(11065.4541, grad_fn=<NegBackward0>) tensor(11065.4326, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11065.416015625
tensor(11065.4326, grad_fn=<NegBackward0>) tensor(11065.4160, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11065.404296875
tensor(11065.4160, grad_fn=<NegBackward0>) tensor(11065.4043, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11065.39453125
tensor(11065.4043, grad_fn=<NegBackward0>) tensor(11065.3945, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11065.384765625
tensor(11065.3945, grad_fn=<NegBackward0>) tensor(11065.3848, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11065.37890625
tensor(11065.3848, grad_fn=<NegBackward0>) tensor(11065.3789, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11065.3720703125
tensor(11065.3789, grad_fn=<NegBackward0>) tensor(11065.3721, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11065.3681640625
tensor(11065.3721, grad_fn=<NegBackward0>) tensor(11065.3682, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11065.3642578125
tensor(11065.3682, grad_fn=<NegBackward0>) tensor(11065.3643, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11065.359375
tensor(11065.3643, grad_fn=<NegBackward0>) tensor(11065.3594, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11065.35546875
tensor(11065.3594, grad_fn=<NegBackward0>) tensor(11065.3555, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11065.3525390625
tensor(11065.3555, grad_fn=<NegBackward0>) tensor(11065.3525, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11065.3505859375
tensor(11065.3525, grad_fn=<NegBackward0>) tensor(11065.3506, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11065.3486328125
tensor(11065.3506, grad_fn=<NegBackward0>) tensor(11065.3486, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11065.3447265625
tensor(11065.3486, grad_fn=<NegBackward0>) tensor(11065.3447, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11065.341796875
tensor(11065.3447, grad_fn=<NegBackward0>) tensor(11065.3418, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11065.3359375
tensor(11065.3418, grad_fn=<NegBackward0>) tensor(11065.3359, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11065.2841796875
tensor(11065.3359, grad_fn=<NegBackward0>) tensor(11065.2842, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10972.693359375
tensor(11065.2842, grad_fn=<NegBackward0>) tensor(10972.6934, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10952.73828125
tensor(10972.6934, grad_fn=<NegBackward0>) tensor(10952.7383, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10947.2265625
tensor(10952.7383, grad_fn=<NegBackward0>) tensor(10947.2266, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10946.9052734375
tensor(10947.2266, grad_fn=<NegBackward0>) tensor(10946.9053, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10946.828125
tensor(10946.9053, grad_fn=<NegBackward0>) tensor(10946.8281, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10940.7841796875
tensor(10946.8281, grad_fn=<NegBackward0>) tensor(10940.7842, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10940.7578125
tensor(10940.7842, grad_fn=<NegBackward0>) tensor(10940.7578, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10940.720703125
tensor(10940.7578, grad_fn=<NegBackward0>) tensor(10940.7207, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10940.6982421875
tensor(10940.7207, grad_fn=<NegBackward0>) tensor(10940.6982, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10940.5908203125
tensor(10940.6982, grad_fn=<NegBackward0>) tensor(10940.5908, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10940.5693359375
tensor(10940.5908, grad_fn=<NegBackward0>) tensor(10940.5693, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10940.5986328125
tensor(10940.5693, grad_fn=<NegBackward0>) tensor(10940.5986, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10940.556640625
tensor(10940.5693, grad_fn=<NegBackward0>) tensor(10940.5566, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10940.5380859375
tensor(10940.5566, grad_fn=<NegBackward0>) tensor(10940.5381, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10940.53125
tensor(10940.5381, grad_fn=<NegBackward0>) tensor(10940.5312, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10940.3505859375
tensor(10940.5312, grad_fn=<NegBackward0>) tensor(10940.3506, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10940.1435546875
tensor(10940.3506, grad_fn=<NegBackward0>) tensor(10940.1436, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10940.0380859375
tensor(10940.1436, grad_fn=<NegBackward0>) tensor(10940.0381, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10940.1474609375
tensor(10940.0381, grad_fn=<NegBackward0>) tensor(10940.1475, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10940.029296875
tensor(10940.0381, grad_fn=<NegBackward0>) tensor(10940.0293, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10940.130859375
tensor(10940.0293, grad_fn=<NegBackward0>) tensor(10940.1309, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -10940.0107421875
tensor(10940.0293, grad_fn=<NegBackward0>) tensor(10940.0107, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10940.0263671875
tensor(10940.0107, grad_fn=<NegBackward0>) tensor(10940.0264, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -10939.9990234375
tensor(10940.0107, grad_fn=<NegBackward0>) tensor(10939.9990, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10939.998046875
tensor(10939.9990, grad_fn=<NegBackward0>) tensor(10939.9980, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10939.994140625
tensor(10939.9980, grad_fn=<NegBackward0>) tensor(10939.9941, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10939.9931640625
tensor(10939.9941, grad_fn=<NegBackward0>) tensor(10939.9932, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10939.9921875
tensor(10939.9932, grad_fn=<NegBackward0>) tensor(10939.9922, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10939.9931640625
tensor(10939.9922, grad_fn=<NegBackward0>) tensor(10939.9932, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -10940.0029296875
tensor(10939.9922, grad_fn=<NegBackward0>) tensor(10940.0029, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -10937.3212890625
tensor(10939.9922, grad_fn=<NegBackward0>) tensor(10937.3213, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10937.298828125
tensor(10937.3213, grad_fn=<NegBackward0>) tensor(10937.2988, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10937.296875
tensor(10937.2988, grad_fn=<NegBackward0>) tensor(10937.2969, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10937.2939453125
tensor(10937.2969, grad_fn=<NegBackward0>) tensor(10937.2939, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10937.29296875
tensor(10937.2939, grad_fn=<NegBackward0>) tensor(10937.2930, grad_fn=<NegBackward0>)
pi: tensor([[0.7888, 0.2112],
        [0.2614, 0.7386]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4482, 0.5518], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2506, 0.1049],
         [0.6073, 0.2018]],

        [[0.6239, 0.0995],
         [0.5292, 0.5051]],

        [[0.5788, 0.1080],
         [0.6197, 0.6772]],

        [[0.6842, 0.1103],
         [0.5219, 0.5933]],

        [[0.5631, 0.0912],
         [0.6502, 0.6362]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7721298052059989
time is 1
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 7
Adjusted Rand Index: 0.7369480537608971
time is 3
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080650161849864
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824062740165256
Global Adjusted Rand Index: 0.8241119837615949
Average Adjusted Rand Index: 0.8240699565526679
[0.8168494591297903, 0.8241119837615949] [0.8170379349054075, 0.8240699565526679] [10937.076171875, 10937.29296875]
-------------------------------------
This iteration is 75
True Objective function: Loss = -11000.944191022678
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22525.29296875
inf tensor(22525.2930, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11111.0830078125
tensor(22525.2930, grad_fn=<NegBackward0>) tensor(11111.0830, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11109.9951171875
tensor(11111.0830, grad_fn=<NegBackward0>) tensor(11109.9951, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11109.708984375
tensor(11109.9951, grad_fn=<NegBackward0>) tensor(11109.7090, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11109.615234375
tensor(11109.7090, grad_fn=<NegBackward0>) tensor(11109.6152, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11109.5703125
tensor(11109.6152, grad_fn=<NegBackward0>) tensor(11109.5703, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11109.5400390625
tensor(11109.5703, grad_fn=<NegBackward0>) tensor(11109.5400, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11109.51953125
tensor(11109.5400, grad_fn=<NegBackward0>) tensor(11109.5195, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11109.501953125
tensor(11109.5195, grad_fn=<NegBackward0>) tensor(11109.5020, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11109.4892578125
tensor(11109.5020, grad_fn=<NegBackward0>) tensor(11109.4893, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11109.478515625
tensor(11109.4893, grad_fn=<NegBackward0>) tensor(11109.4785, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11109.466796875
tensor(11109.4785, grad_fn=<NegBackward0>) tensor(11109.4668, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11109.4599609375
tensor(11109.4668, grad_fn=<NegBackward0>) tensor(11109.4600, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11109.451171875
tensor(11109.4600, grad_fn=<NegBackward0>) tensor(11109.4512, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11109.4453125
tensor(11109.4512, grad_fn=<NegBackward0>) tensor(11109.4453, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11109.439453125
tensor(11109.4453, grad_fn=<NegBackward0>) tensor(11109.4395, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11109.43359375
tensor(11109.4395, grad_fn=<NegBackward0>) tensor(11109.4336, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11109.4287109375
tensor(11109.4336, grad_fn=<NegBackward0>) tensor(11109.4287, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11109.4228515625
tensor(11109.4287, grad_fn=<NegBackward0>) tensor(11109.4229, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11109.41796875
tensor(11109.4229, grad_fn=<NegBackward0>) tensor(11109.4180, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11109.412109375
tensor(11109.4180, grad_fn=<NegBackward0>) tensor(11109.4121, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11109.408203125
tensor(11109.4121, grad_fn=<NegBackward0>) tensor(11109.4082, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11109.4033203125
tensor(11109.4082, grad_fn=<NegBackward0>) tensor(11109.4033, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11109.3984375
tensor(11109.4033, grad_fn=<NegBackward0>) tensor(11109.3984, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11109.3916015625
tensor(11109.3984, grad_fn=<NegBackward0>) tensor(11109.3916, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11109.3837890625
tensor(11109.3916, grad_fn=<NegBackward0>) tensor(11109.3838, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11109.373046875
tensor(11109.3838, grad_fn=<NegBackward0>) tensor(11109.3730, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11109.353515625
tensor(11109.3730, grad_fn=<NegBackward0>) tensor(11109.3535, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11109.30859375
tensor(11109.3535, grad_fn=<NegBackward0>) tensor(11109.3086, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11109.1103515625
tensor(11109.3086, grad_fn=<NegBackward0>) tensor(11109.1104, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11108.8759765625
tensor(11109.1104, grad_fn=<NegBackward0>) tensor(11108.8760, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11108.8076171875
tensor(11108.8760, grad_fn=<NegBackward0>) tensor(11108.8076, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11108.7587890625
tensor(11108.8076, grad_fn=<NegBackward0>) tensor(11108.7588, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11108.720703125
tensor(11108.7588, grad_fn=<NegBackward0>) tensor(11108.7207, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11108.6865234375
tensor(11108.7207, grad_fn=<NegBackward0>) tensor(11108.6865, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11108.6552734375
tensor(11108.6865, grad_fn=<NegBackward0>) tensor(11108.6553, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11108.623046875
tensor(11108.6553, grad_fn=<NegBackward0>) tensor(11108.6230, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11108.5771484375
tensor(11108.6230, grad_fn=<NegBackward0>) tensor(11108.5771, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11108.4951171875
tensor(11108.5771, grad_fn=<NegBackward0>) tensor(11108.4951, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11108.2998046875
tensor(11108.4951, grad_fn=<NegBackward0>) tensor(11108.2998, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11100.2919921875
tensor(11108.2998, grad_fn=<NegBackward0>) tensor(11100.2920, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11099.654296875
tensor(11100.2920, grad_fn=<NegBackward0>) tensor(11099.6543, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11099.5947265625
tensor(11099.6543, grad_fn=<NegBackward0>) tensor(11099.5947, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11099.5263671875
tensor(11099.5947, grad_fn=<NegBackward0>) tensor(11099.5264, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11099.484375
tensor(11099.5264, grad_fn=<NegBackward0>) tensor(11099.4844, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11099.474609375
tensor(11099.4844, grad_fn=<NegBackward0>) tensor(11099.4746, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11099.46484375
tensor(11099.4746, grad_fn=<NegBackward0>) tensor(11099.4648, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11099.455078125
tensor(11099.4648, grad_fn=<NegBackward0>) tensor(11099.4551, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11099.4462890625
tensor(11099.4551, grad_fn=<NegBackward0>) tensor(11099.4463, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11099.421875
tensor(11099.4463, grad_fn=<NegBackward0>) tensor(11099.4219, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11099.416015625
tensor(11099.4219, grad_fn=<NegBackward0>) tensor(11099.4160, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11099.4140625
tensor(11099.4160, grad_fn=<NegBackward0>) tensor(11099.4141, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11099.412109375
tensor(11099.4141, grad_fn=<NegBackward0>) tensor(11099.4121, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11099.408203125
tensor(11099.4121, grad_fn=<NegBackward0>) tensor(11099.4082, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11099.408203125
tensor(11099.4082, grad_fn=<NegBackward0>) tensor(11099.4082, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11099.4052734375
tensor(11099.4082, grad_fn=<NegBackward0>) tensor(11099.4053, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11099.404296875
tensor(11099.4053, grad_fn=<NegBackward0>) tensor(11099.4043, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11099.40234375
tensor(11099.4043, grad_fn=<NegBackward0>) tensor(11099.4023, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11099.40234375
tensor(11099.4023, grad_fn=<NegBackward0>) tensor(11099.4023, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11099.4013671875
tensor(11099.4023, grad_fn=<NegBackward0>) tensor(11099.4014, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11099.400390625
tensor(11099.4014, grad_fn=<NegBackward0>) tensor(11099.4004, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11099.4013671875
tensor(11099.4004, grad_fn=<NegBackward0>) tensor(11099.4014, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11099.3984375
tensor(11099.4004, grad_fn=<NegBackward0>) tensor(11099.3984, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11099.3994140625
tensor(11099.3984, grad_fn=<NegBackward0>) tensor(11099.3994, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11099.3974609375
tensor(11099.3984, grad_fn=<NegBackward0>) tensor(11099.3975, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11099.3974609375
tensor(11099.3975, grad_fn=<NegBackward0>) tensor(11099.3975, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11099.396484375
tensor(11099.3975, grad_fn=<NegBackward0>) tensor(11099.3965, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11099.3955078125
tensor(11099.3965, grad_fn=<NegBackward0>) tensor(11099.3955, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11099.396484375
tensor(11099.3955, grad_fn=<NegBackward0>) tensor(11099.3965, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11099.3955078125
tensor(11099.3955, grad_fn=<NegBackward0>) tensor(11099.3955, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11099.3955078125
tensor(11099.3955, grad_fn=<NegBackward0>) tensor(11099.3955, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11099.396484375
tensor(11099.3955, grad_fn=<NegBackward0>) tensor(11099.3965, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11099.3955078125
tensor(11099.3955, grad_fn=<NegBackward0>) tensor(11099.3955, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11099.3935546875
tensor(11099.3955, grad_fn=<NegBackward0>) tensor(11099.3936, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11099.39453125
tensor(11099.3936, grad_fn=<NegBackward0>) tensor(11099.3945, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11099.3935546875
tensor(11099.3936, grad_fn=<NegBackward0>) tensor(11099.3936, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11099.392578125
tensor(11099.3936, grad_fn=<NegBackward0>) tensor(11099.3926, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11099.4228515625
tensor(11099.3926, grad_fn=<NegBackward0>) tensor(11099.4229, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11099.392578125
tensor(11099.3926, grad_fn=<NegBackward0>) tensor(11099.3926, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11099.3984375
tensor(11099.3926, grad_fn=<NegBackward0>) tensor(11099.3984, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11099.39453125
tensor(11099.3926, grad_fn=<NegBackward0>) tensor(11099.3945, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11099.3935546875
tensor(11099.3926, grad_fn=<NegBackward0>) tensor(11099.3936, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -11099.39453125
tensor(11099.3926, grad_fn=<NegBackward0>) tensor(11099.3945, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -11099.3916015625
tensor(11099.3926, grad_fn=<NegBackward0>) tensor(11099.3916, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11099.3916015625
tensor(11099.3916, grad_fn=<NegBackward0>) tensor(11099.3916, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11099.390625
tensor(11099.3916, grad_fn=<NegBackward0>) tensor(11099.3906, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11099.3916015625
tensor(11099.3906, grad_fn=<NegBackward0>) tensor(11099.3916, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11099.390625
tensor(11099.3906, grad_fn=<NegBackward0>) tensor(11099.3906, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11099.392578125
tensor(11099.3906, grad_fn=<NegBackward0>) tensor(11099.3926, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11099.3916015625
tensor(11099.3906, grad_fn=<NegBackward0>) tensor(11099.3916, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -11099.400390625
tensor(11099.3906, grad_fn=<NegBackward0>) tensor(11099.4004, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -11099.3935546875
tensor(11099.3906, grad_fn=<NegBackward0>) tensor(11099.3936, grad_fn=<NegBackward0>)
4
Iteration 9200: Loss = -11099.390625
tensor(11099.3906, grad_fn=<NegBackward0>) tensor(11099.3906, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11099.396484375
tensor(11099.3906, grad_fn=<NegBackward0>) tensor(11099.3965, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11099.3896484375
tensor(11099.3906, grad_fn=<NegBackward0>) tensor(11099.3896, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11099.3974609375
tensor(11099.3896, grad_fn=<NegBackward0>) tensor(11099.3975, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11099.3896484375
tensor(11099.3896, grad_fn=<NegBackward0>) tensor(11099.3896, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11099.3896484375
tensor(11099.3896, grad_fn=<NegBackward0>) tensor(11099.3896, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11099.392578125
tensor(11099.3896, grad_fn=<NegBackward0>) tensor(11099.3926, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11099.3896484375
tensor(11099.3896, grad_fn=<NegBackward0>) tensor(11099.3896, grad_fn=<NegBackward0>)
pi: tensor([[9.9995e-01, 4.7543e-05],
        [1.6099e-01, 8.3901e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8469, 0.1531], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1747, 0.1007],
         [0.6859, 0.2178]],

        [[0.6421, 0.1135],
         [0.6061, 0.5402]],

        [[0.6955, 0.1378],
         [0.6804, 0.6616]],

        [[0.5708, 0.1399],
         [0.7207, 0.6493]],

        [[0.5008, 0.1512],
         [0.7211, 0.6468]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 39
Adjusted Rand Index: 0.04403151370505218
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 36
Adjusted Rand Index: 0.07271429619122406
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.02814362246873215
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0028334070515487393
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: -0.002057056324302102
Global Adjusted Rand Index: 0.015651287651712618
Average Adjusted Rand Index: 0.029133156618451006
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23011.982421875
inf tensor(23011.9824, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11111.0869140625
tensor(23011.9824, grad_fn=<NegBackward0>) tensor(11111.0869, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11110.0283203125
tensor(11111.0869, grad_fn=<NegBackward0>) tensor(11110.0283, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11109.7900390625
tensor(11110.0283, grad_fn=<NegBackward0>) tensor(11109.7900, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11109.6806640625
tensor(11109.7900, grad_fn=<NegBackward0>) tensor(11109.6807, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11109.615234375
tensor(11109.6807, grad_fn=<NegBackward0>) tensor(11109.6152, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11109.5693359375
tensor(11109.6152, grad_fn=<NegBackward0>) tensor(11109.5693, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11109.5361328125
tensor(11109.5693, grad_fn=<NegBackward0>) tensor(11109.5361, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11109.5087890625
tensor(11109.5361, grad_fn=<NegBackward0>) tensor(11109.5088, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11109.4873046875
tensor(11109.5088, grad_fn=<NegBackward0>) tensor(11109.4873, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11109.4697265625
tensor(11109.4873, grad_fn=<NegBackward0>) tensor(11109.4697, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11109.4541015625
tensor(11109.4697, grad_fn=<NegBackward0>) tensor(11109.4541, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11109.439453125
tensor(11109.4541, grad_fn=<NegBackward0>) tensor(11109.4395, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11109.421875
tensor(11109.4395, grad_fn=<NegBackward0>) tensor(11109.4219, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11109.400390625
tensor(11109.4219, grad_fn=<NegBackward0>) tensor(11109.4004, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11109.369140625
tensor(11109.4004, grad_fn=<NegBackward0>) tensor(11109.3691, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11109.3017578125
tensor(11109.3691, grad_fn=<NegBackward0>) tensor(11109.3018, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11109.158203125
tensor(11109.3018, grad_fn=<NegBackward0>) tensor(11109.1582, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11109.0439453125
tensor(11109.1582, grad_fn=<NegBackward0>) tensor(11109.0439, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11108.9775390625
tensor(11109.0439, grad_fn=<NegBackward0>) tensor(11108.9775, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11108.931640625
tensor(11108.9775, grad_fn=<NegBackward0>) tensor(11108.9316, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11108.8955078125
tensor(11108.9316, grad_fn=<NegBackward0>) tensor(11108.8955, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11108.859375
tensor(11108.8955, grad_fn=<NegBackward0>) tensor(11108.8594, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11108.828125
tensor(11108.8594, grad_fn=<NegBackward0>) tensor(11108.8281, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11108.7939453125
tensor(11108.8281, grad_fn=<NegBackward0>) tensor(11108.7939, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11108.7607421875
tensor(11108.7939, grad_fn=<NegBackward0>) tensor(11108.7607, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11108.724609375
tensor(11108.7607, grad_fn=<NegBackward0>) tensor(11108.7246, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11108.6845703125
tensor(11108.7246, grad_fn=<NegBackward0>) tensor(11108.6846, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11108.634765625
tensor(11108.6846, grad_fn=<NegBackward0>) tensor(11108.6348, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11108.5673828125
tensor(11108.6348, grad_fn=<NegBackward0>) tensor(11108.5674, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11108.46484375
tensor(11108.5674, grad_fn=<NegBackward0>) tensor(11108.4648, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11108.21875
tensor(11108.4648, grad_fn=<NegBackward0>) tensor(11108.2188, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11100.1689453125
tensor(11108.2188, grad_fn=<NegBackward0>) tensor(11100.1689, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11099.63671875
tensor(11100.1689, grad_fn=<NegBackward0>) tensor(11099.6367, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11099.5576171875
tensor(11099.6367, grad_fn=<NegBackward0>) tensor(11099.5576, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11099.5166015625
tensor(11099.5576, grad_fn=<NegBackward0>) tensor(11099.5166, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11099.4892578125
tensor(11099.5166, grad_fn=<NegBackward0>) tensor(11099.4893, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11099.4794921875
tensor(11099.4893, grad_fn=<NegBackward0>) tensor(11099.4795, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11099.4697265625
tensor(11099.4795, grad_fn=<NegBackward0>) tensor(11099.4697, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11099.4638671875
tensor(11099.4697, grad_fn=<NegBackward0>) tensor(11099.4639, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11099.4453125
tensor(11099.4639, grad_fn=<NegBackward0>) tensor(11099.4453, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11099.4365234375
tensor(11099.4453, grad_fn=<NegBackward0>) tensor(11099.4365, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11099.431640625
tensor(11099.4365, grad_fn=<NegBackward0>) tensor(11099.4316, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11099.423828125
tensor(11099.4316, grad_fn=<NegBackward0>) tensor(11099.4238, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11099.4140625
tensor(11099.4238, grad_fn=<NegBackward0>) tensor(11099.4141, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11099.412109375
tensor(11099.4141, grad_fn=<NegBackward0>) tensor(11099.4121, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11099.41015625
tensor(11099.4121, grad_fn=<NegBackward0>) tensor(11099.4102, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11099.4091796875
tensor(11099.4102, grad_fn=<NegBackward0>) tensor(11099.4092, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11099.4052734375
tensor(11099.4092, grad_fn=<NegBackward0>) tensor(11099.4053, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11099.40625
tensor(11099.4053, grad_fn=<NegBackward0>) tensor(11099.4062, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11099.40625
tensor(11099.4053, grad_fn=<NegBackward0>) tensor(11099.4062, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11099.4052734375
tensor(11099.4053, grad_fn=<NegBackward0>) tensor(11099.4053, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11099.4033203125
tensor(11099.4053, grad_fn=<NegBackward0>) tensor(11099.4033, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11099.40234375
tensor(11099.4033, grad_fn=<NegBackward0>) tensor(11099.4023, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11099.3994140625
tensor(11099.4023, grad_fn=<NegBackward0>) tensor(11099.3994, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11099.4013671875
tensor(11099.3994, grad_fn=<NegBackward0>) tensor(11099.4014, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11099.4013671875
tensor(11099.3994, grad_fn=<NegBackward0>) tensor(11099.4014, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11099.3994140625
tensor(11099.3994, grad_fn=<NegBackward0>) tensor(11099.3994, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11099.3984375
tensor(11099.3994, grad_fn=<NegBackward0>) tensor(11099.3984, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11099.3984375
tensor(11099.3984, grad_fn=<NegBackward0>) tensor(11099.3984, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11099.396484375
tensor(11099.3984, grad_fn=<NegBackward0>) tensor(11099.3965, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11099.396484375
tensor(11099.3965, grad_fn=<NegBackward0>) tensor(11099.3965, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11099.3955078125
tensor(11099.3965, grad_fn=<NegBackward0>) tensor(11099.3955, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11099.3955078125
tensor(11099.3955, grad_fn=<NegBackward0>) tensor(11099.3955, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11099.3955078125
tensor(11099.3955, grad_fn=<NegBackward0>) tensor(11099.3955, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11099.400390625
tensor(11099.3955, grad_fn=<NegBackward0>) tensor(11099.4004, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11099.3955078125
tensor(11099.3955, grad_fn=<NegBackward0>) tensor(11099.3955, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11099.39453125
tensor(11099.3955, grad_fn=<NegBackward0>) tensor(11099.3945, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11099.3955078125
tensor(11099.3945, grad_fn=<NegBackward0>) tensor(11099.3955, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11099.39453125
tensor(11099.3945, grad_fn=<NegBackward0>) tensor(11099.3945, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11099.39453125
tensor(11099.3945, grad_fn=<NegBackward0>) tensor(11099.3945, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11099.39453125
tensor(11099.3945, grad_fn=<NegBackward0>) tensor(11099.3945, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11099.392578125
tensor(11099.3945, grad_fn=<NegBackward0>) tensor(11099.3926, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11099.392578125
tensor(11099.3926, grad_fn=<NegBackward0>) tensor(11099.3926, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11099.3916015625
tensor(11099.3926, grad_fn=<NegBackward0>) tensor(11099.3916, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11099.392578125
tensor(11099.3916, grad_fn=<NegBackward0>) tensor(11099.3926, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11099.3935546875
tensor(11099.3916, grad_fn=<NegBackward0>) tensor(11099.3936, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11099.392578125
tensor(11099.3916, grad_fn=<NegBackward0>) tensor(11099.3926, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11099.392578125
tensor(11099.3916, grad_fn=<NegBackward0>) tensor(11099.3926, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -11099.392578125
tensor(11099.3916, grad_fn=<NegBackward0>) tensor(11099.3926, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7900 due to no improvement.
pi: tensor([[8.3843e-01, 1.6157e-01],
        [9.4981e-05, 9.9990e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1537, 0.8463], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2182, 0.1007],
         [0.6264, 0.1750]],

        [[0.7162, 0.1135],
         [0.5929, 0.6847]],

        [[0.5861, 0.1378],
         [0.5267, 0.6175]],

        [[0.6924, 0.1398],
         [0.6276, 0.5054]],

        [[0.6641, 0.1511],
         [0.5598, 0.6174]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 61
Adjusted Rand Index: 0.04403151370505218
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 64
Adjusted Rand Index: 0.07271429619122406
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.02814362246873215
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0028334070515487393
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.002057056324302102
Global Adjusted Rand Index: 0.015651287651712618
Average Adjusted Rand Index: 0.029133156618451006
[0.015651287651712618, 0.015651287651712618] [0.029133156618451006, 0.029133156618451006] [11099.48828125, 11099.392578125]
-------------------------------------
This iteration is 76
True Objective function: Loss = -10952.922316694872
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19414.474609375
inf tensor(19414.4746, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11029.9853515625
tensor(19414.4746, grad_fn=<NegBackward0>) tensor(11029.9854, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11029.8447265625
tensor(11029.9854, grad_fn=<NegBackward0>) tensor(11029.8447, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11029.7802734375
tensor(11029.8447, grad_fn=<NegBackward0>) tensor(11029.7803, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11029.7060546875
tensor(11029.7803, grad_fn=<NegBackward0>) tensor(11029.7061, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11029.5869140625
tensor(11029.7061, grad_fn=<NegBackward0>) tensor(11029.5869, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11029.41796875
tensor(11029.5869, grad_fn=<NegBackward0>) tensor(11029.4180, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11029.2724609375
tensor(11029.4180, grad_fn=<NegBackward0>) tensor(11029.2725, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11029.17578125
tensor(11029.2725, grad_fn=<NegBackward0>) tensor(11029.1758, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11029.0927734375
tensor(11029.1758, grad_fn=<NegBackward0>) tensor(11029.0928, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11029.029296875
tensor(11029.0928, grad_fn=<NegBackward0>) tensor(11029.0293, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11028.98828125
tensor(11029.0293, grad_fn=<NegBackward0>) tensor(11028.9883, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11028.9580078125
tensor(11028.9883, grad_fn=<NegBackward0>) tensor(11028.9580, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11028.9365234375
tensor(11028.9580, grad_fn=<NegBackward0>) tensor(11028.9365, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11028.91796875
tensor(11028.9365, grad_fn=<NegBackward0>) tensor(11028.9180, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11028.8994140625
tensor(11028.9180, grad_fn=<NegBackward0>) tensor(11028.8994, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11028.884765625
tensor(11028.8994, grad_fn=<NegBackward0>) tensor(11028.8848, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11028.8681640625
tensor(11028.8848, grad_fn=<NegBackward0>) tensor(11028.8682, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11028.8447265625
tensor(11028.8682, grad_fn=<NegBackward0>) tensor(11028.8447, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11028.818359375
tensor(11028.8447, grad_fn=<NegBackward0>) tensor(11028.8184, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11028.779296875
tensor(11028.8184, grad_fn=<NegBackward0>) tensor(11028.7793, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11028.73046875
tensor(11028.7793, grad_fn=<NegBackward0>) tensor(11028.7305, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11028.6826171875
tensor(11028.7305, grad_fn=<NegBackward0>) tensor(11028.6826, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11028.6298828125
tensor(11028.6826, grad_fn=<NegBackward0>) tensor(11028.6299, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11028.5498046875
tensor(11028.6299, grad_fn=<NegBackward0>) tensor(11028.5498, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11028.455078125
tensor(11028.5498, grad_fn=<NegBackward0>) tensor(11028.4551, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11028.408203125
tensor(11028.4551, grad_fn=<NegBackward0>) tensor(11028.4082, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11028.3837890625
tensor(11028.4082, grad_fn=<NegBackward0>) tensor(11028.3838, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11028.361328125
tensor(11028.3838, grad_fn=<NegBackward0>) tensor(11028.3613, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11028.341796875
tensor(11028.3613, grad_fn=<NegBackward0>) tensor(11028.3418, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11028.3212890625
tensor(11028.3418, grad_fn=<NegBackward0>) tensor(11028.3213, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11028.298828125
tensor(11028.3213, grad_fn=<NegBackward0>) tensor(11028.2988, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11028.27734375
tensor(11028.2988, grad_fn=<NegBackward0>) tensor(11028.2773, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11028.259765625
tensor(11028.2773, grad_fn=<NegBackward0>) tensor(11028.2598, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11028.2431640625
tensor(11028.2598, grad_fn=<NegBackward0>) tensor(11028.2432, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11028.2275390625
tensor(11028.2432, grad_fn=<NegBackward0>) tensor(11028.2275, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11028.216796875
tensor(11028.2275, grad_fn=<NegBackward0>) tensor(11028.2168, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11028.2080078125
tensor(11028.2168, grad_fn=<NegBackward0>) tensor(11028.2080, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11028.2001953125
tensor(11028.2080, grad_fn=<NegBackward0>) tensor(11028.2002, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11028.1953125
tensor(11028.2002, grad_fn=<NegBackward0>) tensor(11028.1953, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11028.19140625
tensor(11028.1953, grad_fn=<NegBackward0>) tensor(11028.1914, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11028.1875
tensor(11028.1914, grad_fn=<NegBackward0>) tensor(11028.1875, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11028.1845703125
tensor(11028.1875, grad_fn=<NegBackward0>) tensor(11028.1846, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11028.181640625
tensor(11028.1846, grad_fn=<NegBackward0>) tensor(11028.1816, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11028.1796875
tensor(11028.1816, grad_fn=<NegBackward0>) tensor(11028.1797, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11028.177734375
tensor(11028.1797, grad_fn=<NegBackward0>) tensor(11028.1777, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11028.177734375
tensor(11028.1777, grad_fn=<NegBackward0>) tensor(11028.1777, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11028.17578125
tensor(11028.1777, grad_fn=<NegBackward0>) tensor(11028.1758, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11028.17578125
tensor(11028.1758, grad_fn=<NegBackward0>) tensor(11028.1758, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11028.1728515625
tensor(11028.1758, grad_fn=<NegBackward0>) tensor(11028.1729, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11028.1728515625
tensor(11028.1729, grad_fn=<NegBackward0>) tensor(11028.1729, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11028.1708984375
tensor(11028.1729, grad_fn=<NegBackward0>) tensor(11028.1709, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11028.1708984375
tensor(11028.1709, grad_fn=<NegBackward0>) tensor(11028.1709, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11028.171875
tensor(11028.1709, grad_fn=<NegBackward0>) tensor(11028.1719, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11028.1708984375
tensor(11028.1709, grad_fn=<NegBackward0>) tensor(11028.1709, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11028.169921875
tensor(11028.1709, grad_fn=<NegBackward0>) tensor(11028.1699, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11028.169921875
tensor(11028.1699, grad_fn=<NegBackward0>) tensor(11028.1699, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11028.169921875
tensor(11028.1699, grad_fn=<NegBackward0>) tensor(11028.1699, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11028.1689453125
tensor(11028.1699, grad_fn=<NegBackward0>) tensor(11028.1689, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11028.1689453125
tensor(11028.1689, grad_fn=<NegBackward0>) tensor(11028.1689, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11028.1689453125
tensor(11028.1689, grad_fn=<NegBackward0>) tensor(11028.1689, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11028.1689453125
tensor(11028.1689, grad_fn=<NegBackward0>) tensor(11028.1689, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11028.1669921875
tensor(11028.1689, grad_fn=<NegBackward0>) tensor(11028.1670, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11028.1669921875
tensor(11028.1670, grad_fn=<NegBackward0>) tensor(11028.1670, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11028.1689453125
tensor(11028.1670, grad_fn=<NegBackward0>) tensor(11028.1689, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11028.1689453125
tensor(11028.1670, grad_fn=<NegBackward0>) tensor(11028.1689, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11028.1669921875
tensor(11028.1670, grad_fn=<NegBackward0>) tensor(11028.1670, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11028.16796875
tensor(11028.1670, grad_fn=<NegBackward0>) tensor(11028.1680, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11028.1669921875
tensor(11028.1670, grad_fn=<NegBackward0>) tensor(11028.1670, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11028.1669921875
tensor(11028.1670, grad_fn=<NegBackward0>) tensor(11028.1670, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11028.1669921875
tensor(11028.1670, grad_fn=<NegBackward0>) tensor(11028.1670, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11028.1669921875
tensor(11028.1670, grad_fn=<NegBackward0>) tensor(11028.1670, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11028.169921875
tensor(11028.1670, grad_fn=<NegBackward0>) tensor(11028.1699, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11028.1796875
tensor(11028.1670, grad_fn=<NegBackward0>) tensor(11028.1797, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11028.1669921875
tensor(11028.1670, grad_fn=<NegBackward0>) tensor(11028.1670, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11028.16796875
tensor(11028.1670, grad_fn=<NegBackward0>) tensor(11028.1680, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11028.166015625
tensor(11028.1670, grad_fn=<NegBackward0>) tensor(11028.1660, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11028.16796875
tensor(11028.1660, grad_fn=<NegBackward0>) tensor(11028.1680, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11028.1669921875
tensor(11028.1660, grad_fn=<NegBackward0>) tensor(11028.1670, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11028.16796875
tensor(11028.1660, grad_fn=<NegBackward0>) tensor(11028.1680, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11028.166015625
tensor(11028.1660, grad_fn=<NegBackward0>) tensor(11028.1660, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11028.1669921875
tensor(11028.1660, grad_fn=<NegBackward0>) tensor(11028.1670, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11028.166015625
tensor(11028.1660, grad_fn=<NegBackward0>) tensor(11028.1660, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11028.166015625
tensor(11028.1660, grad_fn=<NegBackward0>) tensor(11028.1660, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11028.228515625
tensor(11028.1660, grad_fn=<NegBackward0>) tensor(11028.2285, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11028.166015625
tensor(11028.1660, grad_fn=<NegBackward0>) tensor(11028.1660, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11028.1669921875
tensor(11028.1660, grad_fn=<NegBackward0>) tensor(11028.1670, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11028.169921875
tensor(11028.1660, grad_fn=<NegBackward0>) tensor(11028.1699, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11028.166015625
tensor(11028.1660, grad_fn=<NegBackward0>) tensor(11028.1660, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11028.166015625
tensor(11028.1660, grad_fn=<NegBackward0>) tensor(11028.1660, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11028.16796875
tensor(11028.1660, grad_fn=<NegBackward0>) tensor(11028.1680, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11028.1669921875
tensor(11028.1660, grad_fn=<NegBackward0>) tensor(11028.1670, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11028.1728515625
tensor(11028.1660, grad_fn=<NegBackward0>) tensor(11028.1729, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -11028.1669921875
tensor(11028.1660, grad_fn=<NegBackward0>) tensor(11028.1670, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -11028.1650390625
tensor(11028.1660, grad_fn=<NegBackward0>) tensor(11028.1650, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11028.16796875
tensor(11028.1650, grad_fn=<NegBackward0>) tensor(11028.1680, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11028.1669921875
tensor(11028.1650, grad_fn=<NegBackward0>) tensor(11028.1670, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11028.18359375
tensor(11028.1650, grad_fn=<NegBackward0>) tensor(11028.1836, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -11028.166015625
tensor(11028.1650, grad_fn=<NegBackward0>) tensor(11028.1660, grad_fn=<NegBackward0>)
4
Iteration 9900: Loss = -11028.1669921875
tensor(11028.1650, grad_fn=<NegBackward0>) tensor(11028.1670, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9900 due to no improvement.
pi: tensor([[9.9983e-01, 1.7399e-04],
        [2.3623e-03, 9.9764e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0168, 0.9832], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1714, 0.2023],
         [0.5278, 0.1615]],

        [[0.5795, 0.2224],
         [0.5248, 0.6145]],

        [[0.6208, 0.1954],
         [0.7259, 0.6320]],

        [[0.5531, 0.2265],
         [0.6382, 0.5812]],

        [[0.5892, 0.2079],
         [0.5880, 0.5496]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.007614171548597778
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: 0.00016129143044950168
Average Adjusted Rand Index: 0.0001805543060145954
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22678.279296875
inf tensor(22678.2793, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11030.7841796875
tensor(22678.2793, grad_fn=<NegBackward0>) tensor(11030.7842, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11030.0771484375
tensor(11030.7842, grad_fn=<NegBackward0>) tensor(11030.0771, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11029.904296875
tensor(11030.0771, grad_fn=<NegBackward0>) tensor(11029.9043, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11029.822265625
tensor(11029.9043, grad_fn=<NegBackward0>) tensor(11029.8223, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11029.76953125
tensor(11029.8223, grad_fn=<NegBackward0>) tensor(11029.7695, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11029.7177734375
tensor(11029.7695, grad_fn=<NegBackward0>) tensor(11029.7178, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11029.6630859375
tensor(11029.7178, grad_fn=<NegBackward0>) tensor(11029.6631, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11029.603515625
tensor(11029.6631, grad_fn=<NegBackward0>) tensor(11029.6035, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11029.5263671875
tensor(11029.6035, grad_fn=<NegBackward0>) tensor(11029.5264, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11029.4248046875
tensor(11029.5264, grad_fn=<NegBackward0>) tensor(11029.4248, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11029.3291015625
tensor(11029.4248, grad_fn=<NegBackward0>) tensor(11029.3291, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11029.255859375
tensor(11029.3291, grad_fn=<NegBackward0>) tensor(11029.2559, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11029.1923828125
tensor(11029.2559, grad_fn=<NegBackward0>) tensor(11029.1924, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11029.1328125
tensor(11029.1924, grad_fn=<NegBackward0>) tensor(11029.1328, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11029.0810546875
tensor(11029.1328, grad_fn=<NegBackward0>) tensor(11029.0811, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11029.037109375
tensor(11029.0811, grad_fn=<NegBackward0>) tensor(11029.0371, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11029.001953125
tensor(11029.0371, grad_fn=<NegBackward0>) tensor(11029.0020, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11028.9736328125
tensor(11029.0020, grad_fn=<NegBackward0>) tensor(11028.9736, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11028.9501953125
tensor(11028.9736, grad_fn=<NegBackward0>) tensor(11028.9502, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11028.9287109375
tensor(11028.9502, grad_fn=<NegBackward0>) tensor(11028.9287, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11028.912109375
tensor(11028.9287, grad_fn=<NegBackward0>) tensor(11028.9121, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11028.8955078125
tensor(11028.9121, grad_fn=<NegBackward0>) tensor(11028.8955, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11028.8828125
tensor(11028.8955, grad_fn=<NegBackward0>) tensor(11028.8828, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11028.8671875
tensor(11028.8828, grad_fn=<NegBackward0>) tensor(11028.8672, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11028.8525390625
tensor(11028.8672, grad_fn=<NegBackward0>) tensor(11028.8525, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11028.8349609375
tensor(11028.8525, grad_fn=<NegBackward0>) tensor(11028.8350, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11028.8134765625
tensor(11028.8350, grad_fn=<NegBackward0>) tensor(11028.8135, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11028.783203125
tensor(11028.8135, grad_fn=<NegBackward0>) tensor(11028.7832, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11028.7490234375
tensor(11028.7832, grad_fn=<NegBackward0>) tensor(11028.7490, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11028.7138671875
tensor(11028.7490, grad_fn=<NegBackward0>) tensor(11028.7139, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11028.6796875
tensor(11028.7139, grad_fn=<NegBackward0>) tensor(11028.6797, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11028.6337890625
tensor(11028.6797, grad_fn=<NegBackward0>) tensor(11028.6338, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11028.55078125
tensor(11028.6338, grad_fn=<NegBackward0>) tensor(11028.5508, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11028.45703125
tensor(11028.5508, grad_fn=<NegBackward0>) tensor(11028.4570, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11028.419921875
tensor(11028.4570, grad_fn=<NegBackward0>) tensor(11028.4199, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11028.392578125
tensor(11028.4199, grad_fn=<NegBackward0>) tensor(11028.3926, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11028.3701171875
tensor(11028.3926, grad_fn=<NegBackward0>) tensor(11028.3701, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11028.349609375
tensor(11028.3701, grad_fn=<NegBackward0>) tensor(11028.3496, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11028.328125
tensor(11028.3496, grad_fn=<NegBackward0>) tensor(11028.3281, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11028.3046875
tensor(11028.3281, grad_fn=<NegBackward0>) tensor(11028.3047, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11028.2822265625
tensor(11028.3047, grad_fn=<NegBackward0>) tensor(11028.2822, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11028.263671875
tensor(11028.2822, grad_fn=<NegBackward0>) tensor(11028.2637, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11028.2451171875
tensor(11028.2637, grad_fn=<NegBackward0>) tensor(11028.2451, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11028.2314453125
tensor(11028.2451, grad_fn=<NegBackward0>) tensor(11028.2314, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11028.2197265625
tensor(11028.2314, grad_fn=<NegBackward0>) tensor(11028.2197, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11028.2109375
tensor(11028.2197, grad_fn=<NegBackward0>) tensor(11028.2109, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11028.2021484375
tensor(11028.2109, grad_fn=<NegBackward0>) tensor(11028.2021, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11028.197265625
tensor(11028.2021, grad_fn=<NegBackward0>) tensor(11028.1973, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11028.1923828125
tensor(11028.1973, grad_fn=<NegBackward0>) tensor(11028.1924, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11028.1875
tensor(11028.1924, grad_fn=<NegBackward0>) tensor(11028.1875, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11028.1845703125
tensor(11028.1875, grad_fn=<NegBackward0>) tensor(11028.1846, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11028.18359375
tensor(11028.1846, grad_fn=<NegBackward0>) tensor(11028.1836, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11028.1796875
tensor(11028.1836, grad_fn=<NegBackward0>) tensor(11028.1797, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11028.1787109375
tensor(11028.1797, grad_fn=<NegBackward0>) tensor(11028.1787, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11028.17578125
tensor(11028.1787, grad_fn=<NegBackward0>) tensor(11028.1758, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11028.177734375
tensor(11028.1758, grad_fn=<NegBackward0>) tensor(11028.1777, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11028.1728515625
tensor(11028.1758, grad_fn=<NegBackward0>) tensor(11028.1729, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11028.173828125
tensor(11028.1729, grad_fn=<NegBackward0>) tensor(11028.1738, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11028.1728515625
tensor(11028.1729, grad_fn=<NegBackward0>) tensor(11028.1729, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11028.171875
tensor(11028.1729, grad_fn=<NegBackward0>) tensor(11028.1719, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11028.171875
tensor(11028.1719, grad_fn=<NegBackward0>) tensor(11028.1719, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11028.1708984375
tensor(11028.1719, grad_fn=<NegBackward0>) tensor(11028.1709, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11028.1708984375
tensor(11028.1709, grad_fn=<NegBackward0>) tensor(11028.1709, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11028.1708984375
tensor(11028.1709, grad_fn=<NegBackward0>) tensor(11028.1709, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11028.169921875
tensor(11028.1709, grad_fn=<NegBackward0>) tensor(11028.1699, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11028.1689453125
tensor(11028.1699, grad_fn=<NegBackward0>) tensor(11028.1689, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11028.1689453125
tensor(11028.1689, grad_fn=<NegBackward0>) tensor(11028.1689, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11028.1689453125
tensor(11028.1689, grad_fn=<NegBackward0>) tensor(11028.1689, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11028.169921875
tensor(11028.1689, grad_fn=<NegBackward0>) tensor(11028.1699, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11028.1689453125
tensor(11028.1689, grad_fn=<NegBackward0>) tensor(11028.1689, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11028.1689453125
tensor(11028.1689, grad_fn=<NegBackward0>) tensor(11028.1689, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11028.16796875
tensor(11028.1689, grad_fn=<NegBackward0>) tensor(11028.1680, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11028.16796875
tensor(11028.1680, grad_fn=<NegBackward0>) tensor(11028.1680, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11028.1669921875
tensor(11028.1680, grad_fn=<NegBackward0>) tensor(11028.1670, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11028.16796875
tensor(11028.1670, grad_fn=<NegBackward0>) tensor(11028.1680, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11028.16796875
tensor(11028.1670, grad_fn=<NegBackward0>) tensor(11028.1680, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11028.1689453125
tensor(11028.1670, grad_fn=<NegBackward0>) tensor(11028.1689, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11028.1708984375
tensor(11028.1670, grad_fn=<NegBackward0>) tensor(11028.1709, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -11028.24609375
tensor(11028.1670, grad_fn=<NegBackward0>) tensor(11028.2461, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7900 due to no improvement.
pi: tensor([[9.9756e-01, 2.4407e-03],
        [8.5719e-04, 9.9914e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9830, 0.0170], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1605, 0.2022],
         [0.5537, 0.1724]],

        [[0.5781, 0.2223],
         [0.5139, 0.5215]],

        [[0.5164, 0.1953],
         [0.5111, 0.6932]],

        [[0.5225, 0.2265],
         [0.5499, 0.7170]],

        [[0.7142, 0.2078],
         [0.6035, 0.6550]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.007614171548597778
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: 0.00016129143044950168
Average Adjusted Rand Index: 0.0001805543060145954
[0.00016129143044950168, 0.00016129143044950168] [0.0001805543060145954, 0.0001805543060145954] [11028.1669921875, 11028.24609375]
-------------------------------------
This iteration is 77
True Objective function: Loss = -10955.535864754685
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22979.9921875
inf tensor(22979.9922, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10987.9453125
tensor(22979.9922, grad_fn=<NegBackward0>) tensor(10987.9453, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10985.72265625
tensor(10987.9453, grad_fn=<NegBackward0>) tensor(10985.7227, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10985.0498046875
tensor(10985.7227, grad_fn=<NegBackward0>) tensor(10985.0498, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10984.7158203125
tensor(10985.0498, grad_fn=<NegBackward0>) tensor(10984.7158, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10984.408203125
tensor(10984.7158, grad_fn=<NegBackward0>) tensor(10984.4082, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10984.12109375
tensor(10984.4082, grad_fn=<NegBackward0>) tensor(10984.1211, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10983.8720703125
tensor(10984.1211, grad_fn=<NegBackward0>) tensor(10983.8721, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10983.66796875
tensor(10983.8721, grad_fn=<NegBackward0>) tensor(10983.6680, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10983.4931640625
tensor(10983.6680, grad_fn=<NegBackward0>) tensor(10983.4932, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10983.3251953125
tensor(10983.4932, grad_fn=<NegBackward0>) tensor(10983.3252, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10983.1572265625
tensor(10983.3252, grad_fn=<NegBackward0>) tensor(10983.1572, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10982.9404296875
tensor(10983.1572, grad_fn=<NegBackward0>) tensor(10982.9404, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10981.9296875
tensor(10982.9404, grad_fn=<NegBackward0>) tensor(10981.9297, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10980.828125
tensor(10981.9297, grad_fn=<NegBackward0>) tensor(10980.8281, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10979.79296875
tensor(10980.8281, grad_fn=<NegBackward0>) tensor(10979.7930, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10979.494140625
tensor(10979.7930, grad_fn=<NegBackward0>) tensor(10979.4941, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10979.4091796875
tensor(10979.4941, grad_fn=<NegBackward0>) tensor(10979.4092, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10979.365234375
tensor(10979.4092, grad_fn=<NegBackward0>) tensor(10979.3652, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10979.337890625
tensor(10979.3652, grad_fn=<NegBackward0>) tensor(10979.3379, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10979.3193359375
tensor(10979.3379, grad_fn=<NegBackward0>) tensor(10979.3193, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10979.3056640625
tensor(10979.3193, grad_fn=<NegBackward0>) tensor(10979.3057, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10979.2998046875
tensor(10979.3057, grad_fn=<NegBackward0>) tensor(10979.2998, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10979.2890625
tensor(10979.2998, grad_fn=<NegBackward0>) tensor(10979.2891, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10979.283203125
tensor(10979.2891, grad_fn=<NegBackward0>) tensor(10979.2832, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10979.279296875
tensor(10979.2832, grad_fn=<NegBackward0>) tensor(10979.2793, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10979.2763671875
tensor(10979.2793, grad_fn=<NegBackward0>) tensor(10979.2764, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10979.2724609375
tensor(10979.2764, grad_fn=<NegBackward0>) tensor(10979.2725, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10979.2685546875
tensor(10979.2725, grad_fn=<NegBackward0>) tensor(10979.2686, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10979.2685546875
tensor(10979.2686, grad_fn=<NegBackward0>) tensor(10979.2686, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10979.265625
tensor(10979.2686, grad_fn=<NegBackward0>) tensor(10979.2656, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10979.267578125
tensor(10979.2656, grad_fn=<NegBackward0>) tensor(10979.2676, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -10979.26171875
tensor(10979.2656, grad_fn=<NegBackward0>) tensor(10979.2617, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10979.2607421875
tensor(10979.2617, grad_fn=<NegBackward0>) tensor(10979.2607, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10979.2587890625
tensor(10979.2607, grad_fn=<NegBackward0>) tensor(10979.2588, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10979.259765625
tensor(10979.2588, grad_fn=<NegBackward0>) tensor(10979.2598, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -10979.259765625
tensor(10979.2588, grad_fn=<NegBackward0>) tensor(10979.2598, grad_fn=<NegBackward0>)
2
Iteration 3700: Loss = -10979.255859375
tensor(10979.2588, grad_fn=<NegBackward0>) tensor(10979.2559, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10979.255859375
tensor(10979.2559, grad_fn=<NegBackward0>) tensor(10979.2559, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10979.2568359375
tensor(10979.2559, grad_fn=<NegBackward0>) tensor(10979.2568, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -10979.255859375
tensor(10979.2559, grad_fn=<NegBackward0>) tensor(10979.2559, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10979.255859375
tensor(10979.2559, grad_fn=<NegBackward0>) tensor(10979.2559, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10979.2548828125
tensor(10979.2559, grad_fn=<NegBackward0>) tensor(10979.2549, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10979.25390625
tensor(10979.2549, grad_fn=<NegBackward0>) tensor(10979.2539, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10979.2529296875
tensor(10979.2539, grad_fn=<NegBackward0>) tensor(10979.2529, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10979.25390625
tensor(10979.2529, grad_fn=<NegBackward0>) tensor(10979.2539, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10979.25390625
tensor(10979.2529, grad_fn=<NegBackward0>) tensor(10979.2539, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -10979.2529296875
tensor(10979.2529, grad_fn=<NegBackward0>) tensor(10979.2529, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10979.251953125
tensor(10979.2529, grad_fn=<NegBackward0>) tensor(10979.2520, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10979.251953125
tensor(10979.2520, grad_fn=<NegBackward0>) tensor(10979.2520, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10979.251953125
tensor(10979.2520, grad_fn=<NegBackward0>) tensor(10979.2520, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10979.2529296875
tensor(10979.2520, grad_fn=<NegBackward0>) tensor(10979.2529, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10979.25390625
tensor(10979.2520, grad_fn=<NegBackward0>) tensor(10979.2539, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -10979.251953125
tensor(10979.2520, grad_fn=<NegBackward0>) tensor(10979.2520, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10979.2509765625
tensor(10979.2520, grad_fn=<NegBackward0>) tensor(10979.2510, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10979.2509765625
tensor(10979.2510, grad_fn=<NegBackward0>) tensor(10979.2510, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10979.2509765625
tensor(10979.2510, grad_fn=<NegBackward0>) tensor(10979.2510, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10979.25
tensor(10979.2510, grad_fn=<NegBackward0>) tensor(10979.2500, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10979.25
tensor(10979.2500, grad_fn=<NegBackward0>) tensor(10979.2500, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10979.2509765625
tensor(10979.2500, grad_fn=<NegBackward0>) tensor(10979.2510, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10979.2509765625
tensor(10979.2500, grad_fn=<NegBackward0>) tensor(10979.2510, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -10979.251953125
tensor(10979.2500, grad_fn=<NegBackward0>) tensor(10979.2520, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -10979.2509765625
tensor(10979.2500, grad_fn=<NegBackward0>) tensor(10979.2510, grad_fn=<NegBackward0>)
4
Iteration 6300: Loss = -10979.2578125
tensor(10979.2500, grad_fn=<NegBackward0>) tensor(10979.2578, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6300 due to no improvement.
pi: tensor([[0.9960, 0.0040],
        [0.3733, 0.6267]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9139, 0.0861], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1599, 0.1997],
         [0.6440, 0.6177]],

        [[0.6374, 0.1571],
         [0.6650, 0.5724]],

        [[0.6217, 0.1846],
         [0.6774, 0.5093]],

        [[0.5118, 0.1833],
         [0.7150, 0.6439]],

        [[0.6843, 0.0998],
         [0.7295, 0.5256]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.016344996169141524
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.004878730976372607
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
Global Adjusted Rand Index: -0.0005494212521307512
Average Adjusted Rand Index: 0.0019505994468861092
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23882.810546875
inf tensor(23882.8105, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10987.9140625
tensor(23882.8105, grad_fn=<NegBackward0>) tensor(10987.9141, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10987.2568359375
tensor(10987.9141, grad_fn=<NegBackward0>) tensor(10987.2568, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10987.0732421875
tensor(10987.2568, grad_fn=<NegBackward0>) tensor(10987.0732, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10986.9609375
tensor(10987.0732, grad_fn=<NegBackward0>) tensor(10986.9609, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10986.853515625
tensor(10986.9609, grad_fn=<NegBackward0>) tensor(10986.8535, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10986.7451171875
tensor(10986.8535, grad_fn=<NegBackward0>) tensor(10986.7451, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10986.5673828125
tensor(10986.7451, grad_fn=<NegBackward0>) tensor(10986.5674, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10986.2958984375
tensor(10986.5674, grad_fn=<NegBackward0>) tensor(10986.2959, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10986.0478515625
tensor(10986.2959, grad_fn=<NegBackward0>) tensor(10986.0479, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10985.9072265625
tensor(10986.0479, grad_fn=<NegBackward0>) tensor(10985.9072, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10985.8212890625
tensor(10985.9072, grad_fn=<NegBackward0>) tensor(10985.8213, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10985.7666015625
tensor(10985.8213, grad_fn=<NegBackward0>) tensor(10985.7666, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10985.71484375
tensor(10985.7666, grad_fn=<NegBackward0>) tensor(10985.7148, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10985.6865234375
tensor(10985.7148, grad_fn=<NegBackward0>) tensor(10985.6865, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10985.66796875
tensor(10985.6865, grad_fn=<NegBackward0>) tensor(10985.6680, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10985.650390625
tensor(10985.6680, grad_fn=<NegBackward0>) tensor(10985.6504, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10985.6328125
tensor(10985.6504, grad_fn=<NegBackward0>) tensor(10985.6328, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10985.619140625
tensor(10985.6328, grad_fn=<NegBackward0>) tensor(10985.6191, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10985.6103515625
tensor(10985.6191, grad_fn=<NegBackward0>) tensor(10985.6104, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10985.603515625
tensor(10985.6104, grad_fn=<NegBackward0>) tensor(10985.6035, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10985.5966796875
tensor(10985.6035, grad_fn=<NegBackward0>) tensor(10985.5967, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10985.58984375
tensor(10985.5967, grad_fn=<NegBackward0>) tensor(10985.5898, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10985.583984375
tensor(10985.5898, grad_fn=<NegBackward0>) tensor(10985.5840, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10985.5791015625
tensor(10985.5840, grad_fn=<NegBackward0>) tensor(10985.5791, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10985.57421875
tensor(10985.5791, grad_fn=<NegBackward0>) tensor(10985.5742, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10985.5693359375
tensor(10985.5742, grad_fn=<NegBackward0>) tensor(10985.5693, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10985.564453125
tensor(10985.5693, grad_fn=<NegBackward0>) tensor(10985.5645, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10985.5595703125
tensor(10985.5645, grad_fn=<NegBackward0>) tensor(10985.5596, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10985.5537109375
tensor(10985.5596, grad_fn=<NegBackward0>) tensor(10985.5537, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10985.5478515625
tensor(10985.5537, grad_fn=<NegBackward0>) tensor(10985.5479, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10985.54296875
tensor(10985.5479, grad_fn=<NegBackward0>) tensor(10985.5430, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10985.5380859375
tensor(10985.5430, grad_fn=<NegBackward0>) tensor(10985.5381, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10985.5302734375
tensor(10985.5381, grad_fn=<NegBackward0>) tensor(10985.5303, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10985.5283203125
tensor(10985.5303, grad_fn=<NegBackward0>) tensor(10985.5283, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10985.5224609375
tensor(10985.5283, grad_fn=<NegBackward0>) tensor(10985.5225, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10985.51953125
tensor(10985.5225, grad_fn=<NegBackward0>) tensor(10985.5195, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10985.515625
tensor(10985.5195, grad_fn=<NegBackward0>) tensor(10985.5156, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10985.509765625
tensor(10985.5156, grad_fn=<NegBackward0>) tensor(10985.5098, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10985.5048828125
tensor(10985.5098, grad_fn=<NegBackward0>) tensor(10985.5049, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10985.501953125
tensor(10985.5049, grad_fn=<NegBackward0>) tensor(10985.5020, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10985.498046875
tensor(10985.5020, grad_fn=<NegBackward0>) tensor(10985.4980, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10985.49609375
tensor(10985.4980, grad_fn=<NegBackward0>) tensor(10985.4961, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10985.4931640625
tensor(10985.4961, grad_fn=<NegBackward0>) tensor(10985.4932, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10985.4892578125
tensor(10985.4932, grad_fn=<NegBackward0>) tensor(10985.4893, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10985.486328125
tensor(10985.4893, grad_fn=<NegBackward0>) tensor(10985.4863, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10985.4853515625
tensor(10985.4863, grad_fn=<NegBackward0>) tensor(10985.4854, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10985.4833984375
tensor(10985.4854, grad_fn=<NegBackward0>) tensor(10985.4834, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10985.4814453125
tensor(10985.4834, grad_fn=<NegBackward0>) tensor(10985.4814, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10985.48046875
tensor(10985.4814, grad_fn=<NegBackward0>) tensor(10985.4805, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10985.48046875
tensor(10985.4805, grad_fn=<NegBackward0>) tensor(10985.4805, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10985.478515625
tensor(10985.4805, grad_fn=<NegBackward0>) tensor(10985.4785, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10985.4775390625
tensor(10985.4785, grad_fn=<NegBackward0>) tensor(10985.4775, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10985.4775390625
tensor(10985.4775, grad_fn=<NegBackward0>) tensor(10985.4775, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10985.4765625
tensor(10985.4775, grad_fn=<NegBackward0>) tensor(10985.4766, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10985.478515625
tensor(10985.4766, grad_fn=<NegBackward0>) tensor(10985.4785, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10985.4765625
tensor(10985.4766, grad_fn=<NegBackward0>) tensor(10985.4766, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10985.474609375
tensor(10985.4766, grad_fn=<NegBackward0>) tensor(10985.4746, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10985.4755859375
tensor(10985.4746, grad_fn=<NegBackward0>) tensor(10985.4756, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10985.474609375
tensor(10985.4746, grad_fn=<NegBackward0>) tensor(10985.4746, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10985.4755859375
tensor(10985.4746, grad_fn=<NegBackward0>) tensor(10985.4756, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10985.4736328125
tensor(10985.4746, grad_fn=<NegBackward0>) tensor(10985.4736, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10985.4736328125
tensor(10985.4736, grad_fn=<NegBackward0>) tensor(10985.4736, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10985.474609375
tensor(10985.4736, grad_fn=<NegBackward0>) tensor(10985.4746, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10985.47265625
tensor(10985.4736, grad_fn=<NegBackward0>) tensor(10985.4727, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10985.47265625
tensor(10985.4727, grad_fn=<NegBackward0>) tensor(10985.4727, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10985.3447265625
tensor(10985.4727, grad_fn=<NegBackward0>) tensor(10985.3447, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10982.4052734375
tensor(10985.3447, grad_fn=<NegBackward0>) tensor(10982.4053, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10982.1416015625
tensor(10982.4053, grad_fn=<NegBackward0>) tensor(10982.1416, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10982.04296875
tensor(10982.1416, grad_fn=<NegBackward0>) tensor(10982.0430, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10981.98828125
tensor(10982.0430, grad_fn=<NegBackward0>) tensor(10981.9883, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10981.9560546875
tensor(10981.9883, grad_fn=<NegBackward0>) tensor(10981.9561, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10981.935546875
tensor(10981.9561, grad_fn=<NegBackward0>) tensor(10981.9355, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10981.9189453125
tensor(10981.9355, grad_fn=<NegBackward0>) tensor(10981.9189, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10981.908203125
tensor(10981.9189, grad_fn=<NegBackward0>) tensor(10981.9082, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10981.8974609375
tensor(10981.9082, grad_fn=<NegBackward0>) tensor(10981.8975, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10981.8916015625
tensor(10981.8975, grad_fn=<NegBackward0>) tensor(10981.8916, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10981.884765625
tensor(10981.8916, grad_fn=<NegBackward0>) tensor(10981.8848, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10981.8798828125
tensor(10981.8848, grad_fn=<NegBackward0>) tensor(10981.8799, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10981.8525390625
tensor(10981.8799, grad_fn=<NegBackward0>) tensor(10981.8525, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10981.8486328125
tensor(10981.8525, grad_fn=<NegBackward0>) tensor(10981.8486, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10981.8466796875
tensor(10981.8486, grad_fn=<NegBackward0>) tensor(10981.8467, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10981.8564453125
tensor(10981.8467, grad_fn=<NegBackward0>) tensor(10981.8564, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10981.8408203125
tensor(10981.8467, grad_fn=<NegBackward0>) tensor(10981.8408, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10981.8388671875
tensor(10981.8408, grad_fn=<NegBackward0>) tensor(10981.8389, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10981.8359375
tensor(10981.8389, grad_fn=<NegBackward0>) tensor(10981.8359, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10981.833984375
tensor(10981.8359, grad_fn=<NegBackward0>) tensor(10981.8340, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10981.8408203125
tensor(10981.8340, grad_fn=<NegBackward0>) tensor(10981.8408, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -10981.8310546875
tensor(10981.8340, grad_fn=<NegBackward0>) tensor(10981.8311, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10981.82421875
tensor(10981.8311, grad_fn=<NegBackward0>) tensor(10981.8242, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10981.8251953125
tensor(10981.8242, grad_fn=<NegBackward0>) tensor(10981.8252, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10981.8203125
tensor(10981.8242, grad_fn=<NegBackward0>) tensor(10981.8203, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10981.830078125
tensor(10981.8203, grad_fn=<NegBackward0>) tensor(10981.8301, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -10981.8193359375
tensor(10981.8203, grad_fn=<NegBackward0>) tensor(10981.8193, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10981.818359375
tensor(10981.8193, grad_fn=<NegBackward0>) tensor(10981.8184, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10981.8212890625
tensor(10981.8184, grad_fn=<NegBackward0>) tensor(10981.8213, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -10981.8173828125
tensor(10981.8184, grad_fn=<NegBackward0>) tensor(10981.8174, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10981.8251953125
tensor(10981.8174, grad_fn=<NegBackward0>) tensor(10981.8252, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -10981.8154296875
tensor(10981.8174, grad_fn=<NegBackward0>) tensor(10981.8154, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10982.03515625
tensor(10981.8154, grad_fn=<NegBackward0>) tensor(10982.0352, grad_fn=<NegBackward0>)
1
pi: tensor([[1.0000e+00, 1.8119e-06],
        [6.6783e-05, 9.9993e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9707, 0.0293], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.6199e-01, 1.4806e-01],
         [6.1626e-01, 5.4708e-04]],

        [[5.3142e-01, 2.1670e-01],
         [5.6092e-01, 6.7143e-01]],

        [[6.7624e-01, 2.5993e-01],
         [6.7312e-01, 6.3367e-01]],

        [[7.2522e-01, 1.3083e-01],
         [6.4843e-01, 5.7994e-01]],

        [[6.0310e-01, 1.1403e-01],
         [6.6783e-01, 5.5327e-01]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0007766707522784365
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0031985518644306096
Global Adjusted Rand Index: 0.0004376435010132597
Average Adjusted Rand Index: 0.0006602687541775163
[-0.0005494212521307512, 0.0004376435010132597] [0.0019505994468861092, 0.0006602687541775163] [10979.2578125, 10981.81640625]
-------------------------------------
This iteration is 78
True Objective function: Loss = -10914.057451748846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22196.234375
inf tensor(22196.2344, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10969.365234375
tensor(22196.2344, grad_fn=<NegBackward0>) tensor(10969.3652, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10966.9931640625
tensor(10969.3652, grad_fn=<NegBackward0>) tensor(10966.9932, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10965.6015625
tensor(10966.9932, grad_fn=<NegBackward0>) tensor(10965.6016, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10964.8154296875
tensor(10965.6016, grad_fn=<NegBackward0>) tensor(10964.8154, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10964.6875
tensor(10964.8154, grad_fn=<NegBackward0>) tensor(10964.6875, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10964.634765625
tensor(10964.6875, grad_fn=<NegBackward0>) tensor(10964.6348, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10964.60546875
tensor(10964.6348, grad_fn=<NegBackward0>) tensor(10964.6055, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10964.58984375
tensor(10964.6055, grad_fn=<NegBackward0>) tensor(10964.5898, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10964.5791015625
tensor(10964.5898, grad_fn=<NegBackward0>) tensor(10964.5791, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10964.572265625
tensor(10964.5791, grad_fn=<NegBackward0>) tensor(10964.5723, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10964.56640625
tensor(10964.5723, grad_fn=<NegBackward0>) tensor(10964.5664, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10964.5595703125
tensor(10964.5664, grad_fn=<NegBackward0>) tensor(10964.5596, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10964.552734375
tensor(10964.5596, grad_fn=<NegBackward0>) tensor(10964.5527, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10964.5439453125
tensor(10964.5527, grad_fn=<NegBackward0>) tensor(10964.5439, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10964.533203125
tensor(10964.5439, grad_fn=<NegBackward0>) tensor(10964.5332, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10964.5224609375
tensor(10964.5332, grad_fn=<NegBackward0>) tensor(10964.5225, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10964.505859375
tensor(10964.5225, grad_fn=<NegBackward0>) tensor(10964.5059, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10964.486328125
tensor(10964.5059, grad_fn=<NegBackward0>) tensor(10964.4863, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10964.4462890625
tensor(10964.4863, grad_fn=<NegBackward0>) tensor(10964.4463, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10964.3427734375
tensor(10964.4463, grad_fn=<NegBackward0>) tensor(10964.3428, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10964.2822265625
tensor(10964.3428, grad_fn=<NegBackward0>) tensor(10964.2822, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10964.2060546875
tensor(10964.2822, grad_fn=<NegBackward0>) tensor(10964.2061, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10964.0576171875
tensor(10964.2061, grad_fn=<NegBackward0>) tensor(10964.0576, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10963.6455078125
tensor(10964.0576, grad_fn=<NegBackward0>) tensor(10963.6455, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10963.0634765625
tensor(10963.6455, grad_fn=<NegBackward0>) tensor(10963.0635, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10962.748046875
tensor(10963.0635, grad_fn=<NegBackward0>) tensor(10962.7480, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10962.6298828125
tensor(10962.7480, grad_fn=<NegBackward0>) tensor(10962.6299, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10962.576171875
tensor(10962.6299, grad_fn=<NegBackward0>) tensor(10962.5762, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10962.546875
tensor(10962.5762, grad_fn=<NegBackward0>) tensor(10962.5469, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10962.5283203125
tensor(10962.5469, grad_fn=<NegBackward0>) tensor(10962.5283, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10962.5146484375
tensor(10962.5283, grad_fn=<NegBackward0>) tensor(10962.5146, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10962.5048828125
tensor(10962.5146, grad_fn=<NegBackward0>) tensor(10962.5049, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10962.498046875
tensor(10962.5049, grad_fn=<NegBackward0>) tensor(10962.4980, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10962.4912109375
tensor(10962.4980, grad_fn=<NegBackward0>) tensor(10962.4912, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10962.486328125
tensor(10962.4912, grad_fn=<NegBackward0>) tensor(10962.4863, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10962.4833984375
tensor(10962.4863, grad_fn=<NegBackward0>) tensor(10962.4834, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10962.4794921875
tensor(10962.4834, grad_fn=<NegBackward0>) tensor(10962.4795, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10962.4775390625
tensor(10962.4795, grad_fn=<NegBackward0>) tensor(10962.4775, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10962.4755859375
tensor(10962.4775, grad_fn=<NegBackward0>) tensor(10962.4756, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10962.4736328125
tensor(10962.4756, grad_fn=<NegBackward0>) tensor(10962.4736, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10962.470703125
tensor(10962.4736, grad_fn=<NegBackward0>) tensor(10962.4707, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10962.470703125
tensor(10962.4707, grad_fn=<NegBackward0>) tensor(10962.4707, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10962.4697265625
tensor(10962.4707, grad_fn=<NegBackward0>) tensor(10962.4697, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10962.4677734375
tensor(10962.4697, grad_fn=<NegBackward0>) tensor(10962.4678, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10962.466796875
tensor(10962.4678, grad_fn=<NegBackward0>) tensor(10962.4668, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10962.466796875
tensor(10962.4668, grad_fn=<NegBackward0>) tensor(10962.4668, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10962.46484375
tensor(10962.4668, grad_fn=<NegBackward0>) tensor(10962.4648, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10962.46484375
tensor(10962.4648, grad_fn=<NegBackward0>) tensor(10962.4648, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10962.462890625
tensor(10962.4648, grad_fn=<NegBackward0>) tensor(10962.4629, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10962.4638671875
tensor(10962.4629, grad_fn=<NegBackward0>) tensor(10962.4639, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10962.4638671875
tensor(10962.4629, grad_fn=<NegBackward0>) tensor(10962.4639, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -10962.462890625
tensor(10962.4629, grad_fn=<NegBackward0>) tensor(10962.4629, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10962.4619140625
tensor(10962.4629, grad_fn=<NegBackward0>) tensor(10962.4619, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10962.4619140625
tensor(10962.4619, grad_fn=<NegBackward0>) tensor(10962.4619, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10962.4619140625
tensor(10962.4619, grad_fn=<NegBackward0>) tensor(10962.4619, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10962.4619140625
tensor(10962.4619, grad_fn=<NegBackward0>) tensor(10962.4619, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10962.4599609375
tensor(10962.4619, grad_fn=<NegBackward0>) tensor(10962.4600, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10962.4609375
tensor(10962.4600, grad_fn=<NegBackward0>) tensor(10962.4609, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10962.4599609375
tensor(10962.4600, grad_fn=<NegBackward0>) tensor(10962.4600, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10962.4599609375
tensor(10962.4600, grad_fn=<NegBackward0>) tensor(10962.4600, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10962.458984375
tensor(10962.4600, grad_fn=<NegBackward0>) tensor(10962.4590, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10962.4580078125
tensor(10962.4590, grad_fn=<NegBackward0>) tensor(10962.4580, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10962.4599609375
tensor(10962.4580, grad_fn=<NegBackward0>) tensor(10962.4600, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10962.458984375
tensor(10962.4580, grad_fn=<NegBackward0>) tensor(10962.4590, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -10962.4599609375
tensor(10962.4580, grad_fn=<NegBackward0>) tensor(10962.4600, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -10962.4580078125
tensor(10962.4580, grad_fn=<NegBackward0>) tensor(10962.4580, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10962.4580078125
tensor(10962.4580, grad_fn=<NegBackward0>) tensor(10962.4580, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10962.45703125
tensor(10962.4580, grad_fn=<NegBackward0>) tensor(10962.4570, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10962.4580078125
tensor(10962.4570, grad_fn=<NegBackward0>) tensor(10962.4580, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10962.4580078125
tensor(10962.4570, grad_fn=<NegBackward0>) tensor(10962.4580, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -10962.458984375
tensor(10962.4570, grad_fn=<NegBackward0>) tensor(10962.4590, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -10962.4580078125
tensor(10962.4570, grad_fn=<NegBackward0>) tensor(10962.4580, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -10962.45703125
tensor(10962.4570, grad_fn=<NegBackward0>) tensor(10962.4570, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10962.45703125
tensor(10962.4570, grad_fn=<NegBackward0>) tensor(10962.4570, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10962.4560546875
tensor(10962.4570, grad_fn=<NegBackward0>) tensor(10962.4561, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10962.46875
tensor(10962.4561, grad_fn=<NegBackward0>) tensor(10962.4688, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10962.45703125
tensor(10962.4561, grad_fn=<NegBackward0>) tensor(10962.4570, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -10962.45703125
tensor(10962.4561, grad_fn=<NegBackward0>) tensor(10962.4570, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -10962.455078125
tensor(10962.4561, grad_fn=<NegBackward0>) tensor(10962.4551, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10962.45703125
tensor(10962.4551, grad_fn=<NegBackward0>) tensor(10962.4570, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10962.45703125
tensor(10962.4551, grad_fn=<NegBackward0>) tensor(10962.4570, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -10962.45703125
tensor(10962.4551, grad_fn=<NegBackward0>) tensor(10962.4570, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -10962.455078125
tensor(10962.4551, grad_fn=<NegBackward0>) tensor(10962.4551, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10962.521484375
tensor(10962.4551, grad_fn=<NegBackward0>) tensor(10962.5215, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -10962.455078125
tensor(10962.4551, grad_fn=<NegBackward0>) tensor(10962.4551, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10962.5830078125
tensor(10962.4551, grad_fn=<NegBackward0>) tensor(10962.5830, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10962.4560546875
tensor(10962.4551, grad_fn=<NegBackward0>) tensor(10962.4561, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -10962.4560546875
tensor(10962.4551, grad_fn=<NegBackward0>) tensor(10962.4561, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -10962.45703125
tensor(10962.4551, grad_fn=<NegBackward0>) tensor(10962.4570, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -10962.4560546875
tensor(10962.4551, grad_fn=<NegBackward0>) tensor(10962.4561, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[9.6398e-01, 3.6018e-02],
        [3.3151e-05, 9.9997e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1472, 0.8528], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2155, 0.1596],
         [0.6511, 0.1535]],

        [[0.5247, 0.2048],
         [0.6265, 0.6548]],

        [[0.6492, 0.1898],
         [0.6699, 0.5653]],

        [[0.6799, 0.2025],
         [0.7217, 0.5165]],

        [[0.6064, 0.1684],
         [0.6176, 0.6352]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.04233735702822669
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 40
Adjusted Rand Index: 0.034997426659804425
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.011868861976101014
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0001740483092838456
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.0002920454130617311
Global Adjusted Rand Index: 0.012518216223309436
Average Adjusted Rand Index: 0.01781712971207085
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22860.8984375
inf tensor(22860.8984, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10969.6513671875
tensor(22860.8984, grad_fn=<NegBackward0>) tensor(10969.6514, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10968.638671875
tensor(10969.6514, grad_fn=<NegBackward0>) tensor(10968.6387, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10967.76171875
tensor(10968.6387, grad_fn=<NegBackward0>) tensor(10967.7617, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10966.7587890625
tensor(10967.7617, grad_fn=<NegBackward0>) tensor(10966.7588, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10966.28515625
tensor(10966.7588, grad_fn=<NegBackward0>) tensor(10966.2852, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10965.927734375
tensor(10966.2852, grad_fn=<NegBackward0>) tensor(10965.9277, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10965.6181640625
tensor(10965.9277, grad_fn=<NegBackward0>) tensor(10965.6182, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10965.3330078125
tensor(10965.6182, grad_fn=<NegBackward0>) tensor(10965.3330, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10965.091796875
tensor(10965.3330, grad_fn=<NegBackward0>) tensor(10965.0918, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10964.9091796875
tensor(10965.0918, grad_fn=<NegBackward0>) tensor(10964.9092, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10964.77734375
tensor(10964.9092, grad_fn=<NegBackward0>) tensor(10964.7773, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10964.6875
tensor(10964.7773, grad_fn=<NegBackward0>) tensor(10964.6875, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10964.62890625
tensor(10964.6875, grad_fn=<NegBackward0>) tensor(10964.6289, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10964.58984375
tensor(10964.6289, grad_fn=<NegBackward0>) tensor(10964.5898, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10964.55859375
tensor(10964.5898, grad_fn=<NegBackward0>) tensor(10964.5586, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10964.53125
tensor(10964.5586, grad_fn=<NegBackward0>) tensor(10964.5312, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10964.51171875
tensor(10964.5312, grad_fn=<NegBackward0>) tensor(10964.5117, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10964.4912109375
tensor(10964.5117, grad_fn=<NegBackward0>) tensor(10964.4912, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10964.47265625
tensor(10964.4912, grad_fn=<NegBackward0>) tensor(10964.4727, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10964.4541015625
tensor(10964.4727, grad_fn=<NegBackward0>) tensor(10964.4541, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10964.435546875
tensor(10964.4541, grad_fn=<NegBackward0>) tensor(10964.4355, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10964.41796875
tensor(10964.4355, grad_fn=<NegBackward0>) tensor(10964.4180, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10964.3994140625
tensor(10964.4180, grad_fn=<NegBackward0>) tensor(10964.3994, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10964.37890625
tensor(10964.3994, grad_fn=<NegBackward0>) tensor(10964.3789, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10964.35546875
tensor(10964.3789, grad_fn=<NegBackward0>) tensor(10964.3555, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10964.3271484375
tensor(10964.3555, grad_fn=<NegBackward0>) tensor(10964.3271, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10964.2783203125
tensor(10964.3271, grad_fn=<NegBackward0>) tensor(10964.2783, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10964.15625
tensor(10964.2783, grad_fn=<NegBackward0>) tensor(10964.1562, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10963.716796875
tensor(10964.1562, grad_fn=<NegBackward0>) tensor(10963.7168, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10963.1513671875
tensor(10963.7168, grad_fn=<NegBackward0>) tensor(10963.1514, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10962.8544921875
tensor(10963.1514, grad_fn=<NegBackward0>) tensor(10962.8545, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10962.716796875
tensor(10962.8545, grad_fn=<NegBackward0>) tensor(10962.7168, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10962.6396484375
tensor(10962.7168, grad_fn=<NegBackward0>) tensor(10962.6396, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10962.5947265625
tensor(10962.6396, grad_fn=<NegBackward0>) tensor(10962.5947, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10962.5654296875
tensor(10962.5947, grad_fn=<NegBackward0>) tensor(10962.5654, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10962.544921875
tensor(10962.5654, grad_fn=<NegBackward0>) tensor(10962.5449, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10962.529296875
tensor(10962.5449, grad_fn=<NegBackward0>) tensor(10962.5293, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10962.51953125
tensor(10962.5293, grad_fn=<NegBackward0>) tensor(10962.5195, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10962.5087890625
tensor(10962.5195, grad_fn=<NegBackward0>) tensor(10962.5088, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10962.501953125
tensor(10962.5088, grad_fn=<NegBackward0>) tensor(10962.5020, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10962.4970703125
tensor(10962.5020, grad_fn=<NegBackward0>) tensor(10962.4971, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10962.4921875
tensor(10962.4971, grad_fn=<NegBackward0>) tensor(10962.4922, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10962.48828125
tensor(10962.4922, grad_fn=<NegBackward0>) tensor(10962.4883, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10962.4853515625
tensor(10962.4883, grad_fn=<NegBackward0>) tensor(10962.4854, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10962.4814453125
tensor(10962.4854, grad_fn=<NegBackward0>) tensor(10962.4814, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10962.4794921875
tensor(10962.4814, grad_fn=<NegBackward0>) tensor(10962.4795, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10962.4775390625
tensor(10962.4795, grad_fn=<NegBackward0>) tensor(10962.4775, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10962.4755859375
tensor(10962.4775, grad_fn=<NegBackward0>) tensor(10962.4756, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10962.47265625
tensor(10962.4756, grad_fn=<NegBackward0>) tensor(10962.4727, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10962.4716796875
tensor(10962.4727, grad_fn=<NegBackward0>) tensor(10962.4717, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10962.4697265625
tensor(10962.4717, grad_fn=<NegBackward0>) tensor(10962.4697, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10962.474609375
tensor(10962.4697, grad_fn=<NegBackward0>) tensor(10962.4746, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10962.4677734375
tensor(10962.4697, grad_fn=<NegBackward0>) tensor(10962.4678, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10962.466796875
tensor(10962.4678, grad_fn=<NegBackward0>) tensor(10962.4668, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10962.466796875
tensor(10962.4668, grad_fn=<NegBackward0>) tensor(10962.4668, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10962.4658203125
tensor(10962.4668, grad_fn=<NegBackward0>) tensor(10962.4658, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10962.4658203125
tensor(10962.4658, grad_fn=<NegBackward0>) tensor(10962.4658, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10962.46484375
tensor(10962.4658, grad_fn=<NegBackward0>) tensor(10962.4648, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10962.46484375
tensor(10962.4648, grad_fn=<NegBackward0>) tensor(10962.4648, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10962.462890625
tensor(10962.4648, grad_fn=<NegBackward0>) tensor(10962.4629, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10962.462890625
tensor(10962.4629, grad_fn=<NegBackward0>) tensor(10962.4629, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10962.4638671875
tensor(10962.4629, grad_fn=<NegBackward0>) tensor(10962.4639, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10962.46484375
tensor(10962.4629, grad_fn=<NegBackward0>) tensor(10962.4648, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -10962.4609375
tensor(10962.4629, grad_fn=<NegBackward0>) tensor(10962.4609, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10962.4619140625
tensor(10962.4609, grad_fn=<NegBackward0>) tensor(10962.4619, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10962.4609375
tensor(10962.4609, grad_fn=<NegBackward0>) tensor(10962.4609, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10962.4599609375
tensor(10962.4609, grad_fn=<NegBackward0>) tensor(10962.4600, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10962.4599609375
tensor(10962.4600, grad_fn=<NegBackward0>) tensor(10962.4600, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10962.4609375
tensor(10962.4600, grad_fn=<NegBackward0>) tensor(10962.4609, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10962.4609375
tensor(10962.4600, grad_fn=<NegBackward0>) tensor(10962.4609, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -10962.458984375
tensor(10962.4600, grad_fn=<NegBackward0>) tensor(10962.4590, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10962.4599609375
tensor(10962.4590, grad_fn=<NegBackward0>) tensor(10962.4600, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10962.458984375
tensor(10962.4590, grad_fn=<NegBackward0>) tensor(10962.4590, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10962.4599609375
tensor(10962.4590, grad_fn=<NegBackward0>) tensor(10962.4600, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10962.4580078125
tensor(10962.4590, grad_fn=<NegBackward0>) tensor(10962.4580, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10962.458984375
tensor(10962.4580, grad_fn=<NegBackward0>) tensor(10962.4590, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10962.458984375
tensor(10962.4580, grad_fn=<NegBackward0>) tensor(10962.4590, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -10962.4580078125
tensor(10962.4580, grad_fn=<NegBackward0>) tensor(10962.4580, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10962.4580078125
tensor(10962.4580, grad_fn=<NegBackward0>) tensor(10962.4580, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10962.458984375
tensor(10962.4580, grad_fn=<NegBackward0>) tensor(10962.4590, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10962.4580078125
tensor(10962.4580, grad_fn=<NegBackward0>) tensor(10962.4580, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10962.4580078125
tensor(10962.4580, grad_fn=<NegBackward0>) tensor(10962.4580, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10962.4599609375
tensor(10962.4580, grad_fn=<NegBackward0>) tensor(10962.4600, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10962.4580078125
tensor(10962.4580, grad_fn=<NegBackward0>) tensor(10962.4580, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10962.4716796875
tensor(10962.4580, grad_fn=<NegBackward0>) tensor(10962.4717, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -10962.45703125
tensor(10962.4580, grad_fn=<NegBackward0>) tensor(10962.4570, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10962.45703125
tensor(10962.4570, grad_fn=<NegBackward0>) tensor(10962.4570, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10962.45703125
tensor(10962.4570, grad_fn=<NegBackward0>) tensor(10962.4570, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10962.4580078125
tensor(10962.4570, grad_fn=<NegBackward0>) tensor(10962.4580, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -10962.455078125
tensor(10962.4570, grad_fn=<NegBackward0>) tensor(10962.4551, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10962.45703125
tensor(10962.4551, grad_fn=<NegBackward0>) tensor(10962.4570, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10962.4560546875
tensor(10962.4551, grad_fn=<NegBackward0>) tensor(10962.4561, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -10962.4697265625
tensor(10962.4551, grad_fn=<NegBackward0>) tensor(10962.4697, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -10962.4560546875
tensor(10962.4551, grad_fn=<NegBackward0>) tensor(10962.4561, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -10962.45703125
tensor(10962.4551, grad_fn=<NegBackward0>) tensor(10962.4570, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9500 due to no improvement.
pi: tensor([[9.6390e-01, 3.6104e-02],
        [3.8496e-05, 9.9996e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1472, 0.8528], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2155, 0.1596],
         [0.6806, 0.1535]],

        [[0.5454, 0.2048],
         [0.6951, 0.5509]],

        [[0.6664, 0.1898],
         [0.5758, 0.5638]],

        [[0.5601, 0.2024],
         [0.5804, 0.6680]],

        [[0.5198, 0.1684],
         [0.5212, 0.5685]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.04233735702822669
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 40
Adjusted Rand Index: 0.034997426659804425
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.011868861976101014
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0001740483092838456
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.0002920454130617311
Global Adjusted Rand Index: 0.012518216223309436
Average Adjusted Rand Index: 0.01781712971207085
[0.012518216223309436, 0.012518216223309436] [0.01781712971207085, 0.01781712971207085] [10962.4560546875, 10962.45703125]
-------------------------------------
This iteration is 79
True Objective function: Loss = -10946.713455462863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23012.9140625
inf tensor(23012.9141, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11061.0703125
tensor(23012.9141, grad_fn=<NegBackward0>) tensor(11061.0703, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11059.2705078125
tensor(11061.0703, grad_fn=<NegBackward0>) tensor(11059.2705, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11057.0380859375
tensor(11059.2705, grad_fn=<NegBackward0>) tensor(11057.0381, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11056.3310546875
tensor(11057.0381, grad_fn=<NegBackward0>) tensor(11056.3311, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11055.9697265625
tensor(11056.3311, grad_fn=<NegBackward0>) tensor(11055.9697, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11055.6513671875
tensor(11055.9697, grad_fn=<NegBackward0>) tensor(11055.6514, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11054.8076171875
tensor(11055.6514, grad_fn=<NegBackward0>) tensor(11054.8076, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11053.439453125
tensor(11054.8076, grad_fn=<NegBackward0>) tensor(11053.4395, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11052.765625
tensor(11053.4395, grad_fn=<NegBackward0>) tensor(11052.7656, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11052.142578125
tensor(11052.7656, grad_fn=<NegBackward0>) tensor(11052.1426, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11051.56640625
tensor(11052.1426, grad_fn=<NegBackward0>) tensor(11051.5664, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11051.123046875
tensor(11051.5664, grad_fn=<NegBackward0>) tensor(11051.1230, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11050.8271484375
tensor(11051.1230, grad_fn=<NegBackward0>) tensor(11050.8271, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11050.62890625
tensor(11050.8271, grad_fn=<NegBackward0>) tensor(11050.6289, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11050.4033203125
tensor(11050.6289, grad_fn=<NegBackward0>) tensor(11050.4033, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11049.880859375
tensor(11050.4033, grad_fn=<NegBackward0>) tensor(11049.8809, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11049.3642578125
tensor(11049.8809, grad_fn=<NegBackward0>) tensor(11049.3643, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11048.96875
tensor(11049.3643, grad_fn=<NegBackward0>) tensor(11048.9688, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11048.7392578125
tensor(11048.9688, grad_fn=<NegBackward0>) tensor(11048.7393, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11048.5654296875
tensor(11048.7393, grad_fn=<NegBackward0>) tensor(11048.5654, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11048.3544921875
tensor(11048.5654, grad_fn=<NegBackward0>) tensor(11048.3545, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11048.130859375
tensor(11048.3545, grad_fn=<NegBackward0>) tensor(11048.1309, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11047.951171875
tensor(11048.1309, grad_fn=<NegBackward0>) tensor(11047.9512, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11047.794921875
tensor(11047.9512, grad_fn=<NegBackward0>) tensor(11047.7949, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11047.6201171875
tensor(11047.7949, grad_fn=<NegBackward0>) tensor(11047.6201, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11047.4814453125
tensor(11047.6201, grad_fn=<NegBackward0>) tensor(11047.4814, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11047.3134765625
tensor(11047.4814, grad_fn=<NegBackward0>) tensor(11047.3135, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10955.4345703125
tensor(11047.3135, grad_fn=<NegBackward0>) tensor(10955.4346, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10951.541015625
tensor(10955.4346, grad_fn=<NegBackward0>) tensor(10951.5410, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10951.35546875
tensor(10951.5410, grad_fn=<NegBackward0>) tensor(10951.3555, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10951.2763671875
tensor(10951.3555, grad_fn=<NegBackward0>) tensor(10951.2764, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10951.134765625
tensor(10951.2764, grad_fn=<NegBackward0>) tensor(10951.1348, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10949.673828125
tensor(10951.1348, grad_fn=<NegBackward0>) tensor(10949.6738, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10940.521484375
tensor(10949.6738, grad_fn=<NegBackward0>) tensor(10940.5215, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10916.1669921875
tensor(10940.5215, grad_fn=<NegBackward0>) tensor(10916.1670, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10913.5888671875
tensor(10916.1670, grad_fn=<NegBackward0>) tensor(10913.5889, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10912.1259765625
tensor(10913.5889, grad_fn=<NegBackward0>) tensor(10912.1260, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10911.943359375
tensor(10912.1260, grad_fn=<NegBackward0>) tensor(10911.9434, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10906.62109375
tensor(10911.9434, grad_fn=<NegBackward0>) tensor(10906.6211, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10906.6240234375
tensor(10906.6211, grad_fn=<NegBackward0>) tensor(10906.6240, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -10906.6103515625
tensor(10906.6211, grad_fn=<NegBackward0>) tensor(10906.6104, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10906.6103515625
tensor(10906.6104, grad_fn=<NegBackward0>) tensor(10906.6104, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10906.6064453125
tensor(10906.6104, grad_fn=<NegBackward0>) tensor(10906.6064, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10906.603515625
tensor(10906.6064, grad_fn=<NegBackward0>) tensor(10906.6035, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10906.5458984375
tensor(10906.6035, grad_fn=<NegBackward0>) tensor(10906.5459, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10906.5419921875
tensor(10906.5459, grad_fn=<NegBackward0>) tensor(10906.5420, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10906.541015625
tensor(10906.5420, grad_fn=<NegBackward0>) tensor(10906.5410, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10906.5361328125
tensor(10906.5410, grad_fn=<NegBackward0>) tensor(10906.5361, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10906.43359375
tensor(10906.5361, grad_fn=<NegBackward0>) tensor(10906.4336, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10906.431640625
tensor(10906.4336, grad_fn=<NegBackward0>) tensor(10906.4316, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10906.4287109375
tensor(10906.4316, grad_fn=<NegBackward0>) tensor(10906.4287, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10906.4287109375
tensor(10906.4287, grad_fn=<NegBackward0>) tensor(10906.4287, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10906.4296875
tensor(10906.4287, grad_fn=<NegBackward0>) tensor(10906.4297, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10906.4287109375
tensor(10906.4287, grad_fn=<NegBackward0>) tensor(10906.4287, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10906.427734375
tensor(10906.4287, grad_fn=<NegBackward0>) tensor(10906.4277, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10906.4111328125
tensor(10906.4277, grad_fn=<NegBackward0>) tensor(10906.4111, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10906.333984375
tensor(10906.4111, grad_fn=<NegBackward0>) tensor(10906.3340, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10906.3349609375
tensor(10906.3340, grad_fn=<NegBackward0>) tensor(10906.3350, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10906.333984375
tensor(10906.3340, grad_fn=<NegBackward0>) tensor(10906.3340, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10906.3349609375
tensor(10906.3340, grad_fn=<NegBackward0>) tensor(10906.3350, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10906.3359375
tensor(10906.3340, grad_fn=<NegBackward0>) tensor(10906.3359, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -10906.32421875
tensor(10906.3340, grad_fn=<NegBackward0>) tensor(10906.3242, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10906.32421875
tensor(10906.3242, grad_fn=<NegBackward0>) tensor(10906.3242, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10906.3232421875
tensor(10906.3242, grad_fn=<NegBackward0>) tensor(10906.3232, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10906.3232421875
tensor(10906.3232, grad_fn=<NegBackward0>) tensor(10906.3232, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10906.34375
tensor(10906.3232, grad_fn=<NegBackward0>) tensor(10906.3438, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10906.3232421875
tensor(10906.3232, grad_fn=<NegBackward0>) tensor(10906.3232, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10906.32421875
tensor(10906.3232, grad_fn=<NegBackward0>) tensor(10906.3242, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10906.3251953125
tensor(10906.3232, grad_fn=<NegBackward0>) tensor(10906.3252, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -10906.3212890625
tensor(10906.3232, grad_fn=<NegBackward0>) tensor(10906.3213, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10906.314453125
tensor(10906.3213, grad_fn=<NegBackward0>) tensor(10906.3145, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10906.31640625
tensor(10906.3145, grad_fn=<NegBackward0>) tensor(10906.3164, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10906.314453125
tensor(10906.3145, grad_fn=<NegBackward0>) tensor(10906.3145, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10906.3173828125
tensor(10906.3145, grad_fn=<NegBackward0>) tensor(10906.3174, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10906.31640625
tensor(10906.3145, grad_fn=<NegBackward0>) tensor(10906.3164, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -10906.3134765625
tensor(10906.3145, grad_fn=<NegBackward0>) tensor(10906.3135, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10906.3203125
tensor(10906.3135, grad_fn=<NegBackward0>) tensor(10906.3203, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10906.314453125
tensor(10906.3135, grad_fn=<NegBackward0>) tensor(10906.3145, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -10906.3134765625
tensor(10906.3135, grad_fn=<NegBackward0>) tensor(10906.3135, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10906.3134765625
tensor(10906.3135, grad_fn=<NegBackward0>) tensor(10906.3135, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10906.314453125
tensor(10906.3135, grad_fn=<NegBackward0>) tensor(10906.3145, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10906.3251953125
tensor(10906.3135, grad_fn=<NegBackward0>) tensor(10906.3252, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -10906.3125
tensor(10906.3135, grad_fn=<NegBackward0>) tensor(10906.3125, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10906.34375
tensor(10906.3125, grad_fn=<NegBackward0>) tensor(10906.3438, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -10906.3134765625
tensor(10906.3125, grad_fn=<NegBackward0>) tensor(10906.3135, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -10906.4736328125
tensor(10906.3125, grad_fn=<NegBackward0>) tensor(10906.4736, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -10906.3125
tensor(10906.3125, grad_fn=<NegBackward0>) tensor(10906.3125, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10906.3125
tensor(10906.3125, grad_fn=<NegBackward0>) tensor(10906.3125, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10906.3134765625
tensor(10906.3125, grad_fn=<NegBackward0>) tensor(10906.3135, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -10906.3134765625
tensor(10906.3125, grad_fn=<NegBackward0>) tensor(10906.3135, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -10906.3271484375
tensor(10906.3125, grad_fn=<NegBackward0>) tensor(10906.3271, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -10906.3115234375
tensor(10906.3125, grad_fn=<NegBackward0>) tensor(10906.3115, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10906.32421875
tensor(10906.3115, grad_fn=<NegBackward0>) tensor(10906.3242, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -10906.3125
tensor(10906.3115, grad_fn=<NegBackward0>) tensor(10906.3125, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -10906.3115234375
tensor(10906.3115, grad_fn=<NegBackward0>) tensor(10906.3115, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10906.384765625
tensor(10906.3115, grad_fn=<NegBackward0>) tensor(10906.3848, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -10906.3095703125
tensor(10906.3115, grad_fn=<NegBackward0>) tensor(10906.3096, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10906.314453125
tensor(10906.3096, grad_fn=<NegBackward0>) tensor(10906.3145, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -10906.30859375
tensor(10906.3096, grad_fn=<NegBackward0>) tensor(10906.3086, grad_fn=<NegBackward0>)
pi: tensor([[0.7348, 0.2652],
        [0.2779, 0.7221]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5639, 0.4361], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2050, 0.0994],
         [0.7095, 0.2613]],

        [[0.6853, 0.1022],
         [0.6556, 0.5860]],

        [[0.6903, 0.0916],
         [0.6573, 0.5392]],

        [[0.6843, 0.1039],
         [0.6474, 0.6960]],

        [[0.6328, 0.0992],
         [0.5725, 0.5491]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448509923071951
time is 1
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 3
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369789715403975
time is 4
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.844846433231073
Global Adjusted Rand Index: 0.8534827488659569
Average Adjusted Rand Index: 0.8538175347050572
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20638.69921875
inf tensor(20638.6992, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11060.57421875
tensor(20638.6992, grad_fn=<NegBackward0>) tensor(11060.5742, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11059.349609375
tensor(11060.5742, grad_fn=<NegBackward0>) tensor(11059.3496, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11057.8974609375
tensor(11059.3496, grad_fn=<NegBackward0>) tensor(11057.8975, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11057.23828125
tensor(11057.8975, grad_fn=<NegBackward0>) tensor(11057.2383, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11056.3486328125
tensor(11057.2383, grad_fn=<NegBackward0>) tensor(11056.3486, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11055.6884765625
tensor(11056.3486, grad_fn=<NegBackward0>) tensor(11055.6885, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11055.0400390625
tensor(11055.6885, grad_fn=<NegBackward0>) tensor(11055.0400, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11053.3388671875
tensor(11055.0400, grad_fn=<NegBackward0>) tensor(11053.3389, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11052.2236328125
tensor(11053.3389, grad_fn=<NegBackward0>) tensor(11052.2236, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11049.9052734375
tensor(11052.2236, grad_fn=<NegBackward0>) tensor(11049.9053, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11049.296875
tensor(11049.9053, grad_fn=<NegBackward0>) tensor(11049.2969, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11049.001953125
tensor(11049.2969, grad_fn=<NegBackward0>) tensor(11049.0020, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11048.7939453125
tensor(11049.0020, grad_fn=<NegBackward0>) tensor(11048.7939, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11048.5830078125
tensor(11048.7939, grad_fn=<NegBackward0>) tensor(11048.5830, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11048.4169921875
tensor(11048.5830, grad_fn=<NegBackward0>) tensor(11048.4170, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11048.2568359375
tensor(11048.4170, grad_fn=<NegBackward0>) tensor(11048.2568, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11048.0546875
tensor(11048.2568, grad_fn=<NegBackward0>) tensor(11048.0547, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11047.8857421875
tensor(11048.0547, grad_fn=<NegBackward0>) tensor(11047.8857, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11047.7412109375
tensor(11047.8857, grad_fn=<NegBackward0>) tensor(11047.7412, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11047.6005859375
tensor(11047.7412, grad_fn=<NegBackward0>) tensor(11047.6006, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11047.474609375
tensor(11047.6006, grad_fn=<NegBackward0>) tensor(11047.4746, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11047.3017578125
tensor(11047.4746, grad_fn=<NegBackward0>) tensor(11047.3018, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10956.8408203125
tensor(11047.3018, grad_fn=<NegBackward0>) tensor(10956.8408, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10951.623046875
tensor(10956.8408, grad_fn=<NegBackward0>) tensor(10951.6230, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10951.4462890625
tensor(10951.6230, grad_fn=<NegBackward0>) tensor(10951.4463, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10951.361328125
tensor(10951.4463, grad_fn=<NegBackward0>) tensor(10951.3613, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10951.296875
tensor(10951.3613, grad_fn=<NegBackward0>) tensor(10951.2969, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10951.267578125
tensor(10951.2969, grad_fn=<NegBackward0>) tensor(10951.2676, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10951.2236328125
tensor(10951.2676, grad_fn=<NegBackward0>) tensor(10951.2236, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10951.171875
tensor(10951.2236, grad_fn=<NegBackward0>) tensor(10951.1719, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10950.953125
tensor(10951.1719, grad_fn=<NegBackward0>) tensor(10950.9531, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10948.828125
tensor(10950.9531, grad_fn=<NegBackward0>) tensor(10948.8281, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10942.126953125
tensor(10948.8281, grad_fn=<NegBackward0>) tensor(10942.1270, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10940.919921875
tensor(10942.1270, grad_fn=<NegBackward0>) tensor(10940.9199, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10934.162109375
tensor(10940.9199, grad_fn=<NegBackward0>) tensor(10934.1621, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10912.99609375
tensor(10934.1621, grad_fn=<NegBackward0>) tensor(10912.9961, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10912.1220703125
tensor(10912.9961, grad_fn=<NegBackward0>) tensor(10912.1221, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10911.92578125
tensor(10912.1221, grad_fn=<NegBackward0>) tensor(10911.9258, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10911.8564453125
tensor(10911.9258, grad_fn=<NegBackward0>) tensor(10911.8564, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10906.607421875
tensor(10911.8564, grad_fn=<NegBackward0>) tensor(10906.6074, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10906.509765625
tensor(10906.6074, grad_fn=<NegBackward0>) tensor(10906.5098, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10906.4541015625
tensor(10906.5098, grad_fn=<NegBackward0>) tensor(10906.4541, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10906.4423828125
tensor(10906.4541, grad_fn=<NegBackward0>) tensor(10906.4424, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10906.4423828125
tensor(10906.4424, grad_fn=<NegBackward0>) tensor(10906.4424, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10906.458984375
tensor(10906.4424, grad_fn=<NegBackward0>) tensor(10906.4590, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10906.4404296875
tensor(10906.4424, grad_fn=<NegBackward0>) tensor(10906.4404, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10906.439453125
tensor(10906.4404, grad_fn=<NegBackward0>) tensor(10906.4395, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10906.4404296875
tensor(10906.4395, grad_fn=<NegBackward0>) tensor(10906.4404, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10906.4384765625
tensor(10906.4395, grad_fn=<NegBackward0>) tensor(10906.4385, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10906.439453125
tensor(10906.4385, grad_fn=<NegBackward0>) tensor(10906.4395, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10906.4384765625
tensor(10906.4385, grad_fn=<NegBackward0>) tensor(10906.4385, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10906.4423828125
tensor(10906.4385, grad_fn=<NegBackward0>) tensor(10906.4424, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10906.4375
tensor(10906.4385, grad_fn=<NegBackward0>) tensor(10906.4375, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10906.4365234375
tensor(10906.4375, grad_fn=<NegBackward0>) tensor(10906.4365, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10906.4345703125
tensor(10906.4365, grad_fn=<NegBackward0>) tensor(10906.4346, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10906.43359375
tensor(10906.4346, grad_fn=<NegBackward0>) tensor(10906.4336, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10906.431640625
tensor(10906.4336, grad_fn=<NegBackward0>) tensor(10906.4316, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10906.431640625
tensor(10906.4316, grad_fn=<NegBackward0>) tensor(10906.4316, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10906.431640625
tensor(10906.4316, grad_fn=<NegBackward0>) tensor(10906.4316, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10906.43359375
tensor(10906.4316, grad_fn=<NegBackward0>) tensor(10906.4336, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10906.4326171875
tensor(10906.4316, grad_fn=<NegBackward0>) tensor(10906.4326, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -10906.4296875
tensor(10906.4316, grad_fn=<NegBackward0>) tensor(10906.4297, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10906.431640625
tensor(10906.4297, grad_fn=<NegBackward0>) tensor(10906.4316, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10906.4306640625
tensor(10906.4297, grad_fn=<NegBackward0>) tensor(10906.4307, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -10906.4306640625
tensor(10906.4297, grad_fn=<NegBackward0>) tensor(10906.4307, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -10906.3232421875
tensor(10906.4297, grad_fn=<NegBackward0>) tensor(10906.3232, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10906.322265625
tensor(10906.3232, grad_fn=<NegBackward0>) tensor(10906.3223, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10906.322265625
tensor(10906.3223, grad_fn=<NegBackward0>) tensor(10906.3223, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10906.32421875
tensor(10906.3223, grad_fn=<NegBackward0>) tensor(10906.3242, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10906.322265625
tensor(10906.3223, grad_fn=<NegBackward0>) tensor(10906.3223, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10906.3212890625
tensor(10906.3223, grad_fn=<NegBackward0>) tensor(10906.3213, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10906.3212890625
tensor(10906.3213, grad_fn=<NegBackward0>) tensor(10906.3213, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10906.318359375
tensor(10906.3213, grad_fn=<NegBackward0>) tensor(10906.3184, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10906.3125
tensor(10906.3184, grad_fn=<NegBackward0>) tensor(10906.3125, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10906.3271484375
tensor(10906.3125, grad_fn=<NegBackward0>) tensor(10906.3271, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10906.314453125
tensor(10906.3125, grad_fn=<NegBackward0>) tensor(10906.3145, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -10906.3193359375
tensor(10906.3125, grad_fn=<NegBackward0>) tensor(10906.3193, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -10906.3603515625
tensor(10906.3125, grad_fn=<NegBackward0>) tensor(10906.3604, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -10906.3125
tensor(10906.3125, grad_fn=<NegBackward0>) tensor(10906.3125, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10906.3125
tensor(10906.3125, grad_fn=<NegBackward0>) tensor(10906.3125, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10906.44140625
tensor(10906.3125, grad_fn=<NegBackward0>) tensor(10906.4414, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10906.310546875
tensor(10906.3125, grad_fn=<NegBackward0>) tensor(10906.3105, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10906.4248046875
tensor(10906.3105, grad_fn=<NegBackward0>) tensor(10906.4248, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10906.3095703125
tensor(10906.3105, grad_fn=<NegBackward0>) tensor(10906.3096, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10906.314453125
tensor(10906.3096, grad_fn=<NegBackward0>) tensor(10906.3145, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -10906.310546875
tensor(10906.3096, grad_fn=<NegBackward0>) tensor(10906.3105, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -10906.3359375
tensor(10906.3096, grad_fn=<NegBackward0>) tensor(10906.3359, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -10906.310546875
tensor(10906.3096, grad_fn=<NegBackward0>) tensor(10906.3105, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -10906.310546875
tensor(10906.3096, grad_fn=<NegBackward0>) tensor(10906.3105, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8900 due to no improvement.
pi: tensor([[0.7348, 0.2652],
        [0.2779, 0.7221]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5639, 0.4361], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2050, 0.0994],
         [0.6692, 0.2613]],

        [[0.7147, 0.1022],
         [0.7229, 0.6807]],

        [[0.5422, 0.0916],
         [0.5578, 0.5796]],

        [[0.6900, 0.1038],
         [0.5022, 0.6109]],

        [[0.6763, 0.0991],
         [0.6860, 0.5572]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448509923071951
time is 1
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 3
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369789715403975
time is 4
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.844846433231073
Global Adjusted Rand Index: 0.8534827488659569
Average Adjusted Rand Index: 0.8538175347050572
[0.8534827488659569, 0.8534827488659569] [0.8538175347050572, 0.8538175347050572] [10906.3076171875, 10906.310546875]
-------------------------------------
This iteration is 80
True Objective function: Loss = -10641.519530002
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20384.0078125
inf tensor(20384.0078, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10751.681640625
tensor(20384.0078, grad_fn=<NegBackward0>) tensor(10751.6816, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10751.49609375
tensor(10751.6816, grad_fn=<NegBackward0>) tensor(10751.4961, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10751.3818359375
tensor(10751.4961, grad_fn=<NegBackward0>) tensor(10751.3818, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10751.2548828125
tensor(10751.3818, grad_fn=<NegBackward0>) tensor(10751.2549, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10751.1669921875
tensor(10751.2549, grad_fn=<NegBackward0>) tensor(10751.1670, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10751.1142578125
tensor(10751.1670, grad_fn=<NegBackward0>) tensor(10751.1143, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10751.068359375
tensor(10751.1143, grad_fn=<NegBackward0>) tensor(10751.0684, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10751.0126953125
tensor(10751.0684, grad_fn=<NegBackward0>) tensor(10751.0127, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10750.93359375
tensor(10751.0127, grad_fn=<NegBackward0>) tensor(10750.9336, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10750.75390625
tensor(10750.9336, grad_fn=<NegBackward0>) tensor(10750.7539, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10750.1884765625
tensor(10750.7539, grad_fn=<NegBackward0>) tensor(10750.1885, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10748.27734375
tensor(10750.1885, grad_fn=<NegBackward0>) tensor(10748.2773, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10747.0546875
tensor(10748.2773, grad_fn=<NegBackward0>) tensor(10747.0547, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10746.169921875
tensor(10747.0547, grad_fn=<NegBackward0>) tensor(10746.1699, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10744.9033203125
tensor(10746.1699, grad_fn=<NegBackward0>) tensor(10744.9033, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10744.0595703125
tensor(10744.9033, grad_fn=<NegBackward0>) tensor(10744.0596, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10743.79296875
tensor(10744.0596, grad_fn=<NegBackward0>) tensor(10743.7930, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10743.6708984375
tensor(10743.7930, grad_fn=<NegBackward0>) tensor(10743.6709, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10743.59375
tensor(10743.6709, grad_fn=<NegBackward0>) tensor(10743.5938, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10743.5400390625
tensor(10743.5938, grad_fn=<NegBackward0>) tensor(10743.5400, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10743.4990234375
tensor(10743.5400, grad_fn=<NegBackward0>) tensor(10743.4990, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10743.4658203125
tensor(10743.4990, grad_fn=<NegBackward0>) tensor(10743.4658, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10743.4365234375
tensor(10743.4658, grad_fn=<NegBackward0>) tensor(10743.4365, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10743.41015625
tensor(10743.4365, grad_fn=<NegBackward0>) tensor(10743.4102, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10743.3916015625
tensor(10743.4102, grad_fn=<NegBackward0>) tensor(10743.3916, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10743.373046875
tensor(10743.3916, grad_fn=<NegBackward0>) tensor(10743.3730, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10743.3583984375
tensor(10743.3730, grad_fn=<NegBackward0>) tensor(10743.3584, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10743.345703125
tensor(10743.3584, grad_fn=<NegBackward0>) tensor(10743.3457, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10743.333984375
tensor(10743.3457, grad_fn=<NegBackward0>) tensor(10743.3340, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10743.3251953125
tensor(10743.3340, grad_fn=<NegBackward0>) tensor(10743.3252, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10743.31640625
tensor(10743.3252, grad_fn=<NegBackward0>) tensor(10743.3164, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10743.3115234375
tensor(10743.3164, grad_fn=<NegBackward0>) tensor(10743.3115, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10743.302734375
tensor(10743.3115, grad_fn=<NegBackward0>) tensor(10743.3027, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10743.2978515625
tensor(10743.3027, grad_fn=<NegBackward0>) tensor(10743.2979, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10743.2939453125
tensor(10743.2979, grad_fn=<NegBackward0>) tensor(10743.2939, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10743.2890625
tensor(10743.2939, grad_fn=<NegBackward0>) tensor(10743.2891, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10743.28515625
tensor(10743.2891, grad_fn=<NegBackward0>) tensor(10743.2852, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10743.2822265625
tensor(10743.2852, grad_fn=<NegBackward0>) tensor(10743.2822, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10743.279296875
tensor(10743.2822, grad_fn=<NegBackward0>) tensor(10743.2793, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10743.2763671875
tensor(10743.2793, grad_fn=<NegBackward0>) tensor(10743.2764, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10743.2744140625
tensor(10743.2764, grad_fn=<NegBackward0>) tensor(10743.2744, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10743.2724609375
tensor(10743.2744, grad_fn=<NegBackward0>) tensor(10743.2725, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10743.26953125
tensor(10743.2725, grad_fn=<NegBackward0>) tensor(10743.2695, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10743.2666015625
tensor(10743.2695, grad_fn=<NegBackward0>) tensor(10743.2666, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10743.265625
tensor(10743.2666, grad_fn=<NegBackward0>) tensor(10743.2656, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10743.263671875
tensor(10743.2656, grad_fn=<NegBackward0>) tensor(10743.2637, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10743.26171875
tensor(10743.2637, grad_fn=<NegBackward0>) tensor(10743.2617, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10743.259765625
tensor(10743.2617, grad_fn=<NegBackward0>) tensor(10743.2598, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10743.2587890625
tensor(10743.2598, grad_fn=<NegBackward0>) tensor(10743.2588, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10743.2578125
tensor(10743.2588, grad_fn=<NegBackward0>) tensor(10743.2578, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10743.2548828125
tensor(10743.2578, grad_fn=<NegBackward0>) tensor(10743.2549, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10743.2548828125
tensor(10743.2549, grad_fn=<NegBackward0>) tensor(10743.2549, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10743.255859375
tensor(10743.2549, grad_fn=<NegBackward0>) tensor(10743.2559, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10743.25390625
tensor(10743.2549, grad_fn=<NegBackward0>) tensor(10743.2539, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10743.2529296875
tensor(10743.2539, grad_fn=<NegBackward0>) tensor(10743.2529, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10743.251953125
tensor(10743.2529, grad_fn=<NegBackward0>) tensor(10743.2520, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10743.2509765625
tensor(10743.2520, grad_fn=<NegBackward0>) tensor(10743.2510, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10743.251953125
tensor(10743.2510, grad_fn=<NegBackward0>) tensor(10743.2520, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10743.25
tensor(10743.2510, grad_fn=<NegBackward0>) tensor(10743.2500, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10743.248046875
tensor(10743.2500, grad_fn=<NegBackward0>) tensor(10743.2480, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10743.248046875
tensor(10743.2480, grad_fn=<NegBackward0>) tensor(10743.2480, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10743.2470703125
tensor(10743.2480, grad_fn=<NegBackward0>) tensor(10743.2471, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10743.248046875
tensor(10743.2471, grad_fn=<NegBackward0>) tensor(10743.2480, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10743.248046875
tensor(10743.2471, grad_fn=<NegBackward0>) tensor(10743.2480, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -10743.2451171875
tensor(10743.2471, grad_fn=<NegBackward0>) tensor(10743.2451, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10743.2470703125
tensor(10743.2451, grad_fn=<NegBackward0>) tensor(10743.2471, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10743.2470703125
tensor(10743.2451, grad_fn=<NegBackward0>) tensor(10743.2471, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -10743.2451171875
tensor(10743.2451, grad_fn=<NegBackward0>) tensor(10743.2451, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10743.24609375
tensor(10743.2451, grad_fn=<NegBackward0>) tensor(10743.2461, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10743.244140625
tensor(10743.2451, grad_fn=<NegBackward0>) tensor(10743.2441, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10743.2451171875
tensor(10743.2441, grad_fn=<NegBackward0>) tensor(10743.2451, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10743.2431640625
tensor(10743.2441, grad_fn=<NegBackward0>) tensor(10743.2432, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10743.2421875
tensor(10743.2432, grad_fn=<NegBackward0>) tensor(10743.2422, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10743.24609375
tensor(10743.2422, grad_fn=<NegBackward0>) tensor(10743.2461, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10743.2431640625
tensor(10743.2422, grad_fn=<NegBackward0>) tensor(10743.2432, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -10743.2431640625
tensor(10743.2422, grad_fn=<NegBackward0>) tensor(10743.2432, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -10743.244140625
tensor(10743.2422, grad_fn=<NegBackward0>) tensor(10743.2441, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -10743.2412109375
tensor(10743.2422, grad_fn=<NegBackward0>) tensor(10743.2412, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10743.2421875
tensor(10743.2412, grad_fn=<NegBackward0>) tensor(10743.2422, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10743.244140625
tensor(10743.2412, grad_fn=<NegBackward0>) tensor(10743.2441, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -10743.2412109375
tensor(10743.2412, grad_fn=<NegBackward0>) tensor(10743.2412, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10743.2412109375
tensor(10743.2412, grad_fn=<NegBackward0>) tensor(10743.2412, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10743.2412109375
tensor(10743.2412, grad_fn=<NegBackward0>) tensor(10743.2412, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10743.2421875
tensor(10743.2412, grad_fn=<NegBackward0>) tensor(10743.2422, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -10743.2412109375
tensor(10743.2412, grad_fn=<NegBackward0>) tensor(10743.2412, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10743.240234375
tensor(10743.2412, grad_fn=<NegBackward0>) tensor(10743.2402, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10743.240234375
tensor(10743.2402, grad_fn=<NegBackward0>) tensor(10743.2402, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10743.2421875
tensor(10743.2402, grad_fn=<NegBackward0>) tensor(10743.2422, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10743.2412109375
tensor(10743.2402, grad_fn=<NegBackward0>) tensor(10743.2412, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -10743.2392578125
tensor(10743.2402, grad_fn=<NegBackward0>) tensor(10743.2393, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10743.240234375
tensor(10743.2393, grad_fn=<NegBackward0>) tensor(10743.2402, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10743.240234375
tensor(10743.2393, grad_fn=<NegBackward0>) tensor(10743.2402, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -10743.240234375
tensor(10743.2393, grad_fn=<NegBackward0>) tensor(10743.2402, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -10743.240234375
tensor(10743.2393, grad_fn=<NegBackward0>) tensor(10743.2402, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -10743.240234375
tensor(10743.2393, grad_fn=<NegBackward0>) tensor(10743.2402, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9500 due to no improvement.
pi: tensor([[9.9980e-01, 1.9813e-04],
        [4.5575e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0183, 0.9817], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2002, 0.1260],
         [0.5249, 0.1564]],

        [[0.6266, 0.2270],
         [0.5428, 0.6640]],

        [[0.5288, 0.2290],
         [0.5463, 0.5303]],

        [[0.5677, 0.0567],
         [0.5055, 0.5479]],

        [[0.6453, 0.2038],
         [0.6260, 0.5164]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.01382061954603653
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.00035341041046094813
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.01171303074670571
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: 0.0062187062454228965
Average Adjusted Rand Index: 0.005500900640624464
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23583.84765625
inf tensor(23583.8477, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10752.0419921875
tensor(23583.8477, grad_fn=<NegBackward0>) tensor(10752.0420, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10751.4765625
tensor(10752.0420, grad_fn=<NegBackward0>) tensor(10751.4766, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10751.23828125
tensor(10751.4766, grad_fn=<NegBackward0>) tensor(10751.2383, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10751.029296875
tensor(10751.2383, grad_fn=<NegBackward0>) tensor(10751.0293, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10750.9267578125
tensor(10751.0293, grad_fn=<NegBackward0>) tensor(10750.9268, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10750.8115234375
tensor(10750.9268, grad_fn=<NegBackward0>) tensor(10750.8115, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10750.5791015625
tensor(10750.8115, grad_fn=<NegBackward0>) tensor(10750.5791, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10748.3095703125
tensor(10750.5791, grad_fn=<NegBackward0>) tensor(10748.3096, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10747.0244140625
tensor(10748.3096, grad_fn=<NegBackward0>) tensor(10747.0244, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10746.279296875
tensor(10747.0244, grad_fn=<NegBackward0>) tensor(10746.2793, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10745.4140625
tensor(10746.2793, grad_fn=<NegBackward0>) tensor(10745.4141, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10744.3212890625
tensor(10745.4141, grad_fn=<NegBackward0>) tensor(10744.3213, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10743.9267578125
tensor(10744.3213, grad_fn=<NegBackward0>) tensor(10743.9268, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10743.7587890625
tensor(10743.9268, grad_fn=<NegBackward0>) tensor(10743.7588, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10743.66015625
tensor(10743.7588, grad_fn=<NegBackward0>) tensor(10743.6602, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10743.59765625
tensor(10743.6602, grad_fn=<NegBackward0>) tensor(10743.5977, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10743.5517578125
tensor(10743.5977, grad_fn=<NegBackward0>) tensor(10743.5518, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10743.515625
tensor(10743.5518, grad_fn=<NegBackward0>) tensor(10743.5156, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10743.4873046875
tensor(10743.5156, grad_fn=<NegBackward0>) tensor(10743.4873, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10743.4638671875
tensor(10743.4873, grad_fn=<NegBackward0>) tensor(10743.4639, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10743.44140625
tensor(10743.4639, grad_fn=<NegBackward0>) tensor(10743.4414, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10743.4208984375
tensor(10743.4414, grad_fn=<NegBackward0>) tensor(10743.4209, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10743.4033203125
tensor(10743.4209, grad_fn=<NegBackward0>) tensor(10743.4033, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10743.3876953125
tensor(10743.4033, grad_fn=<NegBackward0>) tensor(10743.3877, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10743.37109375
tensor(10743.3877, grad_fn=<NegBackward0>) tensor(10743.3711, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10743.357421875
tensor(10743.3711, grad_fn=<NegBackward0>) tensor(10743.3574, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10743.3447265625
tensor(10743.3574, grad_fn=<NegBackward0>) tensor(10743.3447, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10743.333984375
tensor(10743.3447, grad_fn=<NegBackward0>) tensor(10743.3340, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10743.32421875
tensor(10743.3340, grad_fn=<NegBackward0>) tensor(10743.3242, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10743.3154296875
tensor(10743.3242, grad_fn=<NegBackward0>) tensor(10743.3154, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10743.306640625
tensor(10743.3154, grad_fn=<NegBackward0>) tensor(10743.3066, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10743.3017578125
tensor(10743.3066, grad_fn=<NegBackward0>) tensor(10743.3018, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10743.294921875
tensor(10743.3018, grad_fn=<NegBackward0>) tensor(10743.2949, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10743.2890625
tensor(10743.2949, grad_fn=<NegBackward0>) tensor(10743.2891, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10743.287109375
tensor(10743.2891, grad_fn=<NegBackward0>) tensor(10743.2871, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10743.283203125
tensor(10743.2871, grad_fn=<NegBackward0>) tensor(10743.2832, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10743.279296875
tensor(10743.2832, grad_fn=<NegBackward0>) tensor(10743.2793, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10743.2763671875
tensor(10743.2793, grad_fn=<NegBackward0>) tensor(10743.2764, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10743.2734375
tensor(10743.2764, grad_fn=<NegBackward0>) tensor(10743.2734, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10743.271484375
tensor(10743.2734, grad_fn=<NegBackward0>) tensor(10743.2715, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10743.26953125
tensor(10743.2715, grad_fn=<NegBackward0>) tensor(10743.2695, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10743.267578125
tensor(10743.2695, grad_fn=<NegBackward0>) tensor(10743.2676, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10743.265625
tensor(10743.2676, grad_fn=<NegBackward0>) tensor(10743.2656, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10743.263671875
tensor(10743.2656, grad_fn=<NegBackward0>) tensor(10743.2637, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10743.26171875
tensor(10743.2637, grad_fn=<NegBackward0>) tensor(10743.2617, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10743.259765625
tensor(10743.2617, grad_fn=<NegBackward0>) tensor(10743.2598, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10743.2587890625
tensor(10743.2598, grad_fn=<NegBackward0>) tensor(10743.2588, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10743.2568359375
tensor(10743.2588, grad_fn=<NegBackward0>) tensor(10743.2568, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10743.2568359375
tensor(10743.2568, grad_fn=<NegBackward0>) tensor(10743.2568, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10743.2568359375
tensor(10743.2568, grad_fn=<NegBackward0>) tensor(10743.2568, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10743.25390625
tensor(10743.2568, grad_fn=<NegBackward0>) tensor(10743.2539, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10743.25390625
tensor(10743.2539, grad_fn=<NegBackward0>) tensor(10743.2539, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10743.25390625
tensor(10743.2539, grad_fn=<NegBackward0>) tensor(10743.2539, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10743.251953125
tensor(10743.2539, grad_fn=<NegBackward0>) tensor(10743.2520, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10743.251953125
tensor(10743.2520, grad_fn=<NegBackward0>) tensor(10743.2520, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10743.251953125
tensor(10743.2520, grad_fn=<NegBackward0>) tensor(10743.2520, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10743.25
tensor(10743.2520, grad_fn=<NegBackward0>) tensor(10743.2500, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10743.2490234375
tensor(10743.2500, grad_fn=<NegBackward0>) tensor(10743.2490, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10743.2490234375
tensor(10743.2490, grad_fn=<NegBackward0>) tensor(10743.2490, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10743.248046875
tensor(10743.2490, grad_fn=<NegBackward0>) tensor(10743.2480, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10743.2470703125
tensor(10743.2480, grad_fn=<NegBackward0>) tensor(10743.2471, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10743.248046875
tensor(10743.2471, grad_fn=<NegBackward0>) tensor(10743.2480, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10743.24609375
tensor(10743.2471, grad_fn=<NegBackward0>) tensor(10743.2461, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10743.24609375
tensor(10743.2461, grad_fn=<NegBackward0>) tensor(10743.2461, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10743.2470703125
tensor(10743.2461, grad_fn=<NegBackward0>) tensor(10743.2471, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10743.24609375
tensor(10743.2461, grad_fn=<NegBackward0>) tensor(10743.2461, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10743.2451171875
tensor(10743.2461, grad_fn=<NegBackward0>) tensor(10743.2451, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10743.2451171875
tensor(10743.2451, grad_fn=<NegBackward0>) tensor(10743.2451, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10743.2451171875
tensor(10743.2451, grad_fn=<NegBackward0>) tensor(10743.2451, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10743.2451171875
tensor(10743.2451, grad_fn=<NegBackward0>) tensor(10743.2451, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10743.2451171875
tensor(10743.2451, grad_fn=<NegBackward0>) tensor(10743.2451, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10743.244140625
tensor(10743.2451, grad_fn=<NegBackward0>) tensor(10743.2441, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10743.2431640625
tensor(10743.2441, grad_fn=<NegBackward0>) tensor(10743.2432, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10743.244140625
tensor(10743.2432, grad_fn=<NegBackward0>) tensor(10743.2441, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10743.2421875
tensor(10743.2432, grad_fn=<NegBackward0>) tensor(10743.2422, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10743.2421875
tensor(10743.2422, grad_fn=<NegBackward0>) tensor(10743.2422, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10743.2421875
tensor(10743.2422, grad_fn=<NegBackward0>) tensor(10743.2422, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10743.244140625
tensor(10743.2422, grad_fn=<NegBackward0>) tensor(10743.2441, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10743.2421875
tensor(10743.2422, grad_fn=<NegBackward0>) tensor(10743.2422, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10743.240234375
tensor(10743.2422, grad_fn=<NegBackward0>) tensor(10743.2402, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10743.2412109375
tensor(10743.2402, grad_fn=<NegBackward0>) tensor(10743.2412, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10743.2421875
tensor(10743.2402, grad_fn=<NegBackward0>) tensor(10743.2422, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -10743.240234375
tensor(10743.2402, grad_fn=<NegBackward0>) tensor(10743.2402, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10743.240234375
tensor(10743.2402, grad_fn=<NegBackward0>) tensor(10743.2402, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10743.24609375
tensor(10743.2402, grad_fn=<NegBackward0>) tensor(10743.2461, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -10743.2412109375
tensor(10743.2402, grad_fn=<NegBackward0>) tensor(10743.2412, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -10743.240234375
tensor(10743.2402, grad_fn=<NegBackward0>) tensor(10743.2402, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10743.26953125
tensor(10743.2402, grad_fn=<NegBackward0>) tensor(10743.2695, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10743.240234375
tensor(10743.2402, grad_fn=<NegBackward0>) tensor(10743.2402, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10743.244140625
tensor(10743.2402, grad_fn=<NegBackward0>) tensor(10743.2441, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10743.240234375
tensor(10743.2402, grad_fn=<NegBackward0>) tensor(10743.2402, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10743.240234375
tensor(10743.2402, grad_fn=<NegBackward0>) tensor(10743.2402, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10743.23828125
tensor(10743.2402, grad_fn=<NegBackward0>) tensor(10743.2383, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10743.2490234375
tensor(10743.2383, grad_fn=<NegBackward0>) tensor(10743.2490, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -10743.240234375
tensor(10743.2383, grad_fn=<NegBackward0>) tensor(10743.2402, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -10743.240234375
tensor(10743.2383, grad_fn=<NegBackward0>) tensor(10743.2402, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -10743.240234375
tensor(10743.2383, grad_fn=<NegBackward0>) tensor(10743.2402, grad_fn=<NegBackward0>)
4
Iteration 9800: Loss = -10743.23828125
tensor(10743.2383, grad_fn=<NegBackward0>) tensor(10743.2383, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10743.2392578125
tensor(10743.2383, grad_fn=<NegBackward0>) tensor(10743.2393, grad_fn=<NegBackward0>)
1
pi: tensor([[9.9981e-01, 1.8597e-04],
        [4.0930e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0183, 0.9817], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2003, 0.1260],
         [0.7168, 0.1564]],

        [[0.5204, 0.2270],
         [0.6714, 0.5988]],

        [[0.5414, 0.2290],
         [0.6414, 0.6203]],

        [[0.6333, 0.0567],
         [0.7179, 0.5561]],

        [[0.5756, 0.2038],
         [0.7308, 0.5708]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.01382061954603653
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.00035341041046094813
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.01171303074670571
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: 0.0062187062454228965
Average Adjusted Rand Index: 0.005500900640624464
[0.0062187062454228965, 0.0062187062454228965] [0.005500900640624464, 0.005500900640624464] [10743.240234375, 10743.23828125]
-------------------------------------
This iteration is 81
True Objective function: Loss = -10914.414996766836
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21562.59765625
inf tensor(21562.5977, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11060.2275390625
tensor(21562.5977, grad_fn=<NegBackward0>) tensor(11060.2275, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11059.42578125
tensor(11060.2275, grad_fn=<NegBackward0>) tensor(11059.4258, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11059.1708984375
tensor(11059.4258, grad_fn=<NegBackward0>) tensor(11059.1709, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11058.87109375
tensor(11059.1709, grad_fn=<NegBackward0>) tensor(11058.8711, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11058.8046875
tensor(11058.8711, grad_fn=<NegBackward0>) tensor(11058.8047, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11058.7783203125
tensor(11058.8047, grad_fn=<NegBackward0>) tensor(11058.7783, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11058.759765625
tensor(11058.7783, grad_fn=<NegBackward0>) tensor(11058.7598, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11058.7421875
tensor(11058.7598, grad_fn=<NegBackward0>) tensor(11058.7422, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11058.7314453125
tensor(11058.7422, grad_fn=<NegBackward0>) tensor(11058.7314, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11058.7197265625
tensor(11058.7314, grad_fn=<NegBackward0>) tensor(11058.7197, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11058.7080078125
tensor(11058.7197, grad_fn=<NegBackward0>) tensor(11058.7080, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11058.697265625
tensor(11058.7080, grad_fn=<NegBackward0>) tensor(11058.6973, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11058.6865234375
tensor(11058.6973, grad_fn=<NegBackward0>) tensor(11058.6865, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11058.6748046875
tensor(11058.6865, grad_fn=<NegBackward0>) tensor(11058.6748, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11058.6640625
tensor(11058.6748, grad_fn=<NegBackward0>) tensor(11058.6641, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11058.6513671875
tensor(11058.6641, grad_fn=<NegBackward0>) tensor(11058.6514, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11058.6396484375
tensor(11058.6514, grad_fn=<NegBackward0>) tensor(11058.6396, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11058.625
tensor(11058.6396, grad_fn=<NegBackward0>) tensor(11058.6250, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11058.60546875
tensor(11058.6250, grad_fn=<NegBackward0>) tensor(11058.6055, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11058.5849609375
tensor(11058.6055, grad_fn=<NegBackward0>) tensor(11058.5850, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11058.55859375
tensor(11058.5850, grad_fn=<NegBackward0>) tensor(11058.5586, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11058.5234375
tensor(11058.5586, grad_fn=<NegBackward0>) tensor(11058.5234, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11058.4794921875
tensor(11058.5234, grad_fn=<NegBackward0>) tensor(11058.4795, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11058.427734375
tensor(11058.4795, grad_fn=<NegBackward0>) tensor(11058.4277, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11058.3603515625
tensor(11058.4277, grad_fn=<NegBackward0>) tensor(11058.3604, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11058.2724609375
tensor(11058.3604, grad_fn=<NegBackward0>) tensor(11058.2725, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11058.095703125
tensor(11058.2725, grad_fn=<NegBackward0>) tensor(11058.0957, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10883.361328125
tensor(11058.0957, grad_fn=<NegBackward0>) tensor(10883.3613, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10876.40625
tensor(10883.3613, grad_fn=<NegBackward0>) tensor(10876.4062, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10876.0751953125
tensor(10876.4062, grad_fn=<NegBackward0>) tensor(10876.0752, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10876.0361328125
tensor(10876.0752, grad_fn=<NegBackward0>) tensor(10876.0361, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10876.0048828125
tensor(10876.0361, grad_fn=<NegBackward0>) tensor(10876.0049, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10875.974609375
tensor(10876.0049, grad_fn=<NegBackward0>) tensor(10875.9746, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10875.95703125
tensor(10875.9746, grad_fn=<NegBackward0>) tensor(10875.9570, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10875.9033203125
tensor(10875.9570, grad_fn=<NegBackward0>) tensor(10875.9033, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10875.8857421875
tensor(10875.9033, grad_fn=<NegBackward0>) tensor(10875.8857, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10875.8798828125
tensor(10875.8857, grad_fn=<NegBackward0>) tensor(10875.8799, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10875.875
tensor(10875.8799, grad_fn=<NegBackward0>) tensor(10875.8750, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10875.8759765625
tensor(10875.8750, grad_fn=<NegBackward0>) tensor(10875.8760, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -10875.8515625
tensor(10875.8750, grad_fn=<NegBackward0>) tensor(10875.8516, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10875.8310546875
tensor(10875.8516, grad_fn=<NegBackward0>) tensor(10875.8311, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10875.8251953125
tensor(10875.8311, grad_fn=<NegBackward0>) tensor(10875.8252, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10875.8046875
tensor(10875.8252, grad_fn=<NegBackward0>) tensor(10875.8047, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10875.8056640625
tensor(10875.8047, grad_fn=<NegBackward0>) tensor(10875.8057, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10875.794921875
tensor(10875.8047, grad_fn=<NegBackward0>) tensor(10875.7949, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10875.7734375
tensor(10875.7949, grad_fn=<NegBackward0>) tensor(10875.7734, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10875.7822265625
tensor(10875.7734, grad_fn=<NegBackward0>) tensor(10875.7822, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10875.771484375
tensor(10875.7734, grad_fn=<NegBackward0>) tensor(10875.7715, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10875.7705078125
tensor(10875.7715, grad_fn=<NegBackward0>) tensor(10875.7705, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10875.771484375
tensor(10875.7705, grad_fn=<NegBackward0>) tensor(10875.7715, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10875.76171875
tensor(10875.7705, grad_fn=<NegBackward0>) tensor(10875.7617, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10875.76171875
tensor(10875.7617, grad_fn=<NegBackward0>) tensor(10875.7617, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10875.76171875
tensor(10875.7617, grad_fn=<NegBackward0>) tensor(10875.7617, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10875.759765625
tensor(10875.7617, grad_fn=<NegBackward0>) tensor(10875.7598, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10875.7529296875
tensor(10875.7598, grad_fn=<NegBackward0>) tensor(10875.7529, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10875.7197265625
tensor(10875.7529, grad_fn=<NegBackward0>) tensor(10875.7197, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10875.7158203125
tensor(10875.7197, grad_fn=<NegBackward0>) tensor(10875.7158, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10875.673828125
tensor(10875.7158, grad_fn=<NegBackward0>) tensor(10875.6738, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10875.6640625
tensor(10875.6738, grad_fn=<NegBackward0>) tensor(10875.6641, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10875.6640625
tensor(10875.6641, grad_fn=<NegBackward0>) tensor(10875.6641, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10875.6611328125
tensor(10875.6641, grad_fn=<NegBackward0>) tensor(10875.6611, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10875.6611328125
tensor(10875.6611, grad_fn=<NegBackward0>) tensor(10875.6611, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10875.66015625
tensor(10875.6611, grad_fn=<NegBackward0>) tensor(10875.6602, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10875.60546875
tensor(10875.6602, grad_fn=<NegBackward0>) tensor(10875.6055, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10875.603515625
tensor(10875.6055, grad_fn=<NegBackward0>) tensor(10875.6035, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10875.59765625
tensor(10875.6035, grad_fn=<NegBackward0>) tensor(10875.5977, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10875.037109375
tensor(10875.5977, grad_fn=<NegBackward0>) tensor(10875.0371, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10875.03515625
tensor(10875.0371, grad_fn=<NegBackward0>) tensor(10875.0352, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10875.0341796875
tensor(10875.0352, grad_fn=<NegBackward0>) tensor(10875.0342, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10875.0322265625
tensor(10875.0342, grad_fn=<NegBackward0>) tensor(10875.0322, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10875.0234375
tensor(10875.0322, grad_fn=<NegBackward0>) tensor(10875.0234, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10875.0224609375
tensor(10875.0234, grad_fn=<NegBackward0>) tensor(10875.0225, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10875.017578125
tensor(10875.0225, grad_fn=<NegBackward0>) tensor(10875.0176, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10875.0166015625
tensor(10875.0176, grad_fn=<NegBackward0>) tensor(10875.0166, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10875.0166015625
tensor(10875.0166, grad_fn=<NegBackward0>) tensor(10875.0166, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10875.1279296875
tensor(10875.0166, grad_fn=<NegBackward0>) tensor(10875.1279, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10875.015625
tensor(10875.0166, grad_fn=<NegBackward0>) tensor(10875.0156, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10875.0380859375
tensor(10875.0156, grad_fn=<NegBackward0>) tensor(10875.0381, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10875.0166015625
tensor(10875.0156, grad_fn=<NegBackward0>) tensor(10875.0166, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -10875.07421875
tensor(10875.0156, grad_fn=<NegBackward0>) tensor(10875.0742, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -10875.013671875
tensor(10875.0156, grad_fn=<NegBackward0>) tensor(10875.0137, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10875.0302734375
tensor(10875.0137, grad_fn=<NegBackward0>) tensor(10875.0303, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10875.013671875
tensor(10875.0137, grad_fn=<NegBackward0>) tensor(10875.0137, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10874.5146484375
tensor(10875.0137, grad_fn=<NegBackward0>) tensor(10874.5146, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10874.517578125
tensor(10874.5146, grad_fn=<NegBackward0>) tensor(10874.5176, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -10874.5146484375
tensor(10874.5146, grad_fn=<NegBackward0>) tensor(10874.5146, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10874.5185546875
tensor(10874.5146, grad_fn=<NegBackward0>) tensor(10874.5186, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -10874.5107421875
tensor(10874.5146, grad_fn=<NegBackward0>) tensor(10874.5107, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10874.5078125
tensor(10874.5107, grad_fn=<NegBackward0>) tensor(10874.5078, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10874.5087890625
tensor(10874.5078, grad_fn=<NegBackward0>) tensor(10874.5088, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10874.5078125
tensor(10874.5078, grad_fn=<NegBackward0>) tensor(10874.5078, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10874.5087890625
tensor(10874.5078, grad_fn=<NegBackward0>) tensor(10874.5088, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -10874.5078125
tensor(10874.5078, grad_fn=<NegBackward0>) tensor(10874.5078, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10874.5078125
tensor(10874.5078, grad_fn=<NegBackward0>) tensor(10874.5078, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10874.5107421875
tensor(10874.5078, grad_fn=<NegBackward0>) tensor(10874.5107, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -10874.513671875
tensor(10874.5078, grad_fn=<NegBackward0>) tensor(10874.5137, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -10874.509765625
tensor(10874.5078, grad_fn=<NegBackward0>) tensor(10874.5098, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -10874.515625
tensor(10874.5078, grad_fn=<NegBackward0>) tensor(10874.5156, grad_fn=<NegBackward0>)
4
Iteration 9900: Loss = -10874.505859375
tensor(10874.5078, grad_fn=<NegBackward0>) tensor(10874.5059, grad_fn=<NegBackward0>)
pi: tensor([[0.7787, 0.2213],
        [0.2025, 0.7975]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5452, 0.4548], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2498, 0.0948],
         [0.5970, 0.2110]],

        [[0.6790, 0.0921],
         [0.5273, 0.5452]],

        [[0.5013, 0.0940],
         [0.6524, 0.6187]],

        [[0.7106, 0.0990],
         [0.6626, 0.7209]],

        [[0.6975, 0.1066],
         [0.5965, 0.7228]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 7
Adjusted Rand Index: 0.7369696969696969
time is 2
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 7
Adjusted Rand Index: 0.7369480537608971
time is 3
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448420005390695
Global Adjusted Rand Index: 0.8387331534064708
Average Adjusted Rand Index: 0.8399133783486853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20739.00390625
inf tensor(20739.0039, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11059.3125
tensor(20739.0039, grad_fn=<NegBackward0>) tensor(11059.3125, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11059.1484375
tensor(11059.3125, grad_fn=<NegBackward0>) tensor(11059.1484, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11059.072265625
tensor(11059.1484, grad_fn=<NegBackward0>) tensor(11059.0723, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11058.9296875
tensor(11059.0723, grad_fn=<NegBackward0>) tensor(11058.9297, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11058.8251953125
tensor(11058.9297, grad_fn=<NegBackward0>) tensor(11058.8252, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11058.80078125
tensor(11058.8252, grad_fn=<NegBackward0>) tensor(11058.8008, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11058.7763671875
tensor(11058.8008, grad_fn=<NegBackward0>) tensor(11058.7764, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11058.75
tensor(11058.7764, grad_fn=<NegBackward0>) tensor(11058.7500, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11058.70703125
tensor(11058.7500, grad_fn=<NegBackward0>) tensor(11058.7070, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11058.560546875
tensor(11058.7070, grad_fn=<NegBackward0>) tensor(11058.5605, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11057.5029296875
tensor(11058.5605, grad_fn=<NegBackward0>) tensor(11057.5029, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11055.935546875
tensor(11057.5029, grad_fn=<NegBackward0>) tensor(11055.9355, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11053.9716796875
tensor(11055.9355, grad_fn=<NegBackward0>) tensor(11053.9717, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11053.166015625
tensor(11053.9717, grad_fn=<NegBackward0>) tensor(11053.1660, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11052.75
tensor(11053.1660, grad_fn=<NegBackward0>) tensor(11052.7500, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11052.6455078125
tensor(11052.7500, grad_fn=<NegBackward0>) tensor(11052.6455, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11052.609375
tensor(11052.6455, grad_fn=<NegBackward0>) tensor(11052.6094, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11052.5869140625
tensor(11052.6094, grad_fn=<NegBackward0>) tensor(11052.5869, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11052.55859375
tensor(11052.5869, grad_fn=<NegBackward0>) tensor(11052.5586, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11052.5
tensor(11052.5586, grad_fn=<NegBackward0>) tensor(11052.5000, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11052.259765625
tensor(11052.5000, grad_fn=<NegBackward0>) tensor(11052.2598, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11047.4755859375
tensor(11052.2598, grad_fn=<NegBackward0>) tensor(11047.4756, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10894.69140625
tensor(11047.4756, grad_fn=<NegBackward0>) tensor(10894.6914, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10889.7216796875
tensor(10894.6914, grad_fn=<NegBackward0>) tensor(10889.7217, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10889.5078125
tensor(10889.7217, grad_fn=<NegBackward0>) tensor(10889.5078, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10889.466796875
tensor(10889.5078, grad_fn=<NegBackward0>) tensor(10889.4668, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10889.4111328125
tensor(10889.4668, grad_fn=<NegBackward0>) tensor(10889.4111, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10889.392578125
tensor(10889.4111, grad_fn=<NegBackward0>) tensor(10889.3926, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10889.3759765625
tensor(10889.3926, grad_fn=<NegBackward0>) tensor(10889.3760, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10889.37109375
tensor(10889.3760, grad_fn=<NegBackward0>) tensor(10889.3711, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10889.3427734375
tensor(10889.3711, grad_fn=<NegBackward0>) tensor(10889.3428, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10883.630859375
tensor(10889.3428, grad_fn=<NegBackward0>) tensor(10883.6309, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10883.6279296875
tensor(10883.6309, grad_fn=<NegBackward0>) tensor(10883.6279, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10883.6240234375
tensor(10883.6279, grad_fn=<NegBackward0>) tensor(10883.6240, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10883.6220703125
tensor(10883.6240, grad_fn=<NegBackward0>) tensor(10883.6221, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10883.6162109375
tensor(10883.6221, grad_fn=<NegBackward0>) tensor(10883.6162, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10883.61328125
tensor(10883.6162, grad_fn=<NegBackward0>) tensor(10883.6133, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10883.611328125
tensor(10883.6133, grad_fn=<NegBackward0>) tensor(10883.6113, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10883.607421875
tensor(10883.6113, grad_fn=<NegBackward0>) tensor(10883.6074, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10879.1513671875
tensor(10883.6074, grad_fn=<NegBackward0>) tensor(10879.1514, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10879.1474609375
tensor(10879.1514, grad_fn=<NegBackward0>) tensor(10879.1475, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10879.1455078125
tensor(10879.1475, grad_fn=<NegBackward0>) tensor(10879.1455, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10879.1455078125
tensor(10879.1455, grad_fn=<NegBackward0>) tensor(10879.1455, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10879.14453125
tensor(10879.1455, grad_fn=<NegBackward0>) tensor(10879.1445, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10879.142578125
tensor(10879.1445, grad_fn=<NegBackward0>) tensor(10879.1426, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10879.142578125
tensor(10879.1426, grad_fn=<NegBackward0>) tensor(10879.1426, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10879.1416015625
tensor(10879.1426, grad_fn=<NegBackward0>) tensor(10879.1416, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10879.1357421875
tensor(10879.1416, grad_fn=<NegBackward0>) tensor(10879.1357, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10877.6728515625
tensor(10879.1357, grad_fn=<NegBackward0>) tensor(10877.6729, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10877.662109375
tensor(10877.6729, grad_fn=<NegBackward0>) tensor(10877.6621, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10877.662109375
tensor(10877.6621, grad_fn=<NegBackward0>) tensor(10877.6621, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10877.6611328125
tensor(10877.6621, grad_fn=<NegBackward0>) tensor(10877.6611, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10877.66015625
tensor(10877.6611, grad_fn=<NegBackward0>) tensor(10877.6602, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10877.6591796875
tensor(10877.6602, grad_fn=<NegBackward0>) tensor(10877.6592, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10877.658203125
tensor(10877.6592, grad_fn=<NegBackward0>) tensor(10877.6582, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10877.6650390625
tensor(10877.6582, grad_fn=<NegBackward0>) tensor(10877.6650, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10877.64453125
tensor(10877.6582, grad_fn=<NegBackward0>) tensor(10877.6445, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10877.6494140625
tensor(10877.6445, grad_fn=<NegBackward0>) tensor(10877.6494, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10877.6435546875
tensor(10877.6445, grad_fn=<NegBackward0>) tensor(10877.6436, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10877.642578125
tensor(10877.6436, grad_fn=<NegBackward0>) tensor(10877.6426, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10877.6435546875
tensor(10877.6426, grad_fn=<NegBackward0>) tensor(10877.6436, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10877.642578125
tensor(10877.6426, grad_fn=<NegBackward0>) tensor(10877.6426, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10877.6416015625
tensor(10877.6426, grad_fn=<NegBackward0>) tensor(10877.6416, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10877.6396484375
tensor(10877.6416, grad_fn=<NegBackward0>) tensor(10877.6396, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10877.6396484375
tensor(10877.6396, grad_fn=<NegBackward0>) tensor(10877.6396, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10877.6396484375
tensor(10877.6396, grad_fn=<NegBackward0>) tensor(10877.6396, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10877.640625
tensor(10877.6396, grad_fn=<NegBackward0>) tensor(10877.6406, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10877.638671875
tensor(10877.6396, grad_fn=<NegBackward0>) tensor(10877.6387, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10877.6396484375
tensor(10877.6387, grad_fn=<NegBackward0>) tensor(10877.6396, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10877.669921875
tensor(10877.6387, grad_fn=<NegBackward0>) tensor(10877.6699, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -10877.638671875
tensor(10877.6387, grad_fn=<NegBackward0>) tensor(10877.6387, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10877.6513671875
tensor(10877.6387, grad_fn=<NegBackward0>) tensor(10877.6514, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10877.642578125
tensor(10877.6387, grad_fn=<NegBackward0>) tensor(10877.6426, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -10877.6357421875
tensor(10877.6387, grad_fn=<NegBackward0>) tensor(10877.6357, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10877.6357421875
tensor(10877.6357, grad_fn=<NegBackward0>) tensor(10877.6357, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10877.646484375
tensor(10877.6357, grad_fn=<NegBackward0>) tensor(10877.6465, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10877.634765625
tensor(10877.6357, grad_fn=<NegBackward0>) tensor(10877.6348, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10877.634765625
tensor(10877.6348, grad_fn=<NegBackward0>) tensor(10877.6348, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10877.64453125
tensor(10877.6348, grad_fn=<NegBackward0>) tensor(10877.6445, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10877.6337890625
tensor(10877.6348, grad_fn=<NegBackward0>) tensor(10877.6338, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10877.6337890625
tensor(10877.6338, grad_fn=<NegBackward0>) tensor(10877.6338, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10877.6337890625
tensor(10877.6338, grad_fn=<NegBackward0>) tensor(10877.6338, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10877.6337890625
tensor(10877.6338, grad_fn=<NegBackward0>) tensor(10877.6338, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10877.6337890625
tensor(10877.6338, grad_fn=<NegBackward0>) tensor(10877.6338, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10877.6201171875
tensor(10877.6338, grad_fn=<NegBackward0>) tensor(10877.6201, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10877.62109375
tensor(10877.6201, grad_fn=<NegBackward0>) tensor(10877.6211, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10877.619140625
tensor(10877.6201, grad_fn=<NegBackward0>) tensor(10877.6191, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10877.623046875
tensor(10877.6191, grad_fn=<NegBackward0>) tensor(10877.6230, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10877.630859375
tensor(10877.6191, grad_fn=<NegBackward0>) tensor(10877.6309, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -10877.6201171875
tensor(10877.6191, grad_fn=<NegBackward0>) tensor(10877.6201, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -10877.6201171875
tensor(10877.6191, grad_fn=<NegBackward0>) tensor(10877.6201, grad_fn=<NegBackward0>)
4
Iteration 9200: Loss = -10874.32421875
tensor(10877.6191, grad_fn=<NegBackward0>) tensor(10874.3242, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10874.3232421875
tensor(10874.3242, grad_fn=<NegBackward0>) tensor(10874.3232, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10874.3994140625
tensor(10874.3232, grad_fn=<NegBackward0>) tensor(10874.3994, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -10874.3212890625
tensor(10874.3232, grad_fn=<NegBackward0>) tensor(10874.3213, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10874.3271484375
tensor(10874.3213, grad_fn=<NegBackward0>) tensor(10874.3271, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -10874.3203125
tensor(10874.3213, grad_fn=<NegBackward0>) tensor(10874.3203, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10874.3427734375
tensor(10874.3203, grad_fn=<NegBackward0>) tensor(10874.3428, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -10874.3203125
tensor(10874.3203, grad_fn=<NegBackward0>) tensor(10874.3203, grad_fn=<NegBackward0>)
pi: tensor([[0.7809, 0.2191],
        [0.2048, 0.7952]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5441, 0.4559], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2491, 0.0943],
         [0.5034, 0.2117]],

        [[0.6288, 0.0920],
         [0.6928, 0.6576]],

        [[0.5500, 0.0938],
         [0.6801, 0.6327]],

        [[0.7101, 0.0984],
         [0.5155, 0.5478]],

        [[0.6952, 0.1064],
         [0.5388, 0.5989]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 7
Adjusted Rand Index: 0.7369696969696969
time is 2
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 7
Adjusted Rand Index: 0.7369480537608971
time is 3
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448420005390695
Global Adjusted Rand Index: 0.8387331534064708
Average Adjusted Rand Index: 0.8399133783486853
[0.8387331534064708, 0.8387331534064708] [0.8399133783486853, 0.8399133783486853] [10874.53125, 10874.3203125]
-------------------------------------
This iteration is 82
True Objective function: Loss = -10724.381260022208
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22901.3828125
inf tensor(22901.3828, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10764.28515625
tensor(22901.3828, grad_fn=<NegBackward0>) tensor(10764.2852, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10763.666015625
tensor(10764.2852, grad_fn=<NegBackward0>) tensor(10763.6660, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10763.5439453125
tensor(10763.6660, grad_fn=<NegBackward0>) tensor(10763.5439, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10763.4921875
tensor(10763.5439, grad_fn=<NegBackward0>) tensor(10763.4922, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10763.4580078125
tensor(10763.4922, grad_fn=<NegBackward0>) tensor(10763.4580, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10763.431640625
tensor(10763.4580, grad_fn=<NegBackward0>) tensor(10763.4316, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10763.40625
tensor(10763.4316, grad_fn=<NegBackward0>) tensor(10763.4062, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10763.3876953125
tensor(10763.4062, grad_fn=<NegBackward0>) tensor(10763.3877, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10763.3701171875
tensor(10763.3877, grad_fn=<NegBackward0>) tensor(10763.3701, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10763.3544921875
tensor(10763.3701, grad_fn=<NegBackward0>) tensor(10763.3545, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10763.3408203125
tensor(10763.3545, grad_fn=<NegBackward0>) tensor(10763.3408, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10763.3271484375
tensor(10763.3408, grad_fn=<NegBackward0>) tensor(10763.3271, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10763.31640625
tensor(10763.3271, grad_fn=<NegBackward0>) tensor(10763.3164, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10763.3056640625
tensor(10763.3164, grad_fn=<NegBackward0>) tensor(10763.3057, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10763.2958984375
tensor(10763.3057, grad_fn=<NegBackward0>) tensor(10763.2959, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10763.287109375
tensor(10763.2959, grad_fn=<NegBackward0>) tensor(10763.2871, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10763.275390625
tensor(10763.2871, grad_fn=<NegBackward0>) tensor(10763.2754, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10763.267578125
tensor(10763.2754, grad_fn=<NegBackward0>) tensor(10763.2676, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10763.2587890625
tensor(10763.2676, grad_fn=<NegBackward0>) tensor(10763.2588, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10763.2470703125
tensor(10763.2588, grad_fn=<NegBackward0>) tensor(10763.2471, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10763.236328125
tensor(10763.2471, grad_fn=<NegBackward0>) tensor(10763.2363, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10763.2236328125
tensor(10763.2363, grad_fn=<NegBackward0>) tensor(10763.2236, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10763.208984375
tensor(10763.2236, grad_fn=<NegBackward0>) tensor(10763.2090, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10763.193359375
tensor(10763.2090, grad_fn=<NegBackward0>) tensor(10763.1934, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10763.177734375
tensor(10763.1934, grad_fn=<NegBackward0>) tensor(10763.1777, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10763.158203125
tensor(10763.1777, grad_fn=<NegBackward0>) tensor(10763.1582, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10763.1396484375
tensor(10763.1582, grad_fn=<NegBackward0>) tensor(10763.1396, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10763.1240234375
tensor(10763.1396, grad_fn=<NegBackward0>) tensor(10763.1240, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10763.1083984375
tensor(10763.1240, grad_fn=<NegBackward0>) tensor(10763.1084, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10763.0927734375
tensor(10763.1084, grad_fn=<NegBackward0>) tensor(10763.0928, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10763.0771484375
tensor(10763.0928, grad_fn=<NegBackward0>) tensor(10763.0771, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10763.0654296875
tensor(10763.0771, grad_fn=<NegBackward0>) tensor(10763.0654, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10763.052734375
tensor(10763.0654, grad_fn=<NegBackward0>) tensor(10763.0527, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10763.041015625
tensor(10763.0527, grad_fn=<NegBackward0>) tensor(10763.0410, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10763.033203125
tensor(10763.0410, grad_fn=<NegBackward0>) tensor(10763.0332, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10763.0234375
tensor(10763.0332, grad_fn=<NegBackward0>) tensor(10763.0234, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10763.015625
tensor(10763.0234, grad_fn=<NegBackward0>) tensor(10763.0156, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10763.0087890625
tensor(10763.0156, grad_fn=<NegBackward0>) tensor(10763.0088, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10763.0009765625
tensor(10763.0088, grad_fn=<NegBackward0>) tensor(10763.0010, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10762.994140625
tensor(10763.0010, grad_fn=<NegBackward0>) tensor(10762.9941, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10762.986328125
tensor(10762.9941, grad_fn=<NegBackward0>) tensor(10762.9863, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10762.9794921875
tensor(10762.9863, grad_fn=<NegBackward0>) tensor(10762.9795, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10762.9716796875
tensor(10762.9795, grad_fn=<NegBackward0>) tensor(10762.9717, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10762.9619140625
tensor(10762.9717, grad_fn=<NegBackward0>) tensor(10762.9619, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10762.9482421875
tensor(10762.9619, grad_fn=<NegBackward0>) tensor(10762.9482, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10762.9140625
tensor(10762.9482, grad_fn=<NegBackward0>) tensor(10762.9141, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10762.841796875
tensor(10762.9141, grad_fn=<NegBackward0>) tensor(10762.8418, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10762.75390625
tensor(10762.8418, grad_fn=<NegBackward0>) tensor(10762.7539, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10762.7236328125
tensor(10762.7539, grad_fn=<NegBackward0>) tensor(10762.7236, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10762.4033203125
tensor(10762.7236, grad_fn=<NegBackward0>) tensor(10762.4033, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10761.8564453125
tensor(10762.4033, grad_fn=<NegBackward0>) tensor(10761.8564, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10761.6884765625
tensor(10761.8564, grad_fn=<NegBackward0>) tensor(10761.6885, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10759.810546875
tensor(10761.6885, grad_fn=<NegBackward0>) tensor(10759.8105, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10758.01171875
tensor(10759.8105, grad_fn=<NegBackward0>) tensor(10758.0117, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10757.4697265625
tensor(10758.0117, grad_fn=<NegBackward0>) tensor(10757.4697, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10757.27734375
tensor(10757.4697, grad_fn=<NegBackward0>) tensor(10757.2773, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10757.1484375
tensor(10757.2773, grad_fn=<NegBackward0>) tensor(10757.1484, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10757.0537109375
tensor(10757.1484, grad_fn=<NegBackward0>) tensor(10757.0537, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10756.9833984375
tensor(10757.0537, grad_fn=<NegBackward0>) tensor(10756.9834, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10756.9296875
tensor(10756.9834, grad_fn=<NegBackward0>) tensor(10756.9297, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10756.888671875
tensor(10756.9297, grad_fn=<NegBackward0>) tensor(10756.8887, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10756.8564453125
tensor(10756.8887, grad_fn=<NegBackward0>) tensor(10756.8564, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10756.8310546875
tensor(10756.8564, grad_fn=<NegBackward0>) tensor(10756.8311, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10756.8095703125
tensor(10756.8311, grad_fn=<NegBackward0>) tensor(10756.8096, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10756.79296875
tensor(10756.8096, grad_fn=<NegBackward0>) tensor(10756.7930, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10756.7783203125
tensor(10756.7930, grad_fn=<NegBackward0>) tensor(10756.7783, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10756.765625
tensor(10756.7783, grad_fn=<NegBackward0>) tensor(10756.7656, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10756.7568359375
tensor(10756.7656, grad_fn=<NegBackward0>) tensor(10756.7568, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10756.7490234375
tensor(10756.7568, grad_fn=<NegBackward0>) tensor(10756.7490, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10756.7392578125
tensor(10756.7490, grad_fn=<NegBackward0>) tensor(10756.7393, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10756.7333984375
tensor(10756.7393, grad_fn=<NegBackward0>) tensor(10756.7334, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10756.7265625
tensor(10756.7334, grad_fn=<NegBackward0>) tensor(10756.7266, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10756.72265625
tensor(10756.7266, grad_fn=<NegBackward0>) tensor(10756.7227, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10756.7294921875
tensor(10756.7227, grad_fn=<NegBackward0>) tensor(10756.7295, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10756.712890625
tensor(10756.7227, grad_fn=<NegBackward0>) tensor(10756.7129, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10756.7109375
tensor(10756.7129, grad_fn=<NegBackward0>) tensor(10756.7109, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10756.7080078125
tensor(10756.7109, grad_fn=<NegBackward0>) tensor(10756.7080, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10756.703125
tensor(10756.7080, grad_fn=<NegBackward0>) tensor(10756.7031, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10756.7021484375
tensor(10756.7031, grad_fn=<NegBackward0>) tensor(10756.7021, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10756.697265625
tensor(10756.7021, grad_fn=<NegBackward0>) tensor(10756.6973, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10756.6982421875
tensor(10756.6973, grad_fn=<NegBackward0>) tensor(10756.6982, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10756.6943359375
tensor(10756.6973, grad_fn=<NegBackward0>) tensor(10756.6943, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10756.69140625
tensor(10756.6943, grad_fn=<NegBackward0>) tensor(10756.6914, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10756.6904296875
tensor(10756.6914, grad_fn=<NegBackward0>) tensor(10756.6904, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10756.6884765625
tensor(10756.6904, grad_fn=<NegBackward0>) tensor(10756.6885, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10756.6865234375
tensor(10756.6885, grad_fn=<NegBackward0>) tensor(10756.6865, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10756.6865234375
tensor(10756.6865, grad_fn=<NegBackward0>) tensor(10756.6865, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10756.68359375
tensor(10756.6865, grad_fn=<NegBackward0>) tensor(10756.6836, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10756.6845703125
tensor(10756.6836, grad_fn=<NegBackward0>) tensor(10756.6846, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -10756.6806640625
tensor(10756.6836, grad_fn=<NegBackward0>) tensor(10756.6807, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10756.720703125
tensor(10756.6807, grad_fn=<NegBackward0>) tensor(10756.7207, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10756.6796875
tensor(10756.6807, grad_fn=<NegBackward0>) tensor(10756.6797, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10756.6787109375
tensor(10756.6797, grad_fn=<NegBackward0>) tensor(10756.6787, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10756.6787109375
tensor(10756.6787, grad_fn=<NegBackward0>) tensor(10756.6787, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10756.6767578125
tensor(10756.6787, grad_fn=<NegBackward0>) tensor(10756.6768, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10756.6767578125
tensor(10756.6768, grad_fn=<NegBackward0>) tensor(10756.6768, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10756.6748046875
tensor(10756.6768, grad_fn=<NegBackward0>) tensor(10756.6748, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10756.736328125
tensor(10756.6748, grad_fn=<NegBackward0>) tensor(10756.7363, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -10756.673828125
tensor(10756.6748, grad_fn=<NegBackward0>) tensor(10756.6738, grad_fn=<NegBackward0>)
pi: tensor([[9.9973e-01, 2.7010e-04],
        [2.7828e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0189, 0.9811], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0015, 0.1010],
         [0.7132, 0.1581]],

        [[0.5883, 0.1533],
         [0.6001, 0.5925]],

        [[0.6977, 0.1754],
         [0.5003, 0.5409]],

        [[0.6790, 0.0706],
         [0.6068, 0.7305]],

        [[0.6197, 0.0777],
         [0.6440, 0.6855]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
Global Adjusted Rand Index: -0.00021613327664538577
Average Adjusted Rand Index: -0.0007044836390510357
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22562.30078125
inf tensor(22562.3008, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10764.30859375
tensor(22562.3008, grad_fn=<NegBackward0>) tensor(10764.3086, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10763.6904296875
tensor(10764.3086, grad_fn=<NegBackward0>) tensor(10763.6904, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10763.5419921875
tensor(10763.6904, grad_fn=<NegBackward0>) tensor(10763.5420, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10763.470703125
tensor(10763.5420, grad_fn=<NegBackward0>) tensor(10763.4707, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10763.4287109375
tensor(10763.4707, grad_fn=<NegBackward0>) tensor(10763.4287, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10763.3984375
tensor(10763.4287, grad_fn=<NegBackward0>) tensor(10763.3984, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10763.376953125
tensor(10763.3984, grad_fn=<NegBackward0>) tensor(10763.3770, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10763.3583984375
tensor(10763.3770, grad_fn=<NegBackward0>) tensor(10763.3584, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10763.3408203125
tensor(10763.3584, grad_fn=<NegBackward0>) tensor(10763.3408, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10763.328125
tensor(10763.3408, grad_fn=<NegBackward0>) tensor(10763.3281, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10763.3125
tensor(10763.3281, grad_fn=<NegBackward0>) tensor(10763.3125, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10763.298828125
tensor(10763.3125, grad_fn=<NegBackward0>) tensor(10763.2988, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10763.28515625
tensor(10763.2988, grad_fn=<NegBackward0>) tensor(10763.2852, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10763.2705078125
tensor(10763.2852, grad_fn=<NegBackward0>) tensor(10763.2705, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10763.25390625
tensor(10763.2705, grad_fn=<NegBackward0>) tensor(10763.2539, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10763.240234375
tensor(10763.2539, grad_fn=<NegBackward0>) tensor(10763.2402, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10763.2265625
tensor(10763.2402, grad_fn=<NegBackward0>) tensor(10763.2266, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10763.2138671875
tensor(10763.2266, grad_fn=<NegBackward0>) tensor(10763.2139, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10763.2041015625
tensor(10763.2139, grad_fn=<NegBackward0>) tensor(10763.2041, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10763.19140625
tensor(10763.2041, grad_fn=<NegBackward0>) tensor(10763.1914, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10763.1806640625
tensor(10763.1914, grad_fn=<NegBackward0>) tensor(10763.1807, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10763.169921875
tensor(10763.1807, grad_fn=<NegBackward0>) tensor(10763.1699, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10763.1572265625
tensor(10763.1699, grad_fn=<NegBackward0>) tensor(10763.1572, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10763.1455078125
tensor(10763.1572, grad_fn=<NegBackward0>) tensor(10763.1455, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10763.134765625
tensor(10763.1455, grad_fn=<NegBackward0>) tensor(10763.1348, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10763.12109375
tensor(10763.1348, grad_fn=<NegBackward0>) tensor(10763.1211, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10763.109375
tensor(10763.1211, grad_fn=<NegBackward0>) tensor(10763.1094, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10763.095703125
tensor(10763.1094, grad_fn=<NegBackward0>) tensor(10763.0957, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10763.08203125
tensor(10763.0957, grad_fn=<NegBackward0>) tensor(10763.0820, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10763.06640625
tensor(10763.0820, grad_fn=<NegBackward0>) tensor(10763.0664, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10763.05078125
tensor(10763.0664, grad_fn=<NegBackward0>) tensor(10763.0508, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10763.0361328125
tensor(10763.0508, grad_fn=<NegBackward0>) tensor(10763.0361, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10763.01953125
tensor(10763.0361, grad_fn=<NegBackward0>) tensor(10763.0195, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10763.0048828125
tensor(10763.0195, grad_fn=<NegBackward0>) tensor(10763.0049, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10762.9951171875
tensor(10763.0049, grad_fn=<NegBackward0>) tensor(10762.9951, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10762.9794921875
tensor(10762.9951, grad_fn=<NegBackward0>) tensor(10762.9795, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10762.96875
tensor(10762.9795, grad_fn=<NegBackward0>) tensor(10762.9688, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10762.9541015625
tensor(10762.9688, grad_fn=<NegBackward0>) tensor(10762.9541, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10762.935546875
tensor(10762.9541, grad_fn=<NegBackward0>) tensor(10762.9355, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10762.9111328125
tensor(10762.9355, grad_fn=<NegBackward0>) tensor(10762.9111, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10762.8798828125
tensor(10762.9111, grad_fn=<NegBackward0>) tensor(10762.8799, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10762.8466796875
tensor(10762.8799, grad_fn=<NegBackward0>) tensor(10762.8467, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10762.806640625
tensor(10762.8467, grad_fn=<NegBackward0>) tensor(10762.8066, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10762.7607421875
tensor(10762.8066, grad_fn=<NegBackward0>) tensor(10762.7607, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10762.716796875
tensor(10762.7607, grad_fn=<NegBackward0>) tensor(10762.7168, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10762.640625
tensor(10762.7168, grad_fn=<NegBackward0>) tensor(10762.6406, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10762.5830078125
tensor(10762.6406, grad_fn=<NegBackward0>) tensor(10762.5830, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10762.5576171875
tensor(10762.5830, grad_fn=<NegBackward0>) tensor(10762.5576, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10762.5478515625
tensor(10762.5576, grad_fn=<NegBackward0>) tensor(10762.5479, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10762.546875
tensor(10762.5479, grad_fn=<NegBackward0>) tensor(10762.5469, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10762.544921875
tensor(10762.5469, grad_fn=<NegBackward0>) tensor(10762.5449, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10762.5439453125
tensor(10762.5449, grad_fn=<NegBackward0>) tensor(10762.5439, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10762.5458984375
tensor(10762.5439, grad_fn=<NegBackward0>) tensor(10762.5459, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10762.544921875
tensor(10762.5439, grad_fn=<NegBackward0>) tensor(10762.5449, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -10762.544921875
tensor(10762.5439, grad_fn=<NegBackward0>) tensor(10762.5449, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -10762.5439453125
tensor(10762.5439, grad_fn=<NegBackward0>) tensor(10762.5439, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10762.544921875
tensor(10762.5439, grad_fn=<NegBackward0>) tensor(10762.5449, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10762.5439453125
tensor(10762.5439, grad_fn=<NegBackward0>) tensor(10762.5439, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10762.544921875
tensor(10762.5439, grad_fn=<NegBackward0>) tensor(10762.5449, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10762.5439453125
tensor(10762.5439, grad_fn=<NegBackward0>) tensor(10762.5439, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10762.5439453125
tensor(10762.5439, grad_fn=<NegBackward0>) tensor(10762.5439, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10762.5439453125
tensor(10762.5439, grad_fn=<NegBackward0>) tensor(10762.5439, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10762.5439453125
tensor(10762.5439, grad_fn=<NegBackward0>) tensor(10762.5439, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10762.544921875
tensor(10762.5439, grad_fn=<NegBackward0>) tensor(10762.5449, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10762.5439453125
tensor(10762.5439, grad_fn=<NegBackward0>) tensor(10762.5439, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10762.544921875
tensor(10762.5439, grad_fn=<NegBackward0>) tensor(10762.5449, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10762.544921875
tensor(10762.5439, grad_fn=<NegBackward0>) tensor(10762.5449, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -10762.546875
tensor(10762.5439, grad_fn=<NegBackward0>) tensor(10762.5469, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -10762.5458984375
tensor(10762.5439, grad_fn=<NegBackward0>) tensor(10762.5459, grad_fn=<NegBackward0>)
4
Iteration 7000: Loss = -10762.5439453125
tensor(10762.5439, grad_fn=<NegBackward0>) tensor(10762.5439, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10762.54296875
tensor(10762.5439, grad_fn=<NegBackward0>) tensor(10762.5430, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10762.5439453125
tensor(10762.5430, grad_fn=<NegBackward0>) tensor(10762.5439, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10762.544921875
tensor(10762.5430, grad_fn=<NegBackward0>) tensor(10762.5449, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -10762.54296875
tensor(10762.5430, grad_fn=<NegBackward0>) tensor(10762.5430, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10762.5419921875
tensor(10762.5430, grad_fn=<NegBackward0>) tensor(10762.5420, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10762.54296875
tensor(10762.5420, grad_fn=<NegBackward0>) tensor(10762.5430, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10762.54296875
tensor(10762.5420, grad_fn=<NegBackward0>) tensor(10762.5430, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -10762.541015625
tensor(10762.5420, grad_fn=<NegBackward0>) tensor(10762.5410, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10762.5537109375
tensor(10762.5410, grad_fn=<NegBackward0>) tensor(10762.5537, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10762.5419921875
tensor(10762.5410, grad_fn=<NegBackward0>) tensor(10762.5420, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -10762.5400390625
tensor(10762.5410, grad_fn=<NegBackward0>) tensor(10762.5400, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10762.541015625
tensor(10762.5400, grad_fn=<NegBackward0>) tensor(10762.5410, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10762.5751953125
tensor(10762.5400, grad_fn=<NegBackward0>) tensor(10762.5752, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -10762.541015625
tensor(10762.5400, grad_fn=<NegBackward0>) tensor(10762.5410, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -10762.541015625
tensor(10762.5400, grad_fn=<NegBackward0>) tensor(10762.5410, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -10762.5380859375
tensor(10762.5400, grad_fn=<NegBackward0>) tensor(10762.5381, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10762.5390625
tensor(10762.5381, grad_fn=<NegBackward0>) tensor(10762.5391, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -10762.6240234375
tensor(10762.5381, grad_fn=<NegBackward0>) tensor(10762.6240, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -10762.5625
tensor(10762.5381, grad_fn=<NegBackward0>) tensor(10762.5625, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -10762.5341796875
tensor(10762.5381, grad_fn=<NegBackward0>) tensor(10762.5342, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10762.5341796875
tensor(10762.5342, grad_fn=<NegBackward0>) tensor(10762.5342, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10762.5322265625
tensor(10762.5342, grad_fn=<NegBackward0>) tensor(10762.5322, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10762.533203125
tensor(10762.5322, grad_fn=<NegBackward0>) tensor(10762.5332, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -10762.5322265625
tensor(10762.5322, grad_fn=<NegBackward0>) tensor(10762.5322, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10762.544921875
tensor(10762.5322, grad_fn=<NegBackward0>) tensor(10762.5449, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -10762.5390625
tensor(10762.5322, grad_fn=<NegBackward0>) tensor(10762.5391, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -10762.53125
tensor(10762.5322, grad_fn=<NegBackward0>) tensor(10762.5312, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10762.53125
tensor(10762.5312, grad_fn=<NegBackward0>) tensor(10762.5312, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10762.53125
tensor(10762.5312, grad_fn=<NegBackward0>) tensor(10762.5312, grad_fn=<NegBackward0>)
pi: tensor([[0.0783, 0.9217],
        [0.7133, 0.2867]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9871, 0.0129], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1536, 0.1301],
         [0.6656, 0.1612]],

        [[0.7232, 0.1601],
         [0.6245, 0.7284]],

        [[0.6844, 0.1555],
         [0.6907, 0.5456]],

        [[0.6561, 0.1564],
         [0.6780, 0.7159]],

        [[0.6753, 0.1588],
         [0.6426, 0.5030]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 63
Adjusted Rand Index: 0.06097423419649044
Global Adjusted Rand Index: 0.001158747886821917
Average Adjusted Rand Index: 0.012194846839298088
[-0.00021613327664538577, 0.001158747886821917] [-0.0007044836390510357, 0.012194846839298088] [10756.705078125, 10762.53125]
-------------------------------------
This iteration is 83
True Objective function: Loss = -10746.331927382842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20266.484375
inf tensor(20266.4844, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10845.4853515625
tensor(20266.4844, grad_fn=<NegBackward0>) tensor(10845.4854, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10845.3291015625
tensor(10845.4854, grad_fn=<NegBackward0>) tensor(10845.3291, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10845.2841796875
tensor(10845.3291, grad_fn=<NegBackward0>) tensor(10845.2842, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10845.244140625
tensor(10845.2842, grad_fn=<NegBackward0>) tensor(10845.2441, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10845.201171875
tensor(10845.2441, grad_fn=<NegBackward0>) tensor(10845.2012, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10845.158203125
tensor(10845.2012, grad_fn=<NegBackward0>) tensor(10845.1582, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10845.1220703125
tensor(10845.1582, grad_fn=<NegBackward0>) tensor(10845.1221, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10845.0947265625
tensor(10845.1221, grad_fn=<NegBackward0>) tensor(10845.0947, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10845.0732421875
tensor(10845.0947, grad_fn=<NegBackward0>) tensor(10845.0732, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10845.05859375
tensor(10845.0732, grad_fn=<NegBackward0>) tensor(10845.0586, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10845.044921875
tensor(10845.0586, grad_fn=<NegBackward0>) tensor(10845.0449, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10845.0361328125
tensor(10845.0449, grad_fn=<NegBackward0>) tensor(10845.0361, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10845.029296875
tensor(10845.0361, grad_fn=<NegBackward0>) tensor(10845.0293, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10845.021484375
tensor(10845.0293, grad_fn=<NegBackward0>) tensor(10845.0215, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10845.015625
tensor(10845.0215, grad_fn=<NegBackward0>) tensor(10845.0156, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10845.0087890625
tensor(10845.0156, grad_fn=<NegBackward0>) tensor(10845.0088, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10845.00390625
tensor(10845.0088, grad_fn=<NegBackward0>) tensor(10845.0039, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10845.0
tensor(10845.0039, grad_fn=<NegBackward0>) tensor(10845., grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10844.9951171875
tensor(10845., grad_fn=<NegBackward0>) tensor(10844.9951, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10844.9921875
tensor(10844.9951, grad_fn=<NegBackward0>) tensor(10844.9922, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10844.990234375
tensor(10844.9922, grad_fn=<NegBackward0>) tensor(10844.9902, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10844.9873046875
tensor(10844.9902, grad_fn=<NegBackward0>) tensor(10844.9873, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10844.984375
tensor(10844.9873, grad_fn=<NegBackward0>) tensor(10844.9844, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10844.9814453125
tensor(10844.9844, grad_fn=<NegBackward0>) tensor(10844.9814, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10844.978515625
tensor(10844.9814, grad_fn=<NegBackward0>) tensor(10844.9785, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10844.9765625
tensor(10844.9785, grad_fn=<NegBackward0>) tensor(10844.9766, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10844.9755859375
tensor(10844.9766, grad_fn=<NegBackward0>) tensor(10844.9756, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10844.970703125
tensor(10844.9756, grad_fn=<NegBackward0>) tensor(10844.9707, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10844.9697265625
tensor(10844.9707, grad_fn=<NegBackward0>) tensor(10844.9697, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10844.9658203125
tensor(10844.9697, grad_fn=<NegBackward0>) tensor(10844.9658, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10844.9619140625
tensor(10844.9658, grad_fn=<NegBackward0>) tensor(10844.9619, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10844.95703125
tensor(10844.9619, grad_fn=<NegBackward0>) tensor(10844.9570, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10844.94921875
tensor(10844.9570, grad_fn=<NegBackward0>) tensor(10844.9492, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10844.9404296875
tensor(10844.9492, grad_fn=<NegBackward0>) tensor(10844.9404, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10844.927734375
tensor(10844.9404, grad_fn=<NegBackward0>) tensor(10844.9277, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10844.9130859375
tensor(10844.9277, grad_fn=<NegBackward0>) tensor(10844.9131, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10844.896484375
tensor(10844.9131, grad_fn=<NegBackward0>) tensor(10844.8965, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10844.8779296875
tensor(10844.8965, grad_fn=<NegBackward0>) tensor(10844.8779, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10844.859375
tensor(10844.8779, grad_fn=<NegBackward0>) tensor(10844.8594, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10844.837890625
tensor(10844.8594, grad_fn=<NegBackward0>) tensor(10844.8379, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10844.8046875
tensor(10844.8379, grad_fn=<NegBackward0>) tensor(10844.8047, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10844.75
tensor(10844.8047, grad_fn=<NegBackward0>) tensor(10844.7500, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10844.666015625
tensor(10844.7500, grad_fn=<NegBackward0>) tensor(10844.6660, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10844.607421875
tensor(10844.6660, grad_fn=<NegBackward0>) tensor(10844.6074, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10844.5712890625
tensor(10844.6074, grad_fn=<NegBackward0>) tensor(10844.5713, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10844.5439453125
tensor(10844.5713, grad_fn=<NegBackward0>) tensor(10844.5439, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10844.5224609375
tensor(10844.5439, grad_fn=<NegBackward0>) tensor(10844.5225, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10844.5048828125
tensor(10844.5225, grad_fn=<NegBackward0>) tensor(10844.5049, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10844.490234375
tensor(10844.5049, grad_fn=<NegBackward0>) tensor(10844.4902, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10844.4775390625
tensor(10844.4902, grad_fn=<NegBackward0>) tensor(10844.4775, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10844.4697265625
tensor(10844.4775, grad_fn=<NegBackward0>) tensor(10844.4697, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10844.4619140625
tensor(10844.4697, grad_fn=<NegBackward0>) tensor(10844.4619, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10844.45703125
tensor(10844.4619, grad_fn=<NegBackward0>) tensor(10844.4570, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10844.4501953125
tensor(10844.4570, grad_fn=<NegBackward0>) tensor(10844.4502, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10844.4482421875
tensor(10844.4502, grad_fn=<NegBackward0>) tensor(10844.4482, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10844.4453125
tensor(10844.4482, grad_fn=<NegBackward0>) tensor(10844.4453, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10844.44140625
tensor(10844.4453, grad_fn=<NegBackward0>) tensor(10844.4414, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10844.439453125
tensor(10844.4414, grad_fn=<NegBackward0>) tensor(10844.4395, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10844.4384765625
tensor(10844.4395, grad_fn=<NegBackward0>) tensor(10844.4385, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10844.4345703125
tensor(10844.4385, grad_fn=<NegBackward0>) tensor(10844.4346, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10844.4345703125
tensor(10844.4346, grad_fn=<NegBackward0>) tensor(10844.4346, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10844.431640625
tensor(10844.4346, grad_fn=<NegBackward0>) tensor(10844.4316, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10844.431640625
tensor(10844.4316, grad_fn=<NegBackward0>) tensor(10844.4316, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10844.4287109375
tensor(10844.4316, grad_fn=<NegBackward0>) tensor(10844.4287, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10844.4296875
tensor(10844.4287, grad_fn=<NegBackward0>) tensor(10844.4297, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10844.4287109375
tensor(10844.4287, grad_fn=<NegBackward0>) tensor(10844.4287, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10844.4267578125
tensor(10844.4287, grad_fn=<NegBackward0>) tensor(10844.4268, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10844.4267578125
tensor(10844.4268, grad_fn=<NegBackward0>) tensor(10844.4268, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10844.42578125
tensor(10844.4268, grad_fn=<NegBackward0>) tensor(10844.4258, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10844.423828125
tensor(10844.4258, grad_fn=<NegBackward0>) tensor(10844.4238, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10844.42578125
tensor(10844.4238, grad_fn=<NegBackward0>) tensor(10844.4258, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10844.4248046875
tensor(10844.4238, grad_fn=<NegBackward0>) tensor(10844.4248, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -10844.4267578125
tensor(10844.4238, grad_fn=<NegBackward0>) tensor(10844.4268, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -10844.423828125
tensor(10844.4238, grad_fn=<NegBackward0>) tensor(10844.4238, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10844.423828125
tensor(10844.4238, grad_fn=<NegBackward0>) tensor(10844.4238, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10844.423828125
tensor(10844.4238, grad_fn=<NegBackward0>) tensor(10844.4238, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10844.421875
tensor(10844.4238, grad_fn=<NegBackward0>) tensor(10844.4219, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10844.421875
tensor(10844.4219, grad_fn=<NegBackward0>) tensor(10844.4219, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10844.4208984375
tensor(10844.4219, grad_fn=<NegBackward0>) tensor(10844.4209, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10844.4208984375
tensor(10844.4209, grad_fn=<NegBackward0>) tensor(10844.4209, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10844.4208984375
tensor(10844.4209, grad_fn=<NegBackward0>) tensor(10844.4209, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10844.4228515625
tensor(10844.4209, grad_fn=<NegBackward0>) tensor(10844.4229, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10844.421875
tensor(10844.4209, grad_fn=<NegBackward0>) tensor(10844.4219, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -10844.4208984375
tensor(10844.4209, grad_fn=<NegBackward0>) tensor(10844.4209, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10844.43359375
tensor(10844.4209, grad_fn=<NegBackward0>) tensor(10844.4336, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -10844.4208984375
tensor(10844.4209, grad_fn=<NegBackward0>) tensor(10844.4209, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10844.419921875
tensor(10844.4209, grad_fn=<NegBackward0>) tensor(10844.4199, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10844.419921875
tensor(10844.4199, grad_fn=<NegBackward0>) tensor(10844.4199, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10844.419921875
tensor(10844.4199, grad_fn=<NegBackward0>) tensor(10844.4199, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10844.419921875
tensor(10844.4199, grad_fn=<NegBackward0>) tensor(10844.4199, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10844.4189453125
tensor(10844.4199, grad_fn=<NegBackward0>) tensor(10844.4189, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10844.423828125
tensor(10844.4189, grad_fn=<NegBackward0>) tensor(10844.4238, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -10844.4189453125
tensor(10844.4189, grad_fn=<NegBackward0>) tensor(10844.4189, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10844.419921875
tensor(10844.4189, grad_fn=<NegBackward0>) tensor(10844.4199, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -10844.41796875
tensor(10844.4189, grad_fn=<NegBackward0>) tensor(10844.4180, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10844.4189453125
tensor(10844.4180, grad_fn=<NegBackward0>) tensor(10844.4189, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -10844.41796875
tensor(10844.4180, grad_fn=<NegBackward0>) tensor(10844.4180, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10844.419921875
tensor(10844.4180, grad_fn=<NegBackward0>) tensor(10844.4199, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -10844.419921875
tensor(10844.4180, grad_fn=<NegBackward0>) tensor(10844.4199, grad_fn=<NegBackward0>)
2
pi: tensor([[9.9934e-01, 6.6102e-04],
        [4.3650e-05, 9.9996e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0197, 0.9803], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2265, 0.1785],
         [0.6668, 0.1576]],

        [[0.6692, 0.2058],
         [0.5945, 0.6055]],

        [[0.5067, 0.2309],
         [0.6865, 0.6648]],

        [[0.7096, 0.1905],
         [0.6558, 0.5098]],

        [[0.5255, 0.1702],
         [0.6540, 0.6920]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.00877236457352431
Global Adjusted Rand Index: 0.003423654763061332
Average Adjusted Rand Index: 0.0032944410002703275
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24194.888671875
inf tensor(24194.8887, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10846.630859375
tensor(24194.8887, grad_fn=<NegBackward0>) tensor(10846.6309, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10845.59375
tensor(10846.6309, grad_fn=<NegBackward0>) tensor(10845.5938, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10845.3720703125
tensor(10845.5938, grad_fn=<NegBackward0>) tensor(10845.3721, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10845.302734375
tensor(10845.3721, grad_fn=<NegBackward0>) tensor(10845.3027, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10845.26953125
tensor(10845.3027, grad_fn=<NegBackward0>) tensor(10845.2695, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10845.24609375
tensor(10845.2695, grad_fn=<NegBackward0>) tensor(10845.2461, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10845.2265625
tensor(10845.2461, grad_fn=<NegBackward0>) tensor(10845.2266, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10845.2109375
tensor(10845.2266, grad_fn=<NegBackward0>) tensor(10845.2109, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10845.1962890625
tensor(10845.2109, grad_fn=<NegBackward0>) tensor(10845.1963, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10845.18359375
tensor(10845.1963, grad_fn=<NegBackward0>) tensor(10845.1836, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10845.1728515625
tensor(10845.1836, grad_fn=<NegBackward0>) tensor(10845.1729, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10845.162109375
tensor(10845.1729, grad_fn=<NegBackward0>) tensor(10845.1621, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10845.15234375
tensor(10845.1621, grad_fn=<NegBackward0>) tensor(10845.1523, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10845.14453125
tensor(10845.1523, grad_fn=<NegBackward0>) tensor(10845.1445, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10845.1357421875
tensor(10845.1445, grad_fn=<NegBackward0>) tensor(10845.1357, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10845.126953125
tensor(10845.1357, grad_fn=<NegBackward0>) tensor(10845.1270, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10845.119140625
tensor(10845.1270, grad_fn=<NegBackward0>) tensor(10845.1191, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10845.11328125
tensor(10845.1191, grad_fn=<NegBackward0>) tensor(10845.1133, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10845.10546875
tensor(10845.1133, grad_fn=<NegBackward0>) tensor(10845.1055, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10845.095703125
tensor(10845.1055, grad_fn=<NegBackward0>) tensor(10845.0957, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10845.091796875
tensor(10845.0957, grad_fn=<NegBackward0>) tensor(10845.0918, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10845.083984375
tensor(10845.0918, grad_fn=<NegBackward0>) tensor(10845.0840, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10845.0751953125
tensor(10845.0840, grad_fn=<NegBackward0>) tensor(10845.0752, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10845.0673828125
tensor(10845.0752, grad_fn=<NegBackward0>) tensor(10845.0674, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10845.0595703125
tensor(10845.0674, grad_fn=<NegBackward0>) tensor(10845.0596, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10845.05078125
tensor(10845.0596, grad_fn=<NegBackward0>) tensor(10845.0508, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10845.0439453125
tensor(10845.0508, grad_fn=<NegBackward0>) tensor(10845.0439, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10845.037109375
tensor(10845.0439, grad_fn=<NegBackward0>) tensor(10845.0371, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10845.0302734375
tensor(10845.0371, grad_fn=<NegBackward0>) tensor(10845.0303, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10845.0224609375
tensor(10845.0303, grad_fn=<NegBackward0>) tensor(10845.0225, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10845.0146484375
tensor(10845.0225, grad_fn=<NegBackward0>) tensor(10845.0146, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10845.0078125
tensor(10845.0146, grad_fn=<NegBackward0>) tensor(10845.0078, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10845.0029296875
tensor(10845.0078, grad_fn=<NegBackward0>) tensor(10845.0029, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10844.99609375
tensor(10845.0029, grad_fn=<NegBackward0>) tensor(10844.9961, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10844.9921875
tensor(10844.9961, grad_fn=<NegBackward0>) tensor(10844.9922, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10844.9853515625
tensor(10844.9922, grad_fn=<NegBackward0>) tensor(10844.9854, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10844.98046875
tensor(10844.9854, grad_fn=<NegBackward0>) tensor(10844.9805, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10844.9755859375
tensor(10844.9805, grad_fn=<NegBackward0>) tensor(10844.9756, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10844.97265625
tensor(10844.9756, grad_fn=<NegBackward0>) tensor(10844.9727, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10844.9658203125
tensor(10844.9727, grad_fn=<NegBackward0>) tensor(10844.9658, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10844.9599609375
tensor(10844.9658, grad_fn=<NegBackward0>) tensor(10844.9600, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10844.9521484375
tensor(10844.9600, grad_fn=<NegBackward0>) tensor(10844.9521, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10844.94140625
tensor(10844.9521, grad_fn=<NegBackward0>) tensor(10844.9414, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10844.9296875
tensor(10844.9414, grad_fn=<NegBackward0>) tensor(10844.9297, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10844.9150390625
tensor(10844.9297, grad_fn=<NegBackward0>) tensor(10844.9150, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10844.8974609375
tensor(10844.9150, grad_fn=<NegBackward0>) tensor(10844.8975, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10844.8828125
tensor(10844.8975, grad_fn=<NegBackward0>) tensor(10844.8828, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10844.865234375
tensor(10844.8828, grad_fn=<NegBackward0>) tensor(10844.8652, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10844.849609375
tensor(10844.8652, grad_fn=<NegBackward0>) tensor(10844.8496, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10844.8310546875
tensor(10844.8496, grad_fn=<NegBackward0>) tensor(10844.8311, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10844.8046875
tensor(10844.8311, grad_fn=<NegBackward0>) tensor(10844.8047, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10844.755859375
tensor(10844.8047, grad_fn=<NegBackward0>) tensor(10844.7559, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10844.6689453125
tensor(10844.7559, grad_fn=<NegBackward0>) tensor(10844.6689, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10844.607421875
tensor(10844.6689, grad_fn=<NegBackward0>) tensor(10844.6074, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10844.572265625
tensor(10844.6074, grad_fn=<NegBackward0>) tensor(10844.5723, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10844.5458984375
tensor(10844.5723, grad_fn=<NegBackward0>) tensor(10844.5459, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10844.5224609375
tensor(10844.5459, grad_fn=<NegBackward0>) tensor(10844.5225, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10844.505859375
tensor(10844.5225, grad_fn=<NegBackward0>) tensor(10844.5059, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10844.4912109375
tensor(10844.5059, grad_fn=<NegBackward0>) tensor(10844.4912, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10844.4794921875
tensor(10844.4912, grad_fn=<NegBackward0>) tensor(10844.4795, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10844.4716796875
tensor(10844.4795, grad_fn=<NegBackward0>) tensor(10844.4717, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10844.462890625
tensor(10844.4717, grad_fn=<NegBackward0>) tensor(10844.4629, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10844.45703125
tensor(10844.4629, grad_fn=<NegBackward0>) tensor(10844.4570, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10844.451171875
tensor(10844.4570, grad_fn=<NegBackward0>) tensor(10844.4512, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10844.44921875
tensor(10844.4512, grad_fn=<NegBackward0>) tensor(10844.4492, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10844.4453125
tensor(10844.4492, grad_fn=<NegBackward0>) tensor(10844.4453, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10844.4423828125
tensor(10844.4453, grad_fn=<NegBackward0>) tensor(10844.4424, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10844.439453125
tensor(10844.4424, grad_fn=<NegBackward0>) tensor(10844.4395, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10844.4365234375
tensor(10844.4395, grad_fn=<NegBackward0>) tensor(10844.4365, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10844.435546875
tensor(10844.4365, grad_fn=<NegBackward0>) tensor(10844.4355, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10844.43359375
tensor(10844.4355, grad_fn=<NegBackward0>) tensor(10844.4336, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10844.43359375
tensor(10844.4336, grad_fn=<NegBackward0>) tensor(10844.4336, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10844.4306640625
tensor(10844.4336, grad_fn=<NegBackward0>) tensor(10844.4307, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10844.4306640625
tensor(10844.4307, grad_fn=<NegBackward0>) tensor(10844.4307, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10844.4296875
tensor(10844.4307, grad_fn=<NegBackward0>) tensor(10844.4297, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10844.4287109375
tensor(10844.4297, grad_fn=<NegBackward0>) tensor(10844.4287, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10844.427734375
tensor(10844.4287, grad_fn=<NegBackward0>) tensor(10844.4277, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10844.5263671875
tensor(10844.4277, grad_fn=<NegBackward0>) tensor(10844.5264, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10844.427734375
tensor(10844.4277, grad_fn=<NegBackward0>) tensor(10844.4277, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10844.42578125
tensor(10844.4277, grad_fn=<NegBackward0>) tensor(10844.4258, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10844.43359375
tensor(10844.4258, grad_fn=<NegBackward0>) tensor(10844.4336, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10844.4248046875
tensor(10844.4258, grad_fn=<NegBackward0>) tensor(10844.4248, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10844.431640625
tensor(10844.4248, grad_fn=<NegBackward0>) tensor(10844.4316, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10844.423828125
tensor(10844.4248, grad_fn=<NegBackward0>) tensor(10844.4238, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10844.423828125
tensor(10844.4238, grad_fn=<NegBackward0>) tensor(10844.4238, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10844.4228515625
tensor(10844.4238, grad_fn=<NegBackward0>) tensor(10844.4229, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10844.4228515625
tensor(10844.4229, grad_fn=<NegBackward0>) tensor(10844.4229, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10844.486328125
tensor(10844.4229, grad_fn=<NegBackward0>) tensor(10844.4863, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10844.4208984375
tensor(10844.4229, grad_fn=<NegBackward0>) tensor(10844.4209, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10844.4208984375
tensor(10844.4209, grad_fn=<NegBackward0>) tensor(10844.4209, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10844.423828125
tensor(10844.4209, grad_fn=<NegBackward0>) tensor(10844.4238, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10844.421875
tensor(10844.4209, grad_fn=<NegBackward0>) tensor(10844.4219, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -10844.421875
tensor(10844.4209, grad_fn=<NegBackward0>) tensor(10844.4219, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -10844.501953125
tensor(10844.4209, grad_fn=<NegBackward0>) tensor(10844.5020, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -10844.419921875
tensor(10844.4209, grad_fn=<NegBackward0>) tensor(10844.4199, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10844.4208984375
tensor(10844.4199, grad_fn=<NegBackward0>) tensor(10844.4209, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -10844.421875
tensor(10844.4199, grad_fn=<NegBackward0>) tensor(10844.4219, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -10844.419921875
tensor(10844.4199, grad_fn=<NegBackward0>) tensor(10844.4199, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10844.427734375
tensor(10844.4199, grad_fn=<NegBackward0>) tensor(10844.4277, grad_fn=<NegBackward0>)
1
pi: tensor([[9.9889e-01, 1.1109e-03],
        [5.7088e-05, 9.9994e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0197, 0.9803], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2266, 0.1785],
         [0.5150, 0.1576]],

        [[0.5841, 0.2058],
         [0.5830, 0.6739]],

        [[0.5072, 0.2309],
         [0.6406, 0.6937]],

        [[0.6752, 0.1905],
         [0.5112, 0.5133]],

        [[0.5257, 0.1702],
         [0.5320, 0.6549]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.00877236457352431
Global Adjusted Rand Index: 0.003423654763061332
Average Adjusted Rand Index: 0.0032944410002703275
[0.003423654763061332, 0.003423654763061332] [0.0032944410002703275, 0.0032944410002703275] [10844.41796875, 10844.4208984375]
-------------------------------------
This iteration is 84
True Objective function: Loss = -10780.489174469172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22305.1171875
inf tensor(22305.1172, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10876.5087890625
tensor(22305.1172, grad_fn=<NegBackward0>) tensor(10876.5088, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10874.4638671875
tensor(10876.5088, grad_fn=<NegBackward0>) tensor(10874.4639, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10873.583984375
tensor(10874.4639, grad_fn=<NegBackward0>) tensor(10873.5840, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10873.033203125
tensor(10873.5840, grad_fn=<NegBackward0>) tensor(10873.0332, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10872.689453125
tensor(10873.0332, grad_fn=<NegBackward0>) tensor(10872.6895, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10872.474609375
tensor(10872.6895, grad_fn=<NegBackward0>) tensor(10872.4746, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10872.3359375
tensor(10872.4746, grad_fn=<NegBackward0>) tensor(10872.3359, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10872.2431640625
tensor(10872.3359, grad_fn=<NegBackward0>) tensor(10872.2432, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10872.1748046875
tensor(10872.2432, grad_fn=<NegBackward0>) tensor(10872.1748, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10872.1201171875
tensor(10872.1748, grad_fn=<NegBackward0>) tensor(10872.1201, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10872.0498046875
tensor(10872.1201, grad_fn=<NegBackward0>) tensor(10872.0498, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10871.8046875
tensor(10872.0498, grad_fn=<NegBackward0>) tensor(10871.8047, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10871.3271484375
tensor(10871.8047, grad_fn=<NegBackward0>) tensor(10871.3271, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10871.12109375
tensor(10871.3271, grad_fn=<NegBackward0>) tensor(10871.1211, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10870.9873046875
tensor(10871.1211, grad_fn=<NegBackward0>) tensor(10870.9873, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10870.8671875
tensor(10870.9873, grad_fn=<NegBackward0>) tensor(10870.8672, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10870.75
tensor(10870.8672, grad_fn=<NegBackward0>) tensor(10870.7500, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10870.6552734375
tensor(10870.7500, grad_fn=<NegBackward0>) tensor(10870.6553, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10870.5859375
tensor(10870.6553, grad_fn=<NegBackward0>) tensor(10870.5859, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10870.529296875
tensor(10870.5859, grad_fn=<NegBackward0>) tensor(10870.5293, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10870.4833984375
tensor(10870.5293, grad_fn=<NegBackward0>) tensor(10870.4834, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10870.44140625
tensor(10870.4834, grad_fn=<NegBackward0>) tensor(10870.4414, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10870.408203125
tensor(10870.4414, grad_fn=<NegBackward0>) tensor(10870.4082, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10870.3779296875
tensor(10870.4082, grad_fn=<NegBackward0>) tensor(10870.3779, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10870.3515625
tensor(10870.3779, grad_fn=<NegBackward0>) tensor(10870.3516, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10870.3271484375
tensor(10870.3516, grad_fn=<NegBackward0>) tensor(10870.3271, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10870.306640625
tensor(10870.3271, grad_fn=<NegBackward0>) tensor(10870.3066, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10870.287109375
tensor(10870.3066, grad_fn=<NegBackward0>) tensor(10870.2871, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10870.271484375
tensor(10870.2871, grad_fn=<NegBackward0>) tensor(10870.2715, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10870.2578125
tensor(10870.2715, grad_fn=<NegBackward0>) tensor(10870.2578, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10870.24609375
tensor(10870.2578, grad_fn=<NegBackward0>) tensor(10870.2461, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10870.234375
tensor(10870.2461, grad_fn=<NegBackward0>) tensor(10870.2344, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10870.2255859375
tensor(10870.2344, grad_fn=<NegBackward0>) tensor(10870.2256, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10870.2158203125
tensor(10870.2256, grad_fn=<NegBackward0>) tensor(10870.2158, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10870.2099609375
tensor(10870.2158, grad_fn=<NegBackward0>) tensor(10870.2100, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10870.2041015625
tensor(10870.2100, grad_fn=<NegBackward0>) tensor(10870.2041, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10870.197265625
tensor(10870.2041, grad_fn=<NegBackward0>) tensor(10870.1973, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10870.193359375
tensor(10870.1973, grad_fn=<NegBackward0>) tensor(10870.1934, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10870.1884765625
tensor(10870.1934, grad_fn=<NegBackward0>) tensor(10870.1885, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10870.1845703125
tensor(10870.1885, grad_fn=<NegBackward0>) tensor(10870.1846, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10870.1806640625
tensor(10870.1846, grad_fn=<NegBackward0>) tensor(10870.1807, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10870.17578125
tensor(10870.1807, grad_fn=<NegBackward0>) tensor(10870.1758, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10870.1748046875
tensor(10870.1758, grad_fn=<NegBackward0>) tensor(10870.1748, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10870.1708984375
tensor(10870.1748, grad_fn=<NegBackward0>) tensor(10870.1709, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10870.1669921875
tensor(10870.1709, grad_fn=<NegBackward0>) tensor(10870.1670, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10870.1669921875
tensor(10870.1670, grad_fn=<NegBackward0>) tensor(10870.1670, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10870.1640625
tensor(10870.1670, grad_fn=<NegBackward0>) tensor(10870.1641, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10870.1611328125
tensor(10870.1641, grad_fn=<NegBackward0>) tensor(10870.1611, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10870.16015625
tensor(10870.1611, grad_fn=<NegBackward0>) tensor(10870.1602, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10870.1591796875
tensor(10870.1602, grad_fn=<NegBackward0>) tensor(10870.1592, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10870.1572265625
tensor(10870.1592, grad_fn=<NegBackward0>) tensor(10870.1572, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10870.15625
tensor(10870.1572, grad_fn=<NegBackward0>) tensor(10870.1562, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10870.154296875
tensor(10870.1562, grad_fn=<NegBackward0>) tensor(10870.1543, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10870.1533203125
tensor(10870.1543, grad_fn=<NegBackward0>) tensor(10870.1533, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10870.1533203125
tensor(10870.1533, grad_fn=<NegBackward0>) tensor(10870.1533, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10870.1494140625
tensor(10870.1533, grad_fn=<NegBackward0>) tensor(10870.1494, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10870.1484375
tensor(10870.1494, grad_fn=<NegBackward0>) tensor(10870.1484, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10870.1494140625
tensor(10870.1484, grad_fn=<NegBackward0>) tensor(10870.1494, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10870.1474609375
tensor(10870.1484, grad_fn=<NegBackward0>) tensor(10870.1475, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10870.146484375
tensor(10870.1475, grad_fn=<NegBackward0>) tensor(10870.1465, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10870.146484375
tensor(10870.1465, grad_fn=<NegBackward0>) tensor(10870.1465, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10870.14453125
tensor(10870.1465, grad_fn=<NegBackward0>) tensor(10870.1445, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10870.1455078125
tensor(10870.1445, grad_fn=<NegBackward0>) tensor(10870.1455, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10870.1455078125
tensor(10870.1445, grad_fn=<NegBackward0>) tensor(10870.1455, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -10870.14453125
tensor(10870.1445, grad_fn=<NegBackward0>) tensor(10870.1445, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10870.1435546875
tensor(10870.1445, grad_fn=<NegBackward0>) tensor(10870.1436, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10870.142578125
tensor(10870.1436, grad_fn=<NegBackward0>) tensor(10870.1426, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10870.1416015625
tensor(10870.1426, grad_fn=<NegBackward0>) tensor(10870.1416, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10870.1416015625
tensor(10870.1416, grad_fn=<NegBackward0>) tensor(10870.1416, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10870.140625
tensor(10870.1416, grad_fn=<NegBackward0>) tensor(10870.1406, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10870.140625
tensor(10870.1406, grad_fn=<NegBackward0>) tensor(10870.1406, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10870.140625
tensor(10870.1406, grad_fn=<NegBackward0>) tensor(10870.1406, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10870.1396484375
tensor(10870.1406, grad_fn=<NegBackward0>) tensor(10870.1396, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10870.1396484375
tensor(10870.1396, grad_fn=<NegBackward0>) tensor(10870.1396, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10870.1396484375
tensor(10870.1396, grad_fn=<NegBackward0>) tensor(10870.1396, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10870.138671875
tensor(10870.1396, grad_fn=<NegBackward0>) tensor(10870.1387, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10870.1669921875
tensor(10870.1387, grad_fn=<NegBackward0>) tensor(10870.1670, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10870.138671875
tensor(10870.1387, grad_fn=<NegBackward0>) tensor(10870.1387, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10870.13671875
tensor(10870.1387, grad_fn=<NegBackward0>) tensor(10870.1367, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10870.1650390625
tensor(10870.1367, grad_fn=<NegBackward0>) tensor(10870.1650, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10870.13671875
tensor(10870.1367, grad_fn=<NegBackward0>) tensor(10870.1367, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10870.1376953125
tensor(10870.1367, grad_fn=<NegBackward0>) tensor(10870.1377, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10870.1376953125
tensor(10870.1367, grad_fn=<NegBackward0>) tensor(10870.1377, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -10870.1513671875
tensor(10870.1367, grad_fn=<NegBackward0>) tensor(10870.1514, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -10870.13671875
tensor(10870.1367, grad_fn=<NegBackward0>) tensor(10870.1367, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10870.1376953125
tensor(10870.1367, grad_fn=<NegBackward0>) tensor(10870.1377, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10870.1396484375
tensor(10870.1367, grad_fn=<NegBackward0>) tensor(10870.1396, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -10870.13671875
tensor(10870.1367, grad_fn=<NegBackward0>) tensor(10870.1367, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10870.13671875
tensor(10870.1367, grad_fn=<NegBackward0>) tensor(10870.1367, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10870.1494140625
tensor(10870.1367, grad_fn=<NegBackward0>) tensor(10870.1494, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10870.134765625
tensor(10870.1367, grad_fn=<NegBackward0>) tensor(10870.1348, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10870.13671875
tensor(10870.1348, grad_fn=<NegBackward0>) tensor(10870.1367, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -10870.2021484375
tensor(10870.1348, grad_fn=<NegBackward0>) tensor(10870.2021, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -10870.1357421875
tensor(10870.1348, grad_fn=<NegBackward0>) tensor(10870.1357, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -10870.134765625
tensor(10870.1348, grad_fn=<NegBackward0>) tensor(10870.1348, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10870.134765625
tensor(10870.1348, grad_fn=<NegBackward0>) tensor(10870.1348, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10870.1357421875
tensor(10870.1348, grad_fn=<NegBackward0>) tensor(10870.1357, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -10870.140625
tensor(10870.1348, grad_fn=<NegBackward0>) tensor(10870.1406, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -10870.1337890625
tensor(10870.1348, grad_fn=<NegBackward0>) tensor(10870.1338, grad_fn=<NegBackward0>)
pi: tensor([[1.0000e+00, 2.2842e-06],
        [1.1977e-03, 9.9880e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9899, 0.0101], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1586, 0.2320],
         [0.7254, 0.2395]],

        [[0.7240, 0.1919],
         [0.6148, 0.6910]],

        [[0.5927, 0.3430],
         [0.6096, 0.6290]],

        [[0.7230, 0.1413],
         [0.5814, 0.5116]],

        [[0.5359, 0.1615],
         [0.5261, 0.6452]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
Global Adjusted Rand Index: 0.000560772937003748
Average Adjusted Rand Index: -0.0007208677057358424
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22191.30078125
inf tensor(22191.3008, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10877.75390625
tensor(22191.3008, grad_fn=<NegBackward0>) tensor(10877.7539, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10877.2861328125
tensor(10877.7539, grad_fn=<NegBackward0>) tensor(10877.2861, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10877.1513671875
tensor(10877.2861, grad_fn=<NegBackward0>) tensor(10877.1514, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10877.06640625
tensor(10877.1514, grad_fn=<NegBackward0>) tensor(10877.0664, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10877.0009765625
tensor(10877.0664, grad_fn=<NegBackward0>) tensor(10877.0010, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10876.947265625
tensor(10877.0010, grad_fn=<NegBackward0>) tensor(10876.9473, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10876.8974609375
tensor(10876.9473, grad_fn=<NegBackward0>) tensor(10876.8975, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10876.8466796875
tensor(10876.8975, grad_fn=<NegBackward0>) tensor(10876.8467, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10876.7919921875
tensor(10876.8467, grad_fn=<NegBackward0>) tensor(10876.7920, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10876.740234375
tensor(10876.7920, grad_fn=<NegBackward0>) tensor(10876.7402, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10876.6923828125
tensor(10876.7402, grad_fn=<NegBackward0>) tensor(10876.6924, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10876.6513671875
tensor(10876.6924, grad_fn=<NegBackward0>) tensor(10876.6514, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10876.6171875
tensor(10876.6514, grad_fn=<NegBackward0>) tensor(10876.6172, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10876.5869140625
tensor(10876.6172, grad_fn=<NegBackward0>) tensor(10876.5869, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10876.5595703125
tensor(10876.5869, grad_fn=<NegBackward0>) tensor(10876.5596, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10876.53125
tensor(10876.5596, grad_fn=<NegBackward0>) tensor(10876.5312, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10876.5048828125
tensor(10876.5312, grad_fn=<NegBackward0>) tensor(10876.5049, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10876.4765625
tensor(10876.5049, grad_fn=<NegBackward0>) tensor(10876.4766, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10876.4482421875
tensor(10876.4766, grad_fn=<NegBackward0>) tensor(10876.4482, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10876.4208984375
tensor(10876.4482, grad_fn=<NegBackward0>) tensor(10876.4209, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10876.39453125
tensor(10876.4209, grad_fn=<NegBackward0>) tensor(10876.3945, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10876.3681640625
tensor(10876.3945, grad_fn=<NegBackward0>) tensor(10876.3682, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10876.3427734375
tensor(10876.3682, grad_fn=<NegBackward0>) tensor(10876.3428, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10876.3173828125
tensor(10876.3428, grad_fn=<NegBackward0>) tensor(10876.3174, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10876.291015625
tensor(10876.3174, grad_fn=<NegBackward0>) tensor(10876.2910, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10876.26171875
tensor(10876.2910, grad_fn=<NegBackward0>) tensor(10876.2617, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10876.2294921875
tensor(10876.2617, grad_fn=<NegBackward0>) tensor(10876.2295, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10876.193359375
tensor(10876.2295, grad_fn=<NegBackward0>) tensor(10876.1934, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10876.1494140625
tensor(10876.1934, grad_fn=<NegBackward0>) tensor(10876.1494, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10876.099609375
tensor(10876.1494, grad_fn=<NegBackward0>) tensor(10876.0996, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10876.0439453125
tensor(10876.0996, grad_fn=<NegBackward0>) tensor(10876.0439, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10875.9794921875
tensor(10876.0439, grad_fn=<NegBackward0>) tensor(10875.9795, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10875.880859375
tensor(10875.9795, grad_fn=<NegBackward0>) tensor(10875.8809, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10875.5810546875
tensor(10875.8809, grad_fn=<NegBackward0>) tensor(10875.5811, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10875.23046875
tensor(10875.5811, grad_fn=<NegBackward0>) tensor(10875.2305, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10875.00390625
tensor(10875.2305, grad_fn=<NegBackward0>) tensor(10875.0039, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10874.818359375
tensor(10875.0039, grad_fn=<NegBackward0>) tensor(10874.8184, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10874.65234375
tensor(10874.8184, grad_fn=<NegBackward0>) tensor(10874.6523, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10874.529296875
tensor(10874.6523, grad_fn=<NegBackward0>) tensor(10874.5293, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10874.453125
tensor(10874.5293, grad_fn=<NegBackward0>) tensor(10874.4531, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10874.4013671875
tensor(10874.4531, grad_fn=<NegBackward0>) tensor(10874.4014, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10874.365234375
tensor(10874.4014, grad_fn=<NegBackward0>) tensor(10874.3652, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10874.337890625
tensor(10874.3652, grad_fn=<NegBackward0>) tensor(10874.3379, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10874.3193359375
tensor(10874.3379, grad_fn=<NegBackward0>) tensor(10874.3193, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10874.3037109375
tensor(10874.3193, grad_fn=<NegBackward0>) tensor(10874.3037, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10874.29296875
tensor(10874.3037, grad_fn=<NegBackward0>) tensor(10874.2930, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10874.2861328125
tensor(10874.2930, grad_fn=<NegBackward0>) tensor(10874.2861, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10874.279296875
tensor(10874.2861, grad_fn=<NegBackward0>) tensor(10874.2793, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10874.275390625
tensor(10874.2793, grad_fn=<NegBackward0>) tensor(10874.2754, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10874.2724609375
tensor(10874.2754, grad_fn=<NegBackward0>) tensor(10874.2725, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10874.26953125
tensor(10874.2725, grad_fn=<NegBackward0>) tensor(10874.2695, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10874.267578125
tensor(10874.2695, grad_fn=<NegBackward0>) tensor(10874.2676, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10874.2666015625
tensor(10874.2676, grad_fn=<NegBackward0>) tensor(10874.2666, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10874.263671875
tensor(10874.2666, grad_fn=<NegBackward0>) tensor(10874.2637, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10874.2626953125
tensor(10874.2637, grad_fn=<NegBackward0>) tensor(10874.2627, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10874.2607421875
tensor(10874.2627, grad_fn=<NegBackward0>) tensor(10874.2607, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10874.259765625
tensor(10874.2607, grad_fn=<NegBackward0>) tensor(10874.2598, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10874.2607421875
tensor(10874.2598, grad_fn=<NegBackward0>) tensor(10874.2607, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10874.2587890625
tensor(10874.2598, grad_fn=<NegBackward0>) tensor(10874.2588, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10874.2578125
tensor(10874.2588, grad_fn=<NegBackward0>) tensor(10874.2578, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10874.2568359375
tensor(10874.2578, grad_fn=<NegBackward0>) tensor(10874.2568, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10874.2578125
tensor(10874.2568, grad_fn=<NegBackward0>) tensor(10874.2578, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10874.255859375
tensor(10874.2568, grad_fn=<NegBackward0>) tensor(10874.2559, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10874.255859375
tensor(10874.2559, grad_fn=<NegBackward0>) tensor(10874.2559, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10874.2548828125
tensor(10874.2559, grad_fn=<NegBackward0>) tensor(10874.2549, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10874.2548828125
tensor(10874.2549, grad_fn=<NegBackward0>) tensor(10874.2549, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10874.255859375
tensor(10874.2549, grad_fn=<NegBackward0>) tensor(10874.2559, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10874.2548828125
tensor(10874.2549, grad_fn=<NegBackward0>) tensor(10874.2549, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10874.2548828125
tensor(10874.2549, grad_fn=<NegBackward0>) tensor(10874.2549, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10874.25390625
tensor(10874.2549, grad_fn=<NegBackward0>) tensor(10874.2539, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10874.255859375
tensor(10874.2539, grad_fn=<NegBackward0>) tensor(10874.2559, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10874.2548828125
tensor(10874.2539, grad_fn=<NegBackward0>) tensor(10874.2549, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -10874.2548828125
tensor(10874.2539, grad_fn=<NegBackward0>) tensor(10874.2549, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -10874.2548828125
tensor(10874.2539, grad_fn=<NegBackward0>) tensor(10874.2549, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -10874.2529296875
tensor(10874.2539, grad_fn=<NegBackward0>) tensor(10874.2529, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10874.2529296875
tensor(10874.2529, grad_fn=<NegBackward0>) tensor(10874.2529, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10874.2529296875
tensor(10874.2529, grad_fn=<NegBackward0>) tensor(10874.2529, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10874.255859375
tensor(10874.2529, grad_fn=<NegBackward0>) tensor(10874.2559, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10874.2529296875
tensor(10874.2529, grad_fn=<NegBackward0>) tensor(10874.2529, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10874.2548828125
tensor(10874.2529, grad_fn=<NegBackward0>) tensor(10874.2549, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10874.2529296875
tensor(10874.2529, grad_fn=<NegBackward0>) tensor(10874.2529, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10874.259765625
tensor(10874.2529, grad_fn=<NegBackward0>) tensor(10874.2598, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10874.251953125
tensor(10874.2529, grad_fn=<NegBackward0>) tensor(10874.2520, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10874.2919921875
tensor(10874.2520, grad_fn=<NegBackward0>) tensor(10874.2920, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -10874.251953125
tensor(10874.2520, grad_fn=<NegBackward0>) tensor(10874.2520, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10874.3623046875
tensor(10874.2520, grad_fn=<NegBackward0>) tensor(10874.3623, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10874.251953125
tensor(10874.2520, grad_fn=<NegBackward0>) tensor(10874.2520, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10874.251953125
tensor(10874.2520, grad_fn=<NegBackward0>) tensor(10874.2520, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10874.2529296875
tensor(10874.2520, grad_fn=<NegBackward0>) tensor(10874.2529, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -10874.251953125
tensor(10874.2520, grad_fn=<NegBackward0>) tensor(10874.2520, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10874.2529296875
tensor(10874.2520, grad_fn=<NegBackward0>) tensor(10874.2529, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10874.28125
tensor(10874.2520, grad_fn=<NegBackward0>) tensor(10874.2812, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -10874.2529296875
tensor(10874.2520, grad_fn=<NegBackward0>) tensor(10874.2529, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -10874.2529296875
tensor(10874.2520, grad_fn=<NegBackward0>) tensor(10874.2529, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -10874.25390625
tensor(10874.2520, grad_fn=<NegBackward0>) tensor(10874.2539, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9500 due to no improvement.
pi: tensor([[0.0093, 0.9907],
        [0.0207, 0.9793]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9798, 0.0202], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1648, 0.1657],
         [0.6915, 0.1593]],

        [[0.5422, 0.0424],
         [0.5681, 0.5666]],

        [[0.5863, 0.1025],
         [0.7287, 0.5197]],

        [[0.7141, 0.1422],
         [0.6101, 0.6928]],

        [[0.7043, 0.2257],
         [0.5806, 0.5371]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.000858953343859174
Average Adjusted Rand Index: 0.0
[0.000560772937003748, -0.000858953343859174] [-0.0007208677057358424, 0.0] [10870.140625, 10874.25390625]
-------------------------------------
This iteration is 85
True Objective function: Loss = -10899.000954790668
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22698.181640625
inf tensor(22698.1816, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11032.55859375
tensor(22698.1816, grad_fn=<NegBackward0>) tensor(11032.5586, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11031.748046875
tensor(11032.5586, grad_fn=<NegBackward0>) tensor(11031.7480, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11031.5498046875
tensor(11031.7480, grad_fn=<NegBackward0>) tensor(11031.5498, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11031.4453125
tensor(11031.5498, grad_fn=<NegBackward0>) tensor(11031.4453, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11031.376953125
tensor(11031.4453, grad_fn=<NegBackward0>) tensor(11031.3770, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11031.3271484375
tensor(11031.3770, grad_fn=<NegBackward0>) tensor(11031.3271, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11031.28515625
tensor(11031.3271, grad_fn=<NegBackward0>) tensor(11031.2852, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11031.24609375
tensor(11031.2852, grad_fn=<NegBackward0>) tensor(11031.2461, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11031.203125
tensor(11031.2461, grad_fn=<NegBackward0>) tensor(11031.2031, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11031.15234375
tensor(11031.2031, grad_fn=<NegBackward0>) tensor(11031.1523, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11031.08984375
tensor(11031.1523, grad_fn=<NegBackward0>) tensor(11031.0898, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11031.015625
tensor(11031.0898, grad_fn=<NegBackward0>) tensor(11031.0156, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11030.912109375
tensor(11031.0156, grad_fn=<NegBackward0>) tensor(11030.9121, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11030.7763671875
tensor(11030.9121, grad_fn=<NegBackward0>) tensor(11030.7764, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11030.6025390625
tensor(11030.7764, grad_fn=<NegBackward0>) tensor(11030.6025, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11030.3681640625
tensor(11030.6025, grad_fn=<NegBackward0>) tensor(11030.3682, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11030.1767578125
tensor(11030.3682, grad_fn=<NegBackward0>) tensor(11030.1768, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11030.0830078125
tensor(11030.1768, grad_fn=<NegBackward0>) tensor(11030.0830, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11030.01953125
tensor(11030.0830, grad_fn=<NegBackward0>) tensor(11030.0195, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11029.9580078125
tensor(11030.0195, grad_fn=<NegBackward0>) tensor(11029.9580, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11029.896484375
tensor(11029.9580, grad_fn=<NegBackward0>) tensor(11029.8965, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11029.837890625
tensor(11029.8965, grad_fn=<NegBackward0>) tensor(11029.8379, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11029.779296875
tensor(11029.8379, grad_fn=<NegBackward0>) tensor(11029.7793, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11029.71875
tensor(11029.7793, grad_fn=<NegBackward0>) tensor(11029.7188, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11029.6708984375
tensor(11029.7188, grad_fn=<NegBackward0>) tensor(11029.6709, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11029.634765625
tensor(11029.6709, grad_fn=<NegBackward0>) tensor(11029.6348, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11029.603515625
tensor(11029.6348, grad_fn=<NegBackward0>) tensor(11029.6035, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11029.5595703125
tensor(11029.6035, grad_fn=<NegBackward0>) tensor(11029.5596, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11027.2353515625
tensor(11029.5596, grad_fn=<NegBackward0>) tensor(11027.2354, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11026.6181640625
tensor(11027.2354, grad_fn=<NegBackward0>) tensor(11026.6182, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11026.5009765625
tensor(11026.6182, grad_fn=<NegBackward0>) tensor(11026.5010, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11026.4501953125
tensor(11026.5010, grad_fn=<NegBackward0>) tensor(11026.4502, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11026.4248046875
tensor(11026.4502, grad_fn=<NegBackward0>) tensor(11026.4248, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11026.40625
tensor(11026.4248, grad_fn=<NegBackward0>) tensor(11026.4062, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11026.3935546875
tensor(11026.4062, grad_fn=<NegBackward0>) tensor(11026.3936, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11026.3837890625
tensor(11026.3936, grad_fn=<NegBackward0>) tensor(11026.3838, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11026.376953125
tensor(11026.3838, grad_fn=<NegBackward0>) tensor(11026.3770, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11026.3701171875
tensor(11026.3770, grad_fn=<NegBackward0>) tensor(11026.3701, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11026.365234375
tensor(11026.3701, grad_fn=<NegBackward0>) tensor(11026.3652, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11026.361328125
tensor(11026.3652, grad_fn=<NegBackward0>) tensor(11026.3613, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11026.357421875
tensor(11026.3613, grad_fn=<NegBackward0>) tensor(11026.3574, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11026.3525390625
tensor(11026.3574, grad_fn=<NegBackward0>) tensor(11026.3525, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11026.349609375
tensor(11026.3525, grad_fn=<NegBackward0>) tensor(11026.3496, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11026.3486328125
tensor(11026.3496, grad_fn=<NegBackward0>) tensor(11026.3486, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11026.3466796875
tensor(11026.3486, grad_fn=<NegBackward0>) tensor(11026.3467, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11026.3447265625
tensor(11026.3467, grad_fn=<NegBackward0>) tensor(11026.3447, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11026.3427734375
tensor(11026.3447, grad_fn=<NegBackward0>) tensor(11026.3428, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11026.3408203125
tensor(11026.3428, grad_fn=<NegBackward0>) tensor(11026.3408, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11026.3408203125
tensor(11026.3408, grad_fn=<NegBackward0>) tensor(11026.3408, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11026.33984375
tensor(11026.3408, grad_fn=<NegBackward0>) tensor(11026.3398, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11026.3388671875
tensor(11026.3398, grad_fn=<NegBackward0>) tensor(11026.3389, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11026.337890625
tensor(11026.3389, grad_fn=<NegBackward0>) tensor(11026.3379, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11026.337890625
tensor(11026.3379, grad_fn=<NegBackward0>) tensor(11026.3379, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11026.337890625
tensor(11026.3379, grad_fn=<NegBackward0>) tensor(11026.3379, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11026.3359375
tensor(11026.3379, grad_fn=<NegBackward0>) tensor(11026.3359, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11026.3359375
tensor(11026.3359, grad_fn=<NegBackward0>) tensor(11026.3359, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11026.333984375
tensor(11026.3359, grad_fn=<NegBackward0>) tensor(11026.3340, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11026.3349609375
tensor(11026.3340, grad_fn=<NegBackward0>) tensor(11026.3350, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11026.3330078125
tensor(11026.3340, grad_fn=<NegBackward0>) tensor(11026.3330, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11026.333984375
tensor(11026.3330, grad_fn=<NegBackward0>) tensor(11026.3340, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11026.333984375
tensor(11026.3330, grad_fn=<NegBackward0>) tensor(11026.3340, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11026.33203125
tensor(11026.3330, grad_fn=<NegBackward0>) tensor(11026.3320, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11026.33203125
tensor(11026.3320, grad_fn=<NegBackward0>) tensor(11026.3320, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11026.33203125
tensor(11026.3320, grad_fn=<NegBackward0>) tensor(11026.3320, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11026.330078125
tensor(11026.3320, grad_fn=<NegBackward0>) tensor(11026.3301, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11026.3310546875
tensor(11026.3301, grad_fn=<NegBackward0>) tensor(11026.3311, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11026.330078125
tensor(11026.3301, grad_fn=<NegBackward0>) tensor(11026.3301, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11026.3291015625
tensor(11026.3301, grad_fn=<NegBackward0>) tensor(11026.3291, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11026.3291015625
tensor(11026.3291, grad_fn=<NegBackward0>) tensor(11026.3291, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11026.328125
tensor(11026.3291, grad_fn=<NegBackward0>) tensor(11026.3281, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11026.3271484375
tensor(11026.3281, grad_fn=<NegBackward0>) tensor(11026.3271, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11026.328125
tensor(11026.3271, grad_fn=<NegBackward0>) tensor(11026.3281, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11026.326171875
tensor(11026.3271, grad_fn=<NegBackward0>) tensor(11026.3262, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11026.3271484375
tensor(11026.3262, grad_fn=<NegBackward0>) tensor(11026.3271, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11026.3251953125
tensor(11026.3262, grad_fn=<NegBackward0>) tensor(11026.3252, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11026.326171875
tensor(11026.3252, grad_fn=<NegBackward0>) tensor(11026.3262, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11026.3251953125
tensor(11026.3252, grad_fn=<NegBackward0>) tensor(11026.3252, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11026.3251953125
tensor(11026.3252, grad_fn=<NegBackward0>) tensor(11026.3252, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11026.3251953125
tensor(11026.3252, grad_fn=<NegBackward0>) tensor(11026.3252, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11026.34375
tensor(11026.3252, grad_fn=<NegBackward0>) tensor(11026.3438, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11026.3251953125
tensor(11026.3252, grad_fn=<NegBackward0>) tensor(11026.3252, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11026.326171875
tensor(11026.3252, grad_fn=<NegBackward0>) tensor(11026.3262, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11026.3251953125
tensor(11026.3252, grad_fn=<NegBackward0>) tensor(11026.3252, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11026.32421875
tensor(11026.3252, grad_fn=<NegBackward0>) tensor(11026.3242, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11026.3271484375
tensor(11026.3242, grad_fn=<NegBackward0>) tensor(11026.3271, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11026.32421875
tensor(11026.3242, grad_fn=<NegBackward0>) tensor(11026.3242, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11026.3251953125
tensor(11026.3242, grad_fn=<NegBackward0>) tensor(11026.3252, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11026.32421875
tensor(11026.3242, grad_fn=<NegBackward0>) tensor(11026.3242, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11026.32421875
tensor(11026.3242, grad_fn=<NegBackward0>) tensor(11026.3242, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11026.326171875
tensor(11026.3242, grad_fn=<NegBackward0>) tensor(11026.3262, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11026.326171875
tensor(11026.3242, grad_fn=<NegBackward0>) tensor(11026.3262, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11026.32421875
tensor(11026.3242, grad_fn=<NegBackward0>) tensor(11026.3242, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11026.3271484375
tensor(11026.3242, grad_fn=<NegBackward0>) tensor(11026.3271, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11026.3251953125
tensor(11026.3242, grad_fn=<NegBackward0>) tensor(11026.3252, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -11026.400390625
tensor(11026.3242, grad_fn=<NegBackward0>) tensor(11026.4004, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -11026.3212890625
tensor(11026.3242, grad_fn=<NegBackward0>) tensor(11026.3213, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11026.322265625
tensor(11026.3213, grad_fn=<NegBackward0>) tensor(11026.3223, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11026.32421875
tensor(11026.3213, grad_fn=<NegBackward0>) tensor(11026.3242, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -11026.322265625
tensor(11026.3213, grad_fn=<NegBackward0>) tensor(11026.3223, grad_fn=<NegBackward0>)
3
pi: tensor([[9.9972e-01, 2.8473e-04],
        [9.2303e-03, 9.9077e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0206, 0.9794], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1986, 0.2603],
         [0.5512, 0.1640]],

        [[0.5609, 0.1250],
         [0.6020, 0.6417]],

        [[0.6692, 0.1942],
         [0.5154, 0.7259]],

        [[0.5572, 0.0815],
         [0.5741, 0.7099]],

        [[0.7113, 0.1772],
         [0.6276, 0.5451]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.004212316740111065
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.003243945514707375
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.002821183194503557
Global Adjusted Rand Index: -0.0005536662537069987
Average Adjusted Rand Index: -0.002045969942743357
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20702.080078125
inf tensor(20702.0801, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11032.0068359375
tensor(20702.0801, grad_fn=<NegBackward0>) tensor(11032.0068, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11031.5009765625
tensor(11032.0068, grad_fn=<NegBackward0>) tensor(11031.5010, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11031.365234375
tensor(11031.5010, grad_fn=<NegBackward0>) tensor(11031.3652, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11031.3046875
tensor(11031.3652, grad_fn=<NegBackward0>) tensor(11031.3047, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11031.2578125
tensor(11031.3047, grad_fn=<NegBackward0>) tensor(11031.2578, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11031.212890625
tensor(11031.2578, grad_fn=<NegBackward0>) tensor(11031.2129, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11031.166015625
tensor(11031.2129, grad_fn=<NegBackward0>) tensor(11031.1660, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11031.111328125
tensor(11031.1660, grad_fn=<NegBackward0>) tensor(11031.1113, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11031.0556640625
tensor(11031.1113, grad_fn=<NegBackward0>) tensor(11031.0557, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11030.9970703125
tensor(11031.0557, grad_fn=<NegBackward0>) tensor(11030.9971, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11030.9365234375
tensor(11030.9971, grad_fn=<NegBackward0>) tensor(11030.9365, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11030.8720703125
tensor(11030.9365, grad_fn=<NegBackward0>) tensor(11030.8721, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11030.8046875
tensor(11030.8721, grad_fn=<NegBackward0>) tensor(11030.8047, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11030.728515625
tensor(11030.8047, grad_fn=<NegBackward0>) tensor(11030.7285, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11030.634765625
tensor(11030.7285, grad_fn=<NegBackward0>) tensor(11030.6348, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11030.5205078125
tensor(11030.6348, grad_fn=<NegBackward0>) tensor(11030.5205, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11030.396484375
tensor(11030.5205, grad_fn=<NegBackward0>) tensor(11030.3965, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11030.267578125
tensor(11030.3965, grad_fn=<NegBackward0>) tensor(11030.2676, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11030.12109375
tensor(11030.2676, grad_fn=<NegBackward0>) tensor(11030.1211, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11029.9775390625
tensor(11030.1211, grad_fn=<NegBackward0>) tensor(11029.9775, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11029.9013671875
tensor(11029.9775, grad_fn=<NegBackward0>) tensor(11029.9014, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11029.869140625
tensor(11029.9014, grad_fn=<NegBackward0>) tensor(11029.8691, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11029.8505859375
tensor(11029.8691, grad_fn=<NegBackward0>) tensor(11029.8506, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11029.828125
tensor(11029.8506, grad_fn=<NegBackward0>) tensor(11029.8281, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11029.798828125
tensor(11029.8281, grad_fn=<NegBackward0>) tensor(11029.7988, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11029.7451171875
tensor(11029.7988, grad_fn=<NegBackward0>) tensor(11029.7451, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11029.607421875
tensor(11029.7451, grad_fn=<NegBackward0>) tensor(11029.6074, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11027.5478515625
tensor(11029.6074, grad_fn=<NegBackward0>) tensor(11027.5479, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11026.7158203125
tensor(11027.5479, grad_fn=<NegBackward0>) tensor(11026.7158, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11026.5458984375
tensor(11026.7158, grad_fn=<NegBackward0>) tensor(11026.5459, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11026.4677734375
tensor(11026.5459, grad_fn=<NegBackward0>) tensor(11026.4678, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11026.43359375
tensor(11026.4678, grad_fn=<NegBackward0>) tensor(11026.4336, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11026.4111328125
tensor(11026.4336, grad_fn=<NegBackward0>) tensor(11026.4111, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11026.396484375
tensor(11026.4111, grad_fn=<NegBackward0>) tensor(11026.3965, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11026.384765625
tensor(11026.3965, grad_fn=<NegBackward0>) tensor(11026.3848, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11026.3740234375
tensor(11026.3848, grad_fn=<NegBackward0>) tensor(11026.3740, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11026.369140625
tensor(11026.3740, grad_fn=<NegBackward0>) tensor(11026.3691, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11026.3623046875
tensor(11026.3691, grad_fn=<NegBackward0>) tensor(11026.3623, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11026.3583984375
tensor(11026.3623, grad_fn=<NegBackward0>) tensor(11026.3584, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11026.35546875
tensor(11026.3584, grad_fn=<NegBackward0>) tensor(11026.3555, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11026.3515625
tensor(11026.3555, grad_fn=<NegBackward0>) tensor(11026.3516, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11026.3486328125
tensor(11026.3516, grad_fn=<NegBackward0>) tensor(11026.3486, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11026.34765625
tensor(11026.3486, grad_fn=<NegBackward0>) tensor(11026.3477, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11026.345703125
tensor(11026.3477, grad_fn=<NegBackward0>) tensor(11026.3457, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11026.3427734375
tensor(11026.3457, grad_fn=<NegBackward0>) tensor(11026.3428, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11026.3408203125
tensor(11026.3428, grad_fn=<NegBackward0>) tensor(11026.3408, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11026.3388671875
tensor(11026.3408, grad_fn=<NegBackward0>) tensor(11026.3389, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11026.337890625
tensor(11026.3389, grad_fn=<NegBackward0>) tensor(11026.3379, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11026.3359375
tensor(11026.3379, grad_fn=<NegBackward0>) tensor(11026.3359, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11026.333984375
tensor(11026.3359, grad_fn=<NegBackward0>) tensor(11026.3340, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11026.3349609375
tensor(11026.3340, grad_fn=<NegBackward0>) tensor(11026.3350, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11026.333984375
tensor(11026.3340, grad_fn=<NegBackward0>) tensor(11026.3340, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11026.3310546875
tensor(11026.3340, grad_fn=<NegBackward0>) tensor(11026.3311, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11026.3330078125
tensor(11026.3311, grad_fn=<NegBackward0>) tensor(11026.3330, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11026.330078125
tensor(11026.3311, grad_fn=<NegBackward0>) tensor(11026.3301, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11026.3291015625
tensor(11026.3301, grad_fn=<NegBackward0>) tensor(11026.3291, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11026.330078125
tensor(11026.3291, grad_fn=<NegBackward0>) tensor(11026.3301, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11026.3291015625
tensor(11026.3291, grad_fn=<NegBackward0>) tensor(11026.3291, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11026.328125
tensor(11026.3291, grad_fn=<NegBackward0>) tensor(11026.3281, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11026.3271484375
tensor(11026.3281, grad_fn=<NegBackward0>) tensor(11026.3271, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11026.326171875
tensor(11026.3271, grad_fn=<NegBackward0>) tensor(11026.3262, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11026.326171875
tensor(11026.3262, grad_fn=<NegBackward0>) tensor(11026.3262, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11026.326171875
tensor(11026.3262, grad_fn=<NegBackward0>) tensor(11026.3262, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11026.326171875
tensor(11026.3262, grad_fn=<NegBackward0>) tensor(11026.3262, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11026.32421875
tensor(11026.3262, grad_fn=<NegBackward0>) tensor(11026.3242, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11026.32421875
tensor(11026.3242, grad_fn=<NegBackward0>) tensor(11026.3242, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11026.32421875
tensor(11026.3242, grad_fn=<NegBackward0>) tensor(11026.3242, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11026.3232421875
tensor(11026.3242, grad_fn=<NegBackward0>) tensor(11026.3232, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11026.3212890625
tensor(11026.3232, grad_fn=<NegBackward0>) tensor(11026.3213, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11026.322265625
tensor(11026.3213, grad_fn=<NegBackward0>) tensor(11026.3223, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11026.322265625
tensor(11026.3213, grad_fn=<NegBackward0>) tensor(11026.3223, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11026.3701171875
tensor(11026.3213, grad_fn=<NegBackward0>) tensor(11026.3701, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -11026.3232421875
tensor(11026.3213, grad_fn=<NegBackward0>) tensor(11026.3232, grad_fn=<NegBackward0>)
4
Iteration 7400: Loss = -11026.3203125
tensor(11026.3213, grad_fn=<NegBackward0>) tensor(11026.3203, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11026.3212890625
tensor(11026.3203, grad_fn=<NegBackward0>) tensor(11026.3213, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11026.3203125
tensor(11026.3203, grad_fn=<NegBackward0>) tensor(11026.3203, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11026.6396484375
tensor(11026.3203, grad_fn=<NegBackward0>) tensor(11026.6396, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11026.318359375
tensor(11026.3203, grad_fn=<NegBackward0>) tensor(11026.3184, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11026.318359375
tensor(11026.3184, grad_fn=<NegBackward0>) tensor(11026.3184, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11026.318359375
tensor(11026.3184, grad_fn=<NegBackward0>) tensor(11026.3184, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11026.318359375
tensor(11026.3184, grad_fn=<NegBackward0>) tensor(11026.3184, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11026.3193359375
tensor(11026.3184, grad_fn=<NegBackward0>) tensor(11026.3193, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11026.3173828125
tensor(11026.3184, grad_fn=<NegBackward0>) tensor(11026.3174, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11026.3564453125
tensor(11026.3174, grad_fn=<NegBackward0>) tensor(11026.3564, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11026.3193359375
tensor(11026.3174, grad_fn=<NegBackward0>) tensor(11026.3193, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11026.3173828125
tensor(11026.3174, grad_fn=<NegBackward0>) tensor(11026.3174, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11026.31640625
tensor(11026.3174, grad_fn=<NegBackward0>) tensor(11026.3164, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11026.31640625
tensor(11026.3164, grad_fn=<NegBackward0>) tensor(11026.3164, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11026.3173828125
tensor(11026.3164, grad_fn=<NegBackward0>) tensor(11026.3174, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11026.31640625
tensor(11026.3164, grad_fn=<NegBackward0>) tensor(11026.3164, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11026.423828125
tensor(11026.3164, grad_fn=<NegBackward0>) tensor(11026.4238, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11026.31640625
tensor(11026.3164, grad_fn=<NegBackward0>) tensor(11026.3164, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11026.3154296875
tensor(11026.3164, grad_fn=<NegBackward0>) tensor(11026.3154, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11026.3154296875
tensor(11026.3154, grad_fn=<NegBackward0>) tensor(11026.3154, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11026.314453125
tensor(11026.3154, grad_fn=<NegBackward0>) tensor(11026.3145, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11026.3154296875
tensor(11026.3145, grad_fn=<NegBackward0>) tensor(11026.3154, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11026.314453125
tensor(11026.3145, grad_fn=<NegBackward0>) tensor(11026.3145, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11026.31640625
tensor(11026.3145, grad_fn=<NegBackward0>) tensor(11026.3164, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11026.3154296875
tensor(11026.3145, grad_fn=<NegBackward0>) tensor(11026.3154, grad_fn=<NegBackward0>)
2
pi: tensor([[9.9077e-01, 9.2333e-03],
        [2.5893e-04, 9.9974e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9793, 0.0207], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1640, 0.2599],
         [0.6312, 0.1988]],

        [[0.6358, 0.1250],
         [0.7089, 0.5063]],

        [[0.6762, 0.1941],
         [0.6612, 0.6686]],

        [[0.6856, 0.0816],
         [0.7213, 0.6660]],

        [[0.5077, 0.1771],
         [0.7033, 0.5903]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.004212316740111065
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.003243945514707375
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.002821183194503557
Global Adjusted Rand Index: -0.0005536662537069987
Average Adjusted Rand Index: -0.002045969942743357
[-0.0005536662537069987, -0.0005536662537069987] [-0.002045969942743357, -0.002045969942743357] [11026.322265625, 11026.3154296875]
-------------------------------------
This iteration is 86
True Objective function: Loss = -10999.188979828867
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22904.279296875
inf tensor(22904.2793, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11074.111328125
tensor(22904.2793, grad_fn=<NegBackward0>) tensor(11074.1113, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11073.5380859375
tensor(11074.1113, grad_fn=<NegBackward0>) tensor(11073.5381, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11072.13671875
tensor(11073.5381, grad_fn=<NegBackward0>) tensor(11072.1367, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11070.2841796875
tensor(11072.1367, grad_fn=<NegBackward0>) tensor(11070.2842, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11069.6015625
tensor(11070.2842, grad_fn=<NegBackward0>) tensor(11069.6016, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11069.2548828125
tensor(11069.6016, grad_fn=<NegBackward0>) tensor(11069.2549, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11069.0322265625
tensor(11069.2549, grad_fn=<NegBackward0>) tensor(11069.0322, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11068.857421875
tensor(11069.0322, grad_fn=<NegBackward0>) tensor(11068.8574, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11068.595703125
tensor(11068.8574, grad_fn=<NegBackward0>) tensor(11068.5957, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11068.05859375
tensor(11068.5957, grad_fn=<NegBackward0>) tensor(11068.0586, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11067.3037109375
tensor(11068.0586, grad_fn=<NegBackward0>) tensor(11067.3037, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11066.9130859375
tensor(11067.3037, grad_fn=<NegBackward0>) tensor(11066.9131, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11066.7626953125
tensor(11066.9131, grad_fn=<NegBackward0>) tensor(11066.7627, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11066.6787109375
tensor(11066.7627, grad_fn=<NegBackward0>) tensor(11066.6787, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11066.619140625
tensor(11066.6787, grad_fn=<NegBackward0>) tensor(11066.6191, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11066.572265625
tensor(11066.6191, grad_fn=<NegBackward0>) tensor(11066.5723, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11066.5302734375
tensor(11066.5723, grad_fn=<NegBackward0>) tensor(11066.5303, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11066.490234375
tensor(11066.5303, grad_fn=<NegBackward0>) tensor(11066.4902, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11066.44921875
tensor(11066.4902, grad_fn=<NegBackward0>) tensor(11066.4492, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11066.40625
tensor(11066.4492, grad_fn=<NegBackward0>) tensor(11066.4062, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11066.3603515625
tensor(11066.4062, grad_fn=<NegBackward0>) tensor(11066.3604, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11066.3095703125
tensor(11066.3604, grad_fn=<NegBackward0>) tensor(11066.3096, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11066.251953125
tensor(11066.3096, grad_fn=<NegBackward0>) tensor(11066.2520, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11066.1845703125
tensor(11066.2520, grad_fn=<NegBackward0>) tensor(11066.1846, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11066.10546875
tensor(11066.1846, grad_fn=<NegBackward0>) tensor(11066.1055, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11066.009765625
tensor(11066.1055, grad_fn=<NegBackward0>) tensor(11066.0098, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11065.91015625
tensor(11066.0098, grad_fn=<NegBackward0>) tensor(11065.9102, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11065.822265625
tensor(11065.9102, grad_fn=<NegBackward0>) tensor(11065.8223, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11065.759765625
tensor(11065.8223, grad_fn=<NegBackward0>) tensor(11065.7598, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11065.7265625
tensor(11065.7598, grad_fn=<NegBackward0>) tensor(11065.7266, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11065.7119140625
tensor(11065.7266, grad_fn=<NegBackward0>) tensor(11065.7119, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11065.7041015625
tensor(11065.7119, grad_fn=<NegBackward0>) tensor(11065.7041, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11065.69921875
tensor(11065.7041, grad_fn=<NegBackward0>) tensor(11065.6992, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11065.697265625
tensor(11065.6992, grad_fn=<NegBackward0>) tensor(11065.6973, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11065.693359375
tensor(11065.6973, grad_fn=<NegBackward0>) tensor(11065.6934, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11065.6923828125
tensor(11065.6934, grad_fn=<NegBackward0>) tensor(11065.6924, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11065.689453125
tensor(11065.6924, grad_fn=<NegBackward0>) tensor(11065.6895, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11065.6884765625
tensor(11065.6895, grad_fn=<NegBackward0>) tensor(11065.6885, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11065.6865234375
tensor(11065.6885, grad_fn=<NegBackward0>) tensor(11065.6865, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11065.685546875
tensor(11065.6865, grad_fn=<NegBackward0>) tensor(11065.6855, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11065.685546875
tensor(11065.6855, grad_fn=<NegBackward0>) tensor(11065.6855, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11065.6826171875
tensor(11065.6855, grad_fn=<NegBackward0>) tensor(11065.6826, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11065.681640625
tensor(11065.6826, grad_fn=<NegBackward0>) tensor(11065.6816, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11065.681640625
tensor(11065.6816, grad_fn=<NegBackward0>) tensor(11065.6816, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11065.6796875
tensor(11065.6816, grad_fn=<NegBackward0>) tensor(11065.6797, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11065.681640625
tensor(11065.6797, grad_fn=<NegBackward0>) tensor(11065.6816, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11065.677734375
tensor(11065.6797, grad_fn=<NegBackward0>) tensor(11065.6777, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11065.677734375
tensor(11065.6777, grad_fn=<NegBackward0>) tensor(11065.6777, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11065.6767578125
tensor(11065.6777, grad_fn=<NegBackward0>) tensor(11065.6768, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11065.677734375
tensor(11065.6768, grad_fn=<NegBackward0>) tensor(11065.6777, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11065.6826171875
tensor(11065.6768, grad_fn=<NegBackward0>) tensor(11065.6826, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11065.67578125
tensor(11065.6768, grad_fn=<NegBackward0>) tensor(11065.6758, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11065.67578125
tensor(11065.6758, grad_fn=<NegBackward0>) tensor(11065.6758, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11065.67578125
tensor(11065.6758, grad_fn=<NegBackward0>) tensor(11065.6758, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11065.6767578125
tensor(11065.6758, grad_fn=<NegBackward0>) tensor(11065.6768, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11065.6748046875
tensor(11065.6758, grad_fn=<NegBackward0>) tensor(11065.6748, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11065.6728515625
tensor(11065.6748, grad_fn=<NegBackward0>) tensor(11065.6729, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11065.68359375
tensor(11065.6729, grad_fn=<NegBackward0>) tensor(11065.6836, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11065.671875
tensor(11065.6729, grad_fn=<NegBackward0>) tensor(11065.6719, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11065.671875
tensor(11065.6719, grad_fn=<NegBackward0>) tensor(11065.6719, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11065.673828125
tensor(11065.6719, grad_fn=<NegBackward0>) tensor(11065.6738, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11065.671875
tensor(11065.6719, grad_fn=<NegBackward0>) tensor(11065.6719, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11065.671875
tensor(11065.6719, grad_fn=<NegBackward0>) tensor(11065.6719, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11065.6708984375
tensor(11065.6719, grad_fn=<NegBackward0>) tensor(11065.6709, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11065.6708984375
tensor(11065.6709, grad_fn=<NegBackward0>) tensor(11065.6709, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11065.6708984375
tensor(11065.6709, grad_fn=<NegBackward0>) tensor(11065.6709, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11065.671875
tensor(11065.6709, grad_fn=<NegBackward0>) tensor(11065.6719, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11065.6708984375
tensor(11065.6709, grad_fn=<NegBackward0>) tensor(11065.6709, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11065.6708984375
tensor(11065.6709, grad_fn=<NegBackward0>) tensor(11065.6709, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11065.671875
tensor(11065.6709, grad_fn=<NegBackward0>) tensor(11065.6719, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11065.6708984375
tensor(11065.6709, grad_fn=<NegBackward0>) tensor(11065.6709, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11065.6767578125
tensor(11065.6709, grad_fn=<NegBackward0>) tensor(11065.6768, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11065.6708984375
tensor(11065.6709, grad_fn=<NegBackward0>) tensor(11065.6709, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11065.669921875
tensor(11065.6709, grad_fn=<NegBackward0>) tensor(11065.6699, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11065.6708984375
tensor(11065.6699, grad_fn=<NegBackward0>) tensor(11065.6709, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11065.669921875
tensor(11065.6699, grad_fn=<NegBackward0>) tensor(11065.6699, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11065.75390625
tensor(11065.6699, grad_fn=<NegBackward0>) tensor(11065.7539, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11065.67578125
tensor(11065.6699, grad_fn=<NegBackward0>) tensor(11065.6758, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11065.669921875
tensor(11065.6699, grad_fn=<NegBackward0>) tensor(11065.6699, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11065.673828125
tensor(11065.6699, grad_fn=<NegBackward0>) tensor(11065.6738, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11065.669921875
tensor(11065.6699, grad_fn=<NegBackward0>) tensor(11065.6699, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11065.671875
tensor(11065.6699, grad_fn=<NegBackward0>) tensor(11065.6719, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11065.66796875
tensor(11065.6699, grad_fn=<NegBackward0>) tensor(11065.6680, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11065.6689453125
tensor(11065.6680, grad_fn=<NegBackward0>) tensor(11065.6689, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11065.66796875
tensor(11065.6680, grad_fn=<NegBackward0>) tensor(11065.6680, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11065.6689453125
tensor(11065.6680, grad_fn=<NegBackward0>) tensor(11065.6689, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11065.6669921875
tensor(11065.6680, grad_fn=<NegBackward0>) tensor(11065.6670, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11065.6689453125
tensor(11065.6670, grad_fn=<NegBackward0>) tensor(11065.6689, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11065.67578125
tensor(11065.6670, grad_fn=<NegBackward0>) tensor(11065.6758, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -11065.6669921875
tensor(11065.6670, grad_fn=<NegBackward0>) tensor(11065.6670, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11065.6787109375
tensor(11065.6670, grad_fn=<NegBackward0>) tensor(11065.6787, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11065.6689453125
tensor(11065.6670, grad_fn=<NegBackward0>) tensor(11065.6689, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11065.6689453125
tensor(11065.6670, grad_fn=<NegBackward0>) tensor(11065.6689, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -11065.66796875
tensor(11065.6670, grad_fn=<NegBackward0>) tensor(11065.6680, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -11065.7568359375
tensor(11065.6670, grad_fn=<NegBackward0>) tensor(11065.7568, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9500 due to no improvement.
pi: tensor([[9.4033e-01, 5.9671e-02],
        [9.9989e-01, 1.0861e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3873, 0.6127], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1618, 0.1704],
         [0.6432, 0.1776]],

        [[0.5935, 0.2717],
         [0.5691, 0.5388]],

        [[0.7260, 0.1684],
         [0.6234, 0.6279]],

        [[0.6366, 0.0877],
         [0.6251, 0.5186]],

        [[0.5113, 0.2434],
         [0.6980, 0.7124]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.010091437982433116
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 4
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.005119313988496187
Global Adjusted Rand Index: 0.003212397685636279
Average Adjusted Rand Index: 0.0021893958647044897
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26829.19140625
inf tensor(26829.1914, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11071.2724609375
tensor(26829.1914, grad_fn=<NegBackward0>) tensor(11071.2725, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11069.544921875
tensor(11071.2725, grad_fn=<NegBackward0>) tensor(11069.5449, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11026.853515625
tensor(11069.5449, grad_fn=<NegBackward0>) tensor(11026.8535, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10941.4619140625
tensor(11026.8535, grad_fn=<NegBackward0>) tensor(10941.4619, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10939.4658203125
tensor(10941.4619, grad_fn=<NegBackward0>) tensor(10939.4658, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10939.44921875
tensor(10939.4658, grad_fn=<NegBackward0>) tensor(10939.4492, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10939.4404296875
tensor(10939.4492, grad_fn=<NegBackward0>) tensor(10939.4404, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10939.4365234375
tensor(10939.4404, grad_fn=<NegBackward0>) tensor(10939.4365, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10939.43359375
tensor(10939.4365, grad_fn=<NegBackward0>) tensor(10939.4336, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10939.4326171875
tensor(10939.4336, grad_fn=<NegBackward0>) tensor(10939.4326, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10939.4326171875
tensor(10939.4326, grad_fn=<NegBackward0>) tensor(10939.4326, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10939.4306640625
tensor(10939.4326, grad_fn=<NegBackward0>) tensor(10939.4307, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10939.4306640625
tensor(10939.4307, grad_fn=<NegBackward0>) tensor(10939.4307, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10939.4296875
tensor(10939.4307, grad_fn=<NegBackward0>) tensor(10939.4297, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10939.4306640625
tensor(10939.4297, grad_fn=<NegBackward0>) tensor(10939.4307, grad_fn=<NegBackward0>)
1
Iteration 1600: Loss = -10939.4287109375
tensor(10939.4297, grad_fn=<NegBackward0>) tensor(10939.4287, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10939.4296875
tensor(10939.4287, grad_fn=<NegBackward0>) tensor(10939.4297, grad_fn=<NegBackward0>)
1
Iteration 1800: Loss = -10939.4287109375
tensor(10939.4287, grad_fn=<NegBackward0>) tensor(10939.4287, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10939.4306640625
tensor(10939.4287, grad_fn=<NegBackward0>) tensor(10939.4307, grad_fn=<NegBackward0>)
1
Iteration 2000: Loss = -10939.4296875
tensor(10939.4287, grad_fn=<NegBackward0>) tensor(10939.4297, grad_fn=<NegBackward0>)
2
Iteration 2100: Loss = -10939.427734375
tensor(10939.4287, grad_fn=<NegBackward0>) tensor(10939.4277, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10939.4287109375
tensor(10939.4277, grad_fn=<NegBackward0>) tensor(10939.4287, grad_fn=<NegBackward0>)
1
Iteration 2300: Loss = -10939.427734375
tensor(10939.4277, grad_fn=<NegBackward0>) tensor(10939.4277, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10939.4306640625
tensor(10939.4277, grad_fn=<NegBackward0>) tensor(10939.4307, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -10939.431640625
tensor(10939.4277, grad_fn=<NegBackward0>) tensor(10939.4316, grad_fn=<NegBackward0>)
2
Iteration 2600: Loss = -10939.4296875
tensor(10939.4277, grad_fn=<NegBackward0>) tensor(10939.4297, grad_fn=<NegBackward0>)
3
Iteration 2700: Loss = -10939.4296875
tensor(10939.4277, grad_fn=<NegBackward0>) tensor(10939.4297, grad_fn=<NegBackward0>)
4
Iteration 2800: Loss = -10939.427734375
tensor(10939.4277, grad_fn=<NegBackward0>) tensor(10939.4277, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10939.4267578125
tensor(10939.4277, grad_fn=<NegBackward0>) tensor(10939.4268, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10939.427734375
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4277, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -10939.427734375
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4277, grad_fn=<NegBackward0>)
2
Iteration 3200: Loss = -10939.427734375
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4277, grad_fn=<NegBackward0>)
3
Iteration 3300: Loss = -10939.4267578125
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4268, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10939.427734375
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4277, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -10939.427734375
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4277, grad_fn=<NegBackward0>)
2
Iteration 3600: Loss = -10939.427734375
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4277, grad_fn=<NegBackward0>)
3
Iteration 3700: Loss = -10939.427734375
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4277, grad_fn=<NegBackward0>)
4
Iteration 3800: Loss = -10939.4267578125
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4268, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10939.4267578125
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4268, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10939.4267578125
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4268, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10939.4267578125
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4268, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10939.4267578125
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4268, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10939.4287109375
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4287, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -10939.4267578125
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4268, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10939.4296875
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4297, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10939.4267578125
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4268, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10939.427734375
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4277, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10939.4326171875
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4326, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -10939.4287109375
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4287, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -10939.435546875
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4355, grad_fn=<NegBackward0>)
4
Iteration 5100: Loss = -10939.427734375
tensor(10939.4268, grad_fn=<NegBackward0>) tensor(10939.4277, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5100 due to no improvement.
pi: tensor([[0.7735, 0.2265],
        [0.3041, 0.6959]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4920, 0.5080], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2464, 0.1119],
         [0.6290, 0.2015]],

        [[0.5612, 0.1066],
         [0.5133, 0.5976]],

        [[0.5262, 0.1016],
         [0.7268, 0.6297]],

        [[0.5110, 0.0833],
         [0.5871, 0.7278]],

        [[0.7104, 0.1032],
         [0.6346, 0.6322]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 8
Adjusted Rand Index: 0.70261383436655
time is 1
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 7
Adjusted Rand Index: 0.7369964883895512
time is 2
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 7
Adjusted Rand Index: 0.7369696969696969
time is 3
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080875752406894
Global Adjusted Rand Index: 0.7739479692318709
Average Adjusted Rand Index: 0.773418367478146
[0.003212397685636279, 0.7739479692318709] [0.0021893958647044897, 0.773418367478146] [11065.7568359375, 10939.427734375]
-------------------------------------
This iteration is 87
True Objective function: Loss = -10823.670064293823
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21489.275390625
inf tensor(21489.2754, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10914.03515625
tensor(21489.2754, grad_fn=<NegBackward0>) tensor(10914.0352, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10913.52734375
tensor(10914.0352, grad_fn=<NegBackward0>) tensor(10913.5273, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10913.3994140625
tensor(10913.5273, grad_fn=<NegBackward0>) tensor(10913.3994, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10913.3359375
tensor(10913.3994, grad_fn=<NegBackward0>) tensor(10913.3359, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10913.2939453125
tensor(10913.3359, grad_fn=<NegBackward0>) tensor(10913.2939, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10913.2607421875
tensor(10913.2939, grad_fn=<NegBackward0>) tensor(10913.2607, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10913.234375
tensor(10913.2607, grad_fn=<NegBackward0>) tensor(10913.2344, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10913.212890625
tensor(10913.2344, grad_fn=<NegBackward0>) tensor(10913.2129, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10913.19140625
tensor(10913.2129, grad_fn=<NegBackward0>) tensor(10913.1914, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10913.1708984375
tensor(10913.1914, grad_fn=<NegBackward0>) tensor(10913.1709, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10913.1494140625
tensor(10913.1709, grad_fn=<NegBackward0>) tensor(10913.1494, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10913.126953125
tensor(10913.1494, grad_fn=<NegBackward0>) tensor(10913.1270, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10913.099609375
tensor(10913.1270, grad_fn=<NegBackward0>) tensor(10913.0996, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10913.06640625
tensor(10913.0996, grad_fn=<NegBackward0>) tensor(10913.0664, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10913.0244140625
tensor(10913.0664, grad_fn=<NegBackward0>) tensor(10913.0244, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10912.982421875
tensor(10913.0244, grad_fn=<NegBackward0>) tensor(10912.9824, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10912.94140625
tensor(10912.9824, grad_fn=<NegBackward0>) tensor(10912.9414, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10912.90625
tensor(10912.9414, grad_fn=<NegBackward0>) tensor(10912.9062, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10912.8740234375
tensor(10912.9062, grad_fn=<NegBackward0>) tensor(10912.8740, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10912.8466796875
tensor(10912.8740, grad_fn=<NegBackward0>) tensor(10912.8467, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10912.8232421875
tensor(10912.8467, grad_fn=<NegBackward0>) tensor(10912.8232, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10912.8017578125
tensor(10912.8232, grad_fn=<NegBackward0>) tensor(10912.8018, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10912.78515625
tensor(10912.8018, grad_fn=<NegBackward0>) tensor(10912.7852, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10912.7724609375
tensor(10912.7852, grad_fn=<NegBackward0>) tensor(10912.7725, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10912.7607421875
tensor(10912.7725, grad_fn=<NegBackward0>) tensor(10912.7607, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10912.7490234375
tensor(10912.7607, grad_fn=<NegBackward0>) tensor(10912.7490, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10912.7333984375
tensor(10912.7490, grad_fn=<NegBackward0>) tensor(10912.7334, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10912.712890625
tensor(10912.7334, grad_fn=<NegBackward0>) tensor(10912.7129, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10912.685546875
tensor(10912.7129, grad_fn=<NegBackward0>) tensor(10912.6855, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10912.66015625
tensor(10912.6855, grad_fn=<NegBackward0>) tensor(10912.6602, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10912.6396484375
tensor(10912.6602, grad_fn=<NegBackward0>) tensor(10912.6396, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10912.623046875
tensor(10912.6396, grad_fn=<NegBackward0>) tensor(10912.6230, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10912.6142578125
tensor(10912.6230, grad_fn=<NegBackward0>) tensor(10912.6143, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10912.6064453125
tensor(10912.6143, grad_fn=<NegBackward0>) tensor(10912.6064, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10912.6005859375
tensor(10912.6064, grad_fn=<NegBackward0>) tensor(10912.6006, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10912.595703125
tensor(10912.6006, grad_fn=<NegBackward0>) tensor(10912.5957, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10912.595703125
tensor(10912.5957, grad_fn=<NegBackward0>) tensor(10912.5957, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10912.5888671875
tensor(10912.5957, grad_fn=<NegBackward0>) tensor(10912.5889, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10912.5888671875
tensor(10912.5889, grad_fn=<NegBackward0>) tensor(10912.5889, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10912.5859375
tensor(10912.5889, grad_fn=<NegBackward0>) tensor(10912.5859, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10912.583984375
tensor(10912.5859, grad_fn=<NegBackward0>) tensor(10912.5840, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10912.583984375
tensor(10912.5840, grad_fn=<NegBackward0>) tensor(10912.5840, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10912.58203125
tensor(10912.5840, grad_fn=<NegBackward0>) tensor(10912.5820, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10912.58203125
tensor(10912.5820, grad_fn=<NegBackward0>) tensor(10912.5820, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10912.58203125
tensor(10912.5820, grad_fn=<NegBackward0>) tensor(10912.5820, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10912.580078125
tensor(10912.5820, grad_fn=<NegBackward0>) tensor(10912.5801, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10912.5810546875
tensor(10912.5801, grad_fn=<NegBackward0>) tensor(10912.5811, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10912.5810546875
tensor(10912.5801, grad_fn=<NegBackward0>) tensor(10912.5811, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -10912.580078125
tensor(10912.5801, grad_fn=<NegBackward0>) tensor(10912.5801, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10912.580078125
tensor(10912.5801, grad_fn=<NegBackward0>) tensor(10912.5801, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10912.5791015625
tensor(10912.5801, grad_fn=<NegBackward0>) tensor(10912.5791, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10912.578125
tensor(10912.5791, grad_fn=<NegBackward0>) tensor(10912.5781, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10912.5771484375
tensor(10912.5781, grad_fn=<NegBackward0>) tensor(10912.5771, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10912.5791015625
tensor(10912.5771, grad_fn=<NegBackward0>) tensor(10912.5791, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10912.5771484375
tensor(10912.5771, grad_fn=<NegBackward0>) tensor(10912.5771, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10912.578125
tensor(10912.5771, grad_fn=<NegBackward0>) tensor(10912.5781, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10912.578125
tensor(10912.5771, grad_fn=<NegBackward0>) tensor(10912.5781, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -10912.58203125
tensor(10912.5771, grad_fn=<NegBackward0>) tensor(10912.5820, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -10912.578125
tensor(10912.5771, grad_fn=<NegBackward0>) tensor(10912.5781, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -10912.576171875
tensor(10912.5771, grad_fn=<NegBackward0>) tensor(10912.5762, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10912.578125
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5781, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10912.578125
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5781, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -10912.5810546875
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5811, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -10912.576171875
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5762, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10912.5966796875
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5967, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10912.5771484375
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5771, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -10912.5791015625
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5791, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -10912.576171875
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5762, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10912.5771484375
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5771, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10912.576171875
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5762, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10912.5771484375
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5771, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10912.580078125
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5801, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -10912.576171875
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5762, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10912.599609375
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5996, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10912.576171875
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5762, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10912.578125
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5781, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10912.576171875
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5762, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10912.5751953125
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5752, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10912.6162109375
tensor(10912.5752, grad_fn=<NegBackward0>) tensor(10912.6162, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10912.576171875
tensor(10912.5752, grad_fn=<NegBackward0>) tensor(10912.5762, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -10912.576171875
tensor(10912.5752, grad_fn=<NegBackward0>) tensor(10912.5762, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -10912.5771484375
tensor(10912.5752, grad_fn=<NegBackward0>) tensor(10912.5771, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -10912.576171875
tensor(10912.5752, grad_fn=<NegBackward0>) tensor(10912.5762, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8300 due to no improvement.
pi: tensor([[2.0251e-04, 9.9980e-01],
        [2.3117e-02, 9.7688e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4437, 0.5563], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1682, 0.1635],
         [0.5923, 0.1595]],

        [[0.5637, 0.0844],
         [0.5795, 0.6856]],

        [[0.6167, 0.2204],
         [0.6214, 0.5965]],

        [[0.6472, 0.1515],
         [0.6190, 0.5294]],

        [[0.6790, 0.2048],
         [0.6495, 0.5851]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0014922741295917668
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 6.693847882683921e-05
Average Adjusted Rand Index: -0.0002984548259183534
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21864.103515625
inf tensor(21864.1035, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10914.1123046875
tensor(21864.1035, grad_fn=<NegBackward0>) tensor(10914.1123, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10913.47265625
tensor(10914.1123, grad_fn=<NegBackward0>) tensor(10913.4727, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10913.3486328125
tensor(10913.4727, grad_fn=<NegBackward0>) tensor(10913.3486, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10913.2861328125
tensor(10913.3486, grad_fn=<NegBackward0>) tensor(10913.2861, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10913.244140625
tensor(10913.2861, grad_fn=<NegBackward0>) tensor(10913.2441, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10913.2099609375
tensor(10913.2441, grad_fn=<NegBackward0>) tensor(10913.2100, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10913.18359375
tensor(10913.2100, grad_fn=<NegBackward0>) tensor(10913.1836, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10913.1611328125
tensor(10913.1836, grad_fn=<NegBackward0>) tensor(10913.1611, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10913.1435546875
tensor(10913.1611, grad_fn=<NegBackward0>) tensor(10913.1436, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10913.125
tensor(10913.1436, grad_fn=<NegBackward0>) tensor(10913.1250, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10913.1064453125
tensor(10913.1250, grad_fn=<NegBackward0>) tensor(10913.1064, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10913.087890625
tensor(10913.1064, grad_fn=<NegBackward0>) tensor(10913.0879, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10913.06640625
tensor(10913.0879, grad_fn=<NegBackward0>) tensor(10913.0664, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10913.044921875
tensor(10913.0664, grad_fn=<NegBackward0>) tensor(10913.0449, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10913.0205078125
tensor(10913.0449, grad_fn=<NegBackward0>) tensor(10913.0205, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10912.9931640625
tensor(10913.0205, grad_fn=<NegBackward0>) tensor(10912.9932, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10912.966796875
tensor(10912.9932, grad_fn=<NegBackward0>) tensor(10912.9668, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10912.9453125
tensor(10912.9668, grad_fn=<NegBackward0>) tensor(10912.9453, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10912.927734375
tensor(10912.9453, grad_fn=<NegBackward0>) tensor(10912.9277, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10912.9111328125
tensor(10912.9277, grad_fn=<NegBackward0>) tensor(10912.9111, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10912.8974609375
tensor(10912.9111, grad_fn=<NegBackward0>) tensor(10912.8975, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10912.8818359375
tensor(10912.8975, grad_fn=<NegBackward0>) tensor(10912.8818, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10912.8642578125
tensor(10912.8818, grad_fn=<NegBackward0>) tensor(10912.8643, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10912.84375
tensor(10912.8643, grad_fn=<NegBackward0>) tensor(10912.8438, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10912.822265625
tensor(10912.8438, grad_fn=<NegBackward0>) tensor(10912.8223, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10912.7998046875
tensor(10912.8223, grad_fn=<NegBackward0>) tensor(10912.7998, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10912.7783203125
tensor(10912.7998, grad_fn=<NegBackward0>) tensor(10912.7783, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10912.7626953125
tensor(10912.7783, grad_fn=<NegBackward0>) tensor(10912.7627, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10912.748046875
tensor(10912.7627, grad_fn=<NegBackward0>) tensor(10912.7480, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10912.7353515625
tensor(10912.7480, grad_fn=<NegBackward0>) tensor(10912.7354, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10912.7255859375
tensor(10912.7354, grad_fn=<NegBackward0>) tensor(10912.7256, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10912.7158203125
tensor(10912.7256, grad_fn=<NegBackward0>) tensor(10912.7158, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10912.705078125
tensor(10912.7158, grad_fn=<NegBackward0>) tensor(10912.7051, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10912.6943359375
tensor(10912.7051, grad_fn=<NegBackward0>) tensor(10912.6943, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10912.68359375
tensor(10912.6943, grad_fn=<NegBackward0>) tensor(10912.6836, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10912.671875
tensor(10912.6836, grad_fn=<NegBackward0>) tensor(10912.6719, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10912.6630859375
tensor(10912.6719, grad_fn=<NegBackward0>) tensor(10912.6631, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10912.654296875
tensor(10912.6631, grad_fn=<NegBackward0>) tensor(10912.6543, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10912.642578125
tensor(10912.6543, grad_fn=<NegBackward0>) tensor(10912.6426, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10912.6337890625
tensor(10912.6426, grad_fn=<NegBackward0>) tensor(10912.6338, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10912.6298828125
tensor(10912.6338, grad_fn=<NegBackward0>) tensor(10912.6299, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10912.62109375
tensor(10912.6299, grad_fn=<NegBackward0>) tensor(10912.6211, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10912.615234375
tensor(10912.6211, grad_fn=<NegBackward0>) tensor(10912.6152, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10912.6162109375
tensor(10912.6152, grad_fn=<NegBackward0>) tensor(10912.6162, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10912.6064453125
tensor(10912.6152, grad_fn=<NegBackward0>) tensor(10912.6064, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10912.603515625
tensor(10912.6064, grad_fn=<NegBackward0>) tensor(10912.6035, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10912.6015625
tensor(10912.6035, grad_fn=<NegBackward0>) tensor(10912.6016, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10912.5986328125
tensor(10912.6016, grad_fn=<NegBackward0>) tensor(10912.5986, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10912.5966796875
tensor(10912.5986, grad_fn=<NegBackward0>) tensor(10912.5967, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10912.5927734375
tensor(10912.5967, grad_fn=<NegBackward0>) tensor(10912.5928, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10912.5927734375
tensor(10912.5928, grad_fn=<NegBackward0>) tensor(10912.5928, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10912.5888671875
tensor(10912.5928, grad_fn=<NegBackward0>) tensor(10912.5889, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10912.5869140625
tensor(10912.5889, grad_fn=<NegBackward0>) tensor(10912.5869, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10912.5849609375
tensor(10912.5869, grad_fn=<NegBackward0>) tensor(10912.5850, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10912.5849609375
tensor(10912.5850, grad_fn=<NegBackward0>) tensor(10912.5850, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10912.5830078125
tensor(10912.5850, grad_fn=<NegBackward0>) tensor(10912.5830, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10912.5830078125
tensor(10912.5830, grad_fn=<NegBackward0>) tensor(10912.5830, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10912.5810546875
tensor(10912.5830, grad_fn=<NegBackward0>) tensor(10912.5811, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10912.5810546875
tensor(10912.5811, grad_fn=<NegBackward0>) tensor(10912.5811, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10912.580078125
tensor(10912.5811, grad_fn=<NegBackward0>) tensor(10912.5801, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10912.580078125
tensor(10912.5801, grad_fn=<NegBackward0>) tensor(10912.5801, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10912.591796875
tensor(10912.5801, grad_fn=<NegBackward0>) tensor(10912.5918, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10912.578125
tensor(10912.5801, grad_fn=<NegBackward0>) tensor(10912.5781, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10912.5791015625
tensor(10912.5781, grad_fn=<NegBackward0>) tensor(10912.5791, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10912.578125
tensor(10912.5781, grad_fn=<NegBackward0>) tensor(10912.5781, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10912.578125
tensor(10912.5781, grad_fn=<NegBackward0>) tensor(10912.5781, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10912.578125
tensor(10912.5781, grad_fn=<NegBackward0>) tensor(10912.5781, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10912.5771484375
tensor(10912.5781, grad_fn=<NegBackward0>) tensor(10912.5771, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10912.578125
tensor(10912.5771, grad_fn=<NegBackward0>) tensor(10912.5781, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10912.5771484375
tensor(10912.5771, grad_fn=<NegBackward0>) tensor(10912.5771, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10912.5859375
tensor(10912.5771, grad_fn=<NegBackward0>) tensor(10912.5859, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10912.5771484375
tensor(10912.5771, grad_fn=<NegBackward0>) tensor(10912.5771, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10912.578125
tensor(10912.5771, grad_fn=<NegBackward0>) tensor(10912.5781, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -10912.578125
tensor(10912.5771, grad_fn=<NegBackward0>) tensor(10912.5781, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -10912.576171875
tensor(10912.5771, grad_fn=<NegBackward0>) tensor(10912.5762, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10912.5771484375
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5771, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10912.576171875
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5762, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10912.5771484375
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5771, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10912.576171875
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5762, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10912.6474609375
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.6475, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10912.576171875
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5762, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10912.576171875
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5762, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10912.5791015625
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5791, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10912.5771484375
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5771, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -10912.5771484375
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5771, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -10912.578125
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5781, grad_fn=<NegBackward0>)
4
Iteration 8700: Loss = -10912.576171875
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5762, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10912.5771484375
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5771, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10912.576171875
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5762, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10912.576171875
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5762, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10912.578125
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5781, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10912.6005859375
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.6006, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -10912.5771484375
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5771, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -10912.576171875
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5762, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10912.5771484375
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5771, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -10912.576171875
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5762, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10912.5751953125
tensor(10912.5762, grad_fn=<NegBackward0>) tensor(10912.5752, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10912.59765625
tensor(10912.5752, grad_fn=<NegBackward0>) tensor(10912.5977, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -10912.607421875
tensor(10912.5752, grad_fn=<NegBackward0>) tensor(10912.6074, grad_fn=<NegBackward0>)
2
pi: tensor([[1.6697e-04, 9.9983e-01],
        [2.2867e-02, 9.7713e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4442, 0.5558], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1681, 0.1630],
         [0.5374, 0.1603]],

        [[0.6776, 0.0844],
         [0.6257, 0.5834]],

        [[0.5576, 0.2204],
         [0.5483, 0.6233]],

        [[0.5022, 0.1515],
         [0.6647, 0.6158]],

        [[0.5216, 0.2048],
         [0.6975, 0.5564]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0014922741295917668
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 6.693847882683921e-05
Average Adjusted Rand Index: -0.0002984548259183534
[6.693847882683921e-05, 6.693847882683921e-05] [-0.0002984548259183534, -0.0002984548259183534] [10912.576171875, 10912.62109375]
-------------------------------------
This iteration is 88
True Objective function: Loss = -10902.1181628145
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24496.578125
inf tensor(24496.5781, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11001.7001953125
tensor(24496.5781, grad_fn=<NegBackward0>) tensor(11001.7002, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11000.890625
tensor(11001.7002, grad_fn=<NegBackward0>) tensor(11000.8906, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11000.6982421875
tensor(11000.8906, grad_fn=<NegBackward0>) tensor(11000.6982, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11000.59765625
tensor(11000.6982, grad_fn=<NegBackward0>) tensor(11000.5977, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11000.52734375
tensor(11000.5977, grad_fn=<NegBackward0>) tensor(11000.5273, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11000.478515625
tensor(11000.5273, grad_fn=<NegBackward0>) tensor(11000.4785, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11000.4345703125
tensor(11000.4785, grad_fn=<NegBackward0>) tensor(11000.4346, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11000.3994140625
tensor(11000.4346, grad_fn=<NegBackward0>) tensor(11000.3994, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11000.369140625
tensor(11000.3994, grad_fn=<NegBackward0>) tensor(11000.3691, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11000.3466796875
tensor(11000.3691, grad_fn=<NegBackward0>) tensor(11000.3467, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11000.3271484375
tensor(11000.3467, grad_fn=<NegBackward0>) tensor(11000.3271, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11000.3115234375
tensor(11000.3271, grad_fn=<NegBackward0>) tensor(11000.3115, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11000.296875
tensor(11000.3115, grad_fn=<NegBackward0>) tensor(11000.2969, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11000.2841796875
tensor(11000.2969, grad_fn=<NegBackward0>) tensor(11000.2842, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11000.2685546875
tensor(11000.2842, grad_fn=<NegBackward0>) tensor(11000.2686, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11000.2529296875
tensor(11000.2686, grad_fn=<NegBackward0>) tensor(11000.2529, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11000.234375
tensor(11000.2529, grad_fn=<NegBackward0>) tensor(11000.2344, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11000.208984375
tensor(11000.2344, grad_fn=<NegBackward0>) tensor(11000.2090, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11000.17578125
tensor(11000.2090, grad_fn=<NegBackward0>) tensor(11000.1758, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11000.1201171875
tensor(11000.1758, grad_fn=<NegBackward0>) tensor(11000.1201, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11000.017578125
tensor(11000.1201, grad_fn=<NegBackward0>) tensor(11000.0176, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10999.82421875
tensor(11000.0176, grad_fn=<NegBackward0>) tensor(10999.8242, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10999.599609375
tensor(10999.8242, grad_fn=<NegBackward0>) tensor(10999.5996, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10999.4521484375
tensor(10999.5996, grad_fn=<NegBackward0>) tensor(10999.4521, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10999.375
tensor(10999.4521, grad_fn=<NegBackward0>) tensor(10999.3750, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10999.3330078125
tensor(10999.3750, grad_fn=<NegBackward0>) tensor(10999.3330, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10999.306640625
tensor(10999.3330, grad_fn=<NegBackward0>) tensor(10999.3066, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10999.2880859375
tensor(10999.3066, grad_fn=<NegBackward0>) tensor(10999.2881, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10999.271484375
tensor(10999.2881, grad_fn=<NegBackward0>) tensor(10999.2715, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10999.2607421875
tensor(10999.2715, grad_fn=<NegBackward0>) tensor(10999.2607, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10999.2509765625
tensor(10999.2607, grad_fn=<NegBackward0>) tensor(10999.2510, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10999.2412109375
tensor(10999.2510, grad_fn=<NegBackward0>) tensor(10999.2412, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10999.23046875
tensor(10999.2412, grad_fn=<NegBackward0>) tensor(10999.2305, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10999.212890625
tensor(10999.2305, grad_fn=<NegBackward0>) tensor(10999.2129, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10999.171875
tensor(10999.2129, grad_fn=<NegBackward0>) tensor(10999.1719, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10999.0830078125
tensor(10999.1719, grad_fn=<NegBackward0>) tensor(10999.0830, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10999.0458984375
tensor(10999.0830, grad_fn=<NegBackward0>) tensor(10999.0459, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10999.025390625
tensor(10999.0459, grad_fn=<NegBackward0>) tensor(10999.0254, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10999.005859375
tensor(10999.0254, grad_fn=<NegBackward0>) tensor(10999.0059, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10998.9794921875
tensor(10999.0059, grad_fn=<NegBackward0>) tensor(10998.9795, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10998.931640625
tensor(10998.9795, grad_fn=<NegBackward0>) tensor(10998.9316, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10998.6865234375
tensor(10998.9316, grad_fn=<NegBackward0>) tensor(10998.6865, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10998.2685546875
tensor(10998.6865, grad_fn=<NegBackward0>) tensor(10998.2686, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10997.9541015625
tensor(10998.2686, grad_fn=<NegBackward0>) tensor(10997.9541, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10997.7060546875
tensor(10997.9541, grad_fn=<NegBackward0>) tensor(10997.7061, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10997.6826171875
tensor(10997.7061, grad_fn=<NegBackward0>) tensor(10997.6826, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10997.6708984375
tensor(10997.6826, grad_fn=<NegBackward0>) tensor(10997.6709, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10997.6630859375
tensor(10997.6709, grad_fn=<NegBackward0>) tensor(10997.6631, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10997.658203125
tensor(10997.6631, grad_fn=<NegBackward0>) tensor(10997.6582, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10997.6533203125
tensor(10997.6582, grad_fn=<NegBackward0>) tensor(10997.6533, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10997.6513671875
tensor(10997.6533, grad_fn=<NegBackward0>) tensor(10997.6514, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10997.6484375
tensor(10997.6514, grad_fn=<NegBackward0>) tensor(10997.6484, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10997.646484375
tensor(10997.6484, grad_fn=<NegBackward0>) tensor(10997.6465, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10997.646484375
tensor(10997.6465, grad_fn=<NegBackward0>) tensor(10997.6465, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10997.6435546875
tensor(10997.6465, grad_fn=<NegBackward0>) tensor(10997.6436, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10997.642578125
tensor(10997.6436, grad_fn=<NegBackward0>) tensor(10997.6426, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10997.6416015625
tensor(10997.6426, grad_fn=<NegBackward0>) tensor(10997.6416, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10997.6396484375
tensor(10997.6416, grad_fn=<NegBackward0>) tensor(10997.6396, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10997.6396484375
tensor(10997.6396, grad_fn=<NegBackward0>) tensor(10997.6396, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10997.638671875
tensor(10997.6396, grad_fn=<NegBackward0>) tensor(10997.6387, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10997.6396484375
tensor(10997.6387, grad_fn=<NegBackward0>) tensor(10997.6396, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10997.63671875
tensor(10997.6387, grad_fn=<NegBackward0>) tensor(10997.6367, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10997.6357421875
tensor(10997.6367, grad_fn=<NegBackward0>) tensor(10997.6357, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10997.63671875
tensor(10997.6357, grad_fn=<NegBackward0>) tensor(10997.6367, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10997.63671875
tensor(10997.6357, grad_fn=<NegBackward0>) tensor(10997.6367, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -10997.63671875
tensor(10997.6357, grad_fn=<NegBackward0>) tensor(10997.6367, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -10997.6357421875
tensor(10997.6357, grad_fn=<NegBackward0>) tensor(10997.6357, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10997.6357421875
tensor(10997.6357, grad_fn=<NegBackward0>) tensor(10997.6357, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10997.634765625
tensor(10997.6357, grad_fn=<NegBackward0>) tensor(10997.6348, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10997.634765625
tensor(10997.6348, grad_fn=<NegBackward0>) tensor(10997.6348, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10997.6337890625
tensor(10997.6348, grad_fn=<NegBackward0>) tensor(10997.6338, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10997.6435546875
tensor(10997.6338, grad_fn=<NegBackward0>) tensor(10997.6436, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10997.634765625
tensor(10997.6338, grad_fn=<NegBackward0>) tensor(10997.6348, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -10997.634765625
tensor(10997.6338, grad_fn=<NegBackward0>) tensor(10997.6348, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -10997.634765625
tensor(10997.6338, grad_fn=<NegBackward0>) tensor(10997.6348, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -10997.6337890625
tensor(10997.6338, grad_fn=<NegBackward0>) tensor(10997.6338, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10997.6337890625
tensor(10997.6338, grad_fn=<NegBackward0>) tensor(10997.6338, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10997.6337890625
tensor(10997.6338, grad_fn=<NegBackward0>) tensor(10997.6338, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10997.6337890625
tensor(10997.6338, grad_fn=<NegBackward0>) tensor(10997.6338, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10997.6328125
tensor(10997.6338, grad_fn=<NegBackward0>) tensor(10997.6328, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10997.6328125
tensor(10997.6328, grad_fn=<NegBackward0>) tensor(10997.6328, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10997.6328125
tensor(10997.6328, grad_fn=<NegBackward0>) tensor(10997.6328, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10997.673828125
tensor(10997.6328, grad_fn=<NegBackward0>) tensor(10997.6738, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10997.6318359375
tensor(10997.6328, grad_fn=<NegBackward0>) tensor(10997.6318, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10997.6337890625
tensor(10997.6318, grad_fn=<NegBackward0>) tensor(10997.6338, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -10997.6328125
tensor(10997.6318, grad_fn=<NegBackward0>) tensor(10997.6328, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -10997.6337890625
tensor(10997.6318, grad_fn=<NegBackward0>) tensor(10997.6338, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -10997.6328125
tensor(10997.6318, grad_fn=<NegBackward0>) tensor(10997.6328, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -10997.6318359375
tensor(10997.6318, grad_fn=<NegBackward0>) tensor(10997.6318, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10997.6328125
tensor(10997.6318, grad_fn=<NegBackward0>) tensor(10997.6328, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10997.6328125
tensor(10997.6318, grad_fn=<NegBackward0>) tensor(10997.6328, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -10997.6875
tensor(10997.6318, grad_fn=<NegBackward0>) tensor(10997.6875, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -10997.6328125
tensor(10997.6318, grad_fn=<NegBackward0>) tensor(10997.6328, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -10997.6708984375
tensor(10997.6318, grad_fn=<NegBackward0>) tensor(10997.6709, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9400 due to no improvement.
pi: tensor([[8.6446e-01, 1.3554e-01],
        [6.5273e-04, 9.9935e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5096, 0.4904], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1390, 0.1624],
         [0.5406, 0.1796]],

        [[0.7115, 0.1648],
         [0.6412, 0.7202]],

        [[0.5628, 0.1431],
         [0.6105, 0.6394]],

        [[0.6931, 0.1598],
         [0.5322, 0.6207]],

        [[0.5997, 0.1579],
         [0.5845, 0.5178]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.009642647065305817
time is 1
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.022813863221606185
time is 2
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 64
Adjusted Rand Index: 0.06982753052128415
time is 3
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.02396468876471811
time is 4
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.012633139431831
Global Adjusted Rand Index: 0.032001051370510764
Average Adjusted Rand Index: 0.027776373800949057
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23586.990234375
inf tensor(23586.9902, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10943.5205078125
tensor(23586.9902, grad_fn=<NegBackward0>) tensor(10943.5205, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10878.0419921875
tensor(10943.5205, grad_fn=<NegBackward0>) tensor(10878.0420, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10870.525390625
tensor(10878.0420, grad_fn=<NegBackward0>) tensor(10870.5254, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10870.4287109375
tensor(10870.5254, grad_fn=<NegBackward0>) tensor(10870.4287, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10870.3994140625
tensor(10870.4287, grad_fn=<NegBackward0>) tensor(10870.3994, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10870.3857421875
tensor(10870.3994, grad_fn=<NegBackward0>) tensor(10870.3857, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10870.3779296875
tensor(10870.3857, grad_fn=<NegBackward0>) tensor(10870.3779, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10870.3720703125
tensor(10870.3779, grad_fn=<NegBackward0>) tensor(10870.3721, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10870.361328125
tensor(10870.3721, grad_fn=<NegBackward0>) tensor(10870.3613, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10870.3583984375
tensor(10870.3613, grad_fn=<NegBackward0>) tensor(10870.3584, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10870.3564453125
tensor(10870.3584, grad_fn=<NegBackward0>) tensor(10870.3564, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10870.3564453125
tensor(10870.3564, grad_fn=<NegBackward0>) tensor(10870.3564, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10870.353515625
tensor(10870.3564, grad_fn=<NegBackward0>) tensor(10870.3535, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10870.353515625
tensor(10870.3535, grad_fn=<NegBackward0>) tensor(10870.3535, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10870.353515625
tensor(10870.3535, grad_fn=<NegBackward0>) tensor(10870.3535, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10870.3525390625
tensor(10870.3535, grad_fn=<NegBackward0>) tensor(10870.3525, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10870.3515625
tensor(10870.3525, grad_fn=<NegBackward0>) tensor(10870.3516, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10870.3505859375
tensor(10870.3516, grad_fn=<NegBackward0>) tensor(10870.3506, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10870.349609375
tensor(10870.3506, grad_fn=<NegBackward0>) tensor(10870.3496, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10870.349609375
tensor(10870.3496, grad_fn=<NegBackward0>) tensor(10870.3496, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10870.34765625
tensor(10870.3496, grad_fn=<NegBackward0>) tensor(10870.3477, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10870.345703125
tensor(10870.3477, grad_fn=<NegBackward0>) tensor(10870.3457, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10870.3447265625
tensor(10870.3457, grad_fn=<NegBackward0>) tensor(10870.3447, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10870.345703125
tensor(10870.3447, grad_fn=<NegBackward0>) tensor(10870.3457, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -10870.3447265625
tensor(10870.3447, grad_fn=<NegBackward0>) tensor(10870.3447, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10870.3447265625
tensor(10870.3447, grad_fn=<NegBackward0>) tensor(10870.3447, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10870.3447265625
tensor(10870.3447, grad_fn=<NegBackward0>) tensor(10870.3447, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10870.3427734375
tensor(10870.3447, grad_fn=<NegBackward0>) tensor(10870.3428, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10870.3447265625
tensor(10870.3428, grad_fn=<NegBackward0>) tensor(10870.3447, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -10870.3427734375
tensor(10870.3428, grad_fn=<NegBackward0>) tensor(10870.3428, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10870.3447265625
tensor(10870.3428, grad_fn=<NegBackward0>) tensor(10870.3447, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -10870.34375
tensor(10870.3428, grad_fn=<NegBackward0>) tensor(10870.3438, grad_fn=<NegBackward0>)
2
Iteration 3300: Loss = -10870.3447265625
tensor(10870.3428, grad_fn=<NegBackward0>) tensor(10870.3447, grad_fn=<NegBackward0>)
3
Iteration 3400: Loss = -10870.34375
tensor(10870.3428, grad_fn=<NegBackward0>) tensor(10870.3438, grad_fn=<NegBackward0>)
4
Iteration 3500: Loss = -10870.34375
tensor(10870.3428, grad_fn=<NegBackward0>) tensor(10870.3438, grad_fn=<NegBackward0>)
5
Stopping early at iteration 3500 due to no improvement.
pi: tensor([[0.1766, 0.8234],
        [0.7400, 0.2600]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4972, 0.5028], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2410, 0.1102],
         [0.6477, 0.2125]],

        [[0.6021, 0.1027],
         [0.6835, 0.5152]],

        [[0.6533, 0.0895],
         [0.5827, 0.5518]],

        [[0.5148, 0.0955],
         [0.6757, 0.5023]],

        [[0.5062, 0.1066],
         [0.6384, 0.6326]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 6
Adjusted Rand Index: 0.7721069260785004
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 92
Adjusted Rand Index: 0.7026170871755089
time is 2
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7721314419105764
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
Global Adjusted Rand Index: 0.044745775546834565
Average Adjusted Rand Index: 0.8178529777294112
[0.032001051370510764, 0.044745775546834565] [0.027776373800949057, 0.8178529777294112] [10997.6708984375, 10870.34375]
-------------------------------------
This iteration is 89
True Objective function: Loss = -11191.242752337863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20818.064453125
inf tensor(20818.0645, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11296.9482421875
tensor(20818.0645, grad_fn=<NegBackward0>) tensor(11296.9482, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11296.5673828125
tensor(11296.9482, grad_fn=<NegBackward0>) tensor(11296.5674, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11296.16796875
tensor(11296.5674, grad_fn=<NegBackward0>) tensor(11296.1680, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11295.5078125
tensor(11296.1680, grad_fn=<NegBackward0>) tensor(11295.5078, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11295.048828125
tensor(11295.5078, grad_fn=<NegBackward0>) tensor(11295.0488, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11294.7578125
tensor(11295.0488, grad_fn=<NegBackward0>) tensor(11294.7578, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11294.48046875
tensor(11294.7578, grad_fn=<NegBackward0>) tensor(11294.4805, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11294.150390625
tensor(11294.4805, grad_fn=<NegBackward0>) tensor(11294.1504, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11293.7861328125
tensor(11294.1504, grad_fn=<NegBackward0>) tensor(11293.7861, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11293.595703125
tensor(11293.7861, grad_fn=<NegBackward0>) tensor(11293.5957, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11293.427734375
tensor(11293.5957, grad_fn=<NegBackward0>) tensor(11293.4277, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11292.7890625
tensor(11293.4277, grad_fn=<NegBackward0>) tensor(11292.7891, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11262.1962890625
tensor(11292.7891, grad_fn=<NegBackward0>) tensor(11262.1963, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11176.5546875
tensor(11262.1963, grad_fn=<NegBackward0>) tensor(11176.5547, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11151.2568359375
tensor(11176.5547, grad_fn=<NegBackward0>) tensor(11151.2568, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11151.02734375
tensor(11151.2568, grad_fn=<NegBackward0>) tensor(11151.0273, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11150.9658203125
tensor(11151.0273, grad_fn=<NegBackward0>) tensor(11150.9658, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11147.2783203125
tensor(11150.9658, grad_fn=<NegBackward0>) tensor(11147.2783, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11146.58203125
tensor(11147.2783, grad_fn=<NegBackward0>) tensor(11146.5820, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11146.4833984375
tensor(11146.5820, grad_fn=<NegBackward0>) tensor(11146.4834, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11146.47265625
tensor(11146.4834, grad_fn=<NegBackward0>) tensor(11146.4727, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11146.4619140625
tensor(11146.4727, grad_fn=<NegBackward0>) tensor(11146.4619, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11146.4453125
tensor(11146.4619, grad_fn=<NegBackward0>) tensor(11146.4453, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11146.44140625
tensor(11146.4453, grad_fn=<NegBackward0>) tensor(11146.4414, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11146.42578125
tensor(11146.4414, grad_fn=<NegBackward0>) tensor(11146.4258, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11146.3857421875
tensor(11146.4258, grad_fn=<NegBackward0>) tensor(11146.3857, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11146.365234375
tensor(11146.3857, grad_fn=<NegBackward0>) tensor(11146.3652, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11146.3623046875
tensor(11146.3652, grad_fn=<NegBackward0>) tensor(11146.3623, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11146.359375
tensor(11146.3623, grad_fn=<NegBackward0>) tensor(11146.3594, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11146.361328125
tensor(11146.3594, grad_fn=<NegBackward0>) tensor(11146.3613, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11146.359375
tensor(11146.3594, grad_fn=<NegBackward0>) tensor(11146.3594, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11146.357421875
tensor(11146.3594, grad_fn=<NegBackward0>) tensor(11146.3574, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11146.357421875
tensor(11146.3574, grad_fn=<NegBackward0>) tensor(11146.3574, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11146.3564453125
tensor(11146.3574, grad_fn=<NegBackward0>) tensor(11146.3564, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11146.30859375
tensor(11146.3564, grad_fn=<NegBackward0>) tensor(11146.3086, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11146.2958984375
tensor(11146.3086, grad_fn=<NegBackward0>) tensor(11146.2959, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11146.294921875
tensor(11146.2959, grad_fn=<NegBackward0>) tensor(11146.2949, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11146.2958984375
tensor(11146.2949, grad_fn=<NegBackward0>) tensor(11146.2959, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11146.2939453125
tensor(11146.2949, grad_fn=<NegBackward0>) tensor(11146.2939, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11146.294921875
tensor(11146.2939, grad_fn=<NegBackward0>) tensor(11146.2949, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11146.2939453125
tensor(11146.2939, grad_fn=<NegBackward0>) tensor(11146.2939, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11146.2919921875
tensor(11146.2939, grad_fn=<NegBackward0>) tensor(11146.2920, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11146.298828125
tensor(11146.2920, grad_fn=<NegBackward0>) tensor(11146.2988, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11146.2919921875
tensor(11146.2920, grad_fn=<NegBackward0>) tensor(11146.2920, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11146.291015625
tensor(11146.2920, grad_fn=<NegBackward0>) tensor(11146.2910, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11146.2900390625
tensor(11146.2910, grad_fn=<NegBackward0>) tensor(11146.2900, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11146.2529296875
tensor(11146.2900, grad_fn=<NegBackward0>) tensor(11146.2529, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11146.2578125
tensor(11146.2529, grad_fn=<NegBackward0>) tensor(11146.2578, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11146.251953125
tensor(11146.2529, grad_fn=<NegBackward0>) tensor(11146.2520, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11146.2529296875
tensor(11146.2520, grad_fn=<NegBackward0>) tensor(11146.2529, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11146.251953125
tensor(11146.2520, grad_fn=<NegBackward0>) tensor(11146.2520, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11146.251953125
tensor(11146.2520, grad_fn=<NegBackward0>) tensor(11146.2520, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11146.25
tensor(11146.2520, grad_fn=<NegBackward0>) tensor(11146.2500, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11146.2529296875
tensor(11146.2500, grad_fn=<NegBackward0>) tensor(11146.2529, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11146.251953125
tensor(11146.2500, grad_fn=<NegBackward0>) tensor(11146.2520, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11146.2509765625
tensor(11146.2500, grad_fn=<NegBackward0>) tensor(11146.2510, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -11146.205078125
tensor(11146.2500, grad_fn=<NegBackward0>) tensor(11146.2051, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11146.2041015625
tensor(11146.2051, grad_fn=<NegBackward0>) tensor(11146.2041, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11146.2060546875
tensor(11146.2041, grad_fn=<NegBackward0>) tensor(11146.2061, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11146.2041015625
tensor(11146.2041, grad_fn=<NegBackward0>) tensor(11146.2041, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11146.2080078125
tensor(11146.2041, grad_fn=<NegBackward0>) tensor(11146.2080, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11146.2119140625
tensor(11146.2041, grad_fn=<NegBackward0>) tensor(11146.2119, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11146.205078125
tensor(11146.2041, grad_fn=<NegBackward0>) tensor(11146.2051, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -11146.203125
tensor(11146.2041, grad_fn=<NegBackward0>) tensor(11146.2031, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11146.2041015625
tensor(11146.2031, grad_fn=<NegBackward0>) tensor(11146.2041, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11146.2041015625
tensor(11146.2031, grad_fn=<NegBackward0>) tensor(11146.2041, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11146.203125
tensor(11146.2031, grad_fn=<NegBackward0>) tensor(11146.2031, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11146.19921875
tensor(11146.2031, grad_fn=<NegBackward0>) tensor(11146.1992, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11146.2001953125
tensor(11146.1992, grad_fn=<NegBackward0>) tensor(11146.2002, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11146.2001953125
tensor(11146.1992, grad_fn=<NegBackward0>) tensor(11146.2002, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11146.205078125
tensor(11146.1992, grad_fn=<NegBackward0>) tensor(11146.2051, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -11146.19921875
tensor(11146.1992, grad_fn=<NegBackward0>) tensor(11146.1992, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11146.2177734375
tensor(11146.1992, grad_fn=<NegBackward0>) tensor(11146.2178, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11146.19921875
tensor(11146.1992, grad_fn=<NegBackward0>) tensor(11146.1992, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11146.2001953125
tensor(11146.1992, grad_fn=<NegBackward0>) tensor(11146.2002, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11146.2041015625
tensor(11146.1992, grad_fn=<NegBackward0>) tensor(11146.2041, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11146.2001953125
tensor(11146.1992, grad_fn=<NegBackward0>) tensor(11146.2002, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11146.1591796875
tensor(11146.1992, grad_fn=<NegBackward0>) tensor(11146.1592, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11146.1572265625
tensor(11146.1592, grad_fn=<NegBackward0>) tensor(11146.1572, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11146.150390625
tensor(11146.1572, grad_fn=<NegBackward0>) tensor(11146.1504, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11146.1513671875
tensor(11146.1504, grad_fn=<NegBackward0>) tensor(11146.1514, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11146.1533203125
tensor(11146.1504, grad_fn=<NegBackward0>) tensor(11146.1533, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11146.1484375
tensor(11146.1504, grad_fn=<NegBackward0>) tensor(11146.1484, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11146.1494140625
tensor(11146.1484, grad_fn=<NegBackward0>) tensor(11146.1494, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11146.1494140625
tensor(11146.1484, grad_fn=<NegBackward0>) tensor(11146.1494, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11146.1494140625
tensor(11146.1484, grad_fn=<NegBackward0>) tensor(11146.1494, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -11146.16796875
tensor(11146.1484, grad_fn=<NegBackward0>) tensor(11146.1680, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -11146.185546875
tensor(11146.1484, grad_fn=<NegBackward0>) tensor(11146.1855, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8800 due to no improvement.
pi: tensor([[0.7577, 0.2423],
        [0.3329, 0.6671]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4931, 0.5069], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2499, 0.1167],
         [0.6759, 0.2214]],

        [[0.6745, 0.0940],
         [0.5616, 0.7283]],

        [[0.5423, 0.0980],
         [0.6350, 0.6915]],

        [[0.6844, 0.1001],
         [0.6625, 0.5558]],

        [[0.5620, 0.1021],
         [0.6777, 0.6661]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080620079101718
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 10
Adjusted Rand Index: 0.6363636363636364
time is 4
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824124176797128
Global Adjusted Rand Index: 0.838732277977142
Average Adjusted Rand Index: 0.8418505259527382
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21307.19140625
inf tensor(21307.1914, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11297.177734375
tensor(21307.1914, grad_fn=<NegBackward0>) tensor(11297.1777, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11296.5634765625
tensor(11297.1777, grad_fn=<NegBackward0>) tensor(11296.5635, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11296.17578125
tensor(11296.5635, grad_fn=<NegBackward0>) tensor(11296.1758, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11295.765625
tensor(11296.1758, grad_fn=<NegBackward0>) tensor(11295.7656, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11295.474609375
tensor(11295.7656, grad_fn=<NegBackward0>) tensor(11295.4746, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11295.123046875
tensor(11295.4746, grad_fn=<NegBackward0>) tensor(11295.1230, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11294.6484375
tensor(11295.1230, grad_fn=<NegBackward0>) tensor(11294.6484, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11294.0703125
tensor(11294.6484, grad_fn=<NegBackward0>) tensor(11294.0703, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11293.529296875
tensor(11294.0703, grad_fn=<NegBackward0>) tensor(11293.5293, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11293.0400390625
tensor(11293.5293, grad_fn=<NegBackward0>) tensor(11293.0400, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11292.5029296875
tensor(11293.0400, grad_fn=<NegBackward0>) tensor(11292.5029, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11291.2353515625
tensor(11292.5029, grad_fn=<NegBackward0>) tensor(11291.2354, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11289.4248046875
tensor(11291.2354, grad_fn=<NegBackward0>) tensor(11289.4248, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11288.8369140625
tensor(11289.4248, grad_fn=<NegBackward0>) tensor(11288.8369, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11280.0087890625
tensor(11288.8369, grad_fn=<NegBackward0>) tensor(11280.0088, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11193.201171875
tensor(11280.0088, grad_fn=<NegBackward0>) tensor(11193.2012, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11163.3037109375
tensor(11193.2012, grad_fn=<NegBackward0>) tensor(11163.3037, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11162.9345703125
tensor(11163.3037, grad_fn=<NegBackward0>) tensor(11162.9346, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11153.208984375
tensor(11162.9346, grad_fn=<NegBackward0>) tensor(11153.2090, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11150.525390625
tensor(11153.2090, grad_fn=<NegBackward0>) tensor(11150.5254, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11150.0654296875
tensor(11150.5254, grad_fn=<NegBackward0>) tensor(11150.0654, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11149.66796875
tensor(11150.0654, grad_fn=<NegBackward0>) tensor(11149.6680, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11149.6337890625
tensor(11149.6680, grad_fn=<NegBackward0>) tensor(11149.6338, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11149.609375
tensor(11149.6338, grad_fn=<NegBackward0>) tensor(11149.6094, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11149.5810546875
tensor(11149.6094, grad_fn=<NegBackward0>) tensor(11149.5811, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11145.3759765625
tensor(11149.5811, grad_fn=<NegBackward0>) tensor(11145.3760, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11145.349609375
tensor(11145.3760, grad_fn=<NegBackward0>) tensor(11145.3496, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11145.3349609375
tensor(11145.3496, grad_fn=<NegBackward0>) tensor(11145.3350, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11145.322265625
tensor(11145.3350, grad_fn=<NegBackward0>) tensor(11145.3223, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11145.3056640625
tensor(11145.3223, grad_fn=<NegBackward0>) tensor(11145.3057, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11145.2978515625
tensor(11145.3057, grad_fn=<NegBackward0>) tensor(11145.2979, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11145.2939453125
tensor(11145.2979, grad_fn=<NegBackward0>) tensor(11145.2939, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11145.2900390625
tensor(11145.2939, grad_fn=<NegBackward0>) tensor(11145.2900, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11145.287109375
tensor(11145.2900, grad_fn=<NegBackward0>) tensor(11145.2871, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11145.2841796875
tensor(11145.2871, grad_fn=<NegBackward0>) tensor(11145.2842, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11145.2841796875
tensor(11145.2842, grad_fn=<NegBackward0>) tensor(11145.2842, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11145.283203125
tensor(11145.2842, grad_fn=<NegBackward0>) tensor(11145.2832, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11145.279296875
tensor(11145.2832, grad_fn=<NegBackward0>) tensor(11145.2793, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11145.27734375
tensor(11145.2793, grad_fn=<NegBackward0>) tensor(11145.2773, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11145.2763671875
tensor(11145.2773, grad_fn=<NegBackward0>) tensor(11145.2764, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11145.2734375
tensor(11145.2764, grad_fn=<NegBackward0>) tensor(11145.2734, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11145.2705078125
tensor(11145.2734, grad_fn=<NegBackward0>) tensor(11145.2705, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11145.2685546875
tensor(11145.2705, grad_fn=<NegBackward0>) tensor(11145.2686, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11145.2587890625
tensor(11145.2686, grad_fn=<NegBackward0>) tensor(11145.2588, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11145.25390625
tensor(11145.2588, grad_fn=<NegBackward0>) tensor(11145.2539, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11145.25
tensor(11145.2539, grad_fn=<NegBackward0>) tensor(11145.2500, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11145.2197265625
tensor(11145.2500, grad_fn=<NegBackward0>) tensor(11145.2197, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11145.2060546875
tensor(11145.2197, grad_fn=<NegBackward0>) tensor(11145.2061, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11145.2119140625
tensor(11145.2061, grad_fn=<NegBackward0>) tensor(11145.2119, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11145.2021484375
tensor(11145.2061, grad_fn=<NegBackward0>) tensor(11145.2021, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11145.201171875
tensor(11145.2021, grad_fn=<NegBackward0>) tensor(11145.2012, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11145.2001953125
tensor(11145.2012, grad_fn=<NegBackward0>) tensor(11145.2002, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11145.1982421875
tensor(11145.2002, grad_fn=<NegBackward0>) tensor(11145.1982, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11145.1943359375
tensor(11145.1982, grad_fn=<NegBackward0>) tensor(11145.1943, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11145.19140625
tensor(11145.1943, grad_fn=<NegBackward0>) tensor(11145.1914, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11145.1865234375
tensor(11145.1914, grad_fn=<NegBackward0>) tensor(11145.1865, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11145.1865234375
tensor(11145.1865, grad_fn=<NegBackward0>) tensor(11145.1865, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11145.1982421875
tensor(11145.1865, grad_fn=<NegBackward0>) tensor(11145.1982, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11145.1826171875
tensor(11145.1865, grad_fn=<NegBackward0>) tensor(11145.1826, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11145.1845703125
tensor(11145.1826, grad_fn=<NegBackward0>) tensor(11145.1846, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11145.17578125
tensor(11145.1826, grad_fn=<NegBackward0>) tensor(11145.1758, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11145.15625
tensor(11145.1758, grad_fn=<NegBackward0>) tensor(11145.1562, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11145.15625
tensor(11145.1562, grad_fn=<NegBackward0>) tensor(11145.1562, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11145.166015625
tensor(11145.1562, grad_fn=<NegBackward0>) tensor(11145.1660, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11145.1552734375
tensor(11145.1562, grad_fn=<NegBackward0>) tensor(11145.1553, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11145.154296875
tensor(11145.1553, grad_fn=<NegBackward0>) tensor(11145.1543, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11145.154296875
tensor(11145.1543, grad_fn=<NegBackward0>) tensor(11145.1543, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11145.1455078125
tensor(11145.1543, grad_fn=<NegBackward0>) tensor(11145.1455, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11145.1455078125
tensor(11145.1455, grad_fn=<NegBackward0>) tensor(11145.1455, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11145.13671875
tensor(11145.1455, grad_fn=<NegBackward0>) tensor(11145.1367, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11145.138671875
tensor(11145.1367, grad_fn=<NegBackward0>) tensor(11145.1387, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11145.134765625
tensor(11145.1367, grad_fn=<NegBackward0>) tensor(11145.1348, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11145.1337890625
tensor(11145.1348, grad_fn=<NegBackward0>) tensor(11145.1338, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11145.1337890625
tensor(11145.1338, grad_fn=<NegBackward0>) tensor(11145.1338, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11145.1259765625
tensor(11145.1338, grad_fn=<NegBackward0>) tensor(11145.1260, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11145.12109375
tensor(11145.1260, grad_fn=<NegBackward0>) tensor(11145.1211, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11145.1201171875
tensor(11145.1211, grad_fn=<NegBackward0>) tensor(11145.1201, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11145.119140625
tensor(11145.1201, grad_fn=<NegBackward0>) tensor(11145.1191, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11145.119140625
tensor(11145.1191, grad_fn=<NegBackward0>) tensor(11145.1191, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11145.119140625
tensor(11145.1191, grad_fn=<NegBackward0>) tensor(11145.1191, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11145.1171875
tensor(11145.1191, grad_fn=<NegBackward0>) tensor(11145.1172, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11145.1435546875
tensor(11145.1172, grad_fn=<NegBackward0>) tensor(11145.1436, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11145.1171875
tensor(11145.1172, grad_fn=<NegBackward0>) tensor(11145.1172, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11145.119140625
tensor(11145.1172, grad_fn=<NegBackward0>) tensor(11145.1191, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11145.1171875
tensor(11145.1172, grad_fn=<NegBackward0>) tensor(11145.1172, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11145.1181640625
tensor(11145.1172, grad_fn=<NegBackward0>) tensor(11145.1182, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11145.1181640625
tensor(11145.1172, grad_fn=<NegBackward0>) tensor(11145.1182, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11145.1279296875
tensor(11145.1172, grad_fn=<NegBackward0>) tensor(11145.1279, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -11145.1181640625
tensor(11145.1172, grad_fn=<NegBackward0>) tensor(11145.1182, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -11145.1171875
tensor(11145.1172, grad_fn=<NegBackward0>) tensor(11145.1172, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11145.142578125
tensor(11145.1172, grad_fn=<NegBackward0>) tensor(11145.1426, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11145.1240234375
tensor(11145.1172, grad_fn=<NegBackward0>) tensor(11145.1240, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11145.12109375
tensor(11145.1172, grad_fn=<NegBackward0>) tensor(11145.1211, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -11145.1162109375
tensor(11145.1172, grad_fn=<NegBackward0>) tensor(11145.1162, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11145.1162109375
tensor(11145.1162, grad_fn=<NegBackward0>) tensor(11145.1162, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11145.1171875
tensor(11145.1162, grad_fn=<NegBackward0>) tensor(11145.1172, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11145.1142578125
tensor(11145.1162, grad_fn=<NegBackward0>) tensor(11145.1143, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11145.1171875
tensor(11145.1143, grad_fn=<NegBackward0>) tensor(11145.1172, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11145.1142578125
tensor(11145.1143, grad_fn=<NegBackward0>) tensor(11145.1143, grad_fn=<NegBackward0>)
pi: tensor([[0.3485, 0.6515],
        [0.7660, 0.2340]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4969, 0.5031], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2255, 0.1126],
         [0.5297, 0.2530]],

        [[0.5830, 0.0918],
         [0.6340, 0.5454]],

        [[0.5015, 0.0988],
         [0.6425, 0.6691]],

        [[0.7085, 0.0974],
         [0.6701, 0.6952]],

        [[0.6828, 0.1031],
         [0.5971, 0.6501]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721212121212121
time is 2
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721016799725718
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 8
Adjusted Rand Index: 0.7026262626262626
time is 4
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208065164923572
Global Adjusted Rand Index: 0.043029291718123495
Average Adjusted Rand Index: 0.8100140478045148
[0.838732277977142, 0.043029291718123495] [0.8418505259527382, 0.8100140478045148] [11146.185546875, 11145.2158203125]
-------------------------------------
This iteration is 90
True Objective function: Loss = -10756.466693007842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21281.4765625
inf tensor(21281.4766, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10826.5947265625
tensor(21281.4766, grad_fn=<NegBackward0>) tensor(10826.5947, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10825.451171875
tensor(10826.5947, grad_fn=<NegBackward0>) tensor(10825.4512, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10825.1982421875
tensor(10825.4512, grad_fn=<NegBackward0>) tensor(10825.1982, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10825.00390625
tensor(10825.1982, grad_fn=<NegBackward0>) tensor(10825.0039, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10824.8603515625
tensor(10825.0039, grad_fn=<NegBackward0>) tensor(10824.8604, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10824.7646484375
tensor(10824.8604, grad_fn=<NegBackward0>) tensor(10824.7646, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10824.7080078125
tensor(10824.7646, grad_fn=<NegBackward0>) tensor(10824.7080, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10824.6748046875
tensor(10824.7080, grad_fn=<NegBackward0>) tensor(10824.6748, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10824.654296875
tensor(10824.6748, grad_fn=<NegBackward0>) tensor(10824.6543, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10824.6416015625
tensor(10824.6543, grad_fn=<NegBackward0>) tensor(10824.6416, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10824.6298828125
tensor(10824.6416, grad_fn=<NegBackward0>) tensor(10824.6299, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10824.6201171875
tensor(10824.6299, grad_fn=<NegBackward0>) tensor(10824.6201, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10824.611328125
tensor(10824.6201, grad_fn=<NegBackward0>) tensor(10824.6113, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10824.6015625
tensor(10824.6113, grad_fn=<NegBackward0>) tensor(10824.6016, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10824.5888671875
tensor(10824.6016, grad_fn=<NegBackward0>) tensor(10824.5889, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10824.5693359375
tensor(10824.5889, grad_fn=<NegBackward0>) tensor(10824.5693, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10824.54296875
tensor(10824.5693, grad_fn=<NegBackward0>) tensor(10824.5430, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10824.486328125
tensor(10824.5430, grad_fn=<NegBackward0>) tensor(10824.4863, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10824.38671875
tensor(10824.4863, grad_fn=<NegBackward0>) tensor(10824.3867, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10824.2587890625
tensor(10824.3867, grad_fn=<NegBackward0>) tensor(10824.2588, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10824.0751953125
tensor(10824.2588, grad_fn=<NegBackward0>) tensor(10824.0752, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10823.6640625
tensor(10824.0752, grad_fn=<NegBackward0>) tensor(10823.6641, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10810.8857421875
tensor(10823.6641, grad_fn=<NegBackward0>) tensor(10810.8857, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10808.5498046875
tensor(10810.8857, grad_fn=<NegBackward0>) tensor(10808.5498, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10808.419921875
tensor(10808.5498, grad_fn=<NegBackward0>) tensor(10808.4199, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10808.3701171875
tensor(10808.4199, grad_fn=<NegBackward0>) tensor(10808.3701, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10808.3583984375
tensor(10808.3701, grad_fn=<NegBackward0>) tensor(10808.3584, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10808.3486328125
tensor(10808.3584, grad_fn=<NegBackward0>) tensor(10808.3486, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10808.34765625
tensor(10808.3486, grad_fn=<NegBackward0>) tensor(10808.3477, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10808.3466796875
tensor(10808.3477, grad_fn=<NegBackward0>) tensor(10808.3467, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10808.345703125
tensor(10808.3467, grad_fn=<NegBackward0>) tensor(10808.3457, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10808.3466796875
tensor(10808.3457, grad_fn=<NegBackward0>) tensor(10808.3467, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -10808.3466796875
tensor(10808.3457, grad_fn=<NegBackward0>) tensor(10808.3467, grad_fn=<NegBackward0>)
2
Iteration 3400: Loss = -10808.345703125
tensor(10808.3457, grad_fn=<NegBackward0>) tensor(10808.3457, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10808.3515625
tensor(10808.3457, grad_fn=<NegBackward0>) tensor(10808.3516, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -10808.345703125
tensor(10808.3457, grad_fn=<NegBackward0>) tensor(10808.3457, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10808.345703125
tensor(10808.3457, grad_fn=<NegBackward0>) tensor(10808.3457, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10808.345703125
tensor(10808.3457, grad_fn=<NegBackward0>) tensor(10808.3457, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10808.3447265625
tensor(10808.3457, grad_fn=<NegBackward0>) tensor(10808.3447, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10808.34375
tensor(10808.3447, grad_fn=<NegBackward0>) tensor(10808.3438, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10808.3427734375
tensor(10808.3438, grad_fn=<NegBackward0>) tensor(10808.3428, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10808.34375
tensor(10808.3428, grad_fn=<NegBackward0>) tensor(10808.3438, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10808.3427734375
tensor(10808.3428, grad_fn=<NegBackward0>) tensor(10808.3428, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10808.3427734375
tensor(10808.3428, grad_fn=<NegBackward0>) tensor(10808.3428, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10808.3408203125
tensor(10808.3428, grad_fn=<NegBackward0>) tensor(10808.3408, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10808.3427734375
tensor(10808.3408, grad_fn=<NegBackward0>) tensor(10808.3428, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -10808.33984375
tensor(10808.3408, grad_fn=<NegBackward0>) tensor(10808.3398, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10808.341796875
tensor(10808.3398, grad_fn=<NegBackward0>) tensor(10808.3418, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10808.34375
tensor(10808.3398, grad_fn=<NegBackward0>) tensor(10808.3438, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -10808.33984375
tensor(10808.3398, grad_fn=<NegBackward0>) tensor(10808.3398, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10808.33984375
tensor(10808.3398, grad_fn=<NegBackward0>) tensor(10808.3398, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10808.3388671875
tensor(10808.3398, grad_fn=<NegBackward0>) tensor(10808.3389, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10808.33984375
tensor(10808.3389, grad_fn=<NegBackward0>) tensor(10808.3398, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10808.3369140625
tensor(10808.3389, grad_fn=<NegBackward0>) tensor(10808.3369, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10808.3359375
tensor(10808.3369, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10808.3369140625
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3369, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10808.3369140625
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3369, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -10808.337890625
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3379, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -10808.337890625
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3379, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -10808.337890625
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3379, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6000 due to no improvement.
pi: tensor([[0.9682, 0.0318],
        [0.2843, 0.7157]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3923, 0.6077], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1546, 0.1204],
         [0.5121, 0.2241]],

        [[0.6067, 0.1021],
         [0.7128, 0.7301]],

        [[0.5742, 0.1510],
         [0.6422, 0.6525]],

        [[0.5611, 0.1535],
         [0.5368, 0.6938]],

        [[0.5023, 0.1807],
         [0.5752, 0.6795]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 82
Adjusted Rand Index: 0.4038795559619877
time is 1
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 90
Adjusted Rand Index: 0.6363408394869687
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 62
Adjusted Rand Index: 0.048319907872549195
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.007421875
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.005968450940084696
Global Adjusted Rand Index: 0.12505340954839753
Average Adjusted Rand Index: 0.21799874547628412
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23215.318359375
inf tensor(23215.3184, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10827.9287109375
tensor(23215.3184, grad_fn=<NegBackward0>) tensor(10827.9287, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10826.783203125
tensor(10827.9287, grad_fn=<NegBackward0>) tensor(10826.7832, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10826.1728515625
tensor(10826.7832, grad_fn=<NegBackward0>) tensor(10826.1729, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10825.7841796875
tensor(10826.1729, grad_fn=<NegBackward0>) tensor(10825.7842, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10825.5751953125
tensor(10825.7842, grad_fn=<NegBackward0>) tensor(10825.5752, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10825.373046875
tensor(10825.5752, grad_fn=<NegBackward0>) tensor(10825.3730, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10825.15625
tensor(10825.3730, grad_fn=<NegBackward0>) tensor(10825.1562, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10824.931640625
tensor(10825.1562, grad_fn=<NegBackward0>) tensor(10824.9316, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10824.72265625
tensor(10824.9316, grad_fn=<NegBackward0>) tensor(10824.7227, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10824.548828125
tensor(10824.7227, grad_fn=<NegBackward0>) tensor(10824.5488, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10824.4189453125
tensor(10824.5488, grad_fn=<NegBackward0>) tensor(10824.4189, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10824.3251953125
tensor(10824.4189, grad_fn=<NegBackward0>) tensor(10824.3252, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10824.2529296875
tensor(10824.3252, grad_fn=<NegBackward0>) tensor(10824.2529, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10824.189453125
tensor(10824.2529, grad_fn=<NegBackward0>) tensor(10824.1895, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10824.1328125
tensor(10824.1895, grad_fn=<NegBackward0>) tensor(10824.1328, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10824.0791015625
tensor(10824.1328, grad_fn=<NegBackward0>) tensor(10824.0791, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10824.0322265625
tensor(10824.0791, grad_fn=<NegBackward0>) tensor(10824.0322, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10823.994140625
tensor(10824.0322, grad_fn=<NegBackward0>) tensor(10823.9941, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10823.96484375
tensor(10823.9941, grad_fn=<NegBackward0>) tensor(10823.9648, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10823.9365234375
tensor(10823.9648, grad_fn=<NegBackward0>) tensor(10823.9365, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10823.91015625
tensor(10823.9365, grad_fn=<NegBackward0>) tensor(10823.9102, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10823.884765625
tensor(10823.9102, grad_fn=<NegBackward0>) tensor(10823.8848, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10823.8623046875
tensor(10823.8848, grad_fn=<NegBackward0>) tensor(10823.8623, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10823.83984375
tensor(10823.8623, grad_fn=<NegBackward0>) tensor(10823.8398, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10823.818359375
tensor(10823.8398, grad_fn=<NegBackward0>) tensor(10823.8184, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10823.7978515625
tensor(10823.8184, grad_fn=<NegBackward0>) tensor(10823.7979, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10823.775390625
tensor(10823.7979, grad_fn=<NegBackward0>) tensor(10823.7754, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10823.7490234375
tensor(10823.7754, grad_fn=<NegBackward0>) tensor(10823.7490, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10823.712890625
tensor(10823.7490, grad_fn=<NegBackward0>) tensor(10823.7129, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10823.6455078125
tensor(10823.7129, grad_fn=<NegBackward0>) tensor(10823.6455, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10823.0224609375
tensor(10823.6455, grad_fn=<NegBackward0>) tensor(10823.0225, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10813.025390625
tensor(10823.0225, grad_fn=<NegBackward0>) tensor(10813.0254, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10809.3525390625
tensor(10813.0254, grad_fn=<NegBackward0>) tensor(10809.3525, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10808.408203125
tensor(10809.3525, grad_fn=<NegBackward0>) tensor(10808.4082, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10808.3564453125
tensor(10808.4082, grad_fn=<NegBackward0>) tensor(10808.3564, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10808.345703125
tensor(10808.3564, grad_fn=<NegBackward0>) tensor(10808.3457, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10808.3427734375
tensor(10808.3457, grad_fn=<NegBackward0>) tensor(10808.3428, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10808.33984375
tensor(10808.3428, grad_fn=<NegBackward0>) tensor(10808.3398, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10808.33984375
tensor(10808.3398, grad_fn=<NegBackward0>) tensor(10808.3398, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10808.3388671875
tensor(10808.3398, grad_fn=<NegBackward0>) tensor(10808.3389, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10808.3642578125
tensor(10808.3389, grad_fn=<NegBackward0>) tensor(10808.3643, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -10808.3388671875
tensor(10808.3389, grad_fn=<NegBackward0>) tensor(10808.3389, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10808.337890625
tensor(10808.3389, grad_fn=<NegBackward0>) tensor(10808.3379, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10808.337890625
tensor(10808.3379, grad_fn=<NegBackward0>) tensor(10808.3379, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10808.337890625
tensor(10808.3379, grad_fn=<NegBackward0>) tensor(10808.3379, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10808.337890625
tensor(10808.3379, grad_fn=<NegBackward0>) tensor(10808.3379, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10808.3388671875
tensor(10808.3379, grad_fn=<NegBackward0>) tensor(10808.3389, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10808.3369140625
tensor(10808.3379, grad_fn=<NegBackward0>) tensor(10808.3369, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10808.337890625
tensor(10808.3369, grad_fn=<NegBackward0>) tensor(10808.3379, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10808.337890625
tensor(10808.3369, grad_fn=<NegBackward0>) tensor(10808.3379, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -10808.3369140625
tensor(10808.3369, grad_fn=<NegBackward0>) tensor(10808.3369, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10808.341796875
tensor(10808.3369, grad_fn=<NegBackward0>) tensor(10808.3418, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10808.3359375
tensor(10808.3369, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10808.341796875
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3418, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10808.3369140625
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3369, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -10808.3369140625
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3369, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -10808.3359375
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10808.3359375
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10808.3369140625
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3369, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10808.337890625
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3379, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -10808.3359375
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10808.3369140625
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3369, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10808.3369140625
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3369, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -10808.3359375
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10808.3359375
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10808.3359375
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10808.3369140625
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3369, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10808.3369140625
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3369, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -10808.3369140625
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3369, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -10808.3359375
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10808.3388671875
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3389, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10808.3359375
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10808.337890625
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3379, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -10808.3359375
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10808.3369140625
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3369, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10808.3359375
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10808.3388671875
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3389, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10808.3359375
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10808.3779296875
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3779, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10808.3359375
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10808.341796875
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3418, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10808.3369140625
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3369, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -10808.3359375
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10808.3359375
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10808.3369140625
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3369, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -10808.3369140625
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3369, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -10808.337890625
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3379, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -10808.3359375
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10808.3349609375
tensor(10808.3359, grad_fn=<NegBackward0>) tensor(10808.3350, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10808.3359375
tensor(10808.3350, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10808.3349609375
tensor(10808.3350, grad_fn=<NegBackward0>) tensor(10808.3350, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10808.3349609375
tensor(10808.3350, grad_fn=<NegBackward0>) tensor(10808.3350, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10808.3359375
tensor(10808.3350, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -10808.3359375
tensor(10808.3350, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -10808.3359375
tensor(10808.3350, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -10808.3359375
tensor(10808.3350, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
4
Iteration 9700: Loss = -10808.3359375
tensor(10808.3350, grad_fn=<NegBackward0>) tensor(10808.3359, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9700 due to no improvement.
pi: tensor([[0.9681, 0.0319],
        [0.2843, 0.7157]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3928, 0.6072], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1546, 0.1204],
         [0.7298, 0.2241]],

        [[0.6577, 0.1021],
         [0.5358, 0.5426]],

        [[0.6217, 0.1510],
         [0.5194, 0.6354]],

        [[0.6657, 0.1535],
         [0.6866, 0.7191]],

        [[0.5849, 0.1807],
         [0.6656, 0.7236]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 82
Adjusted Rand Index: 0.4038795559619877
time is 1
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 90
Adjusted Rand Index: 0.6363408394869687
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 62
Adjusted Rand Index: 0.048319907872549195
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.007421875
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.005968450940084696
Global Adjusted Rand Index: 0.12505340954839753
Average Adjusted Rand Index: 0.21799874547628412
[0.12505340954839753, 0.12505340954839753] [0.21799874547628412, 0.21799874547628412] [10808.337890625, 10808.3359375]
-------------------------------------
This iteration is 91
True Objective function: Loss = -11050.589575692185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23724.455078125
inf tensor(23724.4551, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11166.669921875
tensor(23724.4551, grad_fn=<NegBackward0>) tensor(11166.6699, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11165.0654296875
tensor(11166.6699, grad_fn=<NegBackward0>) tensor(11165.0654, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11161.7109375
tensor(11165.0654, grad_fn=<NegBackward0>) tensor(11161.7109, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11157.6611328125
tensor(11161.7109, grad_fn=<NegBackward0>) tensor(11157.6611, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11152.1767578125
tensor(11157.6611, grad_fn=<NegBackward0>) tensor(11152.1768, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11068.3515625
tensor(11152.1768, grad_fn=<NegBackward0>) tensor(11068.3516, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11053.080078125
tensor(11068.3516, grad_fn=<NegBackward0>) tensor(11053.0801, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11047.849609375
tensor(11053.0801, grad_fn=<NegBackward0>) tensor(11047.8496, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11047.6533203125
tensor(11047.8496, grad_fn=<NegBackward0>) tensor(11047.6533, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11047.55859375
tensor(11047.6533, grad_fn=<NegBackward0>) tensor(11047.5586, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11047.4814453125
tensor(11047.5586, grad_fn=<NegBackward0>) tensor(11047.4814, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11047.4443359375
tensor(11047.4814, grad_fn=<NegBackward0>) tensor(11047.4443, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11047.4208984375
tensor(11047.4443, grad_fn=<NegBackward0>) tensor(11047.4209, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11047.392578125
tensor(11047.4209, grad_fn=<NegBackward0>) tensor(11047.3926, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11047.3681640625
tensor(11047.3926, grad_fn=<NegBackward0>) tensor(11047.3682, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11047.3583984375
tensor(11047.3682, grad_fn=<NegBackward0>) tensor(11047.3584, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11047.345703125
tensor(11047.3584, grad_fn=<NegBackward0>) tensor(11047.3457, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11047.330078125
tensor(11047.3457, grad_fn=<NegBackward0>) tensor(11047.3301, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11047.3359375
tensor(11047.3301, grad_fn=<NegBackward0>) tensor(11047.3359, grad_fn=<NegBackward0>)
1
Iteration 2000: Loss = -11047.2509765625
tensor(11047.3301, grad_fn=<NegBackward0>) tensor(11047.2510, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11047.0234375
tensor(11047.2510, grad_fn=<NegBackward0>) tensor(11047.0234, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11011.3232421875
tensor(11047.0234, grad_fn=<NegBackward0>) tensor(11011.3232, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11009.3505859375
tensor(11011.3232, grad_fn=<NegBackward0>) tensor(11009.3506, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11009.2919921875
tensor(11009.3506, grad_fn=<NegBackward0>) tensor(11009.2920, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11009.1650390625
tensor(11009.2920, grad_fn=<NegBackward0>) tensor(11009.1650, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11009.1611328125
tensor(11009.1650, grad_fn=<NegBackward0>) tensor(11009.1611, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11009.154296875
tensor(11009.1611, grad_fn=<NegBackward0>) tensor(11009.1543, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11009.14453125
tensor(11009.1543, grad_fn=<NegBackward0>) tensor(11009.1445, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11009.142578125
tensor(11009.1445, grad_fn=<NegBackward0>) tensor(11009.1426, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11009.1396484375
tensor(11009.1426, grad_fn=<NegBackward0>) tensor(11009.1396, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11009.138671875
tensor(11009.1396, grad_fn=<NegBackward0>) tensor(11009.1387, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11009.150390625
tensor(11009.1387, grad_fn=<NegBackward0>) tensor(11009.1504, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11009.1357421875
tensor(11009.1387, grad_fn=<NegBackward0>) tensor(11009.1357, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11009.1318359375
tensor(11009.1357, grad_fn=<NegBackward0>) tensor(11009.1318, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11009.1318359375
tensor(11009.1318, grad_fn=<NegBackward0>) tensor(11009.1318, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11009.1328125
tensor(11009.1318, grad_fn=<NegBackward0>) tensor(11009.1328, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11009.1328125
tensor(11009.1318, grad_fn=<NegBackward0>) tensor(11009.1328, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -11009.130859375
tensor(11009.1318, grad_fn=<NegBackward0>) tensor(11009.1309, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11009.130859375
tensor(11009.1309, grad_fn=<NegBackward0>) tensor(11009.1309, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11009.1298828125
tensor(11009.1309, grad_fn=<NegBackward0>) tensor(11009.1299, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11009.12890625
tensor(11009.1299, grad_fn=<NegBackward0>) tensor(11009.1289, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11009.12890625
tensor(11009.1289, grad_fn=<NegBackward0>) tensor(11009.1289, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11009.126953125
tensor(11009.1289, grad_fn=<NegBackward0>) tensor(11009.1270, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11009.125
tensor(11009.1270, grad_fn=<NegBackward0>) tensor(11009.1250, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11009.1162109375
tensor(11009.1250, grad_fn=<NegBackward0>) tensor(11009.1162, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11009.1142578125
tensor(11009.1162, grad_fn=<NegBackward0>) tensor(11009.1143, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11009.11328125
tensor(11009.1143, grad_fn=<NegBackward0>) tensor(11009.1133, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11009.111328125
tensor(11009.1133, grad_fn=<NegBackward0>) tensor(11009.1113, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11009.0419921875
tensor(11009.1113, grad_fn=<NegBackward0>) tensor(11009.0420, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11009.044921875
tensor(11009.0420, grad_fn=<NegBackward0>) tensor(11009.0449, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11009.046875
tensor(11009.0420, grad_fn=<NegBackward0>) tensor(11009.0469, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11009.04296875
tensor(11009.0420, grad_fn=<NegBackward0>) tensor(11009.0430, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -11009.033203125
tensor(11009.0420, grad_fn=<NegBackward0>) tensor(11009.0332, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11009.0322265625
tensor(11009.0332, grad_fn=<NegBackward0>) tensor(11009.0322, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11009.03125
tensor(11009.0322, grad_fn=<NegBackward0>) tensor(11009.0312, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11009.03125
tensor(11009.0312, grad_fn=<NegBackward0>) tensor(11009.0312, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11009.03125
tensor(11009.0312, grad_fn=<NegBackward0>) tensor(11009.0312, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11009.03125
tensor(11009.0312, grad_fn=<NegBackward0>) tensor(11009.0312, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11009.0341796875
tensor(11009.0312, grad_fn=<NegBackward0>) tensor(11009.0342, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11009.03125
tensor(11009.0312, grad_fn=<NegBackward0>) tensor(11009.0312, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11009.0390625
tensor(11009.0312, grad_fn=<NegBackward0>) tensor(11009.0391, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11009.0341796875
tensor(11009.0312, grad_fn=<NegBackward0>) tensor(11009.0342, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11009.0302734375
tensor(11009.0312, grad_fn=<NegBackward0>) tensor(11009.0303, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11009.03125
tensor(11009.0303, grad_fn=<NegBackward0>) tensor(11009.0312, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11009.0302734375
tensor(11009.0303, grad_fn=<NegBackward0>) tensor(11009.0303, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11009.02734375
tensor(11009.0303, grad_fn=<NegBackward0>) tensor(11009.0273, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11009.0263671875
tensor(11009.0273, grad_fn=<NegBackward0>) tensor(11009.0264, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11009.0263671875
tensor(11009.0264, grad_fn=<NegBackward0>) tensor(11009.0264, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11009.025390625
tensor(11009.0264, grad_fn=<NegBackward0>) tensor(11009.0254, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11009.02734375
tensor(11009.0254, grad_fn=<NegBackward0>) tensor(11009.0273, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11009.0263671875
tensor(11009.0254, grad_fn=<NegBackward0>) tensor(11009.0264, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11009.025390625
tensor(11009.0254, grad_fn=<NegBackward0>) tensor(11009.0254, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11009.0185546875
tensor(11009.0254, grad_fn=<NegBackward0>) tensor(11009.0186, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11009.0166015625
tensor(11009.0186, grad_fn=<NegBackward0>) tensor(11009.0166, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11009.015625
tensor(11009.0166, grad_fn=<NegBackward0>) tensor(11009.0156, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11009.0341796875
tensor(11009.0156, grad_fn=<NegBackward0>) tensor(11009.0342, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11009.015625
tensor(11009.0156, grad_fn=<NegBackward0>) tensor(11009.0156, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11009.015625
tensor(11009.0156, grad_fn=<NegBackward0>) tensor(11009.0156, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11009.015625
tensor(11009.0156, grad_fn=<NegBackward0>) tensor(11009.0156, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11009.0146484375
tensor(11009.0156, grad_fn=<NegBackward0>) tensor(11009.0146, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11009.015625
tensor(11009.0146, grad_fn=<NegBackward0>) tensor(11009.0156, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11009.015625
tensor(11009.0146, grad_fn=<NegBackward0>) tensor(11009.0156, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11009.0146484375
tensor(11009.0146, grad_fn=<NegBackward0>) tensor(11009.0146, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11009.015625
tensor(11009.0146, grad_fn=<NegBackward0>) tensor(11009.0156, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11009.0146484375
tensor(11009.0146, grad_fn=<NegBackward0>) tensor(11009.0146, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11009.0166015625
tensor(11009.0146, grad_fn=<NegBackward0>) tensor(11009.0166, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11009.015625
tensor(11009.0146, grad_fn=<NegBackward0>) tensor(11009.0156, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11009.015625
tensor(11009.0146, grad_fn=<NegBackward0>) tensor(11009.0156, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -11009.01953125
tensor(11009.0146, grad_fn=<NegBackward0>) tensor(11009.0195, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -11009.015625
tensor(11009.0146, grad_fn=<NegBackward0>) tensor(11009.0156, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[0.7992, 0.2008],
        [0.2914, 0.7086]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5060, 0.4940], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2535, 0.1018],
         [0.6461, 0.1922]],

        [[0.6801, 0.0942],
         [0.6824, 0.5195]],

        [[0.7052, 0.1030],
         [0.7177, 0.5113]],

        [[0.5724, 0.1156],
         [0.5756, 0.6082]],

        [[0.6687, 0.0975],
         [0.5590, 0.6541]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 8
Adjusted Rand Index: 0.7025959183673469
time is 1
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9206289602688308
time is 3
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 7
Adjusted Rand Index: 0.7369432436752338
time is 4
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.8534546574309102
Average Adjusted Rand Index: 0.8561952406238985
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23322.611328125
inf tensor(23322.6113, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11167.861328125
tensor(23322.6113, grad_fn=<NegBackward0>) tensor(11167.8613, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11166.798828125
tensor(11167.8613, grad_fn=<NegBackward0>) tensor(11166.7988, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11165.0625
tensor(11166.7988, grad_fn=<NegBackward0>) tensor(11165.0625, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11160.388671875
tensor(11165.0625, grad_fn=<NegBackward0>) tensor(11160.3887, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11158.3720703125
tensor(11160.3887, grad_fn=<NegBackward0>) tensor(11158.3721, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11156.662109375
tensor(11158.3721, grad_fn=<NegBackward0>) tensor(11156.6621, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11152.5400390625
tensor(11156.6621, grad_fn=<NegBackward0>) tensor(11152.5400, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11111.8369140625
tensor(11152.5400, grad_fn=<NegBackward0>) tensor(11111.8369, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11061.8984375
tensor(11111.8369, grad_fn=<NegBackward0>) tensor(11061.8984, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11052.5576171875
tensor(11061.8984, grad_fn=<NegBackward0>) tensor(11052.5576, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11048.623046875
tensor(11052.5576, grad_fn=<NegBackward0>) tensor(11048.6230, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11048.3720703125
tensor(11048.6230, grad_fn=<NegBackward0>) tensor(11048.3721, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11047.791015625
tensor(11048.3721, grad_fn=<NegBackward0>) tensor(11047.7910, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11047.708984375
tensor(11047.7910, grad_fn=<NegBackward0>) tensor(11047.7090, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11047.638671875
tensor(11047.7090, grad_fn=<NegBackward0>) tensor(11047.6387, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11047.5908203125
tensor(11047.6387, grad_fn=<NegBackward0>) tensor(11047.5908, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11047.5615234375
tensor(11047.5908, grad_fn=<NegBackward0>) tensor(11047.5615, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11047.5400390625
tensor(11047.5615, grad_fn=<NegBackward0>) tensor(11047.5400, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11047.5224609375
tensor(11047.5400, grad_fn=<NegBackward0>) tensor(11047.5225, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11047.5087890625
tensor(11047.5225, grad_fn=<NegBackward0>) tensor(11047.5088, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11047.4970703125
tensor(11047.5088, grad_fn=<NegBackward0>) tensor(11047.4971, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11047.4853515625
tensor(11047.4971, grad_fn=<NegBackward0>) tensor(11047.4854, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11047.4775390625
tensor(11047.4854, grad_fn=<NegBackward0>) tensor(11047.4775, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11047.4697265625
tensor(11047.4775, grad_fn=<NegBackward0>) tensor(11047.4697, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11047.4638671875
tensor(11047.4697, grad_fn=<NegBackward0>) tensor(11047.4639, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11047.4580078125
tensor(11047.4639, grad_fn=<NegBackward0>) tensor(11047.4580, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11047.453125
tensor(11047.4580, grad_fn=<NegBackward0>) tensor(11047.4531, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11047.4482421875
tensor(11047.4531, grad_fn=<NegBackward0>) tensor(11047.4482, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11047.4423828125
tensor(11047.4482, grad_fn=<NegBackward0>) tensor(11047.4424, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11047.4375
tensor(11047.4424, grad_fn=<NegBackward0>) tensor(11047.4375, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11047.4326171875
tensor(11047.4375, grad_fn=<NegBackward0>) tensor(11047.4326, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11047.4267578125
tensor(11047.4326, grad_fn=<NegBackward0>) tensor(11047.4268, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11047.4169921875
tensor(11047.4268, grad_fn=<NegBackward0>) tensor(11047.4170, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11047.3681640625
tensor(11047.4170, grad_fn=<NegBackward0>) tensor(11047.3682, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11047.3642578125
tensor(11047.3682, grad_fn=<NegBackward0>) tensor(11047.3643, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11047.361328125
tensor(11047.3643, grad_fn=<NegBackward0>) tensor(11047.3613, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11047.3486328125
tensor(11047.3613, grad_fn=<NegBackward0>) tensor(11047.3486, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11047.3330078125
tensor(11047.3486, grad_fn=<NegBackward0>) tensor(11047.3330, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11047.3134765625
tensor(11047.3330, grad_fn=<NegBackward0>) tensor(11047.3135, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11047.310546875
tensor(11047.3135, grad_fn=<NegBackward0>) tensor(11047.3105, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11047.3056640625
tensor(11047.3105, grad_fn=<NegBackward0>) tensor(11047.3057, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11047.2998046875
tensor(11047.3057, grad_fn=<NegBackward0>) tensor(11047.2998, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11047.2978515625
tensor(11047.2998, grad_fn=<NegBackward0>) tensor(11047.2979, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11047.2958984375
tensor(11047.2979, grad_fn=<NegBackward0>) tensor(11047.2959, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11047.2958984375
tensor(11047.2959, grad_fn=<NegBackward0>) tensor(11047.2959, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11047.294921875
tensor(11047.2959, grad_fn=<NegBackward0>) tensor(11047.2949, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11047.2939453125
tensor(11047.2949, grad_fn=<NegBackward0>) tensor(11047.2939, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11047.291015625
tensor(11047.2939, grad_fn=<NegBackward0>) tensor(11047.2910, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11047.287109375
tensor(11047.2910, grad_fn=<NegBackward0>) tensor(11047.2871, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11047.2841796875
tensor(11047.2871, grad_fn=<NegBackward0>) tensor(11047.2842, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11047.28125
tensor(11047.2842, grad_fn=<NegBackward0>) tensor(11047.2812, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11047.2822265625
tensor(11047.2812, grad_fn=<NegBackward0>) tensor(11047.2822, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11047.2890625
tensor(11047.2812, grad_fn=<NegBackward0>) tensor(11047.2891, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -11047.279296875
tensor(11047.2812, grad_fn=<NegBackward0>) tensor(11047.2793, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11047.2783203125
tensor(11047.2793, grad_fn=<NegBackward0>) tensor(11047.2783, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11047.298828125
tensor(11047.2783, grad_fn=<NegBackward0>) tensor(11047.2988, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11047.2763671875
tensor(11047.2783, grad_fn=<NegBackward0>) tensor(11047.2764, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11047.2763671875
tensor(11047.2764, grad_fn=<NegBackward0>) tensor(11047.2764, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11047.2744140625
tensor(11047.2764, grad_fn=<NegBackward0>) tensor(11047.2744, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11047.27734375
tensor(11047.2744, grad_fn=<NegBackward0>) tensor(11047.2773, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11047.2734375
tensor(11047.2744, grad_fn=<NegBackward0>) tensor(11047.2734, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11047.271484375
tensor(11047.2734, grad_fn=<NegBackward0>) tensor(11047.2715, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11047.267578125
tensor(11047.2715, grad_fn=<NegBackward0>) tensor(11047.2676, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11047.267578125
tensor(11047.2676, grad_fn=<NegBackward0>) tensor(11047.2676, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11047.2666015625
tensor(11047.2676, grad_fn=<NegBackward0>) tensor(11047.2666, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11047.265625
tensor(11047.2666, grad_fn=<NegBackward0>) tensor(11047.2656, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11047.2646484375
tensor(11047.2656, grad_fn=<NegBackward0>) tensor(11047.2646, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11047.263671875
tensor(11047.2646, grad_fn=<NegBackward0>) tensor(11047.2637, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11047.2646484375
tensor(11047.2637, grad_fn=<NegBackward0>) tensor(11047.2646, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11047.271484375
tensor(11047.2637, grad_fn=<NegBackward0>) tensor(11047.2715, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11047.2626953125
tensor(11047.2637, grad_fn=<NegBackward0>) tensor(11047.2627, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11047.26953125
tensor(11047.2627, grad_fn=<NegBackward0>) tensor(11047.2695, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11047.263671875
tensor(11047.2627, grad_fn=<NegBackward0>) tensor(11047.2637, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11047.2626953125
tensor(11047.2627, grad_fn=<NegBackward0>) tensor(11047.2627, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11047.263671875
tensor(11047.2627, grad_fn=<NegBackward0>) tensor(11047.2637, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11047.26171875
tensor(11047.2627, grad_fn=<NegBackward0>) tensor(11047.2617, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11047.2822265625
tensor(11047.2617, grad_fn=<NegBackward0>) tensor(11047.2822, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11047.263671875
tensor(11047.2617, grad_fn=<NegBackward0>) tensor(11047.2637, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11047.2626953125
tensor(11047.2617, grad_fn=<NegBackward0>) tensor(11047.2627, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11047.2666015625
tensor(11047.2617, grad_fn=<NegBackward0>) tensor(11047.2666, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -11047.2607421875
tensor(11047.2617, grad_fn=<NegBackward0>) tensor(11047.2607, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11047.26171875
tensor(11047.2607, grad_fn=<NegBackward0>) tensor(11047.2617, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11047.2607421875
tensor(11047.2607, grad_fn=<NegBackward0>) tensor(11047.2607, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11047.259765625
tensor(11047.2607, grad_fn=<NegBackward0>) tensor(11047.2598, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11047.2607421875
tensor(11047.2598, grad_fn=<NegBackward0>) tensor(11047.2607, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11047.2587890625
tensor(11047.2598, grad_fn=<NegBackward0>) tensor(11047.2588, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11047.326171875
tensor(11047.2588, grad_fn=<NegBackward0>) tensor(11047.3262, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11047.255859375
tensor(11047.2588, grad_fn=<NegBackward0>) tensor(11047.2559, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11047.251953125
tensor(11047.2559, grad_fn=<NegBackward0>) tensor(11047.2520, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11047.2451171875
tensor(11047.2520, grad_fn=<NegBackward0>) tensor(11047.2451, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11019.6064453125
tensor(11047.2451, grad_fn=<NegBackward0>) tensor(11019.6064, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11014.388671875
tensor(11019.6064, grad_fn=<NegBackward0>) tensor(11014.3887, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11014.3798828125
tensor(11014.3887, grad_fn=<NegBackward0>) tensor(11014.3799, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11014.373046875
tensor(11014.3799, grad_fn=<NegBackward0>) tensor(11014.3730, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11014.3642578125
tensor(11014.3730, grad_fn=<NegBackward0>) tensor(11014.3643, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11014.359375
tensor(11014.3643, grad_fn=<NegBackward0>) tensor(11014.3594, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11014.3466796875
tensor(11014.3594, grad_fn=<NegBackward0>) tensor(11014.3467, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11014.25390625
tensor(11014.3467, grad_fn=<NegBackward0>) tensor(11014.2539, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11014.2685546875
tensor(11014.2539, grad_fn=<NegBackward0>) tensor(11014.2686, grad_fn=<NegBackward0>)
1
pi: tensor([[0.7140, 0.2860],
        [0.1961, 0.8039]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5020, 0.4980], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1883, 0.1078],
         [0.6110, 0.2544]],

        [[0.6509, 0.0942],
         [0.7065, 0.6957]],

        [[0.5991, 0.1034],
         [0.5850, 0.6114]],

        [[0.5106, 0.1160],
         [0.5364, 0.5041]],

        [[0.5981, 0.0977],
         [0.5950, 0.6125]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 88
Adjusted Rand Index: 0.5732967625018367
time is 1
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9206289602688308
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 93
Adjusted Rand Index: 0.7369432436752338
time is 4
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.8313811374630521
Average Adjusted Rand Index: 0.8381737932891802
[0.8534546574309102, 0.8313811374630521] [0.8561952406238985, 0.8381737932891802] [11009.015625, 11014.24609375]
-------------------------------------
This iteration is 92
True Objective function: Loss = -10771.995948835178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21828.154296875
inf tensor(21828.1543, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10885.470703125
tensor(21828.1543, grad_fn=<NegBackward0>) tensor(10885.4707, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10881.0302734375
tensor(10885.4707, grad_fn=<NegBackward0>) tensor(10881.0303, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10879.7119140625
tensor(10881.0303, grad_fn=<NegBackward0>) tensor(10879.7119, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10878.7978515625
tensor(10879.7119, grad_fn=<NegBackward0>) tensor(10878.7979, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10878.021484375
tensor(10878.7979, grad_fn=<NegBackward0>) tensor(10878.0215, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10877.4619140625
tensor(10878.0215, grad_fn=<NegBackward0>) tensor(10877.4619, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10877.0908203125
tensor(10877.4619, grad_fn=<NegBackward0>) tensor(10877.0908, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10876.9560546875
tensor(10877.0908, grad_fn=<NegBackward0>) tensor(10876.9561, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10876.892578125
tensor(10876.9561, grad_fn=<NegBackward0>) tensor(10876.8926, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10876.8505859375
tensor(10876.8926, grad_fn=<NegBackward0>) tensor(10876.8506, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10876.8037109375
tensor(10876.8506, grad_fn=<NegBackward0>) tensor(10876.8037, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10876.779296875
tensor(10876.8037, grad_fn=<NegBackward0>) tensor(10876.7793, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10876.759765625
tensor(10876.7793, grad_fn=<NegBackward0>) tensor(10876.7598, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10876.7451171875
tensor(10876.7598, grad_fn=<NegBackward0>) tensor(10876.7451, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10876.73046875
tensor(10876.7451, grad_fn=<NegBackward0>) tensor(10876.7305, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10876.720703125
tensor(10876.7305, grad_fn=<NegBackward0>) tensor(10876.7207, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10876.712890625
tensor(10876.7207, grad_fn=<NegBackward0>) tensor(10876.7129, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10876.70703125
tensor(10876.7129, grad_fn=<NegBackward0>) tensor(10876.7070, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10876.7021484375
tensor(10876.7070, grad_fn=<NegBackward0>) tensor(10876.7021, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10876.697265625
tensor(10876.7021, grad_fn=<NegBackward0>) tensor(10876.6973, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10876.693359375
tensor(10876.6973, grad_fn=<NegBackward0>) tensor(10876.6934, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10876.6923828125
tensor(10876.6934, grad_fn=<NegBackward0>) tensor(10876.6924, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10876.689453125
tensor(10876.6924, grad_fn=<NegBackward0>) tensor(10876.6895, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10876.6875
tensor(10876.6895, grad_fn=<NegBackward0>) tensor(10876.6875, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10876.685546875
tensor(10876.6875, grad_fn=<NegBackward0>) tensor(10876.6855, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10876.68359375
tensor(10876.6855, grad_fn=<NegBackward0>) tensor(10876.6836, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10876.681640625
tensor(10876.6836, grad_fn=<NegBackward0>) tensor(10876.6816, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10876.681640625
tensor(10876.6816, grad_fn=<NegBackward0>) tensor(10876.6816, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10876.6806640625
tensor(10876.6816, grad_fn=<NegBackward0>) tensor(10876.6807, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10876.6806640625
tensor(10876.6807, grad_fn=<NegBackward0>) tensor(10876.6807, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10876.6787109375
tensor(10876.6807, grad_fn=<NegBackward0>) tensor(10876.6787, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10876.677734375
tensor(10876.6787, grad_fn=<NegBackward0>) tensor(10876.6777, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10876.6787109375
tensor(10876.6777, grad_fn=<NegBackward0>) tensor(10876.6787, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -10876.677734375
tensor(10876.6777, grad_fn=<NegBackward0>) tensor(10876.6777, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10876.677734375
tensor(10876.6777, grad_fn=<NegBackward0>) tensor(10876.6777, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10876.67578125
tensor(10876.6777, grad_fn=<NegBackward0>) tensor(10876.6758, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10876.67578125
tensor(10876.6758, grad_fn=<NegBackward0>) tensor(10876.6758, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10876.6748046875
tensor(10876.6758, grad_fn=<NegBackward0>) tensor(10876.6748, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10876.6748046875
tensor(10876.6748, grad_fn=<NegBackward0>) tensor(10876.6748, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10876.6748046875
tensor(10876.6748, grad_fn=<NegBackward0>) tensor(10876.6748, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10876.673828125
tensor(10876.6748, grad_fn=<NegBackward0>) tensor(10876.6738, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10876.6748046875
tensor(10876.6738, grad_fn=<NegBackward0>) tensor(10876.6748, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10876.673828125
tensor(10876.6738, grad_fn=<NegBackward0>) tensor(10876.6738, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10876.673828125
tensor(10876.6738, grad_fn=<NegBackward0>) tensor(10876.6738, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10876.6728515625
tensor(10876.6738, grad_fn=<NegBackward0>) tensor(10876.6729, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10876.6728515625
tensor(10876.6729, grad_fn=<NegBackward0>) tensor(10876.6729, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10876.6728515625
tensor(10876.6729, grad_fn=<NegBackward0>) tensor(10876.6729, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10876.671875
tensor(10876.6729, grad_fn=<NegBackward0>) tensor(10876.6719, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10876.669921875
tensor(10876.6719, grad_fn=<NegBackward0>) tensor(10876.6699, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10876.6708984375
tensor(10876.6699, grad_fn=<NegBackward0>) tensor(10876.6709, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10876.6708984375
tensor(10876.6699, grad_fn=<NegBackward0>) tensor(10876.6709, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -10876.6708984375
tensor(10876.6699, grad_fn=<NegBackward0>) tensor(10876.6709, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -10876.6689453125
tensor(10876.6699, grad_fn=<NegBackward0>) tensor(10876.6689, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10876.6689453125
tensor(10876.6689, grad_fn=<NegBackward0>) tensor(10876.6689, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10876.6708984375
tensor(10876.6689, grad_fn=<NegBackward0>) tensor(10876.6709, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10876.6708984375
tensor(10876.6689, grad_fn=<NegBackward0>) tensor(10876.6709, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -10876.6708984375
tensor(10876.6689, grad_fn=<NegBackward0>) tensor(10876.6709, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -10876.669921875
tensor(10876.6689, grad_fn=<NegBackward0>) tensor(10876.6699, grad_fn=<NegBackward0>)
4
Iteration 5900: Loss = -10876.669921875
tensor(10876.6689, grad_fn=<NegBackward0>) tensor(10876.6699, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5900 due to no improvement.
pi: tensor([[0.5596, 0.4404],
        [0.0411, 0.9589]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.7779e-05, 9.9998e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2613, 0.1514],
         [0.6461, 0.1542]],

        [[0.5381, 0.1836],
         [0.5966, 0.6454]],

        [[0.5370, 0.2606],
         [0.5844, 0.6017]],

        [[0.6370, 0.1667],
         [0.5350, 0.5115]],

        [[0.6596, 0.2354],
         [0.5373, 0.7276]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.006809752538456861
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.016449682236070833
Global Adjusted Rand Index: 0.00035113779516488956
Average Adjusted Rand Index: -0.001439467624697399
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25249.259765625
inf tensor(25249.2598, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10884.8681640625
tensor(25249.2598, grad_fn=<NegBackward0>) tensor(10884.8682, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10881.3310546875
tensor(10884.8682, grad_fn=<NegBackward0>) tensor(10881.3311, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10880.3056640625
tensor(10881.3311, grad_fn=<NegBackward0>) tensor(10880.3057, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10879.6337890625
tensor(10880.3057, grad_fn=<NegBackward0>) tensor(10879.6338, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10879.041015625
tensor(10879.6338, grad_fn=<NegBackward0>) tensor(10879.0410, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10878.5537109375
tensor(10879.0410, grad_fn=<NegBackward0>) tensor(10878.5537, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10878.1328125
tensor(10878.5537, grad_fn=<NegBackward0>) tensor(10878.1328, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10877.5322265625
tensor(10878.1328, grad_fn=<NegBackward0>) tensor(10877.5322, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10877.1640625
tensor(10877.5322, grad_fn=<NegBackward0>) tensor(10877.1641, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10876.998046875
tensor(10877.1641, grad_fn=<NegBackward0>) tensor(10876.9980, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10876.8974609375
tensor(10876.9980, grad_fn=<NegBackward0>) tensor(10876.8975, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10876.8271484375
tensor(10876.8975, grad_fn=<NegBackward0>) tensor(10876.8271, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10876.7861328125
tensor(10876.8271, grad_fn=<NegBackward0>) tensor(10876.7861, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10876.7587890625
tensor(10876.7861, grad_fn=<NegBackward0>) tensor(10876.7588, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10876.7412109375
tensor(10876.7588, grad_fn=<NegBackward0>) tensor(10876.7412, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10876.7236328125
tensor(10876.7412, grad_fn=<NegBackward0>) tensor(10876.7236, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10876.7109375
tensor(10876.7236, grad_fn=<NegBackward0>) tensor(10876.7109, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10876.6982421875
tensor(10876.7109, grad_fn=<NegBackward0>) tensor(10876.6982, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10876.6865234375
tensor(10876.6982, grad_fn=<NegBackward0>) tensor(10876.6865, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10876.67578125
tensor(10876.6865, grad_fn=<NegBackward0>) tensor(10876.6758, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10876.662109375
tensor(10876.6758, grad_fn=<NegBackward0>) tensor(10876.6621, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10876.6474609375
tensor(10876.6621, grad_fn=<NegBackward0>) tensor(10876.6475, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10876.6318359375
tensor(10876.6475, grad_fn=<NegBackward0>) tensor(10876.6318, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10876.615234375
tensor(10876.6318, grad_fn=<NegBackward0>) tensor(10876.6152, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10876.591796875
tensor(10876.6152, grad_fn=<NegBackward0>) tensor(10876.5918, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10876.5615234375
tensor(10876.5918, grad_fn=<NegBackward0>) tensor(10876.5615, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10876.51171875
tensor(10876.5615, grad_fn=<NegBackward0>) tensor(10876.5117, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10876.4541015625
tensor(10876.5117, grad_fn=<NegBackward0>) tensor(10876.4541, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10876.4072265625
tensor(10876.4541, grad_fn=<NegBackward0>) tensor(10876.4072, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10876.3603515625
tensor(10876.4072, grad_fn=<NegBackward0>) tensor(10876.3604, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10876.2431640625
tensor(10876.3604, grad_fn=<NegBackward0>) tensor(10876.2432, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10875.6123046875
tensor(10876.2432, grad_fn=<NegBackward0>) tensor(10875.6123, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10864.4462890625
tensor(10875.6123, grad_fn=<NegBackward0>) tensor(10864.4463, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10743.283203125
tensor(10864.4463, grad_fn=<NegBackward0>) tensor(10743.2832, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10738.078125
tensor(10743.2832, grad_fn=<NegBackward0>) tensor(10738.0781, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10737.84765625
tensor(10738.0781, grad_fn=<NegBackward0>) tensor(10737.8477, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10736.45703125
tensor(10737.8477, grad_fn=<NegBackward0>) tensor(10736.4570, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10736.4208984375
tensor(10736.4570, grad_fn=<NegBackward0>) tensor(10736.4209, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10736.416015625
tensor(10736.4209, grad_fn=<NegBackward0>) tensor(10736.4160, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10736.4111328125
tensor(10736.4160, grad_fn=<NegBackward0>) tensor(10736.4111, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10736.408203125
tensor(10736.4111, grad_fn=<NegBackward0>) tensor(10736.4082, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10736.408203125
tensor(10736.4082, grad_fn=<NegBackward0>) tensor(10736.4082, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10736.3984375
tensor(10736.4082, grad_fn=<NegBackward0>) tensor(10736.3984, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10736.388671875
tensor(10736.3984, grad_fn=<NegBackward0>) tensor(10736.3887, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10736.373046875
tensor(10736.3887, grad_fn=<NegBackward0>) tensor(10736.3730, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10736.3544921875
tensor(10736.3730, grad_fn=<NegBackward0>) tensor(10736.3545, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10736.3525390625
tensor(10736.3545, grad_fn=<NegBackward0>) tensor(10736.3525, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10736.3525390625
tensor(10736.3525, grad_fn=<NegBackward0>) tensor(10736.3525, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10736.3515625
tensor(10736.3525, grad_fn=<NegBackward0>) tensor(10736.3516, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10736.357421875
tensor(10736.3516, grad_fn=<NegBackward0>) tensor(10736.3574, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10736.349609375
tensor(10736.3516, grad_fn=<NegBackward0>) tensor(10736.3496, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10736.3427734375
tensor(10736.3496, grad_fn=<NegBackward0>) tensor(10736.3428, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10736.3388671875
tensor(10736.3428, grad_fn=<NegBackward0>) tensor(10736.3389, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10736.3369140625
tensor(10736.3389, grad_fn=<NegBackward0>) tensor(10736.3369, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10736.333984375
tensor(10736.3369, grad_fn=<NegBackward0>) tensor(10736.3340, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10736.33203125
tensor(10736.3340, grad_fn=<NegBackward0>) tensor(10736.3320, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10736.32421875
tensor(10736.3320, grad_fn=<NegBackward0>) tensor(10736.3242, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10736.314453125
tensor(10736.3242, grad_fn=<NegBackward0>) tensor(10736.3145, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10736.3134765625
tensor(10736.3145, grad_fn=<NegBackward0>) tensor(10736.3135, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10736.3115234375
tensor(10736.3135, grad_fn=<NegBackward0>) tensor(10736.3115, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10736.310546875
tensor(10736.3115, grad_fn=<NegBackward0>) tensor(10736.3105, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10736.3095703125
tensor(10736.3105, grad_fn=<NegBackward0>) tensor(10736.3096, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10736.310546875
tensor(10736.3096, grad_fn=<NegBackward0>) tensor(10736.3105, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10736.3095703125
tensor(10736.3096, grad_fn=<NegBackward0>) tensor(10736.3096, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10736.3095703125
tensor(10736.3096, grad_fn=<NegBackward0>) tensor(10736.3096, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10736.3095703125
tensor(10736.3096, grad_fn=<NegBackward0>) tensor(10736.3096, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10736.3095703125
tensor(10736.3096, grad_fn=<NegBackward0>) tensor(10736.3096, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10736.3134765625
tensor(10736.3096, grad_fn=<NegBackward0>) tensor(10736.3135, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10736.3095703125
tensor(10736.3096, grad_fn=<NegBackward0>) tensor(10736.3096, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10736.314453125
tensor(10736.3096, grad_fn=<NegBackward0>) tensor(10736.3145, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10736.3076171875
tensor(10736.3096, grad_fn=<NegBackward0>) tensor(10736.3076, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10736.3095703125
tensor(10736.3076, grad_fn=<NegBackward0>) tensor(10736.3096, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10736.30859375
tensor(10736.3076, grad_fn=<NegBackward0>) tensor(10736.3086, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -10736.30859375
tensor(10736.3076, grad_fn=<NegBackward0>) tensor(10736.3086, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -10736.3076171875
tensor(10736.3076, grad_fn=<NegBackward0>) tensor(10736.3076, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10736.3076171875
tensor(10736.3076, grad_fn=<NegBackward0>) tensor(10736.3076, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10736.3056640625
tensor(10736.3076, grad_fn=<NegBackward0>) tensor(10736.3057, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10736.3046875
tensor(10736.3057, grad_fn=<NegBackward0>) tensor(10736.3047, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10736.2958984375
tensor(10736.3047, grad_fn=<NegBackward0>) tensor(10736.2959, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10736.2919921875
tensor(10736.2959, grad_fn=<NegBackward0>) tensor(10736.2920, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10736.31640625
tensor(10736.2920, grad_fn=<NegBackward0>) tensor(10736.3164, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10736.2880859375
tensor(10736.2920, grad_fn=<NegBackward0>) tensor(10736.2881, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10736.3037109375
tensor(10736.2881, grad_fn=<NegBackward0>) tensor(10736.3037, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10736.2890625
tensor(10736.2881, grad_fn=<NegBackward0>) tensor(10736.2891, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -10736.287109375
tensor(10736.2881, grad_fn=<NegBackward0>) tensor(10736.2871, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10736.2998046875
tensor(10736.2871, grad_fn=<NegBackward0>) tensor(10736.2998, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10736.2890625
tensor(10736.2871, grad_fn=<NegBackward0>) tensor(10736.2891, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -10736.2890625
tensor(10736.2871, grad_fn=<NegBackward0>) tensor(10736.2891, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -10736.2880859375
tensor(10736.2871, grad_fn=<NegBackward0>) tensor(10736.2881, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -10736.2890625
tensor(10736.2871, grad_fn=<NegBackward0>) tensor(10736.2891, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[0.7559, 0.2441],
        [0.2295, 0.7705]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4806, 0.5194], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1944, 0.0937],
         [0.6868, 0.2509]],

        [[0.7014, 0.0975],
         [0.5783, 0.5444]],

        [[0.6569, 0.1047],
         [0.6854, 0.6495]],

        [[0.7050, 0.0962],
         [0.5381, 0.6469]],

        [[0.5322, 0.0947],
         [0.5586, 0.6843]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7721016799725718
time is 1
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.9061159983727102
Average Adjusted Rand Index: 0.9067399376890817
[0.00035113779516488956, 0.9061159983727102] [-0.001439467624697399, 0.9067399376890817] [10876.669921875, 10736.2890625]
-------------------------------------
This iteration is 93
True Objective function: Loss = -10851.276283844172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24069.080078125
inf tensor(24069.0801, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10927.0126953125
tensor(24069.0801, grad_fn=<NegBackward0>) tensor(10927.0127, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10923.3203125
tensor(10927.0127, grad_fn=<NegBackward0>) tensor(10923.3203, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10921.3662109375
tensor(10923.3203, grad_fn=<NegBackward0>) tensor(10921.3662, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10920.73828125
tensor(10921.3662, grad_fn=<NegBackward0>) tensor(10920.7383, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10920.021484375
tensor(10920.7383, grad_fn=<NegBackward0>) tensor(10920.0215, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10919.6962890625
tensor(10920.0215, grad_fn=<NegBackward0>) tensor(10919.6963, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10919.6103515625
tensor(10919.6963, grad_fn=<NegBackward0>) tensor(10919.6104, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10919.5751953125
tensor(10919.6104, grad_fn=<NegBackward0>) tensor(10919.5752, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10919.5615234375
tensor(10919.5752, grad_fn=<NegBackward0>) tensor(10919.5615, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10919.5537109375
tensor(10919.5615, grad_fn=<NegBackward0>) tensor(10919.5537, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10919.5498046875
tensor(10919.5537, grad_fn=<NegBackward0>) tensor(10919.5498, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10919.544921875
tensor(10919.5498, grad_fn=<NegBackward0>) tensor(10919.5449, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10919.5439453125
tensor(10919.5449, grad_fn=<NegBackward0>) tensor(10919.5439, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10919.5361328125
tensor(10919.5439, grad_fn=<NegBackward0>) tensor(10919.5361, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10919.53125
tensor(10919.5361, grad_fn=<NegBackward0>) tensor(10919.5312, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10919.5263671875
tensor(10919.5312, grad_fn=<NegBackward0>) tensor(10919.5264, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10919.5244140625
tensor(10919.5264, grad_fn=<NegBackward0>) tensor(10919.5244, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10919.521484375
tensor(10919.5244, grad_fn=<NegBackward0>) tensor(10919.5215, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10919.5205078125
tensor(10919.5215, grad_fn=<NegBackward0>) tensor(10919.5205, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10919.5205078125
tensor(10919.5205, grad_fn=<NegBackward0>) tensor(10919.5205, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10919.51953125
tensor(10919.5205, grad_fn=<NegBackward0>) tensor(10919.5195, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10919.5185546875
tensor(10919.5195, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10919.5205078125
tensor(10919.5186, grad_fn=<NegBackward0>) tensor(10919.5205, grad_fn=<NegBackward0>)
1
Iteration 2400: Loss = -10919.51953125
tensor(10919.5186, grad_fn=<NegBackward0>) tensor(10919.5195, grad_fn=<NegBackward0>)
2
Iteration 2500: Loss = -10919.5185546875
tensor(10919.5186, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10919.5185546875
tensor(10919.5186, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10919.5185546875
tensor(10919.5186, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10919.5185546875
tensor(10919.5186, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10919.5185546875
tensor(10919.5186, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10919.5185546875
tensor(10919.5186, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10919.5185546875
tensor(10919.5186, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10919.5185546875
tensor(10919.5186, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10919.5185546875
tensor(10919.5186, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10919.51953125
tensor(10919.5186, grad_fn=<NegBackward0>) tensor(10919.5195, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -10919.5185546875
tensor(10919.5186, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10919.517578125
tensor(10919.5186, grad_fn=<NegBackward0>) tensor(10919.5176, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10919.51953125
tensor(10919.5176, grad_fn=<NegBackward0>) tensor(10919.5195, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -10919.5185546875
tensor(10919.5176, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -10919.5185546875
tensor(10919.5176, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
3
Iteration 4000: Loss = -10919.5185546875
tensor(10919.5176, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
4
Iteration 4100: Loss = -10919.5205078125
tensor(10919.5176, grad_fn=<NegBackward0>) tensor(10919.5205, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4100 due to no improvement.
pi: tensor([[0.2574, 0.7426],
        [0.1393, 0.8607]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2103, 0.7897], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2419, 0.1915],
         [0.6190, 0.1474]],

        [[0.6569, 0.1645],
         [0.7275, 0.5748]],

        [[0.5038, 0.2092],
         [0.6893, 0.6069]],

        [[0.5054, 0.1723],
         [0.6603, 0.5406]],

        [[0.5019, 0.1887],
         [0.6826, 0.6982]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.015397100792008605
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.034351502981690596
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: 0.005609148800160623
Average Adjusted Rand Index: -0.003993107267780947
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22981.876953125
inf tensor(22981.8770, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10927.53125
tensor(22981.8770, grad_fn=<NegBackward0>) tensor(10927.5312, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10925.3984375
tensor(10927.5312, grad_fn=<NegBackward0>) tensor(10925.3984, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10923.8115234375
tensor(10925.3984, grad_fn=<NegBackward0>) tensor(10923.8115, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10921.8359375
tensor(10923.8115, grad_fn=<NegBackward0>) tensor(10921.8359, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10921.140625
tensor(10921.8359, grad_fn=<NegBackward0>) tensor(10921.1406, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10920.62890625
tensor(10921.1406, grad_fn=<NegBackward0>) tensor(10920.6289, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10920.2431640625
tensor(10920.6289, grad_fn=<NegBackward0>) tensor(10920.2432, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10920.017578125
tensor(10920.2432, grad_fn=<NegBackward0>) tensor(10920.0176, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10919.865234375
tensor(10920.0176, grad_fn=<NegBackward0>) tensor(10919.8652, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10919.7607421875
tensor(10919.8652, grad_fn=<NegBackward0>) tensor(10919.7607, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10919.69140625
tensor(10919.7607, grad_fn=<NegBackward0>) tensor(10919.6914, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10919.642578125
tensor(10919.6914, grad_fn=<NegBackward0>) tensor(10919.6426, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10919.615234375
tensor(10919.6426, grad_fn=<NegBackward0>) tensor(10919.6152, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10919.5966796875
tensor(10919.6152, grad_fn=<NegBackward0>) tensor(10919.5967, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10919.5869140625
tensor(10919.5967, grad_fn=<NegBackward0>) tensor(10919.5869, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10919.5810546875
tensor(10919.5869, grad_fn=<NegBackward0>) tensor(10919.5811, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10919.58203125
tensor(10919.5811, grad_fn=<NegBackward0>) tensor(10919.5820, grad_fn=<NegBackward0>)
1
Iteration 1800: Loss = -10919.576171875
tensor(10919.5811, grad_fn=<NegBackward0>) tensor(10919.5762, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10919.576171875
tensor(10919.5762, grad_fn=<NegBackward0>) tensor(10919.5762, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10919.57421875
tensor(10919.5762, grad_fn=<NegBackward0>) tensor(10919.5742, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10919.5732421875
tensor(10919.5742, grad_fn=<NegBackward0>) tensor(10919.5732, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10919.5732421875
tensor(10919.5732, grad_fn=<NegBackward0>) tensor(10919.5732, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10919.5703125
tensor(10919.5732, grad_fn=<NegBackward0>) tensor(10919.5703, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10919.5693359375
tensor(10919.5703, grad_fn=<NegBackward0>) tensor(10919.5693, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10919.56640625
tensor(10919.5693, grad_fn=<NegBackward0>) tensor(10919.5664, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10919.564453125
tensor(10919.5664, grad_fn=<NegBackward0>) tensor(10919.5645, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10919.5625
tensor(10919.5645, grad_fn=<NegBackward0>) tensor(10919.5625, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10919.560546875
tensor(10919.5625, grad_fn=<NegBackward0>) tensor(10919.5605, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10919.556640625
tensor(10919.5605, grad_fn=<NegBackward0>) tensor(10919.5566, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10919.5546875
tensor(10919.5566, grad_fn=<NegBackward0>) tensor(10919.5547, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10919.5498046875
tensor(10919.5547, grad_fn=<NegBackward0>) tensor(10919.5498, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10919.546875
tensor(10919.5498, grad_fn=<NegBackward0>) tensor(10919.5469, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10919.5419921875
tensor(10919.5469, grad_fn=<NegBackward0>) tensor(10919.5420, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10919.537109375
tensor(10919.5420, grad_fn=<NegBackward0>) tensor(10919.5371, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10919.533203125
tensor(10919.5371, grad_fn=<NegBackward0>) tensor(10919.5332, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10919.52734375
tensor(10919.5332, grad_fn=<NegBackward0>) tensor(10919.5273, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10919.5263671875
tensor(10919.5273, grad_fn=<NegBackward0>) tensor(10919.5264, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10919.5224609375
tensor(10919.5264, grad_fn=<NegBackward0>) tensor(10919.5225, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10919.521484375
tensor(10919.5225, grad_fn=<NegBackward0>) tensor(10919.5215, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10919.5205078125
tensor(10919.5215, grad_fn=<NegBackward0>) tensor(10919.5205, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10919.5185546875
tensor(10919.5205, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10919.5205078125
tensor(10919.5186, grad_fn=<NegBackward0>) tensor(10919.5205, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10919.5185546875
tensor(10919.5186, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10919.5185546875
tensor(10919.5186, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10919.5185546875
tensor(10919.5186, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10919.517578125
tensor(10919.5186, grad_fn=<NegBackward0>) tensor(10919.5176, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10919.5185546875
tensor(10919.5176, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10919.5185546875
tensor(10919.5176, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -10919.5185546875
tensor(10919.5176, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -10919.51953125
tensor(10919.5176, grad_fn=<NegBackward0>) tensor(10919.5195, grad_fn=<NegBackward0>)
4
Iteration 5100: Loss = -10919.5185546875
tensor(10919.5176, grad_fn=<NegBackward0>) tensor(10919.5186, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5100 due to no improvement.
pi: tensor([[0.8607, 0.1393],
        [0.7435, 0.2565]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7921, 0.2079], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1474, 0.1914],
         [0.5787, 0.2419]],

        [[0.6933, 0.1645],
         [0.5458, 0.7283]],

        [[0.5179, 0.2092],
         [0.6490, 0.6716]],

        [[0.6203, 0.1723],
         [0.5783, 0.5029]],

        [[0.6072, 0.1887],
         [0.5706, 0.6831]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.015397100792008605
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.034351502981690596
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: 0.005609148800160623
Average Adjusted Rand Index: -0.003993107267780947
[0.005609148800160623, 0.005609148800160623] [-0.003993107267780947, -0.003993107267780947] [10919.5205078125, 10919.5185546875]
-------------------------------------
This iteration is 94
True Objective function: Loss = -10996.15386462653
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24418.259765625
inf tensor(24418.2598, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11109.197265625
tensor(24418.2598, grad_fn=<NegBackward0>) tensor(11109.1973, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11107.875
tensor(11109.1973, grad_fn=<NegBackward0>) tensor(11107.8750, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11107.455078125
tensor(11107.8750, grad_fn=<NegBackward0>) tensor(11107.4551, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11107.0966796875
tensor(11107.4551, grad_fn=<NegBackward0>) tensor(11107.0967, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11106.94921875
tensor(11107.0967, grad_fn=<NegBackward0>) tensor(11106.9492, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11106.875
tensor(11106.9492, grad_fn=<NegBackward0>) tensor(11106.8750, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11106.826171875
tensor(11106.8750, grad_fn=<NegBackward0>) tensor(11106.8262, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11106.7880859375
tensor(11106.8262, grad_fn=<NegBackward0>) tensor(11106.7881, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11106.755859375
tensor(11106.7881, grad_fn=<NegBackward0>) tensor(11106.7559, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11106.728515625
tensor(11106.7559, grad_fn=<NegBackward0>) tensor(11106.7285, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11106.7041015625
tensor(11106.7285, grad_fn=<NegBackward0>) tensor(11106.7041, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11106.6865234375
tensor(11106.7041, grad_fn=<NegBackward0>) tensor(11106.6865, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11106.6728515625
tensor(11106.6865, grad_fn=<NegBackward0>) tensor(11106.6729, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11106.6611328125
tensor(11106.6729, grad_fn=<NegBackward0>) tensor(11106.6611, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11106.6513671875
tensor(11106.6611, grad_fn=<NegBackward0>) tensor(11106.6514, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11106.6435546875
tensor(11106.6514, grad_fn=<NegBackward0>) tensor(11106.6436, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11106.6357421875
tensor(11106.6436, grad_fn=<NegBackward0>) tensor(11106.6357, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11106.6298828125
tensor(11106.6357, grad_fn=<NegBackward0>) tensor(11106.6299, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11106.625
tensor(11106.6299, grad_fn=<NegBackward0>) tensor(11106.6250, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11106.619140625
tensor(11106.6250, grad_fn=<NegBackward0>) tensor(11106.6191, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11106.6162109375
tensor(11106.6191, grad_fn=<NegBackward0>) tensor(11106.6162, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11106.6123046875
tensor(11106.6162, grad_fn=<NegBackward0>) tensor(11106.6123, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11106.609375
tensor(11106.6123, grad_fn=<NegBackward0>) tensor(11106.6094, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11106.607421875
tensor(11106.6094, grad_fn=<NegBackward0>) tensor(11106.6074, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11106.6025390625
tensor(11106.6074, grad_fn=<NegBackward0>) tensor(11106.6025, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11106.6015625
tensor(11106.6025, grad_fn=<NegBackward0>) tensor(11106.6016, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11106.6005859375
tensor(11106.6016, grad_fn=<NegBackward0>) tensor(11106.6006, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11106.599609375
tensor(11106.6006, grad_fn=<NegBackward0>) tensor(11106.5996, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11106.59765625
tensor(11106.5996, grad_fn=<NegBackward0>) tensor(11106.5977, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11106.5986328125
tensor(11106.5977, grad_fn=<NegBackward0>) tensor(11106.5986, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11106.595703125
tensor(11106.5977, grad_fn=<NegBackward0>) tensor(11106.5957, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11106.5947265625
tensor(11106.5957, grad_fn=<NegBackward0>) tensor(11106.5947, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11106.5947265625
tensor(11106.5947, grad_fn=<NegBackward0>) tensor(11106.5947, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11106.5947265625
tensor(11106.5947, grad_fn=<NegBackward0>) tensor(11106.5947, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11106.59375
tensor(11106.5947, grad_fn=<NegBackward0>) tensor(11106.5938, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11106.59375
tensor(11106.5938, grad_fn=<NegBackward0>) tensor(11106.5938, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11106.5947265625
tensor(11106.5938, grad_fn=<NegBackward0>) tensor(11106.5947, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11106.5927734375
tensor(11106.5938, grad_fn=<NegBackward0>) tensor(11106.5928, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11106.5927734375
tensor(11106.5928, grad_fn=<NegBackward0>) tensor(11106.5928, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11106.59375
tensor(11106.5928, grad_fn=<NegBackward0>) tensor(11106.5938, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11106.59375
tensor(11106.5928, grad_fn=<NegBackward0>) tensor(11106.5938, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -11106.5927734375
tensor(11106.5928, grad_fn=<NegBackward0>) tensor(11106.5928, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11106.591796875
tensor(11106.5928, grad_fn=<NegBackward0>) tensor(11106.5918, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11106.591796875
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5918, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11106.591796875
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5918, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11106.5927734375
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5928, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11106.5927734375
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5928, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -11106.5927734375
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5928, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -11106.591796875
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5918, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11106.591796875
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5918, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11106.5927734375
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5928, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11106.5927734375
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5928, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11106.591796875
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5918, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11106.59375
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5938, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11106.5927734375
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5928, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11106.591796875
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5918, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11106.59375
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5938, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11106.5927734375
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5928, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11106.5927734375
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5928, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -11106.591796875
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5918, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11106.59375
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5938, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11106.59375
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5938, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11106.5927734375
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5928, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -11106.5927734375
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5928, grad_fn=<NegBackward0>)
4
Iteration 6500: Loss = -11106.5908203125
tensor(11106.5918, grad_fn=<NegBackward0>) tensor(11106.5908, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11106.5927734375
tensor(11106.5908, grad_fn=<NegBackward0>) tensor(11106.5928, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11106.5927734375
tensor(11106.5908, grad_fn=<NegBackward0>) tensor(11106.5928, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11106.591796875
tensor(11106.5908, grad_fn=<NegBackward0>) tensor(11106.5918, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -11106.5927734375
tensor(11106.5908, grad_fn=<NegBackward0>) tensor(11106.5928, grad_fn=<NegBackward0>)
4
Iteration 7000: Loss = -11106.591796875
tensor(11106.5908, grad_fn=<NegBackward0>) tensor(11106.5918, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7000 due to no improvement.
pi: tensor([[0.3997, 0.6003],
        [0.0108, 0.9892]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0216, 0.9784], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1792, 0.2825],
         [0.7253, 0.1638]],

        [[0.6961, 0.1982],
         [0.7243, 0.7230]],

        [[0.5197, 0.0972],
         [0.7196, 0.5759]],

        [[0.5264, 0.1984],
         [0.6794, 0.6276]],

        [[0.5353, 0.2640],
         [0.6293, 0.6718]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
Global Adjusted Rand Index: -0.002503066891692963
Average Adjusted Rand Index: -0.0017364876533894464
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22589.18359375
inf tensor(22589.1836, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11110.28125
tensor(22589.1836, grad_fn=<NegBackward0>) tensor(11110.2812, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11109.849609375
tensor(11110.2812, grad_fn=<NegBackward0>) tensor(11109.8496, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11109.68359375
tensor(11109.8496, grad_fn=<NegBackward0>) tensor(11109.6836, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11109.5546875
tensor(11109.6836, grad_fn=<NegBackward0>) tensor(11109.5547, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11109.2314453125
tensor(11109.5547, grad_fn=<NegBackward0>) tensor(11109.2314, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11108.3583984375
tensor(11109.2314, grad_fn=<NegBackward0>) tensor(11108.3584, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11108.11328125
tensor(11108.3584, grad_fn=<NegBackward0>) tensor(11108.1133, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11107.5205078125
tensor(11108.1133, grad_fn=<NegBackward0>) tensor(11107.5205, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11101.1552734375
tensor(11107.5205, grad_fn=<NegBackward0>) tensor(11101.1553, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11096.552734375
tensor(11101.1553, grad_fn=<NegBackward0>) tensor(11096.5527, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11091.576171875
tensor(11096.5527, grad_fn=<NegBackward0>) tensor(11091.5762, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11023.2109375
tensor(11091.5762, grad_fn=<NegBackward0>) tensor(11023.2109, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10998.939453125
tensor(11023.2109, grad_fn=<NegBackward0>) tensor(10998.9395, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10997.5966796875
tensor(10998.9395, grad_fn=<NegBackward0>) tensor(10997.5967, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10995.5693359375
tensor(10997.5967, grad_fn=<NegBackward0>) tensor(10995.5693, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10995.4755859375
tensor(10995.5693, grad_fn=<NegBackward0>) tensor(10995.4756, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10995.4384765625
tensor(10995.4756, grad_fn=<NegBackward0>) tensor(10995.4385, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10994.6923828125
tensor(10995.4385, grad_fn=<NegBackward0>) tensor(10994.6924, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10994.630859375
tensor(10994.6924, grad_fn=<NegBackward0>) tensor(10994.6309, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10994.6181640625
tensor(10994.6309, grad_fn=<NegBackward0>) tensor(10994.6182, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10994.609375
tensor(10994.6182, grad_fn=<NegBackward0>) tensor(10994.6094, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10994.6005859375
tensor(10994.6094, grad_fn=<NegBackward0>) tensor(10994.6006, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10994.572265625
tensor(10994.6006, grad_fn=<NegBackward0>) tensor(10994.5723, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10994.560546875
tensor(10994.5723, grad_fn=<NegBackward0>) tensor(10994.5605, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10994.548828125
tensor(10994.5605, grad_fn=<NegBackward0>) tensor(10994.5488, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10994.5439453125
tensor(10994.5488, grad_fn=<NegBackward0>) tensor(10994.5439, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10994.5390625
tensor(10994.5439, grad_fn=<NegBackward0>) tensor(10994.5391, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10994.537109375
tensor(10994.5391, grad_fn=<NegBackward0>) tensor(10994.5371, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10994.5322265625
tensor(10994.5371, grad_fn=<NegBackward0>) tensor(10994.5322, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10994.5341796875
tensor(10994.5322, grad_fn=<NegBackward0>) tensor(10994.5342, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -10994.529296875
tensor(10994.5322, grad_fn=<NegBackward0>) tensor(10994.5293, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10994.5234375
tensor(10994.5293, grad_fn=<NegBackward0>) tensor(10994.5234, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10994.44921875
tensor(10994.5234, grad_fn=<NegBackward0>) tensor(10994.4492, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10994.447265625
tensor(10994.4492, grad_fn=<NegBackward0>) tensor(10994.4473, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10994.4462890625
tensor(10994.4473, grad_fn=<NegBackward0>) tensor(10994.4463, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10994.4453125
tensor(10994.4463, grad_fn=<NegBackward0>) tensor(10994.4453, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10994.4443359375
tensor(10994.4453, grad_fn=<NegBackward0>) tensor(10994.4443, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10994.44921875
tensor(10994.4443, grad_fn=<NegBackward0>) tensor(10994.4492, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -10994.44140625
tensor(10994.4443, grad_fn=<NegBackward0>) tensor(10994.4414, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10994.44140625
tensor(10994.4414, grad_fn=<NegBackward0>) tensor(10994.4414, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10994.439453125
tensor(10994.4414, grad_fn=<NegBackward0>) tensor(10994.4395, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10994.4384765625
tensor(10994.4395, grad_fn=<NegBackward0>) tensor(10994.4385, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10994.4365234375
tensor(10994.4385, grad_fn=<NegBackward0>) tensor(10994.4365, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10994.4404296875
tensor(10994.4365, grad_fn=<NegBackward0>) tensor(10994.4404, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10994.435546875
tensor(10994.4365, grad_fn=<NegBackward0>) tensor(10994.4355, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10994.4345703125
tensor(10994.4355, grad_fn=<NegBackward0>) tensor(10994.4346, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10994.435546875
tensor(10994.4346, grad_fn=<NegBackward0>) tensor(10994.4355, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10994.435546875
tensor(10994.4346, grad_fn=<NegBackward0>) tensor(10994.4355, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -10994.43359375
tensor(10994.4346, grad_fn=<NegBackward0>) tensor(10994.4336, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10994.4345703125
tensor(10994.4336, grad_fn=<NegBackward0>) tensor(10994.4346, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10994.4345703125
tensor(10994.4336, grad_fn=<NegBackward0>) tensor(10994.4346, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -10994.4345703125
tensor(10994.4336, grad_fn=<NegBackward0>) tensor(10994.4346, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -10994.4345703125
tensor(10994.4336, grad_fn=<NegBackward0>) tensor(10994.4346, grad_fn=<NegBackward0>)
4
Iteration 5400: Loss = -10994.447265625
tensor(10994.4336, grad_fn=<NegBackward0>) tensor(10994.4473, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5400 due to no improvement.
pi: tensor([[0.7152, 0.2848],
        [0.4242, 0.5758]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0139, 0.9861], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2587, 0.2810],
         [0.5327, 0.1766]],

        [[0.5973, 0.1016],
         [0.6686, 0.7138]],

        [[0.6779, 0.1053],
         [0.6047, 0.6145]],

        [[0.7102, 0.1078],
         [0.6779, 0.7042]],

        [[0.5783, 0.0870],
         [0.7107, 0.6719]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 2
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 7
Adjusted Rand Index: 0.7369583604949977
time is 3
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.5645896816386253
Average Adjusted Rand Index: 0.7082044460410428
[-0.002503066891692963, 0.5645896816386253] [-0.0017364876533894464, 0.7082044460410428] [11106.591796875, 10994.447265625]
-------------------------------------
This iteration is 95
True Objective function: Loss = -10922.777014120691
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21486.859375
inf tensor(21486.8594, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10985.1904296875
tensor(21486.8594, grad_fn=<NegBackward0>) tensor(10985.1904, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10984.2744140625
tensor(10985.1904, grad_fn=<NegBackward0>) tensor(10984.2744, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10983.9716796875
tensor(10984.2744, grad_fn=<NegBackward0>) tensor(10983.9717, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10983.8310546875
tensor(10983.9717, grad_fn=<NegBackward0>) tensor(10983.8311, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10983.71875
tensor(10983.8311, grad_fn=<NegBackward0>) tensor(10983.7188, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10983.609375
tensor(10983.7188, grad_fn=<NegBackward0>) tensor(10983.6094, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10983.5
tensor(10983.6094, grad_fn=<NegBackward0>) tensor(10983.5000, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10983.40234375
tensor(10983.5000, grad_fn=<NegBackward0>) tensor(10983.4023, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10983.3291015625
tensor(10983.4023, grad_fn=<NegBackward0>) tensor(10983.3291, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10983.271484375
tensor(10983.3291, grad_fn=<NegBackward0>) tensor(10983.2715, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10983.2255859375
tensor(10983.2715, grad_fn=<NegBackward0>) tensor(10983.2256, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10983.189453125
tensor(10983.2256, grad_fn=<NegBackward0>) tensor(10983.1895, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10983.15625
tensor(10983.1895, grad_fn=<NegBackward0>) tensor(10983.1562, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10983.1298828125
tensor(10983.1562, grad_fn=<NegBackward0>) tensor(10983.1299, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10983.107421875
tensor(10983.1299, grad_fn=<NegBackward0>) tensor(10983.1074, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10983.0888671875
tensor(10983.1074, grad_fn=<NegBackward0>) tensor(10983.0889, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10983.076171875
tensor(10983.0889, grad_fn=<NegBackward0>) tensor(10983.0762, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10983.06640625
tensor(10983.0762, grad_fn=<NegBackward0>) tensor(10983.0664, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10983.0556640625
tensor(10983.0664, grad_fn=<NegBackward0>) tensor(10983.0557, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10983.048828125
tensor(10983.0557, grad_fn=<NegBackward0>) tensor(10983.0488, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10983.04296875
tensor(10983.0488, grad_fn=<NegBackward0>) tensor(10983.0430, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10983.0361328125
tensor(10983.0430, grad_fn=<NegBackward0>) tensor(10983.0361, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10983.0302734375
tensor(10983.0361, grad_fn=<NegBackward0>) tensor(10983.0303, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10983.025390625
tensor(10983.0303, grad_fn=<NegBackward0>) tensor(10983.0254, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10983.01953125
tensor(10983.0254, grad_fn=<NegBackward0>) tensor(10983.0195, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10983.015625
tensor(10983.0195, grad_fn=<NegBackward0>) tensor(10983.0156, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10983.0107421875
tensor(10983.0156, grad_fn=<NegBackward0>) tensor(10983.0107, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10983.0068359375
tensor(10983.0107, grad_fn=<NegBackward0>) tensor(10983.0068, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10983.001953125
tensor(10983.0068, grad_fn=<NegBackward0>) tensor(10983.0020, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10982.9990234375
tensor(10983.0020, grad_fn=<NegBackward0>) tensor(10982.9990, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10982.9951171875
tensor(10982.9990, grad_fn=<NegBackward0>) tensor(10982.9951, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10982.9921875
tensor(10982.9951, grad_fn=<NegBackward0>) tensor(10982.9922, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10982.98828125
tensor(10982.9922, grad_fn=<NegBackward0>) tensor(10982.9883, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10982.986328125
tensor(10982.9883, grad_fn=<NegBackward0>) tensor(10982.9863, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10982.982421875
tensor(10982.9863, grad_fn=<NegBackward0>) tensor(10982.9824, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10982.98046875
tensor(10982.9824, grad_fn=<NegBackward0>) tensor(10982.9805, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10982.974609375
tensor(10982.9805, grad_fn=<NegBackward0>) tensor(10982.9746, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10982.970703125
tensor(10982.9746, grad_fn=<NegBackward0>) tensor(10982.9707, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10982.9638671875
tensor(10982.9707, grad_fn=<NegBackward0>) tensor(10982.9639, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10982.9580078125
tensor(10982.9639, grad_fn=<NegBackward0>) tensor(10982.9580, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10982.9453125
tensor(10982.9580, grad_fn=<NegBackward0>) tensor(10982.9453, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10982.927734375
tensor(10982.9453, grad_fn=<NegBackward0>) tensor(10982.9277, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10982.896484375
tensor(10982.9277, grad_fn=<NegBackward0>) tensor(10982.8965, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10982.884765625
tensor(10982.8965, grad_fn=<NegBackward0>) tensor(10982.8848, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10982.763671875
tensor(10982.8848, grad_fn=<NegBackward0>) tensor(10982.7637, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10982.6728515625
tensor(10982.7637, grad_fn=<NegBackward0>) tensor(10982.6729, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10982.60546875
tensor(10982.6729, grad_fn=<NegBackward0>) tensor(10982.6055, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10982.580078125
tensor(10982.6055, grad_fn=<NegBackward0>) tensor(10982.5801, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10982.5732421875
tensor(10982.5801, grad_fn=<NegBackward0>) tensor(10982.5732, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10982.572265625
tensor(10982.5732, grad_fn=<NegBackward0>) tensor(10982.5723, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10982.5732421875
tensor(10982.5723, grad_fn=<NegBackward0>) tensor(10982.5732, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10982.5703125
tensor(10982.5723, grad_fn=<NegBackward0>) tensor(10982.5703, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10982.5693359375
tensor(10982.5703, grad_fn=<NegBackward0>) tensor(10982.5693, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10982.5673828125
tensor(10982.5693, grad_fn=<NegBackward0>) tensor(10982.5674, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10982.5654296875
tensor(10982.5674, grad_fn=<NegBackward0>) tensor(10982.5654, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10982.5625
tensor(10982.5654, grad_fn=<NegBackward0>) tensor(10982.5625, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10982.5576171875
tensor(10982.5625, grad_fn=<NegBackward0>) tensor(10982.5576, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10982.548828125
tensor(10982.5576, grad_fn=<NegBackward0>) tensor(10982.5488, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10982.52734375
tensor(10982.5488, grad_fn=<NegBackward0>) tensor(10982.5273, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10980.2861328125
tensor(10982.5273, grad_fn=<NegBackward0>) tensor(10980.2861, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10887.3330078125
tensor(10980.2861, grad_fn=<NegBackward0>) tensor(10887.3330, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10878.1845703125
tensor(10887.3330, grad_fn=<NegBackward0>) tensor(10878.1846, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10877.4892578125
tensor(10878.1846, grad_fn=<NegBackward0>) tensor(10877.4893, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10871.642578125
tensor(10877.4893, grad_fn=<NegBackward0>) tensor(10871.6426, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10871.4091796875
tensor(10871.6426, grad_fn=<NegBackward0>) tensor(10871.4092, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10871.33984375
tensor(10871.4092, grad_fn=<NegBackward0>) tensor(10871.3398, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10871.3095703125
tensor(10871.3398, grad_fn=<NegBackward0>) tensor(10871.3096, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10871.2958984375
tensor(10871.3096, grad_fn=<NegBackward0>) tensor(10871.2959, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10871.271484375
tensor(10871.2959, grad_fn=<NegBackward0>) tensor(10871.2715, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10871.1298828125
tensor(10871.2715, grad_fn=<NegBackward0>) tensor(10871.1299, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10871.12890625
tensor(10871.1299, grad_fn=<NegBackward0>) tensor(10871.1289, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10871.0791015625
tensor(10871.1289, grad_fn=<NegBackward0>) tensor(10871.0791, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10871.0888671875
tensor(10871.0791, grad_fn=<NegBackward0>) tensor(10871.0889, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -10871.0048828125
tensor(10871.0791, grad_fn=<NegBackward0>) tensor(10871.0049, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10870.6103515625
tensor(10871.0049, grad_fn=<NegBackward0>) tensor(10870.6104, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10870.6728515625
tensor(10870.6104, grad_fn=<NegBackward0>) tensor(10870.6729, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10870.4931640625
tensor(10870.6104, grad_fn=<NegBackward0>) tensor(10870.4932, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10870.490234375
tensor(10870.4932, grad_fn=<NegBackward0>) tensor(10870.4902, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10870.484375
tensor(10870.4902, grad_fn=<NegBackward0>) tensor(10870.4844, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10870.4765625
tensor(10870.4844, grad_fn=<NegBackward0>) tensor(10870.4766, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10870.4736328125
tensor(10870.4766, grad_fn=<NegBackward0>) tensor(10870.4736, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10870.466796875
tensor(10870.4736, grad_fn=<NegBackward0>) tensor(10870.4668, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10870.46484375
tensor(10870.4668, grad_fn=<NegBackward0>) tensor(10870.4648, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10870.46484375
tensor(10870.4648, grad_fn=<NegBackward0>) tensor(10870.4648, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10870.4619140625
tensor(10870.4648, grad_fn=<NegBackward0>) tensor(10870.4619, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10870.48046875
tensor(10870.4619, grad_fn=<NegBackward0>) tensor(10870.4805, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10870.4365234375
tensor(10870.4619, grad_fn=<NegBackward0>) tensor(10870.4365, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10870.435546875
tensor(10870.4365, grad_fn=<NegBackward0>) tensor(10870.4355, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10870.435546875
tensor(10870.4355, grad_fn=<NegBackward0>) tensor(10870.4355, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10870.435546875
tensor(10870.4355, grad_fn=<NegBackward0>) tensor(10870.4355, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10870.43359375
tensor(10870.4355, grad_fn=<NegBackward0>) tensor(10870.4336, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10870.3115234375
tensor(10870.4336, grad_fn=<NegBackward0>) tensor(10870.3115, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10870.314453125
tensor(10870.3115, grad_fn=<NegBackward0>) tensor(10870.3145, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -10870.3095703125
tensor(10870.3115, grad_fn=<NegBackward0>) tensor(10870.3096, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10870.32421875
tensor(10870.3096, grad_fn=<NegBackward0>) tensor(10870.3242, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -10870.3076171875
tensor(10870.3096, grad_fn=<NegBackward0>) tensor(10870.3076, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10870.3056640625
tensor(10870.3076, grad_fn=<NegBackward0>) tensor(10870.3057, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10870.314453125
tensor(10870.3057, grad_fn=<NegBackward0>) tensor(10870.3145, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -10870.306640625
tensor(10870.3057, grad_fn=<NegBackward0>) tensor(10870.3066, grad_fn=<NegBackward0>)
2
pi: tensor([[0.3291, 0.6709],
        [0.8192, 0.1808]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4758, 0.5242], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2195, 0.1033],
         [0.6870, 0.2301]],

        [[0.7205, 0.1013],
         [0.5781, 0.6316]],

        [[0.5367, 0.0906],
         [0.5375, 0.6522]],

        [[0.7298, 0.0996],
         [0.5016, 0.7000]],

        [[0.6611, 0.1046],
         [0.5950, 0.5198]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 94
Adjusted Rand Index: 0.7721069260785004
time is 1
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 7
Adjusted Rand Index: 0.7368470050070483
time is 2
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 7
Adjusted Rand Index: 0.7369954580512469
time is 4
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080740404436667
Global Adjusted Rand Index: 0.036493404898812525
Average Adjusted Rand Index: 0.7797743828857894
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22164.3984375
inf tensor(22164.3984, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10985.0888671875
tensor(22164.3984, grad_fn=<NegBackward0>) tensor(10985.0889, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10984.4990234375
tensor(10985.0889, grad_fn=<NegBackward0>) tensor(10984.4990, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10984.294921875
tensor(10984.4990, grad_fn=<NegBackward0>) tensor(10984.2949, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10984.1474609375
tensor(10984.2949, grad_fn=<NegBackward0>) tensor(10984.1475, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10984.0166015625
tensor(10984.1475, grad_fn=<NegBackward0>) tensor(10984.0166, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10983.875
tensor(10984.0166, grad_fn=<NegBackward0>) tensor(10983.8750, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10983.728515625
tensor(10983.8750, grad_fn=<NegBackward0>) tensor(10983.7285, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10983.6083984375
tensor(10983.7285, grad_fn=<NegBackward0>) tensor(10983.6084, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10983.5126953125
tensor(10983.6084, grad_fn=<NegBackward0>) tensor(10983.5127, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10983.435546875
tensor(10983.5127, grad_fn=<NegBackward0>) tensor(10983.4355, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10983.3720703125
tensor(10983.4355, grad_fn=<NegBackward0>) tensor(10983.3721, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10983.3154296875
tensor(10983.3721, grad_fn=<NegBackward0>) tensor(10983.3154, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10983.2626953125
tensor(10983.3154, grad_fn=<NegBackward0>) tensor(10983.2627, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10983.216796875
tensor(10983.2627, grad_fn=<NegBackward0>) tensor(10983.2168, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10983.17578125
tensor(10983.2168, grad_fn=<NegBackward0>) tensor(10983.1758, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10983.1376953125
tensor(10983.1758, grad_fn=<NegBackward0>) tensor(10983.1377, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10983.1103515625
tensor(10983.1377, grad_fn=<NegBackward0>) tensor(10983.1104, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10983.0849609375
tensor(10983.1104, grad_fn=<NegBackward0>) tensor(10983.0850, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10983.0673828125
tensor(10983.0850, grad_fn=<NegBackward0>) tensor(10983.0674, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10983.0537109375
tensor(10983.0674, grad_fn=<NegBackward0>) tensor(10983.0537, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10983.0439453125
tensor(10983.0537, grad_fn=<NegBackward0>) tensor(10983.0439, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10983.0361328125
tensor(10983.0439, grad_fn=<NegBackward0>) tensor(10983.0361, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10983.0302734375
tensor(10983.0361, grad_fn=<NegBackward0>) tensor(10983.0303, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10983.0234375
tensor(10983.0303, grad_fn=<NegBackward0>) tensor(10983.0234, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10983.0185546875
tensor(10983.0234, grad_fn=<NegBackward0>) tensor(10983.0186, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10983.015625
tensor(10983.0186, grad_fn=<NegBackward0>) tensor(10983.0156, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10983.009765625
tensor(10983.0156, grad_fn=<NegBackward0>) tensor(10983.0098, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10983.0068359375
tensor(10983.0098, grad_fn=<NegBackward0>) tensor(10983.0068, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10983.00390625
tensor(10983.0068, grad_fn=<NegBackward0>) tensor(10983.0039, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10983.001953125
tensor(10983.0039, grad_fn=<NegBackward0>) tensor(10983.0020, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10982.998046875
tensor(10983.0020, grad_fn=<NegBackward0>) tensor(10982.9980, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10982.99609375
tensor(10982.9980, grad_fn=<NegBackward0>) tensor(10982.9961, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10982.994140625
tensor(10982.9961, grad_fn=<NegBackward0>) tensor(10982.9941, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10982.9912109375
tensor(10982.9941, grad_fn=<NegBackward0>) tensor(10982.9912, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10982.98828125
tensor(10982.9912, grad_fn=<NegBackward0>) tensor(10982.9883, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10982.9853515625
tensor(10982.9883, grad_fn=<NegBackward0>) tensor(10982.9854, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10982.984375
tensor(10982.9854, grad_fn=<NegBackward0>) tensor(10982.9844, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10982.9814453125
tensor(10982.9844, grad_fn=<NegBackward0>) tensor(10982.9814, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10982.9794921875
tensor(10982.9814, grad_fn=<NegBackward0>) tensor(10982.9795, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10982.9765625
tensor(10982.9795, grad_fn=<NegBackward0>) tensor(10982.9766, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10982.9716796875
tensor(10982.9766, grad_fn=<NegBackward0>) tensor(10982.9717, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10982.96875
tensor(10982.9717, grad_fn=<NegBackward0>) tensor(10982.9688, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10982.9638671875
tensor(10982.9688, grad_fn=<NegBackward0>) tensor(10982.9639, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10982.9560546875
tensor(10982.9639, grad_fn=<NegBackward0>) tensor(10982.9561, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10982.9443359375
tensor(10982.9561, grad_fn=<NegBackward0>) tensor(10982.9443, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10982.9248046875
tensor(10982.9443, grad_fn=<NegBackward0>) tensor(10982.9248, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10982.892578125
tensor(10982.9248, grad_fn=<NegBackward0>) tensor(10982.8926, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10982.8388671875
tensor(10982.8926, grad_fn=<NegBackward0>) tensor(10982.8389, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10982.7822265625
tensor(10982.8389, grad_fn=<NegBackward0>) tensor(10982.7822, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10982.681640625
tensor(10982.7822, grad_fn=<NegBackward0>) tensor(10982.6816, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10982.6103515625
tensor(10982.6816, grad_fn=<NegBackward0>) tensor(10982.6104, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10982.5751953125
tensor(10982.6104, grad_fn=<NegBackward0>) tensor(10982.5752, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10982.5625
tensor(10982.5752, grad_fn=<NegBackward0>) tensor(10982.5625, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10982.55859375
tensor(10982.5625, grad_fn=<NegBackward0>) tensor(10982.5586, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10982.5556640625
tensor(10982.5586, grad_fn=<NegBackward0>) tensor(10982.5557, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10982.5498046875
tensor(10982.5557, grad_fn=<NegBackward0>) tensor(10982.5498, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10982.541015625
tensor(10982.5498, grad_fn=<NegBackward0>) tensor(10982.5410, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10982.5146484375
tensor(10982.5410, grad_fn=<NegBackward0>) tensor(10982.5146, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10973.4169921875
tensor(10982.5146, grad_fn=<NegBackward0>) tensor(10973.4170, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10887.73046875
tensor(10973.4170, grad_fn=<NegBackward0>) tensor(10887.7305, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10878.3017578125
tensor(10887.7305, grad_fn=<NegBackward0>) tensor(10878.3018, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10874.3515625
tensor(10878.3018, grad_fn=<NegBackward0>) tensor(10874.3516, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10874.0810546875
tensor(10874.3516, grad_fn=<NegBackward0>) tensor(10874.0811, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10873.92578125
tensor(10874.0811, grad_fn=<NegBackward0>) tensor(10873.9258, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10871.0498046875
tensor(10873.9258, grad_fn=<NegBackward0>) tensor(10871.0498, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10871.0244140625
tensor(10871.0498, grad_fn=<NegBackward0>) tensor(10871.0244, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10871.0
tensor(10871.0244, grad_fn=<NegBackward0>) tensor(10871., grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10870.98828125
tensor(10871., grad_fn=<NegBackward0>) tensor(10870.9883, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10870.5087890625
tensor(10870.9883, grad_fn=<NegBackward0>) tensor(10870.5088, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10870.4833984375
tensor(10870.5088, grad_fn=<NegBackward0>) tensor(10870.4834, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10870.4814453125
tensor(10870.4834, grad_fn=<NegBackward0>) tensor(10870.4814, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10870.4755859375
tensor(10870.4814, grad_fn=<NegBackward0>) tensor(10870.4756, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10870.4736328125
tensor(10870.4756, grad_fn=<NegBackward0>) tensor(10870.4736, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10870.474609375
tensor(10870.4736, grad_fn=<NegBackward0>) tensor(10870.4746, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10870.4697265625
tensor(10870.4736, grad_fn=<NegBackward0>) tensor(10870.4697, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10870.4638671875
tensor(10870.4697, grad_fn=<NegBackward0>) tensor(10870.4639, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10870.4599609375
tensor(10870.4639, grad_fn=<NegBackward0>) tensor(10870.4600, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10870.458984375
tensor(10870.4600, grad_fn=<NegBackward0>) tensor(10870.4590, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10870.474609375
tensor(10870.4590, grad_fn=<NegBackward0>) tensor(10870.4746, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10870.455078125
tensor(10870.4590, grad_fn=<NegBackward0>) tensor(10870.4551, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10870.4443359375
tensor(10870.4551, grad_fn=<NegBackward0>) tensor(10870.4443, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10870.4453125
tensor(10870.4443, grad_fn=<NegBackward0>) tensor(10870.4453, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10870.4453125
tensor(10870.4443, grad_fn=<NegBackward0>) tensor(10870.4453, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -10870.443359375
tensor(10870.4443, grad_fn=<NegBackward0>) tensor(10870.4434, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10870.4423828125
tensor(10870.4434, grad_fn=<NegBackward0>) tensor(10870.4424, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10870.435546875
tensor(10870.4424, grad_fn=<NegBackward0>) tensor(10870.4355, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10870.4345703125
tensor(10870.4355, grad_fn=<NegBackward0>) tensor(10870.4346, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10870.52734375
tensor(10870.4346, grad_fn=<NegBackward0>) tensor(10870.5273, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10870.4326171875
tensor(10870.4346, grad_fn=<NegBackward0>) tensor(10870.4326, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10870.4326171875
tensor(10870.4326, grad_fn=<NegBackward0>) tensor(10870.4326, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10870.431640625
tensor(10870.4326, grad_fn=<NegBackward0>) tensor(10870.4316, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10870.4326171875
tensor(10870.4316, grad_fn=<NegBackward0>) tensor(10870.4326, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -10870.44921875
tensor(10870.4316, grad_fn=<NegBackward0>) tensor(10870.4492, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -10870.30859375
tensor(10870.4316, grad_fn=<NegBackward0>) tensor(10870.3086, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10870.30859375
tensor(10870.3086, grad_fn=<NegBackward0>) tensor(10870.3086, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10870.3095703125
tensor(10870.3086, grad_fn=<NegBackward0>) tensor(10870.3096, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -10870.3134765625
tensor(10870.3086, grad_fn=<NegBackward0>) tensor(10870.3135, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -10870.310546875
tensor(10870.3086, grad_fn=<NegBackward0>) tensor(10870.3105, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -10870.3076171875
tensor(10870.3086, grad_fn=<NegBackward0>) tensor(10870.3076, grad_fn=<NegBackward0>)
pi: tensor([[0.1800, 0.8200],
        [0.6699, 0.3301]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5229, 0.4771], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2305, 0.1034],
         [0.6581, 0.2191]],

        [[0.6766, 0.1014],
         [0.6441, 0.6592]],

        [[0.5940, 0.0903],
         [0.6908, 0.7041]],

        [[0.6618, 0.0994],
         [0.6185, 0.6563]],

        [[0.6286, 0.1049],
         [0.5103, 0.6796]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7721069260785004
time is 1
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 93
Adjusted Rand Index: 0.7368470050070483
time is 2
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 93
Adjusted Rand Index: 0.7369954580512469
time is 4
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080740404436667
Global Adjusted Rand Index: 0.036493404898812525
Average Adjusted Rand Index: 0.7797743828857894
[0.036493404898812525, 0.036493404898812525] [0.7797743828857894, 0.7797743828857894] [10870.3046875, 10870.310546875]
-------------------------------------
This iteration is 96
True Objective function: Loss = -10865.232338531672
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22757.2734375
inf tensor(22757.2734, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11006.4150390625
tensor(22757.2734, grad_fn=<NegBackward0>) tensor(11006.4150, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11005.1904296875
tensor(11006.4150, grad_fn=<NegBackward0>) tensor(11005.1904, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11004.38671875
tensor(11005.1904, grad_fn=<NegBackward0>) tensor(11004.3867, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11004.0869140625
tensor(11004.3867, grad_fn=<NegBackward0>) tensor(11004.0869, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11003.97265625
tensor(11004.0869, grad_fn=<NegBackward0>) tensor(11003.9727, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11003.9169921875
tensor(11003.9727, grad_fn=<NegBackward0>) tensor(11003.9170, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11003.88671875
tensor(11003.9170, grad_fn=<NegBackward0>) tensor(11003.8867, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11003.8671875
tensor(11003.8867, grad_fn=<NegBackward0>) tensor(11003.8672, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11003.8525390625
tensor(11003.8672, grad_fn=<NegBackward0>) tensor(11003.8525, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11003.8359375
tensor(11003.8525, grad_fn=<NegBackward0>) tensor(11003.8359, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11003.8193359375
tensor(11003.8359, grad_fn=<NegBackward0>) tensor(11003.8193, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11003.796875
tensor(11003.8193, grad_fn=<NegBackward0>) tensor(11003.7969, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11003.767578125
tensor(11003.7969, grad_fn=<NegBackward0>) tensor(11003.7676, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11003.7197265625
tensor(11003.7676, grad_fn=<NegBackward0>) tensor(11003.7197, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11003.5537109375
tensor(11003.7197, grad_fn=<NegBackward0>) tensor(11003.5537, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10979.1142578125
tensor(11003.5537, grad_fn=<NegBackward0>) tensor(10979.1143, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10861.4140625
tensor(10979.1143, grad_fn=<NegBackward0>) tensor(10861.4141, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10838.7392578125
tensor(10861.4141, grad_fn=<NegBackward0>) tensor(10838.7393, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10838.140625
tensor(10838.7393, grad_fn=<NegBackward0>) tensor(10838.1406, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10838.1181640625
tensor(10838.1406, grad_fn=<NegBackward0>) tensor(10838.1182, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10827.9130859375
tensor(10838.1182, grad_fn=<NegBackward0>) tensor(10827.9131, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10827.9052734375
tensor(10827.9131, grad_fn=<NegBackward0>) tensor(10827.9053, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10827.90234375
tensor(10827.9053, grad_fn=<NegBackward0>) tensor(10827.9023, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10827.8994140625
tensor(10827.9023, grad_fn=<NegBackward0>) tensor(10827.8994, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10827.8955078125
tensor(10827.8994, grad_fn=<NegBackward0>) tensor(10827.8955, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10827.875
tensor(10827.8955, grad_fn=<NegBackward0>) tensor(10827.8750, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10827.8447265625
tensor(10827.8750, grad_fn=<NegBackward0>) tensor(10827.8447, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10827.8427734375
tensor(10827.8447, grad_fn=<NegBackward0>) tensor(10827.8428, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10827.837890625
tensor(10827.8428, grad_fn=<NegBackward0>) tensor(10827.8379, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10822.2607421875
tensor(10827.8379, grad_fn=<NegBackward0>) tensor(10822.2607, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10822.2431640625
tensor(10822.2607, grad_fn=<NegBackward0>) tensor(10822.2432, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10821.6953125
tensor(10822.2432, grad_fn=<NegBackward0>) tensor(10821.6953, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10821.6875
tensor(10821.6953, grad_fn=<NegBackward0>) tensor(10821.6875, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10821.6796875
tensor(10821.6875, grad_fn=<NegBackward0>) tensor(10821.6797, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10821.6513671875
tensor(10821.6797, grad_fn=<NegBackward0>) tensor(10821.6514, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10821.642578125
tensor(10821.6514, grad_fn=<NegBackward0>) tensor(10821.6426, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10821.62890625
tensor(10821.6426, grad_fn=<NegBackward0>) tensor(10821.6289, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10821.6259765625
tensor(10821.6289, grad_fn=<NegBackward0>) tensor(10821.6260, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10821.6220703125
tensor(10821.6260, grad_fn=<NegBackward0>) tensor(10821.6221, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10821.6181640625
tensor(10821.6221, grad_fn=<NegBackward0>) tensor(10821.6182, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10821.6162109375
tensor(10821.6182, grad_fn=<NegBackward0>) tensor(10821.6162, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10821.615234375
tensor(10821.6162, grad_fn=<NegBackward0>) tensor(10821.6152, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10821.611328125
tensor(10821.6152, grad_fn=<NegBackward0>) tensor(10821.6113, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10821.6376953125
tensor(10821.6113, grad_fn=<NegBackward0>) tensor(10821.6377, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10821.611328125
tensor(10821.6113, grad_fn=<NegBackward0>) tensor(10821.6113, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10821.59375
tensor(10821.6113, grad_fn=<NegBackward0>) tensor(10821.5938, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10821.5830078125
tensor(10821.5938, grad_fn=<NegBackward0>) tensor(10821.5830, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10821.58203125
tensor(10821.5830, grad_fn=<NegBackward0>) tensor(10821.5820, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10821.5830078125
tensor(10821.5820, grad_fn=<NegBackward0>) tensor(10821.5830, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10821.583984375
tensor(10821.5820, grad_fn=<NegBackward0>) tensor(10821.5840, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -10821.58203125
tensor(10821.5820, grad_fn=<NegBackward0>) tensor(10821.5820, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10821.5810546875
tensor(10821.5820, grad_fn=<NegBackward0>) tensor(10821.5811, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10821.5810546875
tensor(10821.5811, grad_fn=<NegBackward0>) tensor(10821.5811, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10821.58203125
tensor(10821.5811, grad_fn=<NegBackward0>) tensor(10821.5820, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10821.3994140625
tensor(10821.5811, grad_fn=<NegBackward0>) tensor(10821.3994, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10821.3857421875
tensor(10821.3994, grad_fn=<NegBackward0>) tensor(10821.3857, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10821.3837890625
tensor(10821.3857, grad_fn=<NegBackward0>) tensor(10821.3838, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10821.3837890625
tensor(10821.3838, grad_fn=<NegBackward0>) tensor(10821.3838, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10821.3828125
tensor(10821.3838, grad_fn=<NegBackward0>) tensor(10821.3828, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10821.3798828125
tensor(10821.3828, grad_fn=<NegBackward0>) tensor(10821.3799, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10821.0576171875
tensor(10821.3799, grad_fn=<NegBackward0>) tensor(10821.0576, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10821.056640625
tensor(10821.0576, grad_fn=<NegBackward0>) tensor(10821.0566, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10821.0576171875
tensor(10821.0566, grad_fn=<NegBackward0>) tensor(10821.0576, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10821.05859375
tensor(10821.0566, grad_fn=<NegBackward0>) tensor(10821.0586, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -10821.0576171875
tensor(10821.0566, grad_fn=<NegBackward0>) tensor(10821.0576, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -10821.056640625
tensor(10821.0566, grad_fn=<NegBackward0>) tensor(10821.0566, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10821.060546875
tensor(10821.0566, grad_fn=<NegBackward0>) tensor(10821.0605, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10821.056640625
tensor(10821.0566, grad_fn=<NegBackward0>) tensor(10821.0566, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10821.0576171875
tensor(10821.0566, grad_fn=<NegBackward0>) tensor(10821.0576, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10821.0556640625
tensor(10821.0566, grad_fn=<NegBackward0>) tensor(10821.0557, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10821.0556640625
tensor(10821.0557, grad_fn=<NegBackward0>) tensor(10821.0557, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10821.0595703125
tensor(10821.0557, grad_fn=<NegBackward0>) tensor(10821.0596, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10821.0576171875
tensor(10821.0557, grad_fn=<NegBackward0>) tensor(10821.0576, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -10821.068359375
tensor(10821.0557, grad_fn=<NegBackward0>) tensor(10821.0684, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -10821.0556640625
tensor(10821.0557, grad_fn=<NegBackward0>) tensor(10821.0557, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10821.072265625
tensor(10821.0557, grad_fn=<NegBackward0>) tensor(10821.0723, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10821.056640625
tensor(10821.0557, grad_fn=<NegBackward0>) tensor(10821.0566, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -10821.056640625
tensor(10821.0557, grad_fn=<NegBackward0>) tensor(10821.0566, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -10821.0556640625
tensor(10821.0557, grad_fn=<NegBackward0>) tensor(10821.0557, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10821.0556640625
tensor(10821.0557, grad_fn=<NegBackward0>) tensor(10821.0557, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10821.0546875
tensor(10821.0557, grad_fn=<NegBackward0>) tensor(10821.0547, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10821.0400390625
tensor(10821.0547, grad_fn=<NegBackward0>) tensor(10821.0400, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10821.03125
tensor(10821.0400, grad_fn=<NegBackward0>) tensor(10821.0312, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10821.0302734375
tensor(10821.0312, grad_fn=<NegBackward0>) tensor(10821.0303, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10821.0302734375
tensor(10821.0303, grad_fn=<NegBackward0>) tensor(10821.0303, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10821.0400390625
tensor(10821.0303, grad_fn=<NegBackward0>) tensor(10821.0400, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10821.0361328125
tensor(10821.0303, grad_fn=<NegBackward0>) tensor(10821.0361, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -10821.0263671875
tensor(10821.0303, grad_fn=<NegBackward0>) tensor(10821.0264, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10821.0517578125
tensor(10821.0264, grad_fn=<NegBackward0>) tensor(10821.0518, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -10821.0263671875
tensor(10821.0264, grad_fn=<NegBackward0>) tensor(10821.0264, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10821.0283203125
tensor(10821.0264, grad_fn=<NegBackward0>) tensor(10821.0283, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10821.0263671875
tensor(10821.0264, grad_fn=<NegBackward0>) tensor(10821.0264, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10821.0263671875
tensor(10821.0264, grad_fn=<NegBackward0>) tensor(10821.0264, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10821.03515625
tensor(10821.0264, grad_fn=<NegBackward0>) tensor(10821.0352, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -10821.0263671875
tensor(10821.0264, grad_fn=<NegBackward0>) tensor(10821.0264, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10821.0263671875
tensor(10821.0264, grad_fn=<NegBackward0>) tensor(10821.0264, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10821.0615234375
tensor(10821.0264, grad_fn=<NegBackward0>) tensor(10821.0615, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -10821.0302734375
tensor(10821.0264, grad_fn=<NegBackward0>) tensor(10821.0303, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -10821.0693359375
tensor(10821.0264, grad_fn=<NegBackward0>) tensor(10821.0693, grad_fn=<NegBackward0>)
3
pi: tensor([[0.7974, 0.2026],
        [0.2692, 0.7308]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5389, 0.4611], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2478, 0.1024],
         [0.6702, 0.1929]],

        [[0.6565, 0.0904],
         [0.7118, 0.7051]],

        [[0.7147, 0.0907],
         [0.6953, 0.6960]],

        [[0.6136, 0.1067],
         [0.7032, 0.5636]],

        [[0.7206, 0.0887],
         [0.5723, 0.6596]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 1
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9207385189720222
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8444975548124031
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.844814436176263
time is 4
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448338943900694
Global Adjusted Rand Index: 0.868317124326497
Average Adjusted Rand Index: 0.867461729355
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23098.134765625
inf tensor(23098.1348, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11007.9404296875
tensor(23098.1348, grad_fn=<NegBackward0>) tensor(11007.9404, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11006.6875
tensor(11007.9404, grad_fn=<NegBackward0>) tensor(11006.6875, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11003.6630859375
tensor(11006.6875, grad_fn=<NegBackward0>) tensor(11003.6631, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10999.5439453125
tensor(11003.6631, grad_fn=<NegBackward0>) tensor(10999.5439, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10997.19921875
tensor(10999.5439, grad_fn=<NegBackward0>) tensor(10997.1992, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10993.8095703125
tensor(10997.1992, grad_fn=<NegBackward0>) tensor(10993.8096, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10969.4990234375
tensor(10993.8096, grad_fn=<NegBackward0>) tensor(10969.4990, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10889.669921875
tensor(10969.4990, grad_fn=<NegBackward0>) tensor(10889.6699, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10874.8330078125
tensor(10889.6699, grad_fn=<NegBackward0>) tensor(10874.8330, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10868.6220703125
tensor(10874.8330, grad_fn=<NegBackward0>) tensor(10868.6221, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10867.7158203125
tensor(10868.6221, grad_fn=<NegBackward0>) tensor(10867.7158, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10866.2412109375
tensor(10867.7158, grad_fn=<NegBackward0>) tensor(10866.2412, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10866.177734375
tensor(10866.2412, grad_fn=<NegBackward0>) tensor(10866.1777, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10865.748046875
tensor(10866.1777, grad_fn=<NegBackward0>) tensor(10865.7480, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10865.3935546875
tensor(10865.7480, grad_fn=<NegBackward0>) tensor(10865.3936, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10865.34765625
tensor(10865.3936, grad_fn=<NegBackward0>) tensor(10865.3477, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10865.32421875
tensor(10865.3477, grad_fn=<NegBackward0>) tensor(10865.3242, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10865.3046875
tensor(10865.3242, grad_fn=<NegBackward0>) tensor(10865.3047, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10865.1943359375
tensor(10865.3047, grad_fn=<NegBackward0>) tensor(10865.1943, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10865.1728515625
tensor(10865.1943, grad_fn=<NegBackward0>) tensor(10865.1729, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10865.146484375
tensor(10865.1729, grad_fn=<NegBackward0>) tensor(10865.1465, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10865.13671875
tensor(10865.1465, grad_fn=<NegBackward0>) tensor(10865.1367, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10865.1298828125
tensor(10865.1367, grad_fn=<NegBackward0>) tensor(10865.1299, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10865.1220703125
tensor(10865.1299, grad_fn=<NegBackward0>) tensor(10865.1221, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10865.1162109375
tensor(10865.1221, grad_fn=<NegBackward0>) tensor(10865.1162, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10865.103515625
tensor(10865.1162, grad_fn=<NegBackward0>) tensor(10865.1035, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10865.091796875
tensor(10865.1035, grad_fn=<NegBackward0>) tensor(10865.0918, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10865.0732421875
tensor(10865.0918, grad_fn=<NegBackward0>) tensor(10865.0732, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10864.9658203125
tensor(10865.0732, grad_fn=<NegBackward0>) tensor(10864.9658, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10864.6748046875
tensor(10864.9658, grad_fn=<NegBackward0>) tensor(10864.6748, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10864.1201171875
tensor(10864.6748, grad_fn=<NegBackward0>) tensor(10864.1201, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10863.8994140625
tensor(10864.1201, grad_fn=<NegBackward0>) tensor(10863.8994, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10863.7373046875
tensor(10863.8994, grad_fn=<NegBackward0>) tensor(10863.7373, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10863.5908203125
tensor(10863.7373, grad_fn=<NegBackward0>) tensor(10863.5908, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10863.5078125
tensor(10863.5908, grad_fn=<NegBackward0>) tensor(10863.5078, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10863.4892578125
tensor(10863.5078, grad_fn=<NegBackward0>) tensor(10863.4893, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10863.4775390625
tensor(10863.4893, grad_fn=<NegBackward0>) tensor(10863.4775, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10863.4697265625
tensor(10863.4775, grad_fn=<NegBackward0>) tensor(10863.4697, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10863.4609375
tensor(10863.4697, grad_fn=<NegBackward0>) tensor(10863.4609, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10863.4501953125
tensor(10863.4609, grad_fn=<NegBackward0>) tensor(10863.4502, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10863.4384765625
tensor(10863.4502, grad_fn=<NegBackward0>) tensor(10863.4385, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10863.4248046875
tensor(10863.4385, grad_fn=<NegBackward0>) tensor(10863.4248, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10863.3994140625
tensor(10863.4248, grad_fn=<NegBackward0>) tensor(10863.3994, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10863.29296875
tensor(10863.3994, grad_fn=<NegBackward0>) tensor(10863.2930, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10862.572265625
tensor(10863.2930, grad_fn=<NegBackward0>) tensor(10862.5723, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10824.626953125
tensor(10862.5723, grad_fn=<NegBackward0>) tensor(10824.6270, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10824.4169921875
tensor(10824.6270, grad_fn=<NegBackward0>) tensor(10824.4170, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10820.9716796875
tensor(10824.4170, grad_fn=<NegBackward0>) tensor(10820.9717, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10820.96484375
tensor(10820.9717, grad_fn=<NegBackward0>) tensor(10820.9648, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10820.9619140625
tensor(10820.9648, grad_fn=<NegBackward0>) tensor(10820.9619, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10820.9521484375
tensor(10820.9619, grad_fn=<NegBackward0>) tensor(10820.9521, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10820.9443359375
tensor(10820.9521, grad_fn=<NegBackward0>) tensor(10820.9443, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10820.9423828125
tensor(10820.9443, grad_fn=<NegBackward0>) tensor(10820.9424, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10820.9423828125
tensor(10820.9424, grad_fn=<NegBackward0>) tensor(10820.9424, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10820.9345703125
tensor(10820.9424, grad_fn=<NegBackward0>) tensor(10820.9346, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10820.927734375
tensor(10820.9346, grad_fn=<NegBackward0>) tensor(10820.9277, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10820.9306640625
tensor(10820.9277, grad_fn=<NegBackward0>) tensor(10820.9307, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10820.919921875
tensor(10820.9277, grad_fn=<NegBackward0>) tensor(10820.9199, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10820.9189453125
tensor(10820.9199, grad_fn=<NegBackward0>) tensor(10820.9189, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10820.91796875
tensor(10820.9189, grad_fn=<NegBackward0>) tensor(10820.9180, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10820.91796875
tensor(10820.9180, grad_fn=<NegBackward0>) tensor(10820.9180, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10820.9169921875
tensor(10820.9180, grad_fn=<NegBackward0>) tensor(10820.9170, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10820.9033203125
tensor(10820.9170, grad_fn=<NegBackward0>) tensor(10820.9033, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10820.9072265625
tensor(10820.9033, grad_fn=<NegBackward0>) tensor(10820.9072, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10820.9033203125
tensor(10820.9033, grad_fn=<NegBackward0>) tensor(10820.9033, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10820.90234375
tensor(10820.9033, grad_fn=<NegBackward0>) tensor(10820.9023, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10820.9033203125
tensor(10820.9023, grad_fn=<NegBackward0>) tensor(10820.9033, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10820.9033203125
tensor(10820.9023, grad_fn=<NegBackward0>) tensor(10820.9033, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -10820.9013671875
tensor(10820.9023, grad_fn=<NegBackward0>) tensor(10820.9014, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10820.9033203125
tensor(10820.9014, grad_fn=<NegBackward0>) tensor(10820.9033, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10820.90234375
tensor(10820.9014, grad_fn=<NegBackward0>) tensor(10820.9023, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -10820.900390625
tensor(10820.9014, grad_fn=<NegBackward0>) tensor(10820.9004, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10820.9013671875
tensor(10820.9004, grad_fn=<NegBackward0>) tensor(10820.9014, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -10820.8994140625
tensor(10820.9004, grad_fn=<NegBackward0>) tensor(10820.8994, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10820.8974609375
tensor(10820.8994, grad_fn=<NegBackward0>) tensor(10820.8975, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10820.896484375
tensor(10820.8975, grad_fn=<NegBackward0>) tensor(10820.8965, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10820.8955078125
tensor(10820.8965, grad_fn=<NegBackward0>) tensor(10820.8955, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10820.8955078125
tensor(10820.8955, grad_fn=<NegBackward0>) tensor(10820.8955, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10820.8955078125
tensor(10820.8955, grad_fn=<NegBackward0>) tensor(10820.8955, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10820.8974609375
tensor(10820.8955, grad_fn=<NegBackward0>) tensor(10820.8975, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10820.8955078125
tensor(10820.8955, grad_fn=<NegBackward0>) tensor(10820.8955, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10820.896484375
tensor(10820.8955, grad_fn=<NegBackward0>) tensor(10820.8965, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10820.9033203125
tensor(10820.8955, grad_fn=<NegBackward0>) tensor(10820.9033, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -10820.8935546875
tensor(10820.8955, grad_fn=<NegBackward0>) tensor(10820.8936, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10820.8955078125
tensor(10820.8936, grad_fn=<NegBackward0>) tensor(10820.8955, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -10820.8916015625
tensor(10820.8936, grad_fn=<NegBackward0>) tensor(10820.8916, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10820.892578125
tensor(10820.8916, grad_fn=<NegBackward0>) tensor(10820.8926, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -10820.8916015625
tensor(10820.8916, grad_fn=<NegBackward0>) tensor(10820.8916, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10820.8935546875
tensor(10820.8916, grad_fn=<NegBackward0>) tensor(10820.8936, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -10820.8916015625
tensor(10820.8916, grad_fn=<NegBackward0>) tensor(10820.8916, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10820.92578125
tensor(10820.8916, grad_fn=<NegBackward0>) tensor(10820.9258, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10820.8916015625
tensor(10820.8916, grad_fn=<NegBackward0>) tensor(10820.8916, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10820.892578125
tensor(10820.8916, grad_fn=<NegBackward0>) tensor(10820.8926, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -10820.8916015625
tensor(10820.8916, grad_fn=<NegBackward0>) tensor(10820.8916, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10820.89453125
tensor(10820.8916, grad_fn=<NegBackward0>) tensor(10820.8945, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -10820.904296875
tensor(10820.8916, grad_fn=<NegBackward0>) tensor(10820.9043, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -10820.9013671875
tensor(10820.8916, grad_fn=<NegBackward0>) tensor(10820.9014, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -10820.8955078125
tensor(10820.8916, grad_fn=<NegBackward0>) tensor(10820.8955, grad_fn=<NegBackward0>)
4
Iteration 9900: Loss = -10820.8916015625
tensor(10820.8916, grad_fn=<NegBackward0>) tensor(10820.8916, grad_fn=<NegBackward0>)
pi: tensor([[0.7974, 0.2026],
        [0.2686, 0.7314]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5387, 0.4613], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2479, 0.1025],
         [0.5075, 0.1928]],

        [[0.5428, 0.0908],
         [0.6246, 0.7210]],

        [[0.5914, 0.0907],
         [0.5120, 0.6937]],

        [[0.6408, 0.1067],
         [0.6963, 0.6808]],

        [[0.6295, 0.0888],
         [0.5601, 0.5248]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 1
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9207385189720222
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8444975548124031
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.844814436176263
time is 4
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448338943900694
Global Adjusted Rand Index: 0.868317124326497
Average Adjusted Rand Index: 0.867461729355
[0.868317124326497, 0.868317124326497] [0.867461729355, 0.867461729355] [10821.0263671875, 10820.892578125]
-------------------------------------
This iteration is 97
True Objective function: Loss = -11038.69225842151
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25584.955078125
inf tensor(25584.9551, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11076.583984375
tensor(25584.9551, grad_fn=<NegBackward0>) tensor(11076.5840, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11075.9453125
tensor(11076.5840, grad_fn=<NegBackward0>) tensor(11075.9453, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11075.81640625
tensor(11075.9453, grad_fn=<NegBackward0>) tensor(11075.8164, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11075.7392578125
tensor(11075.8164, grad_fn=<NegBackward0>) tensor(11075.7393, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11075.6748046875
tensor(11075.7393, grad_fn=<NegBackward0>) tensor(11075.6748, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11075.6123046875
tensor(11075.6748, grad_fn=<NegBackward0>) tensor(11075.6123, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11075.544921875
tensor(11075.6123, grad_fn=<NegBackward0>) tensor(11075.5449, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11075.46484375
tensor(11075.5449, grad_fn=<NegBackward0>) tensor(11075.4648, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11075.3642578125
tensor(11075.4648, grad_fn=<NegBackward0>) tensor(11075.3643, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11075.228515625
tensor(11075.3643, grad_fn=<NegBackward0>) tensor(11075.2285, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11075.111328125
tensor(11075.2285, grad_fn=<NegBackward0>) tensor(11075.1113, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11075.072265625
tensor(11075.1113, grad_fn=<NegBackward0>) tensor(11075.0723, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11074.9990234375
tensor(11075.0723, grad_fn=<NegBackward0>) tensor(11074.9990, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11074.58203125
tensor(11074.9990, grad_fn=<NegBackward0>) tensor(11074.5820, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11074.0400390625
tensor(11074.5820, grad_fn=<NegBackward0>) tensor(11074.0400, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11073.9091796875
tensor(11074.0400, grad_fn=<NegBackward0>) tensor(11073.9092, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11073.8232421875
tensor(11073.9092, grad_fn=<NegBackward0>) tensor(11073.8232, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11073.771484375
tensor(11073.8232, grad_fn=<NegBackward0>) tensor(11073.7715, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11073.7392578125
tensor(11073.7715, grad_fn=<NegBackward0>) tensor(11073.7393, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11073.7197265625
tensor(11073.7393, grad_fn=<NegBackward0>) tensor(11073.7197, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11073.7080078125
tensor(11073.7197, grad_fn=<NegBackward0>) tensor(11073.7080, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11073.7021484375
tensor(11073.7080, grad_fn=<NegBackward0>) tensor(11073.7021, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11073.6982421875
tensor(11073.7021, grad_fn=<NegBackward0>) tensor(11073.6982, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11073.6953125
tensor(11073.6982, grad_fn=<NegBackward0>) tensor(11073.6953, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11073.6962890625
tensor(11073.6953, grad_fn=<NegBackward0>) tensor(11073.6963, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -11073.697265625
tensor(11073.6953, grad_fn=<NegBackward0>) tensor(11073.6973, grad_fn=<NegBackward0>)
2
Iteration 2700: Loss = -11073.6953125
tensor(11073.6953, grad_fn=<NegBackward0>) tensor(11073.6953, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11073.6943359375
tensor(11073.6953, grad_fn=<NegBackward0>) tensor(11073.6943, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11073.6943359375
tensor(11073.6943, grad_fn=<NegBackward0>) tensor(11073.6943, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11073.6943359375
tensor(11073.6943, grad_fn=<NegBackward0>) tensor(11073.6943, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11073.6962890625
tensor(11073.6943, grad_fn=<NegBackward0>) tensor(11073.6963, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11073.693359375
tensor(11073.6943, grad_fn=<NegBackward0>) tensor(11073.6934, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11073.6943359375
tensor(11073.6934, grad_fn=<NegBackward0>) tensor(11073.6943, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11073.693359375
tensor(11073.6934, grad_fn=<NegBackward0>) tensor(11073.6934, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11073.6943359375
tensor(11073.6934, grad_fn=<NegBackward0>) tensor(11073.6943, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11073.6923828125
tensor(11073.6934, grad_fn=<NegBackward0>) tensor(11073.6924, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11073.6923828125
tensor(11073.6924, grad_fn=<NegBackward0>) tensor(11073.6924, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11073.69140625
tensor(11073.6924, grad_fn=<NegBackward0>) tensor(11073.6914, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11073.6943359375
tensor(11073.6914, grad_fn=<NegBackward0>) tensor(11073.6943, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11073.6923828125
tensor(11073.6914, grad_fn=<NegBackward0>) tensor(11073.6924, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -11073.69140625
tensor(11073.6914, grad_fn=<NegBackward0>) tensor(11073.6914, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11073.6923828125
tensor(11073.6914, grad_fn=<NegBackward0>) tensor(11073.6924, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11073.693359375
tensor(11073.6914, grad_fn=<NegBackward0>) tensor(11073.6934, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -11073.693359375
tensor(11073.6914, grad_fn=<NegBackward0>) tensor(11073.6934, grad_fn=<NegBackward0>)
3
Iteration 4500: Loss = -11073.693359375
tensor(11073.6914, grad_fn=<NegBackward0>) tensor(11073.6934, grad_fn=<NegBackward0>)
4
Iteration 4600: Loss = -11073.693359375
tensor(11073.6914, grad_fn=<NegBackward0>) tensor(11073.6934, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4600 due to no improvement.
pi: tensor([[0.0921, 0.9079],
        [0.0044, 0.9956]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1063, 0.8937], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.5879, 0.1978],
         [0.5758, 0.1619]],

        [[0.6329, 0.2590],
         [0.5243, 0.5196]],

        [[0.7235, 0.1997],
         [0.6424, 0.6586]],

        [[0.6404, 0.1150],
         [0.6496, 0.6769]],

        [[0.6289, 0.1824],
         [0.6714, 0.6084]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 39
Adjusted Rand Index: 0.04468294819844486
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.006983179780411681
Average Adjusted Rand Index: 0.00944735916698623
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21492.142578125
inf tensor(21492.1426, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11079.6005859375
tensor(21492.1426, grad_fn=<NegBackward0>) tensor(11079.6006, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11076.4130859375
tensor(11079.6006, grad_fn=<NegBackward0>) tensor(11076.4131, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11075.943359375
tensor(11076.4131, grad_fn=<NegBackward0>) tensor(11075.9434, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11075.8046875
tensor(11075.9434, grad_fn=<NegBackward0>) tensor(11075.8047, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11075.7255859375
tensor(11075.8047, grad_fn=<NegBackward0>) tensor(11075.7256, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11075.6552734375
tensor(11075.7256, grad_fn=<NegBackward0>) tensor(11075.6553, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11075.5810546875
tensor(11075.6553, grad_fn=<NegBackward0>) tensor(11075.5811, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11075.4951171875
tensor(11075.5811, grad_fn=<NegBackward0>) tensor(11075.4951, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11075.392578125
tensor(11075.4951, grad_fn=<NegBackward0>) tensor(11075.3926, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11075.2568359375
tensor(11075.3926, grad_fn=<NegBackward0>) tensor(11075.2568, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11075.056640625
tensor(11075.2568, grad_fn=<NegBackward0>) tensor(11075.0566, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11074.4091796875
tensor(11075.0566, grad_fn=<NegBackward0>) tensor(11074.4092, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11074.173828125
tensor(11074.4092, grad_fn=<NegBackward0>) tensor(11074.1738, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11074.0244140625
tensor(11074.1738, grad_fn=<NegBackward0>) tensor(11074.0244, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11073.9169921875
tensor(11074.0244, grad_fn=<NegBackward0>) tensor(11073.9170, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11073.837890625
tensor(11073.9170, grad_fn=<NegBackward0>) tensor(11073.8379, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11073.787109375
tensor(11073.8379, grad_fn=<NegBackward0>) tensor(11073.7871, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11073.7529296875
tensor(11073.7871, grad_fn=<NegBackward0>) tensor(11073.7529, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11073.7333984375
tensor(11073.7529, grad_fn=<NegBackward0>) tensor(11073.7334, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11073.7177734375
tensor(11073.7334, grad_fn=<NegBackward0>) tensor(11073.7178, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11073.7099609375
tensor(11073.7178, grad_fn=<NegBackward0>) tensor(11073.7100, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11073.7060546875
tensor(11073.7100, grad_fn=<NegBackward0>) tensor(11073.7061, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11073.7021484375
tensor(11073.7061, grad_fn=<NegBackward0>) tensor(11073.7021, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11073.7001953125
tensor(11073.7021, grad_fn=<NegBackward0>) tensor(11073.7002, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11073.6982421875
tensor(11073.7002, grad_fn=<NegBackward0>) tensor(11073.6982, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11073.697265625
tensor(11073.6982, grad_fn=<NegBackward0>) tensor(11073.6973, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11073.697265625
tensor(11073.6973, grad_fn=<NegBackward0>) tensor(11073.6973, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11073.6962890625
tensor(11073.6973, grad_fn=<NegBackward0>) tensor(11073.6963, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11073.6943359375
tensor(11073.6963, grad_fn=<NegBackward0>) tensor(11073.6943, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11073.6943359375
tensor(11073.6943, grad_fn=<NegBackward0>) tensor(11073.6943, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11073.693359375
tensor(11073.6943, grad_fn=<NegBackward0>) tensor(11073.6934, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11073.6943359375
tensor(11073.6934, grad_fn=<NegBackward0>) tensor(11073.6943, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11073.6953125
tensor(11073.6934, grad_fn=<NegBackward0>) tensor(11073.6953, grad_fn=<NegBackward0>)
2
Iteration 3400: Loss = -11073.693359375
tensor(11073.6934, grad_fn=<NegBackward0>) tensor(11073.6934, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11073.6953125
tensor(11073.6934, grad_fn=<NegBackward0>) tensor(11073.6953, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11073.6943359375
tensor(11073.6934, grad_fn=<NegBackward0>) tensor(11073.6943, grad_fn=<NegBackward0>)
2
Iteration 3700: Loss = -11073.6943359375
tensor(11073.6934, grad_fn=<NegBackward0>) tensor(11073.6943, grad_fn=<NegBackward0>)
3
Iteration 3800: Loss = -11073.6923828125
tensor(11073.6934, grad_fn=<NegBackward0>) tensor(11073.6924, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11073.6923828125
tensor(11073.6924, grad_fn=<NegBackward0>) tensor(11073.6924, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11073.6923828125
tensor(11073.6924, grad_fn=<NegBackward0>) tensor(11073.6924, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11073.6943359375
tensor(11073.6924, grad_fn=<NegBackward0>) tensor(11073.6943, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11073.6904296875
tensor(11073.6924, grad_fn=<NegBackward0>) tensor(11073.6904, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11073.69140625
tensor(11073.6904, grad_fn=<NegBackward0>) tensor(11073.6914, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11073.69140625
tensor(11073.6904, grad_fn=<NegBackward0>) tensor(11073.6914, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -11073.69140625
tensor(11073.6904, grad_fn=<NegBackward0>) tensor(11073.6914, grad_fn=<NegBackward0>)
3
Iteration 4600: Loss = -11073.6923828125
tensor(11073.6904, grad_fn=<NegBackward0>) tensor(11073.6924, grad_fn=<NegBackward0>)
4
Iteration 4700: Loss = -11073.693359375
tensor(11073.6904, grad_fn=<NegBackward0>) tensor(11073.6934, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4700 due to no improvement.
pi: tensor([[0.9957, 0.0043],
        [0.9077, 0.0923]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8937, 0.1063], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1619, 0.1978],
         [0.7202, 0.5879]],

        [[0.5158, 0.2590],
         [0.6307, 0.5940]],

        [[0.6524, 0.1992],
         [0.6479, 0.5413]],

        [[0.7210, 0.1154],
         [0.5333, 0.5706]],

        [[0.6064, 0.1823],
         [0.6130, 0.5903]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 61
Adjusted Rand Index: 0.04468294819844486
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.006983179780411681
Average Adjusted Rand Index: 0.00944735916698623
[0.006983179780411681, 0.006983179780411681] [0.00944735916698623, 0.00944735916698623] [11073.693359375, 11073.693359375]
-------------------------------------
This iteration is 98
True Objective function: Loss = -10884.1533190645
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22718.119140625
inf tensor(22718.1191, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10979.8701171875
tensor(22718.1191, grad_fn=<NegBackward0>) tensor(10979.8701, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10979.203125
tensor(10979.8701, grad_fn=<NegBackward0>) tensor(10979.2031, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10979.0244140625
tensor(10979.2031, grad_fn=<NegBackward0>) tensor(10979.0244, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10978.896484375
tensor(10979.0244, grad_fn=<NegBackward0>) tensor(10978.8965, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10978.7470703125
tensor(10978.8965, grad_fn=<NegBackward0>) tensor(10978.7471, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10978.5673828125
tensor(10978.7471, grad_fn=<NegBackward0>) tensor(10978.5674, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10978.412109375
tensor(10978.5674, grad_fn=<NegBackward0>) tensor(10978.4121, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10978.27734375
tensor(10978.4121, grad_fn=<NegBackward0>) tensor(10978.2773, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10978.1474609375
tensor(10978.2773, grad_fn=<NegBackward0>) tensor(10978.1475, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10978.0205078125
tensor(10978.1475, grad_fn=<NegBackward0>) tensor(10978.0205, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10977.9072265625
tensor(10978.0205, grad_fn=<NegBackward0>) tensor(10977.9072, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10977.8076171875
tensor(10977.9072, grad_fn=<NegBackward0>) tensor(10977.8076, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10977.708984375
tensor(10977.8076, grad_fn=<NegBackward0>) tensor(10977.7090, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10977.603515625
tensor(10977.7090, grad_fn=<NegBackward0>) tensor(10977.6035, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10977.4833984375
tensor(10977.6035, grad_fn=<NegBackward0>) tensor(10977.4834, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10977.3486328125
tensor(10977.4834, grad_fn=<NegBackward0>) tensor(10977.3486, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10977.205078125
tensor(10977.3486, grad_fn=<NegBackward0>) tensor(10977.2051, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10977.080078125
tensor(10977.2051, grad_fn=<NegBackward0>) tensor(10977.0801, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10976.9970703125
tensor(10977.0801, grad_fn=<NegBackward0>) tensor(10976.9971, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10976.947265625
tensor(10976.9971, grad_fn=<NegBackward0>) tensor(10976.9473, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10976.921875
tensor(10976.9473, grad_fn=<NegBackward0>) tensor(10976.9219, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10976.904296875
tensor(10976.9219, grad_fn=<NegBackward0>) tensor(10976.9043, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10976.8916015625
tensor(10976.9043, grad_fn=<NegBackward0>) tensor(10976.8916, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10976.8837890625
tensor(10976.8916, grad_fn=<NegBackward0>) tensor(10976.8838, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10976.8779296875
tensor(10976.8838, grad_fn=<NegBackward0>) tensor(10976.8779, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10976.87109375
tensor(10976.8779, grad_fn=<NegBackward0>) tensor(10976.8711, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10976.865234375
tensor(10976.8711, grad_fn=<NegBackward0>) tensor(10976.8652, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10976.8603515625
tensor(10976.8652, grad_fn=<NegBackward0>) tensor(10976.8604, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10976.85546875
tensor(10976.8604, grad_fn=<NegBackward0>) tensor(10976.8555, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10976.8515625
tensor(10976.8555, grad_fn=<NegBackward0>) tensor(10976.8516, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10976.8466796875
tensor(10976.8516, grad_fn=<NegBackward0>) tensor(10976.8467, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10976.845703125
tensor(10976.8467, grad_fn=<NegBackward0>) tensor(10976.8457, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10976.8427734375
tensor(10976.8457, grad_fn=<NegBackward0>) tensor(10976.8428, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10976.837890625
tensor(10976.8428, grad_fn=<NegBackward0>) tensor(10976.8379, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10976.8388671875
tensor(10976.8379, grad_fn=<NegBackward0>) tensor(10976.8389, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -10976.8359375
tensor(10976.8379, grad_fn=<NegBackward0>) tensor(10976.8359, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10976.8359375
tensor(10976.8359, grad_fn=<NegBackward0>) tensor(10976.8359, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10976.8330078125
tensor(10976.8359, grad_fn=<NegBackward0>) tensor(10976.8330, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10976.83203125
tensor(10976.8330, grad_fn=<NegBackward0>) tensor(10976.8320, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10976.83203125
tensor(10976.8320, grad_fn=<NegBackward0>) tensor(10976.8320, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10976.8310546875
tensor(10976.8320, grad_fn=<NegBackward0>) tensor(10976.8311, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10976.8310546875
tensor(10976.8311, grad_fn=<NegBackward0>) tensor(10976.8311, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10976.830078125
tensor(10976.8311, grad_fn=<NegBackward0>) tensor(10976.8301, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10976.8291015625
tensor(10976.8301, grad_fn=<NegBackward0>) tensor(10976.8291, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10976.8291015625
tensor(10976.8291, grad_fn=<NegBackward0>) tensor(10976.8291, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10976.828125
tensor(10976.8291, grad_fn=<NegBackward0>) tensor(10976.8281, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10976.8271484375
tensor(10976.8281, grad_fn=<NegBackward0>) tensor(10976.8271, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10976.828125
tensor(10976.8271, grad_fn=<NegBackward0>) tensor(10976.8281, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10976.8291015625
tensor(10976.8271, grad_fn=<NegBackward0>) tensor(10976.8291, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -10976.8271484375
tensor(10976.8271, grad_fn=<NegBackward0>) tensor(10976.8271, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10976.826171875
tensor(10976.8271, grad_fn=<NegBackward0>) tensor(10976.8262, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10976.830078125
tensor(10976.8262, grad_fn=<NegBackward0>) tensor(10976.8301, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10976.826171875
tensor(10976.8262, grad_fn=<NegBackward0>) tensor(10976.8262, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10976.826171875
tensor(10976.8262, grad_fn=<NegBackward0>) tensor(10976.8262, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10976.826171875
tensor(10976.8262, grad_fn=<NegBackward0>) tensor(10976.8262, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10976.828125
tensor(10976.8262, grad_fn=<NegBackward0>) tensor(10976.8281, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10976.828125
tensor(10976.8262, grad_fn=<NegBackward0>) tensor(10976.8281, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -10976.828125
tensor(10976.8262, grad_fn=<NegBackward0>) tensor(10976.8281, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -10976.8271484375
tensor(10976.8262, grad_fn=<NegBackward0>) tensor(10976.8271, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -10976.8271484375
tensor(10976.8262, grad_fn=<NegBackward0>) tensor(10976.8271, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6000 due to no improvement.
pi: tensor([[0.9495, 0.0505],
        [0.9972, 0.0028]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1683, 0.8317], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1584, 0.1654],
         [0.6458, 0.1706]],

        [[0.6682, 0.1356],
         [0.5757, 0.5669]],

        [[0.5379, 0.1554],
         [0.5161, 0.6162]],

        [[0.6966, 0.2219],
         [0.5554, 0.5272]],

        [[0.6155, 0.2077],
         [0.6710, 0.7287]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0007304767002590059
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22539.30078125
inf tensor(22539.3008, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10979.7939453125
tensor(22539.3008, grad_fn=<NegBackward0>) tensor(10979.7939, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10979.1669921875
tensor(10979.7939, grad_fn=<NegBackward0>) tensor(10979.1670, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10978.96875
tensor(10979.1670, grad_fn=<NegBackward0>) tensor(10978.9688, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10978.830078125
tensor(10978.9688, grad_fn=<NegBackward0>) tensor(10978.8301, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10978.6923828125
tensor(10978.8301, grad_fn=<NegBackward0>) tensor(10978.6924, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10978.548828125
tensor(10978.6924, grad_fn=<NegBackward0>) tensor(10978.5488, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10978.40625
tensor(10978.5488, grad_fn=<NegBackward0>) tensor(10978.4062, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10978.2548828125
tensor(10978.4062, grad_fn=<NegBackward0>) tensor(10978.2549, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10978.08984375
tensor(10978.2549, grad_fn=<NegBackward0>) tensor(10978.0898, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10977.9140625
tensor(10978.0898, grad_fn=<NegBackward0>) tensor(10977.9141, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10977.73046875
tensor(10977.9141, grad_fn=<NegBackward0>) tensor(10977.7305, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10977.5390625
tensor(10977.7305, grad_fn=<NegBackward0>) tensor(10977.5391, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10977.349609375
tensor(10977.5391, grad_fn=<NegBackward0>) tensor(10977.3496, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10977.1796875
tensor(10977.3496, grad_fn=<NegBackward0>) tensor(10977.1797, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10977.052734375
tensor(10977.1797, grad_fn=<NegBackward0>) tensor(10977.0527, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10976.96875
tensor(10977.0527, grad_fn=<NegBackward0>) tensor(10976.9688, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10976.9228515625
tensor(10976.9688, grad_fn=<NegBackward0>) tensor(10976.9229, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10976.8955078125
tensor(10976.9229, grad_fn=<NegBackward0>) tensor(10976.8955, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10976.8779296875
tensor(10976.8955, grad_fn=<NegBackward0>) tensor(10976.8779, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10976.869140625
tensor(10976.8779, grad_fn=<NegBackward0>) tensor(10976.8691, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10976.86328125
tensor(10976.8691, grad_fn=<NegBackward0>) tensor(10976.8633, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10976.857421875
tensor(10976.8633, grad_fn=<NegBackward0>) tensor(10976.8574, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10976.853515625
tensor(10976.8574, grad_fn=<NegBackward0>) tensor(10976.8535, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10976.849609375
tensor(10976.8535, grad_fn=<NegBackward0>) tensor(10976.8496, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10976.845703125
tensor(10976.8496, grad_fn=<NegBackward0>) tensor(10976.8457, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10976.84375
tensor(10976.8457, grad_fn=<NegBackward0>) tensor(10976.8438, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10976.8408203125
tensor(10976.8438, grad_fn=<NegBackward0>) tensor(10976.8408, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10976.83984375
tensor(10976.8408, grad_fn=<NegBackward0>) tensor(10976.8398, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10976.8369140625
tensor(10976.8398, grad_fn=<NegBackward0>) tensor(10976.8369, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10976.8359375
tensor(10976.8369, grad_fn=<NegBackward0>) tensor(10976.8359, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10976.8349609375
tensor(10976.8359, grad_fn=<NegBackward0>) tensor(10976.8350, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10976.8330078125
tensor(10976.8350, grad_fn=<NegBackward0>) tensor(10976.8330, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10976.83203125
tensor(10976.8330, grad_fn=<NegBackward0>) tensor(10976.8320, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10976.8310546875
tensor(10976.8320, grad_fn=<NegBackward0>) tensor(10976.8311, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10976.83203125
tensor(10976.8311, grad_fn=<NegBackward0>) tensor(10976.8320, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -10976.83203125
tensor(10976.8311, grad_fn=<NegBackward0>) tensor(10976.8320, grad_fn=<NegBackward0>)
2
Iteration 3700: Loss = -10976.830078125
tensor(10976.8311, grad_fn=<NegBackward0>) tensor(10976.8301, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10976.828125
tensor(10976.8301, grad_fn=<NegBackward0>) tensor(10976.8281, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10976.830078125
tensor(10976.8281, grad_fn=<NegBackward0>) tensor(10976.8301, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -10976.830078125
tensor(10976.8281, grad_fn=<NegBackward0>) tensor(10976.8301, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -10976.830078125
tensor(10976.8281, grad_fn=<NegBackward0>) tensor(10976.8301, grad_fn=<NegBackward0>)
3
Iteration 4200: Loss = -10976.8291015625
tensor(10976.8281, grad_fn=<NegBackward0>) tensor(10976.8291, grad_fn=<NegBackward0>)
4
Iteration 4300: Loss = -10976.8291015625
tensor(10976.8281, grad_fn=<NegBackward0>) tensor(10976.8291, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4300 due to no improvement.
pi: tensor([[0.9499, 0.0501],
        [0.9946, 0.0054]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1632, 0.8368], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1584, 0.1654],
         [0.6829, 0.1705]],

        [[0.5816, 0.1370],
         [0.6628, 0.5331]],

        [[0.5292, 0.1551],
         [0.6893, 0.7276]],

        [[0.5488, 0.2219],
         [0.5997, 0.6661]],

        [[0.5740, 0.2077],
         [0.6023, 0.6071]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0007304767002590059
Average Adjusted Rand Index: 0.0
[-0.0007304767002590059, -0.0007304767002590059] [0.0, 0.0] [10976.8271484375, 10976.8291015625]
-------------------------------------
This iteration is 99
True Objective function: Loss = -10906.246822778518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22720.3984375
inf tensor(22720.3984, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11015.8212890625
tensor(22720.3984, grad_fn=<NegBackward0>) tensor(11015.8213, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11015.07421875
tensor(11015.8213, grad_fn=<NegBackward0>) tensor(11015.0742, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11014.572265625
tensor(11015.0742, grad_fn=<NegBackward0>) tensor(11014.5723, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11013.3857421875
tensor(11014.5723, grad_fn=<NegBackward0>) tensor(11013.3857, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11013.08984375
tensor(11013.3857, grad_fn=<NegBackward0>) tensor(11013.0898, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11012.859375
tensor(11013.0898, grad_fn=<NegBackward0>) tensor(11012.8594, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11012.68359375
tensor(11012.8594, grad_fn=<NegBackward0>) tensor(11012.6836, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11012.5673828125
tensor(11012.6836, grad_fn=<NegBackward0>) tensor(11012.5674, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11012.4794921875
tensor(11012.5674, grad_fn=<NegBackward0>) tensor(11012.4795, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11012.3974609375
tensor(11012.4795, grad_fn=<NegBackward0>) tensor(11012.3975, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11012.3095703125
tensor(11012.3975, grad_fn=<NegBackward0>) tensor(11012.3096, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11012.1962890625
tensor(11012.3096, grad_fn=<NegBackward0>) tensor(11012.1963, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11012.0283203125
tensor(11012.1963, grad_fn=<NegBackward0>) tensor(11012.0283, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11011.74609375
tensor(11012.0283, grad_fn=<NegBackward0>) tensor(11011.7461, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11011.0947265625
tensor(11011.7461, grad_fn=<NegBackward0>) tensor(11011.0947, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10907.953125
tensor(11011.0947, grad_fn=<NegBackward0>) tensor(10907.9531, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10854.02734375
tensor(10907.9531, grad_fn=<NegBackward0>) tensor(10854.0273, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10852.15234375
tensor(10854.0273, grad_fn=<NegBackward0>) tensor(10852.1523, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10851.9580078125
tensor(10852.1523, grad_fn=<NegBackward0>) tensor(10851.9580, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10851.5009765625
tensor(10851.9580, grad_fn=<NegBackward0>) tensor(10851.5010, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10851.328125
tensor(10851.5010, grad_fn=<NegBackward0>) tensor(10851.3281, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10851.3056640625
tensor(10851.3281, grad_fn=<NegBackward0>) tensor(10851.3057, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10851.2900390625
tensor(10851.3057, grad_fn=<NegBackward0>) tensor(10851.2900, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10851.2763671875
tensor(10851.2900, grad_fn=<NegBackward0>) tensor(10851.2764, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10851.2666015625
tensor(10851.2764, grad_fn=<NegBackward0>) tensor(10851.2666, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10851.2529296875
tensor(10851.2666, grad_fn=<NegBackward0>) tensor(10851.2529, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10851.244140625
tensor(10851.2529, grad_fn=<NegBackward0>) tensor(10851.2441, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10851.240234375
tensor(10851.2441, grad_fn=<NegBackward0>) tensor(10851.2402, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10851.23828125
tensor(10851.2402, grad_fn=<NegBackward0>) tensor(10851.2383, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10851.234375
tensor(10851.2383, grad_fn=<NegBackward0>) tensor(10851.2344, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10851.2314453125
tensor(10851.2344, grad_fn=<NegBackward0>) tensor(10851.2314, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10851.23046875
tensor(10851.2314, grad_fn=<NegBackward0>) tensor(10851.2305, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10851.2275390625
tensor(10851.2305, grad_fn=<NegBackward0>) tensor(10851.2275, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10851.2265625
tensor(10851.2275, grad_fn=<NegBackward0>) tensor(10851.2266, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10851.2255859375
tensor(10851.2266, grad_fn=<NegBackward0>) tensor(10851.2256, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10851.2236328125
tensor(10851.2256, grad_fn=<NegBackward0>) tensor(10851.2236, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10851.22265625
tensor(10851.2236, grad_fn=<NegBackward0>) tensor(10851.2227, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10851.2197265625
tensor(10851.2227, grad_fn=<NegBackward0>) tensor(10851.2197, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10851.2197265625
tensor(10851.2197, grad_fn=<NegBackward0>) tensor(10851.2197, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10851.2177734375
tensor(10851.2197, grad_fn=<NegBackward0>) tensor(10851.2178, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10851.216796875
tensor(10851.2178, grad_fn=<NegBackward0>) tensor(10851.2168, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10851.2138671875
tensor(10851.2168, grad_fn=<NegBackward0>) tensor(10851.2139, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10851.2119140625
tensor(10851.2139, grad_fn=<NegBackward0>) tensor(10851.2119, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10851.208984375
tensor(10851.2119, grad_fn=<NegBackward0>) tensor(10851.2090, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10851.193359375
tensor(10851.2090, grad_fn=<NegBackward0>) tensor(10851.1934, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10851.1865234375
tensor(10851.1934, grad_fn=<NegBackward0>) tensor(10851.1865, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10851.185546875
tensor(10851.1865, grad_fn=<NegBackward0>) tensor(10851.1855, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10851.1826171875
tensor(10851.1855, grad_fn=<NegBackward0>) tensor(10851.1826, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10851.181640625
tensor(10851.1826, grad_fn=<NegBackward0>) tensor(10851.1816, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10851.1796875
tensor(10851.1816, grad_fn=<NegBackward0>) tensor(10851.1797, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10851.1787109375
tensor(10851.1797, grad_fn=<NegBackward0>) tensor(10851.1787, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10851.193359375
tensor(10851.1787, grad_fn=<NegBackward0>) tensor(10851.1934, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10851.16015625
tensor(10851.1787, grad_fn=<NegBackward0>) tensor(10851.1602, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10851.158203125
tensor(10851.1602, grad_fn=<NegBackward0>) tensor(10851.1582, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10851.158203125
tensor(10851.1582, grad_fn=<NegBackward0>) tensor(10851.1582, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10851.158203125
tensor(10851.1582, grad_fn=<NegBackward0>) tensor(10851.1582, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10851.1591796875
tensor(10851.1582, grad_fn=<NegBackward0>) tensor(10851.1592, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10851.1591796875
tensor(10851.1582, grad_fn=<NegBackward0>) tensor(10851.1592, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -10851.1552734375
tensor(10851.1582, grad_fn=<NegBackward0>) tensor(10851.1553, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10851.1552734375
tensor(10851.1553, grad_fn=<NegBackward0>) tensor(10851.1553, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10851.154296875
tensor(10851.1553, grad_fn=<NegBackward0>) tensor(10851.1543, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10851.1611328125
tensor(10851.1543, grad_fn=<NegBackward0>) tensor(10851.1611, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10851.1552734375
tensor(10851.1543, grad_fn=<NegBackward0>) tensor(10851.1553, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -10851.1513671875
tensor(10851.1543, grad_fn=<NegBackward0>) tensor(10851.1514, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10851.1484375
tensor(10851.1514, grad_fn=<NegBackward0>) tensor(10851.1484, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10851.1474609375
tensor(10851.1484, grad_fn=<NegBackward0>) tensor(10851.1475, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10851.1484375
tensor(10851.1475, grad_fn=<NegBackward0>) tensor(10851.1484, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10851.1552734375
tensor(10851.1475, grad_fn=<NegBackward0>) tensor(10851.1553, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -10851.1474609375
tensor(10851.1475, grad_fn=<NegBackward0>) tensor(10851.1475, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10851.1474609375
tensor(10851.1475, grad_fn=<NegBackward0>) tensor(10851.1475, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10851.1474609375
tensor(10851.1475, grad_fn=<NegBackward0>) tensor(10851.1475, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10851.146484375
tensor(10851.1475, grad_fn=<NegBackward0>) tensor(10851.1465, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10851.15625
tensor(10851.1465, grad_fn=<NegBackward0>) tensor(10851.1562, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -10851.146484375
tensor(10851.1465, grad_fn=<NegBackward0>) tensor(10851.1465, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10851.1513671875
tensor(10851.1465, grad_fn=<NegBackward0>) tensor(10851.1514, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10851.1533203125
tensor(10851.1465, grad_fn=<NegBackward0>) tensor(10851.1533, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -10851.1455078125
tensor(10851.1465, grad_fn=<NegBackward0>) tensor(10851.1455, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10851.1455078125
tensor(10851.1455, grad_fn=<NegBackward0>) tensor(10851.1455, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10851.1376953125
tensor(10851.1455, grad_fn=<NegBackward0>) tensor(10851.1377, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10851.1171875
tensor(10851.1377, grad_fn=<NegBackward0>) tensor(10851.1172, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10851.1162109375
tensor(10851.1172, grad_fn=<NegBackward0>) tensor(10851.1162, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10851.1171875
tensor(10851.1162, grad_fn=<NegBackward0>) tensor(10851.1172, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10851.1171875
tensor(10851.1162, grad_fn=<NegBackward0>) tensor(10851.1172, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -10851.1357421875
tensor(10851.1162, grad_fn=<NegBackward0>) tensor(10851.1357, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -10851.1162109375
tensor(10851.1162, grad_fn=<NegBackward0>) tensor(10851.1162, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10851.1171875
tensor(10851.1162, grad_fn=<NegBackward0>) tensor(10851.1172, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10851.1171875
tensor(10851.1162, grad_fn=<NegBackward0>) tensor(10851.1172, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -10851.1162109375
tensor(10851.1162, grad_fn=<NegBackward0>) tensor(10851.1162, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10851.1181640625
tensor(10851.1162, grad_fn=<NegBackward0>) tensor(10851.1182, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -10851.1162109375
tensor(10851.1162, grad_fn=<NegBackward0>) tensor(10851.1162, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10851.171875
tensor(10851.1162, grad_fn=<NegBackward0>) tensor(10851.1719, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10851.1162109375
tensor(10851.1162, grad_fn=<NegBackward0>) tensor(10851.1162, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10851.1171875
tensor(10851.1162, grad_fn=<NegBackward0>) tensor(10851.1172, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -10851.1181640625
tensor(10851.1162, grad_fn=<NegBackward0>) tensor(10851.1182, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -10851.1171875
tensor(10851.1162, grad_fn=<NegBackward0>) tensor(10851.1172, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -10851.15625
tensor(10851.1162, grad_fn=<NegBackward0>) tensor(10851.1562, grad_fn=<NegBackward0>)
4
Iteration 9700: Loss = -10851.1162109375
tensor(10851.1162, grad_fn=<NegBackward0>) tensor(10851.1162, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10851.11328125
tensor(10851.1162, grad_fn=<NegBackward0>) tensor(10851.1133, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10851.111328125
tensor(10851.1133, grad_fn=<NegBackward0>) tensor(10851.1113, grad_fn=<NegBackward0>)
pi: tensor([[0.7499, 0.2501],
        [0.2665, 0.7335]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5359, 0.4641], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2529, 0.1008],
         [0.6826, 0.1998]],

        [[0.5105, 0.1020],
         [0.5902, 0.5482]],

        [[0.7043, 0.0845],
         [0.5584, 0.6704]],

        [[0.5824, 0.0982],
         [0.5816, 0.6658]],

        [[0.7106, 0.1052],
         [0.5388, 0.5734]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.844846433231073
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080650161849864
time is 2
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 9
Adjusted Rand Index: 0.6690584936388014
Global Adjusted Rand Index: 0.816849462095497
Average Adjusted Rand Index: 0.8175248130497994
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21070.1015625
inf tensor(21070.1016, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11015.626953125
tensor(21070.1016, grad_fn=<NegBackward0>) tensor(11015.6270, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11014.9736328125
tensor(11015.6270, grad_fn=<NegBackward0>) tensor(11014.9736, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11014.0146484375
tensor(11014.9736, grad_fn=<NegBackward0>) tensor(11014.0146, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11013.947265625
tensor(11014.0146, grad_fn=<NegBackward0>) tensor(11013.9473, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11013.8994140625
tensor(11013.9473, grad_fn=<NegBackward0>) tensor(11013.8994, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11013.85546875
tensor(11013.8994, grad_fn=<NegBackward0>) tensor(11013.8555, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11013.8115234375
tensor(11013.8555, grad_fn=<NegBackward0>) tensor(11013.8115, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11013.767578125
tensor(11013.8115, grad_fn=<NegBackward0>) tensor(11013.7676, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11013.732421875
tensor(11013.7676, grad_fn=<NegBackward0>) tensor(11013.7324, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11013.703125
tensor(11013.7324, grad_fn=<NegBackward0>) tensor(11013.7031, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11013.681640625
tensor(11013.7031, grad_fn=<NegBackward0>) tensor(11013.6816, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11013.66796875
tensor(11013.6816, grad_fn=<NegBackward0>) tensor(11013.6680, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11013.658203125
tensor(11013.6680, grad_fn=<NegBackward0>) tensor(11013.6582, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11013.6494140625
tensor(11013.6582, grad_fn=<NegBackward0>) tensor(11013.6494, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11013.6416015625
tensor(11013.6494, grad_fn=<NegBackward0>) tensor(11013.6416, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11013.6328125
tensor(11013.6416, grad_fn=<NegBackward0>) tensor(11013.6328, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11013.625
tensor(11013.6328, grad_fn=<NegBackward0>) tensor(11013.6250, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11013.615234375
tensor(11013.6250, grad_fn=<NegBackward0>) tensor(11013.6152, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11013.603515625
tensor(11013.6152, grad_fn=<NegBackward0>) tensor(11013.6035, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11013.5849609375
tensor(11013.6035, grad_fn=<NegBackward0>) tensor(11013.5850, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11013.5615234375
tensor(11013.5850, grad_fn=<NegBackward0>) tensor(11013.5615, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11013.5302734375
tensor(11013.5615, grad_fn=<NegBackward0>) tensor(11013.5303, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11013.4921875
tensor(11013.5303, grad_fn=<NegBackward0>) tensor(11013.4922, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11013.455078125
tensor(11013.4922, grad_fn=<NegBackward0>) tensor(11013.4551, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11013.427734375
tensor(11013.4551, grad_fn=<NegBackward0>) tensor(11013.4277, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11013.408203125
tensor(11013.4277, grad_fn=<NegBackward0>) tensor(11013.4082, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11013.3974609375
tensor(11013.4082, grad_fn=<NegBackward0>) tensor(11013.3975, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11013.3896484375
tensor(11013.3975, grad_fn=<NegBackward0>) tensor(11013.3896, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11013.3828125
tensor(11013.3896, grad_fn=<NegBackward0>) tensor(11013.3828, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11013.380859375
tensor(11013.3828, grad_fn=<NegBackward0>) tensor(11013.3809, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11013.375
tensor(11013.3809, grad_fn=<NegBackward0>) tensor(11013.3750, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11013.3740234375
tensor(11013.3750, grad_fn=<NegBackward0>) tensor(11013.3740, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11013.37109375
tensor(11013.3740, grad_fn=<NegBackward0>) tensor(11013.3711, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11013.3701171875
tensor(11013.3711, grad_fn=<NegBackward0>) tensor(11013.3701, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11013.369140625
tensor(11013.3701, grad_fn=<NegBackward0>) tensor(11013.3691, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11013.3681640625
tensor(11013.3691, grad_fn=<NegBackward0>) tensor(11013.3682, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11013.3671875
tensor(11013.3682, grad_fn=<NegBackward0>) tensor(11013.3672, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11013.3662109375
tensor(11013.3672, grad_fn=<NegBackward0>) tensor(11013.3662, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11013.3642578125
tensor(11013.3662, grad_fn=<NegBackward0>) tensor(11013.3643, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11013.36328125
tensor(11013.3643, grad_fn=<NegBackward0>) tensor(11013.3633, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11013.361328125
tensor(11013.3633, grad_fn=<NegBackward0>) tensor(11013.3613, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11013.361328125
tensor(11013.3613, grad_fn=<NegBackward0>) tensor(11013.3613, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11013.361328125
tensor(11013.3613, grad_fn=<NegBackward0>) tensor(11013.3613, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11013.36328125
tensor(11013.3613, grad_fn=<NegBackward0>) tensor(11013.3633, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11013.361328125
tensor(11013.3613, grad_fn=<NegBackward0>) tensor(11013.3613, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11013.3603515625
tensor(11013.3613, grad_fn=<NegBackward0>) tensor(11013.3604, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11013.3603515625
tensor(11013.3604, grad_fn=<NegBackward0>) tensor(11013.3604, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11013.359375
tensor(11013.3604, grad_fn=<NegBackward0>) tensor(11013.3594, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11013.3603515625
tensor(11013.3594, grad_fn=<NegBackward0>) tensor(11013.3604, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11013.359375
tensor(11013.3594, grad_fn=<NegBackward0>) tensor(11013.3594, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11013.3603515625
tensor(11013.3594, grad_fn=<NegBackward0>) tensor(11013.3604, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11013.359375
tensor(11013.3594, grad_fn=<NegBackward0>) tensor(11013.3594, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11013.359375
tensor(11013.3594, grad_fn=<NegBackward0>) tensor(11013.3594, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11013.3603515625
tensor(11013.3594, grad_fn=<NegBackward0>) tensor(11013.3604, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11013.357421875
tensor(11013.3594, grad_fn=<NegBackward0>) tensor(11013.3574, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11013.359375
tensor(11013.3574, grad_fn=<NegBackward0>) tensor(11013.3594, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11013.3603515625
tensor(11013.3574, grad_fn=<NegBackward0>) tensor(11013.3604, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11013.359375
tensor(11013.3574, grad_fn=<NegBackward0>) tensor(11013.3594, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -11013.359375
tensor(11013.3574, grad_fn=<NegBackward0>) tensor(11013.3594, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -11013.3583984375
tensor(11013.3574, grad_fn=<NegBackward0>) tensor(11013.3584, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6000 due to no improvement.
pi: tensor([[0.9872, 0.0128],
        [0.9987, 0.0013]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9420, 0.0580], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1624, 0.2208],
         [0.5351, 0.2869]],

        [[0.6461, 0.2022],
         [0.7237, 0.6136]],

        [[0.7228, 0.0849],
         [0.6177, 0.6670]],

        [[0.6312, 0.0897],
         [0.7020, 0.5093]],

        [[0.5629, 0.1768],
         [0.5964, 0.6563]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.005119313988496187
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008164058643848803
Average Adjusted Rand Index: -0.0010238627976992373
[0.816849462095497, -0.0008164058643848803] [0.8175248130497994, -0.0010238627976992373] [10851.111328125, 11013.3583984375]
